<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.12544] Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality</title><meta property="og:description" content="In human interaction, gestures serve various functions such as marking speech rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the imp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.12544">

<!--Generated on Fri Jul  5 19:13:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Multimodal Interaction,  Explanation,  Understanding,  Gesture,  Study">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amelie Sophie Robrecht
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5622-8248" title="ORCID identifier" class="ltx_ref">0000-0001-5622-8248</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_streetaddress">Universitätsstraße 25</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hendric Voss
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0003-3646-7702" title="ORCID identifier" class="ltx_ref">0009-0003-3646-7702</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_streetaddress">Universitätsstraße 25</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lisa Gottschalk
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0007-8084-4439" title="ORCID identifier" class="ltx_ref">0009-0007-8084-4439</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_streetaddress">Universitätsstraße 25</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Kopp
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4047-9277" title="ORCID identifier" class="ltx_ref">0000-0002-4047-9277</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_streetaddress">Universitätsstraße 25</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id17.id1" class="ltx_p">In human interaction, gestures serve various functions such as marking speech rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impact of gestures on explanations provided by virtual agents remains underexplored. A user study was carried out to investigate how different types of gestures influence perceived interaction quality and listener understanding. This study addresses the effect of gestures in explanation by developing an embodied virtual explainer integrating both beat gestures and iconic gestures to enhance its automatically generated verbal explanations. Our model combines beat gestures generated by a learned speech-driven synthesis module with manually captured iconic gestures, supporting the agent’s verbal expressions about the board game Quarto! as an explanation scenario.
Findings indicate that neither the use of iconic gestures alone nor their combination with beat gestures outperforms the baseline or beat-only conditions in terms of understanding. Nonetheless, compared to prior research, the embodied agent significantly enhances understanding.</p>
</div>
<div class="ltx_keywords">Multimodal Interaction, Explanation, Understanding, Gesture, Study
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; ; </span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Intelligent agents</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing User studies</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Multimodal interaction</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure"><img src="/html/2406.12544/assets/pictures/gesture_header_smaller.png" id="S0.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Examples of iconic gestures used by the virtual agent in the explanation of the board game <span id="S0.F1.2.1" class="ltx_text ltx_font_italic">Quarto!</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Successful human communication involves multiple forms of modalities, including spoken language, facial cues, and body language. Understanding and generating these multimodal cues allows us to have meaningful and nuanced interactions in our everyday life <cite class="ltx_cite ltx_citemacro_citep">(Cassell et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">1999</a>; Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>)</cite>.
Current research on the social aspects of gestures is mainly focused on the effect gestures have on collaboration or interpretation tasks, with a clear focus on emerging gesture comprehension in children and young adults <cite class="ltx_cite ltx_citemacro_citep">(Mcneill, <a href="#bib.bib68" title="" class="ltx_ref">1986</a>; Aussems and Kita, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Zvaigzne et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2019</a>; Magid and Pyers, <a href="#bib.bib60" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The importance of non-verbal communication for a successful collaborative or co-constructive explanation is long known <cite class="ltx_cite ltx_citemacro_citep">(McNeill, <a href="#bib.bib67" title="" class="ltx_ref">1985</a>; Lund, <a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite>. Thus there is little research on how the explainee’s understanding in an explanation is influenced by the performed gestures, especially with regard to studies that vary gesture parameters and quantitatively measure learning outcomes. As there is a lack of studies that vary gesture parameters, particularly on a fine scale, and quantitatively measure learning outcomes <cite class="ltx_cite ltx_citemacro_citep">(Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>; Sinatra et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2021</a>)</cite>, we provide such a study.
In addition, most insights into the effects gestures have on an explanation are from human-human interaction <cite class="ltx_cite ltx_citemacro_citep">(Hostetter, <a href="#bib.bib35" title="" class="ltx_ref">2011</a>)</cite> or focus on the human explainee expressing (mis)understanding <cite class="ltx_cite ltx_citemacro_citep">(Lund, <a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite>, while it is still unclear how far these effects can be transferred to human-agent explanations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Regarding the automatic generation of non-verbal behavior, the main focus on co-speech gesture generation currently lies in producing natural and human-like gestural motion from multiple input modalities <cite class="ltx_cite ltx_citemacro_citep">(Nyatsanga et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Although these gestures look natural, they still convey only very limited additional non-verbal meaning <cite class="ltx_cite ltx_citemacro_citep">(Voß and Kopp, <a href="#bib.bib93" title="" class="ltx_ref">2023b</a>)</cite>, and thus do not allow for the interaction quality achievable with virtual agents that employ iconic gestures in human-agent scenarios <cite class="ltx_cite ltx_citemacro_citep">(Bergmann et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we investigate the effects of different types of gestures on the objective understanding and the perceived interaction quality of a multimodal explanation. Our focus lies on studying the understanding and engagement of participants by distinguishing between shallow understanding and deep enabling <cite class="ltx_cite ltx_citemacro_citep">(Buschmeier et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. Therefore, understanding can be divided into comprehension – the knowing that – and enabledness – the knowing how. Both forms of understanding can either appear in a shallow – only surface knowledge – or a deep – being able to draw connections between information – way.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To this end, we developed a multimodal virtual agent designed to explain the mechanics of the board game Quarto!. It is based on a novel model that complements the automatic generation of spoken explanations (from prior work) by generating four different kinds of gestures (baseline, beat, iconic, and mixed). An iconic gesture (illustrated in Figure <a href="#S0.F1" title="Figure 1 ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) conveys semantic information by presenting a depiction of the related aspects <cite class="ltx_cite ltx_citemacro_citep">(Bergmann and Kopp, <a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite>, while beat gestures are biphasic movements of the hand and do not carry any propositional content <cite class="ltx_cite ltx_citemacro_citep">(Bosker and Peeters, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. In our current model, none of the generated gestures introduces new information to the explainee but instead augment the information already conveyed by speech.
In the following, Sect. <a href="#S2" title="2. Related Work ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides relevant background about multimodality and gestures in human-human and human-agent interaction. Sect. <a href="#S3" title="3. Explanation Model ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the model for explanation generation, before Sect. <a href="#S4" title="4. Gesture Generation ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> turns to the approach for beat and iconic gesture generation. Finally, a user study is presented (Sect. <a href="#S5" title="5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) and its results are discussed (Sect. <a href="#S6" title="6. Conclusions ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">While the benefit of gestures in human-human interaction is well known <cite class="ltx_cite ltx_citemacro_citep">(Breckinridge Church et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2007</a>; Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>)</cite>, the results in human-agent interaction are more ambiguous <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib44" title="" class="ltx_ref">2017a</a>)</cite>. At present, little is known about the effect of the explainer’s gestures on the explainee’s understanding in human-agent explanation scenarios.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Gestures in Human-Human Explanation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">An explanation can be seen as a co-constructive process, giving both interlocutors – the explainer and the explainee – an active role in the interaction <cite class="ltx_cite ltx_citemacro_citep">(Rohlfing et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2021</a>)</cite>. As shown in <cite class="ltx_cite ltx_citemacro_citet">Lund (<a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite> gestures in explanations are used for manifold reasons, they can refer to an object, end a verbal utterance, or accompany it to stress its importance. For human-human explanation, it has been shown that it helps the listener to understand the intended meaning and structure if the speaker uses gestures in an interaction <cite class="ltx_cite ltx_citemacro_citep">(Breckinridge Church et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2007</a>; Kita and Özyürek, <a href="#bib.bib43" title="" class="ltx_ref">2003</a>)</cite>. It has also been shown that iconic gestures can support the long-term learning of second language vocabulary in children <cite class="ltx_cite ltx_citemacro_citep">(de Wit et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>; Bergmann and Macedonia, <a href="#bib.bib8" title="" class="ltx_ref">2013</a>; Belpaeme et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>; Rosenthal-von der Pütten and Bergmann, <a href="#bib.bib83" title="" class="ltx_ref">2020</a>; Rohlfing et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2006</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">It is a common approach to transfer human-human interaction patterns to human-agent interaction. For instance, there is research on how to transfer the explainer’s understanding of an explainee’s gestures from human-human to human-agent interaction <cite class="ltx_cite ltx_citemacro_citep">(Rohlfing et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2006</a>)</cite>. Here, the authors introduce a theoretical approach on how agents can interpret posture and gesture based on behavior patterns in parent-child interaction.
So far, little is known about the effects of the explainer’s gestures on the explainee’s understanding and perception of an explanation in human-agent interaction. While there are implementations of gesturing agents for spatial description tasks <cite class="ltx_cite ltx_citemacro_citep">(Bergmann and Kopp, <a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite>, math equations <cite class="ltx_cite ltx_citemacro_citep">(Perry et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">1988</a>)</cite>, Piagetian conservation tasks <cite class="ltx_cite ltx_citemacro_citep">(Ping and Goldin-Meadow, <a href="#bib.bib77" title="" class="ltx_ref">2008</a>)</cite>, or word learning in children <cite class="ltx_cite ltx_citemacro_citep">(de Wit et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, to the best of our knowledge, there are currently no artificial explainers using specific gestures as information-conveying tools in their explanation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Gestures in Virtual Agents</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The generation and integration of nonverbal behavior in virtual agent scenarios have been a long-standing problem in virtual agent research <cite class="ltx_cite ltx_citemacro_citep">(Kurokawa, <a href="#bib.bib51" title="" class="ltx_ref">1992</a>; Cassell et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">1994</a>)</cite>.
Early approaches mainly relied on rule-based models, with gesture templates created by hand, such as the Behavior Markup Language (BML) <cite class="ltx_cite ltx_citemacro_citep">(Kopp et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2006</a>; Vilhjalmsson et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2007</a>)</cite> and the Behavior Expression Animation Toolkit (BEAT) <cite class="ltx_cite ltx_citemacro_citep">(Cassell et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2004</a>)</cite>, while more recent works primarily use deep-learning, graph-based or hybrid approaches to generate gestures from given input modalities <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021</a>; Nyatsanga et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2022</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2023</a>; Voß and Kopp, <a href="#bib.bib92" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Regarding the effect of synthetic gestures on human-agent interaction, much work has been done on the perceived personality of the agent <cite class="ltx_cite ltx_citemacro_citep">(Neff et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2010</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2016</a>)</cite> or the creation of rapport with the agent <cite class="ltx_cite ltx_citemacro_citep">(Gratch et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2007</a>; Bailenson and Yee, <a href="#bib.bib4" title="" class="ltx_ref">2005</a>)</cite>. It has been shown that users are more willing to engage in a human-like way if the agent uses human-like gestures <cite class="ltx_cite ltx_citemacro_citep">(Krämer et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2007</a>; Parise et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">1999</a>)</cite>. At the same time, it has been shown that mismatching gestures have a measurable negative effect on such interactions <cite class="ltx_cite ltx_citemacro_citep">(Salem et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2012</a>; Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>; Kelly et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2004</a>)</cite>.
In general, the main effects found are social perception effects (how is the agent perceived by the user?) and communicative effects (how does gesturing influence the course of interaction?). However, little effects have been found on user understanding and task performance <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib44" title="" class="ltx_ref">2017a</a>; Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>. On the other hand, there are indications that participants benefit from gestures when it comes to learning, but there is not enough data to generalize from it <cite class="ltx_cite ltx_citemacro_citep">(Macedonia, <a href="#bib.bib59" title="" class="ltx_ref">2014</a>; Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>.
Models based on co-construction face difficulty with repetition when users don’t provide sufficient feedback, often due to underestimating the user’s competence <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Cognitive Load in Multimodal Interaction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">There are different possible ways in which gestures can support or hamper the listener’s processing and understanding of the utterance. We focus here on the cognitive load that gestures may impose on the listener, and which may particularly affect the processing of multimodal behavior of artificial agents. A lot of research has investigated the speaker’s cognitive load and how it can be decreased by using different modalities <cite class="ltx_cite ltx_citemacro_citep">(Oviatt et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2024</a>)</cite>. It has been shown that gestures help to structure an interaction and thereby minimize verbal load <cite class="ltx_cite ltx_citemacro_citep">(Goldin-Meadow et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2012</a>)</cite> show which features are relevant to measure the current cognitive load and apply their method to different experimental scenarios. <cite class="ltx_cite ltx_citemacro_citet">Oviatt et al<span class="ltx_text">.</span> (<a href="#bib.bib72" title="" class="ltx_ref">2024</a>)</cite> show that as cognitive load rises people tend to go multimodal, presumably to distribute the load over the used modalities. Only a few studies examined the impact of generated input on the listener. The question of how gestures affect the listener’s cognitive load remains unanswered by them <cite class="ltx_cite ltx_citemacro_citep">(Krieglstein et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Gestures in interaction do influence both, the speaker and listener. Most research focuses on the positive effects gestures have on the person using them.
<cite class="ltx_cite ltx_citemacro_citet">Hostetter and Bahl (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> demonstrate that gesturing and other meaningful hand movements have a beneficial influence on verbal load.
Similarly, research on memory calls showed that prohibiting the use of gestures diminishes the recall rate of memorized words <cite class="ltx_cite ltx_citemacro_citep">(Matthews-Saugstad et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2017</a>)</cite>. It has been shown that gestures also support the human explainer in structuring their explanation <cite class="ltx_cite ltx_citemacro_citep">(Goldin-Meadow et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite>. When a non-existent object is described with gestures instead of speech, research has shown that the cognitive load reduces <cite class="ltx_cite ltx_citemacro_citep">(Ping and Goldin-Meadow, <a href="#bib.bib76" title="" class="ltx_ref">2010</a>)</cite>. There is less research on how gestures are perceived, but studies show that presenting information on different modalities can expand the learner’s capacity of working memory in learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Mayer and Moreno, <a href="#bib.bib65" title="" class="ltx_ref">1998</a>; Tindall-Ford et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">1997</a>)</cite>. <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Cognitive load theory</span> states that presenting a task using different modalities (e.g. dual-mode presentation for solving geometry tasks) supports the expansion of working memory and helps to solve tasks <cite class="ltx_cite ltx_citemacro_citep">(Mousavi et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">1995</a>)</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In contrast to this, <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">cognitive resource theory</span> shows that there can be a competition between modalities when performing a task, as they need to be processed in parallel <cite class="ltx_cite ltx_citemacro_citep">(Wickens et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">1983</a>)</cite>. This approach also considers the cognitive load it takes to transfer input on one modality to a task that needs another modality.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Explanation Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The quality of an explanation is influenced by many aspects, such as adaptivity, multimodality, and information quality. As this paper describes the influence of gestures on understanding and perception of the explanation, all other aspects are kept as static as possible. We adopt a model for explanation generation, called SNAPE <cite class="ltx_cite ltx_citemacro_citep">(Robrecht and Kopp, <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>. This model is capable of generating adaptive explanations, which have been shown to result in better understanding, especially better deep understanding, than a static explanation <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>. This section will give a short overview of the model’s approach and architecture.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">SNAPE is based on a non-stationary Markov Decision Process (MDP) which evaluates the best action (which information to provide) and move (how to verbalize the information) dependent on the current internal model the agent has about the user. This model is called the Partner Model (PM) and consists of (1) an estimation of the user’s current domain knowledge and (2) different global variables, such as expertise and attentiveness, which are based on the amount and quality of feedback that has been generated by the user so far. As transition probabilities and rewards are based on the PM, the MDP needs to be solved online using Monte Carlo Tree Search (MCTS). To keep the process real-time capable, the state space is kept small, which is grounded in the hierarchical structure of the model. A knowledge graph (KG) containing all necessary information for the explanation is extracted from an ontology, containing all possible information about the domain. Similar to the process observable in human-human explanations <cite class="ltx_cite ltx_citemacro_citep">(Fisher et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, the KG is subdivided into semantic blocks. These blocks form the set of information that the MDP can use for inference. We extended the model by updating the template generation: the templates used for the verbalization of the explanation are now generated by a Large Language Model (LLM). This extension will be discussed in the following section<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For further insights into the architecture, please refer to <cite class="ltx_cite ltx_citemacro_citep">(Robrecht and Kopp, <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>, more information about complexity and preconditions are given in <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite></span></span></span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Using LLMs for utterance generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A key lesson learned from the previous model’s performance, was that multiple pieces of information should be mergeable into one utterance if the current PM and the complexity of the information allow it.
Hence, we have extended the SNAPE model to combine multiple triples into one utterance under certain conditions: (1) the triples have to be in the set of the five best next pieces of information to provide that is generated by MCTS, (2) they have to have the same linguistic move (provide new information, give additional information, repeat information, make a comparison), and (3) they have to share at least one entity. The vast number and complexity of potential triple combinations require a general and powerful approach to utterance generation. In the current version, all possible triple combinations are generated and matching utterances are created by prompting a fine-tuned Llama2 7B model <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2023</a>)</cite>. The model is fine-tuned on a dataset containing 487 items, each consisting of a list of triples, the move, and a matching output utterance. The fine-tuned model was used to generate multiple alternatives for each possible combination before running the explanation. The pre-generation allows to prevent hallucinations and minimizes the required computing power to keep the model real-time capable.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Examples of Llama2 generated templates</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Move</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Triple</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Template</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T1.1.2.1.1.1" class="ltx_text">Provide</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p">(Struktur, sein, Figurenmerkmal), (Groesse, sein, Figurenmerkmal)</span>
</span>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.3.1.1" class="ltx_p">Die Größe und Struktur sind Merkmale der Figuren.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p">(structure, is, figure-feature), (size, is, figure-feature)</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p"><span id="S3.T1.1.3.2.2.1.1.1" class="ltx_text ltx_font_italic">Size and structure are features of the figure.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S3.T1.1.4.3.1.1" class="ltx_text">Repeat</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p">(Struktur, sein, Figurenmerkmal), (Groesse, sein, Figurenmerkmal)</span>
</span>
</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.3.1.1" class="ltx_p">Die Größe und die Struktur gehören zu den Figurenmerkmalen.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p">(structure, is, figure-feature), (size, is, figure-feature)</span>
</span>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.2.1.1" class="ltx_p"><span id="S3.T1.1.5.4.2.1.1.1" class="ltx_text ltx_font_italic">The size and structure are among the figure features.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S3.T1.1.6.5.1.1" class="ltx_text">Additional</span></th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.2.1.1" class="ltx_p">(Quarto, haben, Spielfiguren), (Spielfiguren, material, Holz)</span>
</span>
</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.3.1.1" class="ltx_p">Die Spielfiguren bei Quarto sind aus Holz.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p">(quarto, has, figures), (figures, material, wood)</span>
</span>
</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.2.1.1" class="ltx_p"><span id="S3.T1.1.7.6.2.1.1.1" class="ltx_text ltx_font_italic">The figures are made of wood.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" rowspan="2"><span id="S3.T1.1.8.7.1.1" class="ltx_text">Compare</span></th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.2.1.1" class="ltx_p">(Spiel, haben, Ziel), (Reihe, sein, Ziel)</span>
</span>
</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.3.1.1" class="ltx_p">Das Bilden einer Reihe ist das Ziel von Quarto, genau wie bei Vier Gewinnt.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.1.1.1" class="ltx_p">(game, has, goal), (row, is, goal)</span>
</span>
</td>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.2.1.1" class="ltx_p"><span id="S3.T1.1.9.8.2.1.1.1" class="ltx_text ltx_font_italic">Forming a line is the aim of Quarto, just like in Best of Four.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">As shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.1. Using LLMs for utterance generation ‣ 3. Explanation Model ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the prompts for template generation not only contain the information in the form of one or two triplets but also the linguistic move that the system is supposed to use to verbalize the information. If SNAPE introduces new information, the move is called <span id="S3.SS1.p2.4.1" class="ltx_text ltx_font_bold">provide</span> information. For this move, information can only be taken from the knowledge graph. Providing information can either work or fail, which depends on the transition probability <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">T</annotation></semantics></math> in the MDP. The transition probability is influenced by the currently inferred level of attentiveness the user has, as a user who gets distracted easily has a higher probability of missing information. If the move succeeds, the level of understanding <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="lou_{i}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1a" xref="S3.SS1.p2.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS1.p2.2.m2.1.1.4" xref="S3.SS1.p2.2.m2.1.1.4.cmml"><mi id="S3.SS1.p2.2.m2.1.1.4.2" xref="S3.SS1.p2.2.m2.1.1.4.2.cmml">u</mi><mi id="S3.SS1.p2.2.m2.1.1.4.3" xref="S3.SS1.p2.2.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑙</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑜</ci><apply id="S3.SS1.p2.2.m2.1.1.4.cmml" xref="S3.SS1.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.4.1.cmml" xref="S3.SS1.p2.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.4.2.cmml" xref="S3.SS1.p2.2.m2.1.1.4.2">𝑢</ci><ci id="S3.SS1.p2.2.m2.1.1.4.3.cmml" xref="S3.SS1.p2.2.m2.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">lou_{i}</annotation></semantics></math> of this information increases. The strength of growth depends on the currently inferred level of expertise the user has. A <span id="S3.SS1.p2.4.2" class="ltx_text ltx_font_bold">repetition</span> can be verbatim or reformulated, as long as no new information is given <cite class="ltx_cite ltx_citemacro_citep">(Johnstone, <a href="#bib.bib38" title="" class="ltx_ref">1994</a>)</cite>. Only necessary information can be repeated, accordingly the triple that is repeated has to be taken from the knowledge graph. Repetition is the simplest of the three available deepening moves. It has the highest probability of succeeding, but also the lowest increase in the level of understanding <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="lou_{i}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.1a" xref="S3.SS1.p2.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS1.p2.3.m3.1.1.4" xref="S3.SS1.p2.3.m3.1.1.4.cmml"><mi id="S3.SS1.p2.3.m3.1.1.4.2" xref="S3.SS1.p2.3.m3.1.1.4.2.cmml">u</mi><mi id="S3.SS1.p2.3.m3.1.1.4.3" xref="S3.SS1.p2.3.m3.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></times><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑙</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑜</ci><apply id="S3.SS1.p2.3.m3.1.1.4.cmml" xref="S3.SS1.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.4.1.cmml" xref="S3.SS1.p2.3.m3.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.4.2.cmml" xref="S3.SS1.p2.3.m3.1.1.4.2">𝑢</ci><ci id="S3.SS1.p2.3.m3.1.1.4.3.cmml" xref="S3.SS1.p2.3.m3.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">lou_{i}</annotation></semantics></math>. The move <span id="S3.SS1.p2.4.3" class="ltx_text ltx_font_bold">additional</span> adds information that is not necessary but potentially helpful to an already introduced, but not yet grounded, information. When considering giving additional information to the currently under discussion information <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="cud_{i}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1a" xref="S3.SS1.p2.4.m4.1.1.1.cmml">​</mo><msub id="S3.SS1.p2.4.m4.1.1.4" xref="S3.SS1.p2.4.m4.1.1.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.4.2" xref="S3.SS1.p2.4.m4.1.1.4.2.cmml">d</mi><mi id="S3.SS1.p2.4.m4.1.1.4.3" xref="S3.SS1.p2.4.m4.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑐</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑢</ci><apply id="S3.SS1.p2.4.m4.1.1.4.cmml" xref="S3.SS1.p2.4.m4.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.4.2">𝑑</ci><ci id="S3.SS1.p2.4.m4.1.1.4.3.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">cud_{i}</annotation></semantics></math>, the model first needs to check for a fitting triple. A fitting triple is a triple that has to be part of the ontology but does not contain necessary information. Additionally, the triple needs to have the subject or object of the original triple as the subject. If a potential triple does exist, the move is an available action for the next step in the explanation process. A <span id="S3.SS1.p2.4.4" class="ltx_text ltx_font_bold">comparison</span> gives supportive information to an already introduced triple. In this case, the triple is not taken from the Quarto! ontology, but from an ontology of another, comparable board game. Again, the triples have to share at least one entity, but can also be identical. Examples of each move can be seen in Tab.<a href="#S3.T1" title="Table 1 ‣ 3.1. Using LLMs for utterance generation ‣ 3. Explanation Model ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Gesture Generation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In order to augment the explanations produced by the model described above with communicative gestures, we identified three main requirements for gestures to be used in an explanatory setting:
(1) The gestures had to be as human-like as possible and not distract from the given speech. (2) The generated gestures had to be extensible to incorporate additional iconic gestures in a natural and easily modifiable way.
(3) The gesture algorithm should be able to generate gestures in or near real-time, in a format that does not require extensive pre- or post-processing.
To meet these requirements, we examined several different approaches to gesture generation, including but not limited to the models competing in the 2022 and 2023 GENEA challenges <cite class="ltx_cite ltx_citemacro_citep">(Yoon et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>; Kucherenko et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>.
Since all diffusion-based implementations did not meet the real-time requirement and the GAN approaches did not produce satisfactory results in human evaluations, we focused on graph-based implementations. Inspired by the work of both <cite class="ltx_cite ltx_citemacro_citet">Zhou et al<span class="ltx_text">.</span> (<a href="#bib.bib102" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhao et al<span class="ltx_text">.</span> (<a href="#bib.bib101" title="" class="ltx_ref">2023</a>)</cite>, we developed a new graph-based gesture generation algorithm that enables the generation of realistic gestures in a real-time context.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To create our new graph-based gesture generation algorithm, we first needed to get appropriate data that either already had iconic gestures or was usable in a base model on which iconic gestures could be added. Currently, iconic gesture data sets are almost non-existent. To our knowledge, only two annotated data sets for iconic gestures exist. The SaGA data corpus <cite class="ltx_cite ltx_citemacro_citep">(Lücking et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2010</a>)</cite> with a small set of highly specific annotated data and the BEAT corpus <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> with acted interactions and rough categories for annotations. As we are mainly interested in natural interactions, we opted against the use of either of these corpora and instead captured our own data corpus. For this, we took 32 hours of TED and TEDx recordings with their subtitles <cite class="ltx_cite ltx_citemacro_citep">(foundation, <a href="#bib.bib28" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib29" title="" class="ltx_ref">b</a>)</cite> and tracked the body pose data for all recordings using One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer (OSX) <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. Afterward, we split the videos into individual video clips by detecting and cutting the videos along camera cuts using PySceneDetect <cite class="ltx_cite ltx_citemacro_citep">(Castellano, <a href="#bib.bib16" title="" class="ltx_ref">2024</a>)</cite>. Every clip was removed that did not include the main speaker, exhibited a low confidence rating during tracking, or had no perceivable movement. The final dataset consists of 24.2 hours of primarily beat gesture, text, and audio data.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Gesture Segmentation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.14" class="ltx_p">In contrast to deep learning-based approaches, which generate new gesture data from a trained multimodal data set, graph-based approaches are more closely related to retrieval-based techniques, where the input data is not used as training data, but as lightly processed chunking data in which the algorithm searches for an optimal path to generate new gestures <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2023</a>; Nyatsanga et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Using audio, text, and gesture data as input, we first divided the entire training data into clips of length <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">s</annotation></semantics></math> seconds, with an overlap of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mfrac id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">s</mi><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><divide id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"></divide><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑠</ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\frac{s}{2}</annotation></semantics></math> seconds. For our data, we chose a length of 2 for <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">s</annotation></semantics></math>. All clips shorter than <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">s</annotation></semantics></math> seconds were discarded. We then performed individual data processing for each modality. For the text data, we encoded each word with fastText <cite class="ltx_cite ltx_citemacro_citep">(Joulin et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2016</a>; Bojanowski et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> and performed vector encoding for both the short historical context with <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="tx_{1}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">​</mo><msub id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.2" xref="S4.SS2.p1.5.m5.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p1.5.m5.1.1.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><times id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></times><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝑡</ci><apply id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.2">𝑥</ci><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">tx_{1}</annotation></semantics></math> words and the long historical context with <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="tx_{2}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">​</mo><msub id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p1.6.m6.1.1.3.3" xref="S4.SS2.p1.6.m6.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><times id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></times><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">𝑡</ci><apply id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2">𝑥</ci><cn type="integer" id="S4.SS2.p1.6.m6.1.1.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">tx_{2}</annotation></semantics></math> words. We encoded both sequences using a MiniLM model <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2020</a>)</cite> and an openclip embedding model <cite class="ltx_cite ltx_citemacro_citep">(Cherti et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2022</a>; Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2021</a>)</cite> trained on the DataComp1B dataset <cite class="ltx_cite ltx_citemacro_citep">(Gadre et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. As shown in equation <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mn id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><cn type="integer" id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">1</annotation></semantics></math> and <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mn id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><cn type="integer" id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">2</annotation></semantics></math>, given the MiniLM model as <math id="S4.SS2.p1.9.m9.1" class="ltx_Math" alttext="MiLM" display="inline"><semantics id="S4.SS2.p1.9.m9.1a"><mrow id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mi id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1a" xref="S4.SS2.p1.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.9.m9.1.1.4" xref="S4.SS2.p1.9.m9.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1b" xref="S4.SS2.p1.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.9.m9.1.1.5" xref="S4.SS2.p1.9.m9.1.1.5.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><times id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1"></times><ci id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">𝑀</ci><ci id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3">𝑖</ci><ci id="S4.SS2.p1.9.m9.1.1.4.cmml" xref="S4.SS2.p1.9.m9.1.1.4">𝐿</ci><ci id="S4.SS2.p1.9.m9.1.1.5.cmml" xref="S4.SS2.p1.9.m9.1.1.5">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">MiLM</annotation></semantics></math>, the OpenClip model as <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="Oclip" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><mrow id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml"><mi id="S4.SS2.p1.10.m10.1.1.2" xref="S4.SS2.p1.10.m10.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1" xref="S4.SS2.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3" xref="S4.SS2.p1.10.m10.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1a" xref="S4.SS2.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.4" xref="S4.SS2.p1.10.m10.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1b" xref="S4.SS2.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.5" xref="S4.SS2.p1.10.m10.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1c" xref="S4.SS2.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.6" xref="S4.SS2.p1.10.m10.1.1.6.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1"><times id="S4.SS2.p1.10.m10.1.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1.1"></times><ci id="S4.SS2.p1.10.m10.1.1.2.cmml" xref="S4.SS2.p1.10.m10.1.1.2">𝑂</ci><ci id="S4.SS2.p1.10.m10.1.1.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3">𝑐</ci><ci id="S4.SS2.p1.10.m10.1.1.4.cmml" xref="S4.SS2.p1.10.m10.1.1.4">𝑙</ci><ci id="S4.SS2.p1.10.m10.1.1.5.cmml" xref="S4.SS2.p1.10.m10.1.1.5">𝑖</ci><ci id="S4.SS2.p1.10.m10.1.1.6.cmml" xref="S4.SS2.p1.10.m10.1.1.6">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">Oclip</annotation></semantics></math>, the combination of the short and long historical sequence as <math id="S4.SS2.p1.11.m11.1" class="ltx_Math" alttext="vr" display="inline"><semantics id="S4.SS2.p1.11.m11.1a"><mrow id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml"><mi id="S4.SS2.p1.11.m11.1.1.2" xref="S4.SS2.p1.11.m11.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.11.m11.1.1.1" xref="S4.SS2.p1.11.m11.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.11.m11.1.1.3" xref="S4.SS2.p1.11.m11.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><apply id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1"><times id="S4.SS2.p1.11.m11.1.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1.1"></times><ci id="S4.SS2.p1.11.m11.1.1.2.cmml" xref="S4.SS2.p1.11.m11.1.1.2">𝑣</ci><ci id="S4.SS2.p1.11.m11.1.1.3.cmml" xref="S4.SS2.p1.11.m11.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">vr</annotation></semantics></math>, the combined sequence as <math id="S4.SS2.p1.12.m12.1" class="ltx_Math" alttext="vt" display="inline"><semantics id="S4.SS2.p1.12.m12.1a"><mrow id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml"><mi id="S4.SS2.p1.12.m12.1.1.2" xref="S4.SS2.p1.12.m12.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.12.m12.1.1.1" xref="S4.SS2.p1.12.m12.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.12.m12.1.1.3" xref="S4.SS2.p1.12.m12.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><apply id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1"><times id="S4.SS2.p1.12.m12.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1"></times><ci id="S4.SS2.p1.12.m12.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1.2">𝑣</ci><ci id="S4.SS2.p1.12.m12.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">vt</annotation></semantics></math>, and the word at step <math id="S4.SS2.p1.13.m13.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p1.13.m13.1a"><mi id="S4.SS2.p1.13.m13.1.1" xref="S4.SS2.p1.13.m13.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m13.1b"><ci id="S4.SS2.p1.13.m13.1.1.cmml" xref="S4.SS2.p1.13.m13.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m13.1c">i</annotation></semantics></math> as <math id="S4.SS2.p1.14.m14.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.SS2.p1.14.m14.1a"><msub id="S4.SS2.p1.14.m14.1.1" xref="S4.SS2.p1.14.m14.1.1.cmml"><mi id="S4.SS2.p1.14.m14.1.1.2" xref="S4.SS2.p1.14.m14.1.1.2.cmml">w</mi><mi id="S4.SS2.p1.14.m14.1.1.3" xref="S4.SS2.p1.14.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.14.m14.1b"><apply id="S4.SS2.p1.14.m14.1.1.cmml" xref="S4.SS2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.14.m14.1.1.1.cmml" xref="S4.SS2.p1.14.m14.1.1">subscript</csymbol><ci id="S4.SS2.p1.14.m14.1.1.2.cmml" xref="S4.SS2.p1.14.m14.1.1.2">𝑤</ci><ci id="S4.SS2.p1.14.m14.1.1.3.cmml" xref="S4.SS2.p1.14.m14.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.14.m14.1c">w_{i}</annotation></semantics></math>, we formally perform</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\displaystyle vr_{i}" display="inline"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mi id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">r</mi><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><times id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></times><ci id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2">𝑣</ci><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝑟</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle vr_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E1.m2.2" class="ltx_Math" alttext="\displaystyle=\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{1}}w_{i}\oplus\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{2}}w_{i}" display="inline"><semantics id="S4.E1.m2.2a"><mrow id="S4.E1.m2.2.3" xref="S4.E1.m2.2.3.cmml"><mi id="S4.E1.m2.2.3.2" xref="S4.E1.m2.2.3.2.cmml"></mi><mo id="S4.E1.m2.2.3.1" xref="S4.E1.m2.2.3.1.cmml">=</mo><mrow id="S4.E1.m2.2.3.3" xref="S4.E1.m2.2.3.3.cmml"><mrow id="S4.E1.m2.2.3.3.2" xref="S4.E1.m2.2.3.3.2.cmml"><mpadded depth="3.3pt" height="10.0pt" voffset="0.0pt" width="0.0pt" id="S4.E1.m2.1.1a" xref="S4.E1.m2.1.1a.cmml"><mo movablelimits="false" id="S4.E1.m2.1.1aa" xref="S4.E1.m2.1.1a.cmml">∥</mo></mpadded><mrow id="S4.E1.m2.2.3.3.2.1" xref="S4.E1.m2.2.3.3.2.1.cmml"><msubsup id="S4.E1.m2.2.3.3.2.1.2" xref="S4.E1.m2.2.3.3.2.1.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.2.1.2.2.2" xref="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml"><mtext id="S4.E1.m2.2.3.3.2.1.2.2.2a" xref="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml">\slimits@</mtext></merror><mrow id="S4.E1.m2.2.3.3.2.1.2.2.3" xref="S4.E1.m2.2.3.3.2.1.2.2.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.2.3.2" xref="S4.E1.m2.2.3.3.2.1.2.2.3.2.cmml">k</mi><mo id="S4.E1.m2.2.3.3.2.1.2.2.3.1" xref="S4.E1.m2.2.3.3.2.1.2.2.3.1.cmml">=</mo><mi id="S4.E1.m2.2.3.3.2.1.2.2.3.3" xref="S4.E1.m2.2.3.3.2.1.2.2.3.3.cmml">i</mi></mrow><mrow id="S4.E1.m2.2.3.3.2.1.2.3" xref="S4.E1.m2.2.3.3.2.1.2.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.2.cmml">i</mi><mo id="S4.E1.m2.2.3.3.2.1.2.3.1" xref="S4.E1.m2.2.3.3.2.1.2.3.1.cmml">−</mo><mrow id="S4.E1.m2.2.3.3.2.1.2.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.2.1.2.3.3.1" xref="S4.E1.m2.2.3.3.2.1.2.3.3.1.cmml">​</mo><msub id="S4.E1.m2.2.3.3.2.1.2.3.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.3.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.2.cmml">x</mi><mn id="S4.E1.m2.2.3.3.2.1.2.3.3.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.3.cmml">1</mn></msub></mrow></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.2.1.1" xref="S4.E1.m2.2.3.3.2.1.1.cmml">​</mo><msub id="S4.E1.m2.2.3.3.2.1.3" xref="S4.E1.m2.2.3.3.2.1.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.3.2" xref="S4.E1.m2.2.3.3.2.1.3.2.cmml">w</mi><mi id="S4.E1.m2.2.3.3.2.1.3.3" xref="S4.E1.m2.2.3.3.2.1.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S4.E1.m2.2.3.3.1" xref="S4.E1.m2.2.3.3.1.cmml">⊕</mo><mrow id="S4.E1.m2.2.3.3.3" xref="S4.E1.m2.2.3.3.3.cmml"><mpadded depth="3.3pt" height="10.0pt" voffset="0.0pt" width="0.0pt" id="S4.E1.m2.2.2a" xref="S4.E1.m2.2.2a.cmml"><mo movablelimits="false" id="S4.E1.m2.2.2aa" xref="S4.E1.m2.2.2a.cmml">∥</mo></mpadded><mrow id="S4.E1.m2.2.3.3.3.1" xref="S4.E1.m2.2.3.3.3.1.cmml"><msubsup id="S4.E1.m2.2.3.3.3.1.2" xref="S4.E1.m2.2.3.3.3.1.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.3.1.2.2.2" xref="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml"><mtext id="S4.E1.m2.2.3.3.3.1.2.2.2a" xref="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml">\slimits@</mtext></merror><mrow id="S4.E1.m2.2.3.3.3.1.2.2.3" xref="S4.E1.m2.2.3.3.3.1.2.2.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.2.3.2" xref="S4.E1.m2.2.3.3.3.1.2.2.3.2.cmml">k</mi><mo id="S4.E1.m2.2.3.3.3.1.2.2.3.1" xref="S4.E1.m2.2.3.3.3.1.2.2.3.1.cmml">=</mo><mi id="S4.E1.m2.2.3.3.3.1.2.2.3.3" xref="S4.E1.m2.2.3.3.3.1.2.2.3.3.cmml">i</mi></mrow><mrow id="S4.E1.m2.2.3.3.3.1.2.3" xref="S4.E1.m2.2.3.3.3.1.2.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E1.m2.2.3.3.3.1.2.3.1" xref="S4.E1.m2.2.3.3.3.1.2.3.1.cmml">−</mo><mrow id="S4.E1.m2.2.3.3.3.1.2.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.3.1.2.3.3.1" xref="S4.E1.m2.2.3.3.3.1.2.3.3.1.cmml">​</mo><msub id="S4.E1.m2.2.3.3.3.1.2.3.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.3.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.2.cmml">x</mi><mn id="S4.E1.m2.2.3.3.3.1.2.3.3.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.3.cmml">2</mn></msub></mrow></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.3.1.1" xref="S4.E1.m2.2.3.3.3.1.1.cmml">​</mo><msub id="S4.E1.m2.2.3.3.3.1.3" xref="S4.E1.m2.2.3.3.3.1.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.3.2" xref="S4.E1.m2.2.3.3.3.1.3.2.cmml">w</mi><mi id="S4.E1.m2.2.3.3.3.1.3.3" xref="S4.E1.m2.2.3.3.3.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m2.2b"><apply id="S4.E1.m2.2.3.cmml" xref="S4.E1.m2.2.3"><eq id="S4.E1.m2.2.3.1.cmml" xref="S4.E1.m2.2.3.1"></eq><csymbol cd="latexml" id="S4.E1.m2.2.3.2.cmml" xref="S4.E1.m2.2.3.2">absent</csymbol><apply id="S4.E1.m2.2.3.3.cmml" xref="S4.E1.m2.2.3.3"><csymbol cd="latexml" id="S4.E1.m2.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.1">direct-sum</csymbol><apply id="S4.E1.m2.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2"><ci id="S4.E1.m2.1.1a.cmml" xref="S4.E1.m2.1.1a">∥</ci><apply id="S4.E1.m2.2.3.3.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1"><times id="S4.E1.m2.2.3.3.2.1.1.cmml" xref="S4.E1.m2.2.3.3.2.1.1"></times><apply id="S4.E1.m2.2.3.3.2.1.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2">superscript</csymbol><apply id="S4.E1.m2.2.3.3.2.1.2.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.2.1.2.2.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2"><mtext id="S4.E1.m2.2.3.3.2.1.2.2.2a.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2">\slimits@</mtext></merror></ci><apply id="S4.E1.m2.2.3.3.2.1.2.2.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3"><eq id="S4.E1.m2.2.3.3.2.1.2.2.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.1"></eq><ci id="S4.E1.m2.2.3.3.2.1.2.2.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.2">𝑘</ci><ci id="S4.E1.m2.2.3.3.2.1.2.2.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.3">𝑖</ci></apply></apply><apply id="S4.E1.m2.2.3.3.2.1.2.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3"><minus id="S4.E1.m2.2.3.3.2.1.2.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.1"></minus><ci id="S4.E1.m2.2.3.3.2.1.2.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.2">𝑖</ci><apply id="S4.E1.m2.2.3.3.2.1.2.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3"><times id="S4.E1.m2.2.3.3.2.1.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.1"></times><ci id="S4.E1.m2.2.3.3.2.1.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.2">𝑡</ci><apply id="S4.E1.m2.2.3.3.2.1.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.2.3.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.2">𝑥</ci><cn type="integer" id="S4.E1.m2.2.3.3.2.1.2.3.3.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.3">1</cn></apply></apply></apply></apply><apply id="S4.E1.m2.2.3.3.2.1.3.cmml" xref="S4.E1.m2.2.3.3.2.1.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.3.2">𝑤</ci><ci id="S4.E1.m2.2.3.3.2.1.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E1.m2.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3"><ci id="S4.E1.m2.2.2a.cmml" xref="S4.E1.m2.2.2a">∥</ci><apply id="S4.E1.m2.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1"><times id="S4.E1.m2.2.3.3.3.1.1.cmml" xref="S4.E1.m2.2.3.3.3.1.1"></times><apply id="S4.E1.m2.2.3.3.3.1.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2">superscript</csymbol><apply id="S4.E1.m2.2.3.3.3.1.2.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.2.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.3.1.2.2.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2"><mtext id="S4.E1.m2.2.3.3.3.1.2.2.2a.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2">\slimits@</mtext></merror></ci><apply id="S4.E1.m2.2.3.3.3.1.2.2.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3"><eq id="S4.E1.m2.2.3.3.3.1.2.2.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.1"></eq><ci id="S4.E1.m2.2.3.3.3.1.2.2.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.2">𝑘</ci><ci id="S4.E1.m2.2.3.3.3.1.2.2.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.3">𝑖</ci></apply></apply><apply id="S4.E1.m2.2.3.3.3.1.2.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3"><minus id="S4.E1.m2.2.3.3.3.1.2.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.1"></minus><ci id="S4.E1.m2.2.3.3.3.1.2.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.2">𝑖</ci><apply id="S4.E1.m2.2.3.3.3.1.2.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3"><times id="S4.E1.m2.2.3.3.3.1.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.1"></times><ci id="S4.E1.m2.2.3.3.3.1.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.2">𝑡</ci><apply id="S4.E1.m2.2.3.3.3.1.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.2.3.3.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.2">𝑥</ci><cn type="integer" id="S4.E1.m2.2.3.3.3.1.2.3.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.3">2</cn></apply></apply></apply></apply><apply id="S4.E1.m2.2.3.3.3.1.3.cmml" xref="S4.E1.m2.2.3.3.3.1.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.3.2">𝑤</ci><ci id="S4.E1.m2.2.3.3.3.1.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m2.2c">\displaystyle=\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{1}}w_{i}\oplus\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{2}}w_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle vt_{i}" display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">​</mo><msub id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">t</mi><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><times id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></times><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">𝑣</ci><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">𝑡</ci><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle vt_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.2" class="ltx_Math" alttext="\displaystyle=MiLM(vr_{i})\oplus Oclip(vr_{i})" display="inline"><semantics id="S4.E2.m2.2a"><mrow id="S4.E2.m2.2.2" xref="S4.E2.m2.2.2.cmml"><mi id="S4.E2.m2.2.2.4" xref="S4.E2.m2.2.2.4.cmml"></mi><mo id="S4.E2.m2.2.2.3" xref="S4.E2.m2.2.2.3.cmml">=</mo><mrow id="S4.E2.m2.2.2.2" xref="S4.E2.m2.2.2.2.cmml"><mrow id="S4.E2.m2.1.1.1.1" xref="S4.E2.m2.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.2.cmml">​</mo><mi id="S4.E2.m2.1.1.1.1.4" xref="S4.E2.m2.1.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2a" xref="S4.E2.m2.1.1.1.1.2.cmml">​</mo><mi id="S4.E2.m2.1.1.1.1.5" xref="S4.E2.m2.1.1.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2b" xref="S4.E2.m2.1.1.1.1.2.cmml">​</mo><mi id="S4.E2.m2.1.1.1.1.6" xref="S4.E2.m2.1.1.1.1.6.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2c" xref="S4.E2.m2.1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m2.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.E2.m2.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.1.1.1.3.2.cmml">r</mi><mi id="S4.E2.m2.1.1.1.1.1.1.1.3.3" xref="S4.E2.m2.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m2.2.2.2.3" xref="S4.E2.m2.2.2.2.3.cmml">⊕</mo><mrow id="S4.E2.m2.2.2.2.2" xref="S4.E2.m2.2.2.2.2.cmml"><mi id="S4.E2.m2.2.2.2.2.3" xref="S4.E2.m2.2.2.2.2.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2" xref="S4.E2.m2.2.2.2.2.2.cmml">​</mo><mi id="S4.E2.m2.2.2.2.2.4" xref="S4.E2.m2.2.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2a" xref="S4.E2.m2.2.2.2.2.2.cmml">​</mo><mi id="S4.E2.m2.2.2.2.2.5" xref="S4.E2.m2.2.2.2.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2b" xref="S4.E2.m2.2.2.2.2.2.cmml">​</mo><mi id="S4.E2.m2.2.2.2.2.6" xref="S4.E2.m2.2.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2c" xref="S4.E2.m2.2.2.2.2.2.cmml">​</mo><mi id="S4.E2.m2.2.2.2.2.7" xref="S4.E2.m2.2.2.2.2.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2d" xref="S4.E2.m2.2.2.2.2.2.cmml">​</mo><mrow id="S4.E2.m2.2.2.2.2.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.2.2.2.2.1.1.2" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.2.2.2.2.1.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml"><mi id="S4.E2.m2.2.2.2.2.1.1.1.2" xref="S4.E2.m2.2.2.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.1.1.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.1.cmml">​</mo><msub id="S4.E2.m2.2.2.2.2.1.1.1.3" xref="S4.E2.m2.2.2.2.2.1.1.1.3.cmml"><mi id="S4.E2.m2.2.2.2.2.1.1.1.3.2" xref="S4.E2.m2.2.2.2.2.1.1.1.3.2.cmml">r</mi><mi id="S4.E2.m2.2.2.2.2.1.1.1.3.3" xref="S4.E2.m2.2.2.2.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E2.m2.2.2.2.2.1.1.3" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.2b"><apply id="S4.E2.m2.2.2.cmml" xref="S4.E2.m2.2.2"><eq id="S4.E2.m2.2.2.3.cmml" xref="S4.E2.m2.2.2.3"></eq><csymbol cd="latexml" id="S4.E2.m2.2.2.4.cmml" xref="S4.E2.m2.2.2.4">absent</csymbol><apply id="S4.E2.m2.2.2.2.cmml" xref="S4.E2.m2.2.2.2"><csymbol cd="latexml" id="S4.E2.m2.2.2.2.3.cmml" xref="S4.E2.m2.2.2.2.3">direct-sum</csymbol><apply id="S4.E2.m2.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.2"></times><ci id="S4.E2.m2.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.3">𝑀</ci><ci id="S4.E2.m2.1.1.1.1.4.cmml" xref="S4.E2.m2.1.1.1.1.4">𝑖</ci><ci id="S4.E2.m2.1.1.1.1.5.cmml" xref="S4.E2.m2.1.1.1.1.5">𝐿</ci><ci id="S4.E2.m2.1.1.1.1.6.cmml" xref="S4.E2.m2.1.1.1.1.6">𝑀</ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1"></times><ci id="S4.E2.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2">𝑣</ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3.2">𝑟</ci><ci id="S4.E2.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E2.m2.2.2.2.2.cmml" xref="S4.E2.m2.2.2.2.2"><times id="S4.E2.m2.2.2.2.2.2.cmml" xref="S4.E2.m2.2.2.2.2.2"></times><ci id="S4.E2.m2.2.2.2.2.3.cmml" xref="S4.E2.m2.2.2.2.2.3">𝑂</ci><ci id="S4.E2.m2.2.2.2.2.4.cmml" xref="S4.E2.m2.2.2.2.2.4">𝑐</ci><ci id="S4.E2.m2.2.2.2.2.5.cmml" xref="S4.E2.m2.2.2.2.2.5">𝑙</ci><ci id="S4.E2.m2.2.2.2.2.6.cmml" xref="S4.E2.m2.2.2.2.2.6">𝑖</ci><ci id="S4.E2.m2.2.2.2.2.7.cmml" xref="S4.E2.m2.2.2.2.2.7">𝑝</ci><apply id="S4.E2.m2.2.2.2.2.1.1.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1"><times id="S4.E2.m2.2.2.2.2.1.1.1.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.1"></times><ci id="S4.E2.m2.2.2.2.2.1.1.1.2.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.2">𝑣</ci><apply id="S4.E2.m2.2.2.2.2.1.1.1.3.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E2.m2.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3.2">𝑟</ci><ci id="S4.E2.m2.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.2c">\displaystyle=MiLM(vr_{i})\oplus Oclip(vr_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.13" class="ltx_p">which concatenates the MiniLM an OpenClip vectors with the historic context <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="tx_{1}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><msub id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml"><mi id="S4.SS2.p2.1.m1.1.1.3.2" xref="S4.SS2.p2.1.m1.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p2.1.m1.1.1.3.3" xref="S4.SS2.p2.1.m1.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑡</ci><apply id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.p2.1.m1.1.1.3.2">𝑥</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">tx_{1}</annotation></semantics></math> and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="tx_{2}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑡</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">𝑥</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">tx_{2}</annotation></semantics></math>.
For the audio processing, we resample the audio data to 24.000 Hz, and both compute a log normalized spectrogram with consecutive Fourier transformations on the raw audio data, as well as a vector embedding for the entire sequence <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="va" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑣</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">va</annotation></semantics></math> using the wav2vec 2.0 model <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>. To more closely align the audio with the text data and to remove any superfluous information from the audio data, we additionally compute a processed version of the spectrogram and the wav2vec 2.0 vectors. For this, we compute a vector between zero and one, with a starting value of zero and the length of our sequence <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">s</annotation></semantics></math>, multiplied by the given frame rate of the original clip. We call this vector <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="ATs" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1a" xref="S4.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.4" xref="S4.SS2.p2.5.m5.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><times id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></times><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">𝐴</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">𝑇</ci><ci id="S4.SS2.p2.5.m5.1.1.4.cmml" xref="S4.SS2.p2.5.m5.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">ATs</annotation></semantics></math>. We iterate over every word in the sequence <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="Sx" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><times id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></times><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">𝑆</ci><ci id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">Sx</annotation></semantics></math> and note the start- and end timings <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="Ws" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mrow id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mi id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.7.m7.1.1.1" xref="S4.SS2.p2.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><times id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1.1"></times><ci id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">𝑊</ci><ci id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">Ws</annotation></semantics></math> and <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="We" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mrow id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m8.1.1.1" xref="S4.SS2.p2.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><times id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1.1"></times><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">𝑊</ci><ci id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">We</annotation></semantics></math>, respectively.
Using the start- and end timings, we calculate the middle point of the current word timing <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="Wm" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><mrow id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><mi id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m9.1.1.1" xref="S4.SS2.p2.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><times id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1.1"></times><ci id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2">𝑊</ci><ci id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">Wm</annotation></semantics></math>. We linearly interpolate from the start point <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="Ws-1" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mrow id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mrow id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml"><mi id="S4.SS2.p2.10.m10.1.1.2.2" xref="S4.SS2.p2.10.m10.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.10.m10.1.1.2.1" xref="S4.SS2.p2.10.m10.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p2.10.m10.1.1.2.3" xref="S4.SS2.p2.10.m10.1.1.2.3.cmml">s</mi></mrow><mo id="S4.SS2.p2.10.m10.1.1.1" xref="S4.SS2.p2.10.m10.1.1.1.cmml">−</mo><mn id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><minus id="S4.SS2.p2.10.m10.1.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1.1"></minus><apply id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2"><times id="S4.SS2.p2.10.m10.1.1.2.1.cmml" xref="S4.SS2.p2.10.m10.1.1.2.1"></times><ci id="S4.SS2.p2.10.m10.1.1.2.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2.2">𝑊</ci><ci id="S4.SS2.p2.10.m10.1.1.2.3.cmml" xref="S4.SS2.p2.10.m10.1.1.2.3">𝑠</ci></apply><cn type="integer" id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">Ws-1</annotation></semantics></math> with the value zero to the middle point <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="Wm" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><mrow id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.1" xref="S4.SS2.p2.11.m11.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><times id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1"></times><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">𝑊</ci><ci id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">Wm</annotation></semantics></math> with the value one, back down to the value zero at time point <math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="We+1" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><mrow id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml"><mrow id="S4.SS2.p2.12.m12.1.1.2" xref="S4.SS2.p2.12.m12.1.1.2.cmml"><mi id="S4.SS2.p2.12.m12.1.1.2.2" xref="S4.SS2.p2.12.m12.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.12.m12.1.1.2.1" xref="S4.SS2.p2.12.m12.1.1.2.1.cmml">​</mo><mi id="S4.SS2.p2.12.m12.1.1.2.3" xref="S4.SS2.p2.12.m12.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS2.p2.12.m12.1.1.1" xref="S4.SS2.p2.12.m12.1.1.1.cmml">+</mo><mn id="S4.SS2.p2.12.m12.1.1.3" xref="S4.SS2.p2.12.m12.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><apply id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1"><plus id="S4.SS2.p2.12.m12.1.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1.1"></plus><apply id="S4.SS2.p2.12.m12.1.1.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2"><times id="S4.SS2.p2.12.m12.1.1.2.1.cmml" xref="S4.SS2.p2.12.m12.1.1.2.1"></times><ci id="S4.SS2.p2.12.m12.1.1.2.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2.2">𝑊</ci><ci id="S4.SS2.p2.12.m12.1.1.2.3.cmml" xref="S4.SS2.p2.12.m12.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S4.SS2.p2.12.m12.1.1.3.cmml" xref="S4.SS2.p2.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">We+1</annotation></semantics></math>. If the word has a length of exactly one frame, we simply set this specific frame to one. Then, we multiply the vector <math id="S4.SS2.p2.13.m13.1" class="ltx_Math" alttext="ATs" display="inline"><semantics id="S4.SS2.p2.13.m13.1a"><mrow id="S4.SS2.p2.13.m13.1.1" xref="S4.SS2.p2.13.m13.1.1.cmml"><mi id="S4.SS2.p2.13.m13.1.1.2" xref="S4.SS2.p2.13.m13.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.1" xref="S4.SS2.p2.13.m13.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.13.m13.1.1.3" xref="S4.SS2.p2.13.m13.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.1a" xref="S4.SS2.p2.13.m13.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.13.m13.1.1.4" xref="S4.SS2.p2.13.m13.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.13.m13.1b"><apply id="S4.SS2.p2.13.m13.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1"><times id="S4.SS2.p2.13.m13.1.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1.1"></times><ci id="S4.SS2.p2.13.m13.1.1.2.cmml" xref="S4.SS2.p2.13.m13.1.1.2">𝐴</ci><ci id="S4.SS2.p2.13.m13.1.1.3.cmml" xref="S4.SS2.p2.13.m13.1.1.3">𝑇</ci><ci id="S4.SS2.p2.13.m13.1.1.4.cmml" xref="S4.SS2.p2.13.m13.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.13.m13.1c">ATs</annotation></semantics></math> with the spectrogram and wav2vec 2.0 vectors and save the resulting vectors.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For the gesture data, we kept the original position data unchanged. To achieve better coherence between the gesture generation segments, we calculated an additional vector <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="Gv" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝐺</ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">Gv</annotation></semantics></math> by combining the first through sixth derivatives of the gesture position.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Vector Embedding</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To align each dimension of the gesture, audio, and text data, we first normalized each dimension by subtracting the global mean and dividing it by the global standard deviation. As the gesture generation algorithm relies on matching the overlap of the referencing sequence (see chapter <a href="#S4.SS4" title="4.4. Beat Gesture Generation ‣ 4. Gesture Generation ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>), we split the data into two parts of length <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mfrac id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">s</mi><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><divide id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></divide><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑠</ci><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\frac{s}{2}</annotation></semantics></math> along the time axis and only kept the first half of the data.
As the given vectors exhibited a high dimensionality, we performed a Principal Components Analysis (PCA) on each data segment. We deliberately chose this reduction method instead of TSNE, UMAP, VAE, or PaCMAP dimensionality reduction, as all of these methods performed worse in terms of preserving the global structure of our data in our experiments <cite class="ltx_cite ltx_citemacro_citep">(Maaten and Hinton, <a href="#bib.bib58" title="" class="ltx_ref">2008</a>; McInnes et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>; Kingma and Welling, <a href="#bib.bib42" title="" class="ltx_ref">2022</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>. Using PCA, we reduced each dimension to a vector of length 256. Measuring the cumulative explained variance of the data, we preserved 92.6%, 96.7%, and 56.7% of the variance for the gesture–, audio-, and text vectors, respectively. In order to enable fast nearest neighbor vector searches during the actual gesture generation, we stored the resulting vectors, split by modality and a combined vector of all modalities, in a vector database and computed a Hierarchical Navigable Small World graph (HNSW) over the entire vector sets <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib34" title="" class="ltx_ref">2024</a>; Kane, <a href="#bib.bib40" title="" class="ltx_ref">2024</a>; Malkov and Yashunin, <a href="#bib.bib61" title="" class="ltx_ref">2018</a>)</cite>. Finally, we used the resulting vector database to create our vector graph, by defining every clip as a node in our gesture graph and performing an offline search for each singular modality and the combination of all modalities, storing the nearest 20 vectors as the edges for our graph.
Additionally, we added all edges to the graph that are naturally continuous and removed all edges that would lead to a reversed path (i.e. id:80 -¿ id:79) in the graph.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Beat Gesture Generation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.11" class="ltx_p">During the generation of new gestures, we split the given audio and text information into clips of length <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">s</annotation></semantics></math>, with an overlap of <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mfrac id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">s</mi><mn id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><divide id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"></divide><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">𝑠</ci><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\frac{s}{2}</annotation></semantics></math> seconds. We calculated the vector data for the audio and text data for each segment, but contrary to the graph data, we only kept the second half of the vector data along the time axis. As the generation has no gesture information for the first sequence, we retrieved the gesture for the first sequence from our gesture database, by returning the result of the nearest neighbor search for the combined vector of audio and text. Starting with the second segment, we used the vector database to retrieve <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="Gn" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><times id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></times><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">𝐺</ci><ci id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">Gn</annotation></semantics></math> nodes, which minimized the L2 distance of our combined modality vector. For each given node, we calculated the best-performing path up to a depth of <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="Gd" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><times id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"></times><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">𝐺</ci><ci id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">Gd</annotation></semantics></math>, by traveling along the top <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="Ge" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mi id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><times id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></times><ci id="S4.SS4.p1.5.m5.1.1.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2">𝐺</ci><ci id="S4.SS4.p1.5.m5.1.1.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">Ge</annotation></semantics></math> edges for each node. Given the previous sequence vectors as <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><mi id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><ci id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">g</annotation></semantics></math>, the previous audio sequence as <math id="S4.SS4.p1.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS4.p1.7.m7.1a"><mi id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><ci id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">a</annotation></semantics></math>, the previous text sequence as <math id="S4.SS4.p1.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS4.p1.8.m8.1a"><mi id="S4.SS4.p1.8.m8.1.1" xref="S4.SS4.p1.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m8.1b"><ci id="S4.SS4.p1.8.m8.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m8.1c">t</annotation></semantics></math>, the current node vectors as <math id="S4.SS4.p1.9.m9.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS4.p1.9.m9.1a"><mi id="S4.SS4.p1.9.m9.1.1" xref="S4.SS4.p1.9.m9.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.9.m9.1b"><ci id="S4.SS4.p1.9.m9.1.1.cmml" xref="S4.SS4.p1.9.m9.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.9.m9.1c">n</annotation></semantics></math>, the Euclidean distance as <math id="S4.SS4.p1.10.m10.2" class="ltx_Math" alttext="d(x,y)" display="inline"><semantics id="S4.SS4.p1.10.m10.2a"><mrow id="S4.SS4.p1.10.m10.2.3" xref="S4.SS4.p1.10.m10.2.3.cmml"><mi id="S4.SS4.p1.10.m10.2.3.2" xref="S4.SS4.p1.10.m10.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.10.m10.2.3.1" xref="S4.SS4.p1.10.m10.2.3.1.cmml">​</mo><mrow id="S4.SS4.p1.10.m10.2.3.3.2" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.10.m10.2.3.3.2.1" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">(</mo><mi id="S4.SS4.p1.10.m10.1.1" xref="S4.SS4.p1.10.m10.1.1.cmml">x</mi><mo id="S4.SS4.p1.10.m10.2.3.3.2.2" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">,</mo><mi id="S4.SS4.p1.10.m10.2.2" xref="S4.SS4.p1.10.m10.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS4.p1.10.m10.2.3.3.2.3" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.10.m10.2b"><apply id="S4.SS4.p1.10.m10.2.3.cmml" xref="S4.SS4.p1.10.m10.2.3"><times id="S4.SS4.p1.10.m10.2.3.1.cmml" xref="S4.SS4.p1.10.m10.2.3.1"></times><ci id="S4.SS4.p1.10.m10.2.3.2.cmml" xref="S4.SS4.p1.10.m10.2.3.2">𝑑</ci><interval closure="open" id="S4.SS4.p1.10.m10.2.3.3.1.cmml" xref="S4.SS4.p1.10.m10.2.3.3.2"><ci id="S4.SS4.p1.10.m10.1.1.cmml" xref="S4.SS4.p1.10.m10.1.1">𝑥</ci><ci id="S4.SS4.p1.10.m10.2.2.cmml" xref="S4.SS4.p1.10.m10.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.10.m10.2c">d(x,y)</annotation></semantics></math>, and the result of the graph as <math id="S4.SS4.p1.11.m11.1" class="ltx_Math" alttext="d_{step}" display="inline"><semantics id="S4.SS4.p1.11.m11.1a"><msub id="S4.SS4.p1.11.m11.1.1" xref="S4.SS4.p1.11.m11.1.1.cmml"><mi id="S4.SS4.p1.11.m11.1.1.2" xref="S4.SS4.p1.11.m11.1.1.2.cmml">d</mi><mrow id="S4.SS4.p1.11.m11.1.1.3" xref="S4.SS4.p1.11.m11.1.1.3.cmml"><mi id="S4.SS4.p1.11.m11.1.1.3.2" xref="S4.SS4.p1.11.m11.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.11.m11.1.1.3.3" xref="S4.SS4.p1.11.m11.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1a" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.11.m11.1.1.3.4" xref="S4.SS4.p1.11.m11.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1b" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.11.m11.1.1.3.5" xref="S4.SS4.p1.11.m11.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.11.m11.1b"><apply id="S4.SS4.p1.11.m11.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.11.m11.1.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1">subscript</csymbol><ci id="S4.SS4.p1.11.m11.1.1.2.cmml" xref="S4.SS4.p1.11.m11.1.1.2">𝑑</ci><apply id="S4.SS4.p1.11.m11.1.1.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3"><times id="S4.SS4.p1.11.m11.1.1.3.1.cmml" xref="S4.SS4.p1.11.m11.1.1.3.1"></times><ci id="S4.SS4.p1.11.m11.1.1.3.2.cmml" xref="S4.SS4.p1.11.m11.1.1.3.2">𝑠</ci><ci id="S4.SS4.p1.11.m11.1.1.3.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3.3">𝑡</ci><ci id="S4.SS4.p1.11.m11.1.1.3.4.cmml" xref="S4.SS4.p1.11.m11.1.1.3.4">𝑒</ci><ci id="S4.SS4.p1.11.m11.1.1.3.5.cmml" xref="S4.SS4.p1.11.m11.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.11.m11.1c">d_{step}</annotation></semantics></math> we performed the following for every sequence step:</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.2" class="ltx_Math" alttext="\displaystyle d_{node}(x,y)" display="inline"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.3" xref="S4.E3.m1.2.3.cmml"><msub id="S4.E3.m1.2.3.2" xref="S4.E3.m1.2.3.2.cmml"><mi id="S4.E3.m1.2.3.2.2" xref="S4.E3.m1.2.3.2.2.cmml">d</mi><mrow id="S4.E3.m1.2.3.2.3" xref="S4.E3.m1.2.3.2.3.cmml"><mi id="S4.E3.m1.2.3.2.3.2" xref="S4.E3.m1.2.3.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1" xref="S4.E3.m1.2.3.2.3.1.cmml">​</mo><mi id="S4.E3.m1.2.3.2.3.3" xref="S4.E3.m1.2.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1a" xref="S4.E3.m1.2.3.2.3.1.cmml">​</mo><mi id="S4.E3.m1.2.3.2.3.4" xref="S4.E3.m1.2.3.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1b" xref="S4.E3.m1.2.3.2.3.1.cmml">​</mo><mi id="S4.E3.m1.2.3.2.3.5" xref="S4.E3.m1.2.3.2.3.5.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.1" xref="S4.E3.m1.2.3.1.cmml">​</mo><mrow id="S4.E3.m1.2.3.3.2" xref="S4.E3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.E3.m1.2.3.3.2.1" xref="S4.E3.m1.2.3.3.1.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">x</mi><mo id="S4.E3.m1.2.3.3.2.2" xref="S4.E3.m1.2.3.3.1.cmml">,</mo><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">y</mi><mo stretchy="false" id="S4.E3.m1.2.3.3.2.3" xref="S4.E3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.3.cmml" xref="S4.E3.m1.2.3"><times id="S4.E3.m1.2.3.1.cmml" xref="S4.E3.m1.2.3.1"></times><apply id="S4.E3.m1.2.3.2.cmml" xref="S4.E3.m1.2.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.3.2.1.cmml" xref="S4.E3.m1.2.3.2">subscript</csymbol><ci id="S4.E3.m1.2.3.2.2.cmml" xref="S4.E3.m1.2.3.2.2">𝑑</ci><apply id="S4.E3.m1.2.3.2.3.cmml" xref="S4.E3.m1.2.3.2.3"><times id="S4.E3.m1.2.3.2.3.1.cmml" xref="S4.E3.m1.2.3.2.3.1"></times><ci id="S4.E3.m1.2.3.2.3.2.cmml" xref="S4.E3.m1.2.3.2.3.2">𝑛</ci><ci id="S4.E3.m1.2.3.2.3.3.cmml" xref="S4.E3.m1.2.3.2.3.3">𝑜</ci><ci id="S4.E3.m1.2.3.2.3.4.cmml" xref="S4.E3.m1.2.3.2.3.4">𝑑</ci><ci id="S4.E3.m1.2.3.2.3.5.cmml" xref="S4.E3.m1.2.3.2.3.5">𝑒</ci></apply></apply><interval closure="open" id="S4.E3.m1.2.3.3.1.cmml" xref="S4.E3.m1.2.3.3.2"><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">𝑥</ci><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">\displaystyle d_{node}(x,y)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E3.m2.6" class="ltx_Math" alttext="\displaystyle=\lambda_{1}d(x_{g},y_{g})+\lambda_{2}d(x_{a},y_{a})+\lambda_{3}d(x_{t},y_{t})" display="inline"><semantics id="S4.E3.m2.6a"><mrow id="S4.E3.m2.6.6" xref="S4.E3.m2.6.6.cmml"><mi id="S4.E3.m2.6.6.8" xref="S4.E3.m2.6.6.8.cmml"></mi><mo id="S4.E3.m2.6.6.7" xref="S4.E3.m2.6.6.7.cmml">=</mo><mrow id="S4.E3.m2.6.6.6" xref="S4.E3.m2.6.6.6.cmml"><mrow id="S4.E3.m2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.cmml"><msub id="S4.E3.m2.2.2.2.2.4" xref="S4.E3.m2.2.2.2.2.4.cmml"><mi id="S4.E3.m2.2.2.2.2.4.2" xref="S4.E3.m2.2.2.2.2.4.2.cmml">λ</mi><mn id="S4.E3.m2.2.2.2.2.4.3" xref="S4.E3.m2.2.2.2.2.4.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.3.cmml">​</mo><mi id="S4.E3.m2.2.2.2.2.5" xref="S4.E3.m2.2.2.2.2.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.2.2.2.2.3a" xref="S4.E3.m2.2.2.2.2.3.cmml">​</mo><mrow id="S4.E3.m2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.2.2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.2.3.cmml">(</mo><msub id="S4.E3.m2.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.3.cmml">g</mi></msub><mo id="S4.E3.m2.2.2.2.2.2.2.4" xref="S4.E3.m2.2.2.2.2.2.3.cmml">,</mo><msub id="S4.E3.m2.2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.2.2.cmml"><mi id="S4.E3.m2.2.2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.2.2.2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.2.2.2.3.cmml">g</mi></msub><mo stretchy="false" id="S4.E3.m2.2.2.2.2.2.2.5" xref="S4.E3.m2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.6.6.6.7" xref="S4.E3.m2.6.6.6.7.cmml">+</mo><mrow id="S4.E3.m2.4.4.4.4" xref="S4.E3.m2.4.4.4.4.cmml"><msub id="S4.E3.m2.4.4.4.4.4" xref="S4.E3.m2.4.4.4.4.4.cmml"><mi id="S4.E3.m2.4.4.4.4.4.2" xref="S4.E3.m2.4.4.4.4.4.2.cmml">λ</mi><mn id="S4.E3.m2.4.4.4.4.4.3" xref="S4.E3.m2.4.4.4.4.4.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.4.4.4.4.3" xref="S4.E3.m2.4.4.4.4.3.cmml">​</mo><mi id="S4.E3.m2.4.4.4.4.5" xref="S4.E3.m2.4.4.4.4.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.4.4.4.4.3a" xref="S4.E3.m2.4.4.4.4.3.cmml">​</mo><mrow id="S4.E3.m2.4.4.4.4.2.2" xref="S4.E3.m2.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.4.4.4.4.2.2.3" xref="S4.E3.m2.4.4.4.4.2.3.cmml">(</mo><msub id="S4.E3.m2.3.3.3.3.1.1.1" xref="S4.E3.m2.3.3.3.3.1.1.1.cmml"><mi id="S4.E3.m2.3.3.3.3.1.1.1.2" xref="S4.E3.m2.3.3.3.3.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.3.3.3.3.1.1.1.3" xref="S4.E3.m2.3.3.3.3.1.1.1.3.cmml">a</mi></msub><mo id="S4.E3.m2.4.4.4.4.2.2.4" xref="S4.E3.m2.4.4.4.4.2.3.cmml">,</mo><msub id="S4.E3.m2.4.4.4.4.2.2.2" xref="S4.E3.m2.4.4.4.4.2.2.2.cmml"><mi id="S4.E3.m2.4.4.4.4.2.2.2.2" xref="S4.E3.m2.4.4.4.4.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.4.4.4.4.2.2.2.3" xref="S4.E3.m2.4.4.4.4.2.2.2.3.cmml">a</mi></msub><mo stretchy="false" id="S4.E3.m2.4.4.4.4.2.2.5" xref="S4.E3.m2.4.4.4.4.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.6.6.6.7a" xref="S4.E3.m2.6.6.6.7.cmml">+</mo><mrow id="S4.E3.m2.6.6.6.6" xref="S4.E3.m2.6.6.6.6.cmml"><msub id="S4.E3.m2.6.6.6.6.4" xref="S4.E3.m2.6.6.6.6.4.cmml"><mi id="S4.E3.m2.6.6.6.6.4.2" xref="S4.E3.m2.6.6.6.6.4.2.cmml">λ</mi><mn id="S4.E3.m2.6.6.6.6.4.3" xref="S4.E3.m2.6.6.6.6.4.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.6.6.3" xref="S4.E3.m2.6.6.6.6.3.cmml">​</mo><mi id="S4.E3.m2.6.6.6.6.5" xref="S4.E3.m2.6.6.6.6.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.6.6.3a" xref="S4.E3.m2.6.6.6.6.3.cmml">​</mo><mrow id="S4.E3.m2.6.6.6.6.2.2" xref="S4.E3.m2.6.6.6.6.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.6.6.6.6.2.2.3" xref="S4.E3.m2.6.6.6.6.2.3.cmml">(</mo><msub id="S4.E3.m2.5.5.5.5.1.1.1" xref="S4.E3.m2.5.5.5.5.1.1.1.cmml"><mi id="S4.E3.m2.5.5.5.5.1.1.1.2" xref="S4.E3.m2.5.5.5.5.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.5.5.5.5.1.1.1.3" xref="S4.E3.m2.5.5.5.5.1.1.1.3.cmml">t</mi></msub><mo id="S4.E3.m2.6.6.6.6.2.2.4" xref="S4.E3.m2.6.6.6.6.2.3.cmml">,</mo><msub id="S4.E3.m2.6.6.6.6.2.2.2" xref="S4.E3.m2.6.6.6.6.2.2.2.cmml"><mi id="S4.E3.m2.6.6.6.6.2.2.2.2" xref="S4.E3.m2.6.6.6.6.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.6.6.6.6.2.2.2.3" xref="S4.E3.m2.6.6.6.6.2.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="S4.E3.m2.6.6.6.6.2.2.5" xref="S4.E3.m2.6.6.6.6.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m2.6b"><apply id="S4.E3.m2.6.6.cmml" xref="S4.E3.m2.6.6"><eq id="S4.E3.m2.6.6.7.cmml" xref="S4.E3.m2.6.6.7"></eq><csymbol cd="latexml" id="S4.E3.m2.6.6.8.cmml" xref="S4.E3.m2.6.6.8">absent</csymbol><apply id="S4.E3.m2.6.6.6.cmml" xref="S4.E3.m2.6.6.6"><plus id="S4.E3.m2.6.6.6.7.cmml" xref="S4.E3.m2.6.6.6.7"></plus><apply id="S4.E3.m2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2"><times id="S4.E3.m2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.3"></times><apply id="S4.E3.m2.2.2.2.2.4.cmml" xref="S4.E3.m2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.E3.m2.2.2.2.2.4.1.cmml" xref="S4.E3.m2.2.2.2.2.4">subscript</csymbol><ci id="S4.E3.m2.2.2.2.2.4.2.cmml" xref="S4.E3.m2.2.2.2.2.4.2">𝜆</ci><cn type="integer" id="S4.E3.m2.2.2.2.2.4.3.cmml" xref="S4.E3.m2.2.2.2.2.4.3">1</cn></apply><ci id="S4.E3.m2.2.2.2.2.5.cmml" xref="S4.E3.m2.2.2.2.2.5">𝑑</ci><interval closure="open" id="S4.E3.m2.2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.2.2"><apply id="S4.E3.m2.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.3">𝑔</ci></apply><apply id="S4.E3.m2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.2.2.2.2.2.2.2.1.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E3.m2.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2.2">𝑦</ci><ci id="S4.E3.m2.2.2.2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2.3">𝑔</ci></apply></interval></apply><apply id="S4.E3.m2.4.4.4.4.cmml" xref="S4.E3.m2.4.4.4.4"><times id="S4.E3.m2.4.4.4.4.3.cmml" xref="S4.E3.m2.4.4.4.4.3"></times><apply id="S4.E3.m2.4.4.4.4.4.cmml" xref="S4.E3.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S4.E3.m2.4.4.4.4.4.1.cmml" xref="S4.E3.m2.4.4.4.4.4">subscript</csymbol><ci id="S4.E3.m2.4.4.4.4.4.2.cmml" xref="S4.E3.m2.4.4.4.4.4.2">𝜆</ci><cn type="integer" id="S4.E3.m2.4.4.4.4.4.3.cmml" xref="S4.E3.m2.4.4.4.4.4.3">2</cn></apply><ci id="S4.E3.m2.4.4.4.4.5.cmml" xref="S4.E3.m2.4.4.4.4.5">𝑑</ci><interval closure="open" id="S4.E3.m2.4.4.4.4.2.3.cmml" xref="S4.E3.m2.4.4.4.4.2.2"><apply id="S4.E3.m2.3.3.3.3.1.1.1.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.3.3.3.3.1.1.1.1.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1">subscript</csymbol><ci id="S4.E3.m2.3.3.3.3.1.1.1.2.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1.2">𝑥</ci><ci id="S4.E3.m2.3.3.3.3.1.1.1.3.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1.3">𝑎</ci></apply><apply id="S4.E3.m2.4.4.4.4.2.2.2.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.4.4.4.4.2.2.2.1.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2">subscript</csymbol><ci id="S4.E3.m2.4.4.4.4.2.2.2.2.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2.2">𝑦</ci><ci id="S4.E3.m2.4.4.4.4.2.2.2.3.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2.3">𝑎</ci></apply></interval></apply><apply id="S4.E3.m2.6.6.6.6.cmml" xref="S4.E3.m2.6.6.6.6"><times id="S4.E3.m2.6.6.6.6.3.cmml" xref="S4.E3.m2.6.6.6.6.3"></times><apply id="S4.E3.m2.6.6.6.6.4.cmml" xref="S4.E3.m2.6.6.6.6.4"><csymbol cd="ambiguous" id="S4.E3.m2.6.6.6.6.4.1.cmml" xref="S4.E3.m2.6.6.6.6.4">subscript</csymbol><ci id="S4.E3.m2.6.6.6.6.4.2.cmml" xref="S4.E3.m2.6.6.6.6.4.2">𝜆</ci><cn type="integer" id="S4.E3.m2.6.6.6.6.4.3.cmml" xref="S4.E3.m2.6.6.6.6.4.3">3</cn></apply><ci id="S4.E3.m2.6.6.6.6.5.cmml" xref="S4.E3.m2.6.6.6.6.5">𝑑</ci><interval closure="open" id="S4.E3.m2.6.6.6.6.2.3.cmml" xref="S4.E3.m2.6.6.6.6.2.2"><apply id="S4.E3.m2.5.5.5.5.1.1.1.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.5.5.5.5.1.1.1.1.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1">subscript</csymbol><ci id="S4.E3.m2.5.5.5.5.1.1.1.2.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1.2">𝑥</ci><ci id="S4.E3.m2.5.5.5.5.1.1.1.3.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1.3">𝑡</ci></apply><apply id="S4.E3.m2.6.6.6.6.2.2.2.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.6.6.6.6.2.2.2.1.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2">subscript</csymbol><ci id="S4.E3.m2.6.6.6.6.2.2.2.2.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2.2">𝑦</ci><ci id="S4.E3.m2.6.6.6.6.2.2.2.3.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2.3">𝑡</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m2.6c">\displaystyle=\lambda_{1}d(x_{g},y_{g})+\lambda_{2}d(x_{a},y_{a})+\lambda_{3}d(x_{t},y_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.3" class="ltx_Math" alttext="\displaystyle d_{path}(vg_{t},vn_{t},t)" display="inline"><semantics id="S4.E4.m1.3a"><mrow id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml"><msub id="S4.E4.m1.3.3.4" xref="S4.E4.m1.3.3.4.cmml"><mi id="S4.E4.m1.3.3.4.2" xref="S4.E4.m1.3.3.4.2.cmml">d</mi><mrow id="S4.E4.m1.3.3.4.3" xref="S4.E4.m1.3.3.4.3.cmml"><mi id="S4.E4.m1.3.3.4.3.2" xref="S4.E4.m1.3.3.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1" xref="S4.E4.m1.3.3.4.3.1.cmml">​</mo><mi id="S4.E4.m1.3.3.4.3.3" xref="S4.E4.m1.3.3.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1a" xref="S4.E4.m1.3.3.4.3.1.cmml">​</mo><mi id="S4.E4.m1.3.3.4.3.4" xref="S4.E4.m1.3.3.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1b" xref="S4.E4.m1.3.3.4.3.1.cmml">​</mo><mi id="S4.E4.m1.3.3.4.3.5" xref="S4.E4.m1.3.3.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3" xref="S4.E4.m1.3.3.3.cmml">​</mo><mrow id="S4.E4.m1.3.3.2.2" xref="S4.E4.m1.3.3.2.3.cmml"><mo stretchy="false" id="S4.E4.m1.3.3.2.2.3" xref="S4.E4.m1.3.3.2.3.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1" xref="S4.E4.m1.2.2.1.1.1.cmml"><mi id="S4.E4.m1.2.2.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.cmml">​</mo><msub id="S4.E4.m1.2.2.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.3.2.cmml">g</mi><mi id="S4.E4.m1.2.2.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.3.3.cmml">t</mi></msub></mrow><mo id="S4.E4.m1.3.3.2.2.4" xref="S4.E4.m1.3.3.2.3.cmml">,</mo><mrow id="S4.E4.m1.3.3.2.2.2" xref="S4.E4.m1.3.3.2.2.2.cmml"><mi id="S4.E4.m1.3.3.2.2.2.2" xref="S4.E4.m1.3.3.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.2.2.2.1" xref="S4.E4.m1.3.3.2.2.2.1.cmml">​</mo><msub id="S4.E4.m1.3.3.2.2.2.3" xref="S4.E4.m1.3.3.2.2.2.3.cmml"><mi id="S4.E4.m1.3.3.2.2.2.3.2" xref="S4.E4.m1.3.3.2.2.2.3.2.cmml">n</mi><mi id="S4.E4.m1.3.3.2.2.2.3.3" xref="S4.E4.m1.3.3.2.2.2.3.3.cmml">t</mi></msub></mrow><mo id="S4.E4.m1.3.3.2.2.5" xref="S4.E4.m1.3.3.2.3.cmml">,</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">t</mi><mo stretchy="false" id="S4.E4.m1.3.3.2.2.6" xref="S4.E4.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.3b"><apply id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3"><times id="S4.E4.m1.3.3.3.cmml" xref="S4.E4.m1.3.3.3"></times><apply id="S4.E4.m1.3.3.4.cmml" xref="S4.E4.m1.3.3.4"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.4.1.cmml" xref="S4.E4.m1.3.3.4">subscript</csymbol><ci id="S4.E4.m1.3.3.4.2.cmml" xref="S4.E4.m1.3.3.4.2">𝑑</ci><apply id="S4.E4.m1.3.3.4.3.cmml" xref="S4.E4.m1.3.3.4.3"><times id="S4.E4.m1.3.3.4.3.1.cmml" xref="S4.E4.m1.3.3.4.3.1"></times><ci id="S4.E4.m1.3.3.4.3.2.cmml" xref="S4.E4.m1.3.3.4.3.2">𝑝</ci><ci id="S4.E4.m1.3.3.4.3.3.cmml" xref="S4.E4.m1.3.3.4.3.3">𝑎</ci><ci id="S4.E4.m1.3.3.4.3.4.cmml" xref="S4.E4.m1.3.3.4.3.4">𝑡</ci><ci id="S4.E4.m1.3.3.4.3.5.cmml" xref="S4.E4.m1.3.3.4.3.5">ℎ</ci></apply></apply><vector id="S4.E4.m1.3.3.2.3.cmml" xref="S4.E4.m1.3.3.2.2"><apply id="S4.E4.m1.2.2.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1"></times><ci id="S4.E4.m1.2.2.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.2">𝑣</ci><apply id="S4.E4.m1.2.2.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.3.2">𝑔</ci><ci id="S4.E4.m1.2.2.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S4.E4.m1.3.3.2.2.2.cmml" xref="S4.E4.m1.3.3.2.2.2"><times id="S4.E4.m1.3.3.2.2.2.1.cmml" xref="S4.E4.m1.3.3.2.2.2.1"></times><ci id="S4.E4.m1.3.3.2.2.2.2.cmml" xref="S4.E4.m1.3.3.2.2.2.2">𝑣</ci><apply id="S4.E4.m1.3.3.2.2.2.3.cmml" xref="S4.E4.m1.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.2.2.3.1.cmml" xref="S4.E4.m1.3.3.2.2.2.3">subscript</csymbol><ci id="S4.E4.m1.3.3.2.2.2.3.2.cmml" xref="S4.E4.m1.3.3.2.2.2.3.2">𝑛</ci><ci id="S4.E4.m1.3.3.2.2.2.3.3.cmml" xref="S4.E4.m1.3.3.2.2.2.3.3">𝑡</ci></apply></apply><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">𝑡</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.3c">\displaystyle d_{path}(vg_{t},vn_{t},t)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E4.m2.2" class="ltx_Math" alttext="\displaystyle=\sum_{i=0}^{Gd}d_{node}(vg_{t+i},vn_{t+i})" display="inline"><semantics id="S4.E4.m2.2a"><mrow id="S4.E4.m2.2.2" xref="S4.E4.m2.2.2.cmml"><mi id="S4.E4.m2.2.2.4" xref="S4.E4.m2.2.2.4.cmml"></mi><mo id="S4.E4.m2.2.2.3" xref="S4.E4.m2.2.2.3.cmml">=</mo><mrow id="S4.E4.m2.2.2.2" xref="S4.E4.m2.2.2.2.cmml"><mstyle displaystyle="true" id="S4.E4.m2.2.2.2.3" xref="S4.E4.m2.2.2.2.3.cmml"><munderover id="S4.E4.m2.2.2.2.3a" xref="S4.E4.m2.2.2.2.3.cmml"><mo movablelimits="false" id="S4.E4.m2.2.2.2.3.2.2" xref="S4.E4.m2.2.2.2.3.2.2.cmml">∑</mo><mrow id="S4.E4.m2.2.2.2.3.2.3" xref="S4.E4.m2.2.2.2.3.2.3.cmml"><mi id="S4.E4.m2.2.2.2.3.2.3.2" xref="S4.E4.m2.2.2.2.3.2.3.2.cmml">i</mi><mo id="S4.E4.m2.2.2.2.3.2.3.1" xref="S4.E4.m2.2.2.2.3.2.3.1.cmml">=</mo><mn id="S4.E4.m2.2.2.2.3.2.3.3" xref="S4.E4.m2.2.2.2.3.2.3.3.cmml">0</mn></mrow><mrow id="S4.E4.m2.2.2.2.3.3" xref="S4.E4.m2.2.2.2.3.3.cmml"><mi id="S4.E4.m2.2.2.2.3.3.2" xref="S4.E4.m2.2.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.3.3.1" xref="S4.E4.m2.2.2.2.3.3.1.cmml">​</mo><mi id="S4.E4.m2.2.2.2.3.3.3" xref="S4.E4.m2.2.2.2.3.3.3.cmml">d</mi></mrow></munderover></mstyle><mrow id="S4.E4.m2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.cmml"><msub id="S4.E4.m2.2.2.2.2.4" xref="S4.E4.m2.2.2.2.2.4.cmml"><mi id="S4.E4.m2.2.2.2.2.4.2" xref="S4.E4.m2.2.2.2.2.4.2.cmml">d</mi><mrow id="S4.E4.m2.2.2.2.2.4.3" xref="S4.E4.m2.2.2.2.2.4.3.cmml"><mi id="S4.E4.m2.2.2.2.2.4.3.2" xref="S4.E4.m2.2.2.2.2.4.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E4.m2.2.2.2.2.4.3.3" xref="S4.E4.m2.2.2.2.2.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1a" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E4.m2.2.2.2.2.4.3.4" xref="S4.E4.m2.2.2.2.2.4.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1b" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E4.m2.2.2.2.2.4.3.5" xref="S4.E4.m2.2.2.2.2.4.3.5.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.3.cmml">​</mo><mrow id="S4.E4.m2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E4.m2.2.2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.2.3.cmml">(</mo><mrow id="S4.E4.m2.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.1.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.E4.m2.1.1.1.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.2" xref="S4.E4.m2.1.1.1.1.1.1.1.3.2.cmml">g</mi><mrow id="S4.E4.m2.1.1.1.1.1.1.1.3.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.3.2" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S4.E4.m2.1.1.1.1.1.1.1.3.3.1" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.3.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.E4.m2.2.2.2.2.2.2.4" xref="S4.E4.m2.2.2.2.2.2.3.cmml">,</mo><mrow id="S4.E4.m2.2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.2.2.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.2.2.2.1" xref="S4.E4.m2.2.2.2.2.2.2.2.1.cmml">​</mo><msub id="S4.E4.m2.2.2.2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.2" xref="S4.E4.m2.2.2.2.2.2.2.2.3.2.cmml">n</mi><mrow id="S4.E4.m2.2.2.2.2.2.2.2.3.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.3.2" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.2.cmml">t</mi><mo id="S4.E4.m2.2.2.2.2.2.2.2.3.3.1" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.1.cmml">+</mo><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.3.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E4.m2.2.2.2.2.2.2.5" xref="S4.E4.m2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m2.2b"><apply id="S4.E4.m2.2.2.cmml" xref="S4.E4.m2.2.2"><eq id="S4.E4.m2.2.2.3.cmml" xref="S4.E4.m2.2.2.3"></eq><csymbol cd="latexml" id="S4.E4.m2.2.2.4.cmml" xref="S4.E4.m2.2.2.4">absent</csymbol><apply id="S4.E4.m2.2.2.2.cmml" xref="S4.E4.m2.2.2.2"><apply id="S4.E4.m2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.3.1.cmml" xref="S4.E4.m2.2.2.2.3">superscript</csymbol><apply id="S4.E4.m2.2.2.2.3.2.cmml" xref="S4.E4.m2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.3.2.1.cmml" xref="S4.E4.m2.2.2.2.3">subscript</csymbol><sum id="S4.E4.m2.2.2.2.3.2.2.cmml" xref="S4.E4.m2.2.2.2.3.2.2"></sum><apply id="S4.E4.m2.2.2.2.3.2.3.cmml" xref="S4.E4.m2.2.2.2.3.2.3"><eq id="S4.E4.m2.2.2.2.3.2.3.1.cmml" xref="S4.E4.m2.2.2.2.3.2.3.1"></eq><ci id="S4.E4.m2.2.2.2.3.2.3.2.cmml" xref="S4.E4.m2.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S4.E4.m2.2.2.2.3.2.3.3.cmml" xref="S4.E4.m2.2.2.2.3.2.3.3">0</cn></apply></apply><apply id="S4.E4.m2.2.2.2.3.3.cmml" xref="S4.E4.m2.2.2.2.3.3"><times id="S4.E4.m2.2.2.2.3.3.1.cmml" xref="S4.E4.m2.2.2.2.3.3.1"></times><ci id="S4.E4.m2.2.2.2.3.3.2.cmml" xref="S4.E4.m2.2.2.2.3.3.2">𝐺</ci><ci id="S4.E4.m2.2.2.2.3.3.3.cmml" xref="S4.E4.m2.2.2.2.3.3.3">𝑑</ci></apply></apply><apply id="S4.E4.m2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2"><times id="S4.E4.m2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.3"></times><apply id="S4.E4.m2.2.2.2.2.4.cmml" xref="S4.E4.m2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.2.4.1.cmml" xref="S4.E4.m2.2.2.2.2.4">subscript</csymbol><ci id="S4.E4.m2.2.2.2.2.4.2.cmml" xref="S4.E4.m2.2.2.2.2.4.2">𝑑</ci><apply id="S4.E4.m2.2.2.2.2.4.3.cmml" xref="S4.E4.m2.2.2.2.2.4.3"><times id="S4.E4.m2.2.2.2.2.4.3.1.cmml" xref="S4.E4.m2.2.2.2.2.4.3.1"></times><ci id="S4.E4.m2.2.2.2.2.4.3.2.cmml" xref="S4.E4.m2.2.2.2.2.4.3.2">𝑛</ci><ci id="S4.E4.m2.2.2.2.2.4.3.3.cmml" xref="S4.E4.m2.2.2.2.2.4.3.3">𝑜</ci><ci id="S4.E4.m2.2.2.2.2.4.3.4.cmml" xref="S4.E4.m2.2.2.2.2.4.3.4">𝑑</ci><ci id="S4.E4.m2.2.2.2.2.4.3.5.cmml" xref="S4.E4.m2.2.2.2.2.4.3.5">𝑒</ci></apply></apply><interval closure="open" id="S4.E4.m2.2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2"><apply id="S4.E4.m2.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1"><times id="S4.E4.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.1"></times><ci id="S4.E4.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.2">𝑣</ci><apply id="S4.E4.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.2">𝑔</ci><apply id="S4.E4.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3"><plus id="S4.E4.m2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.1"></plus><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E4.m2.2.2.2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2"><times id="S4.E4.m2.2.2.2.2.2.2.2.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.1"></times><ci id="S4.E4.m2.2.2.2.2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.2">𝑣</ci><apply id="S4.E4.m2.2.2.2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.2.2.2.2.3.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.2">𝑛</ci><apply id="S4.E4.m2.2.2.2.2.2.2.2.3.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3"><plus id="S4.E4.m2.2.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.1"></plus><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.2">𝑡</ci><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.3">𝑖</ci></apply></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m2.2c">\displaystyle=\sum_{i=0}^{Gd}d_{node}(vg_{t+i},vn_{t+i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\displaystyle d_{step}" display="inline"><semantics id="S4.E5.m1.1a"><msub id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mi id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2.cmml">d</mi><mrow id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.3.2" xref="S4.E5.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1" xref="S4.E5.m1.1.1.3.1.cmml">​</mo><mi id="S4.E5.m1.1.1.3.3" xref="S4.E5.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1a" xref="S4.E5.m1.1.1.3.1.cmml">​</mo><mi id="S4.E5.m1.1.1.3.4" xref="S4.E5.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1b" xref="S4.E5.m1.1.1.3.1.cmml">​</mo><mi id="S4.E5.m1.1.1.3.5" xref="S4.E5.m1.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1">subscript</csymbol><ci id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2">𝑑</ci><apply id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3"><times id="S4.E5.m1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.3.1"></times><ci id="S4.E5.m1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.3.2">𝑠</ci><ci id="S4.E5.m1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.3.3">𝑡</ci><ci id="S4.E5.m1.1.1.3.4.cmml" xref="S4.E5.m1.1.1.3.4">𝑒</ci><ci id="S4.E5.m1.1.1.3.5.cmml" xref="S4.E5.m1.1.1.3.5">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\displaystyle d_{step}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E5.m2.3" class="ltx_Math" alttext="\displaystyle=\{d_{path}(vg_{0},vn_{0}),...,d_{path}(vg_{Gn},vn_{Gn})\}" display="inline"><semantics id="S4.E5.m2.3a"><mrow id="S4.E5.m2.3.3" xref="S4.E5.m2.3.3.cmml"><mi id="S4.E5.m2.3.3.4" xref="S4.E5.m2.3.3.4.cmml"></mi><mo id="S4.E5.m2.3.3.3" xref="S4.E5.m2.3.3.3.cmml">=</mo><mrow id="S4.E5.m2.3.3.2.2" xref="S4.E5.m2.3.3.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.3.3.2.2.3" xref="S4.E5.m2.3.3.2.3.cmml">{</mo><mrow id="S4.E5.m2.2.2.1.1.1" xref="S4.E5.m2.2.2.1.1.1.cmml"><msub id="S4.E5.m2.2.2.1.1.1.4" xref="S4.E5.m2.2.2.1.1.1.4.cmml"><mi id="S4.E5.m2.2.2.1.1.1.4.2" xref="S4.E5.m2.2.2.1.1.1.4.2.cmml">d</mi><mrow id="S4.E5.m2.2.2.1.1.1.4.3" xref="S4.E5.m2.2.2.1.1.1.4.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.4.3.2" xref="S4.E5.m2.2.2.1.1.1.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.3" xref="S4.E5.m2.2.2.1.1.1.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1a" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.4" xref="S4.E5.m2.2.2.1.1.1.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1b" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.5" xref="S4.E5.m2.2.2.1.1.1.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.3" xref="S4.E5.m2.2.2.1.1.1.3.cmml">​</mo><mrow id="S4.E5.m2.2.2.1.1.1.2.2" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.2.2.1.1.1.2.2.3" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">(</mo><mrow id="S4.E5.m2.2.2.1.1.1.1.1.1" xref="S4.E5.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E5.m2.2.2.1.1.1.1.1.1.2" xref="S4.E5.m2.2.2.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.1.1.1.1" xref="S4.E5.m2.2.2.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.E5.m2.2.2.1.1.1.1.1.1.3" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.1.1.1.3.2" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.2.cmml">g</mi><mn id="S4.E5.m2.2.2.1.1.1.1.1.1.3.3" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S4.E5.m2.2.2.1.1.1.2.2.4" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">,</mo><mrow id="S4.E5.m2.2.2.1.1.1.2.2.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.cmml"><mi id="S4.E5.m2.2.2.1.1.1.2.2.2.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.2.2.2.1" xref="S4.E5.m2.2.2.1.1.1.2.2.2.1.cmml">​</mo><msub id="S4.E5.m2.2.2.1.1.1.2.2.2.3" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.2.2.2.3.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.2.cmml">n</mi><mn id="S4.E5.m2.2.2.1.1.1.2.2.2.3.3" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S4.E5.m2.2.2.1.1.1.2.2.5" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E5.m2.3.3.2.2.4" xref="S4.E5.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.E5.m2.1.1" xref="S4.E5.m2.1.1.cmml">…</mi><mo id="S4.E5.m2.3.3.2.2.5" xref="S4.E5.m2.3.3.2.3.cmml">,</mo><mrow id="S4.E5.m2.3.3.2.2.2" xref="S4.E5.m2.3.3.2.2.2.cmml"><msub id="S4.E5.m2.3.3.2.2.2.4" xref="S4.E5.m2.3.3.2.2.2.4.cmml"><mi id="S4.E5.m2.3.3.2.2.2.4.2" xref="S4.E5.m2.3.3.2.2.2.4.2.cmml">d</mi><mrow id="S4.E5.m2.3.3.2.2.2.4.3" xref="S4.E5.m2.3.3.2.2.2.4.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.4.3.2" xref="S4.E5.m2.3.3.2.2.2.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.3" xref="S4.E5.m2.3.3.2.2.2.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1a" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.4" xref="S4.E5.m2.3.3.2.2.2.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1b" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">​</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.5" xref="S4.E5.m2.3.3.2.2.2.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.3.cmml">​</mo><mrow id="S4.E5.m2.3.3.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.3.3.2.2.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">(</mo><mrow id="S4.E5.m2.3.3.2.2.2.1.1.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.1.1.1.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.1.cmml">​</mo><msub id="S4.E5.m2.3.3.2.2.2.1.1.1.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.2.cmml">g</mi><mrow id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3.cmml">n</mi></mrow></msub></mrow><mo id="S4.E5.m2.3.3.2.2.2.2.2.4" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">,</mo><mrow id="S4.E5.m2.3.3.2.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.2.2.2.1" xref="S4.E5.m2.3.3.2.2.2.2.2.2.1.cmml">​</mo><msub id="S4.E5.m2.3.3.2.2.2.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.2.cmml">n</mi><mrow id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1.cmml">​</mo><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3.cmml">n</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E5.m2.3.3.2.2.2.2.2.5" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m2.3.3.2.2.6" xref="S4.E5.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m2.3b"><apply id="S4.E5.m2.3.3.cmml" xref="S4.E5.m2.3.3"><eq id="S4.E5.m2.3.3.3.cmml" xref="S4.E5.m2.3.3.3"></eq><csymbol cd="latexml" id="S4.E5.m2.3.3.4.cmml" xref="S4.E5.m2.3.3.4">absent</csymbol><set id="S4.E5.m2.3.3.2.3.cmml" xref="S4.E5.m2.3.3.2.2"><apply id="S4.E5.m2.2.2.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1"><times id="S4.E5.m2.2.2.1.1.1.3.cmml" xref="S4.E5.m2.2.2.1.1.1.3"></times><apply id="S4.E5.m2.2.2.1.1.1.4.cmml" xref="S4.E5.m2.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.4.1.cmml" xref="S4.E5.m2.2.2.1.1.1.4">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.4.2.cmml" xref="S4.E5.m2.2.2.1.1.1.4.2">𝑑</ci><apply id="S4.E5.m2.2.2.1.1.1.4.3.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3"><times id="S4.E5.m2.2.2.1.1.1.4.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.1"></times><ci id="S4.E5.m2.2.2.1.1.1.4.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.2">𝑝</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.3">𝑎</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.4.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.4">𝑡</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.5.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.5">ℎ</ci></apply></apply><interval closure="open" id="S4.E5.m2.2.2.1.1.1.2.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2"><apply id="S4.E5.m2.2.2.1.1.1.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1"><times id="S4.E5.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.1"></times><ci id="S4.E5.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.2">𝑣</ci><apply id="S4.E5.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.2">𝑔</ci><cn type="integer" id="S4.E5.m2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.3">0</cn></apply></apply><apply id="S4.E5.m2.2.2.1.1.1.2.2.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2"><times id="S4.E5.m2.2.2.1.1.1.2.2.2.1.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.1"></times><ci id="S4.E5.m2.2.2.1.1.1.2.2.2.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.2">𝑣</ci><apply id="S4.E5.m2.2.2.1.1.1.2.2.2.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.2.2.2.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.2.2.2.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.2">𝑛</ci><cn type="integer" id="S4.E5.m2.2.2.1.1.1.2.2.2.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.3">0</cn></apply></apply></interval></apply><ci id="S4.E5.m2.1.1.cmml" xref="S4.E5.m2.1.1">…</ci><apply id="S4.E5.m2.3.3.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2"><times id="S4.E5.m2.3.3.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.3"></times><apply id="S4.E5.m2.3.3.2.2.2.4.cmml" xref="S4.E5.m2.3.3.2.2.2.4"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.4.1.cmml" xref="S4.E5.m2.3.3.2.2.2.4">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.4.2.cmml" xref="S4.E5.m2.3.3.2.2.2.4.2">𝑑</ci><apply id="S4.E5.m2.3.3.2.2.2.4.3.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3"><times id="S4.E5.m2.3.3.2.2.2.4.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.4.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.2">𝑝</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.3">𝑎</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.4.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.4">𝑡</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.5.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.5">ℎ</ci></apply></apply><interval closure="open" id="S4.E5.m2.3.3.2.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2"><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1"><times id="S4.E5.m2.3.3.2.2.2.1.1.1.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.1"></times><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.2">𝑣</ci><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.1.1.1.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.2">𝑔</ci><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3"><times id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2">𝐺</ci><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3">𝑛</ci></apply></apply></apply><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2"><times id="S4.E5.m2.3.3.2.2.2.2.2.2.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.1"></times><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.2">𝑣</ci><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.2.2.2.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.2">𝑛</ci><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3"><times id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2">𝐺</ci><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3">𝑛</ci></apply></apply></apply></interval></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m2.3c">\displaystyle=\{d_{path}(vg_{0},vn_{0}),...,d_{path}(vg_{Gn},vn_{Gn})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.5" class="ltx_p">The hyperparameters <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\lambda_{1}" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">λ</mi><mn id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">𝜆</ci><cn type="integer" id="S4.SS4.p3.1.m1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\lambda_{1}</annotation></semantics></math>, <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="\lambda_{2}" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">λ</mi><mn id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">𝜆</ci><cn type="integer" id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\lambda_{2}</annotation></semantics></math>, and <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="\lambda_{3}" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><msub id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">λ</mi><mn id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">𝜆</ci><cn type="integer" id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">\lambda_{3}</annotation></semantics></math> were set to 4, 2, and 1, respectively. If a path contained an already selected node, the path was removed. After ordering the sequence list, we sampled from the top <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mi id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><ci id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">k</annotation></semantics></math> paths, by converting the normalized distances to a probability distribution. After choosing the sequence sample, we set the new gesture value to the gesture value of <math id="S4.SS4.p3.5.m5.1" class="ltx_Math" alttext="Vg_{0}" display="inline"><semantics id="S4.SS4.p3.5.m5.1a"><mrow id="S4.SS4.p3.5.m5.1.1" xref="S4.SS4.p3.5.m5.1.1.cmml"><mi id="S4.SS4.p3.5.m5.1.1.2" xref="S4.SS4.p3.5.m5.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.5.m5.1.1.1" xref="S4.SS4.p3.5.m5.1.1.1.cmml">​</mo><msub id="S4.SS4.p3.5.m5.1.1.3" xref="S4.SS4.p3.5.m5.1.1.3.cmml"><mi id="S4.SS4.p3.5.m5.1.1.3.2" xref="S4.SS4.p3.5.m5.1.1.3.2.cmml">g</mi><mn id="S4.SS4.p3.5.m5.1.1.3.3" xref="S4.SS4.p3.5.m5.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.5.m5.1b"><apply id="S4.SS4.p3.5.m5.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1"><times id="S4.SS4.p3.5.m5.1.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1.1"></times><ci id="S4.SS4.p3.5.m5.1.1.2.cmml" xref="S4.SS4.p3.5.m5.1.1.2">𝑉</ci><apply id="S4.SS4.p3.5.m5.1.1.3.cmml" xref="S4.SS4.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.1.1.3.1.cmml" xref="S4.SS4.p3.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS4.p3.5.m5.1.1.3.2.cmml" xref="S4.SS4.p3.5.m5.1.1.3.2">𝑔</ci><cn type="integer" id="S4.SS4.p3.5.m5.1.1.3.3.cmml" xref="S4.SS4.p3.5.m5.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.5.m5.1c">Vg_{0}</annotation></semantics></math> and repeated the process for the next step.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Iconic Gestures</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">As the main goal of the present work is to support the understanding of an explainee, we aimed to include believable representational (iconic) gestures that could be easily added and removed from the original gestures. To the best of our knowledge, no system exists that can automatically generate high-quality, aligned iconic gestures that are semantically coherent to a given verbal input. We hence manually annotated all instances of the generated explanation where an iconic gesture would make sense and pre-recorded suitable iconic gestures. For this, we captured 40 iconic gestures, with 3 different repetitions for variety, with a Logitech C920 in 1080p and used the same pose estimation algorithm as in Sect. <a href="#S4.SS1" title="4.1. Dataset ‣ 4. Gesture Generation ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> to extract the arm and hand positions <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. As misalignment between gestures and speech is known to decrease interaction quality <cite class="ltx_cite ltx_citemacro_citep">(Salem et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2012</a>; Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>; Kelly et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2004</a>)</cite>, we did not use any automatic method to determine the position and length of the iconic gesture clips, but instead placed, aligned, and blended all iconic gesture clips by hand using the software Blender <cite class="ltx_cite ltx_citemacro_citep">(Community, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>. To ensure believable high-quality iconic gestures, we only added iconic gestures if new information during the explanation was given.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation Study</h2>

<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2406.12544/assets/pictures/understanding_comparison_general.png" id="S5.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>General understanding</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2406.12544/assets/pictures/understanding_comparison_deep.png" id="S5.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Deep understanding</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We conducted an online user study<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The study was preregistered in OSF: The anonymized pre-registration can be found at <a target="_blank" href="https://osf.io/db7rz/?view_only=eddd075d791848599e009961b7352e68" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://osf.io/db7rz/?view_only=eddd075d791848599e009961b7352e68</a></span></span></span>
to test the effects of gestures on the explainee’s understanding and perception of the explanation generated by the virtual agent. The study has four conditions. In the baseline condition, the agent keeps the arms at its side and moves them slightly, but does not perform any gestures. In the beat condition, the agent performs the beat gestures generated by the gesture graph algorithm (Sect. <a href="#S4" title="4. Gesture Generation ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The iconic condition consists of the baseline condition to which we added the manually captured iconic gestures (cf. Sect. <a href="#S4.SS5" title="4.5. Iconic Gestures ‣ 4. Gesture Generation ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>). Finally, the mixed condition combines the beat condition with the iconic gestures. <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The videos for each condition can be found in the OSF project <a target="_blank" href="https://osf.io/bf2yk/?view_only=704cf1241b4f4d7490bfb213a84c7fdc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://osf.io/bf2yk/?view_only=704cf1241b4f4d7490bfb213a84c7fdc</a></span></span></span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Furthermore, we compare the user understanding with results obtained with purely textual but adaptive explanations generated by the SNAPE model <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>.
We perform a between-subject online study with 50 participants in each condition. We ensure an even split between male and female participants. The study language is German. Each participant is paid €12 / hour. After excluding outliers based on completion time, the analysis has a power between 0.79 and 0.8 for a Whitney-U-Test. The user perception of the interaction and the objective understanding are measured as dependent variables. The hypotheses we test are:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H1</span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p">The user <span id="S5.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">perceives the interaction</span> as more positive in the mixed gesture condition.</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H2</span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p">The user’s <span id="S5.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">understanding</span> is higher in the mixed gesture condition.</p>
</div>
</li>
<li id="S5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H3</span> 
<div id="S5.I1.ix3.p1" class="ltx_para">
<p id="S5.I1.ix3.p1.1" class="ltx_p">The <span id="S5.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">understanding</span> in the mixed gesture condition is higher than in the previous version of the agent.</p>
</div>
</li>
</ul>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Results: Understanding</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The used understanding instrument is identical to the one used in <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>
, allowing for a comparison between the SNAPE model and our multimodal explainer. The understanding instrument tests the effects on the task performance, which is one of the testable effects described in Section <a href="#S2.SS2" title="2.2. Gestures in Virtual Agents ‣ 2. Related Work ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>. Before comparing with the adaptive agent, we compare the understanding of the different gesture conditions. The instrument contains two kinds of questions. The shallow questions test knowledge recall, in which the user has to decide whether a statement is true or false. The deep understanding test probes the user’s ability to transfer the learned knowledge to in-game situations. Participants are asked to choose the best action in a given game situation shown as a picture. The questions are not only distinctive at the level of understanding depth, but also on forms of understanding <cite class="ltx_cite ltx_citemacro_citep">(Buschmeier et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. While the knowledge recall mainly tests the comprehension of the rules, the in-depth questionnaire tests to which extent the user can transfer and apply the learned knowledge: the user’s enabledness. Thus, while most studies focus on the effects gestures have on long-term learning <cite class="ltx_cite ltx_citemacro_citep">(de Wit et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, this study measures the immediate understanding the user has subsequent to the interaction.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Results show, that the understanding is not significantly higher in the mixed condition. The data is not normally distributed (Shapiro Wilk ¡ 0.05 in each condition: general: p = 0.042, shallow: p = 3.094e-06, deep: p = 3.094e-06), so we performed a Mann-Whitney-U test instead of an ANOVA and posthoc t-tests. The Mann-Whitney-U test<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>ns: p ¡= 1.00e+00; *: 1.00e-02 ¡ p ¡= 5.00e-02; **: 1.00e-03 ¡ p ¡= 1.00e-02; ***: 1.00e-04 ¡ p ¡= 1.00e-03; ****: p ¡= 1.00e-04</span></span></span> shows significant differences between the baseline and the iconic condition in the general (U = 1.302e+03, p = 3.310e-02) (Fig.<a href="#S5.F3" title="Figure 3 ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) as well as the deep understanding (U = 1.318e+03, p = 2.277e-02) (Fig.<a href="#S5.F3" title="Figure 3 ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), while the difference is not significant between the conditions in the shallow understanding. To get a better insight into this effect, we compare the conditions for each question using the Mann-Whitney-U test.
In the shallow understanding, only 3 out of 24 questions are influenced by the conditions. <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">UN09 </span>(<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">You can position any pieces on free spaces on the board.</span>, U=1.246e+03, p = 1.758e-02) and <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">UN23</span> (<span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">Once you have finished a row, you have to shout ”Done”.</span>, U = 1.127e+03, p = 1.984e-02) show a significant difference between the beat and the iconic condition, while <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">UN21</span> (<span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">To win, it is important to distribute the pieces randomly on the playing field at the beginning.</span>, U = 9.080e+02, p = 3.248e-02) shows significant difference between iconic and mixed condition. In all three cases, iconic gestures produce significantly worse understanding. In contrast to the small number of significant items in the shallow understanding, 5 out of 8 test items in the deep understanding are significant: In <span id="S5.SS1.p2.1.7" class="ltx_text ltx_font_italic">UN31</span> (U = 1.231e+03, p = 2.958e-02) and <span id="S5.SS1.p2.1.8" class="ltx_text ltx_font_italic">UN32</span> (U = 1.262e+03, p = 2.774e-02) the baseline outperforms the iconic condition, in <span id="S5.SS1.p2.1.9" class="ltx_text ltx_font_italic">UN36</span> the beat condition produces higher deep understanding than the iconic (U = 1.203e+03, p = 4.183e-02) and the mixed condition (U =1.373e+03, p = 4.881e-03), while it only outperforms the mixed condition in <span id="S5.SS1.p2.1.10" class="ltx_text ltx_font_italic">UN37</span> (U = 1.225e+03, p = 4.504e-02). <span id="S5.SS1.p2.1.11" class="ltx_text ltx_font_italic">UN33</span> (U = 8.790e+02, p = 3.027e-02) is the only question, where the mixed condition outperforms the beat condition. When looking at the results for the individual deep understanding questions there is a trend for the baseline and beat condition to perform better than the iconic and the mixed condition<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The plots for understanding of each question can be found in Appendix <a href="#A2" title="Appendix B Deep Understanding ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a></span></span></span>. As shown above, this trend is significant for half of the questions. In sum, the results do not confirm the hypotheses but are rather contrary. Still, they show an interesting effect in this specific study: the understanding – especially the deep enabledness – is decreased by the usage of iconic gestures. Possible reasons for these results will be discussed in Section <a href="#S6" title="6. Conclusions ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Next to comparing the understanding of the different gesture conditions to each other, the understanding will now be compared to the understanding without an embodiment (<span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">baseline23</span>) and an adaptive explanation without an embodiment (<span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">adaptive23</span>). Although the explanations were given as text and the formulations were not identical to the formulations in this study, since they were generated by the SNAPE model <cite class="ltx_cite ltx_citemacro_citep">(Robrecht et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>, we consider this comparison meaningful as it allows to assess the overall explanation quality achieved by the embodied agent in this specific explanation domain.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">When comparing the general understanding of the non-embodied agent to the updated version, all updated conditions do outperform the baseline23 (baseline: U = 2.666e+03 p = 4.096e-08, beat: U = 2.490e+03 p = 1.755e-06, iconic: U = 2.264e+03 p = 5.353e-05, mixed: U = 2.600e+03, p = 1.194e-06), but only the non-iconic conditions can outperform the adaptive condition (baseline: p = 6.304e-04 U = 1.843e+03, beat: p =2.078e-02 U = 2.214e+03).
When looking at the deep understanding, the embodied conditions also perform well in comparison to the non-embodied conditions. All of the embodied conditions generate a significantly better deep understanding (baseline: U = 2.479e+03 p = 7.813e-06, beat: U = 2.302e+03 p = 1.776e-04, iconic: U = 1.971e+03 p = 1.808e-02, mixed: U = 2.288e+03, p = 1.457e-03). However, only the baseline condition can outperform the adaptive condition when it comes to deep understanding (U = 2.275e+03 p = 1.640e-02).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Results: Interaction Quality</h3>

<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/Trust.png" id="S5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Trust</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/likeability.png" id="S5.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Likeability</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/engagement.png" id="S5.F7.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Engagement</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/acceptance.png" id="S5.F7.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Acceptance</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Next to the objective understanding, the users had to answer a questionnaire on their subjective perception of the interaction. This interaction questionnaire is a selection of 14 dimensions taken from the Artificial Social Agents Questionnaire (ASAQ) questionnaire <cite class="ltx_cite ltx_citemacro_citep">(Fitrianie et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, and two additional, more explanation-related dimensions, the subjective understanding and the connection between agent and understanding <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>for the full questionnaire see Appendix <a href="#A1" title="Appendix A Satisfaction Questionnaire ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a></span></span></span>. It addresses social perception and communicative effects. Each of these 16 dimensions was tested with three different statements and were answered by the participant using a 5-point Likert scale.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">When comparing how the different interactions were perceived by the user, the conditions do not show substantial differences (Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2. Results: Interaction Quality ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). We only see a significant difference in 4 out of the 16 dimensions. In the level of trust the users put into the agent, the beat condition significantly outperforms the iconic condition (U = 1.256e+03 p = 4.811e-02) (Fig.<a href="#S5.F7" title="Figure 7 ‣ 5.2. Results: Interaction Quality ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). The user perceives the baseline condition to be more likable than the mixed condition (U = 1.404e+03 p = 3.841e-02) (Fig. <a href="#S5.F7" title="Figure 7 ‣ 5.2. Results: Interaction Quality ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). When it comes to engagement, the beat (U = 7.135e+02 p = 4.379e-03) and the iconic condition (U = 7.650e+02 p = 3.143e-02) are perceived as more engaging than the baseline (Fig. <a href="#S5.F7" title="Figure 7 ‣ 5.2. Results: Interaction Quality ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Also, the acceptance and willingness to use the agent again are higher in the iconic than in the baseline condition (U = 7.810e+02 p = 3.982e-02) (Fig.<a href="#S5.F7" title="Figure 7 ‣ 5.2. Results: Interaction Quality ‣ 5. Evaluation Study ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2406.12544/assets/pictures/satisfaction_radar.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="445" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Radar chart comparing the four gesture conditions</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Discussion</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The results of the present study suggest that, in the context of a board game explanation delivered by an embodied virtual agent, the use of co-speech gestures does not necessarily improve understanding. On the contrary, the non-iconic conditions (baseline and beat) tended to result in better deep enabledness of the participants than the iconic conditions (iconic and mixed). At the same time, we do see an improvement in understanding in comparison to the SNAPE model. The fact that the embodied baseline condition exceeds the previous one (baseline23) by a significant margin can either be explained by differences in explanation quality or by the added embodiment of the agent. The former is possible as SNAPE can only produce one piece of information per utterance, while our model improves explanation quality by generating utterances with a more complex structure and a higher information density, whenever reasonable (Sect. <a href="#S3" title="3. Explanation Model ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We thus take these results as evidence of a positive impact of our model, even if the explanation was not adaptive. The latter is supported by the so-called ”embodiment effect” <cite class="ltx_cite ltx_citemacro_citep">(Mayer and DaPra, <a href="#bib.bib64" title="" class="ltx_ref">2012</a>)</cite> showing better learning of materials presented with an embodied character. In any case, since the difference in understanding is between the current baseline condition and the baseline23 condition, it cannot be explained by the addition of gestures.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">A comparison of perceived interaction quality between the gesture conditions does not reveal many differences. Only four of the sixteen dimensions show a significant difference. While the performed gestures improve user engagement, they decrease the likability of the agent. When looking at the user’s trust, the overall type of gesture seems to have a smaller impact than the fluidity of the performed gesture. An agent using beat gestures is perceived as more trustworthy while performing iconic gestures significantly increases the probability that a user will reuse the agent compared to the baseline condition.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">In summary, hypotheses H1 and H2 must be rejected, while hypothesis H3 can be partially accepted. We see an improvement in perception in some dimensions when either beat or iconic gestures are used, but the combination never outperforms the baseline. We did not find a significantly better understanding of the mixed condition compared to any of the other conditions. In fact, the iconic condition produces a significantly worse understanding than the baseline. The extended agent outperforms the previous version, especially when looking at deep comprehension. However, it should be noted that the mixed and iconic conditions are the only gesture conditions that do not outperform the adaptive condition in general comprehension, and the difference between the adaptive condition and the baseline, beat, and iconic conditions is even greater than the difference between the mixed and adaptive conditions in deep comprehension.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we investigated how gestures affect users’ perception and understanding of explanations given by an embodied agent. Using a modified version of the SNAPE explanation generation model, coupled with a graph-based gesture generation algorithm, we could systematically adjust gesture parameters and measure participants’ comprehension. Our results show only slight differences in user perception and general understanding if confronted with different gesture types. Notably, our results reveal a significant decrease in deep understanding when presented with iconic gestures. Compared to previous research, these results are rather unexpected. Although previous research has shown that gestures can distract people and reduce performance <cite class="ltx_cite ltx_citemacro_citep">(Krämer et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2003</a>)</cite>, most studies suggested that gestures improve the quality of an interaction <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib45" title="" class="ltx_ref">2017b</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2014</a>; van Merriënboer and Sweller, <a href="#bib.bib90" title="" class="ltx_ref">2005</a>; Dargue et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Ping and Goldin-Meadow, <a href="#bib.bib77" title="" class="ltx_ref">2008</a>)</cite>. Looking at these findings, how can the observed effects be explained?</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">One important consideration is the amount of cognitive load that the stimuli place on the participants <cite class="ltx_cite ltx_citemacro_citep">(Castro-Alonso et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2014</a>; Marcus et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2013</a>; Sweller et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">1998</a>)</cite>. In our study, participants get a verbal explanation of a complex board game in a short period of time, which is likely to incur a high cognitive load <cite class="ltx_cite ltx_citemacro_citep">(van Merriënboer and Sweller, <a href="#bib.bib90" title="" class="ltx_ref">2005</a>)</cite> and thus limit participants’ learning. Additionally, the automatically generated explanations may introduce artifacts that increase cognitive load even further <cite class="ltx_cite ltx_citemacro_citep">(Paris et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2000</a>; Johnsen et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2005</a>)</cite>. In this situation, the presentation of iconic gestures that need to be integrated with verbally conveyed content might have additionally taxed cognitive resources. This is in line with <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">cognitive resource theory</span> which assumes a competition between modalities that need to be processed in parallel <cite class="ltx_cite ltx_citemacro_citep">(Wickens et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">1983</a>)</cite> or when input from one modality needs to be transferred to a task that needs another modality. This is supported by the fact that the negative effects of iconic gestures are found in participant’s deep understanding, but not in the general recall questions. We may thus conclude that the combination of semantically meaningful speech and gesture might have overwhelmed participants (as reported by <cite class="ltx_cite ltx_citemacro_citet">Dargue and Sweller (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>) and that the induced cognitive load <cite class="ltx_cite ltx_citemacro_citep">(Woods et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2002</a>; Fan and Lei, <a href="#bib.bib25" title="" class="ltx_ref">2006</a>)</cite> might have prevented participants from internalizing the new information in the given time.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In contrast, shallow understanding and deep enabledness are not hampered when explanations are given along with beat gestures. This observation is in line with the finding that beat gestures are more beneficial in difficult tasks than in simple ones <cite class="ltx_cite ltx_citemacro_citep">(Gluhareva and Prieto, <a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite>, as well as with the view that iconic and metaphoric gestures are not always more beneficial to comprehension than deictic or beat gestures <cite class="ltx_cite ltx_citemacro_citep">(Dargue et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">However, it is not possible to finally answer this question as the present study only examined redundant iconic gestures that re-instantiate information already conveyed verbally, albeit apparently sometimes in ways that may have led to confusion. It is generally acknowledged that gestures can enhance communication by providing additional information <cite class="ltx_cite ltx_citemacro_citep">(Hostetter, <a href="#bib.bib35" title="" class="ltx_ref">2011</a>)</cite>. This aligns with the concept of spreading information across multiple modalities to reduce cognitive load, as discussed above (Sect. <a href="#S2.SS3" title="2.3. Cognitive Load in Multimodal Interaction ‣ 2. Related Work ‣ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). Thus, to gain further insight into the effects of an embodied agent’s synthetic gestures on listeners’ cognitive load, additional studies on redundant, supplementary, and complementary gestures are necessary. Further, there are multiple metrics available for measuring cognitive load <cite class="ltx_cite ltx_citemacro_citep">(Sweller et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2011</a>)</cite>, which should be included in future studies to better delineate these effects.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Another aspect that goes beyond the scope of this paper is the extent to which the presented iconic gestures are familiar to the addressees. <cite class="ltx_cite ltx_citemacro_citet">Dargue and Sweller (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> found that a well-known gesture is easier to interpret and thus puts less load on the user. Whether this effect is found in the present board game explanations and the pre-recorded gestures will be tested in a follow-up study, measuring the familiarity with the used gestures. As for other limitations of our study, we only tested the effects of specific gestures in a specific type of interaction (an explanation) and a single domain (the board game Quarto!). This study hence cannot provide generalizable results and we are aware of studies showing different or even contrary findings. This again stresses the importance of shifting the perspective to representational gestures and learning more about the effects these gestures have or do not have in different kinds of interactions between human users and virtual multimodal agents. In addition to this, the study is only considering the effects of supplementary gestures on understanding and perceived interaction quality. By now, we cannot derive conclusions about the effects of representational gestures in explanations, which will be a research question for a follow-up study.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">In summary, this study suggests that co-speech gestures do not necessarily improve explanations given by an embodied agent. When incorporating gestures, it is thus important to consider when to use which particular type of gesture, with which particular addressee, and at which particular point in the explanation process. The more meaningful and complex a gesture may seem, the more important this is.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aussems and Kita (2019)</span>
<span class="ltx_bibblock">
Suzanne Aussems and Sotaro Kita. 2019.

</span>
<span class="ltx_bibblock">Seeing Iconic Gestures While Encoding Events Facilitates Children’s Memory of These Events.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Child Development</em> 90, 4 (July 2019), 1123–1137.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/cdev.12988" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/cdev.12988</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2006.11477" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2006.11477</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bailenson and Yee (2005)</span>
<span class="ltx_bibblock">
Jeremy N. Bailenson and Nick Yee. 2005.

</span>
<span class="ltx_bibblock">Digital Chameleons: Automatic Assimilation of Nonverbal Gestures in Immersive Virtual Environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Psychological Science</em> 16, 10 (Oct. 2005), 814–819.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1467-9280.2005.01619.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1467-9280.2005.01619.x</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications Inc.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belpaeme et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tony Belpaeme, Paul Vogt, Rianne van den Berghe, Kirsten Bergmann, Tilbe Göksun, Mirjam de Haas, Junko Kanero, James Kennedy, Aylin C. Küntay, Ora Oudgenoeg-Paz, Fotios Papadopoulos, Thorsten Schodde, Josje Verhagen, Christopher D. Wallbridge, Bram Willemsen, Jan de Wit, Vasfiye Geçkin, Laura Hoffmann, Stefan Kopp, Emiel Krahmer, Ezgi Mamus, Jean-Marc Montanier, Cansu Oranç, and Amit Kumar Pandey. 2018.

</span>
<span class="ltx_bibblock">Guidelines for Designing Social Robots as Second Language Tutors.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">International Journal of Social Robotics</em> 10, 3 (June 2018), 325–341.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s12369-018-0467-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s12369-018-0467-6</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann and Kopp (2009)</span>
<span class="ltx_bibblock">
Kirsten Bergmann and Stefan Kopp. 2009.

</span>
<span class="ltx_bibblock">Increasing the Expressiveness of Virtual Agents– Autonomous Generation of Speech and Gesture for Spatial Description Tasks. In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Joint Conference on Autonomous Agents and Multiagent Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Kirsten Bergmann, Stefan Kopp, and Friederike Eyssel. 2010.

</span>
<span class="ltx_bibblock">Individualized Gesturing Outperforms Average Gesturing – Evaluating Gesture Production in Virtual Humans. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Intelligent Virtual Agents</em>, Jan Allbeck, Norman Badler, Timothy Bickmore, Catherine Pelachaud, and Alla Safonova (Eds.). Springer, Berlin, Heidelberg, 104–117.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-642-15892-6_11" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-642-15892-6_11</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann and Macedonia (2013)</span>
<span class="ltx_bibblock">
Kirsten Bergmann and Manuela Macedonia. 2013.

</span>
<span class="ltx_bibblock">A Virtual Agent as Vocabulary Trainer: Iconic Gestures Help to Improve Learners’ Memory Performance. In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Conference on Intelligent Virtual Agents</em>, Ruth Aylett, Brigitte Krenn, Catherine Pelachaud, and Hiroshi Shimodaira (Eds.). Springer, Berlin, Heidelberg, 139–148.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-642-40415-3_12" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-642-40415-3_12</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock">Enriching Word Vectors with Subword Information.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1607.04606" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1607.04606</a>

</span>
<span class="ltx_bibblock">arXiv:1607.04606 [cs].

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bosker and Peeters (2021)</span>
<span class="ltx_bibblock">
Hans Rutger Bosker and David Peeters. 2021.

</span>
<span class="ltx_bibblock">Beat gestures influence which speech sounds you hear.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Royal Society B: Biological Sciences</em> 288, 1943 (Jan. 2021), 20202419.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1098/rspb.2020.2419" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1098/rspb.2020.2419</a>

</span>
<span class="ltx_bibblock">Publisher: Royal Society.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Breckinridge Church et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Ruth Breckinridge Church, Philip Garber, and Kathryn Rogalski. 2007.

</span>
<span class="ltx_bibblock">The role of gesture in memory and social communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Gesture</em> 7, 2 (June 2007), 137–158.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1075/gest.7.2.02bre" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/gest.7.2.02bre</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buschmeier et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, and Britta Wrede. 2023.

</span>
<span class="ltx_bibblock">Forms of Understanding of XAI-Explanations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2311.08760" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2311.08760</a>

</span>
<span class="ltx_bibblock">arXiv:2311.08760 [cs].

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (1999)</span>
<span class="ltx_bibblock">
Justine Cassell, David McNeill, and Karl-Erik McCullough. 1999.

</span>
<span class="ltx_bibblock">Speech-gesture mismatches: Evidence for one underlying representation of linguistic and nonlinguistic information.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Pragmatics &amp; cognition</em> 7, 1 (1999), 1–34.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1075/pc.7.1.03cas" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/pc.7.1.03cas</a>

</span>
<span class="ltx_bibblock">Publisher: John Benjamins.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (1994)</span>
<span class="ltx_bibblock">
Justine Cassell, Catherine Pelachaud, Norman Badler, Mark Steedman, Brett Achorn, Tripp Becket, Brett Douville, Scott Prevost, and Matthew Stone. 1994.

</span>
<span class="ltx_bibblock">Animated conversation: rule-based generation of facial expression, gesture &amp; spoken intonation for multiple conversational agents. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st annual conference on Computer graphics and interactive techniques</em> <em id="bib.bib14.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH ’94)</em>. Association for Computing Machinery, New York, NY, USA, 413–420.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/192161.192272" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/192161.192272</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Justine Cassell, Hannes Högni Vilhjálmsson, and Timothy Bickmore. 2004.

</span>
<span class="ltx_bibblock">BEAT: the Behavior Expression Animation Toolkit.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Life-Like Characters: Tools, Affective Functions, and Applications</em>, Helmut Prendinger and Mitsuru Ishizuka (Eds.). Springer, Berlin, Heidelberg, 163–185.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-662-08373-4_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-662-08373-4_8</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castellano (2024)</span>
<span class="ltx_bibblock">
Brandon Castellano. 2024.

</span>
<span class="ltx_bibblock">Home - PySceneDetect.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.scenedetect.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.scenedetect.com/</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro-Alonso et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Juan C. Castro-Alonso, Paul Ayres, and Fred Paas. 2014.

</span>
<span class="ltx_bibblock">Learning from observing hands in static and animated versions of non-manipulative tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Learning and Instruction</em> 34 (Dec. 2014), 11–21.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.learninstruc.2014.07.005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.learninstruc.2014.07.005</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Fang Chen, Natalie Ruiz, Eric Choi, Julien Epps, M. Asif Khawaja, Ronnie Taib, Bo Yin, and Yang Wang. 2012.

</span>
<span class="ltx_bibblock">Multimodal behavior and interaction as indicators of cognitive load.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Interactive Intelligent Systems</em> 2, 4 (Dec. 2012), 1–36.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2395123.2395127" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2395123.2395127</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherti et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2022.

</span>
<span class="ltx_bibblock">Reproducible scaling laws for contrastive language-image learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2212.07143" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2212.07143</a>

</span>
<span class="ltx_bibblock">arXiv:2212.07143 [cs].

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Community (2018)</span>
<span class="ltx_bibblock">
Blender Online Community. 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Blender - a 3D modelling and rendering package</em>.

</span>
<span class="ltx_bibblock">Blender Foundation, Stichting Blender Foundation, Amsterdam.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://www.blender.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.blender.org</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dargue and Sweller (2018)</span>
<span class="ltx_bibblock">
Nicole Dargue and Naomi Sweller. 2018.

</span>
<span class="ltx_bibblock">Not All Gestures are Created Equal: The Effects of Typical and Atypical Iconic Gestures on Narrative Comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Nonverbal Behavior</em> 42 (Sept. 2018), 1–19.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s10919-018-0278-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10919-018-0278-3</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dargue et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nicole Dargue, Naomi Sweller, and Michael Jones. 2019.

</span>
<span class="ltx_bibblock">When Our Hands Help Us Understand: A Meta-Analysis Into the Effects of Gesture on Comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Psychological Bulletin</em> 145 (June 2019).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1037/bul0000202" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/bul0000202</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis (2018)</span>
<span class="ltx_bibblock">
Robert O. Davis. 2018.

</span>
<span class="ltx_bibblock">The impact of pedagogical agent gesturing in multimedia learning environments: A meta-analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Educational Research Review</em> (2018).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/J.EDUREV.2018.05.002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/J.EDUREV.2018.05.002</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Wit et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jan de Wit, Thorsten Schodde, Bram Willemsen, Kirsten Bergmann, Mirjam de Haas, Stefan Kopp, Emiel Krahmer, and Paul Vogt. 2018.

</span>
<span class="ltx_bibblock">The Effect of a Robot’s Gestures and Adaptive Tutoring on Children’s Acquisition of Second Language Vocabularies. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction</em> <em id="bib.bib24.4.2" class="ltx_emph ltx_font_italic">(HRI ’18)</em>. Association for Computing Machinery, New York, NY, USA, 50–58.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3171221.3171277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3171221.3171277</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan and Lei (2006)</span>
<span class="ltx_bibblock">
Lisa Fan and Minxiao Lei. 2006.

</span>
<span class="ltx_bibblock">Reducing Cognitive Overload by Meta-Learning Assisted Algorithm Selection. In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2006 5th IEEE International Conference on Cognitive Informatics</em>, Vol. 1. 120–125.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/COGINF.2006.365686" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/COGINF.2006.365686</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fisher et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Josephine B Fisher, Amelie S Robrecht, Stefan Kopp, and Katharina J Rohlfing. 2023.

</span>
<span class="ltx_bibblock">Exploring the Semantic Dialogue Patterns of Explanations – a Case Study of Game Explanations. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th Workshop on the Semantics and Pragmatics of Dialogue (SemDial 2023)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fitrianie et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Siska Fitrianie, Merijn Bruijnes, Fengxiang Li, Amal Abdulrahman, and Willem-Paul Brinkman. 2022.

</span>
<span class="ltx_bibblock">The artificial-social-agent questionnaire: establishing the long and short questionnaire versions. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents</em>. ACM, Faro Portugal, 1–8.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3514197.3549612" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3514197.3549612</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">foundation (2024a)</span>
<span class="ltx_bibblock">
TED foundation. 2024a.

</span>
<span class="ltx_bibblock">TED - YouTube.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.youtube.com/channel/UCAuUUnT6oDeKwE6v1NGQxug" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/channel/UCAuUUnT6oDeKwE6v1NGQxug</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">foundation (2024b)</span>
<span class="ltx_bibblock">
TED foundation. 2024b.

</span>
<span class="ltx_bibblock">TEDx Talks - YouTube.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.youtube.com/user/tedxtalks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/user/tedxtalks</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis,
Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. 2023.

</span>
<span class="ltx_bibblock">DataComp: In search of the next generation of multimodal datasets.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.14108" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.14108</a>

</span>
<span class="ltx_bibblock">arXiv:2304.14108 [cs].

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gluhareva and Prieto (2016)</span>
<span class="ltx_bibblock">
Daria Gluhareva and Pilar Prieto. 2016.

</span>
<span class="ltx_bibblock">Training with rhythmic beat gestures benefits L2 pronunciation in discourse-demanding situations.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Language Teaching Research</em> 21 (June 2016).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/1362168816651463" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/1362168816651463</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldin-Meadow et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Susan Goldin-Meadow, Susan Wagner Cook, and Zachary A. Mitchell. 2009.

</span>
<span class="ltx_bibblock">Gesturing Gives Children New Ideas About Math.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Psychological Science</em> 20, 3 (March 2009), 267–272.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1467-9280.2009.02297.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1467-9280.2009.02297.x</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gratch et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Jonathan Gratch, Ning Wang, Jillian Gerten, Edward Fast, and Robin Duffy. 2007.

</span>
<span class="ltx_bibblock">Creating Rapport with Virtual Agents. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Prodeedings of the 7th International Conference in Intelligent Virtual Agents</em>, Catherine Pelachaud, Jean-Claude Martin, Elisabeth André, Gérard Chollet, Kostas Karpouzis, and Danielle Pelé (Eds.). Springer, Berlin, Heidelberg, 125–138.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_12" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_12</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Group (2024)</span>
<span class="ltx_bibblock">
PostgreSQL Global Development Group. 2024.

</span>
<span class="ltx_bibblock">PostgreSQL.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.postgresql.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.postgresql.org/</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hostetter (2011)</span>
<span class="ltx_bibblock">
Autumn B. Hostetter. 2011.

</span>
<span class="ltx_bibblock">When do gestures communicate? A meta-analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Psychological Bulletin</em> 137, 2 (March 2011), 297–315.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/a0022128" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/a0022128</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hostetter and Bahl (2023)</span>
<span class="ltx_bibblock">
Autumn B. Hostetter and Sonal Bahl. 2023.

</span>
<span class="ltx_bibblock">Comparing the cognitive load of gesture and action production: a dual-task study.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Language and Cognition</em> 15, 3 (Sept. 2023), 601–621.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1017/langcog.2023.23" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/langcog.2023.23</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnsen et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
K. Johnsen, R. Dickerson, A. Raij, B. Lok, J. Jackson, Min Shin, J. Hernandez, A. Stevens, and D.S. Lind. 2005.

</span>
<span class="ltx_bibblock">Experiences in using immersive virtual characters to educate medical communication skills. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">IEEE Proceedings. VR 2005. Virtual Reality, 2005.</em> 179–186.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/VR.2005.1492772" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/VR.2005.1492772</a>

</span>
<span class="ltx_bibblock">ISSN: 2375-5334.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnstone (1994)</span>
<span class="ltx_bibblock">
Barbara Johnstone (Ed.). 1994.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Repetition in discourse: interdisciplinary perspectives</em>.

</span>
<span class="ltx_bibblock">Number v.47-48 in Advances in discourse processes. Ablex Pub. Co, Norwood, N.J.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016.

</span>
<span class="ltx_bibblock">Bag of Tricks for Efficient Text Classification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1607.01759" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1607.01759</a>

</span>
<span class="ltx_bibblock">arXiv:1607.01759 [cs].

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kane (2024)</span>
<span class="ltx_bibblock">
Andrew Kane. 2024.

</span>
<span class="ltx_bibblock">pgvector/pgvector.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://github.com/pgvector/pgvector" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pgvector/pgvector</a>

</span>
<span class="ltx_bibblock">original-date: 2021-04-20T21:13:52Z.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kelly et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
S. Kelly, Corinne Kravitz, and Michael Hopkins. 2004.

</span>
<span class="ltx_bibblock">Neural correlates of bimodal speech and gesture comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Brain and Language</em> 89 (2004), 253–260.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/S0093-934X(03)00335-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0093-934X(03)00335-3</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2022)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Max Welling. 2022.

</span>
<span class="ltx_bibblock">Auto-Encoding Variational Bayes.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1312.6114" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1312.6114</a>

</span>
<span class="ltx_bibblock">arXiv:1312.6114 [cs, stat].

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kita and Özyürek (2003)</span>
<span class="ltx_bibblock">
Sotaro Kita and Asli Özyürek. 2003.

</span>
<span class="ltx_bibblock">What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Journal of Memory and Language</em> 48, 1 (Jan. 2003), 16–32.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/S0749-596X(02)00505-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0749-596X(02)00505-3</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp (2017a)</span>
<span class="ltx_bibblock">
Stefan Kopp. 2017a.

</span>
<span class="ltx_bibblock">Chapter 12. Computational gesture research: Studying the functions of gesture in human-agent interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Gesture Studies</em>, R. Breckinridge Church, Martha W. Alibali, and Spencer D. Kelly (Eds.). Vol. 7. John Benjamins Publishing Company, Amsterdam, 267–284.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1075/gs.7.13kop" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/gs.7.13kop</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp (2017b)</span>
<span class="ltx_bibblock">
Stefan Kopp. 2017b.

</span>
<span class="ltx_bibblock">Computational gesture research: Studying the functions of gesture in human-agent interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Why Gesture?</em> John Benjamins, 267–284.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.jbe-platform.com/content/books/9789027265777-gs.7.13kop" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.jbe-platform.com/content/books/9789027265777-gs.7.13kop</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Stefan Kopp, Brigitte Krenn, Stacy Marsella, Andrew Marshall, Catherine Pelachaud, Hannes Pirker, Kristinn Thórisson, and Hannes Vilhjálmsson. 2006.

</span>
<span class="ltx_bibblock">Towards a Common Framework for Multimodal Generation: The Behavior Markup Language, Vol. 4133. 205–217.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/11821830_17" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/11821830_17</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krieglstein et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Felix Krieglstein, Maik Beege, Günter Daniel Rey, Paul Ginns, Moritz Krell, and Sascha Schneider. 2022.

</span>
<span class="ltx_bibblock">A Systematic Meta-analysis of the Reliability and Validity of Subjective Cognitive Load Questionnaires in Experimental Multimedia Learning Research.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 34, 4 (Dec. 2022), 2485–2541.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10648-022-09683-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10648-022-09683-4</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krämer et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Nicole Krämer, Nina Simons, and Stefan Kopp. 2007.

</span>
<span class="ltx_bibblock">The Effects of an Embodied Conversational Agent’s Nonverbal Behavior on User’s Evaluation and Behavioral Mimicry. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th International Conference on Intelligent Virtual Agents</em>. 238–251.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_22" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_22</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krämer et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2003)</span>
<span class="ltx_bibblock">
Nicole C. Krämer, Bernd Tietz, and Gary Bente. 2003.

</span>
<span class="ltx_bibblock">Effects of Embodied Interface Agents and Their Gestural Activity. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Intelligent Virtual Agents</em>, Thomas Rist, Ruth S. Aylett, Daniel Ballin, and Jeff Rickel (Eds.). Springer, Berlin, Heidelberg, 292–300.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-39396-2_49" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-39396-2_49</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kucherenko et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Taras Kucherenko, Rajmund Nagy, Youngwoo Yoon, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter. 2023.

</span>
<span class="ltx_bibblock">The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2308.12646" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.12646</a>

</span>
<span class="ltx_bibblock">arXiv:2308.12646 [cs].

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurokawa (1992)</span>
<span class="ltx_bibblock">
Takao Kurokawa. 1992.

</span>
<span class="ltx_bibblock">Gesture Coding and a Gesture Dictionary for a Nonverbal Interface.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences</em> E75-A, 2 (Feb. 1992), 112–121.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://search.ieice.org/bin/summary.php?id=e75-a_2_112&amp;category=A&amp;year=1992&amp;lang=E&amp;abst=" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://search.ieice.org/bin/summary.php?id=e75-a_2_112&amp;category=A&amp;year=1992&amp;lang=E&amp;abst=</a>

</span>
<span class="ltx_bibblock">Publisher: The Institute of Electronics, Information and Communication Engineers.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. 2023.

</span>
<span class="ltx_bibblock">One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer. In <em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 21159–21168.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. 2022.

</span>
<span class="ltx_bibblock">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.05297</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kris Liu, Jackson Tolins, Jean E. Fox Tree, Michael Neff, and Marilyn A. Walker. 2016.

</span>
<span class="ltx_bibblock">Two Techniques for Assessing Virtual Agent Personality.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 7, 1 (Jan. 2016), 94–105.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TAFFC.2015.2435780" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TAFFC.2015.2435780</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Liu, Gelareh Mohammadi, Yang Song, and Wafa Johal. 2021.

</span>
<span class="ltx_bibblock">Speech-based Gesture Generation for Robots and Embodied Agents: A Scoping Review. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th International Conference on Human-Agent Interaction</em> <em id="bib.bib55.4.2" class="ltx_emph ltx_font_italic">(HAI ’21)</em>. Association for Computing Machinery, New York, NY, USA, 31–38.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3472307.3484167" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472307.3484167</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lund (2007)</span>
<span class="ltx_bibblock">
Kristine Lund. 2007.

</span>
<span class="ltx_bibblock">The importance of gaze and gesture in interactive multimodal explanation.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em> 41, 3-4 (Dec. 2007), 289–303.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10579-007-9058-0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10579-007-9058-0</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lücking et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Andy Lücking, Kirsten Bergmann, Florian Hahn, Stefan Kopp, and Hannes Rieser. 2010.

</span>
<span class="ltx_bibblock">The Bielefeld Speech and Gesture Alignment Corpus (SaGA).

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">LREC 2010 Workshop: Multimodal Corpora–Advances in Capturing, Coding and Analyzing Multimodality</em> (2010).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://pub.uni-bielefeld.de/record/2001935" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pub.uni-bielefeld.de/record/2001935</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens van der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock">Visualizing Data using t-SNE.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em> 9, 86 (2008), 2579–2605.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://jmlr.org/papers/v9/vandermaaten08a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v9/vandermaaten08a.html</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macedonia (2014)</span>
<span class="ltx_bibblock">
Manuela Macedonia. 2014.

</span>
<span class="ltx_bibblock">Imitation of a Pedagogical Agent’s Gestures Enhances Memory for Words in Second Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Science Journal of Education</em> 2, 5 (2014), 162.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.11648/j.sjedu.20140205.15" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.11648/j.sjedu.20140205.15</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magid and Pyers (2017)</span>
<span class="ltx_bibblock">
Rachel W. Magid and Jennie E. Pyers. 2017.

</span>
<span class="ltx_bibblock">“I use it when I see it”: The role of development and experience in Deaf and hearing children’s understanding of iconic gesture.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Cognition</em> 162 (May 2017), 73–86.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.cognition.2017.01.015" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.cognition.2017.01.015</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2018)</span>
<span class="ltx_bibblock">
Yu A. Malkov and D. A. Yashunin. 2018.

</span>
<span class="ltx_bibblock">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1603.09320" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1603.09320</a>

</span>
<span class="ltx_bibblock">arXiv:1603.09320 [cs].

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Nadine Marcus, Bejay Cleary, Anna Wong, and Paul Ayres. 2013.

</span>
<span class="ltx_bibblock">Should hand actions be observed when learning hand motor skills from instructional animations?

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 29, 6 (Nov. 2013), 2172–2178.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.chb.2013.04.035" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.chb.2013.04.035</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matthews-Saugstad et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Krista M. Matthews-Saugstad, Erik P. Raymakers, and Damian G. Kelty-Stephen. 2017.

</span>
<span class="ltx_bibblock">Gesturing more diminishes recall of abstract words when gesture is allowed and concrete words when it is taboo.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">Quarterly Journal of Experimental Psychology</em> 70, 7 (July 2017), 1099–1105.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/17470218.2016.1263997" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/17470218.2016.1263997</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer and DaPra (2012)</span>
<span class="ltx_bibblock">
Richard E. Mayer and C. Scott DaPra. 2012.

</span>
<span class="ltx_bibblock">An embodiment effect in computer-based learning with animated pedagogical agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Experimental Psychology: Applied</em> 18, 3 (2012), 239–252.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/a0028616" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/a0028616</a>

</span>
<span class="ltx_bibblock">Place: US Publisher: American Psychological Association.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer and Moreno (1998)</span>
<span class="ltx_bibblock">
Richard E. Mayer and Roxana Moreno. 1998.

</span>
<span class="ltx_bibblock">A split-attention effect in multimedia learning: Evidence for dual processing systems in working memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Journal of Educational Psychology</em> 90, 2 (June 1998), 312–320.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-0663.90.2.312" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-0663.90.2.312</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McInnes et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Leland McInnes, John Healy, and James Melville. 2020.

</span>
<span class="ltx_bibblock">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1802.03426" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1802.03426</a>

</span>
<span class="ltx_bibblock">arXiv:1802.03426 [cs, stat].

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McNeill (1985)</span>
<span class="ltx_bibblock">
David McNeill. 1985.

</span>
<span class="ltx_bibblock">So you think gestures are nonverbal?

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Psychological Review</em> 92, 3 (1985), 350–371.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0033-295X.92.3.350" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0033-295X.92.3.350</a>

</span>
<span class="ltx_bibblock">Place: US Publisher: American Psychological Association.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mcneill (1986)</span>
<span class="ltx_bibblock">
David Mcneill. 1986.

</span>
<span class="ltx_bibblock">Iconic gestures of children and adults.

</span>
<span class="ltx_bibblock">62, 1-2 (Jan. 1986), 107–128.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1515/semi.1986.62.1-2.107" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1515/semi.1986.62.1-2.107</a>

</span>
<span class="ltx_bibblock">Publisher: De Gruyter Mouton Section: Semiotica.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavi et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (1995)</span>
<span class="ltx_bibblock">
Seyed Yaghoub Mousavi, Renae Low, and John Sweller. 1995.

</span>
<span class="ltx_bibblock">Reducing cognitive load by mixing auditory and visual presentation modes.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Journal of Educational Psychology</em> 87, 2 (June 1995), 319–334.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-0663.87.2.319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-0663.87.2.319</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neff et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Michael Neff, Yingying Wang, Rob Abbott, and Marilyn Walker. 2010.

</span>
<span class="ltx_bibblock">Evaluating the effect of gesture and language on personality perception in conversational agents. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Intelligent Virtual Agents: 10th International Conference, IVA 2010, Philadelphia, PA, USA, September 20-22, 2010. Proceedings 10</em>. Springer, 222–235.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nyatsanga et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Simbarashe Nyatsanga, Taras Kucherenko, Chaitanya Ahuja, Gustav Eje Henter, and Michael Neff. 2023.

</span>
<span class="ltx_bibblock">A Comprehensive Review of Data-Driven Co-Speech Gesture Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2301.05339" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2301.05339</a>

</span>
<span class="ltx_bibblock">arXiv:2301.05339 [cs].

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oviatt et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Sharon Oviatt, Rachel Coulston, and Rebecca Lunsford. 2024.

</span>
<span class="ltx_bibblock">When Do We Interact Multimodally? Cognitive Load and Multimodal Communication Patterns. In <em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th International Conference on Multimodal Interfaces</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paris et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
Carol R. Paris, Margaret H. Thomas, Richard D. Gilson, and J. Peter Kincaid. 2000.

</span>
<span class="ltx_bibblock">Linguistic Cues and Memory for Synthetic and Natural Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">Human Factors</em> 42, 3 (Sept. 2000), 421–431.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1518/001872000779698132" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1518/001872000779698132</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications Inc.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parise et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (1999)</span>
<span class="ltx_bibblock">
Salvatore Parise, S. Kiesler, L. Sproull, and Keith Waters. 1999.

</span>
<span class="ltx_bibblock">Cooperating with life-like interface agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 15 (1999), 123–142.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/S0747-5632(98)00035-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0747-5632(98)00035-1</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perry et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (1988)</span>
<span class="ltx_bibblock">
Michelle Perry, R. Breckinridge Church, and Susan Goldin-Meadow. 1988.

</span>
<span class="ltx_bibblock">Transitional knowledge in the acquisition of concepts.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Cognitive Development</em> 3, 4 (Oct. 1988), 359–400.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0885-2014(88)90021-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0885-2014(88)90021-4</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping and Goldin-Meadow (2010)</span>
<span class="ltx_bibblock">
Raedy Ping and Susan Goldin-Meadow. 2010.

</span>
<span class="ltx_bibblock">Gesturing Saves Cognitive Resources When Talking About Nonpresent Objects.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Cognitive Science</em> 34, 4 (2010), 602–619.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1551-6709.2010.01102.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1551-6709.2010.01102.x</a>

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2010.01102.x.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping and Goldin-Meadow (2008)</span>
<span class="ltx_bibblock">
Raedy M. Ping and Susan Goldin-Meadow. 2008.

</span>
<span class="ltx_bibblock">Hands in the air: using ungrounded iconic gestures to teach children conservation of quantity.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Developmental Psychology</em> 44, 5 (Sept. 2008), 1277–1287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0012-1649.44.5.1277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0012-1649.44.5.1277</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2103.00020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2103.00020</a>

</span>
<span class="ltx_bibblock">arXiv:2103.00020 [cs].

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robrecht and Kopp (2023)</span>
<span class="ltx_bibblock">
Amelie Robrecht and Stefan Kopp. 2023.

</span>
<span class="ltx_bibblock">SNAPE: A Sequential Non-Stationary Decision Process Model for Adaptive Explanation Generation:. In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th International Conference on Agents and Artificial Intelligence</em>. SCITEPRESS - Science and Technology Publications, Lisbon, Portugal, 48–58.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.5220/0011671300003393" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5220/0011671300003393</a>

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robrecht et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Amelie Sophie Robrecht, Markus Rothgänger, and Stefan Kopp. 2023.

</span>
<span class="ltx_bibblock">A Study on the Benefits and Drawbacks ofAdaptivity in AI-generated Explanations. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3570945.3607339" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3570945.3607339</a>

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohlfing et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Katharina J. Rohlfing, Philipp Cimiano, Ingrid Scharlau, Tobias Matzner, Heike M. Buhl, Hendrik Buschmeier, Elena Esposito, Angela Grimminger, Barbara Hammer, Reinhold Hab-Umbach, Ilona Horwath, Eyke Hüllermeier, Friederike Kern, Stefan Kopp, Kirsten Thommes, Axel Cyrille Ngonga Ngomo, Carsten Schulte, Henning Wachsmuth, Petra Wagner, and Britta Wrede. 2021.

</span>
<span class="ltx_bibblock">Explanation as a Social Practice: Toward a Conceptual Framework for the Social Design of AI Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive and Developmental Systems</em> 13, 3 (Sept. 2021), 717–728.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TCDS.2020.3044366" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TCDS.2020.3044366</a>

</span>
<span class="ltx_bibblock">Publisher: Institute of Electrical and Electronics Engineers Inc..

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohlfing et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Katharina J. Rohlfing, Jannik Fritsch, Britta Wrede, and Tanja Jungmann. 2006.

</span>
<span class="ltx_bibblock">How can multimodal cues from child-directed interaction reduce learning complexity in robots?

</span>
<span class="ltx_bibblock"><em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Advanced Robotics</em> 20, 10 (Jan. 2006), 1183–1199.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1163/156855306778522532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1163/156855306778522532</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal-von der Pütten and Bergmann (2020)</span>
<span class="ltx_bibblock">
Astrid M. Rosenthal-von der Pütten and Kirsten Bergmann. 2020.

</span>
<span class="ltx_bibblock">Non-verbal Enrichment in Vocabulary Learning With a Virtual Pedagogical Agent.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Frontiers in Psychology</em> 11 (Nov. 2020).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/fpsyg.2020.533839" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/fpsyg.2020.533839</a>

</span>
<span class="ltx_bibblock">Publisher: Frontiers.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salem et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Maha Salem, Stefan Kopp, Ipke Wachsmuth, Katharina Rohlfing, and Frank Joublin. 2012.

</span>
<span class="ltx_bibblock">Generation and Evaluation of Communicative Robot Gesture.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">International Journal of Social Robotics</em> 4, 2 (April 2012), 201–217.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s12369-011-0124-9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s12369-011-0124-9</a>

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinatra et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Anne M. Sinatra, Kimberly A. Pollard, Benjamin T. Files, Ashley H. Oiknine, Mark Ericson, and Peter Khooshabeh. 2021.

</span>
<span class="ltx_bibblock">Social fidelity in virtual agents: Impacts on presence and learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 114 (Jan. 2021), 106562.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.chb.2020.106562" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.chb.2020.106562</a>

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sweller et al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
John Sweller, Paul Ayres, and Slava Kalyuga. 2011.

</span>
<span class="ltx_bibblock">Measuring Cognitive Load.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">Cognitive Load Theory</em>, John Sweller, Paul Ayres, and Slava Kalyuga (Eds.). Springer, New York, NY, 71–85.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-1-4419-8126-4_6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-1-4419-8126-4_6</a>

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sweller et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (1998)</span>
<span class="ltx_bibblock">
John Sweller, Jeroen J. G. van Merrienboer, and Fred G. W. C. Paas. 1998.

</span>
<span class="ltx_bibblock">Cognitive Architecture and Instructional Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 10, 3 (Sept. 1998), 251–296.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1023/A:1022193728205" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1023/A:1022193728205</a>

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tindall-Ford et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (1997)</span>
<span class="ltx_bibblock">
Sharon Tindall-Ford, Paul Chandler, and John Sweller. 1997.

</span>
<span class="ltx_bibblock">When two sensory modes are better than one.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">Journal of Experimental Psychology: Applied</em> 3, 4 (Dec. 1997), 257–287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/1076-898X.3.4.257" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/1076-898X.3.4.257</a>

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2307.09288</a>

</span>
<span class="ltx_bibblock">arXiv:2307.09288 [cs].

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Merriënboer and Sweller (2005)</span>
<span class="ltx_bibblock">
Jeroen J. G. van Merriënboer and John Sweller. 2005.

</span>
<span class="ltx_bibblock">Cognitive Load Theory and Complex Learning: Recent Developments and Future Directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 17, 2 (June 2005), 147–177.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10648-005-3951-0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10648-005-3951-0</a>

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilhjalmsson et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
H. Vilhjalmsson, N. Cantelmo, J. Cassell, N. E. Chafai, M. Kipp, and Stefan Kopp. 2007.

</span>
<span class="ltx_bibblock">The Behavior Markup Language: Recent Developments and Challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">Proc. of Intelligent Virtual Agents (IVA 2007)</em> 4722 (2007).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_10" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_10</a>

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voß and Kopp (2023a)</span>
<span class="ltx_bibblock">
Hendric Voß and Stefan Kopp. 2023a.

</span>
<span class="ltx_bibblock">AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th International Conference on Multimodal Interaction</em> <em id="bib.bib92.2.2" class="ltx_emph ltx_font_italic">(ICMI ’23)</em>. Association for Computing Machinery, New York, NY, USA, 60–69.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3577190.3614135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3577190.3614135</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voß and Kopp (2023b)</span>
<span class="ltx_bibblock">
Hendric Voß and Stefan Kopp. 2023b.

</span>
<span class="ltx_bibblock">Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis. In <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3570945.3607337" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3570945.3607337</a>

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Petra Wagner, Zofia Malisz, and Stefan Kopp. 2014.

</span>
<span class="ltx_bibblock">Gesture and speech in interaction: An overview.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Pages: 209–232 Publication Title: Speech Communication Volume: 57.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2002.10957" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2002.10957</a>

</span>
<span class="ltx_bibblock">arXiv:2002.10957 [cs].

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. 2021.

</span>
<span class="ltx_bibblock">Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em> 22, 201 (2021), 1–73.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://jmlr.org/papers/v22/20-1061.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v22/20-1061.html</a>

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wickens et al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (1983)</span>
<span class="ltx_bibblock">
Christopher D. Wickens, Diane L. Sandry, and Michael Vidulich. 1983.

</span>
<span class="ltx_bibblock">Compatibility and Resource Competition between Modalities of Input, Central Processing, and Output.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">Human Factors: The Journal of the Human Factors and Ergonomics Society</em> 25, 2 (April 1983), 227–248.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/001872088302500209" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/001872088302500209</a>

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woods et al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
David Woods, Emily Patterson, and Emilie Roth. 2002.

</span>
<span class="ltx_bibblock">Can We Ever Escape from Data Overload? A Cognitive Systems Diagnosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">Cognition, Technology &amp; Work</em> 4 (April 2002), 22–36.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s101110200002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s101110200002</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Yanxiang Wu, Sabarish V. Babu, Rowan Armstrong, Jeffrey W. Bertrand, Jun Luo, Tania Roy, Shaundra B. Daily, Lauren Cairco Dukes, Larry F. Hodges, and Tracy Fasolino. 2014.

</span>
<span class="ltx_bibblock">Effects of virtual human animation on emotion contagion in simulated inter-personal experiences.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on visualization and computer graphics</em> 20, 4 (April 2014), 626–635.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TVCG.2014.19" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TVCG.2014.19</a>

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter. 2022.

</span>
<span class="ltx_bibblock">The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation. In <em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 International Conference on Multimodal Interaction</em>. 736–747.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3536221.3558058" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558058</a>

</span>
<span class="ltx_bibblock">arXiv:2208.10441 [cs, eess].

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, and Shuwu Zhang. 2023.

</span>
<span class="ltx_bibblock">Gesture Motion Graphs for Few-Shot Speech-Driven Gesture Reenactment.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=CMivR3x5fpC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=CMivR3x5fpC</a>

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chi Zhou, Tengyue Bian, and Kang Chen. 2022.

</span>
<span class="ltx_bibblock">GestureMaster: Graph-based Speech-driven Gesture Generation. In <em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION</em>. ACM, Bengaluru India, 764–770.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3536221.3558063" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558063</a>

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zvaigzne et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Meghan Zvaigzne, Yuriko Oshima-Takane, and Makiko Hirakawa. 2019.

</span>
<span class="ltx_bibblock">How does language proficiency affect children’s iconic gesture use?

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">Applied Psycholinguistics</em> 40, 2 (March 2019), 555–583.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1017/S014271641800070X" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/S014271641800070X</a>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Satisfaction Questionnaire</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The satisfaction questionnaire is a selection of questions taken from the Artificial Social Agents Questionnaire <cite class="ltx_cite ltx_citemacro_citep">(Fitrianie et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, which are translated to German by the authors. Here we will list all original questions used, the test item they are connected to and the translation. In the study the questions were mixed in random order.</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Agent’s Believability</span></p>
<ul id="A1.I1.i1.I1" class="ltx_itemize">
<li id="A1.I1.i1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">HLB3</span> 
<div id="A1.I1.i1.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten erinnert an menschliches Verhalten.
<br class="ltx_break"></span>The agent’s behavior makes me think of human behavior</p>
</div>
</li>
<li id="A1.I1.i1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">HLB4</span> 
<div id="A1.I1.i1.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent verhält sich wie eine echte Person.
<br class="ltx_break"></span>The agent behaves like a real person</p>
</div>
</li>
<li id="A1.I1.i1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">NB2</span> 
<div id="A1.I1.i1.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent agiert natürlich.
<br class="ltx_break"></span>The agent acts naturally</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Performance</span></p>
<ul id="A1.I1.i2.I1" class="ltx_itemize">
<li id="A1.I1.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF1</span> 
<div id="A1.I1.i2.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent macht seine Aufgabe gut.
<br class="ltx_break"></span>The agent does its task well</p>
</div>
</li>
<li id="A1.I1.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF2</span> 
<div id="A1.I1.i2.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent stört mich nicht.
<br class="ltx_break"></span>The agent does not hinder me.</p>
</div>
</li>
<li id="A1.I1.i2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF3</span> 
<div id="A1.I1.i2.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten bin ich in der Lage zu gewinnen.
<br class="ltx_break"></span>I am capable of suceeding with the agent.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Likeability</span></p>
<ul id="A1.I1.i3.I1" class="ltx_itemize">
<li id="A1.I1.i3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL2</span> 
<div id="A1.I1.i3.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich mag den Agenten.
<br class="ltx_break"></span>I like the agent</p>
</div>
</li>
<li id="A1.I1.i3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL3</span> 
<div id="A1.I1.i3.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich mag den Agenten nicht.
<br class="ltx_break"></span>I dislike the agent</p>
</div>
</li>
<li id="A1.I1.i3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL4</span> 
<div id="A1.I1.i3.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist kooperativ.
<br class="ltx_break"></span>The agent is cooperative</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">User Acceptance of the Agent</span></p>
<ul id="A1.I1.i4.I1" class="ltx_itemize">
<li id="A1.I1.i4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA1</span> 
<div id="A1.I1.i4.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich würde den Agenten in Zukunft wieder nutzen.
<br class="ltx_break"></span>I will use the agent again in the future</p>
</div>
</li>
<li id="A1.I1.i4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA2</span> 
<div id="A1.I1.i4.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich kann mich den Agenten zukünftig nutzen sehen.
<br class="ltx_break"></span>I can see myself using the agent in the future</p>
</div>
</li>
<li id="A1.I1.i4.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA3</span> 
<div id="A1.I1.i4.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich vermeide weitere Interaktionen mit dem Agenten.
<br class="ltx_break"></span>I oppose further interaction with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Enjoyability</span></p>
<ul id="A1.I1.i5.I1" class="ltx_itemize">
<li id="A1.I1.i5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE1</span> 
<div id="A1.I1.i5.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist langweilig.
<br class="ltx_break"></span>The agent is boring</p>
</div>
</li>
<li id="A1.I1.i5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE2</span> 
<div id="A1.I1.i5.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Es ist interessant mit dem Agenten zu interagieren.
<br class="ltx_break"></span>It is interesting to interact with the agent</p>
</div>
</li>
<li id="A1.I1.i5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE3</span> 
<div id="A1.I1.i5.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe Spaß mit dem Agenten zu interagieren.
<br class="ltx_break"></span>I enjoy interacting with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p"><span id="A1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Engagement</span></p>
<ul id="A1.I1.i6.I1" class="ltx_itemize">
<li id="A1.I1.i6.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE1</span> 
<div id="A1.I1.i6.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Während der Interaktion mit dem Agenten war ich konzentriert.
<br class="ltx_break"></span>I was concentrated during the interaction with the agent</p>
</div>
</li>
<li id="A1.I1.i6.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE2</span> 
<div id="A1.I1.i6.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Die Interaktion hat meine Aufmerksamkeit erregt.
<br class="ltx_break"></span>The interaction captured my attention</p>
</div>
</li>
<li id="A1.I1.i6.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE3</span> 
<div id="A1.I1.i6.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Während der Interaktion war ich aufmerksam.
<br class="ltx_break"></span>I was alert during the interaction with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(7)</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p"><span id="A1.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Trust</span></p>
<ul id="A1.I1.i7.I1" class="ltx_itemize">
<li id="A1.I1.i7.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT1</span> 
<div id="A1.I1.i7.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent gibt gute Hinweise.
<br class="ltx_break"></span>The agent always gives good advice</p>
</div>
</li>
<li id="A1.I1.i7.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT2</span> 
<div id="A1.I1.i7.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent sagt die Wahrheit.
<br class="ltx_break"></span>The agent acts truthfully</p>
</div>
</li>
<li id="A1.I1.i7.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT3</span> 
<div id="A1.I1.i7.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich kann mich auf den Agenten verlassen.
<br class="ltx_break"></span>I can rely on the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(8)</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p id="A1.I1.i8.p1.1" class="ltx_p"><span id="A1.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">User-Agent Alliance</span></p>
<ul id="A1.I1.i8.I1" class="ltx_itemize">
<li id="A1.I1.i8.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL2</span> 
<div id="A1.I1.i8.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten zusammenzuarbeiten ist wie ein gemeinsames Projekt.
<br class="ltx_break"></span>Collaborating with the agent is like a joint venture</p>
</div>
</li>
<li id="A1.I1.i8.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL4</span> 
<div id="A1.I1.i8.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten kann ich produktiv zusammenarbeiten.
<br class="ltx_break"></span>The agent can collaborate in a productive way</p>
</div>
</li>
<li id="A1.I1.i8.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL6</span> 
<div id="A1.I1.i8.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent versteht mich. 
<br class="ltx_break"></span>The agent understands me</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(9)</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p id="A1.I1.i9.p1.1" class="ltx_p"><span id="A1.I1.i9.p1.1.1" class="ltx_text ltx_font_bold">Attentiveness</span></p>
<ul id="A1.I1.i9.I1" class="ltx_itemize">
<li id="A1.I1.i9.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA1</span> 
<div id="A1.I1.i9.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist während der gesamten Interaktion auf mich konzentriert.
<br class="ltx_break"></span>The agent remains focused on me throughout the interaction</p>
</div>
</li>
<li id="A1.I1.i9.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA2</span> 
<div id="A1.I1.i9.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist aufmerksam.
<br class="ltx_break"></span>The agent is attentive</p>
</div>
</li>
<li id="A1.I1.i9.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA3</span> 
<div id="A1.I1.i9.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich bekomme die gesamte Aufmersamkeit des Agenten.
<br class="ltx_break"></span>I receive the agent’s full attention throughout the interaction</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(10)</span> 
<div id="A1.I1.i10.p1" class="ltx_para">
<p id="A1.I1.i10.p1.1" class="ltx_p"><span id="A1.I1.i10.p1.1.1" class="ltx_text ltx_font_bold">Coherence</span></p>
<ul id="A1.I1.i10.I1" class="ltx_itemize">
<li id="A1.I1.i10.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC1</span> 
<div id="A1.I1.i10.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten macht keinen Sinn.
<br class="ltx_break"></span>The agent’s behavior does not make sense</p>
</div>
</li>
<li id="A1.I1.i10.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC3</span> 
<div id="A1.I1.i10.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten ist inkonsistent.
<br class="ltx_break"></span>The agent is inconsistent</p>
</div>
</li>
<li id="A1.I1.i10.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC4</span> 
<div id="A1.I1.i10.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent wirkt verwirrt.
<br class="ltx_break"></span>The agent appears confused</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(11)</span> 
<div id="A1.I1.i11.p1" class="ltx_para">
<p id="A1.I1.i11.p1.1" class="ltx_p"><span id="A1.I1.i11.p1.1.1" class="ltx_text ltx_font_bold">Intentionality</span></p>
<ul id="A1.I1.i11.I1" class="ltx_itemize">
<li id="A1.I1.i11.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI1</span> 
<div id="A1.I1.i11.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent agiert intentional.
<br class="ltx_break"></span>The agent acts intentionally</p>
</div>
</li>
<li id="A1.I1.i11.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI3</span> 
<div id="A1.I1.i11.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat keine Ahnung was er tut.
<br class="ltx_break"></span>The agent has no clue of what it is doing</p>
</div>
</li>
<li id="A1.I1.i11.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI4</span> 
<div id="A1.I1.i11.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent kann eigene Entscheidungen treffen.
<br class="ltx_break"></span>The agent can make its own decision</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(12)</span> 
<div id="A1.I1.i12.p1" class="ltx_para">
<p id="A1.I1.i12.p1.1" class="ltx_p"><span id="A1.I1.i12.p1.1.1" class="ltx_text ltx_font_bold">Social Presence</span></p>
<ul id="A1.I1.i12.I1" class="ltx_itemize">
<li id="A1.I1.i12.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP1</span> 
<div id="A1.I1.i12.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat eine soziale Präsenz.
<br class="ltx_break"></span>The agent has a social presence</p>
</div>
</li>
<li id="A1.I1.i12.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP2</span> 
<div id="A1.I1.i12.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist eine soziale Entität.
<br class="ltx_break"></span>The agent is a social entity</p>
</div>
</li>
<li id="A1.I1.i12.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP3</span> 
<div id="A1.I1.i12.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die gleiche soziale Präsenz wie der Agent
<br class="ltx_break"></span>I have the same social presence as the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(13)</span> 
<div id="A1.I1.i13.p1" class="ltx_para">
<p id="A1.I1.i13.p1.1" class="ltx_p"><span id="A1.I1.i13.p1.1.1" class="ltx_text ltx_font_bold">Agent’s Emotional Presence</span></p>
<ul id="A1.I1.i13.I1" class="ltx_itemize">
<li id="A1.I1.i13.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP1</span> 
<div id="A1.I1.i13.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist emotional.
<br class="ltx_break"></span>The agent is emotional</p>
</div>
</li>
<li id="A1.I1.i13.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP2</span> 
<div id="A1.I1.i13.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat Emotionen. 
<br class="ltx_break"></span>The agent experiences emotions</p>
</div>
</li>
<li id="A1.I1.i13.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP3</span> 
<div id="A1.I1.i13.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent kann keine Emotionen erleben.
<br class="ltx_break"></span>The agent cannot experience emotions</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(14)</span> 
<div id="A1.I1.i14.p1" class="ltx_para">
<p id="A1.I1.i14.p1.1" class="ltx_p"><span id="A1.I1.i14.p1.1.1" class="ltx_text ltx_font_bold">User’s Emotion</span></p>
<ul id="A1.I1.i14.I1" class="ltx_itemize">
<li id="A1.I1.i14.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP1</span> 
<div id="A1.I1.i14.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Auftreten des Agenten hat beeinflusst wie ich mich fühle.
<br class="ltx_break"></span>The agent’s attitude influences how I feel</p>
</div>
</li>
<li id="A1.I1.i14.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP2</span> 
<div id="A1.I1.i14.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich bin durch die Stimmung des Agenten beeinflusst.
<br class="ltx_break"></span>I am influenced by the agent’s moods</p>
</div>
</li>
<li id="A1.I1.i14.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP3</span> 
<div id="A1.I1.i14.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Die Emotionen, die ich während der Interaktion erlebe sind vom Agenten ausgelöst.
<br class="ltx_break"></span>The emotions I feel during the interaction are caused by the agent</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
<p id="A1.p1.2" class="ltx_p">The following questions were added by the authors and focus more on the explanation itself:</p>
<ol id="A1.I2" class="ltx_enumerate">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p"><span id="A1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">subjective understanding</span></p>
<ul id="A1.I2.i1.I1" class="ltx_itemize">
<li id="A1.I2.i1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU1</span> 
<div id="A1.I2.i1.I1.ix1.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix1.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die Erklärung gut verstanden.
<br class="ltx_break"></span>I understood the explanation well.</p>
</div>
</li>
<li id="A1.I2.i1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU2</span> 
<div id="A1.I2.i1.I1.ix2.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix2.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich bin nun in der Lage Quarto zu spielen.
<br class="ltx_break"></span>I am now enabled to play Quarto.</p>
</div>
</li>
<li id="A1.I2.i1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU3</span> 
<div id="A1.I2.i1.I1.ix3.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix3.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die Regeln des Spieles noch nicht verstanden.
<br class="ltx_break"></span>I do not understand the rules of the game by now.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p"><span id="A1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Agent and Understanding</span></p>
<ul id="A1.I2.i2.I1" class="ltx_itemize">
<li id="A1.I2.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU1</span> 
<div id="A1.I2.i2.I1.ix1.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix1.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Durch den Agenten habe ich die Erklärung besser verstanden.
<br class="ltx_break"></span>Because of the agent, I understood the explanation better.</p>
</div>
</li>
<li id="A1.I2.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU2</span> 
<div id="A1.I2.i2.I1.ix2.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix2.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ohne den Agenten hätte ich die Erklärung besser verstehen können.
<br class="ltx_break"></span>It would have been easier to understand without the agent.</p>
</div>
</li>
<li id="A1.I2.i2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU3</span> 
<div id="A1.I2.i2.I1.ix3.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix3.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Durch den Agenten konnte ich der Erklärung besser folgen.
<br class="ltx_break"></span>Because of the agent, it was easier to follow the explanation.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Deep Understanding</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The following box plot diagrams show the understanding scores for each of the eight deep understanding items distributed by the four conditions.</p>
</div>
<figure id="A2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN30.png" id="A2.F17.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>UN30</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN31.png" id="A2.F17.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>UN31</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN32.png" id="A2.F17.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>UN32</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN33.png" id="A2.F17.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>UN33</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN34.png" id="A2.F17.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>UN34</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN35.png" id="A2.F17.6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>UN35</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.7" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN36.png" id="A2.F17.7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>UN36</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.8" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN37.png" id="A2.F17.8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16. </span>UN37</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17. </span>Comparison between the four conditions for each item in the deep understanding questionnaire</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.12543" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.12544" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.12544">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.12544" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.12545" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:13:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
