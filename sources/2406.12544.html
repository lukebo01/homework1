<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.12544] Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality</title><meta property="og:description" content="In human interaction, gestures serve various functions such as marking speech rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.12544">

<!--Generated on Fri Jul  5 19:13:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Multimodal Interaction,  Explanation,  Understanding,  Gesture,  Study">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amelie Sophie Robrecht
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5622-8248" title="ORCID identifier" class="ltx_ref">0000-0001-5622-8248</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_streetaddress">UniversitÃ¤tsstraÃŸe 25</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hendric Voss
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0003-3646-7702" title="ORCID identifier" class="ltx_ref">0009-0003-3646-7702</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_streetaddress">UniversitÃ¤tsstraÃŸe 25</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lisa Gottschalk
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0007-8084-4439" title="ORCID identifier" class="ltx_ref">0009-0007-8084-4439</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_streetaddress">UniversitÃ¤tsstraÃŸe 25</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Kopp
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4047-9277" title="ORCID identifier" class="ltx_ref">0000-0002-4047-9277</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Social Cognitive Systems Group</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_institution">Bielefeld University</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_streetaddress">UniversitÃ¤tsstraÃŸe 25</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id17.id1" class="ltx_p">In human interaction, gestures serve various functions such as marking speech rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impact of gestures on explanations provided by virtual agents remains underexplored. A user study was carried out to investigate how different types of gestures influence perceived interaction quality and listener understanding. This study addresses the effect of gestures in explanation by developing an embodied virtual explainer integrating both beat gestures and iconic gestures to enhance its automatically generated verbal explanations. Our model combines beat gestures generated by a learned speech-driven synthesis module with manually captured iconic gestures, supporting the agentâ€™s verbal expressions about the board game Quarto! as an explanation scenario.
Findings indicate that neither the use of iconic gestures alone nor their combination with beat gestures outperforms the baseline or beat-only conditions in terms of understanding. Nonetheless, compared to prior research, the embodied agent significantly enhances understanding.</p>
</div>
<div class="ltx_keywords">Multimodal Interaction, Explanation, Understanding, Gesture, Study
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>; ; </span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Intelligent agents</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Human-centered computingÂ User studies</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Human-centered computingÂ Multimodal interaction</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure"><img src="/html/2406.12544/assets/pictures/gesture_header_smaller.png" id="S0.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Examples of iconic gestures used by the virtual agent in the explanation of the board game <span id="S0.F1.2.1" class="ltx_text ltx_font_italic">Quarto!</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Successful human communication involves multiple forms of modalities, including spoken language, facial cues, and body language. Understanding and generating these multimodal cues allows us to have meaningful and nuanced interactions in our everyday life <cite class="ltx_cite ltx_citemacro_citep">(Cassell etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">1999</a>; Wagner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>)</cite>.
Current research on the social aspects of gestures is mainly focused on the effect gestures have on collaboration or interpretation tasks, with a clear focus on emerging gesture comprehension in children and young adults <cite class="ltx_cite ltx_citemacro_citep">(Mcneill, <a href="#bib.bib68" title="" class="ltx_ref">1986</a>; Aussems and Kita, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Zvaigzne etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2019</a>; Magid and Pyers, <a href="#bib.bib60" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The importance of non-verbal communication for a successful collaborative or co-constructive explanation is long known <cite class="ltx_cite ltx_citemacro_citep">(McNeill, <a href="#bib.bib67" title="" class="ltx_ref">1985</a>; Lund, <a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite>. Thus there is little research on how the explaineeâ€™s understanding in an explanation is influenced by the performed gestures, especially with regard to studies that vary gesture parameters and quantitatively measure learning outcomes. As there is a lack of studies that vary gesture parameters, particularly on a fine scale, and quantitatively measure learning outcomes <cite class="ltx_cite ltx_citemacro_citep">(Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>; Sinatra etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2021</a>)</cite>, we provide such a study.
In addition, most insights into the effects gestures have on an explanation are from human-human interaction <cite class="ltx_cite ltx_citemacro_citep">(Hostetter, <a href="#bib.bib35" title="" class="ltx_ref">2011</a>)</cite> or focus on the human explainee expressing (mis)understanding <cite class="ltx_cite ltx_citemacro_citep">(Lund, <a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite>, while it is still unclear how far these effects can be transferred to human-agent explanations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Regarding the automatic generation of non-verbal behavior, the main focus on co-speech gesture generation currently lies in producing natural and human-like gestural motion from multiple input modalities <cite class="ltx_cite ltx_citemacro_citep">(Nyatsanga etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Although these gestures look natural, they still convey only very limited additional non-verbal meaning <cite class="ltx_cite ltx_citemacro_citep">(VoÃŸ and Kopp, <a href="#bib.bib93" title="" class="ltx_ref">2023b</a>)</cite>, and thus do not allow for the interaction quality achievable with virtual agents that employ iconic gestures in human-agent scenarios <cite class="ltx_cite ltx_citemacro_citep">(Bergmann etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we investigate the effects of different types of gestures on the objective understanding and the perceived interaction quality of a multimodal explanation. Our focus lies on studying the understanding and engagement of participants by distinguishing between shallow understanding and deep enabling <cite class="ltx_cite ltx_citemacro_citep">(Buschmeier etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. Therefore, understanding can be divided into comprehension â€“ the knowing that â€“ and enabledness â€“ the knowing how. Both forms of understanding can either appear in a shallow â€“ only surface knowledge â€“ or a deep â€“ being able to draw connections between information â€“ way.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To this end, we developed a multimodal virtual agent designed to explain the mechanics of the board game Quarto!. It is based on a novel model that complements the automatic generation of spoken explanations (from prior work) by generating four different kinds of gestures (baseline, beat, iconic, and mixed). An iconic gesture (illustrated in Figure <a href="#S0.F1" title="Figure 1 â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) conveys semantic information by presenting a depiction of the related aspects <cite class="ltx_cite ltx_citemacro_citep">(Bergmann and Kopp, <a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite>, while beat gestures are biphasic movements of the hand and do not carry any propositional content <cite class="ltx_cite ltx_citemacro_citep">(Bosker and Peeters, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. In our current model, none of the generated gestures introduces new information to the explainee but instead augment the information already conveyed by speech.
In the following, Sect.Â <a href="#S2" title="2. Related Work â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides relevant background about multimodality and gestures in human-human and human-agent interaction. Sect.Â <a href="#S3" title="3. Explanation Model â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the model for explanation generation, before Sect.Â <a href="#S4" title="4. Gesture Generation â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> turns to the approach for beat and iconic gesture generation. Finally, a user study is presented (Sect. <a href="#S5" title="5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) and its results are discussed (Sect. <a href="#S6" title="6. Conclusions â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">While the benefit of gestures in human-human interaction is well known <cite class="ltx_cite ltx_citemacro_citep">(BreckinridgeÂ Church etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2007</a>; Wagner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>)</cite>, the results in human-agent interaction are more ambiguous <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib44" title="" class="ltx_ref">2017a</a>)</cite>. At present, little is known about the effect of the explainerâ€™s gestures on the explaineeâ€™s understanding in human-agent explanation scenarios.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Gestures in Human-Human Explanation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">An explanation can be seen as a co-constructive process, giving both interlocutors â€“ the explainer and the explainee â€“ an active role in the interaction <cite class="ltx_cite ltx_citemacro_citep">(Rohlfing etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2021</a>)</cite>. As shown in <cite class="ltx_cite ltx_citemacro_citet">Lund (<a href="#bib.bib56" title="" class="ltx_ref">2007</a>)</cite> gestures in explanations are used for manifold reasons, they can refer to an object, end a verbal utterance, or accompany it to stress its importance. For human-human explanation, it has been shown that it helps the listener to understand the intended meaning and structure if the speaker uses gestures in an interaction <cite class="ltx_cite ltx_citemacro_citep">(BreckinridgeÂ Church etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2007</a>; Kita and Ã–zyÃ¼rek, <a href="#bib.bib43" title="" class="ltx_ref">2003</a>)</cite>. It has also been shown that iconic gestures can support the long-term learning of second language vocabulary in children <cite class="ltx_cite ltx_citemacro_citep">(deÂ Wit etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>; Bergmann and Macedonia, <a href="#bib.bib8" title="" class="ltx_ref">2013</a>; Belpaeme etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>; Rosenthal-vonÂ der PÃ¼tten and Bergmann, <a href="#bib.bib83" title="" class="ltx_ref">2020</a>; Rohlfing etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2006</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">It is a common approach to transfer human-human interaction patterns to human-agent interaction. For instance, there is research on how to transfer the explainerâ€™s understanding of an explaineeâ€™s gestures from human-human to human-agent interaction <cite class="ltx_cite ltx_citemacro_citep">(Rohlfing etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2006</a>)</cite>. Here, the authors introduce a theoretical approach on how agents can interpret posture and gesture based on behavior patterns in parent-child interaction.
So far, little is known about the effects of the explainerâ€™s gestures on the explaineeâ€™s understanding and perception of an explanation in human-agent interaction. While there are implementations of gesturing agents for spatial description tasks <cite class="ltx_cite ltx_citemacro_citep">(Bergmann and Kopp, <a href="#bib.bib6" title="" class="ltx_ref">2009</a>)</cite>, math equations <cite class="ltx_cite ltx_citemacro_citep">(Perry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">1988</a>)</cite>, Piagetian conservation tasks <cite class="ltx_cite ltx_citemacro_citep">(Ping and Goldin-Meadow, <a href="#bib.bib77" title="" class="ltx_ref">2008</a>)</cite>, or word learning in children <cite class="ltx_cite ltx_citemacro_citep">(deÂ Wit etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, to the best of our knowledge, there are currently no artificial explainers using specific gestures as information-conveying tools in their explanation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Gestures in Virtual Agents</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The generation and integration of nonverbal behavior in virtual agent scenarios have been a long-standing problem in virtual agent research <cite class="ltx_cite ltx_citemacro_citep">(Kurokawa, <a href="#bib.bib51" title="" class="ltx_ref">1992</a>; Cassell etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">1994</a>)</cite>.
Early approaches mainly relied on rule-based models, with gesture templates created by hand, such as the Behavior Markup Language (BML) <cite class="ltx_cite ltx_citemacro_citep">(Kopp etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2006</a>; Vilhjalmsson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2007</a>)</cite> and the Behavior Expression Animation Toolkit (BEAT) <cite class="ltx_cite ltx_citemacro_citep">(Cassell etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2004</a>)</cite>, while more recent works primarily use deep-learning, graph-based or hybrid approaches to generate gestures from given input modalities <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021</a>; Nyatsanga etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2022</a>; Zhao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2023</a>; VoÃŸ and Kopp, <a href="#bib.bib92" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Regarding the effect of synthetic gestures on human-agent interaction, much work has been done on the perceived personality of the agent <cite class="ltx_cite ltx_citemacro_citep">(Neff etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2010</a>; Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2016</a>)</cite> or the creation of rapport with the agent <cite class="ltx_cite ltx_citemacro_citep">(Gratch etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2007</a>; Bailenson and Yee, <a href="#bib.bib4" title="" class="ltx_ref">2005</a>)</cite>. It has been shown that users are more willing to engage in a human-like way if the agent uses human-like gestures <cite class="ltx_cite ltx_citemacro_citep">(KrÃ¤mer etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2007</a>; Parise etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">1999</a>)</cite>. At the same time, it has been shown that mismatching gestures have a measurable negative effect on such interactions <cite class="ltx_cite ltx_citemacro_citep">(Salem etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2012</a>; Wagner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>; Kelly etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2004</a>)</cite>.
In general, the main effects found are social perception effects (how is the agent perceived by the user?) and communicative effects (how does gesturing influence the course of interaction?). However, little effects have been found on user understanding and task performance <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib44" title="" class="ltx_ref">2017a</a>; Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>. On the other hand, there are indications that participants benefit from gestures when it comes to learning, but there is not enough data to generalize from it <cite class="ltx_cite ltx_citemacro_citep">(Macedonia, <a href="#bib.bib59" title="" class="ltx_ref">2014</a>; Davis, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>.
Models based on co-construction face difficulty with repetition when users donâ€™t provide sufficient feedback, often due to underestimating the userâ€™s competence <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Cognitive Load in Multimodal Interaction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">There are different possible ways in which gestures can support or hamper the listenerâ€™s processing and understanding of the utterance. We focus here on the cognitive load that gestures may impose on the listener, and which may particularly affect the processing of multimodal behavior of artificial agents. A lot of research has investigated the speakerâ€™s cognitive load and how it can be decreased by using different modalities <cite class="ltx_cite ltx_citemacro_citep">(Oviatt etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2024</a>)</cite>. It has been shown that gestures help to structure an interaction and thereby minimize verbal load <cite class="ltx_cite ltx_citemacro_citep">(Goldin-Meadow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Chen etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2012</a>)</cite> show which features are relevant to measure the current cognitive load and apply their method to different experimental scenarios. <cite class="ltx_cite ltx_citemacro_citet">Oviatt etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib72" title="" class="ltx_ref">2024</a>)</cite> show that as cognitive load rises people tend to go multimodal, presumably to distribute the load over the used modalities. Only a few studies examined the impact of generated input on the listener. The question of how gestures affect the listenerâ€™s cognitive load remains unanswered by them <cite class="ltx_cite ltx_citemacro_citep">(Krieglstein etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Gestures in interaction do influence both, the speaker and listener. Most research focuses on the positive effects gestures have on the person using them.
<cite class="ltx_cite ltx_citemacro_citet">Hostetter and Bahl (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> demonstrate that gesturing and other meaningful hand movements have a beneficial influence on verbal load.
Similarly, research on memory calls showed that prohibiting the use of gestures diminishes the recall rate of memorized words <cite class="ltx_cite ltx_citemacro_citep">(Matthews-Saugstad etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2017</a>)</cite>. It has been shown that gestures also support the human explainer in structuring their explanation <cite class="ltx_cite ltx_citemacro_citep">(Goldin-Meadow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2009</a>)</cite>. When a non-existent object is described with gestures instead of speech, research has shown that the cognitive load reduces <cite class="ltx_cite ltx_citemacro_citep">(Ping and Goldin-Meadow, <a href="#bib.bib76" title="" class="ltx_ref">2010</a>)</cite>. There is less research on how gestures are perceived, but studies show that presenting information on different modalities can expand the learnerâ€™s capacity of working memory in learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Mayer and Moreno, <a href="#bib.bib65" title="" class="ltx_ref">1998</a>; Tindall-Ford etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">1997</a>)</cite>. <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Cognitive load theory</span> states that presenting a task using different modalities (e.g. dual-mode presentation for solving geometry tasks) supports the expansion of working memory and helps to solve tasks <cite class="ltx_cite ltx_citemacro_citep">(Mousavi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">1995</a>)</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In contrast to this, <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">cognitive resource theory</span> shows that there can be a competition between modalities when performing a task, as they need to be processed in parallel <cite class="ltx_cite ltx_citemacro_citep">(Wickens etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">1983</a>)</cite>. This approach also considers the cognitive load it takes to transfer input on one modality to a task that needs another modality.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Explanation Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The quality of an explanation is influenced by many aspects, such as adaptivity, multimodality, and information quality. As this paper describes the influence of gestures on understanding and perception of the explanation, all other aspects are kept as static as possible. We adopt a model for explanation generation, called SNAPE <cite class="ltx_cite ltx_citemacro_citep">(Robrecht and Kopp, <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>. This model is capable of generating adaptive explanations, which have been shown to result in better understanding, especially better deep understanding, than a static explanation <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>. This section will give a short overview of the modelâ€™s approach and architecture.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">SNAPE is based on a non-stationary Markov Decision Process (MDP) which evaluates the best action (which information to provide) and move (how to verbalize the information) dependent on the current internal model the agent has about the user. This model is called the Partner Model (PM) and consists of (1) an estimation of the userâ€™s current domain knowledge and (2) different global variables, such as expertise and attentiveness, which are based on the amount and quality of feedback that has been generated by the user so far. As transition probabilities and rewards are based on the PM, the MDP needs to be solved online using Monte Carlo Tree Search (MCTS). To keep the process real-time capable, the state space is kept small, which is grounded in the hierarchical structure of the model. A knowledge graph (KG) containing all necessary information for the explanation is extracted from an ontology, containing all possible information about the domain. Similar to the process observable in human-human explanations <cite class="ltx_cite ltx_citemacro_citep">(Fisher etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, the KG is subdivided into semantic blocks. These blocks form the set of information that the MDP can use for inference. We extended the model by updating the template generation: the templates used for the verbalization of the explanation are now generated by a Large Language Model (LLM). This extension will be discussed in the following section<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For further insights into the architecture, please refer to <cite class="ltx_cite ltx_citemacro_citep">(Robrecht and Kopp, <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>, more information about complexity and preconditions are given in <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite></span></span></span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Using LLMs for utterance generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A key lesson learned from the previous modelâ€™s performance, was that multiple pieces of information should be mergeable into one utterance if the current PM and the complexity of the information allow it.
Hence, we have extended the SNAPE model to combine multiple triples into one utterance under certain conditions: (1) the triples have to be in the set of the five best next pieces of information to provide that is generated by MCTS, (2) they have to have the same linguistic move (provide new information, give additional information, repeat information, make a comparison), and (3) they have to share at least one entity. The vast number and complexity of potential triple combinations require a general and powerful approach to utterance generation. In the current version, all possible triple combinations are generated and matching utterances are created by prompting a fine-tuned Llama2 7B model <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2023</a>)</cite>. The model is fine-tuned on a dataset containing 487 items, each consisting of a list of triples, the move, and a matching output utterance. The fine-tuned model was used to generate multiple alternatives for each possible combination before running the explanation. The pre-generation allows to prevent hallucinations and minimizes the required computing power to keep the model real-time capable.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Examples of Llama2 generated templates</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Move</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Triple</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Template</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T1.1.2.1.1.1" class="ltx_text">Provide</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p">(Struktur, sein, Figurenmerkmal), (Groesse, sein, Figurenmerkmal)</span>
</span>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.3.1.1" class="ltx_p">Die GrÃ¶ÃŸe und Struktur sind Merkmale der Figuren.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p">(structure, is, figure-feature), (size, is, figure-feature)</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p"><span id="S3.T1.1.3.2.2.1.1.1" class="ltx_text ltx_font_italic">Size and structure are features of the figure.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S3.T1.1.4.3.1.1" class="ltx_text">Repeat</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p">(Struktur, sein, Figurenmerkmal), (Groesse, sein, Figurenmerkmal)</span>
</span>
</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.3.1.1" class="ltx_p">Die GrÃ¶ÃŸe und die Struktur gehÃ¶ren zu den Figurenmerkmalen.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p">(structure, is, figure-feature), (size, is, figure-feature)</span>
</span>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.2.1.1" class="ltx_p"><span id="S3.T1.1.5.4.2.1.1.1" class="ltx_text ltx_font_italic">The size and structure are among the figure features.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S3.T1.1.6.5.1.1" class="ltx_text">Additional</span></th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.2.1.1" class="ltx_p">(Quarto, haben, Spielfiguren), (Spielfiguren, material, Holz)</span>
</span>
</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.3.1.1" class="ltx_p">Die Spielfiguren bei Quarto sind aus Holz.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p">(quarto, has, figures), (figures, material, wood)</span>
</span>
</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.2.1.1" class="ltx_p"><span id="S3.T1.1.7.6.2.1.1.1" class="ltx_text ltx_font_italic">The figures are made of wood.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" rowspan="2"><span id="S3.T1.1.8.7.1.1" class="ltx_text">Compare</span></th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.2.1.1" class="ltx_p">(Spiel, haben, Ziel), (Reihe, sein, Ziel)</span>
</span>
</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_justify">
<span id="S3.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.3.1.1" class="ltx_p">Das Bilden einer Reihe ist das Ziel von Quarto, genau wie bei Vier Gewinnt.</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.1.1.1" class="ltx_p">(game, has, goal), (row, is, goal)</span>
</span>
</td>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S3.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.2.1.1" class="ltx_p"><span id="S3.T1.1.9.8.2.1.1.1" class="ltx_text ltx_font_italic">Forming a line is the aim of Quarto, just like in Best of Four.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">As shown in Table <a href="#S3.T1" title="Table 1 â€£ 3.1. Using LLMs for utterance generation â€£ 3. Explanation Model â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the prompts for template generation not only contain the information in the form of one or two triplets but also the linguistic move that the system is supposed to use to verbalize the information. If SNAPE introduces new information, the move is called <span id="S3.SS1.p2.4.1" class="ltx_text ltx_font_bold">provide</span> information. For this move, information can only be taken from the knowledge graph. Providing information can either work or fail, which depends on the transition probability <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">T</annotation></semantics></math> in the MDP. The transition probability is influenced by the currently inferred level of attentiveness the user has, as a user who gets distracted easily has a higher probability of missing information. If the move succeeds, the level of understanding <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="lou_{i}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.1a" xref="S3.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS1.p2.2.m2.1.1.4" xref="S3.SS1.p2.2.m2.1.1.4.cmml"><mi id="S3.SS1.p2.2.m2.1.1.4.2" xref="S3.SS1.p2.2.m2.1.1.4.2.cmml">u</mi><mi id="S3.SS1.p2.2.m2.1.1.4.3" xref="S3.SS1.p2.2.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ğ‘™</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">ğ‘œ</ci><apply id="S3.SS1.p2.2.m2.1.1.4.cmml" xref="S3.SS1.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.4.1.cmml" xref="S3.SS1.p2.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.4.2.cmml" xref="S3.SS1.p2.2.m2.1.1.4.2">ğ‘¢</ci><ci id="S3.SS1.p2.2.m2.1.1.4.3.cmml" xref="S3.SS1.p2.2.m2.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">lou_{i}</annotation></semantics></math> of this information increases. The strength of growth depends on the currently inferred level of expertise the user has. A <span id="S3.SS1.p2.4.2" class="ltx_text ltx_font_bold">repetition</span> can be verbatim or reformulated, as long as no new information is given <cite class="ltx_cite ltx_citemacro_citep">(Johnstone, <a href="#bib.bib38" title="" class="ltx_ref">1994</a>)</cite>. Only necessary information can be repeated, accordingly the triple that is repeated has to be taken from the knowledge graph. Repetition is the simplest of the three available deepening moves. It has the highest probability of succeeding, but also the lowest increase in the level of understanding <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="lou_{i}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.1a" xref="S3.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><msub id="S3.SS1.p2.3.m3.1.1.4" xref="S3.SS1.p2.3.m3.1.1.4.cmml"><mi id="S3.SS1.p2.3.m3.1.1.4.2" xref="S3.SS1.p2.3.m3.1.1.4.2.cmml">u</mi><mi id="S3.SS1.p2.3.m3.1.1.4.3" xref="S3.SS1.p2.3.m3.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></times><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğ‘™</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ğ‘œ</ci><apply id="S3.SS1.p2.3.m3.1.1.4.cmml" xref="S3.SS1.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.4.1.cmml" xref="S3.SS1.p2.3.m3.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.4.2.cmml" xref="S3.SS1.p2.3.m3.1.1.4.2">ğ‘¢</ci><ci id="S3.SS1.p2.3.m3.1.1.4.3.cmml" xref="S3.SS1.p2.3.m3.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">lou_{i}</annotation></semantics></math>. The move <span id="S3.SS1.p2.4.3" class="ltx_text ltx_font_bold">additional</span> adds information that is not necessary but potentially helpful to an already introduced, but not yet grounded, information. When considering giving additional information to the currently under discussion information <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="cud_{i}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1a" xref="S3.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><msub id="S3.SS1.p2.4.m4.1.1.4" xref="S3.SS1.p2.4.m4.1.1.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.4.2" xref="S3.SS1.p2.4.m4.1.1.4.2.cmml">d</mi><mi id="S3.SS1.p2.4.m4.1.1.4.3" xref="S3.SS1.p2.4.m4.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ğ‘</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">ğ‘¢</ci><apply id="S3.SS1.p2.4.m4.1.1.4.cmml" xref="S3.SS1.p2.4.m4.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.4.2">ğ‘‘</ci><ci id="S3.SS1.p2.4.m4.1.1.4.3.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">cud_{i}</annotation></semantics></math>, the model first needs to check for a fitting triple. A fitting triple is a triple that has to be part of the ontology but does not contain necessary information. Additionally, the triple needs to have the subject or object of the original triple as the subject. If a potential triple does exist, the move is an available action for the next step in the explanation process. A <span id="S3.SS1.p2.4.4" class="ltx_text ltx_font_bold">comparison</span> gives supportive information to an already introduced triple. In this case, the triple is not taken from the Quarto! ontology, but from an ontology of another, comparable board game. Again, the triples have to share at least one entity, but can also be identical. Examples of each move can be seen in Tab.<a href="#S3.T1" title="Table 1 â€£ 3.1. Using LLMs for utterance generation â€£ 3. Explanation Model â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Gesture Generation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In order to augment the explanations produced by the model described above with communicative gestures, we identified three main requirements for gestures to be used in an explanatory setting:
(1) The gestures had to be as human-like as possible and not distract from the given speech. (2) The generated gestures had to be extensible to incorporate additional iconic gestures in a natural and easily modifiable way.
(3) The gesture algorithm should be able to generate gestures in or near real-time, in a format that does not require extensive pre- or post-processing.
To meet these requirements, we examined several different approaches to gesture generation, including but not limited to the models competing in the 2022 and 2023 GENEA challenges <cite class="ltx_cite ltx_citemacro_citep">(Yoon etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>; Kucherenko etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>.
Since all diffusion-based implementations did not meet the real-time requirement and the GAN approaches did not produce satisfactory results in human evaluations, we focused on graph-based implementations. Inspired by the work of both <cite class="ltx_cite ltx_citemacro_citet">Zhou etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib102" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhao etÂ al<span class="ltx_text">.</span> (<a href="#bib.bib101" title="" class="ltx_ref">2023</a>)</cite>, we developed a new graph-based gesture generation algorithm that enables the generation of realistic gestures in a real-time context.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To create our new graph-based gesture generation algorithm, we first needed to get appropriate data that either already had iconic gestures or was usable in a base model on which iconic gestures could be added. Currently, iconic gesture data sets are almost non-existent. To our knowledge, only two annotated data sets for iconic gestures exist. The SaGA data corpus <cite class="ltx_cite ltx_citemacro_citep">(LÃ¼cking etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2010</a>)</cite> with a small set of highly specific annotated data and the BEAT corpus <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> with acted interactions and rough categories for annotations. As we are mainly interested in natural interactions, we opted against the use of either of these corpora and instead captured our own data corpus. For this, we took 32 hours of TED and TEDx recordings with their subtitles <cite class="ltx_cite ltx_citemacro_citep">(foundation, <a href="#bib.bib28" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib29" title="" class="ltx_ref">b</a>)</cite> and tracked the body pose data for all recordings using One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer (OSX) <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. Afterward, we split the videos into individual video clips by detecting and cutting the videos along camera cuts using PySceneDetect <cite class="ltx_cite ltx_citemacro_citep">(Castellano, <a href="#bib.bib16" title="" class="ltx_ref">2024</a>)</cite>. Every clip was removed that did not include the main speaker, exhibited a low confidence rating during tracking, or had no perceivable movement. The final dataset consists of 24.2 hours of primarily beat gesture, text, and audio data.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Gesture Segmentation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.14" class="ltx_p">In contrast to deep learning-based approaches, which generate new gesture data from a trained multimodal data set, graph-based approaches are more closely related to retrieval-based techniques, where the input data is not used as training data, but as lightly processed chunking data in which the algorithm searches for an optimal path to generate new gestures <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2023</a>; Nyatsanga etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Using audio, text, and gesture data as input, we first divided the entire training data into clips of length <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">s</annotation></semantics></math> seconds, with an overlap of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mfrac id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">s</mi><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><divide id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"></divide><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">ğ‘ </ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\frac{s}{2}</annotation></semantics></math> seconds. For our data, we chose a length of 2 for <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">s</annotation></semantics></math>. All clips shorter than <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">s</annotation></semantics></math> seconds were discarded. We then performed individual data processing for each modality. For the text data, we encoded each word with fastText <cite class="ltx_cite ltx_citemacro_citep">(Joulin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2016</a>; Bojanowski etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> and performed vector encoding for both the short historical context with <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="tx_{1}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS2.p1.5.m5.1.1.3.2" xref="S4.SS2.p1.5.m5.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p1.5.m5.1.1.3.3" xref="S4.SS2.p1.5.m5.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><times id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></times><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">ğ‘¡</ci><apply id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS2.p1.5.m5.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">tx_{1}</annotation></semantics></math> words and the long historical context with <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="tx_{2}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p1.6.m6.1.1.3.3" xref="S4.SS2.p1.6.m6.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><times id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></times><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">ğ‘¡</ci><apply id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S4.SS2.p1.6.m6.1.1.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">tx_{2}</annotation></semantics></math> words. We encoded both sequences using a MiniLM model <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2020</a>)</cite> and an openclip embedding model <cite class="ltx_cite ltx_citemacro_citep">(Cherti etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2022</a>; Radford etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2021</a>)</cite> trained on the DataComp1B dataset <cite class="ltx_cite ltx_citemacro_citep">(Gadre etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. As shown in equation <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mn id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><cn type="integer" id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">1</annotation></semantics></math> and <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mn id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><cn type="integer" id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">2</annotation></semantics></math>, given the MiniLM model as <math id="S4.SS2.p1.9.m9.1" class="ltx_Math" alttext="MiLM" display="inline"><semantics id="S4.SS2.p1.9.m9.1a"><mrow id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mi id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1a" xref="S4.SS2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.9.m9.1.1.4" xref="S4.SS2.p1.9.m9.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.9.m9.1.1.1b" xref="S4.SS2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.9.m9.1.1.5" xref="S4.SS2.p1.9.m9.1.1.5.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><times id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1"></times><ci id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">ğ‘€</ci><ci id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3">ğ‘–</ci><ci id="S4.SS2.p1.9.m9.1.1.4.cmml" xref="S4.SS2.p1.9.m9.1.1.4">ğ¿</ci><ci id="S4.SS2.p1.9.m9.1.1.5.cmml" xref="S4.SS2.p1.9.m9.1.1.5">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">MiLM</annotation></semantics></math>, the OpenClip model as <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="Oclip" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><mrow id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml"><mi id="S4.SS2.p1.10.m10.1.1.2" xref="S4.SS2.p1.10.m10.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1" xref="S4.SS2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.10.m10.1.1.3" xref="S4.SS2.p1.10.m10.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1a" xref="S4.SS2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.10.m10.1.1.4" xref="S4.SS2.p1.10.m10.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1b" xref="S4.SS2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.10.m10.1.1.5" xref="S4.SS2.p1.10.m10.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1c" xref="S4.SS2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.10.m10.1.1.6" xref="S4.SS2.p1.10.m10.1.1.6.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1"><times id="S4.SS2.p1.10.m10.1.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1.1"></times><ci id="S4.SS2.p1.10.m10.1.1.2.cmml" xref="S4.SS2.p1.10.m10.1.1.2">ğ‘‚</ci><ci id="S4.SS2.p1.10.m10.1.1.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3">ğ‘</ci><ci id="S4.SS2.p1.10.m10.1.1.4.cmml" xref="S4.SS2.p1.10.m10.1.1.4">ğ‘™</ci><ci id="S4.SS2.p1.10.m10.1.1.5.cmml" xref="S4.SS2.p1.10.m10.1.1.5">ğ‘–</ci><ci id="S4.SS2.p1.10.m10.1.1.6.cmml" xref="S4.SS2.p1.10.m10.1.1.6">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">Oclip</annotation></semantics></math>, the combination of the short and long historical sequence as <math id="S4.SS2.p1.11.m11.1" class="ltx_Math" alttext="vr" display="inline"><semantics id="S4.SS2.p1.11.m11.1a"><mrow id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml"><mi id="S4.SS2.p1.11.m11.1.1.2" xref="S4.SS2.p1.11.m11.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.11.m11.1.1.1" xref="S4.SS2.p1.11.m11.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.11.m11.1.1.3" xref="S4.SS2.p1.11.m11.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><apply id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1"><times id="S4.SS2.p1.11.m11.1.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1.1"></times><ci id="S4.SS2.p1.11.m11.1.1.2.cmml" xref="S4.SS2.p1.11.m11.1.1.2">ğ‘£</ci><ci id="S4.SS2.p1.11.m11.1.1.3.cmml" xref="S4.SS2.p1.11.m11.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">vr</annotation></semantics></math>, the combined sequence as <math id="S4.SS2.p1.12.m12.1" class="ltx_Math" alttext="vt" display="inline"><semantics id="S4.SS2.p1.12.m12.1a"><mrow id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml"><mi id="S4.SS2.p1.12.m12.1.1.2" xref="S4.SS2.p1.12.m12.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.12.m12.1.1.1" xref="S4.SS2.p1.12.m12.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p1.12.m12.1.1.3" xref="S4.SS2.p1.12.m12.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><apply id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1"><times id="S4.SS2.p1.12.m12.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1"></times><ci id="S4.SS2.p1.12.m12.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1.2">ğ‘£</ci><ci id="S4.SS2.p1.12.m12.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">vt</annotation></semantics></math>, and the word at step <math id="S4.SS2.p1.13.m13.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p1.13.m13.1a"><mi id="S4.SS2.p1.13.m13.1.1" xref="S4.SS2.p1.13.m13.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m13.1b"><ci id="S4.SS2.p1.13.m13.1.1.cmml" xref="S4.SS2.p1.13.m13.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m13.1c">i</annotation></semantics></math> as <math id="S4.SS2.p1.14.m14.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.SS2.p1.14.m14.1a"><msub id="S4.SS2.p1.14.m14.1.1" xref="S4.SS2.p1.14.m14.1.1.cmml"><mi id="S4.SS2.p1.14.m14.1.1.2" xref="S4.SS2.p1.14.m14.1.1.2.cmml">w</mi><mi id="S4.SS2.p1.14.m14.1.1.3" xref="S4.SS2.p1.14.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.14.m14.1b"><apply id="S4.SS2.p1.14.m14.1.1.cmml" xref="S4.SS2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.14.m14.1.1.1.cmml" xref="S4.SS2.p1.14.m14.1.1">subscript</csymbol><ci id="S4.SS2.p1.14.m14.1.1.2.cmml" xref="S4.SS2.p1.14.m14.1.1.2">ğ‘¤</ci><ci id="S4.SS2.p1.14.m14.1.1.3.cmml" xref="S4.SS2.p1.14.m14.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.14.m14.1c">w_{i}</annotation></semantics></math>, we formally perform</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\displaystyle vr_{i}" display="inline"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mi id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">r</mi><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><times id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></times><ci id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2">ğ‘£</ci><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle vr_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E1.m2.2" class="ltx_Math" alttext="\displaystyle=\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{1}}w_{i}\oplus\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{2}}w_{i}" display="inline"><semantics id="S4.E1.m2.2a"><mrow id="S4.E1.m2.2.3" xref="S4.E1.m2.2.3.cmml"><mi id="S4.E1.m2.2.3.2" xref="S4.E1.m2.2.3.2.cmml"></mi><mo id="S4.E1.m2.2.3.1" xref="S4.E1.m2.2.3.1.cmml">=</mo><mrow id="S4.E1.m2.2.3.3" xref="S4.E1.m2.2.3.3.cmml"><mrow id="S4.E1.m2.2.3.3.2" xref="S4.E1.m2.2.3.3.2.cmml"><mpadded depth="3.3pt" height="10.0pt" voffset="0.0pt" width="0.0pt" id="S4.E1.m2.1.1a" xref="S4.E1.m2.1.1a.cmml"><mo movablelimits="false" id="S4.E1.m2.1.1aa" xref="S4.E1.m2.1.1a.cmml">âˆ¥</mo></mpadded><mrow id="S4.E1.m2.2.3.3.2.1" xref="S4.E1.m2.2.3.3.2.1.cmml"><msubsup id="S4.E1.m2.2.3.3.2.1.2" xref="S4.E1.m2.2.3.3.2.1.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.2.1.2.2.2" xref="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml"><mtext id="S4.E1.m2.2.3.3.2.1.2.2.2a" xref="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml">\slimits@</mtext></merror><mrow id="S4.E1.m2.2.3.3.2.1.2.2.3" xref="S4.E1.m2.2.3.3.2.1.2.2.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.2.3.2" xref="S4.E1.m2.2.3.3.2.1.2.2.3.2.cmml">k</mi><mo id="S4.E1.m2.2.3.3.2.1.2.2.3.1" xref="S4.E1.m2.2.3.3.2.1.2.2.3.1.cmml">=</mo><mi id="S4.E1.m2.2.3.3.2.1.2.2.3.3" xref="S4.E1.m2.2.3.3.2.1.2.2.3.3.cmml">i</mi></mrow><mrow id="S4.E1.m2.2.3.3.2.1.2.3" xref="S4.E1.m2.2.3.3.2.1.2.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.2.cmml">i</mi><mo id="S4.E1.m2.2.3.3.2.1.2.3.1" xref="S4.E1.m2.2.3.3.2.1.2.3.1.cmml">âˆ’</mo><mrow id="S4.E1.m2.2.3.3.2.1.2.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.2.1.2.3.3.1" xref="S4.E1.m2.2.3.3.2.1.2.3.3.1.cmml">â€‹</mo><msub id="S4.E1.m2.2.3.3.2.1.2.3.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.2.3.3.3.2" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.2.cmml">x</mi><mn id="S4.E1.m2.2.3.3.2.1.2.3.3.3.3" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.3.cmml">1</mn></msub></mrow></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.2.1.1" xref="S4.E1.m2.2.3.3.2.1.1.cmml">â€‹</mo><msub id="S4.E1.m2.2.3.3.2.1.3" xref="S4.E1.m2.2.3.3.2.1.3.cmml"><mi id="S4.E1.m2.2.3.3.2.1.3.2" xref="S4.E1.m2.2.3.3.2.1.3.2.cmml">w</mi><mi id="S4.E1.m2.2.3.3.2.1.3.3" xref="S4.E1.m2.2.3.3.2.1.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S4.E1.m2.2.3.3.1" xref="S4.E1.m2.2.3.3.1.cmml">âŠ•</mo><mrow id="S4.E1.m2.2.3.3.3" xref="S4.E1.m2.2.3.3.3.cmml"><mpadded depth="3.3pt" height="10.0pt" voffset="0.0pt" width="0.0pt" id="S4.E1.m2.2.2a" xref="S4.E1.m2.2.2a.cmml"><mo movablelimits="false" id="S4.E1.m2.2.2aa" xref="S4.E1.m2.2.2a.cmml">âˆ¥</mo></mpadded><mrow id="S4.E1.m2.2.3.3.3.1" xref="S4.E1.m2.2.3.3.3.1.cmml"><msubsup id="S4.E1.m2.2.3.3.3.1.2" xref="S4.E1.m2.2.3.3.3.1.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.3.1.2.2.2" xref="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml"><mtext id="S4.E1.m2.2.3.3.3.1.2.2.2a" xref="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml">\slimits@</mtext></merror><mrow id="S4.E1.m2.2.3.3.3.1.2.2.3" xref="S4.E1.m2.2.3.3.3.1.2.2.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.2.3.2" xref="S4.E1.m2.2.3.3.3.1.2.2.3.2.cmml">k</mi><mo id="S4.E1.m2.2.3.3.3.1.2.2.3.1" xref="S4.E1.m2.2.3.3.3.1.2.2.3.1.cmml">=</mo><mi id="S4.E1.m2.2.3.3.3.1.2.2.3.3" xref="S4.E1.m2.2.3.3.3.1.2.2.3.3.cmml">i</mi></mrow><mrow id="S4.E1.m2.2.3.3.3.1.2.3" xref="S4.E1.m2.2.3.3.3.1.2.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E1.m2.2.3.3.3.1.2.3.1" xref="S4.E1.m2.2.3.3.3.1.2.3.1.cmml">âˆ’</mo><mrow id="S4.E1.m2.2.3.3.3.1.2.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.3.1.2.3.3.1" xref="S4.E1.m2.2.3.3.3.1.2.3.3.1.cmml">â€‹</mo><msub id="S4.E1.m2.2.3.3.3.1.2.3.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.2.3.3.3.2" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.2.cmml">x</mi><mn id="S4.E1.m2.2.3.3.3.1.2.3.3.3.3" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.3.cmml">2</mn></msub></mrow></mrow></msubsup><mo lspace="0em" rspace="0em" id="S4.E1.m2.2.3.3.3.1.1" xref="S4.E1.m2.2.3.3.3.1.1.cmml">â€‹</mo><msub id="S4.E1.m2.2.3.3.3.1.3" xref="S4.E1.m2.2.3.3.3.1.3.cmml"><mi id="S4.E1.m2.2.3.3.3.1.3.2" xref="S4.E1.m2.2.3.3.3.1.3.2.cmml">w</mi><mi id="S4.E1.m2.2.3.3.3.1.3.3" xref="S4.E1.m2.2.3.3.3.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m2.2b"><apply id="S4.E1.m2.2.3.cmml" xref="S4.E1.m2.2.3"><eq id="S4.E1.m2.2.3.1.cmml" xref="S4.E1.m2.2.3.1"></eq><csymbol cd="latexml" id="S4.E1.m2.2.3.2.cmml" xref="S4.E1.m2.2.3.2">absent</csymbol><apply id="S4.E1.m2.2.3.3.cmml" xref="S4.E1.m2.2.3.3"><csymbol cd="latexml" id="S4.E1.m2.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.1">direct-sum</csymbol><apply id="S4.E1.m2.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2"><ci id="S4.E1.m2.1.1a.cmml" xref="S4.E1.m2.1.1a">âˆ¥</ci><apply id="S4.E1.m2.2.3.3.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1"><times id="S4.E1.m2.2.3.3.2.1.1.cmml" xref="S4.E1.m2.2.3.3.2.1.1"></times><apply id="S4.E1.m2.2.3.3.2.1.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2">superscript</csymbol><apply id="S4.E1.m2.2.3.3.2.1.2.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.2.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.2.2.2b.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.2.1.2.2.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2"><mtext id="S4.E1.m2.2.3.3.2.1.2.2.2a.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.2">\slimits@</mtext></merror></ci><apply id="S4.E1.m2.2.3.3.2.1.2.2.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3"><eq id="S4.E1.m2.2.3.3.2.1.2.2.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.1"></eq><ci id="S4.E1.m2.2.3.3.2.1.2.2.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.2">ğ‘˜</ci><ci id="S4.E1.m2.2.3.3.2.1.2.2.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.2.3.3">ğ‘–</ci></apply></apply><apply id="S4.E1.m2.2.3.3.2.1.2.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3"><minus id="S4.E1.m2.2.3.3.2.1.2.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.1"></minus><ci id="S4.E1.m2.2.3.3.2.1.2.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.2">ğ‘–</ci><apply id="S4.E1.m2.2.3.3.2.1.2.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3"><times id="S4.E1.m2.2.3.3.2.1.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.1"></times><ci id="S4.E1.m2.2.3.3.2.1.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.2">ğ‘¡</ci><apply id="S4.E1.m2.2.3.3.2.1.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.2.3.3.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.2">ğ‘¥</ci><cn type="integer" id="S4.E1.m2.2.3.3.2.1.2.3.3.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.2.3.3.3.3">1</cn></apply></apply></apply></apply><apply id="S4.E1.m2.2.3.3.2.1.3.cmml" xref="S4.E1.m2.2.3.3.2.1.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.2.1.3.1.cmml" xref="S4.E1.m2.2.3.3.2.1.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.2.1.3.2.cmml" xref="S4.E1.m2.2.3.3.2.1.3.2">ğ‘¤</ci><ci id="S4.E1.m2.2.3.3.2.1.3.3.cmml" xref="S4.E1.m2.2.3.3.2.1.3.3">ğ‘–</ci></apply></apply></apply><apply id="S4.E1.m2.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3"><ci id="S4.E1.m2.2.2a.cmml" xref="S4.E1.m2.2.2a">âˆ¥</ci><apply id="S4.E1.m2.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1"><times id="S4.E1.m2.2.3.3.3.1.1.cmml" xref="S4.E1.m2.2.3.3.3.1.1"></times><apply id="S4.E1.m2.2.3.3.3.1.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2">superscript</csymbol><apply id="S4.E1.m2.2.3.3.3.1.2.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.2.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.2.2.2b.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S4.E1.m2.2.3.3.3.1.2.2.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2"><mtext id="S4.E1.m2.2.3.3.3.1.2.2.2a.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.2">\slimits@</mtext></merror></ci><apply id="S4.E1.m2.2.3.3.3.1.2.2.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3"><eq id="S4.E1.m2.2.3.3.3.1.2.2.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.1"></eq><ci id="S4.E1.m2.2.3.3.3.1.2.2.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.2">ğ‘˜</ci><ci id="S4.E1.m2.2.3.3.3.1.2.2.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.2.3.3">ğ‘–</ci></apply></apply><apply id="S4.E1.m2.2.3.3.3.1.2.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3"><minus id="S4.E1.m2.2.3.3.3.1.2.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.1"></minus><ci id="S4.E1.m2.2.3.3.3.1.2.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.2">ğ‘–</ci><apply id="S4.E1.m2.2.3.3.3.1.2.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3"><times id="S4.E1.m2.2.3.3.3.1.2.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.1"></times><ci id="S4.E1.m2.2.3.3.3.1.2.3.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.2">ğ‘¡</ci><apply id="S4.E1.m2.2.3.3.3.1.2.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.2.3.3.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.2.3.3.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.2">ğ‘¥</ci><cn type="integer" id="S4.E1.m2.2.3.3.3.1.2.3.3.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.2.3.3.3.3">2</cn></apply></apply></apply></apply><apply id="S4.E1.m2.2.3.3.3.1.3.cmml" xref="S4.E1.m2.2.3.3.3.1.3"><csymbol cd="ambiguous" id="S4.E1.m2.2.3.3.3.1.3.1.cmml" xref="S4.E1.m2.2.3.3.3.1.3">subscript</csymbol><ci id="S4.E1.m2.2.3.3.3.1.3.2.cmml" xref="S4.E1.m2.2.3.3.3.1.3.2">ğ‘¤</ci><ci id="S4.E1.m2.2.3.3.3.1.3.3.cmml" xref="S4.E1.m2.2.3.3.3.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m2.2c">\displaystyle=\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{1}}w_{i}\oplus\mathop{\vphantom{\sum}\mathchoice{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\displaystyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{10.00012pt}{\raisebox{0.0pt}{$\textstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{7.00009pt}{\raisebox{0.0pt}{$\scriptstyle\|$}}}}}{\vbox{\hbox{\leavevmode\resizebox{0.0pt}{5.00006pt}{\raisebox{0.0pt}{$\scriptscriptstyle\|$}}}}}}\slimits@_{k=i}^{i-tx_{2}}w_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle vt_{i}" display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">t</mi><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><times id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></times><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">ğ‘£</ci><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">ğ‘¡</ci><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle vt_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.2" class="ltx_Math" alttext="\displaystyle=MiLM(vr_{i})\oplus Oclip(vr_{i})" display="inline"><semantics id="S4.E2.m2.2a"><mrow id="S4.E2.m2.2.2" xref="S4.E2.m2.2.2.cmml"><mi id="S4.E2.m2.2.2.4" xref="S4.E2.m2.2.2.4.cmml"></mi><mo id="S4.E2.m2.2.2.3" xref="S4.E2.m2.2.2.3.cmml">=</mo><mrow id="S4.E2.m2.2.2.2" xref="S4.E2.m2.2.2.2.cmml"><mrow id="S4.E2.m2.1.1.1.1" xref="S4.E2.m2.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.2.cmml">â€‹</mo><mi id="S4.E2.m2.1.1.1.1.4" xref="S4.E2.m2.1.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2a" xref="S4.E2.m2.1.1.1.1.2.cmml">â€‹</mo><mi id="S4.E2.m2.1.1.1.1.5" xref="S4.E2.m2.1.1.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2b" xref="S4.E2.m2.1.1.1.1.2.cmml">â€‹</mo><mi id="S4.E2.m2.1.1.1.1.6" xref="S4.E2.m2.1.1.1.1.6.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2c" xref="S4.E2.m2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E2.m2.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S4.E2.m2.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.1.1.1.3.2.cmml">r</mi><mi id="S4.E2.m2.1.1.1.1.1.1.1.3.3" xref="S4.E2.m2.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m2.2.2.2.3" xref="S4.E2.m2.2.2.2.3.cmml">âŠ•</mo><mrow id="S4.E2.m2.2.2.2.2" xref="S4.E2.m2.2.2.2.2.cmml"><mi id="S4.E2.m2.2.2.2.2.3" xref="S4.E2.m2.2.2.2.2.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2" xref="S4.E2.m2.2.2.2.2.2.cmml">â€‹</mo><mi id="S4.E2.m2.2.2.2.2.4" xref="S4.E2.m2.2.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2a" xref="S4.E2.m2.2.2.2.2.2.cmml">â€‹</mo><mi id="S4.E2.m2.2.2.2.2.5" xref="S4.E2.m2.2.2.2.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2b" xref="S4.E2.m2.2.2.2.2.2.cmml">â€‹</mo><mi id="S4.E2.m2.2.2.2.2.6" xref="S4.E2.m2.2.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2c" xref="S4.E2.m2.2.2.2.2.2.cmml">â€‹</mo><mi id="S4.E2.m2.2.2.2.2.7" xref="S4.E2.m2.2.2.2.2.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.2d" xref="S4.E2.m2.2.2.2.2.2.cmml">â€‹</mo><mrow id="S4.E2.m2.2.2.2.2.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.2.2.2.2.1.1.2" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.2.2.2.2.1.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml"><mi id="S4.E2.m2.2.2.2.2.1.1.1.2" xref="S4.E2.m2.2.2.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.2.2.2.2.1.1.1.1" xref="S4.E2.m2.2.2.2.2.1.1.1.1.cmml">â€‹</mo><msub id="S4.E2.m2.2.2.2.2.1.1.1.3" xref="S4.E2.m2.2.2.2.2.1.1.1.3.cmml"><mi id="S4.E2.m2.2.2.2.2.1.1.1.3.2" xref="S4.E2.m2.2.2.2.2.1.1.1.3.2.cmml">r</mi><mi id="S4.E2.m2.2.2.2.2.1.1.1.3.3" xref="S4.E2.m2.2.2.2.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E2.m2.2.2.2.2.1.1.3" xref="S4.E2.m2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.2b"><apply id="S4.E2.m2.2.2.cmml" xref="S4.E2.m2.2.2"><eq id="S4.E2.m2.2.2.3.cmml" xref="S4.E2.m2.2.2.3"></eq><csymbol cd="latexml" id="S4.E2.m2.2.2.4.cmml" xref="S4.E2.m2.2.2.4">absent</csymbol><apply id="S4.E2.m2.2.2.2.cmml" xref="S4.E2.m2.2.2.2"><csymbol cd="latexml" id="S4.E2.m2.2.2.2.3.cmml" xref="S4.E2.m2.2.2.2.3">direct-sum</csymbol><apply id="S4.E2.m2.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.2"></times><ci id="S4.E2.m2.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.3">ğ‘€</ci><ci id="S4.E2.m2.1.1.1.1.4.cmml" xref="S4.E2.m2.1.1.1.1.4">ğ‘–</ci><ci id="S4.E2.m2.1.1.1.1.5.cmml" xref="S4.E2.m2.1.1.1.1.5">ğ¿</ci><ci id="S4.E2.m2.1.1.1.1.6.cmml" xref="S4.E2.m2.1.1.1.1.6">ğ‘€</ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1"></times><ci id="S4.E2.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2">ğ‘£</ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.E2.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply><apply id="S4.E2.m2.2.2.2.2.cmml" xref="S4.E2.m2.2.2.2.2"><times id="S4.E2.m2.2.2.2.2.2.cmml" xref="S4.E2.m2.2.2.2.2.2"></times><ci id="S4.E2.m2.2.2.2.2.3.cmml" xref="S4.E2.m2.2.2.2.2.3">ğ‘‚</ci><ci id="S4.E2.m2.2.2.2.2.4.cmml" xref="S4.E2.m2.2.2.2.2.4">ğ‘</ci><ci id="S4.E2.m2.2.2.2.2.5.cmml" xref="S4.E2.m2.2.2.2.2.5">ğ‘™</ci><ci id="S4.E2.m2.2.2.2.2.6.cmml" xref="S4.E2.m2.2.2.2.2.6">ğ‘–</ci><ci id="S4.E2.m2.2.2.2.2.7.cmml" xref="S4.E2.m2.2.2.2.2.7">ğ‘</ci><apply id="S4.E2.m2.2.2.2.2.1.1.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1"><times id="S4.E2.m2.2.2.2.2.1.1.1.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.1"></times><ci id="S4.E2.m2.2.2.2.2.1.1.1.2.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.2">ğ‘£</ci><apply id="S4.E2.m2.2.2.2.2.1.1.1.3.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E2.m2.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.E2.m2.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E2.m2.2.2.2.2.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.2c">\displaystyle=MiLM(vr_{i})\oplus Oclip(vr_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.13" class="ltx_p">which concatenates the MiniLM an OpenClip vectors with the historic context <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="tx_{1}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml"><mi id="S4.SS2.p2.1.m1.1.1.3.2" xref="S4.SS2.p2.1.m1.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p2.1.m1.1.1.3.3" xref="S4.SS2.p2.1.m1.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">ğ‘¡</ci><apply id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.p2.1.m1.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">tx_{1}</annotation></semantics></math> and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="tx_{2}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">x</mi><mn id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">ğ‘¡</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">ğ‘¥</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">tx_{2}</annotation></semantics></math>.
For the audio processing, we resample the audio data to 24.000 Hz, and both compute a log normalized spectrogram with consecutive Fourier transformations on the raw audio data, as well as a vector embedding for the entire sequence <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="va" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ğ‘£</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">va</annotation></semantics></math> using the wav2vec 2.0 model <cite class="ltx_cite ltx_citemacro_citep">(Baevski etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>. To more closely align the audio with the text data and to remove any superfluous information from the audio data, we additionally compute a processed version of the spectrogram and the wav2vec 2.0 vectors. For this, we compute a vector between zero and one, with a starting value of zero and the length of our sequence <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">s</annotation></semantics></math>, multiplied by the given frame rate of the original clip. We call this vector <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="ATs" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1a" xref="S4.SS2.p2.5.m5.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.5.m5.1.1.4" xref="S4.SS2.p2.5.m5.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><times id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></times><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">ğ´</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">ğ‘‡</ci><ci id="S4.SS2.p2.5.m5.1.1.4.cmml" xref="S4.SS2.p2.5.m5.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">ATs</annotation></semantics></math>. We iterate over every word in the sequence <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="Sx" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><times id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></times><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">ğ‘†</ci><ci id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">Sx</annotation></semantics></math> and note the start- and end timings <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="Ws" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mrow id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mi id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.7.m7.1.1.1" xref="S4.SS2.p2.7.m7.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><times id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1.1"></times><ci id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">ğ‘Š</ci><ci id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">Ws</annotation></semantics></math> and <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="We" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mrow id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m8.1.1.1" xref="S4.SS2.p2.8.m8.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><times id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1.1"></times><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">ğ‘Š</ci><ci id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">We</annotation></semantics></math>, respectively.
Using the start- and end timings, we calculate the middle point of the current word timing <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="Wm" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><mrow id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><mi id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m9.1.1.1" xref="S4.SS2.p2.9.m9.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><times id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1.1"></times><ci id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2">ğ‘Š</ci><ci id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">Wm</annotation></semantics></math>. We linearly interpolate from the start point <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="Ws-1" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mrow id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mrow id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml"><mi id="S4.SS2.p2.10.m10.1.1.2.2" xref="S4.SS2.p2.10.m10.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.10.m10.1.1.2.1" xref="S4.SS2.p2.10.m10.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p2.10.m10.1.1.2.3" xref="S4.SS2.p2.10.m10.1.1.2.3.cmml">s</mi></mrow><mo id="S4.SS2.p2.10.m10.1.1.1" xref="S4.SS2.p2.10.m10.1.1.1.cmml">âˆ’</mo><mn id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><minus id="S4.SS2.p2.10.m10.1.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1.1"></minus><apply id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2"><times id="S4.SS2.p2.10.m10.1.1.2.1.cmml" xref="S4.SS2.p2.10.m10.1.1.2.1"></times><ci id="S4.SS2.p2.10.m10.1.1.2.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2.2">ğ‘Š</ci><ci id="S4.SS2.p2.10.m10.1.1.2.3.cmml" xref="S4.SS2.p2.10.m10.1.1.2.3">ğ‘ </ci></apply><cn type="integer" id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">Ws-1</annotation></semantics></math> with the value zero to the middle point <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="Wm" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><mrow id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.1" xref="S4.SS2.p2.11.m11.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><times id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1"></times><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">ğ‘Š</ci><ci id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">Wm</annotation></semantics></math> with the value one, back down to the value zero at time point <math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="We+1" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><mrow id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml"><mrow id="S4.SS2.p2.12.m12.1.1.2" xref="S4.SS2.p2.12.m12.1.1.2.cmml"><mi id="S4.SS2.p2.12.m12.1.1.2.2" xref="S4.SS2.p2.12.m12.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.12.m12.1.1.2.1" xref="S4.SS2.p2.12.m12.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p2.12.m12.1.1.2.3" xref="S4.SS2.p2.12.m12.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS2.p2.12.m12.1.1.1" xref="S4.SS2.p2.12.m12.1.1.1.cmml">+</mo><mn id="S4.SS2.p2.12.m12.1.1.3" xref="S4.SS2.p2.12.m12.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><apply id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1"><plus id="S4.SS2.p2.12.m12.1.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1.1"></plus><apply id="S4.SS2.p2.12.m12.1.1.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2"><times id="S4.SS2.p2.12.m12.1.1.2.1.cmml" xref="S4.SS2.p2.12.m12.1.1.2.1"></times><ci id="S4.SS2.p2.12.m12.1.1.2.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2.2">ğ‘Š</ci><ci id="S4.SS2.p2.12.m12.1.1.2.3.cmml" xref="S4.SS2.p2.12.m12.1.1.2.3">ğ‘’</ci></apply><cn type="integer" id="S4.SS2.p2.12.m12.1.1.3.cmml" xref="S4.SS2.p2.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">We+1</annotation></semantics></math>. If the word has a length of exactly one frame, we simply set this specific frame to one. Then, we multiply the vector <math id="S4.SS2.p2.13.m13.1" class="ltx_Math" alttext="ATs" display="inline"><semantics id="S4.SS2.p2.13.m13.1a"><mrow id="S4.SS2.p2.13.m13.1.1" xref="S4.SS2.p2.13.m13.1.1.cmml"><mi id="S4.SS2.p2.13.m13.1.1.2" xref="S4.SS2.p2.13.m13.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.1" xref="S4.SS2.p2.13.m13.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.13.m13.1.1.3" xref="S4.SS2.p2.13.m13.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m13.1.1.1a" xref="S4.SS2.p2.13.m13.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.13.m13.1.1.4" xref="S4.SS2.p2.13.m13.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.13.m13.1b"><apply id="S4.SS2.p2.13.m13.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1"><times id="S4.SS2.p2.13.m13.1.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1.1"></times><ci id="S4.SS2.p2.13.m13.1.1.2.cmml" xref="S4.SS2.p2.13.m13.1.1.2">ğ´</ci><ci id="S4.SS2.p2.13.m13.1.1.3.cmml" xref="S4.SS2.p2.13.m13.1.1.3">ğ‘‡</ci><ci id="S4.SS2.p2.13.m13.1.1.4.cmml" xref="S4.SS2.p2.13.m13.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.13.m13.1c">ATs</annotation></semantics></math> with the spectrogram and wav2vec 2.0 vectors and save the resulting vectors.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For the gesture data, we kept the original position data unchanged. To achieve better coherence between the gesture generation segments, we calculated an additional vector <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="Gv" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">ğº</ci><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">Gv</annotation></semantics></math> by combining the first through sixth derivatives of the gesture position.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Vector Embedding</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To align each dimension of the gesture, audio, and text data, we first normalized each dimension by subtracting the global mean and dividing it by the global standard deviation. As the gesture generation algorithm relies on matching the overlap of the referencing sequence (see chapter <a href="#S4.SS4" title="4.4. Beat Gesture Generation â€£ 4. Gesture Generation â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>), we split the data into two parts of length <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mfrac id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">s</mi><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><divide id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></divide><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">ğ‘ </ci><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\frac{s}{2}</annotation></semantics></math> along the time axis and only kept the first half of the data.
As the given vectors exhibited a high dimensionality, we performed a Principal Components Analysis (PCA) on each data segment. We deliberately chose this reduction method instead of TSNE, UMAP, VAE, or PaCMAP dimensionality reduction, as all of these methods performed worse in terms of preserving the global structure of our data in our experiments <cite class="ltx_cite ltx_citemacro_citep">(Maaten and Hinton, <a href="#bib.bib58" title="" class="ltx_ref">2008</a>; McInnes etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>; Kingma and Welling, <a href="#bib.bib42" title="" class="ltx_ref">2022</a>; Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>. Using PCA, we reduced each dimension to a vector of length 256. Measuring the cumulative explained variance of the data, we preserved 92.6%, 96.7%, and 56.7% of the variance for the gestureâ€“, audio-, and text vectors, respectively. In order to enable fast nearest neighbor vector searches during the actual gesture generation, we stored the resulting vectors, split by modality and a combined vector of all modalities, in a vector database and computed a Hierarchical Navigable Small World graph (HNSW) over the entire vector sets <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib34" title="" class="ltx_ref">2024</a>; Kane, <a href="#bib.bib40" title="" class="ltx_ref">2024</a>; Malkov and Yashunin, <a href="#bib.bib61" title="" class="ltx_ref">2018</a>)</cite>. Finally, we used the resulting vector database to create our vector graph, by defining every clip as a node in our gesture graph and performing an offline search for each singular modality and the combination of all modalities, storing the nearest 20 vectors as the edges for our graph.
Additionally, we added all edges to the graph that are naturally continuous and removed all edges that would lead to a reversed path (i.e. id:80 -Â¿ id:79) in the graph.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Beat Gesture Generation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.11" class="ltx_p">During the generation of new gestures, we split the given audio and text information into clips of length <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">s</annotation></semantics></math>, with an overlap of <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="\frac{s}{2}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mfrac id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">s</mi><mn id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><divide id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"></divide><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">ğ‘ </ci><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\frac{s}{2}</annotation></semantics></math> seconds. We calculated the vector data for the audio and text data for each segment, but contrary to the graph data, we only kept the second half of the vector data along the time axis. As the generation has no gesture information for the first sequence, we retrieved the gesture for the first sequence from our gesture database, by returning the result of the nearest neighbor search for the combined vector of audio and text. Starting with the second segment, we used the vector database to retrieve <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="Gn" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><times id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></times><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">ğº</ci><ci id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">Gn</annotation></semantics></math> nodes, which minimized the L2 distance of our combined modality vector. For each given node, we calculated the best-performing path up to a depth of <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="Gd" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><times id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"></times><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">ğº</ci><ci id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">Gd</annotation></semantics></math>, by traveling along the top <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="Ge" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mi id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><times id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></times><ci id="S4.SS4.p1.5.m5.1.1.2.cmml" xref="S4.SS4.p1.5.m5.1.1.2">ğº</ci><ci id="S4.SS4.p1.5.m5.1.1.3.cmml" xref="S4.SS4.p1.5.m5.1.1.3">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">Ge</annotation></semantics></math> edges for each node. Given the previous sequence vectors as <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><mi id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><ci id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">g</annotation></semantics></math>, the previous audio sequence as <math id="S4.SS4.p1.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS4.p1.7.m7.1a"><mi id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><ci id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">a</annotation></semantics></math>, the previous text sequence as <math id="S4.SS4.p1.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS4.p1.8.m8.1a"><mi id="S4.SS4.p1.8.m8.1.1" xref="S4.SS4.p1.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m8.1b"><ci id="S4.SS4.p1.8.m8.1.1.cmml" xref="S4.SS4.p1.8.m8.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m8.1c">t</annotation></semantics></math>, the current node vectors as <math id="S4.SS4.p1.9.m9.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS4.p1.9.m9.1a"><mi id="S4.SS4.p1.9.m9.1.1" xref="S4.SS4.p1.9.m9.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.9.m9.1b"><ci id="S4.SS4.p1.9.m9.1.1.cmml" xref="S4.SS4.p1.9.m9.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.9.m9.1c">n</annotation></semantics></math>, the Euclidean distance as <math id="S4.SS4.p1.10.m10.2" class="ltx_Math" alttext="d(x,y)" display="inline"><semantics id="S4.SS4.p1.10.m10.2a"><mrow id="S4.SS4.p1.10.m10.2.3" xref="S4.SS4.p1.10.m10.2.3.cmml"><mi id="S4.SS4.p1.10.m10.2.3.2" xref="S4.SS4.p1.10.m10.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.10.m10.2.3.1" xref="S4.SS4.p1.10.m10.2.3.1.cmml">â€‹</mo><mrow id="S4.SS4.p1.10.m10.2.3.3.2" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.10.m10.2.3.3.2.1" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">(</mo><mi id="S4.SS4.p1.10.m10.1.1" xref="S4.SS4.p1.10.m10.1.1.cmml">x</mi><mo id="S4.SS4.p1.10.m10.2.3.3.2.2" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">,</mo><mi id="S4.SS4.p1.10.m10.2.2" xref="S4.SS4.p1.10.m10.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS4.p1.10.m10.2.3.3.2.3" xref="S4.SS4.p1.10.m10.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.10.m10.2b"><apply id="S4.SS4.p1.10.m10.2.3.cmml" xref="S4.SS4.p1.10.m10.2.3"><times id="S4.SS4.p1.10.m10.2.3.1.cmml" xref="S4.SS4.p1.10.m10.2.3.1"></times><ci id="S4.SS4.p1.10.m10.2.3.2.cmml" xref="S4.SS4.p1.10.m10.2.3.2">ğ‘‘</ci><interval closure="open" id="S4.SS4.p1.10.m10.2.3.3.1.cmml" xref="S4.SS4.p1.10.m10.2.3.3.2"><ci id="S4.SS4.p1.10.m10.1.1.cmml" xref="S4.SS4.p1.10.m10.1.1">ğ‘¥</ci><ci id="S4.SS4.p1.10.m10.2.2.cmml" xref="S4.SS4.p1.10.m10.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.10.m10.2c">d(x,y)</annotation></semantics></math>, and the result of the graph as <math id="S4.SS4.p1.11.m11.1" class="ltx_Math" alttext="d_{step}" display="inline"><semantics id="S4.SS4.p1.11.m11.1a"><msub id="S4.SS4.p1.11.m11.1.1" xref="S4.SS4.p1.11.m11.1.1.cmml"><mi id="S4.SS4.p1.11.m11.1.1.2" xref="S4.SS4.p1.11.m11.1.1.2.cmml">d</mi><mrow id="S4.SS4.p1.11.m11.1.1.3" xref="S4.SS4.p1.11.m11.1.1.3.cmml"><mi id="S4.SS4.p1.11.m11.1.1.3.2" xref="S4.SS4.p1.11.m11.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS4.p1.11.m11.1.1.3.3" xref="S4.SS4.p1.11.m11.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1a" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS4.p1.11.m11.1.1.3.4" xref="S4.SS4.p1.11.m11.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.11.m11.1.1.3.1b" xref="S4.SS4.p1.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS4.p1.11.m11.1.1.3.5" xref="S4.SS4.p1.11.m11.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.11.m11.1b"><apply id="S4.SS4.p1.11.m11.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.11.m11.1.1.1.cmml" xref="S4.SS4.p1.11.m11.1.1">subscript</csymbol><ci id="S4.SS4.p1.11.m11.1.1.2.cmml" xref="S4.SS4.p1.11.m11.1.1.2">ğ‘‘</ci><apply id="S4.SS4.p1.11.m11.1.1.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3"><times id="S4.SS4.p1.11.m11.1.1.3.1.cmml" xref="S4.SS4.p1.11.m11.1.1.3.1"></times><ci id="S4.SS4.p1.11.m11.1.1.3.2.cmml" xref="S4.SS4.p1.11.m11.1.1.3.2">ğ‘ </ci><ci id="S4.SS4.p1.11.m11.1.1.3.3.cmml" xref="S4.SS4.p1.11.m11.1.1.3.3">ğ‘¡</ci><ci id="S4.SS4.p1.11.m11.1.1.3.4.cmml" xref="S4.SS4.p1.11.m11.1.1.3.4">ğ‘’</ci><ci id="S4.SS4.p1.11.m11.1.1.3.5.cmml" xref="S4.SS4.p1.11.m11.1.1.3.5">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.11.m11.1c">d_{step}</annotation></semantics></math> we performed the following for every sequence step:</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.2" class="ltx_Math" alttext="\displaystyle d_{node}(x,y)" display="inline"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.3" xref="S4.E3.m1.2.3.cmml"><msub id="S4.E3.m1.2.3.2" xref="S4.E3.m1.2.3.2.cmml"><mi id="S4.E3.m1.2.3.2.2" xref="S4.E3.m1.2.3.2.2.cmml">d</mi><mrow id="S4.E3.m1.2.3.2.3" xref="S4.E3.m1.2.3.2.3.cmml"><mi id="S4.E3.m1.2.3.2.3.2" xref="S4.E3.m1.2.3.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1" xref="S4.E3.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.2.3.2.3.3" xref="S4.E3.m1.2.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1a" xref="S4.E3.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.2.3.2.3.4" xref="S4.E3.m1.2.3.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.2.3.1b" xref="S4.E3.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.2.3.2.3.5" xref="S4.E3.m1.2.3.2.3.5.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.3.1" xref="S4.E3.m1.2.3.1.cmml">â€‹</mo><mrow id="S4.E3.m1.2.3.3.2" xref="S4.E3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.E3.m1.2.3.3.2.1" xref="S4.E3.m1.2.3.3.1.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">x</mi><mo id="S4.E3.m1.2.3.3.2.2" xref="S4.E3.m1.2.3.3.1.cmml">,</mo><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">y</mi><mo stretchy="false" id="S4.E3.m1.2.3.3.2.3" xref="S4.E3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.3.cmml" xref="S4.E3.m1.2.3"><times id="S4.E3.m1.2.3.1.cmml" xref="S4.E3.m1.2.3.1"></times><apply id="S4.E3.m1.2.3.2.cmml" xref="S4.E3.m1.2.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.3.2.1.cmml" xref="S4.E3.m1.2.3.2">subscript</csymbol><ci id="S4.E3.m1.2.3.2.2.cmml" xref="S4.E3.m1.2.3.2.2">ğ‘‘</ci><apply id="S4.E3.m1.2.3.2.3.cmml" xref="S4.E3.m1.2.3.2.3"><times id="S4.E3.m1.2.3.2.3.1.cmml" xref="S4.E3.m1.2.3.2.3.1"></times><ci id="S4.E3.m1.2.3.2.3.2.cmml" xref="S4.E3.m1.2.3.2.3.2">ğ‘›</ci><ci id="S4.E3.m1.2.3.2.3.3.cmml" xref="S4.E3.m1.2.3.2.3.3">ğ‘œ</ci><ci id="S4.E3.m1.2.3.2.3.4.cmml" xref="S4.E3.m1.2.3.2.3.4">ğ‘‘</ci><ci id="S4.E3.m1.2.3.2.3.5.cmml" xref="S4.E3.m1.2.3.2.3.5">ğ‘’</ci></apply></apply><interval closure="open" id="S4.E3.m1.2.3.3.1.cmml" xref="S4.E3.m1.2.3.3.2"><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">ğ‘¥</ci><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">\displaystyle d_{node}(x,y)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E3.m2.6" class="ltx_Math" alttext="\displaystyle=\lambda_{1}d(x_{g},y_{g})+\lambda_{2}d(x_{a},y_{a})+\lambda_{3}d(x_{t},y_{t})" display="inline"><semantics id="S4.E3.m2.6a"><mrow id="S4.E3.m2.6.6" xref="S4.E3.m2.6.6.cmml"><mi id="S4.E3.m2.6.6.8" xref="S4.E3.m2.6.6.8.cmml"></mi><mo id="S4.E3.m2.6.6.7" xref="S4.E3.m2.6.6.7.cmml">=</mo><mrow id="S4.E3.m2.6.6.6" xref="S4.E3.m2.6.6.6.cmml"><mrow id="S4.E3.m2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.cmml"><msub id="S4.E3.m2.2.2.2.2.4" xref="S4.E3.m2.2.2.2.2.4.cmml"><mi id="S4.E3.m2.2.2.2.2.4.2" xref="S4.E3.m2.2.2.2.2.4.2.cmml">Î»</mi><mn id="S4.E3.m2.2.2.2.2.4.3" xref="S4.E3.m2.2.2.2.2.4.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.E3.m2.2.2.2.2.5" xref="S4.E3.m2.2.2.2.2.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.2.2.2.2.3a" xref="S4.E3.m2.2.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E3.m2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.2.2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.2.3.cmml">(</mo><msub id="S4.E3.m2.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.3.cmml">g</mi></msub><mo id="S4.E3.m2.2.2.2.2.2.2.4" xref="S4.E3.m2.2.2.2.2.2.3.cmml">,</mo><msub id="S4.E3.m2.2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.2.2.cmml"><mi id="S4.E3.m2.2.2.2.2.2.2.2.2" xref="S4.E3.m2.2.2.2.2.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.2.2.2.2.2.2.2.3" xref="S4.E3.m2.2.2.2.2.2.2.2.3.cmml">g</mi></msub><mo stretchy="false" id="S4.E3.m2.2.2.2.2.2.2.5" xref="S4.E3.m2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.6.6.6.7" xref="S4.E3.m2.6.6.6.7.cmml">+</mo><mrow id="S4.E3.m2.4.4.4.4" xref="S4.E3.m2.4.4.4.4.cmml"><msub id="S4.E3.m2.4.4.4.4.4" xref="S4.E3.m2.4.4.4.4.4.cmml"><mi id="S4.E3.m2.4.4.4.4.4.2" xref="S4.E3.m2.4.4.4.4.4.2.cmml">Î»</mi><mn id="S4.E3.m2.4.4.4.4.4.3" xref="S4.E3.m2.4.4.4.4.4.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.4.4.4.4.3" xref="S4.E3.m2.4.4.4.4.3.cmml">â€‹</mo><mi id="S4.E3.m2.4.4.4.4.5" xref="S4.E3.m2.4.4.4.4.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.4.4.4.4.3a" xref="S4.E3.m2.4.4.4.4.3.cmml">â€‹</mo><mrow id="S4.E3.m2.4.4.4.4.2.2" xref="S4.E3.m2.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.4.4.4.4.2.2.3" xref="S4.E3.m2.4.4.4.4.2.3.cmml">(</mo><msub id="S4.E3.m2.3.3.3.3.1.1.1" xref="S4.E3.m2.3.3.3.3.1.1.1.cmml"><mi id="S4.E3.m2.3.3.3.3.1.1.1.2" xref="S4.E3.m2.3.3.3.3.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.3.3.3.3.1.1.1.3" xref="S4.E3.m2.3.3.3.3.1.1.1.3.cmml">a</mi></msub><mo id="S4.E3.m2.4.4.4.4.2.2.4" xref="S4.E3.m2.4.4.4.4.2.3.cmml">,</mo><msub id="S4.E3.m2.4.4.4.4.2.2.2" xref="S4.E3.m2.4.4.4.4.2.2.2.cmml"><mi id="S4.E3.m2.4.4.4.4.2.2.2.2" xref="S4.E3.m2.4.4.4.4.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.4.4.4.4.2.2.2.3" xref="S4.E3.m2.4.4.4.4.2.2.2.3.cmml">a</mi></msub><mo stretchy="false" id="S4.E3.m2.4.4.4.4.2.2.5" xref="S4.E3.m2.4.4.4.4.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.6.6.6.7a" xref="S4.E3.m2.6.6.6.7.cmml">+</mo><mrow id="S4.E3.m2.6.6.6.6" xref="S4.E3.m2.6.6.6.6.cmml"><msub id="S4.E3.m2.6.6.6.6.4" xref="S4.E3.m2.6.6.6.6.4.cmml"><mi id="S4.E3.m2.6.6.6.6.4.2" xref="S4.E3.m2.6.6.6.6.4.2.cmml">Î»</mi><mn id="S4.E3.m2.6.6.6.6.4.3" xref="S4.E3.m2.6.6.6.6.4.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.6.6.3" xref="S4.E3.m2.6.6.6.6.3.cmml">â€‹</mo><mi id="S4.E3.m2.6.6.6.6.5" xref="S4.E3.m2.6.6.6.6.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.6.6.3a" xref="S4.E3.m2.6.6.6.6.3.cmml">â€‹</mo><mrow id="S4.E3.m2.6.6.6.6.2.2" xref="S4.E3.m2.6.6.6.6.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.6.6.6.6.2.2.3" xref="S4.E3.m2.6.6.6.6.2.3.cmml">(</mo><msub id="S4.E3.m2.5.5.5.5.1.1.1" xref="S4.E3.m2.5.5.5.5.1.1.1.cmml"><mi id="S4.E3.m2.5.5.5.5.1.1.1.2" xref="S4.E3.m2.5.5.5.5.1.1.1.2.cmml">x</mi><mi id="S4.E3.m2.5.5.5.5.1.1.1.3" xref="S4.E3.m2.5.5.5.5.1.1.1.3.cmml">t</mi></msub><mo id="S4.E3.m2.6.6.6.6.2.2.4" xref="S4.E3.m2.6.6.6.6.2.3.cmml">,</mo><msub id="S4.E3.m2.6.6.6.6.2.2.2" xref="S4.E3.m2.6.6.6.6.2.2.2.cmml"><mi id="S4.E3.m2.6.6.6.6.2.2.2.2" xref="S4.E3.m2.6.6.6.6.2.2.2.2.cmml">y</mi><mi id="S4.E3.m2.6.6.6.6.2.2.2.3" xref="S4.E3.m2.6.6.6.6.2.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="S4.E3.m2.6.6.6.6.2.2.5" xref="S4.E3.m2.6.6.6.6.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m2.6b"><apply id="S4.E3.m2.6.6.cmml" xref="S4.E3.m2.6.6"><eq id="S4.E3.m2.6.6.7.cmml" xref="S4.E3.m2.6.6.7"></eq><csymbol cd="latexml" id="S4.E3.m2.6.6.8.cmml" xref="S4.E3.m2.6.6.8">absent</csymbol><apply id="S4.E3.m2.6.6.6.cmml" xref="S4.E3.m2.6.6.6"><plus id="S4.E3.m2.6.6.6.7.cmml" xref="S4.E3.m2.6.6.6.7"></plus><apply id="S4.E3.m2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2"><times id="S4.E3.m2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.3"></times><apply id="S4.E3.m2.2.2.2.2.4.cmml" xref="S4.E3.m2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.E3.m2.2.2.2.2.4.1.cmml" xref="S4.E3.m2.2.2.2.2.4">subscript</csymbol><ci id="S4.E3.m2.2.2.2.2.4.2.cmml" xref="S4.E3.m2.2.2.2.2.4.2">ğœ†</ci><cn type="integer" id="S4.E3.m2.2.2.2.2.4.3.cmml" xref="S4.E3.m2.2.2.2.2.4.3">1</cn></apply><ci id="S4.E3.m2.2.2.2.2.5.cmml" xref="S4.E3.m2.2.2.2.2.5">ğ‘‘</ci><interval closure="open" id="S4.E3.m2.2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.2.2"><apply id="S4.E3.m2.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.3">ğ‘”</ci></apply><apply id="S4.E3.m2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.2.2.2.2.2.2.2.1.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E3.m2.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2.2">ğ‘¦</ci><ci id="S4.E3.m2.2.2.2.2.2.2.2.3.cmml" xref="S4.E3.m2.2.2.2.2.2.2.2.3">ğ‘”</ci></apply></interval></apply><apply id="S4.E3.m2.4.4.4.4.cmml" xref="S4.E3.m2.4.4.4.4"><times id="S4.E3.m2.4.4.4.4.3.cmml" xref="S4.E3.m2.4.4.4.4.3"></times><apply id="S4.E3.m2.4.4.4.4.4.cmml" xref="S4.E3.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S4.E3.m2.4.4.4.4.4.1.cmml" xref="S4.E3.m2.4.4.4.4.4">subscript</csymbol><ci id="S4.E3.m2.4.4.4.4.4.2.cmml" xref="S4.E3.m2.4.4.4.4.4.2">ğœ†</ci><cn type="integer" id="S4.E3.m2.4.4.4.4.4.3.cmml" xref="S4.E3.m2.4.4.4.4.4.3">2</cn></apply><ci id="S4.E3.m2.4.4.4.4.5.cmml" xref="S4.E3.m2.4.4.4.4.5">ğ‘‘</ci><interval closure="open" id="S4.E3.m2.4.4.4.4.2.3.cmml" xref="S4.E3.m2.4.4.4.4.2.2"><apply id="S4.E3.m2.3.3.3.3.1.1.1.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.3.3.3.3.1.1.1.1.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1">subscript</csymbol><ci id="S4.E3.m2.3.3.3.3.1.1.1.2.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1.2">ğ‘¥</ci><ci id="S4.E3.m2.3.3.3.3.1.1.1.3.cmml" xref="S4.E3.m2.3.3.3.3.1.1.1.3">ğ‘</ci></apply><apply id="S4.E3.m2.4.4.4.4.2.2.2.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.4.4.4.4.2.2.2.1.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2">subscript</csymbol><ci id="S4.E3.m2.4.4.4.4.2.2.2.2.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2.2">ğ‘¦</ci><ci id="S4.E3.m2.4.4.4.4.2.2.2.3.cmml" xref="S4.E3.m2.4.4.4.4.2.2.2.3">ğ‘</ci></apply></interval></apply><apply id="S4.E3.m2.6.6.6.6.cmml" xref="S4.E3.m2.6.6.6.6"><times id="S4.E3.m2.6.6.6.6.3.cmml" xref="S4.E3.m2.6.6.6.6.3"></times><apply id="S4.E3.m2.6.6.6.6.4.cmml" xref="S4.E3.m2.6.6.6.6.4"><csymbol cd="ambiguous" id="S4.E3.m2.6.6.6.6.4.1.cmml" xref="S4.E3.m2.6.6.6.6.4">subscript</csymbol><ci id="S4.E3.m2.6.6.6.6.4.2.cmml" xref="S4.E3.m2.6.6.6.6.4.2">ğœ†</ci><cn type="integer" id="S4.E3.m2.6.6.6.6.4.3.cmml" xref="S4.E3.m2.6.6.6.6.4.3">3</cn></apply><ci id="S4.E3.m2.6.6.6.6.5.cmml" xref="S4.E3.m2.6.6.6.6.5">ğ‘‘</ci><interval closure="open" id="S4.E3.m2.6.6.6.6.2.3.cmml" xref="S4.E3.m2.6.6.6.6.2.2"><apply id="S4.E3.m2.5.5.5.5.1.1.1.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.5.5.5.5.1.1.1.1.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1">subscript</csymbol><ci id="S4.E3.m2.5.5.5.5.1.1.1.2.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1.2">ğ‘¥</ci><ci id="S4.E3.m2.5.5.5.5.1.1.1.3.cmml" xref="S4.E3.m2.5.5.5.5.1.1.1.3">ğ‘¡</ci></apply><apply id="S4.E3.m2.6.6.6.6.2.2.2.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.6.6.6.6.2.2.2.1.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2">subscript</csymbol><ci id="S4.E3.m2.6.6.6.6.2.2.2.2.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2.2">ğ‘¦</ci><ci id="S4.E3.m2.6.6.6.6.2.2.2.3.cmml" xref="S4.E3.m2.6.6.6.6.2.2.2.3">ğ‘¡</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m2.6c">\displaystyle=\lambda_{1}d(x_{g},y_{g})+\lambda_{2}d(x_{a},y_{a})+\lambda_{3}d(x_{t},y_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.3" class="ltx_Math" alttext="\displaystyle d_{path}(vg_{t},vn_{t},t)" display="inline"><semantics id="S4.E4.m1.3a"><mrow id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml"><msub id="S4.E4.m1.3.3.4" xref="S4.E4.m1.3.3.4.cmml"><mi id="S4.E4.m1.3.3.4.2" xref="S4.E4.m1.3.3.4.2.cmml">d</mi><mrow id="S4.E4.m1.3.3.4.3" xref="S4.E4.m1.3.3.4.3.cmml"><mi id="S4.E4.m1.3.3.4.3.2" xref="S4.E4.m1.3.3.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1" xref="S4.E4.m1.3.3.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.4.3.3" xref="S4.E4.m1.3.3.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1a" xref="S4.E4.m1.3.3.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.4.3.4" xref="S4.E4.m1.3.3.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.4.3.1b" xref="S4.E4.m1.3.3.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.4.3.5" xref="S4.E4.m1.3.3.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3" xref="S4.E4.m1.3.3.3.cmml">â€‹</mo><mrow id="S4.E4.m1.3.3.2.2" xref="S4.E4.m1.3.3.2.3.cmml"><mo stretchy="false" id="S4.E4.m1.3.3.2.2.3" xref="S4.E4.m1.3.3.2.3.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1" xref="S4.E4.m1.2.2.1.1.1.cmml"><mi id="S4.E4.m1.2.2.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.cmml">â€‹</mo><msub id="S4.E4.m1.2.2.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.3.2.cmml">g</mi><mi id="S4.E4.m1.2.2.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.3.3.cmml">t</mi></msub></mrow><mo id="S4.E4.m1.3.3.2.2.4" xref="S4.E4.m1.3.3.2.3.cmml">,</mo><mrow id="S4.E4.m1.3.3.2.2.2" xref="S4.E4.m1.3.3.2.2.2.cmml"><mi id="S4.E4.m1.3.3.2.2.2.2" xref="S4.E4.m1.3.3.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.2.2.2.1" xref="S4.E4.m1.3.3.2.2.2.1.cmml">â€‹</mo><msub id="S4.E4.m1.3.3.2.2.2.3" xref="S4.E4.m1.3.3.2.2.2.3.cmml"><mi id="S4.E4.m1.3.3.2.2.2.3.2" xref="S4.E4.m1.3.3.2.2.2.3.2.cmml">n</mi><mi id="S4.E4.m1.3.3.2.2.2.3.3" xref="S4.E4.m1.3.3.2.2.2.3.3.cmml">t</mi></msub></mrow><mo id="S4.E4.m1.3.3.2.2.5" xref="S4.E4.m1.3.3.2.3.cmml">,</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">t</mi><mo stretchy="false" id="S4.E4.m1.3.3.2.2.6" xref="S4.E4.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.3b"><apply id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3"><times id="S4.E4.m1.3.3.3.cmml" xref="S4.E4.m1.3.3.3"></times><apply id="S4.E4.m1.3.3.4.cmml" xref="S4.E4.m1.3.3.4"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.4.1.cmml" xref="S4.E4.m1.3.3.4">subscript</csymbol><ci id="S4.E4.m1.3.3.4.2.cmml" xref="S4.E4.m1.3.3.4.2">ğ‘‘</ci><apply id="S4.E4.m1.3.3.4.3.cmml" xref="S4.E4.m1.3.3.4.3"><times id="S4.E4.m1.3.3.4.3.1.cmml" xref="S4.E4.m1.3.3.4.3.1"></times><ci id="S4.E4.m1.3.3.4.3.2.cmml" xref="S4.E4.m1.3.3.4.3.2">ğ‘</ci><ci id="S4.E4.m1.3.3.4.3.3.cmml" xref="S4.E4.m1.3.3.4.3.3">ğ‘</ci><ci id="S4.E4.m1.3.3.4.3.4.cmml" xref="S4.E4.m1.3.3.4.3.4">ğ‘¡</ci><ci id="S4.E4.m1.3.3.4.3.5.cmml" xref="S4.E4.m1.3.3.4.3.5">â„</ci></apply></apply><vector id="S4.E4.m1.3.3.2.3.cmml" xref="S4.E4.m1.3.3.2.2"><apply id="S4.E4.m1.2.2.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1"></times><ci id="S4.E4.m1.2.2.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.2">ğ‘£</ci><apply id="S4.E4.m1.2.2.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.3.2">ğ‘”</ci><ci id="S4.E4.m1.2.2.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.3.3">ğ‘¡</ci></apply></apply><apply id="S4.E4.m1.3.3.2.2.2.cmml" xref="S4.E4.m1.3.3.2.2.2"><times id="S4.E4.m1.3.3.2.2.2.1.cmml" xref="S4.E4.m1.3.3.2.2.2.1"></times><ci id="S4.E4.m1.3.3.2.2.2.2.cmml" xref="S4.E4.m1.3.3.2.2.2.2">ğ‘£</ci><apply id="S4.E4.m1.3.3.2.2.2.3.cmml" xref="S4.E4.m1.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.2.2.3.1.cmml" xref="S4.E4.m1.3.3.2.2.2.3">subscript</csymbol><ci id="S4.E4.m1.3.3.2.2.2.3.2.cmml" xref="S4.E4.m1.3.3.2.2.2.3.2">ğ‘›</ci><ci id="S4.E4.m1.3.3.2.2.2.3.3.cmml" xref="S4.E4.m1.3.3.2.2.2.3.3">ğ‘¡</ci></apply></apply><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">ğ‘¡</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.3c">\displaystyle d_{path}(vg_{t},vn_{t},t)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E4.m2.2" class="ltx_Math" alttext="\displaystyle=\sum_{i=0}^{Gd}d_{node}(vg_{t+i},vn_{t+i})" display="inline"><semantics id="S4.E4.m2.2a"><mrow id="S4.E4.m2.2.2" xref="S4.E4.m2.2.2.cmml"><mi id="S4.E4.m2.2.2.4" xref="S4.E4.m2.2.2.4.cmml"></mi><mo id="S4.E4.m2.2.2.3" xref="S4.E4.m2.2.2.3.cmml">=</mo><mrow id="S4.E4.m2.2.2.2" xref="S4.E4.m2.2.2.2.cmml"><mstyle displaystyle="true" id="S4.E4.m2.2.2.2.3" xref="S4.E4.m2.2.2.2.3.cmml"><munderover id="S4.E4.m2.2.2.2.3a" xref="S4.E4.m2.2.2.2.3.cmml"><mo movablelimits="false" id="S4.E4.m2.2.2.2.3.2.2" xref="S4.E4.m2.2.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S4.E4.m2.2.2.2.3.2.3" xref="S4.E4.m2.2.2.2.3.2.3.cmml"><mi id="S4.E4.m2.2.2.2.3.2.3.2" xref="S4.E4.m2.2.2.2.3.2.3.2.cmml">i</mi><mo id="S4.E4.m2.2.2.2.3.2.3.1" xref="S4.E4.m2.2.2.2.3.2.3.1.cmml">=</mo><mn id="S4.E4.m2.2.2.2.3.2.3.3" xref="S4.E4.m2.2.2.2.3.2.3.3.cmml">0</mn></mrow><mrow id="S4.E4.m2.2.2.2.3.3" xref="S4.E4.m2.2.2.2.3.3.cmml"><mi id="S4.E4.m2.2.2.2.3.3.2" xref="S4.E4.m2.2.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.3.3.1" xref="S4.E4.m2.2.2.2.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m2.2.2.2.3.3.3" xref="S4.E4.m2.2.2.2.3.3.3.cmml">d</mi></mrow></munderover></mstyle><mrow id="S4.E4.m2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.cmml"><msub id="S4.E4.m2.2.2.2.2.4" xref="S4.E4.m2.2.2.2.2.4.cmml"><mi id="S4.E4.m2.2.2.2.2.4.2" xref="S4.E4.m2.2.2.2.2.4.2.cmml">d</mi><mrow id="S4.E4.m2.2.2.2.2.4.3" xref="S4.E4.m2.2.2.2.2.4.3.cmml"><mi id="S4.E4.m2.2.2.2.2.4.3.2" xref="S4.E4.m2.2.2.2.2.4.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m2.2.2.2.2.4.3.3" xref="S4.E4.m2.2.2.2.2.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1a" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m2.2.2.2.2.4.3.4" xref="S4.E4.m2.2.2.2.2.4.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.4.3.1b" xref="S4.E4.m2.2.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E4.m2.2.2.2.2.4.3.5" xref="S4.E4.m2.2.2.2.2.4.3.5.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E4.m2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E4.m2.2.2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.2.3.cmml">(</mo><mrow id="S4.E4.m2.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.1.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S4.E4.m2.1.1.1.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.2" xref="S4.E4.m2.1.1.1.1.1.1.1.3.2.cmml">g</mi><mrow id="S4.E4.m2.1.1.1.1.1.1.1.3.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.3.2" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S4.E4.m2.1.1.1.1.1.1.1.3.3.1" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mi id="S4.E4.m2.1.1.1.1.1.1.1.3.3.3" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.E4.m2.2.2.2.2.2.2.4" xref="S4.E4.m2.2.2.2.2.2.3.cmml">,</mo><mrow id="S4.E4.m2.2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.2.2.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.2" xref="S4.E4.m2.2.2.2.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.2.2.2.2.2.2.2.1" xref="S4.E4.m2.2.2.2.2.2.2.2.1.cmml">â€‹</mo><msub id="S4.E4.m2.2.2.2.2.2.2.2.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.2" xref="S4.E4.m2.2.2.2.2.2.2.2.3.2.cmml">n</mi><mrow id="S4.E4.m2.2.2.2.2.2.2.2.3.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.3.2" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.2.cmml">t</mi><mo id="S4.E4.m2.2.2.2.2.2.2.2.3.3.1" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.1.cmml">+</mo><mi id="S4.E4.m2.2.2.2.2.2.2.2.3.3.3" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E4.m2.2.2.2.2.2.2.5" xref="S4.E4.m2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m2.2b"><apply id="S4.E4.m2.2.2.cmml" xref="S4.E4.m2.2.2"><eq id="S4.E4.m2.2.2.3.cmml" xref="S4.E4.m2.2.2.3"></eq><csymbol cd="latexml" id="S4.E4.m2.2.2.4.cmml" xref="S4.E4.m2.2.2.4">absent</csymbol><apply id="S4.E4.m2.2.2.2.cmml" xref="S4.E4.m2.2.2.2"><apply id="S4.E4.m2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.3.1.cmml" xref="S4.E4.m2.2.2.2.3">superscript</csymbol><apply id="S4.E4.m2.2.2.2.3.2.cmml" xref="S4.E4.m2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.3.2.1.cmml" xref="S4.E4.m2.2.2.2.3">subscript</csymbol><sum id="S4.E4.m2.2.2.2.3.2.2.cmml" xref="S4.E4.m2.2.2.2.3.2.2"></sum><apply id="S4.E4.m2.2.2.2.3.2.3.cmml" xref="S4.E4.m2.2.2.2.3.2.3"><eq id="S4.E4.m2.2.2.2.3.2.3.1.cmml" xref="S4.E4.m2.2.2.2.3.2.3.1"></eq><ci id="S4.E4.m2.2.2.2.3.2.3.2.cmml" xref="S4.E4.m2.2.2.2.3.2.3.2">ğ‘–</ci><cn type="integer" id="S4.E4.m2.2.2.2.3.2.3.3.cmml" xref="S4.E4.m2.2.2.2.3.2.3.3">0</cn></apply></apply><apply id="S4.E4.m2.2.2.2.3.3.cmml" xref="S4.E4.m2.2.2.2.3.3"><times id="S4.E4.m2.2.2.2.3.3.1.cmml" xref="S4.E4.m2.2.2.2.3.3.1"></times><ci id="S4.E4.m2.2.2.2.3.3.2.cmml" xref="S4.E4.m2.2.2.2.3.3.2">ğº</ci><ci id="S4.E4.m2.2.2.2.3.3.3.cmml" xref="S4.E4.m2.2.2.2.3.3.3">ğ‘‘</ci></apply></apply><apply id="S4.E4.m2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2"><times id="S4.E4.m2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.3"></times><apply id="S4.E4.m2.2.2.2.2.4.cmml" xref="S4.E4.m2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.2.4.1.cmml" xref="S4.E4.m2.2.2.2.2.4">subscript</csymbol><ci id="S4.E4.m2.2.2.2.2.4.2.cmml" xref="S4.E4.m2.2.2.2.2.4.2">ğ‘‘</ci><apply id="S4.E4.m2.2.2.2.2.4.3.cmml" xref="S4.E4.m2.2.2.2.2.4.3"><times id="S4.E4.m2.2.2.2.2.4.3.1.cmml" xref="S4.E4.m2.2.2.2.2.4.3.1"></times><ci id="S4.E4.m2.2.2.2.2.4.3.2.cmml" xref="S4.E4.m2.2.2.2.2.4.3.2">ğ‘›</ci><ci id="S4.E4.m2.2.2.2.2.4.3.3.cmml" xref="S4.E4.m2.2.2.2.2.4.3.3">ğ‘œ</ci><ci id="S4.E4.m2.2.2.2.2.4.3.4.cmml" xref="S4.E4.m2.2.2.2.2.4.3.4">ğ‘‘</ci><ci id="S4.E4.m2.2.2.2.2.4.3.5.cmml" xref="S4.E4.m2.2.2.2.2.4.3.5">ğ‘’</ci></apply></apply><interval closure="open" id="S4.E4.m2.2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2"><apply id="S4.E4.m2.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1"><times id="S4.E4.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.1"></times><ci id="S4.E4.m2.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.2">ğ‘£</ci><apply id="S4.E4.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.2">ğ‘”</ci><apply id="S4.E4.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3"><plus id="S4.E4.m2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.1"></plus><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.2">ğ‘¡</ci><ci id="S4.E4.m2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply><apply id="S4.E4.m2.2.2.2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2"><times id="S4.E4.m2.2.2.2.2.2.2.2.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.1"></times><ci id="S4.E4.m2.2.2.2.2.2.2.2.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.2">ğ‘£</ci><apply id="S4.E4.m2.2.2.2.2.2.2.2.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E4.m2.2.2.2.2.2.2.2.3.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.2">ğ‘›</ci><apply id="S4.E4.m2.2.2.2.2.2.2.2.3.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3"><plus id="S4.E4.m2.2.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.1"></plus><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.2">ğ‘¡</ci><ci id="S4.E4.m2.2.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E4.m2.2.2.2.2.2.2.2.3.3.3">ğ‘–</ci></apply></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m2.2c">\displaystyle=\sum_{i=0}^{Gd}d_{node}(vg_{t+i},vn_{t+i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\displaystyle d_{step}" display="inline"><semantics id="S4.E5.m1.1a"><msub id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mi id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2.cmml">d</mi><mrow id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.3.2" xref="S4.E5.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1" xref="S4.E5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E5.m1.1.1.3.3" xref="S4.E5.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1a" xref="S4.E5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E5.m1.1.1.3.4" xref="S4.E5.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.3.1b" xref="S4.E5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E5.m1.1.1.3.5" xref="S4.E5.m1.1.1.3.5.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1">subscript</csymbol><ci id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2">ğ‘‘</ci><apply id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3"><times id="S4.E5.m1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.3.1"></times><ci id="S4.E5.m1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.3.2">ğ‘ </ci><ci id="S4.E5.m1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.3.3">ğ‘¡</ci><ci id="S4.E5.m1.1.1.3.4.cmml" xref="S4.E5.m1.1.1.3.4">ğ‘’</ci><ci id="S4.E5.m1.1.1.3.5.cmml" xref="S4.E5.m1.1.1.3.5">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\displaystyle d_{step}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E5.m2.3" class="ltx_Math" alttext="\displaystyle=\{d_{path}(vg_{0},vn_{0}),...,d_{path}(vg_{Gn},vn_{Gn})\}" display="inline"><semantics id="S4.E5.m2.3a"><mrow id="S4.E5.m2.3.3" xref="S4.E5.m2.3.3.cmml"><mi id="S4.E5.m2.3.3.4" xref="S4.E5.m2.3.3.4.cmml"></mi><mo id="S4.E5.m2.3.3.3" xref="S4.E5.m2.3.3.3.cmml">=</mo><mrow id="S4.E5.m2.3.3.2.2" xref="S4.E5.m2.3.3.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.3.3.2.2.3" xref="S4.E5.m2.3.3.2.3.cmml">{</mo><mrow id="S4.E5.m2.2.2.1.1.1" xref="S4.E5.m2.2.2.1.1.1.cmml"><msub id="S4.E5.m2.2.2.1.1.1.4" xref="S4.E5.m2.2.2.1.1.1.4.cmml"><mi id="S4.E5.m2.2.2.1.1.1.4.2" xref="S4.E5.m2.2.2.1.1.1.4.2.cmml">d</mi><mrow id="S4.E5.m2.2.2.1.1.1.4.3" xref="S4.E5.m2.2.2.1.1.1.4.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.4.3.2" xref="S4.E5.m2.2.2.1.1.1.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.3" xref="S4.E5.m2.2.2.1.1.1.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1a" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.4" xref="S4.E5.m2.2.2.1.1.1.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.4.3.1b" xref="S4.E5.m2.2.2.1.1.1.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.2.2.1.1.1.4.3.5" xref="S4.E5.m2.2.2.1.1.1.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.3" xref="S4.E5.m2.2.2.1.1.1.3.cmml">â€‹</mo><mrow id="S4.E5.m2.2.2.1.1.1.2.2" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.2.2.1.1.1.2.2.3" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">(</mo><mrow id="S4.E5.m2.2.2.1.1.1.1.1.1" xref="S4.E5.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E5.m2.2.2.1.1.1.1.1.1.2" xref="S4.E5.m2.2.2.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.1.1.1.1" xref="S4.E5.m2.2.2.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S4.E5.m2.2.2.1.1.1.1.1.1.3" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.1.1.1.3.2" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.2.cmml">g</mi><mn id="S4.E5.m2.2.2.1.1.1.1.1.1.3.3" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S4.E5.m2.2.2.1.1.1.2.2.4" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">,</mo><mrow id="S4.E5.m2.2.2.1.1.1.2.2.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.cmml"><mi id="S4.E5.m2.2.2.1.1.1.2.2.2.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.2.2.1.1.1.2.2.2.1" xref="S4.E5.m2.2.2.1.1.1.2.2.2.1.cmml">â€‹</mo><msub id="S4.E5.m2.2.2.1.1.1.2.2.2.3" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.cmml"><mi id="S4.E5.m2.2.2.1.1.1.2.2.2.3.2" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.2.cmml">n</mi><mn id="S4.E5.m2.2.2.1.1.1.2.2.2.3.3" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S4.E5.m2.2.2.1.1.1.2.2.5" xref="S4.E5.m2.2.2.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E5.m2.3.3.2.2.4" xref="S4.E5.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.E5.m2.1.1" xref="S4.E5.m2.1.1.cmml">â€¦</mi><mo id="S4.E5.m2.3.3.2.2.5" xref="S4.E5.m2.3.3.2.3.cmml">,</mo><mrow id="S4.E5.m2.3.3.2.2.2" xref="S4.E5.m2.3.3.2.2.2.cmml"><msub id="S4.E5.m2.3.3.2.2.2.4" xref="S4.E5.m2.3.3.2.2.2.4.cmml"><mi id="S4.E5.m2.3.3.2.2.2.4.2" xref="S4.E5.m2.3.3.2.2.2.4.2.cmml">d</mi><mrow id="S4.E5.m2.3.3.2.2.2.4.3" xref="S4.E5.m2.3.3.2.2.2.4.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.4.3.2" xref="S4.E5.m2.3.3.2.2.2.4.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.3" xref="S4.E5.m2.3.3.2.2.2.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1a" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.4" xref="S4.E5.m2.3.3.2.2.2.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.4.3.1b" xref="S4.E5.m2.3.3.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.3.3.2.2.2.4.3.5" xref="S4.E5.m2.3.3.2.2.2.4.3.5.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E5.m2.3.3.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m2.3.3.2.2.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">(</mo><mrow id="S4.E5.m2.3.3.2.2.2.1.1.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.1.1.1.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.1.cmml">â€‹</mo><msub id="S4.E5.m2.3.3.2.2.2.1.1.1.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.2.cmml">g</mi><mrow id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3.cmml">n</mi></mrow></msub></mrow><mo id="S4.E5.m2.3.3.2.2.2.2.2.4" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">,</mo><mrow id="S4.E5.m2.3.3.2.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.2.2.2.1" xref="S4.E5.m2.3.3.2.2.2.2.2.2.1.cmml">â€‹</mo><msub id="S4.E5.m2.3.3.2.2.2.2.2.2.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.2.cmml">n</mi><mrow id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1.cmml">â€‹</mo><mi id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3.cmml">n</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E5.m2.3.3.2.2.2.2.2.5" xref="S4.E5.m2.3.3.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m2.3.3.2.2.6" xref="S4.E5.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m2.3b"><apply id="S4.E5.m2.3.3.cmml" xref="S4.E5.m2.3.3"><eq id="S4.E5.m2.3.3.3.cmml" xref="S4.E5.m2.3.3.3"></eq><csymbol cd="latexml" id="S4.E5.m2.3.3.4.cmml" xref="S4.E5.m2.3.3.4">absent</csymbol><set id="S4.E5.m2.3.3.2.3.cmml" xref="S4.E5.m2.3.3.2.2"><apply id="S4.E5.m2.2.2.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1"><times id="S4.E5.m2.2.2.1.1.1.3.cmml" xref="S4.E5.m2.2.2.1.1.1.3"></times><apply id="S4.E5.m2.2.2.1.1.1.4.cmml" xref="S4.E5.m2.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.4.1.cmml" xref="S4.E5.m2.2.2.1.1.1.4">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.4.2.cmml" xref="S4.E5.m2.2.2.1.1.1.4.2">ğ‘‘</ci><apply id="S4.E5.m2.2.2.1.1.1.4.3.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3"><times id="S4.E5.m2.2.2.1.1.1.4.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.1"></times><ci id="S4.E5.m2.2.2.1.1.1.4.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.2">ğ‘</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.3">ğ‘</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.4.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.4">ğ‘¡</ci><ci id="S4.E5.m2.2.2.1.1.1.4.3.5.cmml" xref="S4.E5.m2.2.2.1.1.1.4.3.5">â„</ci></apply></apply><interval closure="open" id="S4.E5.m2.2.2.1.1.1.2.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2"><apply id="S4.E5.m2.2.2.1.1.1.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1"><times id="S4.E5.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.1"></times><ci id="S4.E5.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.2">ğ‘£</ci><apply id="S4.E5.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.2">ğ‘”</ci><cn type="integer" id="S4.E5.m2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.1.1.1.3.3">0</cn></apply></apply><apply id="S4.E5.m2.2.2.1.1.1.2.2.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2"><times id="S4.E5.m2.2.2.1.1.1.2.2.2.1.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.1"></times><ci id="S4.E5.m2.2.2.1.1.1.2.2.2.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.2">ğ‘£</ci><apply id="S4.E5.m2.2.2.1.1.1.2.2.2.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E5.m2.2.2.1.1.1.2.2.2.3.1.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3">subscript</csymbol><ci id="S4.E5.m2.2.2.1.1.1.2.2.2.3.2.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.2">ğ‘›</ci><cn type="integer" id="S4.E5.m2.2.2.1.1.1.2.2.2.3.3.cmml" xref="S4.E5.m2.2.2.1.1.1.2.2.2.3.3">0</cn></apply></apply></interval></apply><ci id="S4.E5.m2.1.1.cmml" xref="S4.E5.m2.1.1">â€¦</ci><apply id="S4.E5.m2.3.3.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2"><times id="S4.E5.m2.3.3.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.3"></times><apply id="S4.E5.m2.3.3.2.2.2.4.cmml" xref="S4.E5.m2.3.3.2.2.2.4"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.4.1.cmml" xref="S4.E5.m2.3.3.2.2.2.4">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.4.2.cmml" xref="S4.E5.m2.3.3.2.2.2.4.2">ğ‘‘</ci><apply id="S4.E5.m2.3.3.2.2.2.4.3.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3"><times id="S4.E5.m2.3.3.2.2.2.4.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.4.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.2">ğ‘</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.3">ğ‘</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.4.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.4">ğ‘¡</ci><ci id="S4.E5.m2.3.3.2.2.2.4.3.5.cmml" xref="S4.E5.m2.3.3.2.2.2.4.3.5">â„</ci></apply></apply><interval closure="open" id="S4.E5.m2.3.3.2.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2"><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1"><times id="S4.E5.m2.3.3.2.2.2.1.1.1.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.1"></times><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.2">ğ‘£</ci><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.1.1.1.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.2">ğ‘”</ci><apply id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3"><times id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.2">ğº</ci><ci id="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.1.1.1.3.3.3">ğ‘›</ci></apply></apply></apply><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2"><times id="S4.E5.m2.3.3.2.2.2.2.2.2.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.1"></times><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.2">ğ‘£</ci><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E5.m2.3.3.2.2.2.2.2.2.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.2">ğ‘›</ci><apply id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3"><times id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.1"></times><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.2">ğº</ci><ci id="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E5.m2.3.3.2.2.2.2.2.2.3.3.3">ğ‘›</ci></apply></apply></apply></interval></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m2.3c">\displaystyle=\{d_{path}(vg_{0},vn_{0}),...,d_{path}(vg_{Gn},vn_{Gn})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.5" class="ltx_p">The hyperparameters <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\lambda_{1}" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">Î»</mi><mn id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S4.SS4.p3.1.m1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\lambda_{1}</annotation></semantics></math>, <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="\lambda_{2}" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">Î»</mi><mn id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">ğœ†</ci><cn type="integer" id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\lambda_{2}</annotation></semantics></math>, and <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="\lambda_{3}" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><msub id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">Î»</mi><mn id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">ğœ†</ci><cn type="integer" id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">\lambda_{3}</annotation></semantics></math> were set to 4, 2, and 1, respectively. If a path contained an already selected node, the path was removed. After ordering the sequence list, we sampled from the top <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mi id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><ci id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">k</annotation></semantics></math> paths, by converting the normalized distances to a probability distribution. After choosing the sequence sample, we set the new gesture value to the gesture value of <math id="S4.SS4.p3.5.m5.1" class="ltx_Math" alttext="Vg_{0}" display="inline"><semantics id="S4.SS4.p3.5.m5.1a"><mrow id="S4.SS4.p3.5.m5.1.1" xref="S4.SS4.p3.5.m5.1.1.cmml"><mi id="S4.SS4.p3.5.m5.1.1.2" xref="S4.SS4.p3.5.m5.1.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.5.m5.1.1.1" xref="S4.SS4.p3.5.m5.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.p3.5.m5.1.1.3" xref="S4.SS4.p3.5.m5.1.1.3.cmml"><mi id="S4.SS4.p3.5.m5.1.1.3.2" xref="S4.SS4.p3.5.m5.1.1.3.2.cmml">g</mi><mn id="S4.SS4.p3.5.m5.1.1.3.3" xref="S4.SS4.p3.5.m5.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.5.m5.1b"><apply id="S4.SS4.p3.5.m5.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1"><times id="S4.SS4.p3.5.m5.1.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1.1"></times><ci id="S4.SS4.p3.5.m5.1.1.2.cmml" xref="S4.SS4.p3.5.m5.1.1.2">ğ‘‰</ci><apply id="S4.SS4.p3.5.m5.1.1.3.cmml" xref="S4.SS4.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.1.1.3.1.cmml" xref="S4.SS4.p3.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS4.p3.5.m5.1.1.3.2.cmml" xref="S4.SS4.p3.5.m5.1.1.3.2">ğ‘”</ci><cn type="integer" id="S4.SS4.p3.5.m5.1.1.3.3.cmml" xref="S4.SS4.p3.5.m5.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.5.m5.1c">Vg_{0}</annotation></semantics></math> and repeated the process for the next step.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Iconic Gestures</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">As the main goal of the present work is to support the understanding of an explainee, we aimed to include believable representational (iconic) gestures that could be easily added and removed from the original gestures. To the best of our knowledge, no system exists that can automatically generate high-quality, aligned iconic gestures that are semantically coherent to a given verbal input. We hence manually annotated all instances of the generated explanation where an iconic gesture would make sense and pre-recorded suitable iconic gestures. For this, we captured 40 iconic gestures, with 3 different repetitions for variety, with a Logitech C920 in 1080p and used the same pose estimation algorithm as in Sect.Â <a href="#S4.SS1" title="4.1. Dataset â€£ 4. Gesture Generation â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> to extract the arm and hand positions <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. As misalignment between gestures and speech is known to decrease interaction quality <cite class="ltx_cite ltx_citemacro_citep">(Salem etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2012</a>; Wagner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2014</a>; Kelly etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2004</a>)</cite>, we did not use any automatic method to determine the position and length of the iconic gesture clips, but instead placed, aligned, and blended all iconic gesture clips by hand using the software Blender <cite class="ltx_cite ltx_citemacro_citep">(Community, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>. To ensure believable high-quality iconic gestures, we only added iconic gestures if new information during the explanation was given.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation Study</h2>

<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2406.12544/assets/pictures/understanding_comparison_general.png" id="S5.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>General understanding</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2406.12544/assets/pictures/understanding_comparison_deep.png" id="S5.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Deep understanding</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We conducted an online user study<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The study was preregistered in OSF: The anonymized pre-registration can be found at <a target="_blank" href="https://osf.io/db7rz/?view_only=eddd075d791848599e009961b7352e68" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://osf.io/db7rz/?view_only=eddd075d791848599e009961b7352e68</a></span></span></span>
to test the effects of gestures on the explaineeâ€™s understanding and perception of the explanation generated by the virtual agent. The study has four conditions. In the baseline condition, the agent keeps the arms at its side and moves them slightly, but does not perform any gestures. In the beat condition, the agent performs the beat gestures generated by the gesture graph algorithm (Sect. <a href="#S4" title="4. Gesture Generation â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The iconic condition consists of the baseline condition to which we added the manually captured iconic gestures (cf.Â Sect.Â <a href="#S4.SS5" title="4.5. Iconic Gestures â€£ 4. Gesture Generation â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>). Finally, the mixed condition combines the beat condition with the iconic gestures. <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The videos for each condition can be found in the OSF project <a target="_blank" href="https://osf.io/bf2yk/?view_only=704cf1241b4f4d7490bfb213a84c7fdc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://osf.io/bf2yk/?view_only=704cf1241b4f4d7490bfb213a84c7fdc</a></span></span></span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Furthermore, we compare the user understanding with results obtained with purely textual but adaptive explanations generated by the SNAPE model <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>.
We perform a between-subject online study with 50 participants in each condition. We ensure an even split between male and female participants. The study language is German. Each participant is paid â‚¬12 / hour. After excluding outliers based on completion time, the analysis has a power between 0.79 and 0.8 for a Whitney-U-Test. The user perception of the interaction and the objective understanding are measured as dependent variables. The hypotheses we test are:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H1</span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p">The user <span id="S5.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">perceives the interaction</span> as more positive in the mixed gesture condition.</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H2</span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p">The userâ€™s <span id="S5.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">understanding</span> is higher in the mixed gesture condition.</p>
</div>
</li>
<li id="S5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">H3</span> 
<div id="S5.I1.ix3.p1" class="ltx_para">
<p id="S5.I1.ix3.p1.1" class="ltx_p">The <span id="S5.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">understanding</span> in the mixed gesture condition is higher than in the previous version of the agent.</p>
</div>
</li>
</ul>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Results: Understanding</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The used understanding instrument is identical to the one used in <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>
, allowing for a comparison between the SNAPE model and our multimodal explainer. The understanding instrument tests the effects on the task performance, which is one of the testable effects described in Section <a href="#S2.SS2" title="2.2. Gestures in Virtual Agents â€£ 2. Related Work â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>. Before comparing with the adaptive agent, we compare the understanding of the different gesture conditions. The instrument contains two kinds of questions. The shallow questions test knowledge recall, in which the user has to decide whether a statement is true or false. The deep understanding test probes the userâ€™s ability to transfer the learned knowledge to in-game situations. Participants are asked to choose the best action in a given game situation shown as a picture. The questions are not only distinctive at the level of understanding depth, but also on forms of understanding <cite class="ltx_cite ltx_citemacro_citep">(Buschmeier etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. While the knowledge recall mainly tests the comprehension of the rules, the in-depth questionnaire tests to which extent the user can transfer and apply the learned knowledge: the userâ€™s enabledness. Thus, while most studies focus on the effects gestures have on long-term learning <cite class="ltx_cite ltx_citemacro_citep">(deÂ Wit etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>, this study measures the immediate understanding the user has subsequent to the interaction.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Results show, that the understanding is not significantly higher in the mixed condition. The data is not normally distributed (Shapiro Wilk Â¡ 0.05 in each condition: general: p = 0.042, shallow: p = 3.094e-06, deep: p = 3.094e-06), so we performed a Mann-Whitney-U test instead of an ANOVA and posthoc t-tests. The Mann-Whitney-U test<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>ns: p Â¡= 1.00e+00; *: 1.00e-02 Â¡ p Â¡= 5.00e-02; **: 1.00e-03 Â¡ p Â¡= 1.00e-02; ***: 1.00e-04 Â¡ p Â¡= 1.00e-03; ****: p Â¡= 1.00e-04</span></span></span> shows significant differences between the baseline and the iconic condition in the general (U = 1.302e+03, p = 3.310e-02) (Fig.<a href="#S5.F3" title="Figure 3 â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) as well as the deep understanding (U = 1.318e+03, p = 2.277e-02) (Fig.<a href="#S5.F3" title="Figure 3 â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), while the difference is not significant between the conditions in the shallow understanding. To get a better insight into this effect, we compare the conditions for each question using the Mann-Whitney-U test.
In the shallow understanding, only 3 out of 24 questions are influenced by the conditions. <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">UN09 </span>(<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">You can position any pieces on free spaces on the board.</span>, U=1.246e+03, p = 1.758e-02) and <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">UN23</span> (<span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">Once you have finished a row, you have to shout â€Doneâ€.</span>, U = 1.127e+03, p = 1.984e-02) show a significant difference between the beat and the iconic condition, while <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">UN21</span> (<span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">To win, it is important to distribute the pieces randomly on the playing field at the beginning.</span>, U = 9.080e+02, p = 3.248e-02) shows significant difference between iconic and mixed condition. In all three cases, iconic gestures produce significantly worse understanding. In contrast to the small number of significant items in the shallow understanding, 5 out of 8 test items in the deep understanding are significant: In <span id="S5.SS1.p2.1.7" class="ltx_text ltx_font_italic">UN31</span> (U = 1.231e+03, p = 2.958e-02) and <span id="S5.SS1.p2.1.8" class="ltx_text ltx_font_italic">UN32</span> (U = 1.262e+03, p = 2.774e-02) the baseline outperforms the iconic condition, in <span id="S5.SS1.p2.1.9" class="ltx_text ltx_font_italic">UN36</span> the beat condition produces higher deep understanding than the iconic (U = 1.203e+03, p = 4.183e-02) and the mixed condition (U =1.373e+03, p = 4.881e-03), while it only outperforms the mixed condition in <span id="S5.SS1.p2.1.10" class="ltx_text ltx_font_italic">UN37</span> (U = 1.225e+03, p = 4.504e-02). <span id="S5.SS1.p2.1.11" class="ltx_text ltx_font_italic">UN33</span> (U = 8.790e+02, p = 3.027e-02) is the only question, where the mixed condition outperforms the beat condition. When looking at the results for the individual deep understanding questions there is a trend for the baseline and beat condition to perform better than the iconic and the mixed condition<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The plots for understanding of each question can be found in Appendix <a href="#A2" title="Appendix B Deep Understanding â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a></span></span></span>. As shown above, this trend is significant for half of the questions. In sum, the results do not confirm the hypotheses but are rather contrary. Still, they show an interesting effect in this specific study: the understanding â€“ especially the deep enabledness â€“ is decreased by the usage of iconic gestures. Possible reasons for these results will be discussed in Section <a href="#S6" title="6. Conclusions â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Next to comparing the understanding of the different gesture conditions to each other, the understanding will now be compared to the understanding without an embodiment (<span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">baseline23</span>) and an adaptive explanation without an embodiment (<span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">adaptive23</span>). Although the explanations were given as text and the formulations were not identical to the formulations in this study, since they were generated by the SNAPE model <cite class="ltx_cite ltx_citemacro_citep">(Robrecht etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>, we consider this comparison meaningful as it allows to assess the overall explanation quality achieved by the embodied agent in this specific explanation domain.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">When comparing the general understanding of the non-embodied agent to the updated version, all updated conditions do outperform the baseline23 (baseline: U = 2.666e+03 p = 4.096e-08, beat: U = 2.490e+03 p = 1.755e-06, iconic: U = 2.264e+03 p = 5.353e-05, mixed: U = 2.600e+03, p = 1.194e-06), but only the non-iconic conditions can outperform the adaptive condition (baseline: p = 6.304e-04 U = 1.843e+03, beat: p =2.078e-02 U = 2.214e+03).
When looking at the deep understanding, the embodied conditions also perform well in comparison to the non-embodied conditions. All of the embodied conditions generate a significantly better deep understanding (baseline: U = 2.479e+03 p = 7.813e-06, beat: U = 2.302e+03 p = 1.776e-04, iconic: U = 1.971e+03 p = 1.808e-02, mixed: U = 2.288e+03, p = 1.457e-03). However, only the baseline condition can outperform the adaptive condition when it comes to deep understanding (U = 2.275e+03 p = 1.640e-02).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Results: Interaction Quality</h3>

<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/Trust.png" id="S5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Trust</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/likeability.png" id="S5.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Likeability</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/engagement.png" id="S5.F7.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Engagement</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:108.4pt;"><img src="/html/2406.12544/assets/pictures/acceptance.png" id="S5.F7.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Acceptance</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Next to the objective understanding, the users had to answer a questionnaire on their subjective perception of the interaction. This interaction questionnaire is a selection of 14 dimensions taken from the Artificial Social Agents Questionnaire (ASAQ) questionnaire <cite class="ltx_cite ltx_citemacro_citep">(Fitrianie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, and two additional, more explanation-related dimensions, the subjective understanding and the connection between agent and understanding <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>for the full questionnaire see Appendix <a href="#A1" title="Appendix A Satisfaction Questionnaire â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a></span></span></span>. It addresses social perception and communicative effects. Each of these 16 dimensions was tested with three different statements and were answered by the participant using a 5-point Likert scale.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">When comparing how the different interactions were perceived by the user, the conditions do not show substantial differences (Fig. <a href="#S5.F8" title="Figure 8 â€£ 5.2. Results: Interaction Quality â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). We only see a significant difference in 4 out of the 16 dimensions. In the level of trust the users put into the agent, the beat condition significantly outperforms the iconic condition (U = 1.256e+03 p = 4.811e-02) (Fig.<a href="#S5.F7" title="Figure 7 â€£ 5.2. Results: Interaction Quality â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). The user perceives the baseline condition to be more likable than the mixed condition (U = 1.404e+03 p = 3.841e-02) (Fig. <a href="#S5.F7" title="Figure 7 â€£ 5.2. Results: Interaction Quality â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). When it comes to engagement, the beat (U = 7.135e+02 p = 4.379e-03) and the iconic condition (U = 7.650e+02 p = 3.143e-02) are perceived as more engaging than the baseline (Fig. <a href="#S5.F7" title="Figure 7 â€£ 5.2. Results: Interaction Quality â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Also, the acceptance and willingness to use the agent again are higher in the iconic than in the baseline condition (U = 7.810e+02 p = 3.982e-02) (Fig.<a href="#S5.F7" title="Figure 7 â€£ 5.2. Results: Interaction Quality â€£ 5. Evaluation Study â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2406.12544/assets/pictures/satisfaction_radar.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="445" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Radar chart comparing the four gesture conditions</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Discussion</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The results of the present study suggest that, in the context of a board game explanation delivered by an embodied virtual agent, the use of co-speech gestures does not necessarily improve understanding. On the contrary, the non-iconic conditions (baseline and beat) tended to result in better deep enabledness of the participants than the iconic conditions (iconic and mixed). At the same time, we do see an improvement in understanding in comparison to the SNAPE model. The fact that the embodied baseline condition exceeds the previous one (baseline23) by a significant margin can either be explained by differences in explanation quality or by the added embodiment of the agent. The former is possible as SNAPE can only produce one piece of information per utterance, while our model improves explanation quality by generating utterances with a more complex structure and a higher information density, whenever reasonable (Sect. <a href="#S3" title="3. Explanation Model â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We thus take these results as evidence of a positive impact of our model, even if the explanation was not adaptive. The latter is supported by the so-called â€embodiment effectâ€ <cite class="ltx_cite ltx_citemacro_citep">(Mayer and DaPra, <a href="#bib.bib64" title="" class="ltx_ref">2012</a>)</cite> showing better learning of materials presented with an embodied character. In any case, since the difference in understanding is between the current baseline condition and the baseline23 condition, it cannot be explained by the addition of gestures.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">A comparison of perceived interaction quality between the gesture conditions does not reveal many differences. Only four of the sixteen dimensions show a significant difference. While the performed gestures improve user engagement, they decrease the likability of the agent. When looking at the userâ€™s trust, the overall type of gesture seems to have a smaller impact than the fluidity of the performed gesture. An agent using beat gestures is perceived as more trustworthy while performing iconic gestures significantly increases the probability that a user will reuse the agent compared to the baseline condition.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">In summary, hypotheses H1 and H2 must be rejected, while hypothesis H3 can be partially accepted. We see an improvement in perception in some dimensions when either beat or iconic gestures are used, but the combination never outperforms the baseline. We did not find a significantly better understanding of the mixed condition compared to any of the other conditions. In fact, the iconic condition produces a significantly worse understanding than the baseline. The extended agent outperforms the previous version, especially when looking at deep comprehension. However, it should be noted that the mixed and iconic conditions are the only gesture conditions that do not outperform the adaptive condition in general comprehension, and the difference between the adaptive condition and the baseline, beat, and iconic conditions is even greater than the difference between the mixed and adaptive conditions in deep comprehension.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we investigated how gestures affect usersâ€™ perception and understanding of explanations given by an embodied agent. Using a modified version of the SNAPE explanation generation model, coupled with a graph-based gesture generation algorithm, we could systematically adjust gesture parameters and measure participantsâ€™ comprehension. Our results show only slight differences in user perception and general understanding if confronted with different gesture types. Notably, our results reveal a significant decrease in deep understanding when presented with iconic gestures. Compared to previous research, these results are rather unexpected. Although previous research has shown that gestures can distract people and reduce performance <cite class="ltx_cite ltx_citemacro_citep">(KrÃ¤mer etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2003</a>)</cite>, most studies suggested that gestures improve the quality of an interaction <cite class="ltx_cite ltx_citemacro_citep">(Kopp, <a href="#bib.bib45" title="" class="ltx_ref">2017b</a>; Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2014</a>; van MerriÃ«nboer and Sweller, <a href="#bib.bib90" title="" class="ltx_ref">2005</a>; Dargue etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Ping and Goldin-Meadow, <a href="#bib.bib77" title="" class="ltx_ref">2008</a>)</cite>. Looking at these findings, how can the observed effects be explained?</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">One important consideration is the amount of cognitive load that the stimuli place on the participants <cite class="ltx_cite ltx_citemacro_citep">(Castro-Alonso etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2014</a>; Marcus etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2013</a>; Sweller etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">1998</a>)</cite>. In our study, participants get a verbal explanation of a complex board game in a short period of time, which is likely to incur a high cognitive load <cite class="ltx_cite ltx_citemacro_citep">(van MerriÃ«nboer and Sweller, <a href="#bib.bib90" title="" class="ltx_ref">2005</a>)</cite> and thus limit participantsâ€™ learning. Additionally, the automatically generated explanations may introduce artifacts that increase cognitive load even further <cite class="ltx_cite ltx_citemacro_citep">(Paris etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2000</a>; Johnsen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2005</a>)</cite>. In this situation, the presentation of iconic gestures that need to be integrated with verbally conveyed content might have additionally taxed cognitive resources. This is in line with <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">cognitive resource theory</span> which assumes a competition between modalities that need to be processed in parallel <cite class="ltx_cite ltx_citemacro_citep">(Wickens etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">1983</a>)</cite> or when input from one modality needs to be transferred to a task that needs another modality. This is supported by the fact that the negative effects of iconic gestures are found in participantâ€™s deep understanding, but not in the general recall questions. We may thus conclude that the combination of semantically meaningful speech and gesture might have overwhelmed participants (as reported by <cite class="ltx_cite ltx_citemacro_citet">Dargue and Sweller (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>) and that the induced cognitive load <cite class="ltx_cite ltx_citemacro_citep">(Woods etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2002</a>; Fan and Lei, <a href="#bib.bib25" title="" class="ltx_ref">2006</a>)</cite> might have prevented participants from internalizing the new information in the given time.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In contrast, shallow understanding and deep enabledness are not hampered when explanations are given along with beat gestures. This observation is in line with the finding that beat gestures are more beneficial in difficult tasks than in simple ones <cite class="ltx_cite ltx_citemacro_citep">(Gluhareva and Prieto, <a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite>, as well as with the view that iconic and metaphoric gestures are not always more beneficial to comprehension than deictic or beat gestures <cite class="ltx_cite ltx_citemacro_citep">(Dargue etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">However, it is not possible to finally answer this question as the present study only examined redundant iconic gestures that re-instantiate information already conveyed verbally, albeit apparently sometimes in ways that may have led to confusion. It is generally acknowledged that gestures can enhance communication by providing additional information <cite class="ltx_cite ltx_citemacro_citep">(Hostetter, <a href="#bib.bib35" title="" class="ltx_ref">2011</a>)</cite>. This aligns with the concept of spreading information across multiple modalities to reduce cognitive load, as discussed above (Sect. <a href="#S2.SS3" title="2.3. Cognitive Load in Multimodal Interaction â€£ 2. Related Work â€£ Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). Thus, to gain further insight into the effects of an embodied agentâ€™s synthetic gestures on listenersâ€™ cognitive load, additional studies on redundant, supplementary, and complementary gestures are necessary. Further, there are multiple metrics available for measuring cognitive load <cite class="ltx_cite ltx_citemacro_citep">(Sweller etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2011</a>)</cite>, which should be included in future studies to better delineate these effects.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Another aspect that goes beyond the scope of this paper is the extent to which the presented iconic gestures are familiar to the addressees. <cite class="ltx_cite ltx_citemacro_citet">Dargue and Sweller (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> found that a well-known gesture is easier to interpret and thus puts less load on the user. Whether this effect is found in the present board game explanations and the pre-recorded gestures will be tested in a follow-up study, measuring the familiarity with the used gestures. As for other limitations of our study, we only tested the effects of specific gestures in a specific type of interaction (an explanation) and a single domain (the board game Quarto!). This study hence cannot provide generalizable results and we are aware of studies showing different or even contrary findings. This again stresses the importance of shifting the perspective to representational gestures and learning more about the effects these gestures have or do not have in different kinds of interactions between human users and virtual multimodal agents. In addition to this, the study is only considering the effects of supplementary gestures on understanding and perceived interaction quality. By now, we cannot derive conclusions about the effects of representational gestures in explanations, which will be a research question for a follow-up study.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">In summary, this study suggests that co-speech gestures do not necessarily improve explanations given by an embodied agent. When incorporating gestures, it is thus important to consider when to use which particular type of gesture, with which particular addressee, and at which particular point in the explanation process. The more meaningful and complex a gesture may seem, the more important this is.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aussems and Kita (2019)</span>
<span class="ltx_bibblock">
Suzanne Aussems and Sotaro Kita. 2019.

</span>
<span class="ltx_bibblock">Seeing Iconic Gestures While Encoding Events Facilitates Childrenâ€™s Memory of These Events.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Child Development</em> 90, 4 (July 2019), 1123â€“1137.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/cdev.12988" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/cdev.12988</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2006.11477" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2006.11477</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bailenson and Yee (2005)</span>
<span class="ltx_bibblock">
JeremyÂ N. Bailenson and Nick Yee. 2005.

</span>
<span class="ltx_bibblock">Digital Chameleons: Automatic Assimilation of Nonverbal Gestures in Immersive Virtual Environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Psychological Science</em> 16, 10 (Oct. 2005), 814â€“819.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1467-9280.2005.01619.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1467-9280.2005.01619.x</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications Inc.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belpaeme etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tony Belpaeme, Paul Vogt, Rianne vanÂ den Berghe, Kirsten Bergmann, Tilbe GÃ¶ksun, Mirjam de Haas, Junko Kanero, James Kennedy, AylinÂ C. KÃ¼ntay, Ora Oudgenoeg-Paz, Fotios Papadopoulos, Thorsten Schodde, Josje Verhagen, ChristopherÂ D. Wallbridge, Bram Willemsen, Jan de Wit, Vasfiye GeÃ§kin, Laura Hoffmann, Stefan Kopp, Emiel Krahmer, Ezgi Mamus, Jean-Marc Montanier, Cansu OranÃ§, and AmitÂ Kumar Pandey. 2018.

</span>
<span class="ltx_bibblock">Guidelines for Designing Social Robots as Second Language Tutors.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">International Journal of Social Robotics</em> 10, 3 (June 2018), 325â€“341.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s12369-018-0467-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s12369-018-0467-6</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann and Kopp (2009)</span>
<span class="ltx_bibblock">
Kirsten Bergmann and Stefan Kopp. 2009.

</span>
<span class="ltx_bibblock">Increasing the Expressiveness of Virtual Agentsâ€“ Autonomous Generation of Speech and Gesture for Spatial Description Tasks. In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Joint Conference on Autonomous Agents and Multiagent Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Kirsten Bergmann, Stefan Kopp, and Friederike Eyssel. 2010.

</span>
<span class="ltx_bibblock">Individualized Gesturing Outperforms Average Gesturing â€“ Evaluating Gesture Production in Virtual Humans. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Intelligent Virtual Agents</em>, Jan Allbeck, Norman Badler, Timothy Bickmore, Catherine Pelachaud, and Alla Safonova (Eds.). Springer, Berlin, Heidelberg, 104â€“117.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-642-15892-6_11" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-642-15892-6_11</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergmann and Macedonia (2013)</span>
<span class="ltx_bibblock">
Kirsten Bergmann and Manuela Macedonia. 2013.

</span>
<span class="ltx_bibblock">A Virtual Agent as Vocabulary Trainer: Iconic Gestures Help to Improve Learnersâ€™ Memory Performance. In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Conference on Intelligent Virtual Agents</em>, Ruth Aylett, Brigitte Krenn, Catherine Pelachaud, and Hiroshi Shimodaira (Eds.). Springer, Berlin, Heidelberg, 139â€“148.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-642-40415-3_12" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-642-40415-3_12</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock">Enriching Word Vectors with Subword Information.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1607.04606" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1607.04606</a>

</span>
<span class="ltx_bibblock">arXiv:1607.04606 [cs].

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bosker and Peeters (2021)</span>
<span class="ltx_bibblock">
HansÂ Rutger Bosker and David Peeters. 2021.

</span>
<span class="ltx_bibblock">Beat gestures influence which speech sounds you hear.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Royal Society B: Biological Sciences</em> 288, 1943 (Jan. 2021), 20202419.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1098/rspb.2020.2419" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1098/rspb.2020.2419</a>

</span>
<span class="ltx_bibblock">Publisher: Royal Society.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BreckinridgeÂ Church etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Ruth BreckinridgeÂ Church, Philip Garber, and Kathryn Rogalski. 2007.

</span>
<span class="ltx_bibblock">The role of gesture in memory and social communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Gesture</em> 7, 2 (June 2007), 137â€“158.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1075/gest.7.2.02bre" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/gest.7.2.02bre</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buschmeier etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hendrik Buschmeier, HeikeÂ M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, AndrÃ© GroÃŸ, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, and Britta Wrede. 2023.

</span>
<span class="ltx_bibblock">Forms of Understanding of XAI-Explanations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2311.08760" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2311.08760</a>

</span>
<span class="ltx_bibblock">arXiv:2311.08760 [cs].

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (1999)</span>
<span class="ltx_bibblock">
Justine Cassell, David McNeill, and Karl-Erik McCullough. 1999.

</span>
<span class="ltx_bibblock">Speech-gesture mismatches: Evidence for one underlying representation of linguistic and nonlinguistic information.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Pragmatics &amp; cognition</em> 7, 1 (1999), 1â€“34.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1075/pc.7.1.03cas" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/pc.7.1.03cas</a>

</span>
<span class="ltx_bibblock">Publisher: John Benjamins.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (1994)</span>
<span class="ltx_bibblock">
Justine Cassell, Catherine Pelachaud, Norman Badler, Mark Steedman, Brett Achorn, Tripp Becket, Brett Douville, Scott Prevost, and Matthew Stone. 1994.

</span>
<span class="ltx_bibblock">Animated conversation: rule-based generation of facial expression, gesture &amp; spoken intonation for multiple conversational agents. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st annual conference on Computer graphics and interactive techniques</em> <em id="bib.bib14.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH â€™94)</em>. Association for Computing Machinery, New York, NY, USA, 413â€“420.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/192161.192272" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/192161.192272</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Justine Cassell, HannesÂ HÃ¶gni VilhjÃ¡lmsson, and Timothy Bickmore. 2004.

</span>
<span class="ltx_bibblock">BEAT: the Behavior Expression Animation Toolkit.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Life-Like Characters: Tools, Affective Functions, and Applications</em>, Helmut Prendinger and Mitsuru Ishizuka (Eds.). Springer, Berlin, Heidelberg, 163â€“185.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-662-08373-4_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-662-08373-4_8</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castellano (2024)</span>
<span class="ltx_bibblock">
Brandon Castellano. 2024.

</span>
<span class="ltx_bibblock">Home - PySceneDetect.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.scenedetect.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.scenedetect.com/</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro-Alonso etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
JuanÂ C. Castro-Alonso, Paul Ayres, and Fred Paas. 2014.

</span>
<span class="ltx_bibblock">Learning from observing hands in static and animated versions of non-manipulative tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Learning and Instruction</em> 34 (Dec. 2014), 11â€“21.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.learninstruc.2014.07.005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.learninstruc.2014.07.005</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Fang Chen, Natalie Ruiz, Eric Choi, Julien Epps, M.Â Asif Khawaja, Ronnie Taib, Bo Yin, and Yang Wang. 2012.

</span>
<span class="ltx_bibblock">Multimodal behavior and interaction as indicators of cognitive load.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Interactive Intelligent Systems</em> 2, 4 (Dec. 2012), 1â€“36.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2395123.2395127" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2395123.2395127</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherti etÂ al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2022.

</span>
<span class="ltx_bibblock">Reproducible scaling laws for contrastive language-image learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2212.07143" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2212.07143</a>

</span>
<span class="ltx_bibblock">arXiv:2212.07143 [cs].

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Community (2018)</span>
<span class="ltx_bibblock">
BlenderÂ Online Community. 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Blender - a 3D modelling and rendering package</em>.

</span>
<span class="ltx_bibblock">Blender Foundation, Stichting Blender Foundation, Amsterdam.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://www.blender.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.blender.org</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dargue and Sweller (2018)</span>
<span class="ltx_bibblock">
Nicole Dargue and Naomi Sweller. 2018.

</span>
<span class="ltx_bibblock">Not All Gestures are Created Equal: The Effects of Typical and Atypical Iconic Gestures on Narrative Comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Nonverbal Behavior</em> 42 (Sept. 2018), 1â€“19.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s10919-018-0278-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10919-018-0278-3</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dargue etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nicole Dargue, Naomi Sweller, and Michael Jones. 2019.

</span>
<span class="ltx_bibblock">When Our Hands Help Us Understand: A Meta-Analysis Into the Effects of Gesture on Comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Psychological Bulletin</em> 145 (June 2019).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1037/bul0000202" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/bul0000202</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis (2018)</span>
<span class="ltx_bibblock">
RobertÂ O. Davis. 2018.

</span>
<span class="ltx_bibblock">The impact of pedagogical agent gesturing in multimedia learning environments: A meta-analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Educational Research Review</em> (2018).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/J.EDUREV.2018.05.002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/J.EDUREV.2018.05.002</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">deÂ Wit etÂ al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jan de Wit, Thorsten Schodde, Bram Willemsen, Kirsten Bergmann, Mirjam de Haas, Stefan Kopp, Emiel Krahmer, and Paul Vogt. 2018.

</span>
<span class="ltx_bibblock">The Effect of a Robotâ€™s Gestures and Adaptive Tutoring on Childrenâ€™s Acquisition of Second Language Vocabularies. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction</em> <em id="bib.bib24.4.2" class="ltx_emph ltx_font_italic">(HRI â€™18)</em>. Association for Computing Machinery, New York, NY, USA, 50â€“58.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3171221.3171277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3171221.3171277</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan and Lei (2006)</span>
<span class="ltx_bibblock">
Lisa Fan and Minxiao Lei. 2006.

</span>
<span class="ltx_bibblock">Reducing Cognitive Overload by Meta-Learning Assisted Algorithm Selection. In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2006 5th IEEE International Conference on Cognitive Informatics</em>, Vol.Â 1. 120â€“125.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/COGINF.2006.365686" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/COGINF.2006.365686</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fisher etÂ al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
JosephineÂ B Fisher, AmelieÂ S Robrecht, Stefan Kopp, and KatharinaÂ J Rohlfing. 2023.

</span>
<span class="ltx_bibblock">Exploring the Semantic Dialogue Patterns of Explanations â€“ a Case Study of Game Explanations. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th Workshop on the Semantics and Pragmatics of Dialogue (SemDial 2023)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fitrianie etÂ al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Siska Fitrianie, Merijn Bruijnes, Fengxiang Li, Amal Abdulrahman, and Willem-Paul Brinkman. 2022.

</span>
<span class="ltx_bibblock">The artificial-social-agent questionnaire: establishing the long and short questionnaire versions. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents</em>. ACM, Faro Portugal, 1â€“8.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3514197.3549612" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3514197.3549612</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">foundation (2024a)</span>
<span class="ltx_bibblock">
TED foundation. 2024a.

</span>
<span class="ltx_bibblock">TED - YouTube.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.youtube.com/channel/UCAuUUnT6oDeKwE6v1NGQxug" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/channel/UCAuUUnT6oDeKwE6v1NGQxug</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">foundation (2024b)</span>
<span class="ltx_bibblock">
TED foundation. 2024b.

</span>
<span class="ltx_bibblock">TEDx Talks - YouTube.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.youtube.com/user/tedxtalks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/user/tedxtalks</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
SamirÂ Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, PangÂ Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis,
Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. 2023.

</span>
<span class="ltx_bibblock">DataComp: In search of the next generation of multimodal datasets.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2304.14108" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2304.14108</a>

</span>
<span class="ltx_bibblock">arXiv:2304.14108 [cs].

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gluhareva and Prieto (2016)</span>
<span class="ltx_bibblock">
Daria Gluhareva and Pilar Prieto. 2016.

</span>
<span class="ltx_bibblock">Training with rhythmic beat gestures benefits L2 pronunciation in discourse-demanding situations.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Language Teaching Research</em> 21 (June 2016).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/1362168816651463" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/1362168816651463</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldin-Meadow etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Susan Goldin-Meadow, SusanÂ Wagner Cook, and ZacharyÂ A. Mitchell. 2009.

</span>
<span class="ltx_bibblock">Gesturing Gives Children New Ideas About Math.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Psychological Science</em> 20, 3 (March 2009), 267â€“272.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1467-9280.2009.02297.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1467-9280.2009.02297.x</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gratch etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Jonathan Gratch, Ning Wang, Jillian Gerten, Edward Fast, and Robin Duffy. 2007.

</span>
<span class="ltx_bibblock">Creating Rapport with Virtual Agents. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Prodeedings of the 7th International Conference in Intelligent Virtual Agents</em>, Catherine Pelachaud, Jean-Claude Martin, Elisabeth AndrÃ©, GÃ©rard Chollet, Kostas Karpouzis, and Danielle PelÃ© (Eds.). Springer, Berlin, Heidelberg, 125â€“138.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_12" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_12</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Group (2024)</span>
<span class="ltx_bibblock">
PostgreSQL GlobalÂ Development Group. 2024.

</span>
<span class="ltx_bibblock">PostgreSQL.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.postgresql.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.postgresql.org/</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hostetter (2011)</span>
<span class="ltx_bibblock">
AutumnÂ B. Hostetter. 2011.

</span>
<span class="ltx_bibblock">When do gestures communicate? A meta-analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Psychological Bulletin</em> 137, 2 (March 2011), 297â€“315.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/a0022128" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/a0022128</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hostetter and Bahl (2023)</span>
<span class="ltx_bibblock">
AutumnÂ B. Hostetter and Sonal Bahl. 2023.

</span>
<span class="ltx_bibblock">Comparing the cognitive load of gesture and action production: a dual-task study.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Language and Cognition</em> 15, 3 (Sept. 2023), 601â€“621.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1017/langcog.2023.23" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/langcog.2023.23</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnsen etÂ al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
K. Johnsen, R. Dickerson, A. Raij, B. Lok, J. Jackson, Min Shin, J. Hernandez, A. Stevens, and D.S. Lind. 2005.

</span>
<span class="ltx_bibblock">Experiences in using immersive virtual characters to educate medical communication skills. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">IEEE Proceedings. VR 2005. Virtual Reality, 2005.</em> 179â€“186.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/VR.2005.1492772" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/VR.2005.1492772</a>

</span>
<span class="ltx_bibblock">ISSN: 2375-5334.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnstone (1994)</span>
<span class="ltx_bibblock">
Barbara Johnstone (Ed.). 1994.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Repetition in discourse: interdisciplinary perspectives</em>.

</span>
<span class="ltx_bibblock">Number v.47-48 in Advances in discourse processes. Ablex Pub. Co, Norwood, N.J.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016.

</span>
<span class="ltx_bibblock">Bag of Tricks for Efficient Text Classification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1607.01759" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1607.01759</a>

</span>
<span class="ltx_bibblock">arXiv:1607.01759 [cs].

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kane (2024)</span>
<span class="ltx_bibblock">
Andrew Kane. 2024.

</span>
<span class="ltx_bibblock">pgvector/pgvector.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://github.com/pgvector/pgvector" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pgvector/pgvector</a>

</span>
<span class="ltx_bibblock">original-date: 2021-04-20T21:13:52Z.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kelly etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
S. Kelly, Corinne Kravitz, and Michael Hopkins. 2004.

</span>
<span class="ltx_bibblock">Neural correlates of bimodal speech and gesture comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Brain and Language</em> 89 (2004), 253â€“260.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/S0093-934X(03)00335-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0093-934X(03)00335-3</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2022)</span>
<span class="ltx_bibblock">
DiederikÂ P. Kingma and Max Welling. 2022.

</span>
<span class="ltx_bibblock">Auto-Encoding Variational Bayes.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1312.6114" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1312.6114</a>

</span>
<span class="ltx_bibblock">arXiv:1312.6114 [cs, stat].

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kita and Ã–zyÃ¼rek (2003)</span>
<span class="ltx_bibblock">
Sotaro Kita and Asli Ã–zyÃ¼rek. 2003.

</span>
<span class="ltx_bibblock">What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Journal of Memory and Language</em> 48, 1 (Jan. 2003), 16â€“32.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/S0749-596X(02)00505-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0749-596X(02)00505-3</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp (2017a)</span>
<span class="ltx_bibblock">
Stefan Kopp. 2017a.

</span>
<span class="ltx_bibblock">Chapter 12. Computational gesture research: Studying the functions of gesture in human-agent interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Gesture Studies</em>, R.Â Breckinridge Church, MarthaÂ W. Alibali, and SpencerÂ D. Kelly (Eds.). Vol.Â 7. John Benjamins Publishing Company, Amsterdam, 267â€“284.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1075/gs.7.13kop" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1075/gs.7.13kop</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp (2017b)</span>
<span class="ltx_bibblock">
Stefan Kopp. 2017b.

</span>
<span class="ltx_bibblock">Computational gesture research: Studying the functions of gesture in human-agent interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Why Gesture?</em> John Benjamins, 267â€“284.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.jbe-platform.com/content/books/9789027265777-gs.7.13kop" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.jbe-platform.com/content/books/9789027265777-gs.7.13kop</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Stefan Kopp, Brigitte Krenn, Stacy Marsella, Andrew Marshall, Catherine Pelachaud, Hannes Pirker, Kristinn ThÃ³risson, and Hannes VilhjÃ¡lmsson. 2006.

</span>
<span class="ltx_bibblock">Towards a Common Framework for Multimodal Generation: The Behavior Markup Language, Vol.Â 4133. 205â€“217.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/11821830_17" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/11821830_17</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krieglstein etÂ al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Felix Krieglstein, Maik Beege, GÃ¼nterÂ Daniel Rey, Paul Ginns, Moritz Krell, and Sascha Schneider. 2022.

</span>
<span class="ltx_bibblock">A Systematic Meta-analysis of the Reliability and Validity of Subjective Cognitive Load Questionnaires in Experimental Multimedia Learning Research.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 34, 4 (Dec. 2022), 2485â€“2541.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10648-022-09683-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10648-022-09683-4</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KrÃ¤mer etÂ al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Nicole KrÃ¤mer, Nina Simons, and Stefan Kopp. 2007.

</span>
<span class="ltx_bibblock">The Effects of an Embodied Conversational Agentâ€™s Nonverbal Behavior on Userâ€™s Evaluation and Behavioral Mimicry. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th International Conference on Intelligent Virtual Agents</em>. 238â€“251.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_22" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_22</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KrÃ¤mer etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2003)</span>
<span class="ltx_bibblock">
NicoleÂ C. KrÃ¤mer, Bernd Tietz, and Gary Bente. 2003.

</span>
<span class="ltx_bibblock">Effects of Embodied Interface Agents and Their Gestural Activity. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Intelligent Virtual Agents</em>, Thomas Rist, RuthÂ S. Aylett, Daniel Ballin, and Jeff Rickel (Eds.). Springer, Berlin, Heidelberg, 292â€“300.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-540-39396-2_49" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-39396-2_49</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kucherenko etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Taras Kucherenko, Rajmund Nagy, Youngwoo Yoon, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, and GustavÂ Eje Henter. 2023.

</span>
<span class="ltx_bibblock">The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2308.12646" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2308.12646</a>

</span>
<span class="ltx_bibblock">arXiv:2308.12646 [cs].

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurokawa (1992)</span>
<span class="ltx_bibblock">
Takao Kurokawa. 1992.

</span>
<span class="ltx_bibblock">Gesture Coding and a Gesture Dictionary for a Nonverbal Interface.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences</em> E75-A, 2 (Feb. 1992), 112â€“121.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://search.ieice.org/bin/summary.php?id=e75-a_2_112&amp;category=A&amp;year=1992&amp;lang=E&amp;abst=" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://search.ieice.org/bin/summary.php?id=e75-a_2_112&amp;category=A&amp;year=1992&amp;lang=E&amp;abst=</a>

</span>
<span class="ltx_bibblock">Publisher: The Institute of Electronics, Information and Communication Engineers.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. 2023.

</span>
<span class="ltx_bibblock">One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer. In <em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 21159â€“21168.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. 2022.

</span>
<span class="ltx_bibblock">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.05297</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kris Liu, Jackson Tolins, JeanÂ E. FoxÂ Tree, Michael Neff, and MarilynÂ A. Walker. 2016.

</span>
<span class="ltx_bibblock">Two Techniques for Assessing Virtual Agent Personality.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 7, 1 (Jan. 2016), 94â€“105.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TAFFC.2015.2435780" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TAFFC.2015.2435780</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Liu, Gelareh Mohammadi, Yang Song, and Wafa Johal. 2021.

</span>
<span class="ltx_bibblock">Speech-based Gesture Generation for Robots and Embodied Agents: A Scoping Review. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th International Conference on Human-Agent Interaction</em> <em id="bib.bib55.4.2" class="ltx_emph ltx_font_italic">(HAI â€™21)</em>. Association for Computing Machinery, New York, NY, USA, 31â€“38.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3472307.3484167" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472307.3484167</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lund (2007)</span>
<span class="ltx_bibblock">
Kristine Lund. 2007.

</span>
<span class="ltx_bibblock">The importance of gaze and gesture in interactive multimodal explanation.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em> 41, 3-4 (Dec. 2007), 289â€“303.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10579-007-9058-0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10579-007-9058-0</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LÃ¼cking etÂ al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Andy LÃ¼cking, Kirsten Bergmann, Florian Hahn, Stefan Kopp, and Hannes Rieser. 2010.

</span>
<span class="ltx_bibblock">The Bielefeld Speech and Gesture Alignment Corpus (SaGA).

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">LREC 2010 Workshop: Multimodal Corporaâ€“Advances in Capturing, Coding and Analyzing Multimodality</em> (2010).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://pub.uni-bielefeld.de/record/2001935" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pub.uni-bielefeld.de/record/2001935</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens vanÂ der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock">Visualizing Data using t-SNE.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em> 9, 86 (2008), 2579â€“2605.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://jmlr.org/papers/v9/vandermaaten08a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v9/vandermaaten08a.html</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macedonia (2014)</span>
<span class="ltx_bibblock">
Manuela Macedonia. 2014.

</span>
<span class="ltx_bibblock">Imitation of a Pedagogical Agentâ€™s Gestures Enhances Memory for Words in Second Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Science Journal of Education</em> 2, 5 (2014), 162.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.11648/j.sjedu.20140205.15" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.11648/j.sjedu.20140205.15</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magid and Pyers (2017)</span>
<span class="ltx_bibblock">
RachelÂ W. Magid and JennieÂ E. Pyers. 2017.

</span>
<span class="ltx_bibblock">â€œI use it when I see itâ€: The role of development and experience in Deaf and hearing childrenâ€™s understanding of iconic gesture.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Cognition</em> 162 (May 2017), 73â€“86.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.cognition.2017.01.015" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.cognition.2017.01.015</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2018)</span>
<span class="ltx_bibblock">
YuÂ A. Malkov and D.Â A. Yashunin. 2018.

</span>
<span class="ltx_bibblock">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1603.09320" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1603.09320</a>

</span>
<span class="ltx_bibblock">arXiv:1603.09320 [cs].

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus etÂ al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Nadine Marcus, Bejay Cleary, Anna Wong, and Paul Ayres. 2013.

</span>
<span class="ltx_bibblock">Should hand actions be observed when learning hand motor skills from instructional animations?

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 29, 6 (Nov. 2013), 2172â€“2178.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.chb.2013.04.035" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.chb.2013.04.035</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matthews-Saugstad etÂ al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
KristaÂ M. Matthews-Saugstad, ErikÂ P. Raymakers, and DamianÂ G. Kelty-Stephen. 2017.

</span>
<span class="ltx_bibblock">Gesturing more diminishes recall of abstract words when gesture is allowed and concrete words when it is taboo.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">Quarterly Journal of Experimental Psychology</em> 70, 7 (July 2017), 1099â€“1105.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/17470218.2016.1263997" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/17470218.2016.1263997</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer and DaPra (2012)</span>
<span class="ltx_bibblock">
RichardÂ E. Mayer and C.Â Scott DaPra. 2012.

</span>
<span class="ltx_bibblock">An embodiment effect in computer-based learning with animated pedagogical agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Experimental Psychology: Applied</em> 18, 3 (2012), 239â€“252.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/a0028616" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/a0028616</a>

</span>
<span class="ltx_bibblock">Place: US Publisher: American Psychological Association.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer and Moreno (1998)</span>
<span class="ltx_bibblock">
RichardÂ E. Mayer and Roxana Moreno. 1998.

</span>
<span class="ltx_bibblock">A split-attention effect in multimedia learning: Evidence for dual processing systems in working memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Journal of Educational Psychology</em> 90, 2 (June 1998), 312â€“320.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-0663.90.2.312" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-0663.90.2.312</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McInnes etÂ al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Leland McInnes, John Healy, and James Melville. 2020.

</span>
<span class="ltx_bibblock">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.1802.03426" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.1802.03426</a>

</span>
<span class="ltx_bibblock">arXiv:1802.03426 [cs, stat].

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McNeill (1985)</span>
<span class="ltx_bibblock">
David McNeill. 1985.

</span>
<span class="ltx_bibblock">So you think gestures are nonverbal?

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Psychological Review</em> 92, 3 (1985), 350â€“371.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0033-295X.92.3.350" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0033-295X.92.3.350</a>

</span>
<span class="ltx_bibblock">Place: US Publisher: American Psychological Association.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mcneill (1986)</span>
<span class="ltx_bibblock">
David Mcneill. 1986.

</span>
<span class="ltx_bibblock">Iconic gestures of children and adults.

</span>
<span class="ltx_bibblock">62, 1-2 (Jan. 1986), 107â€“128.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1515/semi.1986.62.1-2.107" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1515/semi.1986.62.1-2.107</a>

</span>
<span class="ltx_bibblock">Publisher: De Gruyter Mouton Section: Semiotica.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavi etÂ al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (1995)</span>
<span class="ltx_bibblock">
SeyedÂ Yaghoub Mousavi, Renae Low, and John Sweller. 1995.

</span>
<span class="ltx_bibblock">Reducing cognitive load by mixing auditory and visual presentation modes.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Journal of Educational Psychology</em> 87, 2 (June 1995), 319â€“334.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-0663.87.2.319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-0663.87.2.319</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neff etÂ al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Michael Neff, Yingying Wang, Rob Abbott, and Marilyn Walker. 2010.

</span>
<span class="ltx_bibblock">Evaluating the effect of gesture and language on personality perception in conversational agents. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Intelligent Virtual Agents: 10th International Conference, IVA 2010, Philadelphia, PA, USA, September 20-22, 2010. Proceedings 10</em>. Springer, 222â€“235.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nyatsanga etÂ al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Simbarashe Nyatsanga, Taras Kucherenko, Chaitanya Ahuja, GustavÂ Eje Henter, and Michael Neff. 2023.

</span>
<span class="ltx_bibblock">A Comprehensive Review of Data-Driven Co-Speech Gesture Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2301.05339" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2301.05339</a>

</span>
<span class="ltx_bibblock">arXiv:2301.05339 [cs].

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oviatt etÂ al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Sharon Oviatt, Rachel Coulston, and Rebecca Lunsford. 2024.

</span>
<span class="ltx_bibblock">When Do We Interact Multimodally? Cognitive Load and Multimodal Communication Patterns. In <em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th International Conference on Multimodal Interfaces</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paris etÂ al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
CarolÂ R. Paris, MargaretÂ H. Thomas, RichardÂ D. Gilson, and J.Â Peter Kincaid. 2000.

</span>
<span class="ltx_bibblock">Linguistic Cues and Memory for Synthetic and Natural Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">Human Factors</em> 42, 3 (Sept. 2000), 421â€“431.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1518/001872000779698132" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1518/001872000779698132</a>

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications Inc.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parise etÂ al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (1999)</span>
<span class="ltx_bibblock">
Salvatore Parise, S. Kiesler, L. Sproull, and Keith Waters. 1999.

</span>
<span class="ltx_bibblock">Cooperating with life-like interface agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 15 (1999), 123â€“142.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/S0747-5632(98)00035-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0747-5632(98)00035-1</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perry etÂ al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (1988)</span>
<span class="ltx_bibblock">
Michelle Perry, R. BreckinridgeÂ Church, and Susan Goldin-Meadow. 1988.

</span>
<span class="ltx_bibblock">Transitional knowledge in the acquisition of concepts.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Cognitive Development</em> 3, 4 (Oct. 1988), 359â€“400.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0885-2014(88)90021-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0885-2014(88)90021-4</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping and Goldin-Meadow (2010)</span>
<span class="ltx_bibblock">
Raedy Ping and Susan Goldin-Meadow. 2010.

</span>
<span class="ltx_bibblock">Gesturing Saves Cognitive Resources When Talking About Nonpresent Objects.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Cognitive Science</em> 34, 4 (2010), 602â€“619.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1551-6709.2010.01102.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1551-6709.2010.01102.x</a>

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6709.2010.01102.x.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping and Goldin-Meadow (2008)</span>
<span class="ltx_bibblock">
RaedyÂ M. Ping and Susan Goldin-Meadow. 2008.

</span>
<span class="ltx_bibblock">Hands in the air: using ungrounded iconic gestures to teach children conservation of quantity.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Developmental Psychology</em> 44, 5 (Sept. 2008), 1277â€“1287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0012-1649.44.5.1277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0012-1649.44.5.1277</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2103.00020" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2103.00020</a>

</span>
<span class="ltx_bibblock">arXiv:2103.00020 [cs].

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robrecht and Kopp (2023)</span>
<span class="ltx_bibblock">
Amelie Robrecht and Stefan Kopp. 2023.

</span>
<span class="ltx_bibblock">SNAPE: A Sequential Non-Stationary Decision Process Model for Adaptive Explanation Generation:. In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th International Conference on Agents and Artificial Intelligence</em>. SCITEPRESS - Science and Technology Publications, Lisbon, Portugal, 48â€“58.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.5220/0011671300003393" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5220/0011671300003393</a>

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robrecht etÂ al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
AmelieÂ Sophie Robrecht, Markus RothgÃ¤nger, and Stefan Kopp. 2023.

</span>
<span class="ltx_bibblock">A Study on the Benefits and Drawbacks ofAdaptivity in AI-generated Explanations. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3570945.3607339" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3570945.3607339</a>

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohlfing etÂ al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
KatharinaÂ J. Rohlfing, Philipp Cimiano, Ingrid Scharlau, Tobias Matzner, HeikeÂ M. Buhl, Hendrik Buschmeier, Elena Esposito, Angela Grimminger, Barbara Hammer, Reinhold Hab-Umbach, Ilona Horwath, Eyke HÃ¼llermeier, Friederike Kern, Stefan Kopp, Kirsten Thommes, AxelÂ Cyrille NgongaÂ Ngomo, Carsten Schulte, Henning Wachsmuth, Petra Wagner, and Britta Wrede. 2021.

</span>
<span class="ltx_bibblock">Explanation as a Social Practice: Toward a Conceptual Framework for the Social Design of AI Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive and Developmental Systems</em> 13, 3 (Sept. 2021), 717â€“728.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TCDS.2020.3044366" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TCDS.2020.3044366</a>

</span>
<span class="ltx_bibblock">Publisher: Institute of Electrical and Electronics Engineers Inc..

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohlfing etÂ al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
KatharinaÂ J. Rohlfing, Jannik Fritsch, Britta Wrede, and Tanja Jungmann. 2006.

</span>
<span class="ltx_bibblock">How can multimodal cues from child-directed interaction reduce learning complexity in robots?

</span>
<span class="ltx_bibblock"><em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Advanced Robotics</em> 20, 10 (Jan. 2006), 1183â€“1199.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1163/156855306778522532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1163/156855306778522532</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal-vonÂ der PÃ¼tten and Bergmann (2020)</span>
<span class="ltx_bibblock">
AstridÂ M. Rosenthal-vonÂ der PÃ¼tten and Kirsten Bergmann. 2020.

</span>
<span class="ltx_bibblock">Non-verbal Enrichment in Vocabulary Learning With a Virtual Pedagogical Agent.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Frontiers in Psychology</em> 11 (Nov. 2020).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/fpsyg.2020.533839" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/fpsyg.2020.533839</a>

</span>
<span class="ltx_bibblock">Publisher: Frontiers.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salem etÂ al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Maha Salem, Stefan Kopp, Ipke Wachsmuth, Katharina Rohlfing, and Frank Joublin. 2012.

</span>
<span class="ltx_bibblock">Generation and Evaluation of Communicative Robot Gesture.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">International Journal of Social Robotics</em> 4, 2 (April 2012), 201â€“217.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s12369-011-0124-9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s12369-011-0124-9</a>

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinatra etÂ al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
AnneÂ M. Sinatra, KimberlyÂ A. Pollard, BenjaminÂ T. Files, AshleyÂ H. Oiknine, Mark Ericson, and Peter Khooshabeh. 2021.

</span>
<span class="ltx_bibblock">Social fidelity in virtual agents: Impacts on presence and learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 114 (Jan. 2021), 106562.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.chb.2020.106562" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.chb.2020.106562</a>

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sweller etÂ al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
John Sweller, Paul Ayres, and Slava Kalyuga. 2011.

</span>
<span class="ltx_bibblock">Measuring Cognitive Load.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">Cognitive Load Theory</em>, John Sweller, Paul Ayres, and Slava Kalyuga (Eds.). Springer, New York, NY, 71â€“85.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-1-4419-8126-4_6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-1-4419-8126-4_6</a>

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sweller etÂ al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (1998)</span>
<span class="ltx_bibblock">
John Sweller, Jeroen J.Â G. van Merrienboer, and Fred G. W.Â C. Paas. 1998.

</span>
<span class="ltx_bibblock">Cognitive Architecture and Instructional Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 10, 3 (Sept. 1998), 251â€“296.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1023/A:1022193728205" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1023/A:1022193728205</a>

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tindall-Ford etÂ al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (1997)</span>
<span class="ltx_bibblock">
Sharon Tindall-Ford, Paul Chandler, and John Sweller. 1997.

</span>
<span class="ltx_bibblock">When two sensory modes are better than one.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">Journal of Experimental Psychology: Applied</em> 3, 4 (Dec. 1997), 257â€“287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/1076-898X.3.4.257" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/1076-898X.3.4.257</a>

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2307.09288</a>

</span>
<span class="ltx_bibblock">arXiv:2307.09288 [cs].

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van MerriÃ«nboer and Sweller (2005)</span>
<span class="ltx_bibblock">
Jeroen J.Â G. van MerriÃ«nboer and John Sweller. 2005.

</span>
<span class="ltx_bibblock">Cognitive Load Theory and Complex Learning: Recent Developments and Future Directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Educational Psychology Review</em> 17, 2 (June 2005), 147â€“177.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10648-005-3951-0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10648-005-3951-0</a>

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilhjalmsson etÂ al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
H. Vilhjalmsson, N. Cantelmo, J. Cassell, N.Â E. Chafai, M. Kipp, and Stefan Kopp. 2007.

</span>
<span class="ltx_bibblock">The Behavior Markup Language: Recent Developments and Challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">Proc. of Intelligent Virtual Agents (IVA 2007)</em> 4722 (2007).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-3-540-74997-4_10" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-74997-4_10</a>

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VoÃŸ and Kopp (2023a)</span>
<span class="ltx_bibblock">
Hendric VoÃŸ and Stefan Kopp. 2023a.

</span>
<span class="ltx_bibblock">AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th International Conference on Multimodal Interaction</em> <em id="bib.bib92.2.2" class="ltx_emph ltx_font_italic">(ICMI â€™23)</em>. Association for Computing Machinery, New York, NY, USA, 60â€“69.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3577190.3614135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3577190.3614135</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VoÃŸ and Kopp (2023b)</span>
<span class="ltx_bibblock">
Hendric VoÃŸ and Stefan Kopp. 2023b.

</span>
<span class="ltx_bibblock">Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis. In <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3570945.3607337" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3570945.3607337</a>

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner etÂ al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Petra Wagner, Zofia Malisz, and Stefan Kopp. 2014.

</span>
<span class="ltx_bibblock">Gesture and speech in interaction: An overview.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Pages: 209â€“232 Publication Title: Speech Communication Volume: 57.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2002.10957" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2002.10957</a>

</span>
<span class="ltx_bibblock">arXiv:2002.10957 [cs].

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. 2021.

</span>
<span class="ltx_bibblock">Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em> 22, 201 (2021), 1â€“73.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://jmlr.org/papers/v22/20-1061.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v22/20-1061.html</a>

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wickens etÂ al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (1983)</span>
<span class="ltx_bibblock">
ChristopherÂ D. Wickens, DianeÂ L. Sandry, and Michael Vidulich. 1983.

</span>
<span class="ltx_bibblock">Compatibility and Resource Competition between Modalities of Input, Central Processing, and Output.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">Human Factors: The Journal of the Human Factors and Ergonomics Society</em> 25, 2 (April 1983), 227â€“248.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1177/001872088302500209" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/001872088302500209</a>

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woods etÂ al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
David Woods, Emily Patterson, and Emilie Roth. 2002.

</span>
<span class="ltx_bibblock">Can We Ever Escape from Data Overload? A Cognitive Systems Diagnosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">Cognition, Technology &amp; Work</em> 4 (April 2002), 22â€“36.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s101110200002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s101110200002</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Yanxiang Wu, SabarishÂ V. Babu, Rowan Armstrong, JeffreyÂ W. Bertrand, Jun Luo, Tania Roy, ShaundraÂ B. Daily, LaurenÂ Cairco Dukes, LarryÂ F. Hodges, and Tracy Fasolino. 2014.

</span>
<span class="ltx_bibblock">Effects of virtual human animation on emotion contagion in simulated inter-personal experiences.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on visualization and computer graphics</em> 20, 4 (April 2014), 626â€“635.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TVCG.2014.19" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TVCG.2014.19</a>

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon etÂ al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and GustavÂ Eje Henter. 2022.

</span>
<span class="ltx_bibblock">The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation. In <em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 International Conference on Multimodal Interaction</em>. 736â€“747.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3536221.3558058" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558058</a>

</span>
<span class="ltx_bibblock">arXiv:2208.10441 [cs, eess].

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, and Shuwu Zhang. 2023.

</span>
<span class="ltx_bibblock">Gesture Motion Graphs for Few-Shot Speech-Driven Gesture Reenactment.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=CMivR3x5fpC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=CMivR3x5fpC</a>

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chi Zhou, Tengyue Bian, and Kang Chen. 2022.

</span>
<span class="ltx_bibblock">GestureMaster: Graph-based Speech-driven Gesture Generation. In <em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION</em>. ACM, Bengaluru India, 764â€“770.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3536221.3558063" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558063</a>

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zvaigzne etÂ al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Meghan Zvaigzne, Yuriko Oshima-Takane, and Makiko Hirakawa. 2019.

</span>
<span class="ltx_bibblock">How does language proficiency affect childrenâ€™s iconic gesture use?

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">Applied Psycholinguistics</em> 40, 2 (March 2019), 555â€“583.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1017/S014271641800070X" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/S014271641800070X</a>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Satisfaction Questionnaire</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The satisfaction questionnaire is a selection of questions taken from the Artificial Social Agents Questionnaire <cite class="ltx_cite ltx_citemacro_citep">(Fitrianie etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, which are translated to German by the authors. Here we will list all original questions used, the test item they are connected to and the translation. In the study the questions were mixed in random order.</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Agentâ€™s Believability</span></p>
<ul id="A1.I1.i1.I1" class="ltx_itemize">
<li id="A1.I1.i1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">HLB3</span> 
<div id="A1.I1.i1.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten erinnert an menschliches Verhalten.
<br class="ltx_break"></span>The agentâ€™s behavior makes me think of human behavior</p>
</div>
</li>
<li id="A1.I1.i1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">HLB4</span> 
<div id="A1.I1.i1.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent verhÃ¤lt sich wie eine echte Person.
<br class="ltx_break"></span>The agent behaves like a real person</p>
</div>
</li>
<li id="A1.I1.i1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">NB2</span> 
<div id="A1.I1.i1.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i1.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i1.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent agiert natÃ¼rlich.
<br class="ltx_break"></span>The agent acts naturally</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Performance</span></p>
<ul id="A1.I1.i2.I1" class="ltx_itemize">
<li id="A1.I1.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF1</span> 
<div id="A1.I1.i2.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent macht seine Aufgabe gut.
<br class="ltx_break"></span>The agent does its task well</p>
</div>
</li>
<li id="A1.I1.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF2</span> 
<div id="A1.I1.i2.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent stÃ¶rt mich nicht.
<br class="ltx_break"></span>The agent does not hinder me.</p>
</div>
</li>
<li id="A1.I1.i2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">PF3</span> 
<div id="A1.I1.i2.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i2.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i2.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten bin ich in der Lage zu gewinnen.
<br class="ltx_break"></span>I am capable of suceeding with the agent.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Likeability</span></p>
<ul id="A1.I1.i3.I1" class="ltx_itemize">
<li id="A1.I1.i3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL2</span> 
<div id="A1.I1.i3.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich mag den Agenten.
<br class="ltx_break"></span>I like the agent</p>
</div>
</li>
<li id="A1.I1.i3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL3</span> 
<div id="A1.I1.i3.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich mag den Agenten nicht.
<br class="ltx_break"></span>I dislike the agent</p>
</div>
</li>
<li id="A1.I1.i3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AL4</span> 
<div id="A1.I1.i3.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i3.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i3.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist kooperativ.
<br class="ltx_break"></span>The agent is cooperative</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">User Acceptance of the Agent</span></p>
<ul id="A1.I1.i4.I1" class="ltx_itemize">
<li id="A1.I1.i4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA1</span> 
<div id="A1.I1.i4.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich wÃ¼rde den Agenten in Zukunft wieder nutzen.
<br class="ltx_break"></span>I will use the agent again in the future</p>
</div>
</li>
<li id="A1.I1.i4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA2</span> 
<div id="A1.I1.i4.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich kann mich den Agenten zukÃ¼nftig nutzen sehen.
<br class="ltx_break"></span>I can see myself using the agent in the future</p>
</div>
</li>
<li id="A1.I1.i4.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAA3</span> 
<div id="A1.I1.i4.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i4.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i4.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich vermeide weitere Interaktionen mit dem Agenten.
<br class="ltx_break"></span>I oppose further interaction with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Enjoyability</span></p>
<ul id="A1.I1.i5.I1" class="ltx_itemize">
<li id="A1.I1.i5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE1</span> 
<div id="A1.I1.i5.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist langweilig.
<br class="ltx_break"></span>The agent is boring</p>
</div>
</li>
<li id="A1.I1.i5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE2</span> 
<div id="A1.I1.i5.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Es ist interessant mit dem Agenten zu interagieren.
<br class="ltx_break"></span>It is interesting to interact with the agent</p>
</div>
</li>
<li id="A1.I1.i5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AE3</span> 
<div id="A1.I1.i5.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i5.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i5.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe SpaÃŸ mit dem Agenten zu interagieren.
<br class="ltx_break"></span>I enjoy interacting with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p"><span id="A1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Engagement</span></p>
<ul id="A1.I1.i6.I1" class="ltx_itemize">
<li id="A1.I1.i6.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE1</span> 
<div id="A1.I1.i6.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">WÃ¤hrend der Interaktion mit dem Agenten war ich konzentriert.
<br class="ltx_break"></span>I was concentrated during the interaction with the agent</p>
</div>
</li>
<li id="A1.I1.i6.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE2</span> 
<div id="A1.I1.i6.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Die Interaktion hat meine Aufmerksamkeit erregt.
<br class="ltx_break"></span>The interaction captured my attention</p>
</div>
</li>
<li id="A1.I1.i6.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UE3</span> 
<div id="A1.I1.i6.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i6.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i6.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">WÃ¤hrend der Interaktion war ich aufmerksam.
<br class="ltx_break"></span>I was alert during the interaction with the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(7)</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p"><span id="A1.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Trust</span></p>
<ul id="A1.I1.i7.I1" class="ltx_itemize">
<li id="A1.I1.i7.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT1</span> 
<div id="A1.I1.i7.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent gibt gute Hinweise.
<br class="ltx_break"></span>The agent always gives good advice</p>
</div>
</li>
<li id="A1.I1.i7.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT2</span> 
<div id="A1.I1.i7.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent sagt die Wahrheit.
<br class="ltx_break"></span>The agent acts truthfully</p>
</div>
</li>
<li id="A1.I1.i7.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UT3</span> 
<div id="A1.I1.i7.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i7.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i7.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich kann mich auf den Agenten verlassen.
<br class="ltx_break"></span>I can rely on the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(8)</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p id="A1.I1.i8.p1.1" class="ltx_p"><span id="A1.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">User-Agent Alliance</span></p>
<ul id="A1.I1.i8.I1" class="ltx_itemize">
<li id="A1.I1.i8.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL2</span> 
<div id="A1.I1.i8.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten zusammenzuarbeiten ist wie ein gemeinsames Projekt.
<br class="ltx_break"></span>Collaborating with the agent is like a joint venture</p>
</div>
</li>
<li id="A1.I1.i8.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL4</span> 
<div id="A1.I1.i8.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Mit dem Agenten kann ich produktiv zusammenarbeiten.
<br class="ltx_break"></span>The agent can collaborate in a productive way</p>
</div>
</li>
<li id="A1.I1.i8.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UAL6</span> 
<div id="A1.I1.i8.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i8.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i8.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent versteht mich. 
<br class="ltx_break"></span>The agent understands me</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(9)</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p id="A1.I1.i9.p1.1" class="ltx_p"><span id="A1.I1.i9.p1.1.1" class="ltx_text ltx_font_bold">Attentiveness</span></p>
<ul id="A1.I1.i9.I1" class="ltx_itemize">
<li id="A1.I1.i9.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA1</span> 
<div id="A1.I1.i9.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist wÃ¤hrend der gesamten Interaktion auf mich konzentriert.
<br class="ltx_break"></span>The agent remains focused on me throughout the interaction</p>
</div>
</li>
<li id="A1.I1.i9.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA2</span> 
<div id="A1.I1.i9.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist aufmerksam.
<br class="ltx_break"></span>The agent is attentive</p>
</div>
</li>
<li id="A1.I1.i9.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AA3</span> 
<div id="A1.I1.i9.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i9.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i9.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich bekomme die gesamte Aufmersamkeit des Agenten.
<br class="ltx_break"></span>I receive the agentâ€™s full attention throughout the interaction</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(10)</span> 
<div id="A1.I1.i10.p1" class="ltx_para">
<p id="A1.I1.i10.p1.1" class="ltx_p"><span id="A1.I1.i10.p1.1.1" class="ltx_text ltx_font_bold">Coherence</span></p>
<ul id="A1.I1.i10.I1" class="ltx_itemize">
<li id="A1.I1.i10.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC1</span> 
<div id="A1.I1.i10.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten macht keinen Sinn.
<br class="ltx_break"></span>The agentâ€™s behavior does not make sense</p>
</div>
</li>
<li id="A1.I1.i10.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC3</span> 
<div id="A1.I1.i10.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Das Verhalten des Agenten ist inkonsistent.
<br class="ltx_break"></span>The agent is inconsistent</p>
</div>
</li>
<li id="A1.I1.i10.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AC4</span> 
<div id="A1.I1.i10.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i10.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i10.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent wirkt verwirrt.
<br class="ltx_break"></span>The agent appears confused</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(11)</span> 
<div id="A1.I1.i11.p1" class="ltx_para">
<p id="A1.I1.i11.p1.1" class="ltx_p"><span id="A1.I1.i11.p1.1.1" class="ltx_text ltx_font_bold">Intentionality</span></p>
<ul id="A1.I1.i11.I1" class="ltx_itemize">
<li id="A1.I1.i11.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI1</span> 
<div id="A1.I1.i11.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent agiert intentional.
<br class="ltx_break"></span>The agent acts intentionally</p>
</div>
</li>
<li id="A1.I1.i11.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI3</span> 
<div id="A1.I1.i11.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat keine Ahnung was er tut.
<br class="ltx_break"></span>The agent has no clue of what it is doing</p>
</div>
</li>
<li id="A1.I1.i11.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AI4</span> 
<div id="A1.I1.i11.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i11.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i11.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent kann eigene Entscheidungen treffen.
<br class="ltx_break"></span>The agent can make its own decision</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(12)</span> 
<div id="A1.I1.i12.p1" class="ltx_para">
<p id="A1.I1.i12.p1.1" class="ltx_p"><span id="A1.I1.i12.p1.1.1" class="ltx_text ltx_font_bold">Social Presence</span></p>
<ul id="A1.I1.i12.I1" class="ltx_itemize">
<li id="A1.I1.i12.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP1</span> 
<div id="A1.I1.i12.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat eine soziale PrÃ¤senz.
<br class="ltx_break"></span>The agent has a social presence</p>
</div>
</li>
<li id="A1.I1.i12.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP2</span> 
<div id="A1.I1.i12.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist eine soziale EntitÃ¤t.
<br class="ltx_break"></span>The agent is a social entity</p>
</div>
</li>
<li id="A1.I1.i12.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SP3</span> 
<div id="A1.I1.i12.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i12.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i12.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die gleiche soziale PrÃ¤senz wie der Agent
<br class="ltx_break"></span>I have the same social presence as the agent</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(13)</span> 
<div id="A1.I1.i13.p1" class="ltx_para">
<p id="A1.I1.i13.p1.1" class="ltx_p"><span id="A1.I1.i13.p1.1.1" class="ltx_text ltx_font_bold">Agentâ€™s Emotional Presence</span></p>
<ul id="A1.I1.i13.I1" class="ltx_itemize">
<li id="A1.I1.i13.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP1</span> 
<div id="A1.I1.i13.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Der Agent ist emotional.
<br class="ltx_break"></span>The agent is emotional</p>
</div>
</li>
<li id="A1.I1.i13.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP2</span> 
<div id="A1.I1.i13.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Der Agent hat Emotionen. 
<br class="ltx_break"></span>The agent experiences emotions</p>
</div>
</li>
<li id="A1.I1.i13.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AEP3</span> 
<div id="A1.I1.i13.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i13.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i13.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Der Agent kann keine Emotionen erleben.
<br class="ltx_break"></span>The agent cannot experience emotions</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I1.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(14)</span> 
<div id="A1.I1.i14.p1" class="ltx_para">
<p id="A1.I1.i14.p1.1" class="ltx_p"><span id="A1.I1.i14.p1.1.1" class="ltx_text ltx_font_bold">Userâ€™s Emotion</span></p>
<ul id="A1.I1.i14.I1" class="ltx_itemize">
<li id="A1.I1.i14.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP1</span> 
<div id="A1.I1.i14.I1.ix1.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix1.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Das Auftreten des Agenten hat beeinflusst wie ich mich fÃ¼hle.
<br class="ltx_break"></span>The agentâ€™s attitude influences how I feel</p>
</div>
</li>
<li id="A1.I1.i14.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP2</span> 
<div id="A1.I1.i14.I1.ix2.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix2.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich bin durch die Stimmung des Agenten beeinflusst.
<br class="ltx_break"></span>I am influenced by the agentâ€™s moods</p>
</div>
</li>
<li id="A1.I1.i14.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">UEP3</span> 
<div id="A1.I1.i14.I1.ix3.p1" class="ltx_para">
<p id="A1.I1.i14.I1.ix3.p1.1" class="ltx_p"><span id="A1.I1.i14.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Die Emotionen, die ich wÃ¤hrend der Interaktion erlebe sind vom Agenten ausgelÃ¶st.
<br class="ltx_break"></span>The emotions I feel during the interaction are caused by the agent</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
<p id="A1.p1.2" class="ltx_p">The following questions were added by the authors and focus more on the explanation itself:</p>
<ol id="A1.I2" class="ltx_enumerate">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p"><span id="A1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">subjective understanding</span></p>
<ul id="A1.I2.i1.I1" class="ltx_itemize">
<li id="A1.I2.i1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU1</span> 
<div id="A1.I2.i1.I1.ix1.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix1.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die ErklÃ¤rung gut verstanden.
<br class="ltx_break"></span>I understood the explanation well.</p>
</div>
</li>
<li id="A1.I2.i1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU2</span> 
<div id="A1.I2.i1.I1.ix2.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix2.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ich bin nun in der Lage Quarto zu spielen.
<br class="ltx_break"></span>I am now enabled to play Quarto.</p>
</div>
</li>
<li id="A1.I2.i1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">SU3</span> 
<div id="A1.I2.i1.I1.ix3.p1" class="ltx_para">
<p id="A1.I2.i1.I1.ix3.p1.1" class="ltx_p"><span id="A1.I2.i1.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Ich habe die Regeln des Spieles noch nicht verstanden.
<br class="ltx_break"></span>I do not understand the rules of the game by now.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p"><span id="A1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Agent and Understanding</span></p>
<ul id="A1.I2.i2.I1" class="ltx_itemize">
<li id="A1.I2.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU1</span> 
<div id="A1.I2.i2.I1.ix1.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix1.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Durch den Agenten habe ich die ErklÃ¤rung besser verstanden.
<br class="ltx_break"></span>Because of the agent, I understood the explanation better.</p>
</div>
</li>
<li id="A1.I2.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU2</span> 
<div id="A1.I2.i2.I1.ix2.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix2.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Ohne den Agenten hÃ¤tte ich die ErklÃ¤rung besser verstehen kÃ¶nnen.
<br class="ltx_break"></span>It would have been easier to understand without the agent.</p>
</div>
</li>
<li id="A1.I2.i2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">AU3</span> 
<div id="A1.I2.i2.I1.ix3.p1" class="ltx_para">
<p id="A1.I2.i2.I1.ix3.p1.1" class="ltx_p"><span id="A1.I2.i2.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Durch den Agenten konnte ich der ErklÃ¤rung besser folgen.
<br class="ltx_break"></span>Because of the agent, it was easier to follow the explanation.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Deep Understanding</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The following box plot diagrams show the understanding scores for each of the eight deep understanding items distributed by the four conditions.</p>
</div>
<figure id="A2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN30.png" id="A2.F17.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>UN30</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN31.png" id="A2.F17.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>UN31</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN32.png" id="A2.F17.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>UN32</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN33.png" id="A2.F17.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>UN33</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN34.png" id="A2.F17.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>UN34</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN35.png" id="A2.F17.6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>UN35</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.7" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN36.png" id="A2.F17.7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>UN36</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F17.8" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:107.5pt;"><img src="/html/2406.12544/assets/pictures/UN37.png" id="A2.F17.8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16. </span>UN37</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17. </span>Comparison between the four conditions for each item in the deep understanding questionnaire</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.12543" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.12544" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.12544">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.12544" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.12545" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:13:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
