<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.06438] Multimodal Contextualized Semantic Parsing from Speech</title><meta property="og:description" content="We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agentsâ€™ contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional sâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Contextualized Semantic Parsing from Speech">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Contextualized Semantic Parsing from Speech">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.06438">

<!--Generated on Fri Jul  5 23:06:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Multimodal Contextualized Semantic Parsing from Speech</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jordan Voas
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raymond Mooney
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Harwath
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">jvoas@utexas.edu</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">mooney@utexas.edu</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter">harwath@utexas.edu</span> 
<br class="ltx_break">The University of Texas at Austin 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agentsâ€™ contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agentâ€™s knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/jvoas655/VG-SPICE</span></span></span> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/jvoas655/AViD-SP</span></span></span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Multimodal Contextualized Semantic Parsing from Speech</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Jordan Voas â€‚</span>and<span id="p1.1.2.1.1.1.1.1.2" class="ltx_text ltx_font_bold">â€‚Raymond Mooney â€‚</span>and<span id="p1.1.2.1.1.1.1.1.3" class="ltx_text ltx_font_bold">â€‚David Harwath</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.2.2.1.1" class="ltx_text ltx_font_typewriter">jvoas@utexas.edu</span> â€‚andâ€‚<span id="p1.1.2.1.1.2.2.1.2" class="ltx_text ltx_font_typewriter">mooney@utexas.edu</span> â€‚andâ€‚<span id="p1.1.2.1.1.2.2.1.3" class="ltx_text ltx_font_typewriter">harwath@utexas.edu</span></span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">The University of Texas at Austin</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.06438/assets/figures/vg-spice-example.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of VG-SPICE inputs as well as a plausible output to produce the correct next state context. New information that the agent is expected to add to the context is shown in green while already known information is noted in red. Grounding entities that have new information being added to them are noted in blue and orange. The current context is shown as a textually prompted representation of the actual knowledge graph (discussed in Section <a href="#A6" title="Appendix F Contextual State Representation â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>). </figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Imagine you are taking a guided tour of an art museum. During the tour as you visit each piece of art, your guide describes not only the artworks themselves but also the history and unique features of the galleries and building itself. Through this dialog, you are able to construct a mental map of the museum, whose entities and their relationships with one another are grounded to their real-world counterparts in the museum. We engage in this type of iterative construction of grounded knowledge through dialog every day, such as when teaching a friend how to change the oil in their car or going over a set of X-rays with our dentist. As intelligent agents continue to become more ubiquitous and integrated into our lives, it is increasingly important to develop these same sorts of capabilities in them.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Toward this goal, this work introduces Semantic Parsing in Contextual Environments (SPICE), a task designed to capture the process of iterative knowledge construction through grounded language. It emphasizes the continuous need to update contextual states based on prior knowledge and new information. SPICE requires agents to maintain their contextual state within a structured, dense information framework that is scalable and interpretable, facilitating inspection by users or integration with downstream system components. SPICE accomplishes this by formulating updates as Formal Semantic Parsing, with the formal language defining the allowable solution space of the constructed context.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Because the SPICE task is designed to model real-world and embodied applications, such as teaching a mobile robot about an environment or assisting a doctor with medical image annotations, there are crucial differences between SPICE and traditional text-based semantic parsing. First, SPICE considers parsing language within a grounded, multimodal context. The language in cases like these may have ambiguities that can only be resolved by taking into account multimodal contextual information, such as from vision.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, SPICE supports linguistic input that comes in the form of both speech and text. In real-world embodied interactions, language is predominantly spoken, not written. While modern automatic speech recognition (ASR) technology is highly accurate, it is still sensitive to environmental noise and reverberation, and representing the input language as both a waveform as well as a noisy ASR transcript can improve robustness. While we do not consider it here, the SPICE framework also supports paralinguistic input such as facial expressions, eye gaze, and hand gestures.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We present a novel dataset, VG-SPICE, derived from the Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>, an existing dataset comprised of annotated visual scene graphs representing constituent entities and relational prepositions, enhanced with additional processing and synthetic augmentation to form a foundational representation for SPICE tasks. VG-SPICE simulates the conversational construction of visual scene graphs, wherein a knowledge graph representation of the entities and relationships contained within an image must be collected from the visual inputs and audio dialogue. This dataset, along with an initial model trained for VG-SPICE, sets the baseline for future efforts. Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of a typical VG-SPICE sample. The figure shows how potential semantic parses can be extracted from the visual scene and spoken utterance conditioned on what information is already known about the scene.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The remainder of this paper is structured as follows: It begins with a detailed analysis of the SPICE task, introduces the VG-SPICE dataset, and presents our AViD-SP model. It then delves into experimental results, showcasing the modelâ€™s ability to process and interpret context consistent with the SPICE framework. Finally we outline the implications and directions for future research. The main contributions include:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A definition of the Semantic Parsing in Contextual Environments (SPICE) task, highlighting its challenges, scope, and significance in enhancing human-AI communication.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The creation of a large, machine-generated SPICE dataset, VG-SPICE, leveraging existing machine learning models and the Visual Genome dataset, to motivate SPICE research.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An initial baseline model, Audio-Vision Dialogue Scene Parser (AViD-SP), for VG-SPICE that integrates Language Models with Audio/Visual feature extractors, establishing a research benchmark for SPICE. As a component of AViD-SP, we also introduce a novel pretrained encoder adaption and multimodal fusion method, the Grouped Multimodal Attention Down Sampler (GMADS) to motivate the exploration of additional multimodal adaptation methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The SPICE task intersects with research in dialogue systems and semantic parsing. While previous efforts in these areas have addressed some elements of SPICE, none have fully encapsulated the comprehensive requirements of the SPICE task.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dialogue Systems and Multimodality</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Dialogue systems share similarities with SPICE tasks, particularly in their aim to emulate human conversational skills, including referencing prior conversational context. However, SPICE differentiates itself by necessitating multimodal interactions, the utilization of structured and interpretable knowledge representations, and the capability for dynamic knowledge updates during conversations, setting it apart from conventional dialogue models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Recent advancements in dialogue systems, particularly through large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Chowdhery etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Ouyang etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2022</a>; Jiang etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Touvron etÂ al., <a href="#bib.bib36" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib37" title="" class="ltx_ref">b</a>)</cite>, have enhanced the ability to manage complex, multi-turn conversations. This is largely thanks to the employment of extensive context windows <cite class="ltx_cite ltx_citemacro_citep">(Dao, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, improving language comprehension and generation for more coherent and contextually appropriate exchanges. Nevertheless, LLMsâ€™ reliance on broad textual contexts can compromise efficiency and interpretability in many applications. Not only must all prior inputs be reprocessed for future updates but the uncompressed format prevents easy end-user inspection of the information the model is tracking for future interactions.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Advances in multimodal dialogue systems, incorporating text, image, and audio inputs <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Zhu etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Dai etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Zhang etÂ al., <a href="#bib.bib45" title="" class="ltx_ref">2023a</a>; Maaz etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, edge closer to SPICEâ€™s vision of multimodal communication. Yet, these systems cannot often distill accumulated knowledge into concise, understandable formats, instead still relying on raw dialogue histories or opaque embeddings for prior context.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">While some systems are beginning to interact with and update external knowledge bases, these interactions tend to be unidirectional <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Wu etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> or involve knowledge storage as extensive, barely processed texts <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a href="#bib.bib48" title="" class="ltx_ref">2023</a>; Wang etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>. Dialogue State Tracking (DST) <cite class="ltx_cite ltx_citemacro_citep">(Balaraman etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> shares similarities with SPICE in that agents use and update their knowledge bases during dialogues. However, most DST efforts are unimodal, with limited exploration of multimodal inputs <cite class="ltx_cite ltx_citemacro_citep">(Kottur etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>. Moreover, existing datasets and models for DST do not align with the SPICE framework, as they often rely on regenerating the knowledge base with each dialogue step from all historical dialogue inputs without offering a structured representation of the prior context. SPICE, conversely, envisions sequential updates based on and directly applied to prior context, a feature not yet explored in DST. Further, we are unaware of any DST work that has attempted to utilize spoken audio.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Semantic Parsing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Semantic Parsing involves translating natural language into a structured, symbolic-meaning representation. Traditional semantic parsing research focuses on processing individual, short-span inputs to produce their semantic representations <cite class="ltx_cite ltx_citemacro_citep">(Kamath and Das, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>. Some studies have explored semantic parsing in dialogues or with contextual inputs, known as Semantic Parsing in Context (SPiC) or Context Dependent Semantic Parsing (CDSP) <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. However, most CDSP research has been aimed at database applications, where the context is a static schema <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite>. While these tasks leverage context for query execution, they do not involve dynamic schema updates, instead maintaining a static context between interactions. Outside these applications, CDSP is mainly applied in DST <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>; Cheng etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Moradshahi etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Heck etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, which we have previously differentiated from SPICE.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Furthermore, semantic parsing has traditionally been limited to textual inputs and unimodal applications. It has been extended to visual modalities, notably in automated Scene Graph Generation (SGG) tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a href="#bib.bib47" title="" class="ltx_ref">2023b</a>; Abdelsalam etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>; Zareian etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite>. Although there has been exploration into using spoken audio for semantic parsing <cite class="ltx_cite ltx_citemacro_citep">(Tomasello etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>; Coucke etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>; Lugosch etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Sen and Groves, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>, these efforts have been constrained by focusing on simple intent and slot prediction tasks, and have not incorporated contextual updates or complex semantic outputs.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">As such, we believe SPICE to be considerably distinct from any works that have come previously. While individual components of SPICEâ€™s framework have been studied, such as semantic parsing from audio, context, or multimodal inputs, no work has utilized all of these at once. Additionally, SPICE goes beyond most semantic parsing and dialogue works, even those operating on some form of knowledge representation, by tasking the agent to produce continual updates to said knowledge graph and to maintain them in an interpretable format.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">#Scenes</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">#Nodes</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">#Predicates</span></th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Avg. Size</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">108077</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">76,340</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left">VG80K <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center">104832</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center">53304</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center">29086</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center">19.02</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left">VG150 <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center">105414</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center">150</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center">50</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_center">6.98</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">Ours</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">22346</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">2032</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">282</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">19.64</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Comparison of our Visual Genome curation statistics to other works. Further details are in Section <a href="#A4" title="Appendix D Visual Genome Preprocessing â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.
</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Task Definition</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.5" class="ltx_p">Semantic Parsing in Contextual Environments (SPICE) is defined as follows. Consider a model agent, denoted as <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">a</annotation></semantics></math>, designed to maintain and update a world state across interaction timesteps. Let <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">C</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ¶</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">C_{i}</annotation></semantics></math> represent this world state during the <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.p1.3.m3.1a"><msup id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">i</mi><mrow id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml"><mi id="S3.p1.3.m3.1.1.3.2" xref="S3.p1.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p1.3.m3.1.1.3.1" xref="S3.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.p1.3.m3.1.1.3.3" xref="S3.p1.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğ‘–</ci><apply id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"><times id="S3.p1.3.m3.1.1.3.1.cmml" xref="S3.p1.3.m3.1.1.3.1"></times><ci id="S3.p1.3.m3.1.1.3.2.cmml" xref="S3.p1.3.m3.1.1.3.2">ğ‘¡</ci><ci id="S3.p1.3.m3.1.1.3.3.cmml" xref="S3.p1.3.m3.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">i^{th}</annotation></semantics></math> turn. For interpretability and downstream use <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">C</mi><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ğ¶</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">C_{i}</annotation></semantics></math> is represented as a formal knowledge graph <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. This state represents the accumulated context from prior interactions. Initially, <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">C</mi><mi id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ğ¶</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">C_{i}</annotation></semantics></math> can be set to a default or empty state.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.8" class="ltx_p">During each interaction turn, the agent encounters a set of new inputs, referred to as information inputs <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="F_{i}^{m}" display="inline"><semantics id="S3.p2.1.m1.1a"><msubsup id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2.2" xref="S3.p2.1.m1.1.1.2.2.cmml">F</mi><mi id="S3.p2.1.m1.1.1.2.3" xref="S3.p2.1.m1.1.1.2.3.cmml">i</mi><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">superscript</csymbol><apply id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.2.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.2.cmml" xref="S3.p2.1.m1.1.1.2.2">ğ¹</ci><ci id="S3.p2.1.m1.1.1.2.3.cmml" xref="S3.p2.1.m1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">F_{i}^{m}</annotation></semantics></math>, with <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">m</annotation></semantics></math> indicating the diversity of modalities the agent is processing. The agentâ€™s goal is to construct a formal semantic parse, <math id="S3.p2.3.m3.2" class="ltx_Math" alttext="P_{i}=a(F_{i}^{m},C_{i})" display="inline"><semantics id="S3.p2.3.m3.2a"><mrow id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml"><msub id="S3.p2.3.m3.2.2.4" xref="S3.p2.3.m3.2.2.4.cmml"><mi id="S3.p2.3.m3.2.2.4.2" xref="S3.p2.3.m3.2.2.4.2.cmml">P</mi><mi id="S3.p2.3.m3.2.2.4.3" xref="S3.p2.3.m3.2.2.4.3.cmml">i</mi></msub><mo id="S3.p2.3.m3.2.2.3" xref="S3.p2.3.m3.2.2.3.cmml">=</mo><mrow id="S3.p2.3.m3.2.2.2" xref="S3.p2.3.m3.2.2.2.cmml"><mi id="S3.p2.3.m3.2.2.2.4" xref="S3.p2.3.m3.2.2.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.2.2.2.3" xref="S3.p2.3.m3.2.2.2.3.cmml">â€‹</mo><mrow id="S3.p2.3.m3.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.p2.3.m3.2.2.2.2.2.3" xref="S3.p2.3.m3.2.2.2.2.3.cmml">(</mo><msubsup id="S3.p2.3.m3.1.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.1.1.1.2.2" xref="S3.p2.3.m3.1.1.1.1.1.1.2.2.cmml">F</mi><mi id="S3.p2.3.m3.1.1.1.1.1.1.2.3" xref="S3.p2.3.m3.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.1.1.3.cmml">m</mi></msubsup><mo id="S3.p2.3.m3.2.2.2.2.2.4" xref="S3.p2.3.m3.2.2.2.2.3.cmml">,</mo><msub id="S3.p2.3.m3.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.2.cmml">C</mi><mi id="S3.p2.3.m3.2.2.2.2.2.2.3" xref="S3.p2.3.m3.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.3.m3.2.2.2.2.2.5" xref="S3.p2.3.m3.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.2b"><apply id="S3.p2.3.m3.2.2.cmml" xref="S3.p2.3.m3.2.2"><eq id="S3.p2.3.m3.2.2.3.cmml" xref="S3.p2.3.m3.2.2.3"></eq><apply id="S3.p2.3.m3.2.2.4.cmml" xref="S3.p2.3.m3.2.2.4"><csymbol cd="ambiguous" id="S3.p2.3.m3.2.2.4.1.cmml" xref="S3.p2.3.m3.2.2.4">subscript</csymbol><ci id="S3.p2.3.m3.2.2.4.2.cmml" xref="S3.p2.3.m3.2.2.4.2">ğ‘ƒ</ci><ci id="S3.p2.3.m3.2.2.4.3.cmml" xref="S3.p2.3.m3.2.2.4.3">ğ‘–</ci></apply><apply id="S3.p2.3.m3.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2"><times id="S3.p2.3.m3.2.2.2.3.cmml" xref="S3.p2.3.m3.2.2.2.3"></times><ci id="S3.p2.3.m3.2.2.2.4.cmml" xref="S3.p2.3.m3.2.2.2.4">ğ‘</ci><interval closure="open" id="S3.p2.3.m3.2.2.2.2.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2"><apply id="S3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1">superscript</csymbol><apply id="S3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2.2">ğ¹</ci><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.3">ğ‘š</ci></apply><apply id="S3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.2">ğ¶</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.2c">P_{i}=a(F_{i}^{m},C_{i})</annotation></semantics></math>. This parse is formulated by integrating the prior context <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p2.4.m4.1a"><msub id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">C</mi><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">ğ¶</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">C_{i}</annotation></semantics></math> with the new information inputs <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="F_{i}^{m}" display="inline"><semantics id="S3.p2.5.m5.1a"><msubsup id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2.2" xref="S3.p2.5.m5.1.1.2.2.cmml">F</mi><mi id="S3.p2.5.m5.1.1.2.3" xref="S3.p2.5.m5.1.1.2.3.cmml">i</mi><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">superscript</csymbol><apply id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.2.1.cmml" xref="S3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.p2.5.m5.1.1.2.2.cmml" xref="S3.p2.5.m5.1.1.2.2">ğ¹</ci><ci id="S3.p2.5.m5.1.1.2.3.cmml" xref="S3.p2.5.m5.1.1.2.3">ğ‘–</ci></apply><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">F_{i}^{m}</annotation></semantics></math>. With the aid of an execution function <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">e</annotation></semantics></math>, this results in an updated context <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">C</mi><mrow id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml"><mi id="S3.p2.7.m7.1.1.3.2" xref="S3.p2.7.m7.1.1.3.2.cmml">i</mi><mo id="S3.p2.7.m7.1.1.3.1" xref="S3.p2.7.m7.1.1.3.1.cmml">+</mo><mn id="S3.p2.7.m7.1.1.3.3" xref="S3.p2.7.m7.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ğ¶</ci><apply id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3"><plus id="S3.p2.7.m7.1.1.3.1.cmml" xref="S3.p2.7.m7.1.1.3.1"></plus><ci id="S3.p2.7.m7.1.1.3.2.cmml" xref="S3.p2.7.m7.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p2.7.m7.1.1.3.3.cmml" xref="S3.p2.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">C_{i+1}</annotation></semantics></math> = <math id="S3.p2.8.m8.2" class="ltx_Math" alttext="e(P_{i},C_{i})" display="inline"><semantics id="S3.p2.8.m8.2a"><mrow id="S3.p2.8.m8.2.2" xref="S3.p2.8.m8.2.2.cmml"><mi id="S3.p2.8.m8.2.2.4" xref="S3.p2.8.m8.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p2.8.m8.2.2.3" xref="S3.p2.8.m8.2.2.3.cmml">â€‹</mo><mrow id="S3.p2.8.m8.2.2.2.2" xref="S3.p2.8.m8.2.2.2.3.cmml"><mo stretchy="false" id="S3.p2.8.m8.2.2.2.2.3" xref="S3.p2.8.m8.2.2.2.3.cmml">(</mo><msub id="S3.p2.8.m8.1.1.1.1.1" xref="S3.p2.8.m8.1.1.1.1.1.cmml"><mi id="S3.p2.8.m8.1.1.1.1.1.2" xref="S3.p2.8.m8.1.1.1.1.1.2.cmml">P</mi><mi id="S3.p2.8.m8.1.1.1.1.1.3" xref="S3.p2.8.m8.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.8.m8.2.2.2.2.4" xref="S3.p2.8.m8.2.2.2.3.cmml">,</mo><msub id="S3.p2.8.m8.2.2.2.2.2" xref="S3.p2.8.m8.2.2.2.2.2.cmml"><mi id="S3.p2.8.m8.2.2.2.2.2.2" xref="S3.p2.8.m8.2.2.2.2.2.2.cmml">C</mi><mi id="S3.p2.8.m8.2.2.2.2.2.3" xref="S3.p2.8.m8.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.8.m8.2.2.2.2.5" xref="S3.p2.8.m8.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.2b"><apply id="S3.p2.8.m8.2.2.cmml" xref="S3.p2.8.m8.2.2"><times id="S3.p2.8.m8.2.2.3.cmml" xref="S3.p2.8.m8.2.2.3"></times><ci id="S3.p2.8.m8.2.2.4.cmml" xref="S3.p2.8.m8.2.2.4">ğ‘’</ci><interval closure="open" id="S3.p2.8.m8.2.2.2.3.cmml" xref="S3.p2.8.m8.2.2.2.2"><apply id="S3.p2.8.m8.1.1.1.1.1.cmml" xref="S3.p2.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.p2.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.1.1.1.2.cmml" xref="S3.p2.8.m8.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.p2.8.m8.1.1.1.1.1.3.cmml" xref="S3.p2.8.m8.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.p2.8.m8.2.2.2.2.2.cmml" xref="S3.p2.8.m8.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.8.m8.2.2.2.2.2.1.cmml" xref="S3.p2.8.m8.2.2.2.2.2">subscript</csymbol><ci id="S3.p2.8.m8.2.2.2.2.2.2.cmml" xref="S3.p2.8.m8.2.2.2.2.2.2">ğ¶</ci><ci id="S3.p2.8.m8.2.2.2.2.2.3.cmml" xref="S3.p2.8.m8.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.2c">e(P_{i},C_{i})</annotation></semantics></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p">This newly formed context <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">C</mi><mrow id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml"><mi id="S3.p3.1.m1.1.1.3.2" xref="S3.p3.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.p3.1.m1.1.1.3.1" xref="S3.p3.1.m1.1.1.3.1.cmml">+</mo><mn id="S3.p3.1.m1.1.1.3.3" xref="S3.p3.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ğ¶</ci><apply id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3"><plus id="S3.p3.1.m1.1.1.3.1.cmml" xref="S3.p3.1.m1.1.1.3.1"></plus><ci id="S3.p3.1.m1.1.1.3.2.cmml" xref="S3.p3.1.m1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p3.1.m1.1.1.3.3.cmml" xref="S3.p3.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">C_{i+1}</annotation></semantics></math> should represent all task essential information, both from previous context <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">C</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ğ¶</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">C_{i}</annotation></semantics></math> and the most recent interaction round, for future rounds. <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">C</mi><mrow id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml"><mi id="S3.p3.3.m3.1.1.3.2" xref="S3.p3.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.p3.3.m3.1.1.3.1" xref="S3.p3.3.m3.1.1.3.1.cmml">+</mo><mn id="S3.p3.3.m3.1.1.3.3" xref="S3.p3.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">ğ¶</ci><apply id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3"><plus id="S3.p3.3.m3.1.1.3.1.cmml" xref="S3.p3.3.m3.1.1.3.1"></plus><ci id="S3.p3.3.m3.1.1.3.2.cmml" xref="S3.p3.3.m3.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p3.3.m3.1.1.3.3.cmml" xref="S3.p3.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">C_{i+1}</annotation></semantics></math> is expected to align with a reference context, denoted as <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="\hat{C}_{i+1}" display="inline"><semantics id="S3.p3.4.m4.1a"><msub id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mover accent="true" id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml"><mi id="S3.p3.4.m4.1.1.2.2" xref="S3.p3.4.m4.1.1.2.2.cmml">C</mi><mo id="S3.p3.4.m4.1.1.2.1" xref="S3.p3.4.m4.1.1.2.1.cmml">^</mo></mover><mrow id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml"><mi id="S3.p3.4.m4.1.1.3.2" xref="S3.p3.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.p3.4.m4.1.1.3.1" xref="S3.p3.4.m4.1.1.3.1.cmml">+</mo><mn id="S3.p3.4.m4.1.1.3.3" xref="S3.p3.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1">subscript</csymbol><apply id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2"><ci id="S3.p3.4.m4.1.1.2.1.cmml" xref="S3.p3.4.m4.1.1.2.1">^</ci><ci id="S3.p3.4.m4.1.1.2.2.cmml" xref="S3.p3.4.m4.1.1.2.2">ğ¶</ci></apply><apply id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3"><plus id="S3.p3.4.m4.1.1.3.1.cmml" xref="S3.p3.4.m4.1.1.3.1"></plus><ci id="S3.p3.4.m4.1.1.3.2.cmml" xref="S3.p3.4.m4.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.p3.4.m4.1.1.3.3.cmml" xref="S3.p3.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">\hat{C}_{i+1}</annotation></semantics></math>, which represents the ideal post-interaction state.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section introduces VG-SPICE, a novel dataset for SPICE tasks, providing a structured benchmark for model training and evaluation. To our knowledge, VG-SPICE is the first of its kind and is derived from the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_citep">(Krishna etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite> to simulate a â€œtour guideâ€ providing sequential descriptions of aspects of the environment. In these scenarios, the tour guide describes a visual scene with sequential utterances, each introducing new elements to the scene. These descriptions, combined with a pre-established world state of the scene, mimic the accumulation of world state information through successive interactions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">VG-SPICE utilizes the Visual Genomeâ€™s 108k images with human-annotated scene graphs for entity identification via bounding boxes, originally detected using an object identification model. The graphs include named nodes, optional attributes, and directed edges for relational predicates.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.4" class="ltx_p">The dataset is constructed by extracting sub-graphs from scene graphs as the initial context, <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S4.p3.1.m1.1a"><msub id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">C</mi><mi id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">ğ¶</ci><ci id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">C_{i}</annotation></semantics></math>, sampled from empty to nearly complete. These are then augmented by reintegrating a portion of the omitted graph to form the updated context, <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">C</mi><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mi id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">i</mi><mo id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">+</mo><mn id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">ğ¶</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><plus id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></plus><ci id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">ğ‘–</ci><cn type="integer" id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">C_{i+1}</annotation></semantics></math>. Before extracting our samples, the Visual Genome data underwent preprocessing to enhance dataset quality (Section <a href="#A4" title="Appendix D Visual Genome Preprocessing â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> and summary results shown in Table <a href="#S2.T1" title="Table 1 â€£ 2.2 Semantic Parsing â€£ 2 Related Work â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The dataset allows flexible model implementation with semantic parses (<math id="S4.p3.3.m3.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S4.p3.3.m3.1a"><msub id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mi id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">P</mi><mi id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">ğ‘ƒ</ci><ci id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">P_{i}</annotation></semantics></math>) and parsing functions (<math id="S4.p3.4.m4.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S4.p3.4.m4.1a"><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">e</annotation></semantics></math>) not predefined, allowing flexibility in modeling implementation. Our modelâ€™s semantic parse format is discussed in Section <a href="#A7" title="Appendix G Formal Language Definition â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.7" class="ltx_p">For each context pair (<math id="S4.p4.1.m1.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S4.p4.1.m1.1a"><msub id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">C</mi><mi id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">ğ¶</ci><ci id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">C_{i}</annotation></semantics></math>, <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S4.p4.2.m2.1a"><msub id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">C</mi><mrow id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml"><mi id="S4.p4.2.m2.1.1.3.2" xref="S4.p4.2.m2.1.1.3.2.cmml">i</mi><mo id="S4.p4.2.m2.1.1.3.1" xref="S4.p4.2.m2.1.1.3.1.cmml">+</mo><mn id="S4.p4.2.m2.1.1.3.3" xref="S4.p4.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1">subscript</csymbol><ci id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">ğ¶</ci><apply id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"><plus id="S4.p4.2.m2.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.3.1"></plus><ci id="S4.p4.2.m2.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.3.2">ğ‘–</ci><cn type="integer" id="S4.p4.2.m2.1.1.3.3.cmml" xref="S4.p4.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">C_{i+1}</annotation></semantics></math>), features from <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S4.p4.3.m3.1a"><msub id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mi id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml">C</mi><mi id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1">subscript</csymbol><ci id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">ğ¶</ci><ci id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">C_{i}</annotation></semantics></math> and modified features for <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S4.p4.4.m4.1a"><msub id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">C</mi><mrow id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml"><mi id="S4.p4.4.m4.1.1.3.2" xref="S4.p4.4.m4.1.1.3.2.cmml">i</mi><mo id="S4.p4.4.m4.1.1.3.1" xref="S4.p4.4.m4.1.1.3.1.cmml">+</mo><mn id="S4.p4.4.m4.1.1.3.3" xref="S4.p4.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1">subscript</csymbol><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">ğ¶</ci><apply id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3"><plus id="S4.p4.4.m4.1.1.3.1.cmml" xref="S4.p4.4.m4.1.1.3.1"></plus><ci id="S4.p4.4.m4.1.1.3.2.cmml" xref="S4.p4.4.m4.1.1.3.2">ğ‘–</ci><cn type="integer" id="S4.p4.4.m4.1.1.3.3.cmml" xref="S4.p4.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">C_{i+1}</annotation></semantics></math> are structured into natural language prompts. These prompts are processed by the Llama 2 70B LLM <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a href="#bib.bib36" title="" class="ltx_ref">2023a</a>)</cite> to generate plausible sentences that describe the difference between <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S4.p4.5.m5.1a"><msub id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml"><mi id="S4.p4.5.m5.1.1.2" xref="S4.p4.5.m5.1.1.2.cmml">C</mi><mi id="S4.p4.5.m5.1.1.3" xref="S4.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><apply id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p4.5.m5.1.1.1.cmml" xref="S4.p4.5.m5.1.1">subscript</csymbol><ci id="S4.p4.5.m5.1.1.2.cmml" xref="S4.p4.5.m5.1.1.2">ğ¶</ci><ci id="S4.p4.5.m5.1.1.3.cmml" xref="S4.p4.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">C_{i}</annotation></semantics></math> and <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="S4.p4.6.m6.1a"><msub id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml"><mi id="S4.p4.6.m6.1.1.2" xref="S4.p4.6.m6.1.1.2.cmml">C</mi><mrow id="S4.p4.6.m6.1.1.3" xref="S4.p4.6.m6.1.1.3.cmml"><mi id="S4.p4.6.m6.1.1.3.2" xref="S4.p4.6.m6.1.1.3.2.cmml">i</mi><mo id="S4.p4.6.m6.1.1.3.1" xref="S4.p4.6.m6.1.1.3.1.cmml">+</mo><mn id="S4.p4.6.m6.1.1.3.3" xref="S4.p4.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><apply id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p4.6.m6.1.1.1.cmml" xref="S4.p4.6.m6.1.1">subscript</csymbol><ci id="S4.p4.6.m6.1.1.2.cmml" xref="S4.p4.6.m6.1.1.2">ğ¶</ci><apply id="S4.p4.6.m6.1.1.3.cmml" xref="S4.p4.6.m6.1.1.3"><plus id="S4.p4.6.m6.1.1.3.1.cmml" xref="S4.p4.6.m6.1.1.3.1"></plus><ci id="S4.p4.6.m6.1.1.3.2.cmml" xref="S4.p4.6.m6.1.1.3.2">ğ‘–</ci><cn type="integer" id="S4.p4.6.m6.1.1.3.3.cmml" xref="S4.p4.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">C_{i+1}</annotation></semantics></math>. We then synthesize spoken versions of these sentences via the Tortoise-TTS-V2 <cite class="ltx_cite ltx_citemacro_citep">(Betker, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite> text-to-speech (TTS) synthesis system. We configure the TTS model to randomly sample speaker characteristics from its pretrained latent space, and use the built-in â€œhigh_qualityâ€ setup for other generation settings. Before TTS conversion filtering is performed on the textual utterances to remove common recurrent terms indicative of new information (eg., "there now is a" versus "there is a"). The audio recordings and visual images are the multimodal inputs <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="F_{i}^{m}" display="inline"><semantics id="S4.p4.7.m7.1a"><msubsup id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml"><mi id="S4.p4.7.m7.1.1.2.2" xref="S4.p4.7.m7.1.1.2.2.cmml">F</mi><mi id="S4.p4.7.m7.1.1.2.3" xref="S4.p4.7.m7.1.1.2.3.cmml">i</mi><mi id="S4.p4.7.m7.1.1.3" xref="S4.p4.7.m7.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><apply id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p4.7.m7.1.1.1.cmml" xref="S4.p4.7.m7.1.1">superscript</csymbol><apply id="S4.p4.7.m7.1.1.2.cmml" xref="S4.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p4.7.m7.1.1.2.1.cmml" xref="S4.p4.7.m7.1.1">subscript</csymbol><ci id="S4.p4.7.m7.1.1.2.2.cmml" xref="S4.p4.7.m7.1.1.2.2">ğ¹</ci><ci id="S4.p4.7.m7.1.1.2.3.cmml" xref="S4.p4.7.m7.1.1.2.3">ğ‘–</ci></apply><ci id="S4.p4.7.m7.1.1.3.cmml" xref="S4.p4.7.m7.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">F_{i}^{m}</annotation></semantics></math> of VG-SPICE, emphasizing spoken audio for practicality in real-world applications and necessitating addressing the challenges of semantic parsing from audio such as speaker diversity and noise robustness. The presence of both textual and spoken audio representations for the update utterances allows VG-SPICE to be utilized for semantic parsing evaluations in either modality.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">VG-SPICE includes over 131k SPICE update samples from 20k unique scenes, with <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="2.5\%" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mn id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">2.5</mn><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">2.5\%</annotation></semantics></math> allocated to each of the validation and test sets, ensuring distinct scenes across splits. We perform noise augmentation on the input speech using the CHiME5 dataset <cite class="ltx_cite ltx_citemacro_citep">(Barker etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> to simulate realistic noise conditions, with performance evaluated at various Signal to Noise Ratios (SNR). VG-SPICE samples and summary statistics are presented in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S4.T2" title="Table 2 â€£ 4 Dataset â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Statistic</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># Samples</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">131362</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"># Unique Scenes</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">22346</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Hours of Audio</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">10.56</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Avg. Words per Utterance</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">71.83</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Avg. Nodes Added</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center">1.27</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Avg. Attributes Added</th>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center">0.93</td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<th id="S4.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Avg. Edges Added</th>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b">0.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Summary statistics for our VG-SPICE dataset.
</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Challenge Subset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In addition to the standard test set, we augment VG-SPICE with an additional Challenge Subset, VG-SPICE-C. Although this subset is small, spanning only 50 individual visual scenes, it provides distinct capabilities not present in the primary VG-SPICE test dataset, as detailed below.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Broad Visual Representation</span>: To sample the Challenge Subset, we used a representation-based process to promote diverse image types. We obtained the CLIP<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>openai/clip-vit-base-patch32 from Huggingface</span></span></span> representations for each image in the original VG-SPICE test split. Using KMeans clustering, the dataset was partitioned into 50 distinct groupings of visual representations, with a single sample taken from each cluster.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Manual Scene Graph Quality Enhancements</span>: Despite automated generation processes in VG-SPICE aiming to improve scene graph quality, persistent issues remain. To ensure a clean and reliable testing subset, manual scene graph improvements were made to ensure the final scene graph for each image was accurate. This involved removing incorrect, low-quality, or duplicate scene features and enhancing the scene graphs to achieve far greater density than originally present in VG-SPICE or Visual Genome, particularly for Edges and Attributes.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Coherent Iterative Updates</span>: To improve sample diversity, VG-SPICE was generated in an iteratively incoherent fashion, meaning samples for a single update cannot be used to coherently evaluate end-to-end SPICE evaluations. For the Challenge Subset, we manually annotated each of the 50 sampled scenes with five individual utterances, each adding novel information while referring to previously mentioned details. These utterances are of greater diversity and quality (due to manual annotation rather than LLM production) and can be used sequentially to evaluate scene graph generation errors over multiple interaction rounds.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">OOD and Real Speech</span>: To enhance the evaluative capabilities of the Challenge Set, we provide speech samples for the utterances from two sources: Tortoise-TTS as used for the remainder of VG-SPICE (with three random voice samples per utterance) as well as manual recordings of the spoken utterances by a individual human annotator.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">This Challenge Subset offers a rigorous evaluation framework for models, promoting advancements in handling diverse visual representations, maintaining high-quality scene graphs, performing coherent iterative updates, and managing out-of-domain and real-world speech scenarios.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>AViD-SP Model</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To address the challenges of VG-SPICE, our approach utilizes a range of pretrained models, specifically fine-tuned to enhance SPICE-focused semantic parsing capabilities. Figure <a href="#S5.F2" title="Figure 2 â€£ 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates our model architecture, termed Audio-Vision Dialogue Scene Parser (AViD-SP). At the core of our framework lies the pretrained Llama 2 7B model <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2023b</a>)</cite>. Despite deploying its smallest variant, the extensive pretraining endows our model with robust functional abilities, particularly beneficial for processing the diverse semantic parses inherent to VG-SPICE. However, Llama 2, trained on textual data, lacks inherent support for the multimodal inputs typical in VG-SPICE.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">To accommodate diverse inputs, we extend techniques from prior studies <cite class="ltx_cite ltx_citemacro_citep">(Rubenstein etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Gong etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>; Lin etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> by projecting embeddings from pretrained modality-specific feature extractors. This approach has been proven to enable text-based LLMs to process information across various modalities. Directly integrating these projected embeddings into the LLMâ€™s context window, however, introduces significant computational overhead due to their typically extensive context lengths. While previous research often employed pooling methods <cite class="ltx_cite ltx_citemacro_citep">(Gong etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> to condense embeddings by modality, this strategy incompletely addresses the challenges of merging varied modality embeddings for LLM use. For instance, audio embeddings offer finer temporal granularity than textual embeddings, and the reverse is often true for vision embeddings, complicating the adjustment of downsampling factors. Moreover, even with optimized downsampling, pooled embeddings must preserve their original sequential order and are restricted to information from solely the pooled segments. Many applications could benefit from capabilities to establish downsampled features encompassing both local and global contexts and to rearrange these features to an extent.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.2" class="ltx_p">To surmount these challenges, we introduce a novel Grouped Modality Attention Down Sampler (GMADS) module. This module initially projects embeddings from non-textual modalities into a unified, fixed-dimensional space. We form a set of modality groupings, one for each input modality (audio and visual with VG-SPICE), and a cross-modality grouping derived from concatenating all modality embeddings, each prefixed with a modality-specific token. A series of self-attention layers processes each embedding sequence and downsamples the outputs by a factor of <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S5.p3.1.m1.1a"><mi id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">S</annotation></semantics></math> through mean pooling. These values are then concatenated with the mean-pooled pre-self-attention embeddings along the embedding dimension, akin to a skip connection. A final projection adjusts the outputs to match the dimensionality of the Llama 2 7B decoder, and all embedding sequences are concatenated. This process yields an embedding output that is effectively downsampled by a factor of <math id="S5.p3.2.m2.1" class="ltx_Math" alttext="S/2" display="inline"><semantics id="S5.p3.2.m2.1a"><mrow id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml"><mi id="S5.p3.2.m2.1.1.2" xref="S5.p3.2.m2.1.1.2.cmml">S</mi><mo id="S5.p3.2.m2.1.1.1" xref="S5.p3.2.m2.1.1.1.cmml">/</mo><mn id="S5.p3.2.m2.1.1.3" xref="S5.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1"><divide id="S5.p3.2.m2.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1"></divide><ci id="S5.p3.2.m2.1.1.2.cmml" xref="S5.p3.2.m2.1.1.2">ğ‘†</ci><cn type="integer" id="S5.p3.2.m2.1.1.3.cmml" xref="S5.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">S/2</annotation></semantics></math>. All weights in the GMADS module are shared across the groups, substantially reducing the parameter count. Additionally, we employ a self-supervised representation learning objective on the embeddings from the downsampled cross-modality group outputs by upsampling them to their original size and then processing them through a secondary set of self-attention layers. The reconstructed cross-modality embeddings are then segmented by modality, with per-modality projections striving to restore them to their original input size. We apply a contrastive reconstruction loss objective as outlined in Eq. <a href="#S5.E1" title="In 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, using the corresponding ground truth embedding as an anchor and all other embeddings in the batch as contrastive samples.</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.8" class="ltx_Math" alttext="\ell_{n,Contrast}=\scriptstyle{\sum_{j=1}^{B*K}\log\frac{\exp(sim(z_{i},z_{j})/\tau)}{\sum_{k=1}^{B*K}[k\neq i]\exp(sim(z_{i},z_{k})/\tau)}}" display="block"><semantics id="S5.E1.m1.8a"><mtable displaystyle="true" id="S5.E1.m1.8.8" xref="S5.E1.m1.8.9.1.cmml"><mtr id="S5.E1.m1.8.8a" xref="S5.E1.m1.8.9.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E1.m1.8.8b" xref="S5.E1.m1.8.9.1.cmml"><mrow id="S5.E1.m1.8.8.8.8.8" xref="S5.E1.m1.8.9.1.cmml"><msub id="S5.E1.m1.8.8.8.8.8.9" xref="S5.E1.m1.8.9.1.cmml"><mi mathvariant="normal" id="S5.E1.m1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.cmml">â„“</mi><mrow id="S5.E1.m1.2.2.2.2.2.2.1.2" xref="S5.E1.m1.2.2.2.2.2.2.1.3.cmml"><mi id="S5.E1.m1.2.2.2.2.2.2.1.1" xref="S5.E1.m1.2.2.2.2.2.2.1.1.cmml">n</mi><mo id="S5.E1.m1.2.2.2.2.2.2.1.2.2" xref="S5.E1.m1.2.2.2.2.2.2.1.3.cmml">,</mo><mrow id="S5.E1.m1.2.2.2.2.2.2.1.2.1" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.cmml"><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.2" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.3" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1a" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.4" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1b" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.5" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1c" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.6" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1d" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.7" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1e" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.8" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1f" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml">â€‹</mo><mi id="S5.E1.m1.2.2.2.2.2.2.1.2.1.9" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.9.cmml">t</mi></mrow></mrow></msub><mo id="S5.E1.m1.3.3.3.3.3.3" xref="S5.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S5.E1.m1.8.8.8.8.8.10" xref="S5.E1.m1.8.9.1.cmml"><mstyle displaystyle="false" id="S5.E1.m1.8.8.8.8.8.10.1" xref="S5.E1.m1.8.9.1.cmml"><msubsup id="S5.E1.m1.8.8.8.8.8.10.1a" xref="S5.E1.m1.8.9.1.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S5.E1.m1.4.4.4.4.4.4" xref="S5.E1.m1.4.4.4.4.4.4.cmml">âˆ‘</mo><mrow id="S5.E1.m1.5.5.5.5.5.5.1" xref="S5.E1.m1.5.5.5.5.5.5.1.cmml"><mi mathsize="71%" id="S5.E1.m1.5.5.5.5.5.5.1.2" xref="S5.E1.m1.5.5.5.5.5.5.1.2.cmml">j</mi><mo mathsize="71%" id="S5.E1.m1.5.5.5.5.5.5.1.1" xref="S5.E1.m1.5.5.5.5.5.5.1.1.cmml">=</mo><mn mathsize="71%" id="S5.E1.m1.5.5.5.5.5.5.1.3" xref="S5.E1.m1.5.5.5.5.5.5.1.3.cmml">1</mn></mrow><mrow id="S5.E1.m1.6.6.6.6.6.6.1" xref="S5.E1.m1.6.6.6.6.6.6.1.cmml"><mi mathsize="71%" id="S5.E1.m1.6.6.6.6.6.6.1.2" xref="S5.E1.m1.6.6.6.6.6.6.1.2.cmml">B</mi><mo lspace="0.222em" mathsize="71%" rspace="0.222em" id="S5.E1.m1.6.6.6.6.6.6.1.1" xref="S5.E1.m1.6.6.6.6.6.6.1.1.cmml">âˆ—</mo><mi mathsize="71%" id="S5.E1.m1.6.6.6.6.6.6.1.3" xref="S5.E1.m1.6.6.6.6.6.6.1.3.cmml">K</mi></mrow></msubsup></mstyle><mrow id="S5.E1.m1.8.8.8.8.8.10.2" xref="S5.E1.m1.8.9.1.cmml"><mi mathsize="70%" id="S5.E1.m1.7.7.7.7.7.7" xref="S5.E1.m1.7.7.7.7.7.7.cmml">log</mi><mo lspace="0.167em" id="S5.E1.m1.8.8.8.8.8.10.2a" xref="S5.E1.m1.8.9.1.cmml">â¡</mo><mstyle displaystyle="false" scriptlevel="+1" id="S5.E1.m1.8.8.8.8.8.8" xref="S5.E1.m1.8.8.8.8.8.8.cmml"><mfrac id="S5.E1.m1.8.8.8.8.8.8a" xref="S5.E1.m1.8.8.8.8.8.8.cmml"><mrow id="S5.E1.m1.8.8.8.8.8.8.2.2" xref="S5.E1.m1.8.8.8.8.8.8.2.3.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.1.1" xref="S5.E1.m1.8.8.8.8.8.8.1.1.cmml">exp</mi><mo id="S5.E1.m1.8.8.8.8.8.8.2.2a" xref="S5.E1.m1.8.8.8.8.8.8.2.3.cmml">â¡</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.2.2.1" xref="S5.E1.m1.8.8.8.8.8.8.2.3.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.2" xref="S5.E1.m1.8.8.8.8.8.8.2.3.cmml">(</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.cmml"><mrow id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.4" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3.cmml">â€‹</mo><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.5" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3a" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3.cmml">â€‹</mo><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.6" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3b" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3.cmml">â€‹</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.3" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.3.cmml">(</mo><msub id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.2.cmml">z</mi><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.4" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.3.cmml">,</mo><msub id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.2.cmml">z</mi><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.3" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.5" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.3.cmml">/</mo><mi id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.4" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.4.cmml">Ï„</mi></mrow><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.3" xref="S5.E1.m1.8.8.8.8.8.8.2.3.cmml">)</mo></mrow></mrow><mrow id="S5.E1.m1.8.8.8.8.8.8.5" xref="S5.E1.m1.8.8.8.8.8.8.5.cmml"><mstyle displaystyle="false" id="S5.E1.m1.8.8.8.8.8.8.5.4" xref="S5.E1.m1.8.8.8.8.8.8.5.4.cmml"><msubsup id="S5.E1.m1.8.8.8.8.8.8.5.4a" xref="S5.E1.m1.8.8.8.8.8.8.5.4.cmml"><mo id="S5.E1.m1.8.8.8.8.8.8.5.4.2.2" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.2.cmml">âˆ‘</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.2" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.2.cmml">k</mi><mo id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.1" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.1.cmml">=</mo><mn id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.3" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.3.cmml">1</mn></mrow><mrow id="S5.E1.m1.8.8.8.8.8.8.5.4.3" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.5.4.3.2" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.2.cmml">B</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E1.m1.8.8.8.8.8.8.5.4.3.1" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.1.cmml">âˆ—</mo><mi id="S5.E1.m1.8.8.8.8.8.8.5.4.3.3" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.3.cmml">K</mi></mrow></msubsup></mstyle><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.cmml"><mrow id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.2.1.cmml">[</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.2.cmml">k</mi><mo id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.1" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.1.cmml">â‰ </mo><mi id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.2.1.cmml">]</mo></mrow><mo lspace="0.167em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.5.3.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.3.cmml">â€‹</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.3.1" xref="S5.E1.m1.8.8.8.8.8.8.3.1.cmml">exp</mi><mo id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1a" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml">â¡</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml">(</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.cmml"><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.4" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3.cmml">â€‹</mo><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.5" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3a" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3.cmml">â€‹</mo><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.6" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3b" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3.cmml">â€‹</mo><mrow id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.3.cmml">(</mo><msub id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.4" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.3.cmml">,</mo><msub id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.cmml"><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.2" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.2.cmml">z</mi><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.5" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.3.cmml">/</mo><mi id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.4" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.4.cmml">Ï„</mi></mrow><mo stretchy="false" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.3" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S5.E1.m1.8b"><apply id="S5.E1.m1.8.9.1.cmml" xref="S5.E1.m1.8.8"><eq id="S5.E1.m1.3.3.3.3.3.3.cmml" xref="S5.E1.m1.3.3.3.3.3.3"></eq><apply id="S5.E1.m1.8.9.1.2.cmml" xref="S5.E1.m1.8.8"><csymbol cd="ambiguous" id="S5.E1.m1.8.9.1.2.1.cmml" xref="S5.E1.m1.8.8">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1">â„“</ci><list id="S5.E1.m1.2.2.2.2.2.2.1.3.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2"><ci id="S5.E1.m1.2.2.2.2.2.2.1.1.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.1">ğ‘›</ci><apply id="S5.E1.m1.2.2.2.2.2.2.1.2.1.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1"><times id="S5.E1.m1.2.2.2.2.2.2.1.2.1.1.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.1"></times><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.2.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.2">ğ¶</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.3.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.3">ğ‘œ</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.4.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.4">ğ‘›</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.5.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.5">ğ‘¡</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.6.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.6">ğ‘Ÿ</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.7.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.7">ğ‘</ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.8.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.8">ğ‘ </ci><ci id="S5.E1.m1.2.2.2.2.2.2.1.2.1.9.cmml" xref="S5.E1.m1.2.2.2.2.2.2.1.2.1.9">ğ‘¡</ci></apply></list></apply><apply id="S5.E1.m1.8.9.1.3.cmml" xref="S5.E1.m1.8.8"><apply id="S5.E1.m1.8.9.1.3.1.cmml" xref="S5.E1.m1.8.8"><csymbol cd="ambiguous" id="S5.E1.m1.8.9.1.3.1.1.cmml" xref="S5.E1.m1.8.8">superscript</csymbol><apply id="S5.E1.m1.8.9.1.3.1.2.cmml" xref="S5.E1.m1.8.8"><csymbol cd="ambiguous" id="S5.E1.m1.8.9.1.3.1.2.1.cmml" xref="S5.E1.m1.8.8">subscript</csymbol><sum id="S5.E1.m1.4.4.4.4.4.4.cmml" xref="S5.E1.m1.4.4.4.4.4.4"></sum><apply id="S5.E1.m1.5.5.5.5.5.5.1.cmml" xref="S5.E1.m1.5.5.5.5.5.5.1"><eq id="S5.E1.m1.5.5.5.5.5.5.1.1.cmml" xref="S5.E1.m1.5.5.5.5.5.5.1.1"></eq><ci id="S5.E1.m1.5.5.5.5.5.5.1.2.cmml" xref="S5.E1.m1.5.5.5.5.5.5.1.2">ğ‘—</ci><cn type="integer" id="S5.E1.m1.5.5.5.5.5.5.1.3.cmml" xref="S5.E1.m1.5.5.5.5.5.5.1.3">1</cn></apply></apply><apply id="S5.E1.m1.6.6.6.6.6.6.1.cmml" xref="S5.E1.m1.6.6.6.6.6.6.1"><times id="S5.E1.m1.6.6.6.6.6.6.1.1.cmml" xref="S5.E1.m1.6.6.6.6.6.6.1.1"></times><ci id="S5.E1.m1.6.6.6.6.6.6.1.2.cmml" xref="S5.E1.m1.6.6.6.6.6.6.1.2">ğµ</ci><ci id="S5.E1.m1.6.6.6.6.6.6.1.3.cmml" xref="S5.E1.m1.6.6.6.6.6.6.1.3">ğ¾</ci></apply></apply><apply id="S5.E1.m1.8.9.1.3.2.cmml" xref="S5.E1.m1.8.8"><log id="S5.E1.m1.7.7.7.7.7.7.cmml" xref="S5.E1.m1.7.7.7.7.7.7"></log><apply id="S5.E1.m1.8.8.8.8.8.8.cmml" xref="S5.E1.m1.8.8.8.8.8.8"><divide id="S5.E1.m1.8.8.8.8.8.8.6.cmml" xref="S5.E1.m1.8.8.8.8.8.8"></divide><apply id="S5.E1.m1.8.8.8.8.8.8.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2"><exp id="S5.E1.m1.8.8.8.8.8.8.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.1.1"></exp><apply id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1"><divide id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.3"></divide><apply id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2"><times id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.3"></times><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.4.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.4">ğ‘ </ci><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.5.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.5">ğ‘–</ci><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.6.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.6">ğ‘š</ci><interval closure="open" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2"><apply id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.2">ğ‘§</ci><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.2.2.2.2.3">ğ‘—</ci></apply></interval></apply><ci id="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.4.cmml" xref="S5.E1.m1.8.8.8.8.8.8.2.2.1.1.4">ğœ</ci></apply></apply><apply id="S5.E1.m1.8.8.8.8.8.8.5.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5"><apply id="S5.E1.m1.8.8.8.8.8.8.5.4.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.5.4.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4">superscript</csymbol><apply id="S5.E1.m1.8.8.8.8.8.8.5.4.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.5.4.2.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4">subscript</csymbol><sum id="S5.E1.m1.8.8.8.8.8.8.5.4.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.2"></sum><apply id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3"><eq id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.1"></eq><ci id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.2">ğ‘˜</ci><cn type="integer" id="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.2.3.3">1</cn></apply></apply><apply id="S5.E1.m1.8.8.8.8.8.8.5.4.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3"><times id="S5.E1.m1.8.8.8.8.8.8.5.4.3.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.1"></times><ci id="S5.E1.m1.8.8.8.8.8.8.5.4.3.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.2">ğµ</ci><ci id="S5.E1.m1.8.8.8.8.8.8.5.4.3.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.4.3.3">ğ¾</ci></apply></apply><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3"><times id="S5.E1.m1.8.8.8.8.8.8.5.3.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.3"></times><apply id="S5.E1.m1.8.8.8.8.8.8.4.2.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1"><csymbol cd="latexml" id="S5.E1.m1.8.8.8.8.8.8.4.2.1.2.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.2">delimited-[]</csymbol><apply id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1"><neq id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.1"></neq><ci id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.2">ğ‘˜</ci><ci id="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.4.2.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1"><exp id="S5.E1.m1.8.8.8.8.8.8.3.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.3.1"></exp><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1"><divide id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.3"></divide><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2"><times id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.3"></times><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.4.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.4">ğ‘ </ci><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.5.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.5">ğ‘–</ci><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.6.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.6">ğ‘š</ci><interval closure="open" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2"><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.1.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.2.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.2">ğ‘§</ci><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.3.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.2.2.2.2.3">ğ‘˜</ci></apply></interval></apply><ci id="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.4.cmml" xref="S5.E1.m1.8.8.8.8.8.8.5.3.2.1.1.1.4">ğœ</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.8c">\ell_{n,Contrast}=\scriptstyle{\sum_{j=1}^{B*K}\log\frac{\exp(sim(z_{i},z_{j})/\tau)}{\sum_{k=1}^{B*K}[k\neq i]\exp(sim(z_{i},z_{k})/\tau)}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.p3.6" class="ltx_p">In this equation <math id="S5.p3.3.m1.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="S5.p3.3.m1.1a"><msub id="S5.p3.3.m1.1.1" xref="S5.p3.3.m1.1.1.cmml"><mi id="S5.p3.3.m1.1.1.2" xref="S5.p3.3.m1.1.1.2.cmml">z</mi><mi id="S5.p3.3.m1.1.1.3" xref="S5.p3.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p3.3.m1.1b"><apply id="S5.p3.3.m1.1.1.cmml" xref="S5.p3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m1.1.1.1.cmml" xref="S5.p3.3.m1.1.1">subscript</csymbol><ci id="S5.p3.3.m1.1.1.2.cmml" xref="S5.p3.3.m1.1.1.2">ğ‘§</ci><ci id="S5.p3.3.m1.1.1.3.cmml" xref="S5.p3.3.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m1.1c">z_{i}</annotation></semantics></math> denotes the reconstructed input embedding, <math id="S5.p3.4.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.p3.4.m2.1a"><mi id="S5.p3.4.m2.1.1" xref="S5.p3.4.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.p3.4.m2.1b"><ci id="S5.p3.4.m2.1.1.cmml" xref="S5.p3.4.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.4.m2.1c">K</annotation></semantics></math> represents the length of each sequence, <math id="S5.p3.5.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S5.p3.5.m3.1a"><mi id="S5.p3.5.m3.1.1" xref="S5.p3.5.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S5.p3.5.m3.1b"><ci id="S5.p3.5.m3.1.1.cmml" xref="S5.p3.5.m3.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.5.m3.1c">B</annotation></semantics></math> denotes the batch size, and <math id="S5.p3.6.m4.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.p3.6.m4.1a"><mi id="S5.p3.6.m4.1.1" xref="S5.p3.6.m4.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S5.p3.6.m4.1b"><ci id="S5.p3.6.m4.1.1.cmml" xref="S5.p3.6.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.6.m4.1c">\tau</annotation></semantics></math> is a tunable temperature hyperparameter.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.3" class="ltx_p">We also observed that non-textual modality inputs tended to collapse when combined with simpler textual inputs, such as prior context or ASR transcripts. To counter this, we include an additional orthogonality loss, designed to encourage maximal dissimilarity among aligned embeddings in each batch sequence. This methodology is similar to previous efforts to promote distinct class embeddings <cite class="ltx_cite ltx_citemacro_cite">Ranasinghe etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, but in our case, we treat each embedding as a distinct class sample. However, given the nature of these embedding sequences, some level of similarity is expected, and entirely dissimilar values (cosine similarity less than zero) are not feasible. Thus, we modify Eq. <a href="#S5.E2" title="In 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to include a slight margin allowing for minimal similarity. Below, <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="e_{i}" display="inline"><semantics id="S5.p4.1.m1.1a"><msub id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.2" xref="S5.p4.1.m1.1.1.2.cmml">e</mi><mi id="S5.p4.1.m1.1.1.3" xref="S5.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><apply id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.cmml" xref="S5.p4.1.m1.1.1">subscript</csymbol><ci id="S5.p4.1.m1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.2">ğ‘’</ci><ci id="S5.p4.1.m1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">e_{i}</annotation></semantics></math> represents a single GMADS output embedding (pre-output projection) within a batch of <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">B</annotation></semantics></math> sequences, each of length <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.p4.3.m3.1a"><mi id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><ci id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">K</annotation></semantics></math>.</p>
</div>
<div id="S5.p5" class="ltx_para">
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.4" class="ltx_math_unparsed" alttext="\ell_{Ortho}={\scriptstyle\frac{2\sum_{i=1}^{B*K-1}\sum_{j=i+1}^{B*K}max(\frac{e_{i}*e_{j}}{\|e_{i}\|*\|e_{j}\|}-h,0))}{B*K*(B*K-1)}}" display="block"><semantics id="S5.E2.m1.4a"><mtable displaystyle="true" id="S5.E2.m1.4.4"><mtr id="S5.E2.m1.4.4a"><mtd class="ltx_align_left" columnalign="left" id="S5.E2.m1.4.4b"><mrow id="S5.E2.m1.4.4.4.4.4"><msub id="S5.E2.m1.4.4.4.4.4.5"><mi mathvariant="normal" id="S5.E2.m1.1.1.1.1.1.1">â„“</mi><mrow id="S5.E2.m1.2.2.2.2.2.2.1"><mi id="S5.E2.m1.2.2.2.2.2.2.1.2">O</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.2.2.2.2.2.2.1.1">â€‹</mo><mi id="S5.E2.m1.2.2.2.2.2.2.1.3">r</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.2.2.2.2.2.2.1.1a">â€‹</mo><mi id="S5.E2.m1.2.2.2.2.2.2.1.4">t</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.2.2.2.2.2.2.1.1b">â€‹</mo><mi id="S5.E2.m1.2.2.2.2.2.2.1.5">h</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.2.2.2.2.2.2.1.1c">â€‹</mo><mi id="S5.E2.m1.2.2.2.2.2.2.1.6">o</mi></mrow></msub><mo id="S5.E2.m1.3.3.3.3.3.3">=</mo><mstyle displaystyle="false" scriptlevel="+1" id="S5.E2.m1.4.4.4.4.4.4"><mfrac id="S5.E2.m1.4.4.4.4.4.4a"><mrow id="S5.E2.m1.4.4.4.4.4.4.3"><mn id="S5.E2.m1.4.4.4.4.4.4.3.4">2</mn><mstyle displaystyle="false" id="S5.E2.m1.4.4.4.4.4.4.3.5"><msubsup id="S5.E2.m1.4.4.4.4.4.4.3.5a"><mo id="S5.E2.m1.4.4.4.4.4.4.3.5.2.2">âˆ‘</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.3.5.2.3"><mi id="S5.E2.m1.4.4.4.4.4.4.3.5.2.3.2">i</mi><mo id="S5.E2.m1.4.4.4.4.4.4.3.5.2.3.1">=</mo><mn id="S5.E2.m1.4.4.4.4.4.4.3.5.2.3.3">1</mn></mrow><mrow id="S5.E2.m1.4.4.4.4.4.4.3.5.3"><mrow id="S5.E2.m1.4.4.4.4.4.4.3.5.3.2"><mi id="S5.E2.m1.4.4.4.4.4.4.3.5.3.2.2">B</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.3.5.3.2.1">âˆ—</mo><mi id="S5.E2.m1.4.4.4.4.4.4.3.5.3.2.3">K</mi></mrow><mo id="S5.E2.m1.4.4.4.4.4.4.3.5.3.1">âˆ’</mo><mn id="S5.E2.m1.4.4.4.4.4.4.3.5.3.3">1</mn></mrow></msubsup></mstyle><mstyle displaystyle="false" id="S5.E2.m1.4.4.4.4.4.4.3.6"><msubsup id="S5.E2.m1.4.4.4.4.4.4.3.6a"><mo id="S5.E2.m1.4.4.4.4.4.4.3.6.2.2">âˆ‘</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3"><mi id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.2">j</mi><mo id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.1">=</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.3"><mi id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.3.2">i</mi><mo id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.3.1">+</mo><mn id="S5.E2.m1.4.4.4.4.4.4.3.6.2.3.3.3">1</mn></mrow></mrow><mrow id="S5.E2.m1.4.4.4.4.4.4.3.6.3"><mi id="S5.E2.m1.4.4.4.4.4.4.3.6.3.2">B</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.3.6.3.1">âˆ—</mo><mi id="S5.E2.m1.4.4.4.4.4.4.3.6.3.3">K</mi></mrow></msubsup></mstyle><mi id="S5.E2.m1.4.4.4.4.4.4.3.7">m</mi><mi id="S5.E2.m1.4.4.4.4.4.4.3.8">a</mi><mi id="S5.E2.m1.4.4.4.4.4.4.3.9">x</mi><mrow id="S5.E2.m1.4.4.4.4.4.4.3.10"><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.3.10.1">(</mo><mfrac id="S5.E2.m1.4.4.4.4.4.4.2.2"><mrow id="S5.E2.m1.4.4.4.4.4.4.2.2.4"><msub id="S5.E2.m1.4.4.4.4.4.4.2.2.4.2"><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.4.2.2">e</mi><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.4.2.3">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.2.2.4.1">âˆ—</mo><msub id="S5.E2.m1.4.4.4.4.4.4.2.2.4.3"><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.4.3.2">e</mi><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.4.3.3">j</mi></msub></mrow><mrow id="S5.E2.m1.4.4.4.4.4.4.2.2.2"><mrow id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1"><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1.2">â€–</mo><msub id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1.1"><mi id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1.1.2">e</mi><mi id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1.1.3">i</mi></msub><mo rspace="0.055em" stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.1.1.1.1.1.3">â€–</mo></mrow><mo rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.2.2.2.3">âˆ—</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1"><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1.2">â€–</mo><msub id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1.1"><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1.1.2">e</mi><mi id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1.1.3">j</mi></msub><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.2.2.2.2.1.3">â€–</mo></mrow></mrow></mfrac><mo id="S5.E2.m1.4.4.4.4.4.4.3.10.2">âˆ’</mo><mi id="S5.E2.m1.4.4.4.4.4.4.3.10.3">h</mi><mo id="S5.E2.m1.4.4.4.4.4.4.3.10.4">,</mo><mn id="S5.E2.m1.4.4.4.4.4.4.3.3">0</mn><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.3.10.5">)</mo></mrow><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.3.11">)</mo></mrow><mrow id="S5.E2.m1.4.4.4.4.4.4.4"><mi id="S5.E2.m1.4.4.4.4.4.4.4.3">B</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.4.2">âˆ—</mo><mi id="S5.E2.m1.4.4.4.4.4.4.4.4">K</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.4.2a">âˆ—</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.4.1.1"><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.4.1.1.2">(</mo><mrow id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1"><mrow id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.2"><mi id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.2.2">B</mi><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.2.1">âˆ—</mo><mi id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.2.3">K</mi></mrow><mo id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.1">âˆ’</mo><mn id="S5.E2.m1.4.4.4.4.4.4.4.1.1.1.3">1</mn></mrow><mo stretchy="false" id="S5.E2.m1.4.4.4.4.4.4.4.1.1.3">)</mo></mrow></mrow></mfrac></mstyle></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S5.E2.m1.4b">\ell_{Ortho}={\scriptstyle\frac{2\sum_{i=1}^{B*K-1}\sum_{j=i+1}^{B*K}max(\frac{e_{i}*e_{j}}{\|e_{i}\|*\|e_{j}\|}-h,0))}{B*K*(B*K-1)}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">The GMADS module attempts to provide several advantages over the direct use of raw modality embeddings with the LLM decoder or mean pooling. Firstly, GMADS operates at reduced dimensional scales compared to the pretrained LLM, which significantly lowers memory requirements, requiring the much larger decoder to process shorter (reduced to only <math id="S5.p6.1.m1.1" class="ltx_Math" alttext="2/S" display="inline"><semantics id="S5.p6.1.m1.1a"><mrow id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml"><mn id="S5.p6.1.m1.1.1.2" xref="S5.p6.1.m1.1.1.2.cmml">2</mn><mo id="S5.p6.1.m1.1.1.1" xref="S5.p6.1.m1.1.1.1.cmml">/</mo><mi id="S5.p6.1.m1.1.1.3" xref="S5.p6.1.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><apply id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1"><divide id="S5.p6.1.m1.1.1.1.cmml" xref="S5.p6.1.m1.1.1.1"></divide><cn type="integer" id="S5.p6.1.m1.1.1.2.cmml" xref="S5.p6.1.m1.1.1.2">2</cn><ci id="S5.p6.1.m1.1.1.3.cmml" xref="S5.p6.1.m1.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">2/S</annotation></semantics></math> the size) input sequences. Moreover, the modality inputs do not necessitate autoregressive generation alongside these inputs, further conserving cost. Secondly, GMADS empowers the model to selectively learn its downsampling process, including choices on whether to focus locally or integrate global features, allowing some degree of information restructuring. The incorporation of cross-modality encoding enables parts of the downsampled embeddings to capture essential information across modalities while maintaining individual modality components in the outputs ensuring that some portion of the output embeddings is conditioned on each modality, requiring the attention mechanisms to remain sensitive to all modalities.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">For feature extraction, we utilize the visual encoder from DINOv2 <cite class="ltx_cite ltx_citemacro_citep">(Oquab etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2024</a>)</cite> for visual inputs and the encoder from Whisper-Large V3 <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> for audio. We retain only the necessary encoder portions of these pretrained models. In alignment with successful semantic parsing efforts from speech <cite class="ltx_cite ltx_citemacro_citep">(Arora etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, we perform ASR transcription on the audio, appending these textual embeddings to the prior context embeddings. ASR transcriptions are generated using the Whisper-medium.en model. To enable scalable fine-tuning, we integrate LoRa adaptation layers into Llama 2 7B and freeze all feature extractors.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2406.06438/assets/figures/vg-spice-model_2.jpg" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="720" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>a) The architecture of the AViD-SP model for VG-SPICE, integrating pretrained encoders and large language models (LLMs) with LoRa adapters and feature fusion modules. Trained and frozen segments of the model are denoted by fire and snowflake icons, respectively. b) Our novel Grouped Modality Attention Down Sampler module, enabling integrated cross-modality fusion and downsampling. Green modules share weights. For downsampling, we utilize meanpooling, and for upsampling we linearly interpolate the embeddings. </figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Training Routine</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.4" class="ltx_p">We train AViD-SP using cross-entropy loss (Eq. <a href="#S5.E3" title="In 5.1 Training Routine â€£ 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) between the predicted and reference Formal Semantic Parses, alongside the objectives in Eq. <a href="#S5.E1" title="In 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S5.E2" title="In 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our comprehensive loss function is outlined below in Eq. <a href="#S5.E4" title="In 5.1 Training Routine â€£ 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where <math id="S5.SS1.p1.1.m1.2" class="ltx_Math" alttext="p_{i,k}" display="inline"><semantics id="S5.SS1.p1.1.m1.2a"><msub id="S5.SS1.p1.1.m1.2.3" xref="S5.SS1.p1.1.m1.2.3.cmml"><mi id="S5.SS1.p1.1.m1.2.3.2" xref="S5.SS1.p1.1.m1.2.3.2.cmml">p</mi><mrow id="S5.SS1.p1.1.m1.2.2.2.4" xref="S5.SS1.p1.1.m1.2.2.2.3.cmml"><mi id="S5.SS1.p1.1.m1.1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p1.1.m1.2.2.2.4.1" xref="S5.SS1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p1.1.m1.2.2.2.2" xref="S5.SS1.p1.1.m1.2.2.2.2.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.2b"><apply id="S5.SS1.p1.1.m1.2.3.cmml" xref="S5.SS1.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.2.3.1.cmml" xref="S5.SS1.p1.1.m1.2.3">subscript</csymbol><ci id="S5.SS1.p1.1.m1.2.3.2.cmml" xref="S5.SS1.p1.1.m1.2.3.2">ğ‘</ci><list id="S5.SS1.p1.1.m1.2.2.2.3.cmml" xref="S5.SS1.p1.1.m1.2.2.2.4"><ci id="S5.SS1.p1.1.m1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1">ğ‘–</ci><ci id="S5.SS1.p1.1.m1.2.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2">ğ‘˜</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.2c">p_{i,k}</annotation></semantics></math> denotes the softmax prediction for each of the <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">k</annotation></semantics></math> tokens in <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><msub id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">P</mi><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">ğ‘ƒ</ci><ci id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">P_{i}</annotation></semantics></math>, and <math id="S5.SS1.p1.4.m4.2" class="ltx_Math" alttext="t_{i,k}" display="inline"><semantics id="S5.SS1.p1.4.m4.2a"><msub id="S5.SS1.p1.4.m4.2.3" xref="S5.SS1.p1.4.m4.2.3.cmml"><mi id="S5.SS1.p1.4.m4.2.3.2" xref="S5.SS1.p1.4.m4.2.3.2.cmml">t</mi><mrow id="S5.SS1.p1.4.m4.2.2.2.4" xref="S5.SS1.p1.4.m4.2.2.2.3.cmml"><mi id="S5.SS1.p1.4.m4.1.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p1.4.m4.2.2.2.4.1" xref="S5.SS1.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p1.4.m4.2.2.2.2" xref="S5.SS1.p1.4.m4.2.2.2.2.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.2b"><apply id="S5.SS1.p1.4.m4.2.3.cmml" xref="S5.SS1.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.2.3.1.cmml" xref="S5.SS1.p1.4.m4.2.3">subscript</csymbol><ci id="S5.SS1.p1.4.m4.2.3.2.cmml" xref="S5.SS1.p1.4.m4.2.3.2">ğ‘¡</ci><list id="S5.SS1.p1.4.m4.2.2.2.3.cmml" xref="S5.SS1.p1.4.m4.2.2.2.4"><ci id="S5.SS1.p1.4.m4.1.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1.1">ğ‘–</ci><ci id="S5.SS1.p1.4.m4.2.2.2.2.cmml" xref="S5.SS1.p1.4.m4.2.2.2.2">ğ‘˜</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.2c">t_{i,k}</annotation></semantics></math> represents the corresponding ground-truth token label.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.6" class="ltx_Math" alttext="\ell_{CE}=-\sum_{k=1}^{n}t_{i,k}\log(p_{i,k})" display="block"><semantics id="S5.E3.m1.6a"><mrow id="S5.E3.m1.6.6" xref="S5.E3.m1.6.6.cmml"><msub id="S5.E3.m1.6.6.3" xref="S5.E3.m1.6.6.3.cmml"><mi mathvariant="normal" id="S5.E3.m1.6.6.3.2" xref="S5.E3.m1.6.6.3.2.cmml">â„“</mi><mrow id="S5.E3.m1.6.6.3.3" xref="S5.E3.m1.6.6.3.3.cmml"><mi id="S5.E3.m1.6.6.3.3.2" xref="S5.E3.m1.6.6.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.6.6.3.3.1" xref="S5.E3.m1.6.6.3.3.1.cmml">â€‹</mo><mi id="S5.E3.m1.6.6.3.3.3" xref="S5.E3.m1.6.6.3.3.3.cmml">E</mi></mrow></msub><mo id="S5.E3.m1.6.6.2" xref="S5.E3.m1.6.6.2.cmml">=</mo><mrow id="S5.E3.m1.6.6.1" xref="S5.E3.m1.6.6.1.cmml"><mo id="S5.E3.m1.6.6.1a" xref="S5.E3.m1.6.6.1.cmml">âˆ’</mo><mrow id="S5.E3.m1.6.6.1.1" xref="S5.E3.m1.6.6.1.1.cmml"><munderover id="S5.E3.m1.6.6.1.1.2" xref="S5.E3.m1.6.6.1.1.2.cmml"><mo movablelimits="false" id="S5.E3.m1.6.6.1.1.2.2.2" xref="S5.E3.m1.6.6.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S5.E3.m1.6.6.1.1.2.2.3" xref="S5.E3.m1.6.6.1.1.2.2.3.cmml"><mi id="S5.E3.m1.6.6.1.1.2.2.3.2" xref="S5.E3.m1.6.6.1.1.2.2.3.2.cmml">k</mi><mo id="S5.E3.m1.6.6.1.1.2.2.3.1" xref="S5.E3.m1.6.6.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E3.m1.6.6.1.1.2.2.3.3" xref="S5.E3.m1.6.6.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E3.m1.6.6.1.1.2.3" xref="S5.E3.m1.6.6.1.1.2.3.cmml">n</mi></munderover><mrow id="S5.E3.m1.6.6.1.1.1" xref="S5.E3.m1.6.6.1.1.1.cmml"><msub id="S5.E3.m1.6.6.1.1.1.3" xref="S5.E3.m1.6.6.1.1.1.3.cmml"><mi id="S5.E3.m1.6.6.1.1.1.3.2" xref="S5.E3.m1.6.6.1.1.1.3.2.cmml">t</mi><mrow id="S5.E3.m1.2.2.2.4" xref="S5.E3.m1.2.2.2.3.cmml"><mi id="S5.E3.m1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml">i</mi><mo id="S5.E3.m1.2.2.2.4.1" xref="S5.E3.m1.2.2.2.3.cmml">,</mo><mi id="S5.E3.m1.2.2.2.2" xref="S5.E3.m1.2.2.2.2.cmml">k</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S5.E3.m1.6.6.1.1.1.2" xref="S5.E3.m1.6.6.1.1.1.2.cmml">â€‹</mo><mrow id="S5.E3.m1.6.6.1.1.1.1.1" xref="S5.E3.m1.6.6.1.1.1.1.2.cmml"><mi id="S5.E3.m1.5.5" xref="S5.E3.m1.5.5.cmml">log</mi><mo id="S5.E3.m1.6.6.1.1.1.1.1a" xref="S5.E3.m1.6.6.1.1.1.1.2.cmml">â¡</mo><mrow id="S5.E3.m1.6.6.1.1.1.1.1.1" xref="S5.E3.m1.6.6.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.E3.m1.6.6.1.1.1.1.1.1.2" xref="S5.E3.m1.6.6.1.1.1.1.2.cmml">(</mo><msub id="S5.E3.m1.6.6.1.1.1.1.1.1.1" xref="S5.E3.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S5.E3.m1.6.6.1.1.1.1.1.1.1.2" xref="S5.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml">p</mi><mrow id="S5.E3.m1.4.4.2.4" xref="S5.E3.m1.4.4.2.3.cmml"><mi id="S5.E3.m1.3.3.1.1" xref="S5.E3.m1.3.3.1.1.cmml">i</mi><mo id="S5.E3.m1.4.4.2.4.1" xref="S5.E3.m1.4.4.2.3.cmml">,</mo><mi id="S5.E3.m1.4.4.2.2" xref="S5.E3.m1.4.4.2.2.cmml">k</mi></mrow></msub><mo stretchy="false" id="S5.E3.m1.6.6.1.1.1.1.1.1.3" xref="S5.E3.m1.6.6.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.6b"><apply id="S5.E3.m1.6.6.cmml" xref="S5.E3.m1.6.6"><eq id="S5.E3.m1.6.6.2.cmml" xref="S5.E3.m1.6.6.2"></eq><apply id="S5.E3.m1.6.6.3.cmml" xref="S5.E3.m1.6.6.3"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.3.1.cmml" xref="S5.E3.m1.6.6.3">subscript</csymbol><ci id="S5.E3.m1.6.6.3.2.cmml" xref="S5.E3.m1.6.6.3.2">â„“</ci><apply id="S5.E3.m1.6.6.3.3.cmml" xref="S5.E3.m1.6.6.3.3"><times id="S5.E3.m1.6.6.3.3.1.cmml" xref="S5.E3.m1.6.6.3.3.1"></times><ci id="S5.E3.m1.6.6.3.3.2.cmml" xref="S5.E3.m1.6.6.3.3.2">ğ¶</ci><ci id="S5.E3.m1.6.6.3.3.3.cmml" xref="S5.E3.m1.6.6.3.3.3">ğ¸</ci></apply></apply><apply id="S5.E3.m1.6.6.1.cmml" xref="S5.E3.m1.6.6.1"><minus id="S5.E3.m1.6.6.1.2.cmml" xref="S5.E3.m1.6.6.1"></minus><apply id="S5.E3.m1.6.6.1.1.cmml" xref="S5.E3.m1.6.6.1.1"><apply id="S5.E3.m1.6.6.1.1.2.cmml" xref="S5.E3.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.1.1.2.1.cmml" xref="S5.E3.m1.6.6.1.1.2">superscript</csymbol><apply id="S5.E3.m1.6.6.1.1.2.2.cmml" xref="S5.E3.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.1.1.2.2.1.cmml" xref="S5.E3.m1.6.6.1.1.2">subscript</csymbol><sum id="S5.E3.m1.6.6.1.1.2.2.2.cmml" xref="S5.E3.m1.6.6.1.1.2.2.2"></sum><apply id="S5.E3.m1.6.6.1.1.2.2.3.cmml" xref="S5.E3.m1.6.6.1.1.2.2.3"><eq id="S5.E3.m1.6.6.1.1.2.2.3.1.cmml" xref="S5.E3.m1.6.6.1.1.2.2.3.1"></eq><ci id="S5.E3.m1.6.6.1.1.2.2.3.2.cmml" xref="S5.E3.m1.6.6.1.1.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S5.E3.m1.6.6.1.1.2.2.3.3.cmml" xref="S5.E3.m1.6.6.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E3.m1.6.6.1.1.2.3.cmml" xref="S5.E3.m1.6.6.1.1.2.3">ğ‘›</ci></apply><apply id="S5.E3.m1.6.6.1.1.1.cmml" xref="S5.E3.m1.6.6.1.1.1"><times id="S5.E3.m1.6.6.1.1.1.2.cmml" xref="S5.E3.m1.6.6.1.1.1.2"></times><apply id="S5.E3.m1.6.6.1.1.1.3.cmml" xref="S5.E3.m1.6.6.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.1.1.1.3.1.cmml" xref="S5.E3.m1.6.6.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.6.6.1.1.1.3.2.cmml" xref="S5.E3.m1.6.6.1.1.1.3.2">ğ‘¡</ci><list id="S5.E3.m1.2.2.2.3.cmml" xref="S5.E3.m1.2.2.2.4"><ci id="S5.E3.m1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1">ğ‘–</ci><ci id="S5.E3.m1.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2">ğ‘˜</ci></list></apply><apply id="S5.E3.m1.6.6.1.1.1.1.2.cmml" xref="S5.E3.m1.6.6.1.1.1.1.1"><log id="S5.E3.m1.5.5.cmml" xref="S5.E3.m1.5.5"></log><apply id="S5.E3.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.6.6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.6.6.1.1.1.1.1.1.1.2">ğ‘</ci><list id="S5.E3.m1.4.4.2.3.cmml" xref="S5.E3.m1.4.4.2.4"><ci id="S5.E3.m1.3.3.1.1.cmml" xref="S5.E3.m1.3.3.1.1">ğ‘–</ci><ci id="S5.E3.m1.4.4.2.2.cmml" xref="S5.E3.m1.4.4.2.2">ğ‘˜</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.6c">\ell_{CE}=-\sum_{k=1}^{n}t_{i,k}\log(p_{i,k})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.2" class="ltx_Math" alttext="L=\alpha\ell_{CE}+\beta\ell_{Ortho}+\frac{\gamma}{N}\sum_{n=1}^{N}\ell_{n,Contrast}" display="block"><semantics id="S5.E4.m1.2a"><mrow id="S5.E4.m1.2.3" xref="S5.E4.m1.2.3.cmml"><mi id="S5.E4.m1.2.3.2" xref="S5.E4.m1.2.3.2.cmml">L</mi><mo id="S5.E4.m1.2.3.1" xref="S5.E4.m1.2.3.1.cmml">=</mo><mrow id="S5.E4.m1.2.3.3" xref="S5.E4.m1.2.3.3.cmml"><mrow id="S5.E4.m1.2.3.3.2" xref="S5.E4.m1.2.3.3.2.cmml"><mi id="S5.E4.m1.2.3.3.2.2" xref="S5.E4.m1.2.3.3.2.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.2.1" xref="S5.E4.m1.2.3.3.2.1.cmml">â€‹</mo><msub id="S5.E4.m1.2.3.3.2.3" xref="S5.E4.m1.2.3.3.2.3.cmml"><mi mathvariant="normal" id="S5.E4.m1.2.3.3.2.3.2" xref="S5.E4.m1.2.3.3.2.3.2.cmml">â„“</mi><mrow id="S5.E4.m1.2.3.3.2.3.3" xref="S5.E4.m1.2.3.3.2.3.3.cmml"><mi id="S5.E4.m1.2.3.3.2.3.3.2" xref="S5.E4.m1.2.3.3.2.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.2.3.3.1" xref="S5.E4.m1.2.3.3.2.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.3.3.2.3.3.3" xref="S5.E4.m1.2.3.3.2.3.3.3.cmml">E</mi></mrow></msub></mrow><mo id="S5.E4.m1.2.3.3.1" xref="S5.E4.m1.2.3.3.1.cmml">+</mo><mrow id="S5.E4.m1.2.3.3.3" xref="S5.E4.m1.2.3.3.3.cmml"><mi id="S5.E4.m1.2.3.3.3.2" xref="S5.E4.m1.2.3.3.3.2.cmml">Î²</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.3.1" xref="S5.E4.m1.2.3.3.3.1.cmml">â€‹</mo><msub id="S5.E4.m1.2.3.3.3.3" xref="S5.E4.m1.2.3.3.3.3.cmml"><mi mathvariant="normal" id="S5.E4.m1.2.3.3.3.3.2" xref="S5.E4.m1.2.3.3.3.3.2.cmml">â„“</mi><mrow id="S5.E4.m1.2.3.3.3.3.3" xref="S5.E4.m1.2.3.3.3.3.3.cmml"><mi id="S5.E4.m1.2.3.3.3.3.3.2" xref="S5.E4.m1.2.3.3.3.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.3.3.3.1" xref="S5.E4.m1.2.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.3.3.3.3.3.3" xref="S5.E4.m1.2.3.3.3.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.3.3.3.1a" xref="S5.E4.m1.2.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.3.3.3.3.3.4" xref="S5.E4.m1.2.3.3.3.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.3.3.3.1b" xref="S5.E4.m1.2.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.3.3.3.3.3.5" xref="S5.E4.m1.2.3.3.3.3.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.3.3.3.1c" xref="S5.E4.m1.2.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.3.3.3.3.3.6" xref="S5.E4.m1.2.3.3.3.3.3.6.cmml">o</mi></mrow></msub></mrow><mo id="S5.E4.m1.2.3.3.1a" xref="S5.E4.m1.2.3.3.1.cmml">+</mo><mrow id="S5.E4.m1.2.3.3.4" xref="S5.E4.m1.2.3.3.4.cmml"><mfrac id="S5.E4.m1.2.3.3.4.2" xref="S5.E4.m1.2.3.3.4.2.cmml"><mi id="S5.E4.m1.2.3.3.4.2.2" xref="S5.E4.m1.2.3.3.4.2.2.cmml">Î³</mi><mi id="S5.E4.m1.2.3.3.4.2.3" xref="S5.E4.m1.2.3.3.4.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.3.3.4.1" xref="S5.E4.m1.2.3.3.4.1.cmml">â€‹</mo><mrow id="S5.E4.m1.2.3.3.4.3" xref="S5.E4.m1.2.3.3.4.3.cmml"><munderover id="S5.E4.m1.2.3.3.4.3.1" xref="S5.E4.m1.2.3.3.4.3.1.cmml"><mo movablelimits="false" id="S5.E4.m1.2.3.3.4.3.1.2.2" xref="S5.E4.m1.2.3.3.4.3.1.2.2.cmml">âˆ‘</mo><mrow id="S5.E4.m1.2.3.3.4.3.1.2.3" xref="S5.E4.m1.2.3.3.4.3.1.2.3.cmml"><mi id="S5.E4.m1.2.3.3.4.3.1.2.3.2" xref="S5.E4.m1.2.3.3.4.3.1.2.3.2.cmml">n</mi><mo id="S5.E4.m1.2.3.3.4.3.1.2.3.1" xref="S5.E4.m1.2.3.3.4.3.1.2.3.1.cmml">=</mo><mn id="S5.E4.m1.2.3.3.4.3.1.2.3.3" xref="S5.E4.m1.2.3.3.4.3.1.2.3.3.cmml">1</mn></mrow><mi id="S5.E4.m1.2.3.3.4.3.1.3" xref="S5.E4.m1.2.3.3.4.3.1.3.cmml">N</mi></munderover><msub id="S5.E4.m1.2.3.3.4.3.2" xref="S5.E4.m1.2.3.3.4.3.2.cmml"><mi mathvariant="normal" id="S5.E4.m1.2.3.3.4.3.2.2" xref="S5.E4.m1.2.3.3.4.3.2.2.cmml">â„“</mi><mrow id="S5.E4.m1.2.2.2.2" xref="S5.E4.m1.2.2.2.3.cmml"><mi id="S5.E4.m1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.cmml">n</mi><mo id="S5.E4.m1.2.2.2.2.2" xref="S5.E4.m1.2.2.2.3.cmml">,</mo><mrow id="S5.E4.m1.2.2.2.2.1" xref="S5.E4.m1.2.2.2.2.1.cmml"><mi id="S5.E4.m1.2.2.2.2.1.2" xref="S5.E4.m1.2.2.2.2.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.3" xref="S5.E4.m1.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1a" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.4" xref="S5.E4.m1.2.2.2.2.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1b" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.5" xref="S5.E4.m1.2.2.2.2.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1c" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.6" xref="S5.E4.m1.2.2.2.2.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1d" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.7" xref="S5.E4.m1.2.2.2.2.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1e" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.8" xref="S5.E4.m1.2.2.2.2.1.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.2.2.2.2.1.1f" xref="S5.E4.m1.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S5.E4.m1.2.2.2.2.1.9" xref="S5.E4.m1.2.2.2.2.1.9.cmml">t</mi></mrow></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.2b"><apply id="S5.E4.m1.2.3.cmml" xref="S5.E4.m1.2.3"><eq id="S5.E4.m1.2.3.1.cmml" xref="S5.E4.m1.2.3.1"></eq><ci id="S5.E4.m1.2.3.2.cmml" xref="S5.E4.m1.2.3.2">ğ¿</ci><apply id="S5.E4.m1.2.3.3.cmml" xref="S5.E4.m1.2.3.3"><plus id="S5.E4.m1.2.3.3.1.cmml" xref="S5.E4.m1.2.3.3.1"></plus><apply id="S5.E4.m1.2.3.3.2.cmml" xref="S5.E4.m1.2.3.3.2"><times id="S5.E4.m1.2.3.3.2.1.cmml" xref="S5.E4.m1.2.3.3.2.1"></times><ci id="S5.E4.m1.2.3.3.2.2.cmml" xref="S5.E4.m1.2.3.3.2.2">ğ›¼</ci><apply id="S5.E4.m1.2.3.3.2.3.cmml" xref="S5.E4.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S5.E4.m1.2.3.3.2.3.1.cmml" xref="S5.E4.m1.2.3.3.2.3">subscript</csymbol><ci id="S5.E4.m1.2.3.3.2.3.2.cmml" xref="S5.E4.m1.2.3.3.2.3.2">â„“</ci><apply id="S5.E4.m1.2.3.3.2.3.3.cmml" xref="S5.E4.m1.2.3.3.2.3.3"><times id="S5.E4.m1.2.3.3.2.3.3.1.cmml" xref="S5.E4.m1.2.3.3.2.3.3.1"></times><ci id="S5.E4.m1.2.3.3.2.3.3.2.cmml" xref="S5.E4.m1.2.3.3.2.3.3.2">ğ¶</ci><ci id="S5.E4.m1.2.3.3.2.3.3.3.cmml" xref="S5.E4.m1.2.3.3.2.3.3.3">ğ¸</ci></apply></apply></apply><apply id="S5.E4.m1.2.3.3.3.cmml" xref="S5.E4.m1.2.3.3.3"><times id="S5.E4.m1.2.3.3.3.1.cmml" xref="S5.E4.m1.2.3.3.3.1"></times><ci id="S5.E4.m1.2.3.3.3.2.cmml" xref="S5.E4.m1.2.3.3.3.2">ğ›½</ci><apply id="S5.E4.m1.2.3.3.3.3.cmml" xref="S5.E4.m1.2.3.3.3.3"><csymbol cd="ambiguous" id="S5.E4.m1.2.3.3.3.3.1.cmml" xref="S5.E4.m1.2.3.3.3.3">subscript</csymbol><ci id="S5.E4.m1.2.3.3.3.3.2.cmml" xref="S5.E4.m1.2.3.3.3.3.2">â„“</ci><apply id="S5.E4.m1.2.3.3.3.3.3.cmml" xref="S5.E4.m1.2.3.3.3.3.3"><times id="S5.E4.m1.2.3.3.3.3.3.1.cmml" xref="S5.E4.m1.2.3.3.3.3.3.1"></times><ci id="S5.E4.m1.2.3.3.3.3.3.2.cmml" xref="S5.E4.m1.2.3.3.3.3.3.2">ğ‘‚</ci><ci id="S5.E4.m1.2.3.3.3.3.3.3.cmml" xref="S5.E4.m1.2.3.3.3.3.3.3">ğ‘Ÿ</ci><ci id="S5.E4.m1.2.3.3.3.3.3.4.cmml" xref="S5.E4.m1.2.3.3.3.3.3.4">ğ‘¡</ci><ci id="S5.E4.m1.2.3.3.3.3.3.5.cmml" xref="S5.E4.m1.2.3.3.3.3.3.5">â„</ci><ci id="S5.E4.m1.2.3.3.3.3.3.6.cmml" xref="S5.E4.m1.2.3.3.3.3.3.6">ğ‘œ</ci></apply></apply></apply><apply id="S5.E4.m1.2.3.3.4.cmml" xref="S5.E4.m1.2.3.3.4"><times id="S5.E4.m1.2.3.3.4.1.cmml" xref="S5.E4.m1.2.3.3.4.1"></times><apply id="S5.E4.m1.2.3.3.4.2.cmml" xref="S5.E4.m1.2.3.3.4.2"><divide id="S5.E4.m1.2.3.3.4.2.1.cmml" xref="S5.E4.m1.2.3.3.4.2"></divide><ci id="S5.E4.m1.2.3.3.4.2.2.cmml" xref="S5.E4.m1.2.3.3.4.2.2">ğ›¾</ci><ci id="S5.E4.m1.2.3.3.4.2.3.cmml" xref="S5.E4.m1.2.3.3.4.2.3">ğ‘</ci></apply><apply id="S5.E4.m1.2.3.3.4.3.cmml" xref="S5.E4.m1.2.3.3.4.3"><apply id="S5.E4.m1.2.3.3.4.3.1.cmml" xref="S5.E4.m1.2.3.3.4.3.1"><csymbol cd="ambiguous" id="S5.E4.m1.2.3.3.4.3.1.1.cmml" xref="S5.E4.m1.2.3.3.4.3.1">superscript</csymbol><apply id="S5.E4.m1.2.3.3.4.3.1.2.cmml" xref="S5.E4.m1.2.3.3.4.3.1"><csymbol cd="ambiguous" id="S5.E4.m1.2.3.3.4.3.1.2.1.cmml" xref="S5.E4.m1.2.3.3.4.3.1">subscript</csymbol><sum id="S5.E4.m1.2.3.3.4.3.1.2.2.cmml" xref="S5.E4.m1.2.3.3.4.3.1.2.2"></sum><apply id="S5.E4.m1.2.3.3.4.3.1.2.3.cmml" xref="S5.E4.m1.2.3.3.4.3.1.2.3"><eq id="S5.E4.m1.2.3.3.4.3.1.2.3.1.cmml" xref="S5.E4.m1.2.3.3.4.3.1.2.3.1"></eq><ci id="S5.E4.m1.2.3.3.4.3.1.2.3.2.cmml" xref="S5.E4.m1.2.3.3.4.3.1.2.3.2">ğ‘›</ci><cn type="integer" id="S5.E4.m1.2.3.3.4.3.1.2.3.3.cmml" xref="S5.E4.m1.2.3.3.4.3.1.2.3.3">1</cn></apply></apply><ci id="S5.E4.m1.2.3.3.4.3.1.3.cmml" xref="S5.E4.m1.2.3.3.4.3.1.3">ğ‘</ci></apply><apply id="S5.E4.m1.2.3.3.4.3.2.cmml" xref="S5.E4.m1.2.3.3.4.3.2"><csymbol cd="ambiguous" id="S5.E4.m1.2.3.3.4.3.2.1.cmml" xref="S5.E4.m1.2.3.3.4.3.2">subscript</csymbol><ci id="S5.E4.m1.2.3.3.4.3.2.2.cmml" xref="S5.E4.m1.2.3.3.4.3.2.2">â„“</ci><list id="S5.E4.m1.2.2.2.3.cmml" xref="S5.E4.m1.2.2.2.2"><ci id="S5.E4.m1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1">ğ‘›</ci><apply id="S5.E4.m1.2.2.2.2.1.cmml" xref="S5.E4.m1.2.2.2.2.1"><times id="S5.E4.m1.2.2.2.2.1.1.cmml" xref="S5.E4.m1.2.2.2.2.1.1"></times><ci id="S5.E4.m1.2.2.2.2.1.2.cmml" xref="S5.E4.m1.2.2.2.2.1.2">ğ¶</ci><ci id="S5.E4.m1.2.2.2.2.1.3.cmml" xref="S5.E4.m1.2.2.2.2.1.3">ğ‘œ</ci><ci id="S5.E4.m1.2.2.2.2.1.4.cmml" xref="S5.E4.m1.2.2.2.2.1.4">ğ‘›</ci><ci id="S5.E4.m1.2.2.2.2.1.5.cmml" xref="S5.E4.m1.2.2.2.2.1.5">ğ‘¡</ci><ci id="S5.E4.m1.2.2.2.2.1.6.cmml" xref="S5.E4.m1.2.2.2.2.1.6">ğ‘Ÿ</ci><ci id="S5.E4.m1.2.2.2.2.1.7.cmml" xref="S5.E4.m1.2.2.2.2.1.7">ğ‘</ci><ci id="S5.E4.m1.2.2.2.2.1.8.cmml" xref="S5.E4.m1.2.2.2.2.1.8">ğ‘ </ci><ci id="S5.E4.m1.2.2.2.2.1.9.cmml" xref="S5.E4.m1.2.2.2.2.1.9">ğ‘¡</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.2c">L=\alpha\ell_{CE}+\beta\ell_{Ortho}+\frac{\gamma}{N}\sum_{n=1}^{N}\ell_{n,Contrast}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">AViD-SP employs a three-layer self-attention transformer as the primary encoder transformer, each layer having an embedding dimensionality of 1024 and 8 attention heads. The secondary encoder transformer, used for the upsampled reconstruction training objective, is of the same configuration. The GMADS module employs a downsampling factor, <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">S</annotation></semantics></math>, of 16. Additionally, we enhance the key, query, and value layers of the Llama 2 7B model with Low-Rank Adaptation (LoRa) layers. No hyperparameter optimization was conducted.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">We train AViD-SP by incorporating randomly sampled CHiME5 noise to simulate audio corruption, adding this noise at various Signal-to-Noise Ratios (SNR) of 0, 2, 5, 10, or 20dB. Further details on training and inference hyperparameters are discussed in Section <a href="#A5" title="Appendix E Training and Inference Hyperparameters â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. To ensure robustness to various input feature combinations, we implement random input dropout with a probability of 30%. In these instances, we randomly omit one of the input modalities, either audio embeddings, visual embeddings, or audio transcriptions. We do not omit the prior context, as we found the task too difficult to learn under such conditions since it requires both the already known information as well as their current assigned labels under our semantic parsing framework. AViD-SP is trained in a two-stage pipeline, with the initial stage acting as pretraining without the ASR transcriptions to allow the GMADS module to reach a semi-trained state for enhanced efficiency. Subsequently, we continue fine-tuning the model with ASR transcriptions until convergence. Our initial pretraining lasts one full epochs, followed by the fine-tuning stage.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Metrics</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We use several metrics to measure how closely the generated semantic parse aligns with the ground truth and how accurately the scene graph context updates match the reference. Unlike conventional semantic parsing assessments <cite class="ltx_cite ltx_citemacro_citep">(Tomasello etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>, we omit exact-match metrics due to their unsuitability for our problem, which allows for permutation invariance in the formal-language output (see Section <a href="#A7" title="Appendix G Formal Language Definition â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>). This permits the parser to generate scene-graph updates in any order and assign node IDs freely, as long as the resulting scene graph is isomorphic to the reference.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For each below metric, we examine hard ("H") and soft ("S") variants. The hard variant penalizes missing and unnecessary information, while the soft variant only penalizes omissions. This approach accounts for the Visual Genome datasetâ€™s sparsity and the possibility of LLMs generating extraneous yet potentially valid content. For example, an LLM might enhance a "blue table" to a "vibrant blue table," making "vibrant" an acceptable attribute. Our analysis shows such inclusions are common in the VG-SPICE dataset, leading us to focus on the soft metric and qualitatively show in Section <a href="#S6" title="6 Results â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> how updated utterances accommodate these extraneous additions. We include results for GED in the supplement Table <a href="#A2.T5" title="Table 5 â€£ Appendix B Additional AViD-SP Results â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Graph Edit Distance (GED):</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">GED calculates the normalized cost to transform the predicted context to the reference one, considering only perfectly semantically equivalent Nodes, Attributes, and Edges. Missing or extra Nodes or Edges increase the error by one, while incorrect Attributes have a smaller penalty of 0.25. GED is not normalized and should be interpreted as the magnitude of incorrect features compared with the reference solution and not as a recall or precision metric. GED is particularly reliant on exact matches, so minor discrepancies (like "snow board" vs. "snowboard") can incur significant penalties, with misalignments doubly penalized in the hard variant.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Representation Edit Distance (RED):</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">RED addresses the limitations of GED by employing a â€œsofterâ€ semantic similarity to evaluate entity pairings. Using a transformer model for sentence semantic similarity<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span> The â€œen_stsb_roberta_baseâ€ model from https://github.com/MartinoMensio/spacy-sentence-bert</span></span></span>, RED groups Nodes and their Attributes into descriptive phrases (for example, a "table" Node with "vibrant" and "blue" Attributes becomes "vibrant blue table") and assesses the dissimilarity between potential pairings, using an exhaustic search for optimal pairings of Nodes and Edges. Unmatched Nodes and Edges are considered entirely dissimilar. Since unmodified graph portions from the prior context are pre-matched and excluded from the exhaustive search, the computation of the pairings remains manageable. RED is normalized by the representation edit distance needed to transform the prior context into the reference context, and so numerically can be interpreted as the percentage of missing and/or extra information relative to the reference context.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Baselines and Evaluation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To thoroughly evaluate our AViD-SP model, we conducted a series of ablation studies to explore the impact of various input modality combinations. Given that AViD-SP was trained under diverse noise conditions, its performance was tested across noise levels of 0, 2, and 20 dB using the CHiME5 dataset. We assessed the modelâ€™s capability to resolve ambiguities in audio input by introducing tests with and without visual modality, and by evaluating the model with incorrectly matched images in the GMADS module. Additionally, we explored potential enhancements in ASR performance by incorporating ground truth ASR transcriptions in our evaluations. To ablate the effects our GMADS module has on performance we compare against a version of AViD-SP trained using traditional meanpooling after a per modality projection layer to downsample the audio and visual input embeddings, with all hyperparameter and training methods matched between the two except the meanpooling baseline only utilizing the cross entropy component of the full training objective.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">We also extended our evaluations to the VG-SPICE-C Subset. Here, we analyze the subset through a single-step evaluation approach, with ground truth prior context provided and metrics measured after each individual SPICE update.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2" class="ltx_tr">
<th id="S5.T3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2">Model Type</th>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4">H-RED<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="4">S-RED<math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.2.3.1" class="ltx_tr">
<th id="S5.T3.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" colspan="2"></th>
<td id="S5.T3.2.3.1.2" class="ltx_td ltx_align_center">0dB</td>
<td id="S5.T3.2.3.1.3" class="ltx_td ltx_align_center">2dB</td>
<td id="S5.T3.2.3.1.4" class="ltx_td ltx_align_center">20dB</td>
<td id="S5.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r">Gold*</td>
<td id="S5.T3.2.3.1.6" class="ltx_td ltx_align_center">0dB</td>
<td id="S5.T3.2.3.1.7" class="ltx_td ltx_align_center">2dB</td>
<td id="S5.T3.2.3.1.8" class="ltx_td ltx_align_center">20dB</td>
<td id="S5.T3.2.3.1.9" class="ltx_td ltx_align_center">Gold*</td>
</tr>
<tr id="S5.T3.2.4.2" class="ltx_tr">
<th id="S5.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AViD-SP + GMADS</th>
<th id="S5.T3.2.4.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">Base</th>
<td id="S5.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">1.618</td>
<td id="S5.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">1.517</td>
<td id="S5.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">1.412</td>
<td id="S5.T3.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.272</td>
<td id="S5.T3.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t">0.402</td>
<td id="S5.T3.2.4.2.8" class="ltx_td ltx_align_center ltx_border_t">0.383</td>
<td id="S5.T3.2.4.2.9" class="ltx_td ltx_align_center ltx_border_t">0.3765</td>
<td id="S5.T3.2.4.2.10" class="ltx_td ltx_align_center ltx_border_t">0.348</td>
</tr>
<tr id="S5.T3.2.5.3" class="ltx_tr">
<th id="S5.T3.2.5.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.2.5.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Image</th>
<td id="S5.T3.2.5.3.3" class="ltx_td ltx_align_center">1.611</td>
<td id="S5.T3.2.5.3.4" class="ltx_td ltx_align_center">1.527</td>
<td id="S5.T3.2.5.3.5" class="ltx_td ltx_align_center">1.430</td>
<td id="S5.T3.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r">1.33</td>
<td id="S5.T3.2.5.3.7" class="ltx_td ltx_align_center">0.407</td>
<td id="S5.T3.2.5.3.8" class="ltx_td ltx_align_center">0.393</td>
<td id="S5.T3.2.5.3.9" class="ltx_td ltx_align_center">0.384</td>
<td id="S5.T3.2.5.3.10" class="ltx_td ltx_align_center">0.364</td>
</tr>
<tr id="S5.T3.2.6.4" class="ltx_tr">
<th id="S5.T3.2.6.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.2.6.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Audio</th>
<td id="S5.T3.2.6.4.3" class="ltx_td ltx_align_center">1.660</td>
<td id="S5.T3.2.6.4.4" class="ltx_td ltx_align_center">1.607</td>
<td id="S5.T3.2.6.4.5" class="ltx_td ltx_align_center">1.590</td>
<td id="S5.T3.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r">1.540</td>
<td id="S5.T3.2.6.4.7" class="ltx_td ltx_align_center">0.570</td>
<td id="S5.T3.2.6.4.8" class="ltx_td ltx_align_center">0.559</td>
<td id="S5.T3.2.6.4.9" class="ltx_td ltx_align_center">0.538</td>
<td id="S5.T3.2.6.4.10" class="ltx_td ltx_align_center">0.481</td>
</tr>
<tr id="S5.T3.2.7.5" class="ltx_tr">
<th id="S5.T3.2.7.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.2.7.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w Incorrect Image**</th>
<td id="S5.T3.2.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.7.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.7.5.5" class="ltx_td ltx_align_center">1.423</td>
<td id="S5.T3.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T3.2.7.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.7.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.7.5.9" class="ltx_td ltx_align_center">0.381</td>
<td id="S5.T3.2.7.5.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.2.8.6" class="ltx_tr">
<th id="S5.T3.2.8.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.2.8.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Prior Context***</th>
<td id="S5.T3.2.8.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.8.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.8.6.5" class="ltx_td ltx_align_center">3.428</td>
<td id="S5.T3.2.8.6.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T3.2.8.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.8.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.2.8.6.9" class="ltx_td ltx_align_center">0.478</td>
<td id="S5.T3.2.8.6.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.2.9.7" class="ltx_tr">
<th id="S5.T3.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AViD-SP + Meanpool</th>
<th id="S5.T3.2.9.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">Base</th>
<td id="S5.T3.2.9.7.3" class="ltx_td ltx_align_center ltx_border_t">1.083</td>
<td id="S5.T3.2.9.7.4" class="ltx_td ltx_align_center ltx_border_t">1.038</td>
<td id="S5.T3.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">0.940</td>
<td id="S5.T3.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.817</td>
<td id="S5.T3.2.9.7.7" class="ltx_td ltx_align_center ltx_border_t">0.377</td>
<td id="S5.T3.2.9.7.8" class="ltx_td ltx_align_center ltx_border_t">0.368</td>
<td id="S5.T3.2.9.7.9" class="ltx_td ltx_align_center ltx_border_t">0.359</td>
<td id="S5.T3.2.9.7.10" class="ltx_td ltx_align_center ltx_border_t">0.323</td>
</tr>
<tr id="S5.T3.2.10.8" class="ltx_tr">
<th id="S5.T3.2.10.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.2.10.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Image</th>
<td id="S5.T3.2.10.8.3" class="ltx_td ltx_align_center">1.051</td>
<td id="S5.T3.2.10.8.4" class="ltx_td ltx_align_center">0.980</td>
<td id="S5.T3.2.10.8.5" class="ltx_td ltx_align_center">0.911</td>
<td id="S5.T3.2.10.8.6" class="ltx_td ltx_align_center ltx_border_r">0.826</td>
<td id="S5.T3.2.10.8.7" class="ltx_td ltx_align_center">0.386</td>
<td id="S5.T3.2.10.8.8" class="ltx_td ltx_align_center">0.377</td>
<td id="S5.T3.2.10.8.9" class="ltx_td ltx_align_center">0.362</td>
<td id="S5.T3.2.10.8.10" class="ltx_td ltx_align_center">0.330</td>
</tr>
<tr id="S5.T3.2.11.9" class="ltx_tr">
<th id="S5.T3.2.11.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<th id="S5.T3.2.11.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r">w/o Audio</th>
<td id="S5.T3.2.11.9.3" class="ltx_td ltx_align_center ltx_border_b">0.946</td>
<td id="S5.T3.2.11.9.4" class="ltx_td ltx_align_center ltx_border_b">0.897</td>
<td id="S5.T3.2.11.9.5" class="ltx_td ltx_align_center ltx_border_b">0.804</td>
<td id="S5.T3.2.11.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.759</td>
<td id="S5.T3.2.11.9.7" class="ltx_td ltx_align_center ltx_border_b">0.414</td>
<td id="S5.T3.2.11.9.8" class="ltx_td ltx_align_center ltx_border_b">0.397</td>
<td id="S5.T3.2.11.9.9" class="ltx_td ltx_align_center ltx_border_b">0.385</td>
<td id="S5.T3.2.11.9.10" class="ltx_td ltx_align_center ltx_border_b">0.363</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
RED results on the VG-SPICE test set for our AViD-SP model. AViD-SP was trained with CHiME5 noise augmentation sampled between 0db and 20dB SNR (all CHiME5 noise followed the provided train/eval/test splits). *Given the ground truth utterance transcripts in place of the ASR transcriptions. **Evaluated by offsetting visual features within batch so incorrect image features are paired with the other input components. ***Evaluated with "Empty Context" prior state scene graphs summaries instead of the correct ones.
</figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.4.5.1" class="ltx_tr">
<th id="S5.T4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Variant</th>
<td id="S5.T4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="2">TTS</td>
<td id="S5.T4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="2">Read</td>
</tr>
<tr id="S5.T4.4.4" class="ltx_tr">
<th id="S5.T4.4.4.5" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center">H-RED<math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r">S-RED<math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mo stretchy="false" id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_center">H-RED<math id="S5.T4.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.3.3.3.m1.1a"><mo stretchy="false" id="S5.T4.3.3.3.m1.1.1" xref="S5.T4.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.4.4.4" class="ltx_td ltx_align_center">S-RED<math id="S5.T4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T4.4.4.4.m1.1.1" xref="S5.T4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.m1.1b"><ci id="S5.T4.4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.4.6.2" class="ltx_tr">
<th id="S5.T4.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GMADS</th>
<td id="S5.T4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_t">0.739</td>
<td id="S5.T4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.497</td>
<td id="S5.T4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_t">0.731</td>
<td id="S5.T4.4.6.2.5" class="ltx_td ltx_align_center ltx_border_t">0.497</td>
</tr>
<tr id="S5.T4.4.7.3" class="ltx_tr">
<th id="S5.T4.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Meanpool</th>
<td id="S5.T4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.640</td>
<td id="S5.T4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.460</td>
<td id="S5.T4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1.415</td>
<td id="S5.T4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.628</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
RED results on the VG-SPICE-C challenge test set for AViD-SP with Single Step (ground truth prior context provided for each step) metrics reported.
</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The performance of the AViD-SP model on the VG-SPICE test set, as shown in Table <a href="#S5.T3" title="Table 3 â€£ 5.3 Baselines and Evaluation â€£ 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrates that the baseline AViD-SP achieves S-RED scores just below 0.4, with the meanpooling variant slightly lower, approaching 0.38. This performance suggests a substantial effectiveness (over 60%) in assimilating desired information into the scene graph. However, the H-RED metrics indicate the introduction of moderate quantities of irrelevant information, particularly in the GMADS version. Given that VG-SPICE scene graphs are often overly sparse, the elevated H-RED values for GMADS may reflect an increased utilization of visual inputs, possibly learning to incorporate non-essential features detected through visual cues. While this interpretation is speculative, some level of elevated H-RED could be reasonable for VG-SPICE in its current state (Section <a href="#A3" title="Appendix C Qualitative AViD-SP Examples â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Under varying SNR conditions, both GMADS and meanpooling configurations of AViD-SP show minimal performance degradation at lower SNRs, indicating resilience to reasonable background noise levels. The use of accurate ASR transcriptions substantially boosts parsing accuracy, emphasizing the benefits of reliable ASR.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Experiments omitting visual inputs or incorporating incorrectly paired visual inputs exhibit minor performance declines. For the meanpooling based AViD-SP a slightly larger, but still quite minor, degradation in metric performance is observed when audio inputs are excluded, with only ASR transcriptions being provided. However, a more significant degradation is observed for the GMADS variant of AViD-SP under these same conditions. This implies that the GMADS multimodal adaptation process has resulted in a model which is more sensitive to the raw audio inputs than when meanpooling is used, which seems to dominantly rely on the natively textual ASR transcriptions. We theorize that the enhanced capability of GMADS to process multimodal inputs may lead to its overall worse results, as it produces a more complex optimization landscape compared with simply collapsing to utilize only the native textual ASR transcripts. Additionally, the absence of prior context markedly increases error rates, underscoring the importance of historical context for accurate SPICE updates.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 â€£ 5.3 Baselines and Evaluation â€£ 5 AViD-SP Model â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the performance of AViD-SP on the VG-SPICE-C test set. For TTS audio, the metrics diverge significantly from those of the standard VG-SPICE test set, featuring higher S-RED and lower H-RED scores. The higher density of VG-SPICE-Câ€™s scene graphs, which include fewer visually or auditorily supported features that are untracked in reference scene graphs, likely contributes to these lower Hard metric scores. However, this increased density also presents a greater challenge in achieving improved Soft metric scores, as the model must correctly incorporate a substantial amount of information at each update step.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">For the GMADS-based AViD-SP, performance metrics on the read audio portion of VG-SPICE-C align closely with those observed in the TTS portion. Conversely, the meanpooling variant shows a substantial performance reduction. This discrepancy suggests that GMADS possesses more robust multimodal processing capabilities, especially in processing out-of-domain real audio distributions. Since both model variants use the same ASR model without parameter tuning, the observed differences indicate that GMADS compensates more effectively for poorer ASR performance.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we introduced Semantic Parsing in Contextual Environments (SPICE), an innovative task designed to enhance artificial agentsâ€™ contextual understanding by integrating multimodal inputs with prior contexts. Through the development of the VG-SPICE dataset and the Audio-Vision Dialogue Scene Parser (AViD-SP) model, we established a framework for agents to dynamically update their knowledge in response to new information, closely mirroring human communication processes. The VG-SPICE dataset, crafted to challenge agents with the task of visual scene graph construction from spoken conversational exchanges, represents a significant step forward in the field of semantic parsing by incorporating both speech and visual data integration. Meanwhile, the AViD-SP model, equipped with the novel Grouped Multimodal Attention Down Sampler (GMADS), provides a strong initial baseline for VG-SPICE as well as insights into potential methods to improve multimodal information processing and integration.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Our work highlights the importance of developing systems capable of understanding and interacting within complex, multimodal environments. By focusing on the continuous update of contextual states based on new, and multimodal, information, SPICE represents a shift towards more natural and effective human-AI communication.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">While VG-SPICE and AViD-SP are novel approaches, they have several limitations and should be treated as initial attempts toward further SPICE implementations and benchmarks. The main limitation stems from the extensive use of synthetic data augmentation in VG-SPICEâ€™s creation. The process involved several steps, including dataset preprocessing with BERT-like POS taggers, crafting update utterances using the Llama 2 70B LLM, and generating synthetic TTS audio. These stages may introduce errors, hallucinations, or overly simple data distributions, potentially misaligning with real-world applications. For example, our modelsâ€™ resilience to background noise may reflect the specific TTS audio distribution, possibly simplifying the ASR modelâ€™s speech discernment. Additionally, the Visual Genome, our workâ€™s foundation, suffers from notable quality issues, such as poor annotations and unreliable synthetic object segmentation, which, despite efforts to mitigate, remain challenges in VG-SPICE. While the included VG-SPICE-C test subset attempts to improve these limitations, and indeed the hard versions of are metrics are significantly improved on the manually cleaned samples of this subset, they are still comprised of intentionally crafted utterances with read audio, which may not transfer to real-world applications and natural spoken audio. Further, this work only includes analysis of the VG-SPICE-C challenge subset in the simple Single Step task and does not evaluate in end-to-end sequence-based analysis.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">The various version of AViD-SP we introduce also provides indications of further development for efficient multimodal adaptation methodologies. While the version utilizing GMADS generally failed to outperform the results of the traditional meanpooling version the GMADS method also provided a stronger indication of cross-modality feature utilization, whereas integration of simplistically downsampled multimodal features alongside native textual features appears to cause strong underutilization and feature collapse for the multimodal features. This is further supported by the poor performance achieved by the meanpooling version of AViD-SP, relative to the GMADS version, on real human recorded audio, indicating the meanpooling version adapts much worse to out-of-domain multimodal inputs. We suggest future work to continue investigating methods similar to GMADS to further realize their theoretical benefits.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Moreover, VG-SPICE, while pioneering in SPICE tasks, is only a start, limited to audio and images, with a basic language for knowledge graph updates. Future research should address these limitations by incorporating more realistic inputs, like video, 3D environments, and paralinguistic cues, and by exploring dynamic tasks beyond simple scene graph updates. Environments like Matterport3D <cite class="ltx_cite ltx_citemacro_citep">(Chang etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite> or Habitat 3.0 <cite class="ltx_cite ltx_citemacro_citep">(Puig etÂ al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> offer promising avenues for embodied SPICE research. Expanding SPICE to include secondary tasks that rely on an agentâ€™s contextual understanding can also enhance its utility, such as aiding in medical image annotation with co-dialogue.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelsalam etÂ al. (2022)</span>
<span class="ltx_bibblock">
MohamedÂ Ashraf Abdelsalam, Zhan Shi, Federico Fancellu, Kalliopi Basioti, Dhaivat Bhatt, Vladimir Pavlovic, and Afsaneh Fazly. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.conll-1.19" title="" class="ltx_ref ltx_href">Visual semantic parsing: From images to Abstract Meaning Representation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)</em>, pages 282â€“300, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora etÂ al. (2023)</span>
<span class="ltx_bibblock">
Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, and Shinji Watanabe. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:259991137" title="" class="ltx_ref ltx_href">Integrating pretrained asr and lm to perform sequence generation for spoken language understanding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2307.11005.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balaraman etÂ al. (2021)</span>
<span class="ltx_bibblock">
Vevake Balaraman, Seyedmostafa Sheikhalishahi, and Bernardo Magnini. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.sigdial-1.25" title="" class="ltx_ref ltx_href">Recent neural methods on dialogue state tracking for task-oriented dialogue systems: A survey</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</em>, pages 239â€“251, Singapore and Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barker etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jon Barker, Shinji Watanabe, Emmanuel Vincent, and Jan Trmal. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1803.10609" title="" class="ltx_ref ltx_href">The fifth â€™chimeâ€™ speech separation and recognition challenge: Dataset, task and baselines</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Betker (2022)</span>
<span class="ltx_bibblock">
James Betker. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/neonbjb/tortoise-tts" title="" class="ltx_ref ltx_href">TorToiSe text-to-speech</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017.

</span>
<span class="ltx_bibblock">Matterport3d: Learning from rgb-d data in indoor environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Conference on 3D Vision (3DV)</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Xiaojun Chen, Shengbin Jia, and Yang Xiang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.eswa.2019.112948" title="" class="ltx_ref ltx_href">A review: Knowledge reasoning over knowledge graph</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, 141:112948.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jianpeng Cheng, Devang Agrawal, HÃ©ctor MartÃ­nezÂ Alonso, Shruti Bhargava, Joris Driesen, Federico Flego, Dain Kaplan, Dimitri Kartsaklis, Lin Li, Dhivya Piraviperumal, JasonÂ D. Williams, Hong Yu, Diarmuid Ã“Â SÃ©aghdha, and Anders Johannsen. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.651" title="" class="ltx_ref ltx_href">Conversational semantic parsing for dialog state tracking</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 8107â€“8117, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.78" title="" class="ltx_ref ltx_href">HiTab: A hierarchical table dataset for question answering and natural language generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1094â€“1110, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, YiÂ Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, AndrewÂ M. Dai, ThanumalayanÂ Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2204.02311" title="" class="ltx_ref ltx_href">Palm: Scaling language modeling with pathways</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coucke etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alice Coucke, Alaa Saade, Adrien Ball, ThÃ©odore Bluche, Alexandre Caulier, David Leroy, ClÃ©ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, MaÃ«l Primet, and Joseph Dureau. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1805.10190" title="" class="ltx_ref ltx_href">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony MengÂ Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.06500" title="" class="ltx_ref ltx_href">Instructblip: Towards general-purpose vision-language models with instruction tuning</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2023)</span>
<span class="ltx_bibblock">
Tri Dao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.08691" title="" class="ltx_ref ltx_href">Flashattention-2: Faster attention with better parallelism and work partitioning</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuan Gong, Hongyin Luo, AlexanderÂ H. Liu, Leonid Karlinsky, and James Glass. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.10790" title="" class="ltx_ref ltx_href">Listen, think, and understand</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heck etÂ al. (2020)</span>
<span class="ltx_bibblock">
Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.sigdial-1.4" title="" class="ltx_ref ltx_href">TripPy: A triple copy strategy for value independent neural dialog state tracking</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</em>, pages 35â€“44, 1st virtual meeting. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
AlbertÂ Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lioÂ Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, TevenÂ Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and WilliamÂ El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.06825" title="" class="ltx_ref ltx_href">Mistral 7b</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath and Das (2019)</span>
<span class="ltx_bibblock">
Aishwarya Kamath and Rajarshi Das. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1812.00978" title="" class="ltx_ref ltx_href">A survey on semantic parsing</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kottur etÂ al. (2021)</span>
<span class="ltx_bibblock">
Satwik Kottur, Seungwhan Moon, Alborz Geramifard, and Babak Damavandi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.401" title="" class="ltx_ref ltx_href">SIMMC 2.0: A task-oriented dialog dataset for immersive multimodal conversations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 4903â€“4912, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2016)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A. Shamma, MichaelÂ S. Bernstein, and Fei-Fei Li. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1602.07332" title="" class="ltx_ref ltx_href">Visual genome: Connecting language and vision using crowdsourced dense image annotations</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhuang Li, Lizhen Qu, and Gholamreza Haffari. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.coling-main.226" title="" class="ltx_ref ltx_href">Context dependent semantic parsing: A survey</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 2509â€“2521, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, LiÂ Zhu, and Tao Mei. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.00313" title="" class="ltx_ref ltx_href">Vrr-vg: Refocusing visually-relevant relationships</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and YuÂ Qiao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.07575" title="" class="ltx_ref ltx_href">Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.08485" title="" class="ltx_ref ltx_href">Visual instruction tuning</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugosch etÂ al. (2019)</span>
<span class="ltx_bibblock">
Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, VikrantÂ Singh Tomar, and Yoshua Bengio. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1904.03670" title="" class="ltx_ref ltx_href">Speech model pre-training for end-to-end spoken language understanding</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz etÂ al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and FahadÂ Shahbaz Khan. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.05424" title="" class="ltx_ref ltx_href">Video-chatgpt: Towards detailed video understanding via large vision and language models</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MaÃ«lic etÂ al. (2023)</span>
<span class="ltx_bibblock">
Neau MaÃ«lic, PauloÂ E. Santos, Anne-Gwenn Bosser, and CÃ©dric Buche. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.18668" title="" class="ltx_ref ltx_href">Fine-grained is too coarse: A novel data-centric approach for efficient scene graph generation</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moradshahi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Mehrad Moradshahi, Victoria Tsai, Giovanni Campagna, and Monica Lam. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.eacl-main.63" title="" class="ltx_ref ltx_href">Contextual semantic parsing for multilingual task-oriented dialogues</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 902â€“915, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab etÂ al. (2024)</span>
<span class="ltx_bibblock">
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, HuÂ Xu, HervÃ© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.07193" title="" class="ltx_ref ltx_href">Dinov2: Learning robust visual features without supervision</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, XuÂ Jiang, Diogo Almeida, CarrollÂ L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2203.02155" title="" class="ltx_ref ltx_href">Training language models to follow instructions with human feedback</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puig etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xavier Puig, Eric Undersander, Andrew Szot, MikaelÂ Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, AlexanderÂ William Clegg, Michal Hlavac, SoÂ Yeon Min, VladimÃ­r VondruÅ¡, Theophile Gervet, Vincent-Pierre Berges, JohnÂ M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, DevendraÂ Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.13724" title="" class="ltx_ref ltx_href">Habitat 3.0: A co-habitat for humans, avatars and robots</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan, and FahadÂ Shahbaz Khan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2103.14021" title="" class="ltx_ref ltx_href">Orthogonal projection loss</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubenstein etÂ al. (2023)</span>
<span class="ltx_bibblock">
PaulÂ K. Rubenstein, Chulayuth Asawaroengchai, DucÂ Dung Nguyen, Ankur Bapna, ZalÃ¡n Borsos, FÃ©lix deÂ ChaumontÂ Quitry, Peter Chen, DaliaÂ El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, MichelleÂ Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo VelimiroviÄ‡, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, YuÂ Zhang, Zhishuai Zhang, Lukas Zilka, and Christian Frank. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.12925" title="" class="ltx_ref ltx_href">Audiopalm: A large language model that can speak and listen</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sen and Groves (2021)</span>
<span class="ltx_bibblock">
Priyanka Sen and Isabel Groves. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.eacl-main.150" title="" class="ltx_ref ltx_href">Semantic parsing of disfluent speech</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 1748â€“1753, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomasello etÂ al. (2022)</span>
<span class="ltx_bibblock">
Paden Tomasello, Akshat Shrivastava, Daniel Lazar, Po-Chun Hsu, Duc Le, Adithya Sagar, Ali Elkahky, Jade Copet, Wei-Ning Hsu, Yossi Adi, Robin Algayres, TuÂ Ahn Nguyen, Emmanuel Dupoux, Luke Zettlemoyer, and Abdelrahman Mohamed. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2207.10643" title="" class="ltx_ref ltx_href">Stop: A dataset for spoken task oriented semantic parsing</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and LiÂ Guo. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.15022" title="" class="ltx_ref ltx_href">Recursively summarizing enables long-term dialogue memory in large language models</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, AdamsÂ Wei Yu, Brian Lester, Nan Du, AndrewÂ M. Dai, and QuocÂ V Le. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=gEZrGCozdqR" title="" class="ltx_ref ltx_href">Finetuned language models are zero-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sixing Wu, Ying Li, Minghui Wang, Dawei Zhang, Yang Zhou, and Zhonghai Wu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.175" title="" class="ltx_ref ltx_href">More is better: Enhancing open-domain dialogue generation via multi-source heterogeneous knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 2286â€“2300, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2017)</span>
<span class="ltx_bibblock">
Danfei Xu, Yuke Zhu, ChristopherÂ B. Choy, and LiÂ Fei-Fei. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1701.02426" title="" class="ltx_ref ltx_href">Scene graph generation by iterative message passing</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2021)</span>
<span class="ltx_bibblock">
Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li, and Emine Yilmaz. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2101.09374" title="" class="ltx_ref ltx_href">Slot self-attentive dialogue state tracking</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, BoÂ Pang, XiÂ Victoria Lin, YiÂ Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1204" title="" class="ltx_ref ltx_href">CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1962â€“1979, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zareian etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alireza Zareian, Svebor Karaman, and Shih-Fu Chang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2001.02359" title="" class="ltx_ref ltx_href">Weakly supervised visual semantic parsing</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.02858" title="" class="ltx_ref ltx_href">Video-llama: An instruction-tuned audio-visual language model for video understanding</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2019)</span>
<span class="ltx_bibblock">
JiÂ Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, and Mohamed Elhoseiny. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1804.10660" title="" class="ltx_ref ltx_href">Large-scale visual relationship understanding</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
YongÂ Hong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, and ChangÂ Wen Chen. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:260976241" title="" class="ltx_ref ltx_href">Learning to generate language-supervised and open-vocabulary scene graph using pre-trained visual-semantic space</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 2915â€“2924.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wanjun Zhong, Lianghong Guo, Qiqi Gao, HeÂ Ye, and Yanlin Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.10250" title="" class="ltx_ref ltx_href">Memorybank: Enhancing large language models with long-term memory</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.10592" title="" class="ltx_ref ltx_href">Minigpt-4: Enhancing vision-language understanding with advanced large language models</a>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Acknowledgements</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">This material is based upon work supported by the National Science Foundation under Grant No. 2238605</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional AViD-SP Results</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We report the Graph Edit Distance (GED) results for AViD-SP, and the tested baselines, here.</p>
</div>
<figure id="A2.T5" class="ltx_table">
<table id="A2.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T5.2.2" class="ltx_tr">
<th id="A2.T5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2">Model Type</th>
<th id="A2.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="4">H-GED<math id="A2.T5.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A2.T5.1.1.1.m1.1a"><mo stretchy="false" id="A2.T5.1.1.1.m1.1.1" xref="A2.T5.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T5.1.1.1.m1.1b"><ci id="A2.T5.1.1.1.m1.1.1.cmml" xref="A2.T5.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="A2.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t" colspan="4">S-GED<math id="A2.T5.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="A2.T5.2.2.2.m1.1a"><mo stretchy="false" id="A2.T5.2.2.2.m1.1.1" xref="A2.T5.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T5.2.2.2.m1.1b"><ci id="A2.T5.2.2.2.m1.1.1.cmml" xref="A2.T5.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
<tr id="A2.T5.2.3.1" class="ltx_tr">
<th id="A2.T5.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="A2.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">0dB</th>
<th id="A2.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">2dB</th>
<th id="A2.T5.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">20dB</th>
<th id="A2.T5.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Gold*</th>
<th id="A2.T5.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">0dB</th>
<th id="A2.T5.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">2dB</th>
<th id="A2.T5.2.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">20dB</th>
<th id="A2.T5.2.3.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Gold*</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T5.2.4.1" class="ltx_tr">
<th id="A2.T5.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AViD-SP + GMADS</th>
<th id="A2.T5.2.4.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">Base</th>
<td id="A2.T5.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">2.010</td>
<td id="A2.T5.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">1.921</td>
<td id="A2.T5.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">1.811</td>
<td id="A2.T5.2.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.621</td>
<td id="A2.T5.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">0.924</td>
<td id="A2.T5.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">0.889</td>
<td id="A2.T5.2.4.1.9" class="ltx_td ltx_align_center ltx_border_t">0.862</td>
<td id="A2.T5.2.4.1.10" class="ltx_td ltx_align_center ltx_border_t">0.778</td>
</tr>
<tr id="A2.T5.2.5.2" class="ltx_tr">
<th id="A2.T5.2.5.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.5.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Image</th>
<td id="A2.T5.2.5.2.3" class="ltx_td ltx_align_center">2.044</td>
<td id="A2.T5.2.5.2.4" class="ltx_td ltx_align_center">1.973</td>
<td id="A2.T5.2.5.2.5" class="ltx_td ltx_align_center">1.816</td>
<td id="A2.T5.2.5.2.6" class="ltx_td ltx_align_center ltx_border_r">1.642</td>
<td id="A2.T5.2.5.2.7" class="ltx_td ltx_align_center">0.944</td>
<td id="A2.T5.2.5.2.8" class="ltx_td ltx_align_center">0.923</td>
<td id="A2.T5.2.5.2.9" class="ltx_td ltx_align_center">0.878</td>
<td id="A2.T5.2.5.2.10" class="ltx_td ltx_align_center">0.791</td>
</tr>
<tr id="A2.T5.2.6.3" class="ltx_tr">
<th id="A2.T5.2.6.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.6.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Audio</th>
<td id="A2.T5.2.6.3.3" class="ltx_td ltx_align_center">2.168</td>
<td id="A2.T5.2.6.3.4" class="ltx_td ltx_align_center">2.101</td>
<td id="A2.T5.2.6.3.5" class="ltx_td ltx_align_center">2.071</td>
<td id="A2.T5.2.6.3.6" class="ltx_td ltx_align_center ltx_border_r">1.863</td>
<td id="A2.T5.2.6.3.7" class="ltx_td ltx_align_center">1.209</td>
<td id="A2.T5.2.6.3.8" class="ltx_td ltx_align_center">1.186</td>
<td id="A2.T5.2.6.3.9" class="ltx_td ltx_align_center">1.158</td>
<td id="A2.T5.2.6.3.10" class="ltx_td ltx_align_center">1.004</td>
</tr>
<tr id="A2.T5.2.7.4" class="ltx_tr">
<th id="A2.T5.2.7.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.7.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w Incorrect Image**</th>
<td id="A2.T5.2.7.4.3" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.7.4.4" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.7.4.5" class="ltx_td ltx_align_center">1.806</td>
<td id="A2.T5.2.7.4.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="A2.T5.2.7.4.7" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.7.4.8" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.7.4.9" class="ltx_td ltx_align_center">0.861</td>
<td id="A2.T5.2.7.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A2.T5.2.8.5" class="ltx_tr">
<th id="A2.T5.2.8.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.8.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Prior Context***</th>
<td id="A2.T5.2.8.5.3" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.8.5.4" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.8.5.5" class="ltx_td ltx_align_center">4.656</td>
<td id="A2.T5.2.8.5.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="A2.T5.2.8.5.7" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.8.5.8" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.8.5.9" class="ltx_td ltx_align_center">0.909</td>
<td id="A2.T5.2.8.5.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A2.T5.2.9.6" class="ltx_tr">
<th id="A2.T5.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AViD-SP + Meanpool</th>
<th id="A2.T5.2.9.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">Base</th>
<td id="A2.T5.2.9.6.3" class="ltx_td ltx_align_center ltx_border_t">1.739</td>
<td id="A2.T5.2.9.6.4" class="ltx_td ltx_align_center ltx_border_t">1.617</td>
<td id="A2.T5.2.9.6.5" class="ltx_td ltx_align_center ltx_border_t">1.514</td>
<td id="A2.T5.2.9.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.295</td>
<td id="A2.T5.2.9.6.7" class="ltx_td ltx_align_center ltx_border_t">0.935</td>
<td id="A2.T5.2.9.6.8" class="ltx_td ltx_align_center ltx_border_t">0.889</td>
<td id="A2.T5.2.9.6.9" class="ltx_td ltx_align_center ltx_border_t">0.859</td>
<td id="A2.T5.2.9.6.10" class="ltx_td ltx_align_center ltx_border_t">0.759</td>
</tr>
<tr id="A2.T5.2.10.7" class="ltx_tr">
<th id="A2.T5.2.10.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.10.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Image</th>
<td id="A2.T5.2.10.7.3" class="ltx_td ltx_align_center">1.732</td>
<td id="A2.T5.2.10.7.4" class="ltx_td ltx_align_center">1.599</td>
<td id="A2.T5.2.10.7.5" class="ltx_td ltx_align_center">1.514</td>
<td id="A2.T5.2.10.7.6" class="ltx_td ltx_align_center ltx_border_r">1.285</td>
<td id="A2.T5.2.10.7.7" class="ltx_td ltx_align_center">0.939</td>
<td id="A2.T5.2.10.7.8" class="ltx_td ltx_align_center">0.910</td>
<td id="A2.T5.2.10.7.9" class="ltx_td ltx_align_center">0.872</td>
<td id="A2.T5.2.10.7.10" class="ltx_td ltx_align_center">0.759</td>
</tr>
<tr id="A2.T5.2.11.8" class="ltx_tr">
<th id="A2.T5.2.11.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.11.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w/o Audio</th>
<td id="A2.T5.2.11.8.3" class="ltx_td ltx_align_center">1.622</td>
<td id="A2.T5.2.11.8.4" class="ltx_td ltx_align_center">1.560</td>
<td id="A2.T5.2.11.8.5" class="ltx_td ltx_align_center">1.428</td>
<td id="A2.T5.2.11.8.6" class="ltx_td ltx_align_center ltx_border_r">1.244</td>
<td id="A2.T5.2.11.8.7" class="ltx_td ltx_align_center">1.002</td>
<td id="A2.T5.2.11.8.8" class="ltx_td ltx_align_center">0.964</td>
<td id="A2.T5.2.11.8.9" class="ltx_td ltx_align_center">0.909</td>
<td id="A2.T5.2.11.8.10" class="ltx_td ltx_align_center">0.815</td>
</tr>
<tr id="A2.T5.2.12.9" class="ltx_tr">
<th id="A2.T5.2.12.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.T5.2.12.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">w Incorrect Image**</th>
<td id="A2.T5.2.12.9.3" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.12.9.4" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.12.9.5" class="ltx_td ltx_align_center">1.517</td>
<td id="A2.T5.2.12.9.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="A2.T5.2.12.9.7" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.12.9.8" class="ltx_td ltx_align_center">-</td>
<td id="A2.T5.2.12.9.9" class="ltx_td ltx_align_center">0.857</td>
<td id="A2.T5.2.12.9.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="A2.T5.2.13.10" class="ltx_tr">
<th id="A2.T5.2.13.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<th id="A2.T5.2.13.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r">w/o Prior Context***</th>
<td id="A2.T5.2.13.10.3" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="A2.T5.2.13.10.4" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="A2.T5.2.13.10.5" class="ltx_td ltx_align_center ltx_border_b">4.778</td>
<td id="A2.T5.2.13.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td id="A2.T5.2.13.10.7" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="A2.T5.2.13.10.8" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="A2.T5.2.13.10.9" class="ltx_td ltx_align_center ltx_border_b">0.905</td>
<td id="A2.T5.2.13.10.10" class="ltx_td ltx_align_center ltx_border_b">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
GED results on the VG-SPICE test set for our AViD-SP model. AViD-SP was trained with CHiME5 noise augmentation sampled between 0db and 20dB SNR (all CHiME5 noise followed the provided train/eval/test splits). *Given the ground truth utterance transcripts in place of the ASR transcriptions. **Evaluated by offsetting visual features within batch so incorrect image features are paired with the other input components. ***Evaluated with "Empty Context" prior state scene graphs summaries instead of the correct ones.
</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Qualitative AViD-SP Examples</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We include an example of a typical AViD-SP generation in Figure <a href="#A3.F3" title="Figure 3 â€£ Appendix C Qualitative AViD-SP Examples â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with metric scores approximately at the average obtained across the full testing set. In this example it is evident that all of the ground truth reference information was successfully added to the updated scene graph, leading to the Soft-RED score of 0.0. However, considerable extraneous information is also observed to have been added. In Figure <a href="#A3.F3" title="Figure 3 â€£ Appendix C Qualitative AViD-SP Examples â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> three additional Nodes are added, with two of them being duplicates of ones that already exist in the scene graph, along with one Edge.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">However, considering the Transcription and Visual Scene for the illustrated sample reveals that these features, while not included in the reference, likely are logically reasonable for the agent to include. For the additional Node of â€œrunwayâ€ the motivation is obvious. Not only is the runway and its corresponding edge relationship mentioned by the LLM, but a runway is even present in the scene visual. Similar conditions apply to the two duplicate nodes added. While those nodes already exist, they are mentioned in the Audio Transcription at two distinct times. Inspection of the highlighted and blown-up parts of the image also reveals that there are in fact duplicates of these entities in the scene, making their addition to the updated context reasonable.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">This is not to say all extraneous additions should be treated as correct since many should not. However, it does illustrate a key area to seek further improvement in the VG-SPICE dataset and why, for this work, we focus more on the â€œsoftâ€ capability to add all known good information tot he graph.</p>
</div>
<figure id="A3.F3" class="ltx_figure"><img src="/html/2406.06438/assets/figures/AViD_output.jpg" id="A3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sample generation output with corresponding inputs from AViD-SP. Scored a Soft-RED of 0.0 and Hard-RED of 6.727. Significant features highlighted in colors. Qualitative evaluation reveals that the majority of extraneous additions were either supported by the Audio Transcription, the scene image, or both. </figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Visual Genome Preprocessing</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">The Visual Genome serves as a strong basis for VG-SPICE but has quality issues such as inconsistent naming for Nodes, Attributes, and Predicates, duplicate Nodes, and unnecessary Nodes (e.g., <span id="A4.p1.1.1" class="ltx_text ltx_font_bold">&lt;man, has, head&gt;</span>). Prior solutions for Scene Graph Generation (SGG) tasks <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Zhang etÂ al., <a href="#bib.bib46" title="" class="ltx_ref">2019</a>; Xu etÂ al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>; MaÃ«lic etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> curated versions by limiting predicates and node names, reducing predicates from 27k to 50 and node names from 53k to 150. While the Visual Genome contains a substantial portion of single-sample terms, typically of lower quality, such restrictions can oversimplify and yield smaller, less representative scene graphs.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">Our approach refines the Visual Genome by:</p>
</div>
<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Standardization and Correction:</h4>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p1.1" class="ltx_p">We applied rule-based systems with Sentence Transformer Part of Speech taggers <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Using â€all-mpnet-base-v2â€ from Python Sentence Transformers</span></span></span> to fix inconsistencies and improve scene graph density by retaining rare Node names (e.g., "red table", identifying "red" as an attribute). We removed low-quality attributes and predicates by limiting them to specific parts of speech conditions, such as removing proper and common nouns from attributes/edges. Furthermore, we imposed several straightforward constraints to refine the scene graph structure. These included setting limits on the word counts for individual scene graph elements and consolidating attributes when redundancy was detected within a specific node, for instance, merging "reddish" and "red" when both attributes described the same entity.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Duplicate Node Elimination:</h4>

<div id="A4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px2.p1.1" class="ltx_p">We added a post-standardization phase to remove duplicate nodes. Unlike earlier methods <cite class="ltx_cite ltx_citemacro_citep">(MaÃ«lic etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> relying solely on a high Intersection over Union (IoU) threshold for exact node matches, we included a semantic similarity check from the contextualized embeddings from the same Sentence Transformer utilized in the Standardization and Correction phase. This allows for the detection of duplicate Nodes with significant name similarities and IoUs. With a preference for visually supported scene graphs over the potential exclusion of some valid Nodes, we set a lower IoU threshold (0.5, compared to prior worksâ€™ 0.9) and a semantic similarity threshold of 0.7.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Term Frequency Analysis:</h4>

<div id="A4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px3.p1.1" class="ltx_p">Next, we manually curated terms in the filtered dataset to establish a relevant set for the SPICE task, excluding single-occurrence terms for their low quality, and filtered scene graphs based on this list.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scene Graph Size Restriction:</h4>

<div id="A4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px4.p1.1" class="ltx_p">Finally, we filtered out small graphs to ensure a diverse set for VG-SPICE, excluding graphs with fewer than four Nodes or Edges and applying dynamically increased threshold for graphs with duplicate nodes.</p>
</div>
<div id="A4.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="A4.SS0.SSS0.Px4.p2.1" class="ltx_p">These methods enhanced the Visual Genomeâ€™s graphs, yielding a dataset with improved quality and annotation density, as illustrated in Table <a href="#S2.T1" title="Table 1 â€£ 2.2 Semantic Parsing â€£ 2 Related Work â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training and Inference Hyperparameters</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.4" class="ltx_p">The training regimen for AViD-SP spans two epochs across the dataset, using a combined batch size of 72 on six Nvidia L40 GPUs. An initial learning rate of <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mn id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml"><mn id="A5.p1.1.m1.1.1.3.2" xref="A5.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A5.p1.1.m1.1.1.3.3" xref="A5.p1.1.m1.1.1.3.3.cmml"><mo id="A5.p1.1.m1.1.1.3.3a" xref="A5.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="A5.p1.1.m1.1.1.3.3.2" xref="A5.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><times id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></times><cn type="integer" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">5</cn><apply id="A5.p1.1.m1.1.1.3.cmml" xref="A5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A5.p1.1.m1.1.1.3.1.cmml" xref="A5.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="A5.p1.1.m1.1.1.3.2.cmml" xref="A5.p1.1.m1.1.1.3.2">10</cn><apply id="A5.p1.1.m1.1.1.3.3.cmml" xref="A5.p1.1.m1.1.1.3.3"><minus id="A5.p1.1.m1.1.1.3.3.1.cmml" xref="A5.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="A5.p1.1.m1.1.1.3.3.2.cmml" xref="A5.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">5\times 10^{-5}</annotation></semantics></math> is applied, followed by exponential decay. We employ cross-entropy loss for the prediction of target semantic parses, introducing loss masking for padding and for the prompt that combines prior context with multimodal inputs. We utilize loss factors of <math id="A5.p1.2.m2.1" class="ltx_Math" alttext="\alpha=1.0" display="inline"><semantics id="A5.p1.2.m2.1a"><mrow id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><mi id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml">Î±</mi><mo id="A5.p1.2.m2.1.1.1" xref="A5.p1.2.m2.1.1.1.cmml">=</mo><mn id="A5.p1.2.m2.1.1.3" xref="A5.p1.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><eq id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1.1"></eq><ci id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2">ğ›¼</ci><cn type="float" id="A5.p1.2.m2.1.1.3.cmml" xref="A5.p1.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">\alpha=1.0</annotation></semantics></math>, <math id="A5.p1.3.m3.1" class="ltx_Math" alttext="\beta=0.1" display="inline"><semantics id="A5.p1.3.m3.1a"><mrow id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml"><mi id="A5.p1.3.m3.1.1.2" xref="A5.p1.3.m3.1.1.2.cmml">Î²</mi><mo id="A5.p1.3.m3.1.1.1" xref="A5.p1.3.m3.1.1.1.cmml">=</mo><mn id="A5.p1.3.m3.1.1.3" xref="A5.p1.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><apply id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1"><eq id="A5.p1.3.m3.1.1.1.cmml" xref="A5.p1.3.m3.1.1.1"></eq><ci id="A5.p1.3.m3.1.1.2.cmml" xref="A5.p1.3.m3.1.1.2">ğ›½</ci><cn type="float" id="A5.p1.3.m3.1.1.3.cmml" xref="A5.p1.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">\beta=0.1</annotation></semantics></math>, and <math id="A5.p1.4.m4.1" class="ltx_Math" alttext="\gamma=0.1" display="inline"><semantics id="A5.p1.4.m4.1a"><mrow id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml"><mi id="A5.p1.4.m4.1.1.2" xref="A5.p1.4.m4.1.1.2.cmml">Î³</mi><mo id="A5.p1.4.m4.1.1.1" xref="A5.p1.4.m4.1.1.1.cmml">=</mo><mn id="A5.p1.4.m4.1.1.3" xref="A5.p1.4.m4.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><apply id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1"><eq id="A5.p1.4.m4.1.1.1.cmml" xref="A5.p1.4.m4.1.1.1"></eq><ci id="A5.p1.4.m4.1.1.2.cmml" xref="A5.p1.4.m4.1.1.2">ğ›¾</ci><cn type="float" id="A5.p1.4.m4.1.1.3.cmml" xref="A5.p1.4.m4.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">\gamma=0.1</annotation></semantics></math>.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">Inference leverages a greedy decoding strategy with a max generation length of 160 tokens and otherwise default generation parameters for LLAMA 2 7B.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Contextual State Representation</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">SPICE formulates the prior context to be utilized by the agent as a structured knowledge graph. However, top-performing semantic parsing generation models, such as those best on the Llama architecture as used in this work, are decoder-only models that can accept inputs from linear text sequences only. This requires utilizing either a compatible knowledge graph encoder which can embed and project the knowledge graph representation for use by the semantic parse generation model, or representing the knowledge graph in the form of a textually formatted prompt. For AViD-SP developed in this work, we utilized the second, with the format of the textually prompted representation of the prior context shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="A6.p2" class="ltx_para">
<p id="A6.p2.1" class="ltx_p">When generating the context representations all existing Nodes are assigned Node IDs, and semantic parses are expected to operate in reference to these Node IDs (Section <a href="#A7" title="Appendix G Formal Language Definition â€£ Multimodal Contextualized Semantic Parsing from Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>). We provide Nodes and Attributes first, followed by any Edges. The ordering of all information is sorted by Node ID in ascending order. In practice, all Node IDs are randomly assigned for each training iteration to diversity training inputs.</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Formal Language Definition</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.5" class="ltx_p">The formal language we used in the semantic parses <math id="A7.p1.1.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="A7.p1.1.m1.1a"><msub id="A7.p1.1.m1.1.1" xref="A7.p1.1.m1.1.1.cmml"><mi id="A7.p1.1.m1.1.1.2" xref="A7.p1.1.m1.1.1.2.cmml">P</mi><mi id="A7.p1.1.m1.1.1.3" xref="A7.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A7.p1.1.m1.1b"><apply id="A7.p1.1.m1.1.1.cmml" xref="A7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.p1.1.m1.1.1.1.cmml" xref="A7.p1.1.m1.1.1">subscript</csymbol><ci id="A7.p1.1.m1.1.1.2.cmml" xref="A7.p1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="A7.p1.1.m1.1.1.3.cmml" xref="A7.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.1.m1.1c">P_{i}</annotation></semantics></math> and the corresponding execution function <math id="A7.p1.2.m2.1" class="ltx_Math" alttext="e" display="inline"><semantics id="A7.p1.2.m2.1a"><mi id="A7.p1.2.m2.1.1" xref="A7.p1.2.m2.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A7.p1.2.m2.1b"><ci id="A7.p1.2.m2.1.1.cmml" xref="A7.p1.2.m2.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.2.m2.1c">e</annotation></semantics></math> contained the following executable function, which together could deterministically update the scene graph prior context <math id="A7.p1.3.m3.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="A7.p1.3.m3.1a"><msub id="A7.p1.3.m3.1.1" xref="A7.p1.3.m3.1.1.cmml"><mi id="A7.p1.3.m3.1.1.2" xref="A7.p1.3.m3.1.1.2.cmml">C</mi><mi id="A7.p1.3.m3.1.1.3" xref="A7.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A7.p1.3.m3.1b"><apply id="A7.p1.3.m3.1.1.cmml" xref="A7.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A7.p1.3.m3.1.1.1.cmml" xref="A7.p1.3.m3.1.1">subscript</csymbol><ci id="A7.p1.3.m3.1.1.2.cmml" xref="A7.p1.3.m3.1.1.2">ğ¶</ci><ci id="A7.p1.3.m3.1.1.3.cmml" xref="A7.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.3.m3.1c">C_{i}</annotation></semantics></math> to the next context state <math id="A7.p1.4.m4.1" class="ltx_Math" alttext="C_{i+1}" display="inline"><semantics id="A7.p1.4.m4.1a"><msub id="A7.p1.4.m4.1.1" xref="A7.p1.4.m4.1.1.cmml"><mi id="A7.p1.4.m4.1.1.2" xref="A7.p1.4.m4.1.1.2.cmml">C</mi><mrow id="A7.p1.4.m4.1.1.3" xref="A7.p1.4.m4.1.1.3.cmml"><mi id="A7.p1.4.m4.1.1.3.2" xref="A7.p1.4.m4.1.1.3.2.cmml">i</mi><mo id="A7.p1.4.m4.1.1.3.1" xref="A7.p1.4.m4.1.1.3.1.cmml">+</mo><mn id="A7.p1.4.m4.1.1.3.3" xref="A7.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A7.p1.4.m4.1b"><apply id="A7.p1.4.m4.1.1.cmml" xref="A7.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A7.p1.4.m4.1.1.1.cmml" xref="A7.p1.4.m4.1.1">subscript</csymbol><ci id="A7.p1.4.m4.1.1.2.cmml" xref="A7.p1.4.m4.1.1.2">ğ¶</ci><apply id="A7.p1.4.m4.1.1.3.cmml" xref="A7.p1.4.m4.1.1.3"><plus id="A7.p1.4.m4.1.1.3.1.cmml" xref="A7.p1.4.m4.1.1.3.1"></plus><ci id="A7.p1.4.m4.1.1.3.2.cmml" xref="A7.p1.4.m4.1.1.3.2">ğ‘–</ci><cn type="integer" id="A7.p1.4.m4.1.1.3.3.cmml" xref="A7.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.4.m4.1c">C_{i+1}</annotation></semantics></math>. Since VG-SPICE only represents the conversational construction of scene graphs, and not deletion or alterations, our formal language is comprised of three distinct operations: 1) <span id="A7.p1.5.1" class="ltx_text ltx_font_italic">#ADD_NODE</span> accepting a new Node ID, name, and optionally a set of attributes to add along with it, 2) <span id="A7.p1.5.2" class="ltx_text ltx_font_italic">#ADD_ATTR</span> accepting an existing Node ID as well as a set of attributes to be added to the specified node, and 3) <span id="A7.p1.5.3" class="ltx_text ltx_font_italic">#ADD_EDGE</span> accepting a source and target pair of existing node IDs along with the predicate to be assigned between them. Our formal language always generates reference semantic parses with new attributes added first, followed by new Nodes (and assigned attributes), and lastly new edges. However, when evaluating our model outputs the execution function <math id="A7.p1.5.m5.1" class="ltx_Math" alttext="e" display="inline"><semantics id="A7.p1.5.m5.1a"><mi id="A7.p1.5.m5.1.1" xref="A7.p1.5.m5.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A7.p1.5.m5.1b"><ci id="A7.p1.5.m5.1.1.cmml" xref="A7.p1.5.m5.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.5.m5.1c">e</annotation></semantics></math> can accept these commands in any order, so long as the referenced node IDs already have been added.</p>
</div>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Licensing</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">Our paper utilized the Visual Genome dataset which is listed under a Creative Commons license. All other tools utilized are available from either Pythons Spacy or Huggingface and are available for academic use. To the best of our knowledge, all artifacts utilized are aligned with their intended use cases.</p>
</div>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>AI Assistance</h2>

<div id="A9.p1" class="ltx_para">
<p id="A9.p1.1" class="ltx_p">A minor portion of code development was done with the assistance of ChatGPT. All research ideas and writing are of the authorâ€™s original creation. Grammarly was utilized for writing assistance.</p>
</div>
<div id="A9.p2" class="ltx_para">
<p id="A9.p2.1" class="ltx_p"></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.06437" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.06438" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.06438">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.06438" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.06439" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:06:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
