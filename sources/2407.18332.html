<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.18332] Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation</title><meta property="og:description" content="Recent advancements in textless speech-to-speech translation systems have been driven by the adoption of self-supervised learning techniques.
Although most state-of-the-art systems adopt a similar architecture to trans…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.18332">

<!--Generated on Mon Aug  5 17:39:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]JarodDuret
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]YannickEsteve
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]TitouanParcollet



</p>
</div>
<h1 class="ltx_title ltx_title_document">Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Recent advancements in textless speech-to-speech translation systems have been driven by the adoption of self-supervised learning techniques.
Although most state-of-the-art systems adopt a similar architecture to transform source language speech into sequences of discrete representations in the target language, the criteria for selecting these target speech units remains an open question.
This work explores the selection process through a study of downstream tasks such as automatic speech recognition, speech synthesis, speaker recognition, and emotion recognition.
Interestingly, our findings reveal a discrepancy in the optimization of discrete speech units: units that perform well in resynthesis performance do not necessarily correlate with those that enhance translation efficacy.
This discrepancy underscores the nuanced complexity of target feature selection and its impact on the overall performance of speech-to-speech translation systems.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech translation, discrete audio token, self-supervised learning
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech-to-speech translation (S2ST) provides a powerful means of overcoming the communication gap between people speaking different languages by enabling effective communication across diverse languages and cultures.
Several approaches have been proposed in the literature, including cascaded approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> that combine automatic speech recognition (ASR), machine translation (MT) and text-to-speech (TTS).
More recently, textless approach which leverages discrete speech units extracted from self-supervised representation has been introduced  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
This technique is specifically designed to capture the linguistic content of the target speech effectively while minimizing the influence of the speaker's prosodic features.
Previous studies demonstrated that the use of discrete speech units effectively separates linguistic content from prosodic characteristics and speaker identity.
However, an open question remains regarding the construction and selection of these discrete speech units.
In  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the authors opted to utilize HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, as this model has demonstrated superior performance in automatic speech recognition (ASR), spoken language modeling, and speech synthesis compared to other unsupervised representations, as shown in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Although this superiority has been established for continuous representations, there is a notable lack of analysis concerning discrete ones, especially on a layer-wise basis.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Furthermore, available speech translation corpora, such as Fisher and CALLHOME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or CoVoST 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, do not contain parallel speech, the target language speech must be synthesized from text translations.
A few datasets already provide TTS-synthesized speech, such as CVSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and more recently, SpeechMatrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, a large-scale multilingual corpus containing real speech.
The common issue with all these datasets is the mismatch in speaker identity and emotion, they are not consistent across the source and target speech.
This inconsistency underscores the need to take these elements into account when selecting the target speech representation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this study, we explore the challenge of choosing effective discrete units for textless speech-to-speech translation.
Additionally, we evaluate discrete self-supervised representations from various encoders reported in the literature across four downstream tasks: automatic speech recognition, speech synthesis, speaker recognition, and emotion recognition.
Then, we investigate the potential of using semantically aligned (speech-text) speech representations to improve the ability of discrete speech units to preserve semantic information.
This approach aims to enhance robustness against acoustic variations that could otherwise lead to diminished translation performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.3" class="ltx_p">For this study, we considered all existing models in the literature for speech-to-speech translation (S2ST) to the best of our knowledge.
Consequently, two self-supervised encoders have been selected: Wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, each available in both monolingual and multilingual versions.
Additionally, SAMU-XLSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, a distilled version of Wav2Vec XLS-R <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> fine-tuned to predict text embeddings from a LaBSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> text encoder, is also included in our analysis.
All considered models generate output at the same frequency, producing a representation of size D every 20 ms of the audio signal. For the Large versions, D = 1,024, and for the Base versions, D = 768.
The models are based on very similar Transformer-based architectures, yet they differ in their pretraining pretext tasks.
The training of Wav2vec 2.0 is based on the contrastive predictive coding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> (CPC) objective, which aims to maximize the mutual information between a set of context features and predicted future samples.
Meanwhile, HuBERT's approach involves mapping unlabeled audio to sequences of pseudo-labels obtained through the clustering of previous representations.
To extract the sequence of speech units, we employ k-means clustering on the raw speech features, using the learned centroids of the <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">K</annotation></semantics></math> clusters to convert audio into a sequence of cluster indices for every 20ms segment of the input audio signal.
For the base model, we extract representations from every second layer, and for the large model, from every fourth layer, to maintain manageable experiment scales.
Another parameter is the choice of <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">k</annotation></semantics></math>. In line with prior research, we explore three values of <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">k</annotation></semantics></math>: 128, 512, and 1024. This approach allows us to assess the impact of cluster granularity on the performance of our downstream tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Downstream Tasks and Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To align with previous self-supervised learning (SSL) studies, we evaluate discrete speech representations across various tasks assessing different aspects of the speech signal
We present four tasks designed to analyze aspects related to phonetics, speaker identity, emotions, and semantics. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Emotion Recognition (ER):</span> ESD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, a multilingual emotional database, consists of 350 parallel utterances recorded by 10 native English and 10 native Chinese speakers (10 females, 10 males), containing five emotional states (neutral, happy, angry, sad, and surprise).
In this study, we focus exclusively on the English subset. The official training, development, and testing splits are utilized for evaluation, with accuracy serving as the evaluation metric. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Automatic Speech Recognition (ASR):</span> LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a corpus of approximately 1000 hours of 16kHz read English speech derived from read audiobooks.
In this study, we concentrate on the train-clean-100 subset for training.
The dev-clean subset is used for validation, while the test-clean and test-other subsets are employed for testing.
Character Error Rate (CER) serves as the error metric. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Automatic Speaker Verification (ASV):</span> VoxCeleb1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, a large-scale speaker identification dataset, contains over 100,000 utterances from 1,251 celebrities, extracted from videos uploaded to YouTube.
Official training, development, and testing splits are utilized for evaluation, ensuring no overlap between speakers in the training and testing sets.
The evaluation metric is the Equal Error Rate (EER). 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">Speech Synthesis:</span> LJSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a dataset comprising 13,100 short audio clips from a single speaker reading passages from 7 non-fiction books, totaling approximately 24 hours.
We randomly split the dataset into training, development, and testing sets with a ratio of 80:10:10%.
The evaluation metric is the Mean Opinion Score (MOS).
Given the number of models, we opted for the UTokyo-SaruLab MOS prediction system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to automatically assess the quality of the trained models.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Systems description</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">This section provides a brief description of the downstream probes employed in our study.
For all downstream tasks, the discrete tokens are
initially passed through an embedding layer that is randomly initialized.
The code for all experiments, training logs, and hyperparameters will be accessible once the review process has been completed. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">ER &amp; ASV:</span> For the classification tasks, we follow previous benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and utilize ECAPA-TDNN, which combines convolutional and residual blocks. This system is trained using negative log-likelihood loss for ER and Additive Margin Softmax Loss for ASV. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">ASR:</span> In the speech recognition task, we replicate a previously established benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, utilizing a vanilla 2-layer BiLSTM with 1,024 units each, followed by a linear layer that maps audio to characters. The system is trained using the Connectionist Temporal Classification (CTC) loss at the character level. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Speech Synthesis:</span> Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we use the HiFi-GAN neural vocoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to synthesize speech.
HiFiGAN is a generative adversarial network (GAN) consisting of one generator and a set of discriminators.
We adapted the generator architecture to take as input a sequence of discrete-unit. 
<br class="ltx_break"></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Speech to Speech Translation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The following section describes the dataset and the speech-to-unit translation (S2UT) model used to assess the choice of discrete speech units.
We use the CVSS corpora to train and evaluate our speech-to-unit translation model.
CVSS is a massively multilingual-to-English speech-to-speech translation corpus, covering pairs from 21 languages to English.
However, only the French-to-English translation is considered in this study.
The dataset includes two versions of spoken translation: CVSS-C and CVSS-T. While both versions can be utilized to train our system, we use CVSS-C because of its superior speech quality.
Official training, development and testing splits are utilized for evaluation.
We build the S2UT model by adapting the transformer encoder-decoder framework presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
The encoder is composed of a Wav2Vec 2.0 base pre-trained on 3K hours of French speech <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>huggingface.co/LeBenchmark/wav2vec2-FR-3K-base</span></span></span>.
As a decoder, we use <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mn id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><cn type="integer" id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">6</annotation></semantics></math> transformer layers with a random weight initialization.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We combined the Wav2Vec 2.0 encoder along with the transformer decoder and we finetune the whole model end-to-end.
During inference, the S2UT model's predictions are fed into a vocoder trained on discrete speech units for speech synthesis
Recent research in speech-to-speech translation advocates for using BLEU scores to evaluate translation quality.
First, we use a speech recognition model <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>huggingface.co/speechbrain/asr-transformer-librispeech</span></span></span> to compute the transcriptions of the generated speech.
Then, we compute the BLEU score for the ASR-decoded text in comparison to the reference translations.
We acknowledge that the ASR BLEU score may be influenced by ASR model performance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the following section, we first discuss the results of the four downstream tasks independently.
Next, we evaluate the translation quality of the retained encoders and k-means (k=number of discrete speech units) against a baseline setup reproduced from the literature.
Finally, we discuss the correlation between downstream task and speech-to-speech translation performance.
In the following tables, for Base model, we report scores from every second layer, and for Large model, from every fourth layer.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Emotion recognition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">From Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Emotion recognition ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can observe similar performance for HuBERT and Wav2Vec2.
Both encoders start with relatively high accuracy in initial layers, indicating their capability to capture emotional content effectively at these stages.
The best performance is observed with the HuBERT Base model using k=1024 and layer <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">2</annotation></semantics></math>, achieving the highest accuracy of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="66.1\%" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">66.1</mn><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">66.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">66.1\%</annotation></semantics></math>.
We denote a progressive decrease in performance as layers progress, this decrease is especially pronounced in the Wav2Vec2 Base model, where accuracy drops significantly from initial to subsequent layers, highlighting a potential issue in maintaining emotional content representation in deeper layers.
For Multilingual encoders, Wav2Vec2 XLS-R generally achieves better performance across most layers.
The decline in SAMU-XLSR performance across consecutive layers likely stems from its specialization in encoding semantic information more effectively in the upper layers, albeit at the cost of less efficient encoding of emotional information.
In addition, due to the presence of identical linguistic in the utterances for all emotional states in the dataset, the model struggles to effectively utilize semantic information for accurate label prediction.
</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Benchmarking results for the emotion recognition task across various self-supervised learning (SSL) models, both in base and large configurations, using different cluster sizes (k=128, 512, 1024) for speech unit extraction. The Accuracy is used as the performance metric</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:182.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-73.7pt,61.9pt) scale(0.595267366245029,0.595267366245029) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.1.1.1.2.1" class="ltx_text">SSL Model</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.1.1.1.3.1" class="ltx_text">Setting</span></th>
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">Layer Base/Large - ACC <math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">2/4</th>
<th id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">4/8</th>
<th id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">6/12</th>
<th id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">8/16</th>
<th id="S3.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">10/20</th>
<th id="S3.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">12/24</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.3.1" class="ltx_tr">
<th id="S3.T1.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T1.1.1.3.1.1.1" class="ltx_text">Hubert Base</span></th>
<th id="S3.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">64.4</td>
<td id="S3.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">62.1</td>
<td id="S3.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">60.8</td>
<td id="S3.T1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">62.7</td>
<td id="S3.T1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">54.6</td>
<td id="S3.T1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">60.1</td>
</tr>
<tr id="S3.T1.1.1.4.2" class="ltx_tr">
<th id="S3.T1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T1.1.1.4.2.2" class="ltx_td ltx_align_center">65.4</td>
<td id="S3.T1.1.1.4.2.3" class="ltx_td ltx_align_center">64.2</td>
<td id="S3.T1.1.1.4.2.4" class="ltx_td ltx_align_center">59.5</td>
<td id="S3.T1.1.1.4.2.5" class="ltx_td ltx_align_center">56.9</td>
<td id="S3.T1.1.1.4.2.6" class="ltx_td ltx_align_center">57.3</td>
<td id="S3.T1.1.1.4.2.7" class="ltx_td ltx_align_center">63.7</td>
</tr>
<tr id="S3.T1.1.1.5.3" class="ltx_tr">
<th id="S3.T1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T1.1.1.5.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.5.3.2.1" class="ltx_text ltx_font_bold">66.1</span></td>
<td id="S3.T1.1.1.5.3.3" class="ltx_td ltx_align_center">63.3</td>
<td id="S3.T1.1.1.5.3.4" class="ltx_td ltx_align_center">62.7</td>
<td id="S3.T1.1.1.5.3.5" class="ltx_td ltx_align_center">55.3</td>
<td id="S3.T1.1.1.5.3.6" class="ltx_td ltx_align_center">59.9</td>
<td id="S3.T1.1.1.5.3.7" class="ltx_td ltx_align_center">63.0</td>
</tr>
<tr id="S3.T1.1.1.6.4" class="ltx_tr">
<th id="S3.T1.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T1.1.1.6.4.1.1" class="ltx_text">Wav2Vec2 Base</span></th>
<th id="S3.T1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">63.8</td>
<td id="S3.T1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">54.8</td>
<td id="S3.T1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">49.7</td>
<td id="S3.T1.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">48.3</td>
<td id="S3.T1.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">44.5</td>
<td id="S3.T1.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">38.8</td>
</tr>
<tr id="S3.T1.1.1.7.5" class="ltx_tr">
<th id="S3.T1.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T1.1.1.7.5.2" class="ltx_td ltx_align_center">62.8</td>
<td id="S3.T1.1.1.7.5.3" class="ltx_td ltx_align_center">56.9</td>
<td id="S3.T1.1.1.7.5.4" class="ltx_td ltx_align_center">49.1</td>
<td id="S3.T1.1.1.7.5.5" class="ltx_td ltx_align_center">47.4</td>
<td id="S3.T1.1.1.7.5.6" class="ltx_td ltx_align_center">44.7</td>
<td id="S3.T1.1.1.7.5.7" class="ltx_td ltx_align_center">36.1</td>
</tr>
<tr id="S3.T1.1.1.8.6" class="ltx_tr">
<th id="S3.T1.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T1.1.1.8.6.2" class="ltx_td ltx_align_center">65.9</td>
<td id="S3.T1.1.1.8.6.3" class="ltx_td ltx_align_center">59.6</td>
<td id="S3.T1.1.1.8.6.4" class="ltx_td ltx_align_center">48.5</td>
<td id="S3.T1.1.1.8.6.5" class="ltx_td ltx_align_center">43.8</td>
<td id="S3.T1.1.1.8.6.6" class="ltx_td ltx_align_center">42.2</td>
<td id="S3.T1.1.1.8.6.7" class="ltx_td ltx_align_center">36.0</td>
</tr>
<tr id="S3.T1.1.1.9.7" class="ltx_tr">
<th id="S3.T1.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="3"><span id="S3.T1.1.1.9.7.1.1" class="ltx_text">mHuBERT Base</span></th>
<th id="S3.T1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">k=128</th>
<td id="S3.T1.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_tt">65.2</td>
<td id="S3.T1.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_tt">61.4</td>
<td id="S3.T1.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_tt">55.7</td>
<td id="S3.T1.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_tt">52.3</td>
<td id="S3.T1.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_tt">54.4</td>
<td id="S3.T1.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_tt">57.9</td>
</tr>
<tr id="S3.T1.1.1.10.8" class="ltx_tr">
<th id="S3.T1.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T1.1.1.10.8.2" class="ltx_td ltx_align_center">64.2</td>
<td id="S3.T1.1.1.10.8.3" class="ltx_td ltx_align_center">63.3</td>
<td id="S3.T1.1.1.10.8.4" class="ltx_td ltx_align_center">57.9</td>
<td id="S3.T1.1.1.10.8.5" class="ltx_td ltx_align_center">55.3</td>
<td id="S3.T1.1.1.10.8.6" class="ltx_td ltx_align_center">59.3</td>
<td id="S3.T1.1.1.10.8.7" class="ltx_td ltx_align_center">59.2</td>
</tr>
<tr id="S3.T1.1.1.11.9" class="ltx_tr">
<th id="S3.T1.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T1.1.1.11.9.2" class="ltx_td ltx_align_center">62.7</td>
<td id="S3.T1.1.1.11.9.3" class="ltx_td ltx_align_center">63.8</td>
<td id="S3.T1.1.1.11.9.4" class="ltx_td ltx_align_center">58.3</td>
<td id="S3.T1.1.1.11.9.5" class="ltx_td ltx_align_center">56.5</td>
<td id="S3.T1.1.1.11.9.6" class="ltx_td ltx_align_center">57.4</td>
<td id="S3.T1.1.1.11.9.7" class="ltx_td ltx_align_center">58.5</td>
</tr>
<tr id="S3.T1.1.1.12.10" class="ltx_tr">
<th id="S3.T1.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T1.1.1.12.10.1.1" class="ltx_text">Wav2Vec2 XLS-R Large</span></th>
<th id="S3.T1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T1.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_t">65.7</td>
<td id="S3.T1.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_t">64.5</td>
<td id="S3.T1.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_t">65.1</td>
<td id="S3.T1.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_t">66.9</td>
<td id="S3.T1.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_t">69.7</td>
<td id="S3.T1.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_t">51.3</td>
</tr>
<tr id="S3.T1.1.1.13.11" class="ltx_tr">
<th id="S3.T1.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T1.1.1.13.11.2" class="ltx_td ltx_align_center">66.3</td>
<td id="S3.T1.1.1.13.11.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.13.11.3.1" class="ltx_text ltx_font_bold">69.6</span></td>
<td id="S3.T1.1.1.13.11.4" class="ltx_td ltx_align_center">67.7</td>
<td id="S3.T1.1.1.13.11.5" class="ltx_td ltx_align_center">66.9</td>
<td id="S3.T1.1.1.13.11.6" class="ltx_td ltx_align_center">68.3</td>
<td id="S3.T1.1.1.13.11.7" class="ltx_td ltx_align_center">56.4</td>
</tr>
<tr id="S3.T1.1.1.14.12" class="ltx_tr">
<th id="S3.T1.1.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T1.1.1.14.12.2" class="ltx_td ltx_align_center">69.3</td>
<td id="S3.T1.1.1.14.12.3" class="ltx_td ltx_align_center">66.6</td>
<td id="S3.T1.1.1.14.12.4" class="ltx_td ltx_align_center">65.4</td>
<td id="S3.T1.1.1.14.12.5" class="ltx_td ltx_align_center">66.4</td>
<td id="S3.T1.1.1.14.12.6" class="ltx_td ltx_align_center">67.7</td>
<td id="S3.T1.1.1.14.12.7" class="ltx_td ltx_align_center">57.9</td>
</tr>
<tr id="S3.T1.1.1.15.13" class="ltx_tr">
<th id="S3.T1.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T1.1.1.15.13.1.1" class="ltx_text">SAMU-XLSR Large</span></th>
<th id="S3.T1.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T1.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_t">65.5</td>
<td id="S3.T1.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_t">48.1</td>
<td id="S3.T1.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_t">42.3</td>
<td id="S3.T1.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_t">36.8</td>
<td id="S3.T1.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_t">31.1</td>
<td id="S3.T1.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_t">29.6</td>
</tr>
<tr id="S3.T1.1.1.16.14" class="ltx_tr">
<th id="S3.T1.1.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T1.1.1.16.14.2" class="ltx_td ltx_align_center">65.4</td>
<td id="S3.T1.1.1.16.14.3" class="ltx_td ltx_align_center">48.1</td>
<td id="S3.T1.1.1.16.14.4" class="ltx_td ltx_align_center">39.3</td>
<td id="S3.T1.1.1.16.14.5" class="ltx_td ltx_align_center">34.3</td>
<td id="S3.T1.1.1.16.14.6" class="ltx_td ltx_align_center">31.3</td>
<td id="S3.T1.1.1.16.14.7" class="ltx_td ltx_align_center">30.1</td>
</tr>
<tr id="S3.T1.1.1.17.15" class="ltx_tr">
<th id="S3.T1.1.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">k=1024</th>
<td id="S3.T1.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb">65.4</td>
<td id="S3.T1.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">49.1</td>
<td id="S3.T1.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_bb">40.1</td>
<td id="S3.T1.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_bb">34.3</td>
<td id="S3.T1.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_bb">30.1</td>
<td id="S3.T1.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_bb">30.5</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Automatic speech recognition</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As expected for the Automatic Speech Recognition (ASR) task, Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Automatic speech recognition ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates that discrete speech units generated by the high middle layers generally yield superior results in Wav2Vec 2.0 models. Regarding continuous speech representation, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> has demonstrated that linguistic word-level information is better encoded in the high middle layers of these self-supervised learning (SSL) models. HuBERT exhibits a distinct behavior, as the discrete speech units generated in its deepest layers (excluding the last one) achieve optimal performance, consistent with observations for continuous speech representations presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p">Wav2Vec2 Base consistently outperforms HuBERT Base across all cluster sizes, achieving the lowest Character Error Rate (CER) of <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="1.38" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">1.38</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="float" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">1.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">1.38</annotation></semantics></math> at layer <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><cn type="integer" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">10</annotation></semantics></math> with <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="k=1024" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">k</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑘</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">k=1024</annotation></semantics></math>.
The multilingual HuBERT model demonstrates a decrease in performance compared to its monolingual counterpart, indicating potential challenges in handling diverse languages for ASR tasks.
Notably, Wav2Vec2 XLS-R surpasses SAMU-XLSR across all layers, despite SAMU-XLSR being fine-tuned for text embedding prediction.
Furthermore, we observe that the impact of cluster size is more pronounced in the larger models than in the base models, suggesting that larger models benefit from a higher number of clusters.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Benchmarking results for the automatic speech recognition task across various self-supervised learning (SSL) models, both in base and large configurations, using different cluster sizes (k=128, 512, 1024) for speech unit extraction. The Character Error Rate (CER) is used as the performance metric.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:174.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.5pt,65.7pt) scale(0.570885409043285,0.570885409043285) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.2.1" class="ltx_text">SSL Model</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.3.1" class="ltx_text">Setting</span></th>
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">Layer Base/Large - CER <math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<th id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">2/4</th>
<th id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">4/8</th>
<th id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">6/12</th>
<th id="S3.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">8/16</th>
<th id="S3.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">10/20</th>
<th id="S3.T2.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">12/24</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.3.1" class="ltx_tr">
<th id="S3.T2.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.1.3.1.1.1" class="ltx_text">Hubert Base</span></th>
<th id="S3.T2.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T2.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">13.44</td>
<td id="S3.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">9.15</td>
<td id="S3.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">6.10</td>
<td id="S3.T2.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">5.14</td>
<td id="S3.T2.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">4.64</td>
<td id="S3.T2.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">5.70</td>
</tr>
<tr id="S3.T2.1.1.4.2" class="ltx_tr">
<th id="S3.T2.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T2.1.1.4.2.2" class="ltx_td ltx_align_center">11.36</td>
<td id="S3.T2.1.1.4.2.3" class="ltx_td ltx_align_center">8.34</td>
<td id="S3.T2.1.1.4.2.4" class="ltx_td ltx_align_center">5.54</td>
<td id="S3.T2.1.1.4.2.5" class="ltx_td ltx_align_center">4.03</td>
<td id="S3.T2.1.1.4.2.6" class="ltx_td ltx_align_center">3.37</td>
<td id="S3.T2.1.1.4.2.7" class="ltx_td ltx_align_center">4.82</td>
</tr>
<tr id="S3.T2.1.1.5.3" class="ltx_tr">
<th id="S3.T2.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T2.1.1.5.3.2" class="ltx_td ltx_align_center">11.44</td>
<td id="S3.T2.1.1.5.3.3" class="ltx_td ltx_align_center">8.14</td>
<td id="S3.T2.1.1.5.3.4" class="ltx_td ltx_align_center">5.50</td>
<td id="S3.T2.1.1.5.3.5" class="ltx_td ltx_align_center">3.94</td>
<td id="S3.T2.1.1.5.3.6" class="ltx_td ltx_align_center">3.16</td>
<td id="S3.T2.1.1.5.3.7" class="ltx_td ltx_align_center">4.96</td>
</tr>
<tr id="S3.T2.1.1.6.4" class="ltx_tr">
<th id="S3.T2.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.1.6.4.1.1" class="ltx_text">Wav2Vec2 Base</span></th>
<th id="S3.T2.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T2.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">10.66</td>
<td id="S3.T2.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">6.54</td>
<td id="S3.T2.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">4.49</td>
<td id="S3.T2.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">3.96</td>
<td id="S3.T2.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">1.49</td>
<td id="S3.T2.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">4.31</td>
</tr>
<tr id="S3.T2.1.1.7.5" class="ltx_tr">
<th id="S3.T2.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T2.1.1.7.5.2" class="ltx_td ltx_align_center">9.44</td>
<td id="S3.T2.1.1.7.5.3" class="ltx_td ltx_align_center">5.94</td>
<td id="S3.T2.1.1.7.5.4" class="ltx_td ltx_align_center">4.00</td>
<td id="S3.T2.1.1.7.5.5" class="ltx_td ltx_align_center">2.68</td>
<td id="S3.T2.1.1.7.5.6" class="ltx_td ltx_align_center">1.40</td>
<td id="S3.T2.1.1.7.5.7" class="ltx_td ltx_align_center">4.37</td>
</tr>
<tr id="S3.T2.1.1.8.6" class="ltx_tr">
<th id="S3.T2.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T2.1.1.8.6.2" class="ltx_td ltx_align_center">9.45</td>
<td id="S3.T2.1.1.8.6.3" class="ltx_td ltx_align_center">6.06</td>
<td id="S3.T2.1.1.8.6.4" class="ltx_td ltx_align_center">3.90</td>
<td id="S3.T2.1.1.8.6.5" class="ltx_td ltx_align_center">2.54</td>
<td id="S3.T2.1.1.8.6.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.6.6.1" class="ltx_text ltx_font_bold">1.38</span></td>
<td id="S3.T2.1.1.8.6.7" class="ltx_td ltx_align_center">4.24</td>
</tr>
<tr id="S3.T2.1.1.9.7" class="ltx_tr">
<th id="S3.T2.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="3"><span id="S3.T2.1.1.9.7.1.1" class="ltx_text">mHuBERT Base</span></th>
<th id="S3.T2.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">k=128</th>
<td id="S3.T2.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_tt">15.03</td>
<td id="S3.T2.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_tt">10.22</td>
<td id="S3.T2.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_tt">7.36</td>
<td id="S3.T2.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_tt">6.79</td>
<td id="S3.T2.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_tt">6.66</td>
<td id="S3.T2.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_tt">6.04</td>
</tr>
<tr id="S3.T2.1.1.10.8" class="ltx_tr">
<th id="S3.T2.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T2.1.1.10.8.2" class="ltx_td ltx_align_center">12.81</td>
<td id="S3.T2.1.1.10.8.3" class="ltx_td ltx_align_center">9.16</td>
<td id="S3.T2.1.1.10.8.4" class="ltx_td ltx_align_center">6.89</td>
<td id="S3.T2.1.1.10.8.5" class="ltx_td ltx_align_center">6.23</td>
<td id="S3.T2.1.1.10.8.6" class="ltx_td ltx_align_center">5.96</td>
<td id="S3.T2.1.1.10.8.7" class="ltx_td ltx_align_center">5.74</td>
</tr>
<tr id="S3.T2.1.1.11.9" class="ltx_tr">
<th id="S3.T2.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T2.1.1.11.9.2" class="ltx_td ltx_align_center">12.79</td>
<td id="S3.T2.1.1.11.9.3" class="ltx_td ltx_align_center">9.03</td>
<td id="S3.T2.1.1.11.9.4" class="ltx_td ltx_align_center">6.98</td>
<td id="S3.T2.1.1.11.9.5" class="ltx_td ltx_align_center">6.23</td>
<td id="S3.T2.1.1.11.9.6" class="ltx_td ltx_align_center">5.87</td>
<td id="S3.T2.1.1.11.9.7" class="ltx_td ltx_align_center">5.69</td>
</tr>
<tr id="S3.T2.1.1.12.10" class="ltx_tr">
<th id="S3.T2.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.1.12.10.1.1" class="ltx_text">Wav2Vec2 XLS-R Large</span></th>
<th id="S3.T2.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T2.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_t">16.33</td>
<td id="S3.T2.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_t">11.08</td>
<td id="S3.T2.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_t">9.15</td>
<td id="S3.T2.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_t">5.72</td>
<td id="S3.T2.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_t">15.98</td>
<td id="S3.T2.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_t">45.35</td>
</tr>
<tr id="S3.T2.1.1.13.11" class="ltx_tr">
<th id="S3.T2.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T2.1.1.13.11.2" class="ltx_td ltx_align_center">12.32</td>
<td id="S3.T2.1.1.13.11.3" class="ltx_td ltx_align_center">8.09</td>
<td id="S3.T2.1.1.13.11.4" class="ltx_td ltx_align_center">6.73</td>
<td id="S3.T2.1.1.13.11.5" class="ltx_td ltx_align_center">4.30</td>
<td id="S3.T2.1.1.13.11.6" class="ltx_td ltx_align_center">10.28</td>
<td id="S3.T2.1.1.13.11.7" class="ltx_td ltx_align_center">32.86</td>
</tr>
<tr id="S3.T2.1.1.14.12" class="ltx_tr">
<th id="S3.T2.1.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T2.1.1.14.12.2" class="ltx_td ltx_align_center">11.87</td>
<td id="S3.T2.1.1.14.12.3" class="ltx_td ltx_align_center">7.82</td>
<td id="S3.T2.1.1.14.12.4" class="ltx_td ltx_align_center">6.26</td>
<td id="S3.T2.1.1.14.12.5" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.14.12.5.1" class="ltx_text ltx_font_bold">4.07</span></td>
<td id="S3.T2.1.1.14.12.6" class="ltx_td ltx_align_center">9.09</td>
<td id="S3.T2.1.1.14.12.7" class="ltx_td ltx_align_center">29.22</td>
</tr>
<tr id="S3.T2.1.1.15.13" class="ltx_tr">
<th id="S3.T2.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.1.15.13.1.1" class="ltx_text">SAMU-XLSR Large</span></th>
<th id="S3.T2.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T2.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_t">11.70</td>
<td id="S3.T2.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_t">6.66</td>
<td id="S3.T2.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_t">11.43</td>
<td id="S3.T2.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_t">16.89</td>
<td id="S3.T2.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_t">27.01</td>
<td id="S3.T2.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_t">71.41</td>
</tr>
<tr id="S3.T2.1.1.16.14" class="ltx_tr">
<th id="S3.T2.1.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T2.1.1.16.14.2" class="ltx_td ltx_align_center">10.06</td>
<td id="S3.T2.1.1.16.14.3" class="ltx_td ltx_align_center">6.28</td>
<td id="S3.T2.1.1.16.14.4" class="ltx_td ltx_align_center">6.36</td>
<td id="S3.T2.1.1.16.14.5" class="ltx_td ltx_align_center">7.80</td>
<td id="S3.T2.1.1.16.14.6" class="ltx_td ltx_align_center">13.90</td>
<td id="S3.T2.1.1.16.14.7" class="ltx_td ltx_align_center">66.91</td>
</tr>
<tr id="S3.T2.1.1.17.15" class="ltx_tr">
<th id="S3.T2.1.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">k=1024</th>
<td id="S3.T2.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb">9.89</td>
<td id="S3.T2.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">6.14</td>
<td id="S3.T2.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_bb">5.72</td>
<td id="S3.T2.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_bb">5.77</td>
<td id="S3.T2.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_bb">10.13</td>
<td id="S3.T2.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_bb">70.94</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Automatic speaker verification</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">From Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Automatic speaker verification ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we denote that HuBERT Base model systematically outperforms the Wav2Vec2 Base across different layers and cluster sizes.
The table also illustrates a notable increase in the EER for SAMU-XLSR at higher layers across all cluster sizes, significantly underperforming compared to Wav2Vec2 XLS-R.
This trend suggests a loss of speaker-specific information in SAMU-XLSR's higher layers, which might be due to its focus on retaining semantic information.
We observe a consistent pattern from the data, showing that models generally achieve better performance at lower to mid layers than at the highest layers for speaker verification tasks.
The impact of cluster size on EER varies across models, but the general improvement in performance with increasing cluster size suggests that more granular speech unit representations can enhance ASV performance.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Benchmarking results for the automatic speaker verification task across various self-supervised learning (SSL) models, both in base and large configurations, using different cluster sizes (k=128, 512, 1024) for speech unit extraction. The Equal Error Rate (EER) is used as the performance metric.</figcaption>
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:174.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.5pt,65.7pt) scale(0.570885409043285,0.570885409043285) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T3.1.1.1.2.1" class="ltx_text">SSL Model</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T3.1.1.1.3.1" class="ltx_text">Setting</span></th>
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">Layer Base/Large - EER <math id="S3.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
<tr id="S3.T3.1.1.2.1" class="ltx_tr">
<th id="S3.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">2/4</th>
<th id="S3.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">4/8</th>
<th id="S3.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">6/12</th>
<th id="S3.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">8/16</th>
<th id="S3.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">10/20</th>
<th id="S3.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">12/24</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.3.1" class="ltx_tr">
<th id="S3.T3.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.1.3.1.1.1" class="ltx_text">Hubert Base</span></th>
<th id="S3.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">19.43</td>
<td id="S3.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">19.88</td>
<td id="S3.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">21.26</td>
<td id="S3.T3.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">21.25</td>
<td id="S3.T3.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">20.70</td>
<td id="S3.T3.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">19.05</td>
</tr>
<tr id="S3.T3.1.1.4.2" class="ltx_tr">
<th id="S3.T3.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T3.1.1.4.2.2" class="ltx_td ltx_align_center">17.73</td>
<td id="S3.T3.1.1.4.2.3" class="ltx_td ltx_align_center">18.28</td>
<td id="S3.T3.1.1.4.2.4" class="ltx_td ltx_align_center">19.99</td>
<td id="S3.T3.1.1.4.2.5" class="ltx_td ltx_align_center">21.19</td>
<td id="S3.T3.1.1.4.2.6" class="ltx_td ltx_align_center">19.20</td>
<td id="S3.T3.1.1.4.2.7" class="ltx_td ltx_align_center">16.54</td>
</tr>
<tr id="S3.T3.1.1.5.3" class="ltx_tr">
<th id="S3.T3.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T3.1.1.5.3.2" class="ltx_td ltx_align_center">17.11</td>
<td id="S3.T3.1.1.5.3.3" class="ltx_td ltx_align_center">18.54</td>
<td id="S3.T3.1.1.5.3.4" class="ltx_td ltx_align_center">19.93</td>
<td id="S3.T3.1.1.5.3.5" class="ltx_td ltx_align_center">21.26</td>
<td id="S3.T3.1.1.5.3.6" class="ltx_td ltx_align_center">18.28</td>
<td id="S3.T3.1.1.5.3.7" class="ltx_td ltx_align_center"><span id="S3.T3.1.1.5.3.7.1" class="ltx_text ltx_font_bold">16.46</span></td>
</tr>
<tr id="S3.T3.1.1.6.4" class="ltx_tr">
<th id="S3.T3.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.1.6.4.1.1" class="ltx_text">Wav2Vec2 Base</span></th>
<th id="S3.T3.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T3.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">21.10</td>
<td id="S3.T3.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">23.20</td>
<td id="S3.T3.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">26.65</td>
<td id="S3.T3.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">27.25</td>
<td id="S3.T3.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">31.27</td>
<td id="S3.T3.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">33.35</td>
</tr>
<tr id="S3.T3.1.1.7.5" class="ltx_tr">
<th id="S3.T3.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T3.1.1.7.5.2" class="ltx_td ltx_align_center">19.58</td>
<td id="S3.T3.1.1.7.5.3" class="ltx_td ltx_align_center">22.50</td>
<td id="S3.T3.1.1.7.5.4" class="ltx_td ltx_align_center">26.42</td>
<td id="S3.T3.1.1.7.5.5" class="ltx_td ltx_align_center">26.96</td>
<td id="S3.T3.1.1.7.5.6" class="ltx_td ltx_align_center">30.63</td>
<td id="S3.T3.1.1.7.5.7" class="ltx_td ltx_align_center">33.68</td>
</tr>
<tr id="S3.T3.1.1.8.6" class="ltx_tr">
<th id="S3.T3.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T3.1.1.8.6.2" class="ltx_td ltx_align_center">19.27</td>
<td id="S3.T3.1.1.8.6.3" class="ltx_td ltx_align_center">23.15</td>
<td id="S3.T3.1.1.8.6.4" class="ltx_td ltx_align_center">26.34</td>
<td id="S3.T3.1.1.8.6.5" class="ltx_td ltx_align_center">27.17</td>
<td id="S3.T3.1.1.8.6.6" class="ltx_td ltx_align_center">30.06</td>
<td id="S3.T3.1.1.8.6.7" class="ltx_td ltx_align_center">33.03</td>
</tr>
<tr id="S3.T3.1.1.9.7" class="ltx_tr">
<th id="S3.T3.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="3"><span id="S3.T3.1.1.9.7.1.1" class="ltx_text">mHuBERT Base</span></th>
<th id="S3.T3.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">k=128</th>
<td id="S3.T3.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_tt">19.07</td>
<td id="S3.T3.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_tt">21.50</td>
<td id="S3.T3.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_tt">23.97</td>
<td id="S3.T3.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_tt">25.46</td>
<td id="S3.T3.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_tt">25.36</td>
<td id="S3.T3.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_tt">23.79</td>
</tr>
<tr id="S3.T3.1.1.10.8" class="ltx_tr">
<th id="S3.T3.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T3.1.1.10.8.2" class="ltx_td ltx_align_center">18.49</td>
<td id="S3.T3.1.1.10.8.3" class="ltx_td ltx_align_center">20.09</td>
<td id="S3.T3.1.1.10.8.4" class="ltx_td ltx_align_center">21.83</td>
<td id="S3.T3.1.1.10.8.5" class="ltx_td ltx_align_center">23.97</td>
<td id="S3.T3.1.1.10.8.6" class="ltx_td ltx_align_center">24.66</td>
<td id="S3.T3.1.1.10.8.7" class="ltx_td ltx_align_center">22.07</td>
</tr>
<tr id="S3.T3.1.1.11.9" class="ltx_tr">
<th id="S3.T3.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T3.1.1.11.9.2" class="ltx_td ltx_align_center">17.34</td>
<td id="S3.T3.1.1.11.9.3" class="ltx_td ltx_align_center">20.27</td>
<td id="S3.T3.1.1.11.9.4" class="ltx_td ltx_align_center">23.03</td>
<td id="S3.T3.1.1.11.9.5" class="ltx_td ltx_align_center">23.93</td>
<td id="S3.T3.1.1.11.9.6" class="ltx_td ltx_align_center">23.60</td>
<td id="S3.T3.1.1.11.9.7" class="ltx_td ltx_align_center">22.74</td>
</tr>
<tr id="S3.T3.1.1.12.10" class="ltx_tr">
<th id="S3.T3.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.1.12.10.1.1" class="ltx_text">Wav2Vec2 XLS-R Large</span></th>
<th id="S3.T3.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T3.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_t">20.14</td>
<td id="S3.T3.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_t">21.48</td>
<td id="S3.T3.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_t">23.68</td>
<td id="S3.T3.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_t">20.83</td>
<td id="S3.T3.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_t">23.38</td>
<td id="S3.T3.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_t">38.67</td>
</tr>
<tr id="S3.T3.1.1.13.11" class="ltx_tr">
<th id="S3.T3.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T3.1.1.13.11.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.1.13.11.2.1" class="ltx_text ltx_font_bold">18.07</span></td>
<td id="S3.T3.1.1.13.11.3" class="ltx_td ltx_align_center">19.49</td>
<td id="S3.T3.1.1.13.11.4" class="ltx_td ltx_align_center">21.66</td>
<td id="S3.T3.1.1.13.11.5" class="ltx_td ltx_align_center">22.49</td>
<td id="S3.T3.1.1.13.11.6" class="ltx_td ltx_align_center">22.78</td>
<td id="S3.T3.1.1.13.11.7" class="ltx_td ltx_align_center">38.58</td>
</tr>
<tr id="S3.T3.1.1.14.12" class="ltx_tr">
<th id="S3.T3.1.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T3.1.1.14.12.2" class="ltx_td ltx_align_center">18.79</td>
<td id="S3.T3.1.1.14.12.3" class="ltx_td ltx_align_center">20.92</td>
<td id="S3.T3.1.1.14.12.4" class="ltx_td ltx_align_center">23.59</td>
<td id="S3.T3.1.1.14.12.5" class="ltx_td ltx_align_center">19.62</td>
<td id="S3.T3.1.1.14.12.6" class="ltx_td ltx_align_center">22.51</td>
<td id="S3.T3.1.1.14.12.7" class="ltx_td ltx_align_center">37.01</td>
</tr>
<tr id="S3.T3.1.1.15.13" class="ltx_tr">
<th id="S3.T3.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.1.15.13.1.1" class="ltx_text">SAMU-XLSR Large</span></th>
<th id="S3.T3.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T3.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_t">19.49</td>
<td id="S3.T3.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_t">26.21</td>
<td id="S3.T3.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_t">31.53</td>
<td id="S3.T3.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_t">36.11</td>
<td id="S3.T3.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_t">41.71</td>
<td id="S3.T3.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_t">46.11</td>
</tr>
<tr id="S3.T3.1.1.16.14" class="ltx_tr">
<th id="S3.T3.1.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T3.1.1.16.14.2" class="ltx_td ltx_align_center">18.19</td>
<td id="S3.T3.1.1.16.14.3" class="ltx_td ltx_align_center">23.84</td>
<td id="S3.T3.1.1.16.14.4" class="ltx_td ltx_align_center">29.90</td>
<td id="S3.T3.1.1.16.14.5" class="ltx_td ltx_align_center">34.46</td>
<td id="S3.T3.1.1.16.14.6" class="ltx_td ltx_align_center">39.74</td>
<td id="S3.T3.1.1.16.14.7" class="ltx_td ltx_align_center">45.55</td>
</tr>
<tr id="S3.T3.1.1.17.15" class="ltx_tr">
<th id="S3.T3.1.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">k=1024</th>
<td id="S3.T3.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb">18.21</td>
<td id="S3.T3.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">26.11</td>
<td id="S3.T3.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_bb">29.30</td>
<td id="S3.T3.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_bb">33.73</td>
<td id="S3.T3.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_bb">39.59</td>
<td id="S3.T3.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_bb">45.84</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Speech Synthesis</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.7" class="ltx_p">In Table <a href="#S3.T4" title="Table 4 ‣ 3.4 Speech Synthesis ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can see that the vocoder fed by the discrete speech units computed from HuBERT Base model outperforms the one fed by the discrete speech units generated by Wav2Vec2 Base model, particularly on layer <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mn id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><cn type="integer" id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">6</annotation></semantics></math> with a cluster size of <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mn id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><cn type="integer" id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">512</annotation></semantics></math>, achieving a MOS score of <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="3.45" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mn id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">3.45</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><cn type="float" id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">3.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">3.45</annotation></semantics></math>.
The highest score at layer <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><mn id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><cn type="integer" id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">6</annotation></semantics></math> aligns with previous work's on speech-to-speech translation, highlighting the significance of this configuration for achieving high-quality speech synthesis.
Among the multilingual models, Wav2Vec2-XLS-R outperforms both mHuBERT Base and SAMU XLSR Large with the highest MOS score of <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="3.80" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mn id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">3.80</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><cn type="float" id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">3.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">3.80</annotation></semantics></math> on layer 20 with a cluster size of <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><mn id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><cn type="integer" id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">512</annotation></semantics></math>.
This indicates a potential benefit of larger model and extensive training data, meanwhile mHuBERT base shows competitive performance, especially with a cluster size of <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><mn id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><cn type="integer" id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">512</annotation></semantics></math>.
Finally, SAMU-XLSR Large demonstrates a significant decline in MOS scores at higher layers and for all cluster sizes.
This drop can be attributed to the model's focus on semantic over acoustic information which is critical for a vocoder.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Benchmarking results for the speech synthesis task across various self-supervised learning (SSL) models, both in base and large configurations, using different cluster sizes (k=128, 512, 1024) for speech unit extraction. The Mean Opinion Score (MOS) is used as the performance metric.</figcaption>
<div id="S3.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:182.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-73.7pt,61.9pt) scale(0.595267366245029,0.595267366245029) ;">
<table id="S3.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T4.1.1.1.2.1" class="ltx_text">SSL Model</span></th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T4.1.1.1.3.1" class="ltx_text">Setting</span></th>
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6">Layer Base/Large - MOS <math id="S3.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T4.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="S3.T4.1.1.2.1" class="ltx_tr">
<th id="S3.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">2/4</th>
<th id="S3.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">4/8</th>
<th id="S3.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">6/12</th>
<th id="S3.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">8/16</th>
<th id="S3.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">10/20</th>
<th id="S3.T4.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">12/24</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.3.1" class="ltx_tr">
<th id="S3.T4.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.1.3.1.1.1" class="ltx_text">Hubert Base</span></th>
<th id="S3.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">3.24</td>
<td id="S3.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">3.43</td>
<td id="S3.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">3.15</td>
<td id="S3.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">2.88</td>
<td id="S3.T4.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">3.25</td>
<td id="S3.T4.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">3.35</td>
</tr>
<tr id="S3.T4.1.1.4.2" class="ltx_tr">
<th id="S3.T4.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T4.1.1.4.2.2" class="ltx_td ltx_align_center">3.33</td>
<td id="S3.T4.1.1.4.2.3" class="ltx_td ltx_align_center">3.32</td>
<td id="S3.T4.1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S3.T4.1.1.4.2.4.1" class="ltx_text ltx_font_bold">3.45</span></td>
<td id="S3.T4.1.1.4.2.5" class="ltx_td ltx_align_center">3.18</td>
<td id="S3.T4.1.1.4.2.6" class="ltx_td ltx_align_center">3.05</td>
<td id="S3.T4.1.1.4.2.7" class="ltx_td ltx_align_center">3.28</td>
</tr>
<tr id="S3.T4.1.1.5.3" class="ltx_tr">
<th id="S3.T4.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T4.1.1.5.3.2" class="ltx_td ltx_align_center">3.33</td>
<td id="S3.T4.1.1.5.3.3" class="ltx_td ltx_align_center">3.33</td>
<td id="S3.T4.1.1.5.3.4" class="ltx_td ltx_align_center">3.26</td>
<td id="S3.T4.1.1.5.3.5" class="ltx_td ltx_align_center">3.13</td>
<td id="S3.T4.1.1.5.3.6" class="ltx_td ltx_align_center">3.17</td>
<td id="S3.T4.1.1.5.3.7" class="ltx_td ltx_align_center">3.32</td>
</tr>
<tr id="S3.T4.1.1.6.4" class="ltx_tr">
<th id="S3.T4.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.1.6.4.1.1" class="ltx_text">Wav2Vec2 Base</span></th>
<th id="S3.T4.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T4.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">3.30</td>
<td id="S3.T4.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">3.01</td>
<td id="S3.T4.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">2.87</td>
<td id="S3.T4.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">2.94</td>
<td id="S3.T4.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">3.04</td>
<td id="S3.T4.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">3.06</td>
</tr>
<tr id="S3.T4.1.1.7.5" class="ltx_tr">
<th id="S3.T4.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T4.1.1.7.5.2" class="ltx_td ltx_align_center">3.35</td>
<td id="S3.T4.1.1.7.5.3" class="ltx_td ltx_align_center">3.26</td>
<td id="S3.T4.1.1.7.5.4" class="ltx_td ltx_align_center">3.17</td>
<td id="S3.T4.1.1.7.5.5" class="ltx_td ltx_align_center">3.05</td>
<td id="S3.T4.1.1.7.5.6" class="ltx_td ltx_align_center">3.13</td>
<td id="S3.T4.1.1.7.5.7" class="ltx_td ltx_align_center">2.96</td>
</tr>
<tr id="S3.T4.1.1.8.6" class="ltx_tr">
<th id="S3.T4.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T4.1.1.8.6.2" class="ltx_td ltx_align_center">3.41</td>
<td id="S3.T4.1.1.8.6.3" class="ltx_td ltx_align_center">3.26</td>
<td id="S3.T4.1.1.8.6.4" class="ltx_td ltx_align_center">2.90</td>
<td id="S3.T4.1.1.8.6.5" class="ltx_td ltx_align_center">3.17</td>
<td id="S3.T4.1.1.8.6.6" class="ltx_td ltx_align_center">2.79</td>
<td id="S3.T4.1.1.8.6.7" class="ltx_td ltx_align_center">2.79</td>
</tr>
<tr id="S3.T4.1.1.9.7" class="ltx_tr">
<th id="S3.T4.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="3"><span id="S3.T4.1.1.9.7.1.1" class="ltx_text">mHuBERT Base</span></th>
<th id="S3.T4.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">k=128</th>
<td id="S3.T4.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_tt">3.26</td>
<td id="S3.T4.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_tt">3.27</td>
<td id="S3.T4.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_tt">3.17</td>
<td id="S3.T4.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_tt">3.20</td>
<td id="S3.T4.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_tt">2.94</td>
<td id="S3.T4.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_tt">3.00</td>
</tr>
<tr id="S3.T4.1.1.10.8" class="ltx_tr">
<th id="S3.T4.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T4.1.1.10.8.2" class="ltx_td ltx_align_center">3.55</td>
<td id="S3.T4.1.1.10.8.3" class="ltx_td ltx_align_center">3.38</td>
<td id="S3.T4.1.1.10.8.4" class="ltx_td ltx_align_center">3.34</td>
<td id="S3.T4.1.1.10.8.5" class="ltx_td ltx_align_center">3.27</td>
<td id="S3.T4.1.1.10.8.6" class="ltx_td ltx_align_center">3.24</td>
<td id="S3.T4.1.1.10.8.7" class="ltx_td ltx_align_center">3.12</td>
</tr>
<tr id="S3.T4.1.1.11.9" class="ltx_tr">
<th id="S3.T4.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T4.1.1.11.9.2" class="ltx_td ltx_align_center">3.47</td>
<td id="S3.T4.1.1.11.9.3" class="ltx_td ltx_align_center">3.40</td>
<td id="S3.T4.1.1.11.9.4" class="ltx_td ltx_align_center">3.41</td>
<td id="S3.T4.1.1.11.9.5" class="ltx_td ltx_align_center">3.24</td>
<td id="S3.T4.1.1.11.9.6" class="ltx_td ltx_align_center">3.25</td>
<td id="S3.T4.1.1.11.9.7" class="ltx_td ltx_align_center">3.25</td>
</tr>
<tr id="S3.T4.1.1.12.10" class="ltx_tr">
<th id="S3.T4.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.1.12.10.1.1" class="ltx_text">Wav2Vec2 XLS-R Large</span></th>
<th id="S3.T4.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T4.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_t">3.67</td>
<td id="S3.T4.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_t">3.52</td>
<td id="S3.T4.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_t">3.39</td>
<td id="S3.T4.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_t">3.25</td>
<td id="S3.T4.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_t">3.55</td>
<td id="S3.T4.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_t">2.42</td>
</tr>
<tr id="S3.T4.1.1.13.11" class="ltx_tr">
<th id="S3.T4.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T4.1.1.13.11.2" class="ltx_td ltx_align_center">3.65</td>
<td id="S3.T4.1.1.13.11.3" class="ltx_td ltx_align_center">3.62</td>
<td id="S3.T4.1.1.13.11.4" class="ltx_td ltx_align_center">3.59</td>
<td id="S3.T4.1.1.13.11.5" class="ltx_td ltx_align_center">3.62</td>
<td id="S3.T4.1.1.13.11.6" class="ltx_td ltx_align_center"><span id="S3.T4.1.1.13.11.6.1" class="ltx_text ltx_font_bold">3.80</span></td>
<td id="S3.T4.1.1.13.11.7" class="ltx_td ltx_align_center">2.93</td>
</tr>
<tr id="S3.T4.1.1.14.12" class="ltx_tr">
<th id="S3.T4.1.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=1024</th>
<td id="S3.T4.1.1.14.12.2" class="ltx_td ltx_align_center">3.69</td>
<td id="S3.T4.1.1.14.12.3" class="ltx_td ltx_align_center">3.53</td>
<td id="S3.T4.1.1.14.12.4" class="ltx_td ltx_align_center">3.51</td>
<td id="S3.T4.1.1.14.12.5" class="ltx_td ltx_align_center">3.50</td>
<td id="S3.T4.1.1.14.12.6" class="ltx_td ltx_align_center">3.69</td>
<td id="S3.T4.1.1.14.12.7" class="ltx_td ltx_align_center">3.14</td>
</tr>
<tr id="S3.T4.1.1.15.13" class="ltx_tr">
<th id="S3.T4.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.1.15.13.1.1" class="ltx_text">SAMU-XLSR Large</span></th>
<th id="S3.T4.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">k=128</th>
<td id="S3.T4.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_t">3.38</td>
<td id="S3.T4.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_t">3.27</td>
<td id="S3.T4.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_t">2.71</td>
<td id="S3.T4.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_t">2.19</td>
<td id="S3.T4.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_t">1.71</td>
<td id="S3.T4.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_t">1.34</td>
</tr>
<tr id="S3.T4.1.1.16.14" class="ltx_tr">
<th id="S3.T4.1.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">k=512</th>
<td id="S3.T4.1.1.16.14.2" class="ltx_td ltx_align_center">3.28</td>
<td id="S3.T4.1.1.16.14.3" class="ltx_td ltx_align_center">3.06</td>
<td id="S3.T4.1.1.16.14.4" class="ltx_td ltx_align_center">3.19</td>
<td id="S3.T4.1.1.16.14.5" class="ltx_td ltx_align_center">2.75</td>
<td id="S3.T4.1.1.16.14.6" class="ltx_td ltx_align_center">2.12</td>
<td id="S3.T4.1.1.16.14.7" class="ltx_td ltx_align_center">1.25</td>
</tr>
<tr id="S3.T4.1.1.17.15" class="ltx_tr">
<th id="S3.T4.1.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">k=1024</th>
<td id="S3.T4.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb">3.47</td>
<td id="S3.T4.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">3.21</td>
<td id="S3.T4.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_bb">2.80</td>
<td id="S3.T4.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_bb">2.80</td>
<td id="S3.T4.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_bb">2.30</td>
<td id="S3.T4.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_bb">1.57</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Speech to Speech Translation</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.7" class="ltx_p">In the following section, we evaluate a subset of previous configurations on the speech-to-speech translation task.
To ensure comparable results with previous studies, we chose to include the HuBERT Base at layer <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mn id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><cn type="integer" id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">6</annotation></semantics></math> with a cluster size of <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mn id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><cn type="integer" id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">128</annotation></semantics></math> as the baseline.
Additionally, to assess the effect of the number of clusters on the translation task, we include results at layer <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><mn id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><cn type="integer" id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">6</annotation></semantics></math> with cluster sizes of <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><mn id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><cn type="integer" id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">512</annotation></semantics></math> and <math id="S3.SS5.p1.5.m5.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS5.p1.5.m5.1a"><mn id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><cn type="integer" id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">1024</annotation></semantics></math>.
To understand the impact between semantic and acoustic on the S2ST task, we selected both Wav2Vec2 XLS-R and SAMU-XLSR, choosing configurations that maximize performance on the ASR task (layer <math id="S3.SS5.p1.6.m6.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS5.p1.6.m6.1a"><mn id="S3.SS5.p1.6.m6.1.1" xref="S3.SS5.p1.6.m6.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.1b"><cn type="integer" id="S3.SS5.p1.6.m6.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.1c">16</annotation></semantics></math>) and configurations that maximize performance on the speech synthesis task (layer <math id="S3.SS5.p1.7.m7.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS5.p1.7.m7.1a"><mn id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><cn type="integer" id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">8</annotation></semantics></math>). 
<br class="ltx_break"></p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.7" class="ltx_p">From Figure <a href="#S3.F1" title="Figure 1 ‣ 3.5 Speech to Speech Translation ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can observe that BLEU scores improve as the cluster count increases from <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mn id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><cn type="integer" id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">128</annotation></semantics></math> to <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mn id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><cn type="integer" id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">1024</annotation></semantics></math>, with a minor reduction when transitioning from <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mn id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><cn type="integer" id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">128</annotation></semantics></math> to <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><mn id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><cn type="integer" id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">512</annotation></semantics></math> clusters. It is interesting considering that the configuration Hubert Base with layer <math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><mn id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><cn type="integer" id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">6</annotation></semantics></math> and <math id="S3.SS5.p2.6.m6.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS5.p2.6.m6.1a"><mn id="S3.SS5.p2.6.m6.1.1" xref="S3.SS5.p2.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.6.m6.1b"><cn type="integer" id="S3.SS5.p2.6.m6.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.6.m6.1c">512</annotation></semantics></math> clusters achieved the highest Mean Opinion Score (MOS).
This pattern indicates that a higher number of clusters generally enhances model performance.
The most effective configuration tested is the Hubert Base model with 1024 clusters, which achieved the highest BLEU score of <math id="S3.SS5.p2.7.m7.1" class="ltx_Math" alttext="20.14" display="inline"><semantics id="S3.SS5.p2.7.m7.1a"><mn id="S3.SS5.p2.7.m7.1.1" xref="S3.SS5.p2.7.m7.1.1.cmml">20.14</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.7.m7.1b"><cn type="float" id="S3.SS5.p2.7.m7.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1">20.14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.7.m7.1c">20.14</annotation></semantics></math>. 
<br class="ltx_break"></p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.18332/assets/cluster_plot.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Effect of number of clusters BLEU scores. We adopt the baseline configuration, HuBERT Base at layer <math id="S3.F1.2.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.F1.2.m1.1b"><mn id="S3.F1.2.m1.1.1" xref="S3.F1.2.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.F1.2.m1.1c"><cn type="integer" id="S3.F1.2.m1.1.1.cmml" xref="S3.F1.2.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.2.m1.1d">6</annotation></semantics></math> with (k=128, 512, 1024)</figcaption>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Looking at the BLEU scores presented in Table <a href="#S3.T5" title="Table 5 ‣ 3.5 Speech to Speech Translation ‣ 3 Results and Discussion ‣ Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the Wav2Vec2 XLS-R Large model consistently outperforms the SAMU-XLSR Large model across both layer configurations.
This suggests that the fine-tuning of SAMU-XLSR Large on text embeddings may not effectively contribute to speech-to-speech translation tasks.
Furthermore, the results on Wav2Vec2 XLS-R Large underscore the complexity in selecting the optimal layer for extracting discrete speech units. Relying solely on the Mean Opinion Score (MOS) for token selection does not appear to yield the best results, compared to adopting a balanced approach that considers both the Character Error Rate (CER) and MOS scores. 
<br class="ltx_break"></p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>This table compares the BLEU scores of Wav2Vec2 XLS-R Large and SAMU-XLSR Large models under two distinct configurations: one optimizing for CER performance (layer 8) and the other for maximizing MOS scores (layer 16), both using 1024 clusters.</figcaption>
<div id="S3.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:63.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.8pt,13.2pt) scale(0.707564908976248,0.707564908976248) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">SSL Model</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Layer</th>
<th id="S3.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Number Of Clusters</th>
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BLEU <math id="S3.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T5.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.m1.1b"><ci id="S3.T5.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.2.1" class="ltx_tr">
<td id="S3.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Wav2Vec2 XLS-R Large</td>
<td id="S3.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S3.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1024</td>
<td id="S3.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">17.55</td>
</tr>
<tr id="S3.T5.1.1.3.2" class="ltx_tr">
<td id="S3.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Wav2Vec2 XLS-R Large</td>
<td id="S3.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
<td id="S3.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1024</td>
<td id="S3.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">16.93</td>
</tr>
<tr id="S3.T5.1.1.4.3" class="ltx_tr">
<td id="S3.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SAMU-XLSR Large</td>
<td id="S3.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S3.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1024</td>
<td id="S3.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">16.9</td>
</tr>
<tr id="S3.T5.1.1.5.4" class="ltx_tr">
<td id="S3.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">SAMU-XLSR Large</td>
<td id="S3.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">16</td>
<td id="S3.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">1024</td>
<td id="S3.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13.29</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we have described various experiments to evaluate the robustness of discrete speech units in downstream tasks.
The results obtained in textless speech-to-speech translation underscore the complexity in selecting encoders and the number of clusters.
We hope this analysis will help the community in better understanding the extraction of discrete tokens.
Looking forward, future research will explore the combination of multiple encoders and layers within the same task and extend the approach to additional language pairs, particularly those that are unwritten.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgements</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work received funding from the European SELMA project (grant N°957017).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Lavie, A. Waibel, L. Levin, M. Finke, D. Gates, M. Gavalda, T. Zeppenfeld,
and Z. Puming, ``Janus-iii: speech-to-speech translation in multiple
languages,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">1997 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP)</em>, vol. 1, 1997, pp. 99–102.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Nakamura, K. Markov, H. Nakaiwa, G. Kikui, H. Kawai, T. Jitsuhiro, J.-S.
Zhang, H. Yamamoto, E. Sumita, and S. Yamamoto, ``The atr multilingual
speech-to-speech translation system,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio,
Speech, and Language Processing</em>, vol. 14, no. 2, pp. 365–376, 2006.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Lee, P. Chen, C. Wang, J. Gu, S. Popuri, X. Ma, A. Polyak, Y. Adi, Q. He,
Y. Tang, J. Pino, and W. Hsu, ``Direct speech-to-speech translation with
discrete units,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics</em>, 2022, pp. 3327–3339.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Duret, B. O’Brien, Y. Estève <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Enhancing expressivity
transfer in textless speech-to-speech translation,'' in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">2023 IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Hsu, B. Bolte, Y. Tsai <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Hubert: Self-supervised speech
representation learning by masked prediction of hidden units,''
<em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>,
vol. 29, p. 3451–3460, 2021. [Online]. Available:
<a target="_blank" href="http://dx.doi.org/10.1109/TASLP.2021.3122291" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/TASLP.2021.3122291</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Yang, P. Chi, Y. Chuang <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Superb: Speech processing universal
performance benchmark,'' <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Post, G. Kumar, A. Lopez <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Fisher and callhome
spanish–english speech translation,'' <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">LDC2014T23. Web Download.
Philadelphia: Linguistic Data Consortium</em>, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. Wang, A. Wu, J. Gu, and J. Pino, ``Covost 2 and massively multilingual
speech translation.'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2021, pp. 2247–2251.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Jia, M. Ramanovich, Q. Wang, and H. Zen, ``Cvss corpus and massively
multilingual speech-to-speech translation,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2201.03713</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
P. Duquenne, H. Gong, N. Dong <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Speechmatrix: A large-scale
mined corpus of multilingual speech-to-speech translations,'' <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2211.04508</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, ``Unsupervised
cross-lingual representation learning for speech recognition,'' 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Khurana, A. Laurent, and J. Glass, ``Samu-xlsr: Semantically-aligned
multimodal utterance-level cross-lingual speech representation,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE
Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, p.
1493–1504, 2022. [Online]. Available:
<a target="_blank" href="http://dx.doi.org/10.1109/JSTSP.2022.3192714" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/JSTSP.2022.3192714</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Babu, C. Wang, A. Tjandra <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Xls-r: Self-supervised
cross-lingual speech representation learning at scale,'' 2021. [Online].
Available: <a target="_blank" href="https://arxiv.org/abs/2111.09296" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2111.09296</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang, ``Language-agnostic bert
sentence embedding,'' 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Oord, Y. Li, and O. Vinyals, ``Representation learning with contrastive
predictive coding,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.03748</em>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. Zhou, B. Sisman, R. Liu, and H. Li, ``Emotional voice conversion: Theory,
databases and esd,'' 2021. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2105.14762" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2105.14762</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ``Librispeech: an asr corpus
based on public domain audio books,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international
conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Nagrani, J. Chung, and A. Zisserman, ``Voxceleb: a large-scale speaker
identification dataset,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.08612</em>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
I. Keith and J. Linda, ``The lj speech dataset,''
<a target="_blank" href="https://keithito.com/LJ-Speech-Dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Saeki, D. Xin, W. Nakata, T. Koriyama <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Utmos: Utokyo-sarulab
system for voicemos challenge 2022,'' <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02152</em>,
2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Zaiem, Y. Kemiche, T. Parcollet <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Speech self-supervised
representation benchmarking: Are we doing it right?'' <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2306.00452</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W. N. Hsu, A. Mohamed,
and E. Dupoux, ``Speech resynthesis from discrete disentangled
self-supervised representations,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>.   ISCA, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae, ``Hifi-gan: Generative adversarial networks for
efficient and high fidelity speech synthesis,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 33.   Curran Associates, Inc., 2020, pp. 17 022–17 033. [Online].
Available:
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Popuri, P. Chen, C. Wang <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Enhanced direct speech-to-speech
translation using self-supervised pre-training and data augmentation,''
<em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02967</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Pasad, J.-C. Chou, and K. Livescu, ``Layer-wise analysis of a
self-supervised speech representation model,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2021, pp. 914–921.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Pasad, B. Shi, and K. Livescu, ``Comparative layer-wise analysis of
self-supervised speech models,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.18330" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.18332" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.18332">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.18332" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.18333" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:39:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
