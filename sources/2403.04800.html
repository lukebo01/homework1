<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04800] (Un)paired signal-to-signal translation with 1D conditional GANs</title><meta property="og:description" content="I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal (sig2sig) translation. Using a simplified CycleGAN ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(Un)paired signal-to-signal translation with 1D conditional GANs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="(Un)paired signal-to-signal translation with 1D conditional GANs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04800">

<!--Generated on Fri Apr  5 15:32:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %% No date .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">(Un)paired signal-to-signal translation with 1D conditional GANs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eric Easthope 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">University of British Columbia 
<br class="ltx_break">Department of Electrical and Computer Engineering 
<br class="ltx_break">Vancouver, British Columbia, Canada
</span>
</span></span>
</div>

<div class="ltx_abstract">
<p id="id2.id1" class="ltx_p">I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal (<span id="id2.id1.1" class="ltx_text ltx_font_typewriter">sig2sig</span>) translation. Using a simplified CycleGAN model with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-to-image translation task to a 1D signal-to-signal translation task with deep convolutional GANs is possible without substantial modification to the conventional U-Net model and adversarial architecture developed as CycleGAN. With this I show for a small tunable dataset that noisy test signals unseen by the 1D CycleGAN model and without paired training transform from the source domain to signals similar to paired test signals in the translated domain, especially in terms of frequency, and I quantify these differences in terms of correlation and error.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_runin ltx_font_bold ltx_title_subsection">1.1.¬†¬†¬†Background.</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The past few years have seen a significant rise in research and public interest in the use of generative machine learning and artificial intelligence (ML/AI) models for image-to-image translation tasks.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Perhaps one of the more recognizable models is <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">pix2pix</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a deep generative model (DGM) and particularly a deep convolutional generative adversarial network (DCGAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> that is capable of translating between pairs of high-resolution images within a learned image data domain. The novelty of pix2pix laid in its model architecture which combined a deep U-Net generator that learns to generate mock data samples with a convolutional PatchGAN discriminator that learns to label regions, ‚Äúpatches,‚Äù of inputs as ‚Äúreal‚Äù (<span id="S1.SS1.p2.1.2" class="ltx_text ltx_font_italic">sampled</span> data) or ‚Äúfake‚Äù (<span id="S1.SS1.p2.1.3" class="ltx_text ltx_font_italic">generated</span> data). Much of the research interest in pix2pix has centred on image translation tasks but the inherent structure of the U-Net model does not limit it to images alone. In fact original developments of U-Net were for semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Research into GANs as they stand within the wider DGM and even wider generative ML/AI ecosystem have not been limited to images either. Parallel work on one-dimensional (1D) GANs where time series training data is periodic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has observed that derived models that decompose demonstrated two-dimensional models into 1D counterparts with a wider learning aperture, which we set ourselves with the size of convolution kernels, are capable of generating convincing high-accuracy 1D time series (including audio) from a learned signal data domain. Wider convolutional apertures are necessary for models to see and learn the time series periodicity. Others before have taken the conceptual essence of signal-to-signal translation and adapted its generator U-Net models for other signal domains; spectrum translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (spectral/frequency series-to-series), sensor translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (time series-to-series, 2D), and sound translation [<a target="_blank" href="https://maxgraf.space/" title="" class="ltx_ref ltx_href">9</a>] (time series-to-series, 1D) to name a few. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> focused on the translation of sensor-to-sensor to adapt sensor conditions for object/material detection. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> focused on the re-use of sensor models, translating old sensor measurements to new ones; they used a modified 2D pix2pix-like architecture to do this. [<a target="_blank" href="https://maxgraf.space/" title="" class="ltx_ref ltx_href">9</a>] focused on timbre-to-timbre translation in (optionally musical) audio signals, accomplishing this in 1D but again through what was actually a spectral-to-spectral translation like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">All of these share the idea of using machine learning to translate between types of windowed signals whether expressed in one or two-dimensional bases. The takeover of research interest in generic sequence-to-sequence and equivalently series-to-series translation models and their continued capacity to surpass many of the performance metrics that were previously maximized by more specialized convolutional neural networks and the like suggest that some model generality is possible. WaveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> showed that a general treatment of 2D convolutional models could make 1D convolutional counterparts suitable to generate audio with only a change of dimension and data shape. Still, where this generality starts and stops is unknown and to my knowledge little has been done to establish the generality of these models across 1D/2D signal paradigms.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_runin ltx_font_bold ltx_title_subsection">1.2.¬†¬†¬†Objective.</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">This work aims to establish by existence proof that the practice of transcribing convolutional models between 2D to 1D and possibly other dimensional configurations has sufficient generality to solve several signal-to-signal translation problems. To this end I focus first on the 1D unpaired signal-to-signal translation task.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods &amp; Materials</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_runin ltx_font_bold ltx_title_subsection">2.1.¬†¬†¬†Model.</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the interests of model re-usability I use a model that is readily available online: a modified CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> architecture that combines simplified versions of the generator and discriminator models (using only three U-Net down/upsampling layers) from the pix2pix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> architecture replacing 2D layers with 1D layers and widening convolutional kernels to roughly the square of their size. Notably CycleGAN uses the same generator and discriminator as pix2pix, differing in function only by training on paired/unpaired data and using a different training loss; CycleGAN combines two pix2pix models to learn signal translations to <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">and</span> from both signal translation domains by minimizing ‚Äúroundtrip‚Äù error.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.3" class="ltx_p">This model was trained over the course of a few minutes using the standard CycleGAN losses, <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\lambda=10" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">Œª</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><eq id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></eq><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ùúÜ</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\lambda=10</annotation></semantics></math>, and <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\beta_{1}=0.5" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><msub id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2.2" xref="S2.SS1.p2.2.m2.1.1.2.2.cmml">Œ≤</mi><mn id="S2.SS1.p2.2.m2.1.1.2.3" xref="S2.SS1.p2.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><eq id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></eq><apply id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2.2">ùõΩ</ci><cn type="integer" id="S2.SS1.p2.2.m2.1.1.2.3.cmml" xref="S2.SS1.p2.2.m2.1.1.2.3">1</cn></apply><cn type="float" id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\beta_{1}=0.5</annotation></semantics></math> for 100 epochs with a batch size of one using TensorFlow on an M1 MacBook Pro. Signals and their translated counterparts were scored using Pearson product-moment correlation (<math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">r</annotation></semantics></math>-value) and mean absolute error (MAE).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_runin ltx_font_bold ltx_title_subsection">2.2.¬†¬†¬†Dataset.</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">A mock dataset simulates randomly windowed samples from a bandlimited signal where we can vary the window length, bandwidth, and maximum phase offset to study the capacities of the simplified 1D CycleGAN model.
I use a small collection of sixteen tunably (a)synchronous periodic bandlimited signals sampled reproducibly with a reusable randomness seed; each signal is made up of multiple sine waves with seed-generated random amplitudes, phase offsets (up to <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="2\pi" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">œÄ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">2</cn><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ùúã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">2\pi</annotation></semantics></math> radians), and frequency offsets. The (a)synchronicity of paired/unpaired signals is controllable through the extent of phase offsets. A sample of dataset elements is shown below.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><a name="fig..1" id="fig..1" class="ltx_anchor ltx_centering"><img src="/html/2403.04800/assets/figure-1.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="1335" height="735" alt="Refer to caption"></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sixteen randomly sampled dataset elements used for 1D CycleGAN training between one-dimensional signal domains (blue/purple lines) with arbitrary scales.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<figure id="S3.F2" class="ltx_figure"><a name="fig..2" id="fig..2" class="ltx_anchor ltx_centering"><img src="/html/2403.04800/assets/figure-2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="1335" height="435" alt="Refer to caption"></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Paired signal-to-signal translations by the simplified 1D CycleGAN architecture against a small unseen four-element test dataset in the <span id="S3.F2.4.1" class="ltx_text ltx_font_italic">time</span> domain (<math id="S3.F2.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.F2.2.m1.1b"><mi id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">r</annotation></semantics></math>-values in discussion).</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><a name="fig..3" id="fig..3" class="ltx_anchor ltx_centering"><img src="/html/2403.04800/assets/figure-3.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="1335" height="435" alt="Refer to caption"></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Paired signal-to-signal translations by the simplified 1D CycleGAN architecture against a small unseen four-element test dataset in the <span id="S3.F3.4.1" class="ltx_text ltx_font_italic">frequency</span> domain after discrete Fast Fourier Transform. Frequency-wise signals seem to match more than time-wise signals above (<math id="S3.F3.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.F3.2.m1.1b"><mi id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><ci id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">r</annotation></semantics></math>-values in discussion).</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><a name="fig..4" id="fig..4" class="ltx_anchor ltx_centering"><img src="/html/2403.04800/assets/figure-4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="1335" height="735" alt="Refer to caption"></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Signal-to-signal translations (supplementary anonymized dataset) by the simplified CycleGAN architecture, which combines elements of the pix2pix architecture with a different training pattern and loss for unpaired signal data. Differences in reconstruction are shown at both signal <math id="S3.F4.3.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.F4.3.m1.1b"><mi id="S3.F4.3.m1.1.1" xref="S3.F4.3.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.F4.3.m1.1c"><ci id="S3.F4.3.m1.1.1.cmml" xref="S3.F4.3.m1.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.3.m1.1d">X</annotation></semantics></math> and signal <math id="S3.F4.4.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.F4.4.m2.1b"><mi id="S3.F4.4.m2.1.1" xref="S3.F4.4.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.F4.4.m2.1c"><ci id="S3.F4.4.m2.1.1.cmml" xref="S3.F4.4.m2.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m2.1d">Y</annotation></semantics></math> translation domains (compared as solid/transparent lines).</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.8" class="ltx_p">The simplified 1D CycleGAN model performs reasonably well with the synthetic dataset despite relatively little training (less than 10 minutes); for all four test samples (0.21 <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><lt id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">&lt;</annotation></semantics></math> r <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><lt id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">&lt;</annotation></semantics></math> 0.46 and (0.032 <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.3.m3.1a"><mo id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><lt id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">&lt;</annotation></semantics></math> MAE <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.4.m4.1a"><mo id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><lt id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">&lt;</annotation></semantics></math> 0.037) in the time domain and (0.71 <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.5.m5.1a"><mo id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><lt id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">&lt;</annotation></semantics></math> r <math id="S4.p1.6.m6.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.6.m6.1a"><mo id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><lt id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">&lt;</annotation></semantics></math> 0.89) and (0.13 <math id="S4.p1.7.m7.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.7.m7.1a"><mo id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><lt id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">&lt;</annotation></semantics></math> MAE <math id="S4.p1.8.m8.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.p1.8.m8.1a"><mo id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><lt id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">&lt;</annotation></semantics></math> 0.15) in the frequency domain. Yet while the 1D CycleGAN model is trained on unpaired data it seems to be capable of accurately transforming between <span id="S4.p1.8.1" class="ltx_text ltx_font_italic">paired</span> signals never seen in training while also conserving much of their frequency content. This suggests interesting potential for CycleGAN and similar convolutional architectures to learn translations between (a)synchronous 1D signals.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Further testing with a larger synthetic data schema and several performance measures is needed to fully validate CycleGAN as a viable option for 1D signal-to-signal translation in more practical and wild data contexts.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">I showed that a common generative architecture for 2D image-to-image translation model could be transformed into a 1D model by applying some of the same concepts as used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to transform the 2D image DCGAN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> into a 1D audio model. I showed that CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which applies the same principles as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> but with unpaired training data, performs with a small mock dataset of tunable periodic signals even without paired training suggesting some potential for CycleGAN and similar convolutional architectures to learn translations between (a)synchronous 1D signals.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Chris Donahue, Julian McAuley, and Miller Puckette.

</span>
<span class="ltx_bibblock">Adversarial Audio Synthesis, February 2019.

</span>
<span class="ltx_bibblock">arXiv:1802.04208 [cs].

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ian¬†J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks, June 2014.

</span>
<span class="ltx_bibblock">arXiv:1406.2661 [cs, stat].

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei¬†A. Efros.

</span>
<span class="ltx_bibblock">Image-to-Image Translation with Conditional Adversarial Networks, November 2018.

</span>
<span class="ltx_bibblock">arXiv:1611.07004 [cs].

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
SangYeon Kim, Hyunwoo Lee, Jonghee Han, and Joon-Ho Kim.

</span>
<span class="ltx_bibblock">Sig2Sig: Signal Translation Networks to Take the Remains of the Past.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 3620‚Äì3624, June 2021.

</span>
<span class="ltx_bibblock">ISSN: 2379-190X.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully Convolutional Networks for Semantic Segmentation, March 2015.

</span>
<span class="ltx_bibblock">arXiv:1411.4038 [cs].

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Cara¬†P. Murphy and John Kerekes.

</span>
<span class="ltx_bibblock">1D conditional generative adversarial network for spectrum-to-spectrum translation of simulated chemical reflectance signatures.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Journal of Spectral Imaging</span>, 10, June 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Alec Radford, Luke Metz, and Soumith Chintala.

</span>
<span class="ltx_bibblock">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, January 2016.

</span>
<span class="ltx_bibblock">arXiv:1511.06434 [cs].

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei¬†A. Efros.

</span>
<span class="ltx_bibblock">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, August 2020.

</span>
<span class="ltx_bibblock">arXiv:1703.10593 [cs].

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.04798" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.04800" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.04800">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04800" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.04801" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:32:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
