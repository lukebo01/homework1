<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.06274] Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time</title><meta property="og:description" content="Spectral subtraction, widely used for its simplicity, has been employed to address the Robot Ego Speech Filtering (RESF) problem for detecting speech contents of human interruption from robotâ€™s single-channel microphonâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.06274">

<!--Generated on Sat Oct  5 18:12:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SpectralÂ Oversubtraction?Â AnÂ ApproachÂ forÂ SpeechÂ EnhancementÂ afterÂ RobotÂ EgoÂ SpeechÂ Filtering in Semi-Real-Time
</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Spectral subtraction, widely used for its simplicity, has been employed to address the Robot Ego Speech Filtering (RESF) problem for detecting speech contents of human interruption from robotâ€™s single-channel microphone recordings when it is speaking. However, this approach suffers from oversubtraction in the fundamental frequency range (FFR), leading to degraded speech content recognition.
To address this, we propose a Two-Mask Conformer-based Metric Generative Adversarial NetworkÂ (CMGAN) to enhance the detected speech and improve recognition results. Our model compensates for oversubtracted FFR values with high-frequency information and long-term features and then de-noises the new spectrogram. In addition, we introduce an incremental processing method that allows semi-real-time audio processing with streaming input on a network trained on long fixed-length input. Evaluations of two datasets, including one with unseen noise, demonstrate significant improvements in recognition accuracy and the effectiveness of the proposed two-mask approach and incremental processing, enhancing the robustness of the proposed RESF pipeline in real-world HRI scenarios.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Spectral subtractionÂ (SS) is one of the most prevalent signal processing methods for speech enhancementÂ (SE), comprising the subtraction of the estimated noise spectrum from the recorded signal spectrumÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Due to its simplicity and effectiveness, SS has been widely adopted in various practical applications, such as mobile communication, hearing aids, and speech recognition systemsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In this study, we focus on the case of robot ego speech filteringÂ (RESF) during human-robot interactionÂ (HRI) based on a well-known social robot, which is most often used in HRI, PepperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. We intend to improve the performance in detecting and recognizing speech contents when a human interrupts robot speech from its embedded single-channel recordings. Such a capability, when done right, can make HRI much more fluent, as the human can barge in as it would do in human-human interactions and would not have to wait until the robot starts listening with the microphone after finishing its utterance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite its widespread use, SS has notable drawbacks, in particular spectral oversubtraction caused by excessive noise spectrum estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In dynamic and non-stationary noise environments, accurately estimating the noise spectrum is inherently challenging, leading to distorted enhanced speechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The primary issue is oversubtraction in the fundamental frequency rangeÂ (FFR) when spectrally subtracting Pepper ego speech from its single-channel recordings Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, further leading to misrecognition of words with nasal or plosive sounds by state-of-the-artÂ (SOTA) automatic speech recognition (ASR) systemsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. To address this, researchers have developed adaptive noise estimation filters and spectral floor adjustmentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Kamath and LoizouÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed a multi-band non-linear spectral subtraction method,
using a frequency-dependent subtraction factor to account for different types of noise. Such methods perform well in stationary noise environments or when the signal-to-noise ratioÂ (SNR) is not less than 10Â dBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, they struggle in non-stationary noise conditions where the SNR is significantly lower, a condition that is quite common to HRI where constraints of humanoid robot design can cause SNR values as low as -20Â dBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To correctly interpret human interruptions during HRI, an SE method is needed that improves the intelligibility and the recognition result of detected speech. Through sophisticated machine learning algorithms,
SE systems have advanced significantly, manifested in approaches such as DCCRNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and FullSubNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Among these advanced networks, generative adversarial networksÂ (GAN) come with the advantages of speed - its generator can learn to produce accurate representations in a short time - and conservation - its discriminator maintains most of the information from the original data distributionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
In 2017, Pascual et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed SEGAN, one of the initial efforts to apply GAN to speech enhancement. A new avenue for GAN-based SE research was then initiated by Fu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> who proposed MetricGAN to directly optimize the evaluation metrics for this task. Based on these advancements, Cao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed a Conformer-based MetricGANÂ (CMGAN), achieving the highest Perceptual Evaluation of Speech QualityÂ (PESQ) score in the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">VoiceBank+DEMAND</span> datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as of 2022. Despite these advancements, the ASR results in enhanced speech have declined rather than improvedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For example, Donahue et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed FSEGAN and jointly trained it with an ASR system, finding that the usefulness of GANs was limited compared to replacement of the training object.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To the best of our knowledge, no SE system specifically targets recovery from spectral oversubtraction distortion in the FFR and performs on recorded streaming audio buffers to achieve semi-real-time processing. Neither is there a dataset consisting of speech that is distorted by spectral oversubtraction. Previous work suggests that values in the HFR can be learned to restore the oversubtracted values in the FFR and improve the vulnerability of detected speech to noise.
Furthermore, the success of MetricGAN, which was outside the scope of the study by Donahue et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, shows the potential of GAN-based SE systems to improve the ASR result after SE.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The aim of this work is two-fold: 1. Develop and verify a MetricGAN-based network that can learn from the information in HFR to restore the oversubtracted values in FFR and to improve the ASR results; 2. Improve the robustness of the RESF system that is vulnerable to common ambient noise in real world HRIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We propose a Two-Mask CMGAN inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
We examine the impact of different inputs for the discriminator network on the ASR results and propose a method that allows a network trained on fixed-length audio segments to process short audio buffers while accessing long-term information. Evaluation on offline datasets with and without additional noise demonstrates the effectiveness and robustness of the proposed pipeline.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Two-Mask CMGAN</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Given that the spectrum of human speech is short-time invariant and the restored information in the higher frequency range is the formant of the oversubtracted information in FFRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, we propose a variant of CMGAN: Two-Mask CMGAN to improve the recognition result of detected human interruption speech. The overview of the proposed two-mask CMGAN is shown in Fig.Â <a href="#S2.F1" title="Figure 1 â€£ 2.1 Two-Mask CMGAN â€£ 2 Methods â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.06274/assets/figs/pipeline_for_se.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="280" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Comparison of the original CMGAN and proposed Two-Mask CMGAN. Components marked in red are proposed in this work and absent in the original CMGAN.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.6" class="ltx_p">The generator contains three parts: <span id="S2.SS1.p2.6.1" class="ltx_text ltx_font_italic">Encoder</span>, <span id="S2.SS1.p2.6.2" class="ltx_text ltx_font_italic">Conformer</span>, and <span id="S2.SS1.p2.6.3" class="ltx_text ltx_font_italic">Decoder</span>. To verify the hypothesis that oversubtracted values in FFR can be restored based on HFR information, we propose to split the original one ideal ratio maskÂ (<span id="S2.SS1.p2.6.4" class="ltx_text ltx_font_italic">IRM</span>) generation procedure into two subprocedures, explained in Eq.Â <a href="#S2.E1" title="In 2.1 Two-Mask CMGAN â€£ 2 Methods â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\hat{S}(tf)=\left(Y(tf)+IRM_{1}\right)\cdot IRM_{2}" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mover accent="true" id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.3.2.cmml">S</mi><mo id="S2.E1.m1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">f</mi></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml">Y</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml">f</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml">+</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.1.1.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.3.1" xref="S2.E1.m1.2.2.2.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.3.1a" xref="S2.E1.m1.2.2.2.1.1.1.1.3.1.cmml">â€‹</mo><msub id="S2.E1.m1.2.2.2.1.1.1.1.3.4" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.3.4.2" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4.2.cmml">M</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.3.4.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4.3.cmml">1</mn></msub></mrow></mrow><mo rspace="0.055em" id="S2.E1.m1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">â‹…</mo><mi id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml">I</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2a" xref="S2.E1.m1.2.2.2.2.cmml">â€‹</mo><msub id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.4.cmml"><mi id="S2.E1.m1.2.2.2.4.2" xref="S2.E1.m1.2.2.2.4.2.cmml">M</mi><mn id="S2.E1.m1.2.2.2.4.3" xref="S2.E1.m1.2.2.2.4.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3"><ci id="S2.E1.m1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.3.1">^</ci><ci id="S2.E1.m1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.3.2">ğ‘†</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">ğ‘“</ci></apply></apply><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><times id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2"></times><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><ci id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2">â‹…</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><plus id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"></plus><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3">ğ‘Œ</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3">ğ‘“</ci></apply></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3"><times id="S2.E1.m1.2.2.2.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.1"></times><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.2">ğ¼</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.3">ğ‘…</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.3.4.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.3.4.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.4.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4.2">ğ‘€</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.3.4.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3.4.3">1</cn></apply></apply></apply><ci id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3">ğ¼</ci></apply><ci id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3">ğ‘…</ci><apply id="S2.E1.m1.2.2.2.4.cmml" xref="S2.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.4.1.cmml" xref="S2.E1.m1.2.2.2.4">subscript</csymbol><ci id="S2.E1.m1.2.2.2.4.2.cmml" xref="S2.E1.m1.2.2.2.4.2">ğ‘€</ci><cn type="integer" id="S2.E1.m1.2.2.2.4.3.cmml" xref="S2.E1.m1.2.2.2.4.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\hat{S}(tf)=\left(Y(tf)+IRM_{1}\right)\cdot IRM_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.5" class="ltx_p">where <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="IRM_{1}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.1a" xref="S2.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><msub id="S2.SS1.p2.1.m1.1.1.4" xref="S2.SS1.p2.1.m1.1.1.4.cmml"><mi id="S2.SS1.p2.1.m1.1.1.4.2" xref="S2.SS1.p2.1.m1.1.1.4.2.cmml">M</mi><mn id="S2.SS1.p2.1.m1.1.1.4.3" xref="S2.SS1.p2.1.m1.1.1.4.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><times id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></times><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ¼</ci><ci id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">ğ‘…</ci><apply id="S2.SS1.p2.1.m1.1.1.4.cmml" xref="S2.SS1.p2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.4.1.cmml" xref="S2.SS1.p2.1.m1.1.1.4">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.4.2.cmml" xref="S2.SS1.p2.1.m1.1.1.4.2">ğ‘€</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.4.3.cmml" xref="S2.SS1.p2.1.m1.1.1.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">IRM_{1}</annotation></semantics></math> is the compensation ratio mask in the FFR generated according to the information in the HFR, and <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="IRM_{2}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.1.1a" xref="S2.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><msub id="S2.SS1.p2.2.m2.1.1.4" xref="S2.SS1.p2.2.m2.1.1.4.cmml"><mi id="S2.SS1.p2.2.m2.1.1.4.2" xref="S2.SS1.p2.2.m2.1.1.4.2.cmml">M</mi><mn id="S2.SS1.p2.2.m2.1.1.4.3" xref="S2.SS1.p2.2.m2.1.1.4.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><times id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></times><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">ğ¼</ci><ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">ğ‘…</ci><apply id="S2.SS1.p2.2.m2.1.1.4.cmml" xref="S2.SS1.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.4.1.cmml" xref="S2.SS1.p2.2.m2.1.1.4">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.4.2.cmml" xref="S2.SS1.p2.2.m2.1.1.4.2">ğ‘€</ci><cn type="integer" id="S2.SS1.p2.2.m2.1.1.4.3.cmml" xref="S2.SS1.p2.2.m2.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">IRM_{2}</annotation></semantics></math> is the denoising ratio mask generated according to the compensated information in the whole frequency range. Because oversubtraction can reduce some values to <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mn id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><cn type="integer" id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">0</cn></annotation-xml></semantics></math>, which cannot be restored by multiplying the ratio mask, we choose to first generate a compensation ratio mask and add it to the noisy input. Instead of using the <span id="S2.SS1.p2.5.1" class="ltx_text ltx_font_italic">Complex Decoder</span> as Cao et al. in the original CMGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we adopt Eq.Â <a href="#S2.E2" title="In 2.1 Two-Mask CMGAN â€£ 2 Methods â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to update the values in the real and imaginary spectrograms, <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="Y_{r}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><msub id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">Y</mi><mi id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">ğ‘Œ</ci><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">Y_{r}</annotation></semantics></math> and <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="Y_{i}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><msub id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.2" xref="S2.SS1.p2.5.m5.1.1.2.cmml">Y</mi><mi id="S2.SS1.p2.5.m5.1.1.3" xref="S2.SS1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.2">ğ‘Œ</ci><ci id="S2.SS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.p2.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">Y_{i}</annotation></semantics></math>, to reduce the complexity of the network.</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\begin{cases}Y^{\prime}_{r}=Y^{\prime}_{m}\cos{Y_{p}}\\
Y^{\prime}_{i}=Y^{\prime}_{m}\sin{Y_{p}}\end{cases}" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2" xref="S2.E2.m1.2.3.1.cmml"><mo id="S2.E2.m1.2.2.3" xref="S2.E2.m1.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.3.1.cmml"><mtr id="S2.E2.m1.2.2.2a" xref="S2.E2.m1.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E2.m1.2.2.2b" xref="S2.E2.m1.2.3.1.cmml"><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><msubsup id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.2.cmml">Y</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.cmml">r</mi><mo id="S2.E2.m1.1.1.1.1.1.1.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.2.3.cmml">â€²</mo></msubsup><mo id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml"><msubsup id="S2.E2.m1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.3.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.3.2.2.2.cmml">Y</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.1.1.3.2.3.cmml">m</mi><mo id="S2.E2.m1.1.1.1.1.1.1.3.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.3.2.2.3.cmml">â€²</mo></msubsup><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.3.3.1" xref="S2.E2.m1.1.1.1.1.1.1.3.3.1.cmml">cos</mi><mo lspace="0.167em" id="S2.E2.m1.1.1.1.1.1.1.3.3a" xref="S2.E2.m1.1.1.1.1.1.1.3.3.cmml">â¡</mo><msub id="S2.E2.m1.1.1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.3.3.2.2" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2.2.cmml">Y</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3.3.2.3" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2.3.cmml">p</mi></msub></mrow></mrow></mrow></mtd><mtd id="S2.E2.m1.2.2.2c" xref="S2.E2.m1.2.3.1.1.cmml"></mtd></mtr><mtr id="S2.E2.m1.2.2.2d" xref="S2.E2.m1.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E2.m1.2.2.2e" xref="S2.E2.m1.2.3.1.cmml"><mrow id="S2.E2.m1.2.2.2.2.1.1" xref="S2.E2.m1.2.2.2.2.1.1.cmml"><msubsup id="S2.E2.m1.2.2.2.2.1.1.2" xref="S2.E2.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E2.m1.2.2.2.2.1.1.2.2.2" xref="S2.E2.m1.2.2.2.2.1.1.2.2.2.cmml">Y</mi><mi id="S2.E2.m1.2.2.2.2.1.1.2.3" xref="S2.E2.m1.2.2.2.2.1.1.2.3.cmml">i</mi><mo id="S2.E2.m1.2.2.2.2.1.1.2.2.3" xref="S2.E2.m1.2.2.2.2.1.1.2.2.3.cmml">â€²</mo></msubsup><mo id="S2.E2.m1.2.2.2.2.1.1.1" xref="S2.E2.m1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.2.2.2.2.1.1.3" xref="S2.E2.m1.2.2.2.2.1.1.3.cmml"><msubsup id="S2.E2.m1.2.2.2.2.1.1.3.2" xref="S2.E2.m1.2.2.2.2.1.1.3.2.cmml"><mi id="S2.E2.m1.2.2.2.2.1.1.3.2.2.2" xref="S2.E2.m1.2.2.2.2.1.1.3.2.2.2.cmml">Y</mi><mi id="S2.E2.m1.2.2.2.2.1.1.3.2.3" xref="S2.E2.m1.2.2.2.2.1.1.3.2.3.cmml">m</mi><mo id="S2.E2.m1.2.2.2.2.1.1.3.2.2.3" xref="S2.E2.m1.2.2.2.2.1.1.3.2.2.3.cmml">â€²</mo></msubsup><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.2.2.2.2.1.1.3.1" xref="S2.E2.m1.2.2.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.2.2.1.1.3.3" xref="S2.E2.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S2.E2.m1.2.2.2.2.1.1.3.3.1" xref="S2.E2.m1.2.2.2.2.1.1.3.3.1.cmml">sin</mi><mo lspace="0.167em" id="S2.E2.m1.2.2.2.2.1.1.3.3a" xref="S2.E2.m1.2.2.2.2.1.1.3.3.cmml">â¡</mo><msub id="S2.E2.m1.2.2.2.2.1.1.3.3.2" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2.cmml"><mi id="S2.E2.m1.2.2.2.2.1.1.3.3.2.2" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2.2.cmml">Y</mi><mi id="S2.E2.m1.2.2.2.2.1.1.3.3.2.3" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2.3.cmml">p</mi></msub></mrow></mrow></mrow></mtd><mtd id="S2.E2.m1.2.2.2f" xref="S2.E2.m1.2.3.1.1.cmml"></mtd></mtr></mtable></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.3.1.cmml" xref="S2.E2.m1.2.2"><csymbol cd="latexml" id="S2.E2.m1.2.3.1.1.cmml" xref="S2.E2.m1.2.2.3">cases</csymbol><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"></eq><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2.3">â€²</ci></apply><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3">ğ‘Ÿ</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3"><times id="S2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.1"></times><apply id="S2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2">subscript</csymbol><apply id="S2.E2.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2.2.3">â€²</ci></apply><ci id="S2.E2.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2.3">ğ‘š</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3"><cos id="S2.E2.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3.1"></cos><apply id="S2.E2.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3.2.3">ğ‘</ci></apply></apply></apply></apply><ci id="S2.E2.m1.2.3.1.3a.cmml" xref="S2.E2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S2.E2.m1.2.3.1.3.cmml" xref="S2.E2.m1.2.2.3">otherwise</mtext></ci><apply id="S2.E2.m1.2.2.2.2.1.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1"><eq id="S2.E2.m1.2.2.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.1"></eq><apply id="S2.E2.m1.2.2.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2">subscript</csymbol><apply id="S2.E2.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.1.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2">superscript</csymbol><ci id="S2.E2.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2.2.3">â€²</ci></apply><ci id="S2.E2.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.2.3">ğ‘–</ci></apply><apply id="S2.E2.m1.2.2.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3"><times id="S2.E2.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.1"></times><apply id="S2.E2.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2">subscript</csymbol><apply id="S2.E2.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.1.3.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2">superscript</csymbol><ci id="S2.E2.m1.2.2.2.2.1.1.3.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.2.2.2.2.1.1.3.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2.2.3">â€²</ci></apply><ci id="S2.E2.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.2.3">ğ‘š</ci></apply><apply id="S2.E2.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3"><sin id="S2.E2.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3.1"></sin><apply id="S2.E2.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.1.3.3.2.1.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2">subscript</csymbol><ci id="S2.E2.m1.2.2.2.2.1.1.3.3.2.2.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2.2">ğ‘Œ</ci><ci id="S2.E2.m1.2.2.2.2.1.1.3.3.2.3.cmml" xref="S2.E2.m1.2.2.2.2.1.1.3.3.2.3">ğ‘</ci></apply></apply></apply></apply><ci id="S2.E2.m1.2.3.1.5a.cmml" xref="S2.E2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S2.E2.m1.2.3.1.5.cmml" xref="S2.E2.m1.2.2.3">otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\begin{cases}Y^{\prime}_{r}=Y^{\prime}_{m}\cos{Y_{p}}\\
Y^{\prime}_{i}=Y^{\prime}_{m}\sin{Y_{p}}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In order to fairly compare the performance of the proposed network with the original CMGAN and alleviate the influence of the number of network parameters, we choose two <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">Conformer</span> modules for each mask generation, which in total is the same as the total number of <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">Conformer</span> blocks in the original CMGAN.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Due to the limited usefulness of GAN when replacing the training objectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and the better performance of complex spectrum masking than that of magnitude spectrum maskingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, it is not the best practice to use the same magnitude spectrum as input for the discriminator or directly replace the training object from the complex spectrum to the magnitude spectrum. Therefore, we choose to use the same discriminator architecture as the original CMGAN. However, in order to improve the ASR performance in enhanced speech, we make a trade-off and replace the input of the enhanced speech spectrogram with the input of the SOTA ASR system, <span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_italic">Whisper<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span id="footnote1.4" class="ltx_text ltx_font_upright">https://huggingface.co/openai/whisper-large-v3</span></span></span></span></span>, to obtain better predictions of the values in FFR. Hence, the discriminator in the proposed architecture aims to mimic the difference between the mel spectrograms of the enhanced and clean speech and further uses it as a part of the loss function.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.5" class="ltx_p">For loss function design, we adopt the same strategy as Cao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which consists of a linear combination of the loss in the TF domain <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{TF}}" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><msub id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.1.m1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.2.cmml">â„’</mi><mtext id="S2.SS1.p5.1.m1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.3a.cmml">TF</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2">â„’</ci><ci id="S2.SS1.p5.1.m1.1.1.3a.cmml" xref="S2.SS1.p5.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.1.m1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3">TF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">\mathcal{L}_{\text{TF}}</annotation></semantics></math>, the loss in the generator <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{GAN}}" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><msub id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.2.m2.1.1.2" xref="S2.SS1.p5.2.m2.1.1.2.cmml">â„’</mi><mtext id="S2.SS1.p5.2.m2.1.1.3" xref="S2.SS1.p5.2.m2.1.1.3a.cmml">GAN</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><apply id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.2.m2.1.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p5.2.m2.1.1.2.cmml" xref="S2.SS1.p5.2.m2.1.1.2">â„’</ci><ci id="S2.SS1.p5.2.m2.1.1.3a.cmml" xref="S2.SS1.p5.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.2.m2.1.1.3.cmml" xref="S2.SS1.p5.2.m2.1.1.3">GAN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">\mathcal{L}_{\text{GAN}}</annotation></semantics></math> and the resultant waveform loss <math id="S2.SS1.p5.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{Time}}" display="inline"><semantics id="S2.SS1.p5.3.m3.1a"><msub id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.3.m3.1.1.2" xref="S2.SS1.p5.3.m3.1.1.2.cmml">â„’</mi><mtext id="S2.SS1.p5.3.m3.1.1.3" xref="S2.SS1.p5.3.m3.1.1.3a.cmml">Time</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><apply id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.3.m3.1.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p5.3.m3.1.1.2.cmml" xref="S2.SS1.p5.3.m3.1.1.2">â„’</ci><ci id="S2.SS1.p5.3.m3.1.1.3a.cmml" xref="S2.SS1.p5.3.m3.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.3.m3.1.1.3.cmml" xref="S2.SS1.p5.3.m3.1.1.3">Time</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">\mathcal{L}_{\text{Time}}</annotation></semantics></math>.
Based on the work of Mao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, the adversarial training follows a min-min optimization task over discriminator loss <math id="S2.SS1.p5.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{D}}" display="inline"><semantics id="S2.SS1.p5.4.m4.1a"><msub id="S2.SS1.p5.4.m4.1.1" xref="S2.SS1.p5.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.4.m4.1.1.2" xref="S2.SS1.p5.4.m4.1.1.2.cmml">â„’</mi><mtext id="S2.SS1.p5.4.m4.1.1.3" xref="S2.SS1.p5.4.m4.1.1.3a.cmml">D</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.4.m4.1b"><apply id="S2.SS1.p5.4.m4.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.4.m4.1.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p5.4.m4.1.1.2.cmml" xref="S2.SS1.p5.4.m4.1.1.2">â„’</ci><ci id="S2.SS1.p5.4.m4.1.1.3a.cmml" xref="S2.SS1.p5.4.m4.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.4.m4.1.1.3.cmml" xref="S2.SS1.p5.4.m4.1.1.3">D</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.4.m4.1c">\mathcal{L}_{\text{D}}</annotation></semantics></math> and generator loss <math id="S2.SS1.p5.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{\text{GAN}}" display="inline"><semantics id="S2.SS1.p5.5.m5.1a"><msub id="S2.SS1.p5.5.m5.1.1" xref="S2.SS1.p5.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p5.5.m5.1.1.2" xref="S2.SS1.p5.5.m5.1.1.2.cmml">â„’</mi><mtext id="S2.SS1.p5.5.m5.1.1.3" xref="S2.SS1.p5.5.m5.1.1.3a.cmml">GAN</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.5.m5.1b"><apply id="S2.SS1.p5.5.m5.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.5.m5.1.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p5.5.m5.1.1.2.cmml" xref="S2.SS1.p5.5.m5.1.1.2">â„’</ci><ci id="S2.SS1.p5.5.m5.1.1.3a.cmml" xref="S2.SS1.p5.5.m5.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.5.m5.1.1.3.cmml" xref="S2.SS1.p5.5.m5.1.1.3">GAN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.5.m5.1c">\mathcal{L}_{\text{GAN}}</annotation></semantics></math>. While Cao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> use the magnitude spectrogram as discriminator input, we opt for the mel spectrogram. The discriminator loss is as follows:
</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.43" class="ltx_Math" alttext="\begin{split}&amp;\mathcal{L}_{\text{D}}=\mathbb{E}_{X_{\text{mel}},\hat{X}_{\text{mel}}}[\parallel D(X_{\text{mel}},\hat{X}_{\text{mel}})-1\parallel^{2}]\\
&amp;\quad+\mathbb{E}_{X_{\text{mel}},\hat{X}_{\text{mel}}}[\parallel D(X_{\text{mel}},\hat{X}_{\text{mel}})-Q_{\text{PESQ}}\parallel^{2}]\end{split}" display="block"><semantics id="S2.E3.m1.43a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S2.E3.m1.43.43.4" xref="S2.E3.m1.41.41.2.cmml"><mtr id="S2.E3.m1.43.43.4a" xref="S2.E3.m1.41.41.2.cmml"><mtd id="S2.E3.m1.43.43.4b" xref="S2.E3.m1.41.41.2.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.43.43.4c" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.42.42.3.40.21.21" xref="S2.E3.m1.41.41.2.cmml"><msub id="S2.E3.m1.42.42.3.40.21.21.22" xref="S2.E3.m1.41.41.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml">â„’</mi><mtext id="S2.E3.m1.2.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.2.1a.cmml">D</mtext></msub><mo id="S2.E3.m1.3.3.3.3.3.3" xref="S2.E3.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S2.E3.m1.42.42.3.40.21.21.21" xref="S2.E3.m1.41.41.2.cmml"><msub id="S2.E3.m1.42.42.3.40.21.21.21.3" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.4.4.4.4.4.4" xref="S2.E3.m1.4.4.4.4.4.4.cmml">ğ”¼</mi><mrow id="S2.E3.m1.5.5.5.5.5.5.1.2" xref="S2.E3.m1.5.5.5.5.5.5.1.3.cmml"><msub id="S2.E3.m1.5.5.5.5.5.5.1.1.1" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.cmml"><mi id="S2.E3.m1.5.5.5.5.5.5.1.1.1.2" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.2.cmml">X</mi><mtext id="S2.E3.m1.5.5.5.5.5.5.1.1.1.3" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.3a.cmml">mel</mtext></msub><mo id="S2.E3.m1.5.5.5.5.5.5.1.2.3" xref="S2.E3.m1.5.5.5.5.5.5.1.3.cmml">,</mo><msub id="S2.E3.m1.5.5.5.5.5.5.1.2.2" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.cmml"><mover accent="true" id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.cmml"><mi id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.2" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.2.cmml">X</mi><mo id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.1" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.1.cmml">^</mo></mover><mtext id="S2.E3.m1.5.5.5.5.5.5.1.2.2.3" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.3a.cmml">mel</mtext></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.42.42.3.40.21.21.21.2" xref="S2.E3.m1.41.41.2.cmml">â€‹</mo><mrow id="S2.E3.m1.42.42.3.40.21.21.21.1.1" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.6.6.6.6.6.6" xref="S2.E3.m1.41.41.2.cmml">[</mo><msup id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.7.7.7.7.7.7" xref="S2.E3.m1.41.41.2.cmml">â€–</mo><mrow id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1.2" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.8.8.8.8.8.8" xref="S2.E3.m1.8.8.8.8.8.8.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1.2.3" xref="S2.E3.m1.41.41.2.cmml">â€‹</mo><mrow id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.9.9.9.9.9.9" xref="S2.E3.m1.41.41.2.cmml">(</mo><msub id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.10.10.10.10.10.10" xref="S2.E3.m1.10.10.10.10.10.10.cmml">X</mi><mtext id="S2.E3.m1.11.11.11.11.11.11.1" xref="S2.E3.m1.11.11.11.11.11.11.1a.cmml">mel</mtext></msub><mo id="S2.E3.m1.12.12.12.12.12.12" xref="S2.E3.m1.41.41.2.cmml">,</mo><msub id="S2.E3.m1.42.42.3.40.21.21.21.1.1.1.1.1.1.2.2.2.2" xref="S2.E3.m1.41.41.2.cmml"><mover accent="true" id="S2.E3.m1.13.13.13.13.13.13" xref="S2.E3.m1.13.13.13.13.13.13.cmml"><mi id="S2.E3.m1.13.13.13.13.13.13.2" xref="S2.E3.m1.13.13.13.13.13.13.2.cmml">X</mi><mo id="S2.E3.m1.13.13.13.13.13.13.1" xref="S2.E3.m1.13.13.13.13.13.13.1.cmml">^</mo></mover><mtext id="S2.E3.m1.14.14.14.14.14.14.1" xref="S2.E3.m1.14.14.14.14.14.14.1a.cmml">mel</mtext></msub><mo stretchy="false" id="S2.E3.m1.15.15.15.15.15.15" xref="S2.E3.m1.41.41.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.16.16.16.16.16.16" xref="S2.E3.m1.16.16.16.16.16.16.cmml">âˆ’</mo><mn id="S2.E3.m1.17.17.17.17.17.17" xref="S2.E3.m1.17.17.17.17.17.17.cmml">1</mn></mrow><mo stretchy="false" id="S2.E3.m1.18.18.18.18.18.18" xref="S2.E3.m1.41.41.2.cmml">â€–</mo></mrow><mn id="S2.E3.m1.19.19.19.19.19.19.1" xref="S2.E3.m1.19.19.19.19.19.19.1.cmml">2</mn></msup><mo stretchy="false" id="S2.E3.m1.20.20.20.20.20.20" xref="S2.E3.m1.41.41.2.cmml">]</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S2.E3.m1.43.43.4d" xref="S2.E3.m1.41.41.2.cmml"><mtd id="S2.E3.m1.43.43.4e" xref="S2.E3.m1.41.41.2.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.43.43.4f" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.43.43.4.41.20.20" xref="S2.E3.m1.41.41.2.cmml"><mo id="S2.E3.m1.43.43.4.41.20.20a" xref="S2.E3.m1.41.41.2.cmml">+</mo><mrow id="S2.E3.m1.43.43.4.41.20.20.20" xref="S2.E3.m1.41.41.2.cmml"><msub id="S2.E3.m1.43.43.4.41.20.20.20.3" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.22.22.22.2.2.2" xref="S2.E3.m1.22.22.22.2.2.2.cmml">ğ”¼</mi><mrow id="S2.E3.m1.23.23.23.3.3.3.1.2" xref="S2.E3.m1.23.23.23.3.3.3.1.3.cmml"><msub id="S2.E3.m1.23.23.23.3.3.3.1.1.1" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.cmml"><mi id="S2.E3.m1.23.23.23.3.3.3.1.1.1.2" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.2.cmml">X</mi><mtext id="S2.E3.m1.23.23.23.3.3.3.1.1.1.3" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.3a.cmml">mel</mtext></msub><mo id="S2.E3.m1.23.23.23.3.3.3.1.2.3" xref="S2.E3.m1.23.23.23.3.3.3.1.3.cmml">,</mo><msub id="S2.E3.m1.23.23.23.3.3.3.1.2.2" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.cmml"><mover accent="true" id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.cmml"><mi id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.2" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.2.cmml">X</mi><mo id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.1" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.1.cmml">^</mo></mover><mtext id="S2.E3.m1.23.23.23.3.3.3.1.2.2.3" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.3a.cmml">mel</mtext></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.43.43.4.41.20.20.20.2" xref="S2.E3.m1.41.41.2.cmml">â€‹</mo><mrow id="S2.E3.m1.43.43.4.41.20.20.20.1.1" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.24.24.24.4.4.4" xref="S2.E3.m1.41.41.2.cmml">[</mo><msup id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.25.25.25.5.5.5" xref="S2.E3.m1.41.41.2.cmml">â€–</mo><mrow id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mrow id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.2" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.26.26.26.6.6.6" xref="S2.E3.m1.26.26.26.6.6.6.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.2.3" xref="S2.E3.m1.41.41.2.cmml">â€‹</mo><mrow id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.41.41.2.cmml"><mo stretchy="false" id="S2.E3.m1.27.27.27.7.7.7" xref="S2.E3.m1.41.41.2.cmml">(</mo><msub id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.28.28.28.8.8.8" xref="S2.E3.m1.28.28.28.8.8.8.cmml">X</mi><mtext id="S2.E3.m1.29.29.29.9.9.9.1" xref="S2.E3.m1.29.29.29.9.9.9.1a.cmml">mel</mtext></msub><mo id="S2.E3.m1.30.30.30.10.10.10" xref="S2.E3.m1.41.41.2.cmml">,</mo><msub id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.2.2.2.2" xref="S2.E3.m1.41.41.2.cmml"><mover accent="true" id="S2.E3.m1.31.31.31.11.11.11" xref="S2.E3.m1.31.31.31.11.11.11.cmml"><mi id="S2.E3.m1.31.31.31.11.11.11.2" xref="S2.E3.m1.31.31.31.11.11.11.2.cmml">X</mi><mo id="S2.E3.m1.31.31.31.11.11.11.1" xref="S2.E3.m1.31.31.31.11.11.11.1.cmml">^</mo></mover><mtext id="S2.E3.m1.32.32.32.12.12.12.1" xref="S2.E3.m1.32.32.32.12.12.12.1a.cmml">mel</mtext></msub><mo stretchy="false" id="S2.E3.m1.33.33.33.13.13.13" xref="S2.E3.m1.41.41.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.34.34.34.14.14.14" xref="S2.E3.m1.34.34.34.14.14.14.cmml">âˆ’</mo><msub id="S2.E3.m1.43.43.4.41.20.20.20.1.1.1.1.1.1.3" xref="S2.E3.m1.41.41.2.cmml"><mi id="S2.E3.m1.35.35.35.15.15.15" xref="S2.E3.m1.35.35.35.15.15.15.cmml">Q</mi><mtext id="S2.E3.m1.36.36.36.16.16.16.1" xref="S2.E3.m1.36.36.36.16.16.16.1a.cmml">PESQ</mtext></msub></mrow><mo stretchy="false" id="S2.E3.m1.37.37.37.17.17.17" xref="S2.E3.m1.41.41.2.cmml">â€–</mo></mrow><mn id="S2.E3.m1.38.38.38.18.18.18.1" xref="S2.E3.m1.38.38.38.18.18.18.1.cmml">2</mn></msup><mo stretchy="false" id="S2.E3.m1.39.39.39.19.19.19" xref="S2.E3.m1.41.41.2.cmml">]</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E3.m1.43b"><apply id="S2.E3.m1.41.41.2.cmml" xref="S2.E3.m1.43.43.4"><eq id="S2.E3.m1.3.3.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3"></eq><apply id="S2.E3.m1.41.41.2.4.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.4.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1">â„’</ci><ci id="S2.E3.m1.2.2.2.2.2.2.1a.cmml" xref="S2.E3.m1.2.2.2.2.2.2.1"><mtext mathsize="70%" id="S2.E3.m1.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.1">D</mtext></ci></apply><apply id="S2.E3.m1.41.41.2.2.cmml" xref="S2.E3.m1.43.43.4"><plus id="S2.E3.m1.21.21.21.1.1.1.cmml" xref="S2.E3.m1.43.43.4"></plus><apply id="S2.E3.m1.40.40.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><times id="S2.E3.m1.40.40.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"></times><apply id="S2.E3.m1.40.40.1.1.1.3.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.40.40.1.1.1.3.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.4.4.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4.4.4">ğ”¼</ci><list id="S2.E3.m1.5.5.5.5.5.5.1.3.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2"><apply id="S2.E3.m1.5.5.5.5.5.5.1.1.1.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.5.5.5.5.1.1.1.1.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1">subscript</csymbol><ci id="S2.E3.m1.5.5.5.5.5.5.1.1.1.2.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.2">ğ‘‹</ci><ci id="S2.E3.m1.5.5.5.5.5.5.1.1.1.3a.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.3"><mtext mathsize="50%" id="S2.E3.m1.5.5.5.5.5.5.1.1.1.3.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.1.1.3">mel</mtext></ci></apply><apply id="S2.E3.m1.5.5.5.5.5.5.1.2.2.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.5.5.5.5.1.2.2.1.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2">subscript</csymbol><apply id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2"><ci id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.1.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.1">^</ci><ci id="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.2.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.2.2">ğ‘‹</ci></apply><ci id="S2.E3.m1.5.5.5.5.5.5.1.2.2.3a.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.3"><mtext mathsize="50%" id="S2.E3.m1.5.5.5.5.5.5.1.2.2.3.cmml" xref="S2.E3.m1.5.5.5.5.5.5.1.2.2.3">mel</mtext></ci></apply></list></apply><apply id="S2.E3.m1.40.40.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="latexml" id="S2.E3.m1.40.40.1.1.1.1.2.1.cmml" xref="S2.E3.m1.43.43.4">delimited-[]</csymbol><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.40.40.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4">superscript</csymbol><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="latexml" id="S2.E3.m1.40.40.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.43.43.4">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><minus id="S2.E3.m1.16.16.16.16.16.16.cmml" xref="S2.E3.m1.16.16.16.16.16.16"></minus><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"><times id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.43.43.4"></times><ci id="S2.E3.m1.8.8.8.8.8.8.cmml" xref="S2.E3.m1.8.8.8.8.8.8">ğ·</ci><interval closure="open" id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.43.43.4"><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.10.10.10.10.10.10.cmml" xref="S2.E3.m1.10.10.10.10.10.10">ğ‘‹</ci><ci id="S2.E3.m1.11.11.11.11.11.11.1a.cmml" xref="S2.E3.m1.11.11.11.11.11.11.1"><mtext mathsize="70%" id="S2.E3.m1.11.11.11.11.11.11.1.cmml" xref="S2.E3.m1.11.11.11.11.11.11.1">mel</mtext></ci></apply><apply id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.40.40.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><apply id="S2.E3.m1.13.13.13.13.13.13.cmml" xref="S2.E3.m1.13.13.13.13.13.13"><ci id="S2.E3.m1.13.13.13.13.13.13.1.cmml" xref="S2.E3.m1.13.13.13.13.13.13.1">^</ci><ci id="S2.E3.m1.13.13.13.13.13.13.2.cmml" xref="S2.E3.m1.13.13.13.13.13.13.2">ğ‘‹</ci></apply><ci id="S2.E3.m1.14.14.14.14.14.14.1a.cmml" xref="S2.E3.m1.14.14.14.14.14.14.1"><mtext mathsize="70%" id="S2.E3.m1.14.14.14.14.14.14.1.cmml" xref="S2.E3.m1.14.14.14.14.14.14.1">mel</mtext></ci></apply></interval></apply><cn type="integer" id="S2.E3.m1.17.17.17.17.17.17.cmml" xref="S2.E3.m1.17.17.17.17.17.17">1</cn></apply></apply><cn type="integer" id="S2.E3.m1.19.19.19.19.19.19.1.cmml" xref="S2.E3.m1.19.19.19.19.19.19.1">2</cn></apply></apply></apply><apply id="S2.E3.m1.41.41.2.2.2.cmml" xref="S2.E3.m1.43.43.4"><times id="S2.E3.m1.41.41.2.2.2.2.cmml" xref="S2.E3.m1.43.43.4"></times><apply id="S2.E3.m1.41.41.2.2.2.3.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.2.2.3.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.22.22.22.2.2.2.cmml" xref="S2.E3.m1.22.22.22.2.2.2">ğ”¼</ci><list id="S2.E3.m1.23.23.23.3.3.3.1.3.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2"><apply id="S2.E3.m1.23.23.23.3.3.3.1.1.1.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.23.23.23.3.3.3.1.1.1.1.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1">subscript</csymbol><ci id="S2.E3.m1.23.23.23.3.3.3.1.1.1.2.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.2">ğ‘‹</ci><ci id="S2.E3.m1.23.23.23.3.3.3.1.1.1.3a.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.3"><mtext mathsize="50%" id="S2.E3.m1.23.23.23.3.3.3.1.1.1.3.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.1.1.3">mel</mtext></ci></apply><apply id="S2.E3.m1.23.23.23.3.3.3.1.2.2.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.23.23.23.3.3.3.1.2.2.1.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2">subscript</csymbol><apply id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2"><ci id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.1.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.1">^</ci><ci id="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.2.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.2.2">ğ‘‹</ci></apply><ci id="S2.E3.m1.23.23.23.3.3.3.1.2.2.3a.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.3"><mtext mathsize="50%" id="S2.E3.m1.23.23.23.3.3.3.1.2.2.3.cmml" xref="S2.E3.m1.23.23.23.3.3.3.1.2.2.3">mel</mtext></ci></apply></list></apply><apply id="S2.E3.m1.41.41.2.2.2.1.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="latexml" id="S2.E3.m1.41.41.2.2.2.1.2.1.cmml" xref="S2.E3.m1.43.43.4">delimited-[]</csymbol><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.2.2.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4">superscript</csymbol><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="latexml" id="S2.E3.m1.41.41.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E3.m1.43.43.4">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><minus id="S2.E3.m1.34.34.34.14.14.14.cmml" xref="S2.E3.m1.34.34.34.14.14.14"></minus><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.43.43.4"><times id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.43.43.4"></times><ci id="S2.E3.m1.26.26.26.6.6.6.cmml" xref="S2.E3.m1.26.26.26.6.6.6">ğ·</ci><interval closure="open" id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.43.43.4"><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.28.28.28.8.8.8.cmml" xref="S2.E3.m1.28.28.28.8.8.8">ğ‘‹</ci><ci id="S2.E3.m1.29.29.29.9.9.9.1a.cmml" xref="S2.E3.m1.29.29.29.9.9.9.1"><mtext mathsize="70%" id="S2.E3.m1.29.29.29.9.9.9.1.cmml" xref="S2.E3.m1.29.29.29.9.9.9.1">mel</mtext></ci></apply><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><apply id="S2.E3.m1.31.31.31.11.11.11.cmml" xref="S2.E3.m1.31.31.31.11.11.11"><ci id="S2.E3.m1.31.31.31.11.11.11.1.cmml" xref="S2.E3.m1.31.31.31.11.11.11.1">^</ci><ci id="S2.E3.m1.31.31.31.11.11.11.2.cmml" xref="S2.E3.m1.31.31.31.11.11.11.2">ğ‘‹</ci></apply><ci id="S2.E3.m1.32.32.32.12.12.12.1a.cmml" xref="S2.E3.m1.32.32.32.12.12.12.1"><mtext mathsize="70%" id="S2.E3.m1.32.32.32.12.12.12.1.cmml" xref="S2.E3.m1.32.32.32.12.12.12.1">mel</mtext></ci></apply></interval></apply><apply id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.4.cmml" xref="S2.E3.m1.43.43.4"><csymbol cd="ambiguous" id="S2.E3.m1.41.41.2.2.2.1.1.1.1.1.1.4.1.cmml" xref="S2.E3.m1.43.43.4">subscript</csymbol><ci id="S2.E3.m1.35.35.35.15.15.15.cmml" xref="S2.E3.m1.35.35.35.15.15.15">ğ‘„</ci><ci id="S2.E3.m1.36.36.36.16.16.16.1a.cmml" xref="S2.E3.m1.36.36.36.16.16.16.1"><mtext mathsize="70%" id="S2.E3.m1.36.36.36.16.16.16.1.cmml" xref="S2.E3.m1.36.36.36.16.16.16.1">PESQ</mtext></ci></apply></apply></apply><cn type="integer" id="S2.E3.m1.38.38.38.18.18.18.1.cmml" xref="S2.E3.m1.38.38.38.18.18.18.1">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.43c">\begin{split}&amp;\mathcal{L}_{\text{D}}=\mathbb{E}_{X_{\text{mel}},\hat{X}_{\text{mel}}}[\parallel D(X_{\text{mel}},\hat{X}_{\text{mel}})-1\parallel^{2}]\\
&amp;\quad+\mathbb{E}_{X_{\text{mel}},\hat{X}_{\text{mel}}}[\parallel D(X_{\text{mel}},\hat{X}_{\text{mel}})-Q_{\text{PESQ}}\parallel^{2}]\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p5.9" class="ltx_p">where <math id="S2.SS1.p5.6.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS1.p5.6.m1.1a"><mi id="S2.SS1.p5.6.m1.1.1" xref="S2.SS1.p5.6.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.6.m1.1b"><ci id="S2.SS1.p5.6.m1.1.1.cmml" xref="S2.SS1.p5.6.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.6.m1.1c">D</annotation></semantics></math> is the discriminator, <math id="S2.SS1.p5.7.m2.1" class="ltx_Math" alttext="Q_{\text{PESQ}}" display="inline"><semantics id="S2.SS1.p5.7.m2.1a"><msub id="S2.SS1.p5.7.m2.1.1" xref="S2.SS1.p5.7.m2.1.1.cmml"><mi id="S2.SS1.p5.7.m2.1.1.2" xref="S2.SS1.p5.7.m2.1.1.2.cmml">Q</mi><mtext id="S2.SS1.p5.7.m2.1.1.3" xref="S2.SS1.p5.7.m2.1.1.3a.cmml">PESQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.7.m2.1b"><apply id="S2.SS1.p5.7.m2.1.1.cmml" xref="S2.SS1.p5.7.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.7.m2.1.1.1.cmml" xref="S2.SS1.p5.7.m2.1.1">subscript</csymbol><ci id="S2.SS1.p5.7.m2.1.1.2.cmml" xref="S2.SS1.p5.7.m2.1.1.2">ğ‘„</ci><ci id="S2.SS1.p5.7.m2.1.1.3a.cmml" xref="S2.SS1.p5.7.m2.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.7.m2.1.1.3.cmml" xref="S2.SS1.p5.7.m2.1.1.3">PESQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.7.m2.1c">Q_{\text{PESQ}}</annotation></semantics></math> refers to the normalized PESQ score in the range [0,1], <math id="S2.SS1.p5.8.m3.1" class="ltx_Math" alttext="X_{\text{mel}}" display="inline"><semantics id="S2.SS1.p5.8.m3.1a"><msub id="S2.SS1.p5.8.m3.1.1" xref="S2.SS1.p5.8.m3.1.1.cmml"><mi id="S2.SS1.p5.8.m3.1.1.2" xref="S2.SS1.p5.8.m3.1.1.2.cmml">X</mi><mtext id="S2.SS1.p5.8.m3.1.1.3" xref="S2.SS1.p5.8.m3.1.1.3a.cmml">mel</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.8.m3.1b"><apply id="S2.SS1.p5.8.m3.1.1.cmml" xref="S2.SS1.p5.8.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.8.m3.1.1.1.cmml" xref="S2.SS1.p5.8.m3.1.1">subscript</csymbol><ci id="S2.SS1.p5.8.m3.1.1.2.cmml" xref="S2.SS1.p5.8.m3.1.1.2">ğ‘‹</ci><ci id="S2.SS1.p5.8.m3.1.1.3a.cmml" xref="S2.SS1.p5.8.m3.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.8.m3.1.1.3.cmml" xref="S2.SS1.p5.8.m3.1.1.3">mel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.8.m3.1c">X_{\text{mel}}</annotation></semantics></math> and <math id="S2.SS1.p5.9.m4.1" class="ltx_Math" alttext="\hat{X}_{\text{mel}}" display="inline"><semantics id="S2.SS1.p5.9.m4.1a"><msub id="S2.SS1.p5.9.m4.1.1" xref="S2.SS1.p5.9.m4.1.1.cmml"><mover accent="true" id="S2.SS1.p5.9.m4.1.1.2" xref="S2.SS1.p5.9.m4.1.1.2.cmml"><mi id="S2.SS1.p5.9.m4.1.1.2.2" xref="S2.SS1.p5.9.m4.1.1.2.2.cmml">X</mi><mo id="S2.SS1.p5.9.m4.1.1.2.1" xref="S2.SS1.p5.9.m4.1.1.2.1.cmml">^</mo></mover><mtext id="S2.SS1.p5.9.m4.1.1.3" xref="S2.SS1.p5.9.m4.1.1.3a.cmml">mel</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.9.m4.1b"><apply id="S2.SS1.p5.9.m4.1.1.cmml" xref="S2.SS1.p5.9.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.9.m4.1.1.1.cmml" xref="S2.SS1.p5.9.m4.1.1">subscript</csymbol><apply id="S2.SS1.p5.9.m4.1.1.2.cmml" xref="S2.SS1.p5.9.m4.1.1.2"><ci id="S2.SS1.p5.9.m4.1.1.2.1.cmml" xref="S2.SS1.p5.9.m4.1.1.2.1">^</ci><ci id="S2.SS1.p5.9.m4.1.1.2.2.cmml" xref="S2.SS1.p5.9.m4.1.1.2.2">ğ‘‹</ci></apply><ci id="S2.SS1.p5.9.m4.1.1.3a.cmml" xref="S2.SS1.p5.9.m4.1.1.3"><mtext mathsize="70%" id="S2.SS1.p5.9.m4.1.1.3.cmml" xref="S2.SS1.p5.9.m4.1.1.3">mel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.9.m4.1c">\hat{X}_{\text{mel}}</annotation></semantics></math> are the mel-spectrogram of the enhanced and target speech signal with 128 bins.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Semi-real-time Incremental Processing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The attention mechanism was first introduced by Bahdanau et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to learn the weights between the hidden spaces of different components in a long input sequence. Its variants demonstrate its ability to improve the performance of the encoder-decoder model in machine translation. Kim et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and Cao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> respectively implemented <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">Transformer</span> and <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">Conformer</span> in speech enhancement, and achieved satisfactory results. However, the performance of these models in real-world HRI applications is not satisfying, due to the conflict between the requirement for real-time processing during HRI and the fixed length of input during training. These speech enhancement models are trained on fixed length audio input to learn the weights between the hidden spaces, which should be long enough to carry the right information. For example, Cao et al. used fragments of 2 seconds as input during trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which is impractical during HRI. We propose to address this issue by means of Incremental ProcessingÂ (IP). </p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">From the Pepper NAOqi SDK<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://doc.aldebaran.com/2-5/naoqi/audio/alaudiodevice.html</span></span></span>, every 170Â ms audio buffer recorded by its single channel microphone can be used for local processing. We adopt 3 buffers as one block, adding up to 510Â ms audio, and concatenate it with the blocks previously recorded by Pepper to create an input with a length of 2,040Â ms, similar to the input length reported by Cao et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. This is done with a sliding window, where the earliest block is removed upon each added block, maintaining the fixed input length. The first blocks that do not have enough previous recorded audio to add up to 2,040Â ms are padded with 0-values to meet this length. </p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Result and Discussions</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data &amp; Experimental Setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We generated two data sets to evaluate the proposed Two-Mask CMGAN and IP procedure.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Training set and Evaluation Set I</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">As discussed in SectionÂ <a href="#S1" title="1 Introduction â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we are not aware of any public dataset consisting of distorted human speech, of which spectral values in FFR are oversubtracted during speech separation or enhancement. Driven by the goal of enhancing the application of the RESF system, we followed the same dataset generation scheme asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and created 10,000 triplets using the public datasets Robot Voice<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://osf.io/v4y6h/</span></span></span> and Librispeech<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.openslr.org/12</span></span></span>. The distorted detection of human speech from the RESF pipeline was considered as <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Distortion Speech</span> and its corresponding clean human speech was taken as the <span id="S3.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">Target Speech</span>. We randomly adopted 1,000 combinations of <span id="S3.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_italic">Distortion Speech</span> and <span id="S3.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_italic">Target Speech</span> as the <span id="S3.SS1.SSS1.p1.1.5" class="ltx_text ltx_font_italic">Evaluation Set I</span> and the other 9,000 as the training set.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Evaluation Set II</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">To assess the robustness of the combination of RESF and Two-Mask CMGAN in HRI scenarios where a robot is typically placed in a location with sounds in the background, we added noise fragments from the Microsoft Scalable Noisy Speech DatasetÂ (MS-SNSD)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Specifically, we mixed 61 clean human speech audio files with 17 types of challenging nonstationary noise from MS-SNSD. These can be categorized into six groups: <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">1) Airport</span> - AirportAnouncement;
<span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">2) Babble Speech</span> - Babble, NeighborSpeaking;
<span id="S3.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">3) Noisy Indoor</span> - Cafe, Cafeteria, Restaurant, LivingRoom;
<span id="S3.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_italic">4) Tranquil Indoor</span> - Copier, Kitchen, Office, Hallway, Typing;
<span id="S3.SS1.SSS2.p1.1.5" class="ltx_text ltx_font_italic">5) Outdoor</span> - Park, Square, Station, Traffic;
<span id="S3.SS1.SSS2.p1.1.6" class="ltx_text ltx_font_italic">6) Vaccuum cleaner</span> - VacuumCleaner.
Given the provided toolkit<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/microsoft/MS-SNSD</span></span></span>, all noisy speech clips were scaled to have nine global SNRs of <math id="S3.SS1.SSS2.p1.1.m1.9" class="ltx_Math" alttext="{40,35,30,25,20,15,10,5,0}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.9a"><mrow id="S3.SS1.SSS2.p1.1.m1.9.10.2" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml"><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">40</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.1" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.cmml">35</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.2" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.3.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.cmml">30</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.3" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.4.4" xref="S3.SS1.SSS2.p1.1.m1.4.4.cmml">25</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.4" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.5.5" xref="S3.SS1.SSS2.p1.1.m1.5.5.cmml">20</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.5" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.6.6" xref="S3.SS1.SSS2.p1.1.m1.6.6.cmml">15</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.6" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.7.7" xref="S3.SS1.SSS2.p1.1.m1.7.7.cmml">10</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.7" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.8.8" xref="S3.SS1.SSS2.p1.1.m1.8.8.cmml">5</mn><mo id="S3.SS1.SSS2.p1.1.m1.9.10.2.8" xref="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.9.9" xref="S3.SS1.SSS2.p1.1.m1.9.9.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.9b"><list id="S3.SS1.SSS2.p1.1.m1.9.10.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.9.10.2"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">40</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2">35</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3">30</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4">25</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.5.5.cmml" xref="S3.SS1.SSS2.p1.1.m1.5.5">20</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.6.6.cmml" xref="S3.SS1.SSS2.p1.1.m1.6.6">15</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.7.7.cmml" xref="S3.SS1.SSS2.p1.1.m1.7.7">10</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.8.8.cmml" xref="S3.SS1.SSS2.p1.1.m1.8.8">5</cn><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.9.9.cmml" xref="S3.SS1.SSS2.p1.1.m1.9.9">0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.9c">{40,35,30,25,20,15,10,5,0}</annotation></semantics></math>Â dB each, which gave out a 5Â hour <span id="S3.SS1.SSS2.p1.1.7" class="ltx_text ltx_font_italic">Evaluation Set II</span>. Then we created noisy overlapping human-robot speech following the same generation scheme as in SectionÂ <a href="#S3.SS1.SSS1" title="3.1.1 Training set and Evaluation Set I â€£ 3.1 Data &amp; Experimental Setup â€£ 3 Experimental Result and Discussions â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>.
Subsequently, these noisy overlapping speech segments were cut into 510Â ms blocks and fed into the RESF pipeline to obtain the detected human interruption speech blocks. These blocks were finally concatenated as the <span id="S3.SS1.SSS2.p1.1.8" class="ltx_text ltx_font_italic">Distortion Speech</span> in <span id="S3.SS1.SSS2.p1.1.9" class="ltx_text ltx_font_italic">Evaluation Set II</span>. Its corresponding clean speech was taken as the <span id="S3.SS1.SSS2.p1.1.10" class="ltx_text ltx_font_italic">Target Speech</span>.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Experimental Set-up</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Using the training set and the two evaluation sets, we compared the performance of our proposed Two-Mask CMGAN approach to that of the pretrained CMGAN<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/ruizhecao96/CMGAN</span></span></span> and the CMGAN retrained on our training set. We set the ASR result on the detected human speech from RESF as the baseline to compare which method would improve the ASR result. In addition, we tested how well both approaches performed without IP, and how well the two-mask CMGAN performed without the mel discriminator. During training, we randomly cut the audio files to 2,040Â ms to align with the same input length asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. To evaluate our proposed IP
, we cut the input into 510Â ms blocks and fed them to the models as described in SectionÂ <a href="#S2.SS2" title="2.2 Semi-real-time Incremental Processing â€£ 2 Methods â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>. The performance of the models in improving the interpretation of the detected human interruption speech is evaluated by calculating the WER after applying ASR to the enhanced sound fragment. We adopted Whisper as the ASR system to translate all detected speech by RESF, enhanced speech by CMGAN and Two-Mask CMGAN, and target human speech. The recognition results of the <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">Target Speech</span> were taken as ground truth to alleviate the influence of the choice of the ASR system. We report on the mean WER and standard deviations, as well as the percentage of files whose WER is lower than 20%, which indicates that no more than two words were misrecognized, inserted, or replaced in a 10-word utterance. We selected this because we found that only a single word was misidentified as a combination of two words in most cases.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Results &amp; Discussions</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text ltx_font_italic">Evaluation Set I</span> results. The input is processed by the proposed IP method if it is not specified. </figcaption>
<table id="S3.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.5.1.1" class="ltx_tr">
<th id="S3.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T1.5.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S3.T1.5.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"></th>
<th id="S3.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">WERÂ %</th>
</tr>
<tr id="S3.T1.5.2.2" class="ltx_tr">
<th id="S3.T1.5.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T1.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">STD</th>
<th id="S3.T1.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">&lt;=20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.3.1" class="ltx_tr">
<th id="S3.T1.5.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<th id="S3.T1.5.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<td id="S3.T1.5.3.1.3" class="ltx_td ltx_align_center ltx_border_t">14.43</td>
<td id="S3.T1.5.3.1.4" class="ltx_td ltx_align_center ltx_border_t">7.69</td>
<td id="S3.T1.5.3.1.5" class="ltx_td ltx_align_center ltx_border_t">75.55</td>
</tr>
<tr id="S3.T1.5.4.2" class="ltx_tr">
<th id="S3.T1.5.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="3"><span id="S3.T1.5.4.2.1.1" class="ltx_text">CMGAN</span></th>
<th id="S3.T1.5.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original</th>
<td id="S3.T1.5.4.2.3" class="ltx_td ltx_align_center">30.34</td>
<td id="S3.T1.5.4.2.4" class="ltx_td ltx_align_center">28.49</td>
<td id="S3.T1.5.4.2.5" class="ltx_td ltx_align_center">44.86</td>
</tr>
<tr id="S3.T1.5.5.3" class="ltx_tr">
<th id="S3.T1.5.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Retrained</th>
<td id="S3.T1.5.5.3.2" class="ltx_td ltx_align_center">10.29</td>
<td id="S3.T1.5.5.3.3" class="ltx_td ltx_align_center">6.06</td>
<td id="S3.T1.5.5.3.4" class="ltx_td ltx_align_center">84.99</td>
</tr>
<tr id="S3.T1.5.6.4" class="ltx_tr">
<th id="S3.T1.5.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o IP</th>
<td id="S3.T1.5.6.4.2" class="ltx_td ltx_align_center">11.35</td>
<td id="S3.T1.5.6.4.3" class="ltx_td ltx_align_center">10.08</td>
<td id="S3.T1.5.6.4.4" class="ltx_td ltx_align_center">81.99</td>
</tr>
<tr id="S3.T1.5.7.5" class="ltx_tr">
<th id="S3.T1.5.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" rowspan="3"><span id="S3.T1.5.7.5.1.1" class="ltx_text">
<span id="S3.T1.5.7.5.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.7.5.1.1.1.1" class="ltx_tr">
<span id="S3.T1.5.7.5.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Two-Mask</span></span>
<span id="S3.T1.5.7.5.1.1.1.2" class="ltx_tr">
<span id="S3.T1.5.7.5.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">CMGAN</span></span>
</span></span></th>
<th id="S3.T1.5.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">proposed</th>
<td id="S3.T1.5.7.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.5.7.5.3.1" class="ltx_text ltx_font_bold">7.44</span></td>
<td id="S3.T1.5.7.5.4" class="ltx_td ltx_align_center">3.92</td>
<td id="S3.T1.5.7.5.5" class="ltx_td ltx_align_center"><span id="S3.T1.5.7.5.5.1" class="ltx_text ltx_font_bold">91.40</span></td>
</tr>
<tr id="S3.T1.5.8.6" class="ltx_tr">
<th id="S3.T1.5.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o IP</th>
<td id="S3.T1.5.8.6.2" class="ltx_td ltx_align_center">12.30</td>
<td id="S3.T1.5.8.6.3" class="ltx_td ltx_align_center">14.14</td>
<td id="S3.T1.5.8.6.4" class="ltx_td ltx_align_center">78.92</td>
</tr>
<tr id="S3.T1.5.9.7" class="ltx_tr">
<th id="S3.T1.5.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">w/o mel</th>
<td id="S3.T1.5.9.7.2" class="ltx_td ltx_align_center ltx_border_b">9.56</td>
<td id="S3.T1.5.9.7.3" class="ltx_td ltx_align_center ltx_border_b">5.26</td>
<td id="S3.T1.5.9.7.4" class="ltx_td ltx_align_center ltx_border_b">86.00</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2 Results &amp; Discussions â€£ 3 Experimental Result and Discussions â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we present the results of <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Evaluation Set I</span>. It is observed that the original CMGAN aggravates rather than improves the distortion caused by spectral oversubtraction, increasing the average WER from 14.43% to 30.34%. The WERs of less than half of the files are greater than 20%. Comparing the results of the retrained CMGAN and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">w/o mel</span>, we find that the mask generation strategy of the proposed two-mask performs better than that of the single-mask. The adoption of the mel spectrogram as input to the discriminator improves the performance from 9.56% to 7.44%. In addition, if only the audio blocks are directly processed (w/o IP), the performance can be as poor as 12.30%, while the proposed IP method for streaming blocks can improve the performance by 39.51%.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The performance on Evaluation set II for different SNRs is presented in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3.2 Results &amp; Discussions â€£ 3 Experimental Result and Discussions â€£ Spectral Oversubtraction? An Approach for Speech Enhancement after Robot Ego Speech Filtering in Semi-Real-Time" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We find that the proposed method performed particularly well when Airport, Tranquil Indoor and Outdoor groups are mixed in. The average WER is reduced to less than 20% when the SNR is no less than 10Â dB. In more challenging scenarios, such as <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">Babble Speech</span> and <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">Noisy Indoor</span>, it can still improve the WER to less than 20% when the SNR is no less than 20Â dB. The reason why it performs relatively poorly in these scenarios is that the HRF values of competitive speakers and noises are non-stationary and high.
The enhancement process will generate relatively more artifacts according to these and will further result in poor recognition. In comparison, the spectrogram of other noise groups has more features in FFR, which is subtracted during the interruption detection by RESF. Especially when there are no competitive speakers or sharp noises present during human interruption, the proposed work can enhance the detected interruption and reduce misrecognized words to a reasonable ratio.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.06274/assets/figs/set2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="336" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>Evaluation result on <span id="S3.F2.4.2" class="ltx_text ltx_font_italic">Evaluation Set II</span>.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we propose a Two-Mask CMGAN network that targets the enhancement of the detected human interruption during robot speech, which suffers from spectral oversubtraction in FFR. We also propose an IP method that allows networks trained on long fixed-length input to process streaming audio blocks. Evaluations of two datasets, including one with unseen noise, demonstrate significant recognition improvements. The combination of Two-Mask CMGAN and IP is verified to improve the robustness of the RESF pipeline. In conclusion, the combination of RESF and the proposed Two-Mask CMGAN with IP shows potential for deployment
in a real-world HRI setting, enabling the robot to keep its single-channel microphone open even when it is speaking. In future work, we will deploy the network in the humanoid robot Pepper and test its effectiveness in such real-world HRI scenarios.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.Â Paliwal, K.Â WÃ³jcicki, and B.Â Schwerin, â€œSingle-channel speech enhancement using spectral subtraction in the short-time modulation domain,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Speech communication</em>, 2010.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
N.Â Das, S.Â Chakraborty, J.Â Chaki, N.Â Padhy, and N.Â Dey, â€œFundamentals, present and future perspectives of speech enhancement,â€ <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Journal of Speech Technology</em>, vol.Â 24, no.Â 4, pp. 883â€“901, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y.Â Li, K.Â Hindriks, and F.Â Kunneman, â€œSingle-channel robot ego-speech filtering during human-robot interaction,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 International Symposium on Technological Advances in Human-Robot Interaction</em>, ser. TAHRI 2024.Â Â Â ACM, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
N.Â Upadhyay and A.Â Karmakar, â€œSpeech enhancement using spectral subtraction-type algorithms: A comparison and simulation study,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Procedia Computer Science</em>, vol.Â 54, pp. 574â€“584, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T.Â F. Kleinschmidt, â€œRobust speech recognition using speech enhancement,â€ Ph.D. dissertation, Queensland University of Technology, 2010.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y.Â Li, F.Â A. Kunneman, and K.Â V. Hindriks, â€œA near-real-time processing ego speech filtering pipeline designed for speech interruption during human-robot interaction,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.13477</em>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
G.Â Parikh and P.Â C. Loizou, â€œThe influence of noise on vowel and consonant cues,â€ <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 2005.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â Chaudhari and S.Â Dhonde, â€œA review on speech enhancement techniques,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2015 International Conference on Pervasive Computing (ICPC)</em>.Â Â Â IEEE, 2015, pp. 1â€“3.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.Â Kamath, P.Â Loizou <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œA multi-band spectral subtraction method for enhancing speech corrupted by colored noise.â€ in <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">ICASSP</em>, vol.Â 4.Â Â Â Citeseer, 2002, pp. 44â€‰164â€“44â€‰164.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.Â Hu, Y.Â Liu, S.Â Lv, M.Â Xing, S.Â Zhang, Y.Â Fu, J.Â Wu, B.Â Zhang, and L.Â Xie, â€œDccrn: Deep complex convolution recurrent network for phase-aware speech enhancement,â€ <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.00264</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X.Â Hao, X.Â Su, R.Â Horaud, and X.Â Li, â€œFullsubnet: A full-band and sub-band fusion model for real-time single-channel speech enhancement,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2021, pp. 6633â€“6637.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.Â Wali, Z.Â Alamgir, S.Â Karim, A.Â Fawaz, M.Â B. Ali, M.Â Adan, and M.Â Mujtaba, â€œGenerative adversarial networks for speech processing: A review,â€ <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol.Â 72, p. 101308, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S.Â Pascual, A.Â Bonafonte, and J.Â Serra, â€œSegan: Speech enhancement generative adversarial network,â€ <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1703.09452</em>, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S.-W. Fu, C.-F. Liao, Y.Â Tsao, and S.-D. Lin, â€œMetricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2019, pp. 2031â€“2041.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
R.Â Cao, S.Â Abdulatif, and B.Â Yang, â€œCmgan: Conformer-based metric gan for speech enhancement,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15149</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.Â Thiemann, N.Â Ito, and E.Â Vincent, â€œThe diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of Meetings on Acoustics</em>, vol.Â 19, no.Â 1.Â Â Â AIP Publishing, 2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
N.Â C. Ristea, A.Â Saabas, R.Â Cutler, B.Â Naderi, S.Â Braun, and S.Â Branets, â€œIcassp 2024 speech signal improvement challenge,â€ <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.14444</em>, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C.Â Donahue, B.Â Li, and R.Â Prabhavalkar, â€œExploring speech enhancement with generative adversarial networks for robust speech recognition,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2018, pp. 5024â€“5028.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
I.Â R. Titze and D.Â W. Martin, â€œPrinciples of voice production,â€ 1998.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S.Â Abdulatif, R.Â Cao, and B.Â Yang, â€œCmgan: Conformer-based metric-gan for monaural speech enhancement,â€ <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
X.Â Mao, Q.Â Li, H.Â Xie, R.Â Y. Lau, Z.Â Wang, and S.Â PaulÂ Smolley, â€œLeast squares generative adversarial networks,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, Oct 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D.Â Bahdanau, K.Â Cho, and Y.Â Bengio, â€œNeural machine translation by jointly learning to align and translate,â€ <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.0473</em>, 2014.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J.Â Kim, M.Â El-Khamy, and J.Â Lee, â€œT-gsa: Transformer with gaussian-weighted self-attention for speech enhancement,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 6649â€“6653.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C.Â K. Reddy, E.Â Beyrami, J.Â Pool, R.Â Cutler, S.Â Srinivasan, and J.Â Gehrke, â€œA scalable noisy speech dataset and online subjective test framework,â€ <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2019</em>, pp. 1816â€“1820, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.06273" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.06274" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.06274">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.06274" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.06275" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 18:12:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
