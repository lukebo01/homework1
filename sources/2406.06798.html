<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.06798] The Reasonable Effectiveness of Speaker Embeddings for Violence Detection</title><meta property="og:description" content="In this paper, we focus on audio violence detection (AVD). AVD is necessary for several reasons,
especially in the context of maintaining safety, preventing harm, and ensuring security in various environments. This cal…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Reasonable Effectiveness of Speaker Embeddings for Violence Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Reasonable Effectiveness of Speaker Embeddings for Violence Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.06798">

<!--Generated on Fri Jul  5 18:09:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]SarthakJain*
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]OrchidChetia Phukan*
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]ArunBalaji Buduru
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=2,3]RajeshSharma





</p>
</div>
<h1 class="ltx_title ltx_title_document">The Reasonable Effectiveness of Speaker Embeddings for Violence Detection
</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">In this paper, we focus on audio violence detection (AVD). AVD is necessary for several reasons,
especially in the context of maintaining safety, preventing harm, and ensuring security in various environments. This calls for accurate AVD systems. Like many related applications in audio processing, the most common approach for improving the performance, would be by leveraging self-supervised (SSL) pre-trained models (PTMs). However, as these SSL models are very large models with million of parameters and this can hinder real-world deployment especially in compute-constraint environment. To resolve this, we propose the usage of speaker recognition models which are much smaller compared to the SSL models. Experimentation with speaker recognition model embeddings with SVM &amp; Random Forest as classifiers, we show that speaker recognition model embeddings perform the best in comparison to state-of-the-art (SOTA) SSL models and achieve SOTA results.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Violence Detection, Speaker Embeddings, x-vector
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Violence not only inflicts immediate harm but also leaves lasting scars on the fabric of society. The ripple effects extend beyond individual victims, impacting families, neighborhoods, and entire communities. As such, ensuring public safety and security becomes paramount, requiring proactive measures to address violence and foster environments where all members can thrive without fear of harm or injustice.
In response to the negative effects associated with violence, previous research has made substantial leap towards violence detection through use of various modalities. In this work, we focus on audio violence detection (AVD). AVD has advantages in conditions such as in areas with low visibility or at night for detecting violence, in comparison to visual or audio-visual violence where access to a visual monitoring device will be required. Researchers have explored AVD with methods ranging from classical machine learning to deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, despite all these advancements AVD still lacks in accuracy and considering the importance of AVD, systems with higher accuracy are need of the hour.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Following related speech and audio processing applications, the most common and beneficial way to boost up a system performance would be using embeddings from pre-trained models (PTMs) especially self-supervised (SSL) PTMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, these state-of-the-art (SOTA) SSL PTMs are very large with millions of parameters that may hinder real-world deployment of AVD systems especiallly in scenarios with less compute power. In response, in this work, we propose to use embeddings extracted from speaker recognition PTMs, models pre-trained primarily for speaker recognition for AVD which is much smaller in size than the SSL PTMs. With comprehensive comparative analysis, we show that x-vector (speaker recognition) embeddings attains the topmost performance. This topmost behavior of x-vector embeddings can be attributed to its effectiveness in capturing intensity, intonation, etc more effectively than SSL PTMs for AVD.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.06798/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="424" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Audio Violence Detection Application; Application architecture pipeline is presented in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Reasonable Effectiveness of Speaker Embeddings for Violence Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a) with the flow of information when the input audio is provided to the final inference received by the user through the user interface; Figure 1(c) shows the confusion matrix of the best model RF with x-vector embeddings</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Application</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss various parts of the proposed AVD application. Firstly, we discuss the various PTMs whose embeddings are used in our study and the classifiers. Followed by the database considered and the results of the experiments. Lastly, we discuss the workings of the application and how a user can interact with the application.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Pre-Trained Models</span>: We use SOTA SSL PTMs WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Unispeech-SAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> because of its performance in various applications in SUPERB. For speaker recognition PTMs, we choose x-vector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and ECAPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> due to their SOTA performance in speaker recognition. The number of parameters of the PTMs are 94.7M, 94.68M, 7M, 20M for wavLM, Unispeech-SAT, x-vector, ECAPA respectively. We extract embeddings of 768 from the last hidden state of SSL PTMs through average pooling and for x-vector, we extract embeddings of dimension of 512.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Classifiers</span>: We experiment with Random Forest (RF) and Support Vector Machine (SVM) classifier with default parameters. We train the models with 5-fold where four folds will be used for training and fifth fold for testing.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Database and Pre-Processing</span>: We use AVD database<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.kaggle.com/datasets/fangfangz/audio-based-violence-detection-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/fangfangz/audio-based-violence-detection-dataset</a></span></span></span> used by Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We divided each of the violence and non violence audio files into sub audio files of 2.5 sec following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and results in dataset with non-violence audios of 7241 and violent audios of 1374. We resample the audios to 16kHz for passing as input to the PTMs.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Performance Scores; Scores are average of 5 folds; Scores are presented in %; F1 is macro-average F1-Score; MFCCs are used as baseline</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S2.T1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">PTM</span></th>
<td id="S2.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">Accuracy</span></td>
<td id="S2.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.4.1.1.3.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">F1</span></td>
</tr>
<tr id="S2.T1.4.2.2" class="ltx_tr">
<th id="S2.T1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S2.T1.4.2.2.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Random Forest</span></th>
</tr>
<tr id="S2.T1.4.3.3" class="ltx_tr">
<th id="S2.T1.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">x-vector</th>
<td id="S2.T1.4.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.4.3.3.2.1" class="ltx_text ltx_font_bold">99.14</span></td>
<td id="S2.T1.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.4.3.3.3.1" class="ltx_text ltx_font_bold">98.40</span></td>
</tr>
<tr id="S2.T1.4.4.4" class="ltx_tr">
<th id="S2.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ECAPA</th>
<td id="S2.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r">94.01</td>
<td id="S2.T1.4.4.4.3" class="ltx_td ltx_align_center">87.42</td>
</tr>
<tr id="S2.T1.4.5.5" class="ltx_tr">
<th id="S2.T1.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MFCC</th>
<td id="S2.T1.4.5.5.2" class="ltx_td ltx_align_center ltx_border_r">72.46</td>
<td id="S2.T1.4.5.5.3" class="ltx_td ltx_align_center">49.01</td>
</tr>
<tr id="S2.T1.4.6.6" class="ltx_tr">
<th id="S2.T1.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Unispeech-SAT</th>
<td id="S2.T1.4.6.6.2" class="ltx_td ltx_align_center ltx_border_r">94.07</td>
<td id="S2.T1.4.6.6.3" class="ltx_td ltx_align_center">87.99</td>
</tr>
<tr id="S2.T1.4.7.7" class="ltx_tr">
<th id="S2.T1.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">WaveLM</th>
<td id="S2.T1.4.7.7.2" class="ltx_td ltx_align_center ltx_border_r">91.05</td>
<td id="S2.T1.4.7.7.3" class="ltx_td ltx_align_center">80.61</td>
</tr>
<tr id="S2.T1.4.8.8" class="ltx_tr">
<th id="S2.T1.4.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S2.T1.4.8.8.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Support Vector Machine</span></th>
</tr>
<tr id="S2.T1.4.9.9" class="ltx_tr">
<th id="S2.T1.4.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">x-vector</th>
<td id="S2.T1.4.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.13</td>
<td id="S2.T1.4.9.9.3" class="ltx_td ltx_align_center ltx_border_t">98.38</td>
</tr>
<tr id="S2.T1.4.10.10" class="ltx_tr">
<th id="S2.T1.4.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ECAPA</th>
<td id="S2.T1.4.10.10.2" class="ltx_td ltx_align_center ltx_border_r">98.02</td>
<td id="S2.T1.4.10.10.3" class="ltx_td ltx_align_center">98.02</td>
</tr>
<tr id="S2.T1.4.11.11" class="ltx_tr">
<th id="S2.T1.4.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MFCC</th>
<td id="S2.T1.4.11.11.2" class="ltx_td ltx_align_center ltx_border_r">71.93</td>
<td id="S2.T1.4.11.11.3" class="ltx_td ltx_align_center">48.71</td>
</tr>
<tr id="S2.T1.4.12.12" class="ltx_tr">
<th id="S2.T1.4.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Unispeech-SAT</th>
<td id="S2.T1.4.12.12.2" class="ltx_td ltx_align_center ltx_border_r">97.73</td>
<td id="S2.T1.4.12.12.3" class="ltx_td ltx_align_center">95.87</td>
</tr>
<tr id="S2.T1.4.13.13" class="ltx_tr">
<th id="S2.T1.4.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">WaveLM</th>
<td id="S2.T1.4.13.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">95.53</td>
<td id="S2.T1.4.13.13.3" class="ltx_td ltx_align_center ltx_border_bb">91.40</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Experimental Results</span>: The evaluation results of the various models are presented in Table <a href="#S2.T1" title="Table 1 ‣ 2 Application ‣ The Reasonable Effectiveness of Speaker Embeddings for Violence Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Models trained with speaker recognition models (x-vector, ECAPA) embeddings performed the best. This can be attributed to their supremacy in capturing speech characteristics such as intensity, intonation, etc more effectively. We use the best model Random Forest trained with x-vector embeddings as backend model in the application.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">User Interface: Working and Backend Details</span>: The user interface of the application is presented in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Reasonable Effectiveness of Speaker Embeddings for Violence Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In mode 1 (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Reasonable Effectiveness of Speaker Embeddings for Violence Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a)), user can use <span id="S2.p6.1.2" class="ltx_text ltx_font_italic">Start Recording</span> and <span id="S2.p6.1.3" class="ltx_text ltx_font_italic">Stop Recording</span> button to record real-world audio for AVD and in mode 2 (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Reasonable Effectiveness of Speaker Embeddings for Violence Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b)) user can upload a audio with <span id="S2.p6.1.4" class="ltx_text ltx_font_italic">Choose File</span> button for detecting violence in the uploaded audio. With <span id="S2.p6.1.5" class="ltx_text ltx_font_italic">Predict</span> button, we can get the predictions out of the application. We have used React.Js and Flask for creating the Front-End and the Back-End for model inference respectively. The average inference time for a 1 min audio file is around 1 sec, thus making it deployable to real-world use.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Conclusion</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this demonstration, we've created an application that harnesses speaker recognition model embeddings to enhance AVD. The application streamlines the process of detecting violent activities across surveillance feeds without access to visual-monitoring device . Overall, the application offers law enforcement agencies a powerful tool for more efficient and effective monitoring, ultimately enhancing public safety.

</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. M. Yildiz, P. D. Barua, S. Dogan, M. Baygin, T. Tuncer, C. P. Ooi, H. Fujita, and U. R. Acharya, ``A novel tree pattern-based violence detection model using audio signals,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Expert systems with applications</em>, vol. 224, p. 120031, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Bakhshi, J. García-Gómez, R. Gil-Pita, and S. Chalup, ``Violence detection in real-life audio signals using lightweight deep neural networks,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Procedia Computer Science</em>, vol. 222, pp. 244–251, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, ``SUPERB: Speech Processing Universal PERformance Benchmark,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 1194–1198.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Chen, Y. Wu, C. Wang, Z. Chen, Z. Chen, S. Liu, J. Wu, Y. Qian, F. Wei, J. Li, and X. Yu, ``Unispeech-sat: Universal speech representation learning with speaker aware pre-training,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2022, pp. 6152–6156.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, ``X-vectors: Robust dnn embeddings for speaker recognition,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2018, pp. 5329–5333.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, ``ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, 2020, pp. 3830–3834.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
F. Zhu-Zhou, D. Tejera-Berengué, R. Gil-Pita, M. Utrilla-Manso, and M. Rosa-Zurera, ``Computationally constrained audio-based violence detection through transfer learning and data augmentation techniques,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Applied Acoustics</em>, vol. 213, p. 109638, 2023.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.06797" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.06798" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.06798">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.06798" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.06799" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 18:09:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
