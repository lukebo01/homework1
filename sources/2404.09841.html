<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.09841] Anatomy of Industrial Scale Multilingual ASR</title><meta property="og:description" content="This paper describes AssemblyAI‚Äôs industrial-scale automatic speech recognition (ASR) system, designed to meet the requirements of large-scale, multilingual ASR serving various application needs. Our system leverages a‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Anatomy of Industrial Scale Multilingual ASR">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Anatomy of Industrial Scale Multilingual ASR">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.09841">

<!--Generated on Sun May  5 15:06:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Anatomy of Industrial Scale Multilingual ASR</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francis McCann Ramirez* ‚ÄÉLuka Chkhetiani* ‚ÄÉAndrew Ehrenberg ‚ÄÉRobert McHardy 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Rami Botros</span> ‚ÄÉ<span id="id2.2.id2" class="ltx_text ltx_font_bold">Yash Khare</span> ‚ÄÉ<span id="id3.3.id3" class="ltx_text ltx_font_bold">Andrea Vanzo</span> ‚ÄÉ<span id="id4.4.id4" class="ltx_text ltx_font_bold">Taufiquzzaman Peyash</span> ‚ÄÉ<span id="id5.5.id5" class="ltx_text ltx_font_bold">Gabriel Oexle</span> 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_text ltx_font_bold">Michael Liang</span> ‚ÄÉ<span id="id7.7.id7" class="ltx_text ltx_font_bold">Ilya Sklyar</span> ‚ÄÉ<span id="id8.8.id8" class="ltx_text ltx_font_bold">Enver Fakhan</span> ‚ÄÉ<span id="id9.9.id9" class="ltx_text ltx_font_bold">Ahmed Etefy</span> ‚ÄÉ<span id="id10.10.id10" class="ltx_text ltx_font_bold">Daniel McCrystal</span> 
<br class="ltx_break"><span id="id11.11.id11" class="ltx_text ltx_font_bold">Sam Flamini</span> ‚ÄÉ<span id="id12.12.id12" class="ltx_text ltx_font_bold">Domenic Donato</span> ‚ÄÉ<span id="id13.13.id13" class="ltx_text ltx_font_bold">Takuya Yoshioka</span>

<br class="ltx_break">AssemblyAI Inc.
<br class="ltx_break"><a target="_blank" href="https://www.assemblyai.com/research/universal-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.assemblyai.com/research/universal-1</a> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">This paper describes AssemblyAI‚Äôs industrial-scale automatic speech recognition (ASR) system, designed to meet the requirements of large-scale, multilingual ASR serving various application needs. Our system leverages a diverse training dataset comprising unsupervised (12.5M hours), supervised (188k hours), and pseudo-labeled (1.6M hours) data across four languages. We provide a detailed description of our model architecture, consisting of a full-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an RNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation demonstrates competitive word error rates (WERs) against larger and more computationally expensive models, such as Whisper large and Canary-1B. Furthermore, our architectural choices yield several key advantages, including an improved code-switching capability, a 5x inference speedup compared to an optimized Whisper baseline, a 30% reduction in hallucination rate on speech data, and a 90% reduction in ambient noise compared to Whisper, along with significantly improved time-stamp accuracy.
Throughout this work, we adopt a system-centric approach to analyzing various aspects of fully-fledged ASR models to gain practically relevant insights useful for real-world services operating at scale.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work.</span></span></span>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past decade, both the research community and the industry have achieved unprecedented progress in artificial intelligence (AI). Especially after the invention of the Transformer¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, technical differences among various AI domains, including automatic speech recognition (ASR), computer vision (CV), and natural language processing (NLP), have largely diminished. As a result, the pursuit of higher-quality AI models has become largely synonymous with scaling in terms of model size and the amount of training data¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, while exploration of Transformer variants, such as Conformer¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, has continued. This principle has been empirically validated across various AI domains. The correlation between data abundance, parameter count, and model performance has been a cornerstone of AI research, driving efforts to amass extensive datasets for training purposes and scale up the training hardware. Importantly, scaling with respect to both the model size and data quantity has resolved issues that smaller models trained on well-organized yet limited datasets have traditionally encountered, including the lack of generalizability.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the field of ASR, OpenAI‚Äôs Whisper¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> has shown the critical impact of data and model scaling. Whisper models are based on a Transformer encoder-decoder architecture, each having a different number of parameters. All models are trained on 680k hours of transcribed speech in multiple languages. The largest model demonstrated industry-leading (at the time of publication of¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>) English ASR performance across various test sets while exhibiting strong multilingual ASR and translation capabilities. Following this seminal work, multiple papers have been published describing full-fledged ASR systems, including Open Whisper-Style Speech Models (OWSM)¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Universal Speech Model (USM)¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, SeamlessM4T¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and Canary-1B¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Notably, all these models are multilingual by design.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This trend gives rise to the need for a new approach to understanding an ASR system in its entirety. In this approach, one first builds a full-blown ASR system with model architectures and training procedures of interest by using an industry-scale training dataset, and subsequently analyzes the behavior of the system from various viewpoints. This is in contrast to a more traditional reductionist approach where one concentrates on studying a single particular aspect (such as a model architecture or a training algorithm) while normalizing all other variables. Considering that the goal of ASR is to work reliably out of the box under various conditions, we believe the system-centric approach is a valid methodology when one wants to obtain practically relevant insights. It would also shed light on certain aspects that have received less attention thus far, including ASR hallucinations and code-switching, which are traits that Whisper is known to exhibit empirically.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With this perspective in mind, in this paper, we build multilingual ASR models using 12.5M hours of pre-training audio and a 1.8M-hour fine-tuning speech dataset.
Reflecting our own needs, we focus on a set of high-resource languages‚ÄîEnglish, Spanish, German, and French.
For model creation, we employ a Conformer encoder¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> with a Recurrent Neural Network Transducer (RNN-T) decoder¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as the model architecture and mostly follow the Google USM paper¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> for training. Through rigorous experimentation and evaluation, we demonstrate that our multilingual models exhibit remarkable robustness and competitive performance with state-of-the-art ASR models such as Whisper large-v3<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span><a target="_blank" href="https://github.com/openai/whisper/discussions/1762" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper/discussions/1762</a></span></span></span> and Canary-1B across languages and test sets, while having around half the parameter count. We analyze various aspects of our models, including code-switching, hallucinations, inference latency, timestamp estimation, and the effect of pre-training. Our analysis reveals that ASR models trained on multilingual corpora exhibit an ability of handling code-switching even though the training dataset did not contain code-switching samples.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This work served as a foundation for building Universal-1, AssemblyAI‚Äôs commercial ASR system<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span><a target="_blank" href="https://www.assemblyai.com/blog/announcing-universal-1-speech-recognition-model/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.assemblyai.com/blog/announcing-universal-1-speech-recognition-model/</a></span></span></span>.
As such, the models and inference code used in this paper are not exactly the same as those deployed in our production system. For ease of reference, we call our models as Universal-1 in this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Self-supervised learning and scaling ASR training</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Numerous studies have shown the effectiveness of learning speech representations from speech-only corpora with self-supervised approaches. Since learning to reconstruct audio signals can be challenging, much of the recent research has moved away from auto-encoding approaches <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Instead, a speech encoder can be trained to solve proxy pretext tasks, which helps it obtain rich latent speech representations. The encoder can then be further fine-tuned in conjunction with a decoder with an ASR loss.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is trained to solve a contrastive task, where it learns to distinguish between positive and distractor samples, which are obtained after encoding, masking and quantizing different time slices within an utterance. HuBERT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> uses a BERT-like <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> masked prediction loss on k-means-clustered audio features. BEST-RQ <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, a method which we adopt in this paper, also uses a masked BERT-style loss. However, it obtains the discrete target labels using frozen random projections and codebooks, thus bypassing the need for elaborate representation learning of discrete prediction targets. These studies and others have shown how pre-training with self-supervised learning (SSL) can reduce the need for large amounts of supervised data.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">There are other approaches to scaling the training data, and these studies have also shown that sheer scaling of the training data as well as the model size remains an effective way for improving ASR performance.
One established approach is
using pseudo-labels, as in a noisy-student training setting. It was shown that training using pseudo-labels could result in better performance than training with human labels alone¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Our previous work <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> also showed the benefits of scaling, which we expand upon in this paper with larger models trained on more data. An alternative approach is using weakly supervised data, as with Whisper.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multilingual ASR</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Thanks to the increased training data quantities and the training algorithms for effectively utilizing them, including SSL, it is becoming possible to train a unified multilingual model encompassing a number of languages¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Overall, combining SSL and model scaling has been shown to simultaneously achieve good performance on hundreds of languages, including low-resource ones.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Conversely, our work focuses on a specific language family, aiming to study more practical aspects of a production ASR system that go beyond Word Error Rate (WER) evaluation, including timestamp estimation, inference speed, and robustness against hallucination.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Practical aspects of ASR</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">End-to-end ASR models are trained to accurately output token sequences, without explicit loss terms for word-level timestamps. Nevertheless, studies have shown that RNN-T and CTC losses, which explore all possible monotonic alignments between the audio and the target transcripts, can learn time alignments as a side effect <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. We experimentally demonstrate that a scaled up bidirectional encoder pre-trained with BEST-RQ and fine-tuned with the RNN-T loss yields accurate word-level timestamp estimates. On the other hand, encoder-decoder models such as Whisper large-v3 and Canary-1B have no notion of blank tokens or alignment-based loss functions. Therefore, additional models are often used to perform forced alignment and obtain the word-level timestamps, as with <cite class="ltx_cite ltx_citemacro_citet">Zhao et¬†al. [<a href="#bib.bib50" title="" class="ltx_ref">50</a>], Bain et¬†al. [<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In a broader AI context, there are various studies investigating the hallucination problem of large language models and other generative models. See, for example, <cite class="ltx_cite ltx_citemacro_citet">Ji et¬†al. [<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for an overview. Recently, this problem has started gaining attention in ASR, becoming increasingly noticeable as ASR models grow in scale <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Previous investigations have attempted to illustrate hallucination examples qualitatively. In this paper, we introduce new hallucination metrics to quantitatively analyze the ASR hallucination problem for our RNN-T-based model as well as Whisper and Canary-1B, both of which are based on encoder-decoder models.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Universal-1</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section describes the process of developing our multilingual ASR model, Universal-1. The description is split into four parts: training data, model architecture, training method, and inference, each described in Section <a href="#S3.SS1" title="3.1 Data preparation ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, <a href="#S3.SS2" title="3.2 Model architecture ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, <a href="#S3.SS3" title="3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, and
<a href="#S3.SS4" title="3.4 Inference ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, respectively. In real usage, additional text formatting is applied for inverse text normalization, punctuation, and truecasing, which we describe in <a href="#A3" title="Appendix C Text formatting ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data preparation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To build a robust multilingual model that will perform well under a wide range of conditions, we utilize large, high-quality datasets for training. We employ three types of data, each differing in quantity and reference transcription accuracy levels, as detailed below.
The data quantity of each category for each language is summarized in <a href="#S3.T1" title="In 3.1.1 Unsupervised data ‚Ä£ 3.1 Data preparation ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Unsupervised data</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Firstly, we utilize 12.5 million hours of unsupervised, or untranscribed, multilingual audio-only data. This dataset is used for pre-training the encoder of our model, as discussed in <a href="#S3.SS3" title="3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">3.3</span></a>. Obtaining unsupervised data in large quantities is relatively straightforward.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Our unsupervised data is acquired from publicly available sources as well as our partners. To ensure that the data contains speech, we employ neural Voice Activity Detection (VAD) from the Silero VAD package¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to calculate a speech existence ratio for each file. This ratio is defined as the ratio of the speech segment duration to the total audio duration. Any audio files with a speech existence ratio below 70% are discarded. Additionally, we limit the duration of audio files to be between 8 and 64 seconds. During pre-training, when utilizing this dataset, we randomly select a 32-second segment or apply padding to reach 32 seconds.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Amounts of data (in hours) per language for varying levels of supervision. The table demonstrates the scarcity of supervised data for non-English languages, necessitating the use of pseudo-labeled data.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Language</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Unsupervised</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Supervised</td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Pseudo-labeled</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">English</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">5,192,686</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">149,070</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1,086,291</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left">Spanish</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_center">1,501,603</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center">10,517</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center">171,857</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left">German</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_center">1,500,073</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_center">14,696</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_align_center">164,737</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left">French</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_center">1,452,665</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center">13,993</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_align_center">198,263</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left">Others</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_center">2,921,147</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Total</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">12,568,174</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">188,276</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1,621,148</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Supervised data</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">To train an ASR model end-to-end, including a decoder, supervised data with reference transcriptions are indispensable. We have purchased and curated our supervised dataset from various sources, including publicly accessible sources with permissive licensing terms and our partners, among others. Despite the expectation that reference transcriptions are 100% accurate, we have found that this is not always the case. Therefore, we have developed a filtering scheme for quality control that compares the reference transcriptions against ASR outputs for verification. This quality control pipeline employs several heuristics using various metrics, including WER, consecutive deletion errors, and language detection consistency. After the quality control filtering, our supervised dataset amounts to approximately 188k hours.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Pseudo-labeled data</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">While our supervised dataset may be comparable to or exceed the training data typically used in published studies in terms of quantity, it still does not seem to reflect the variability of real-world speech. To bridge the gap between the quantity of supervised data and the amount ideally required, we also employ a pseudo-labeling approach, in which we generate reference transcriptions with an existing, well-trained ASR model. The effectiveness of pseudo-labeling has been demonstrated previously¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, including in our past Conformer-1 project¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">Since ASR models generate transcription errors, we utilize two ASR models and discard any samples where the two machine transcriptions disagree to the extent that the WER between them exceeds 20%. This step helps prevent the model from replicating error patterns of the existing ASR models to some extent. This yields 1.62M hours of pseudo-labeled data. All transcribed samples are segmented to 30 seconds or shorter for training.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">Our ASR model is based on Conformer¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, following the recipe of <cite class="ltx_cite ltx_citemacro_citet">Zhang et¬†al. [<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
Our encoder first performs <math id="S3.SS2.p1.1.m1.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1b"><mn id="S3.SS2.p1.1.m1.1.1">4</mn><mo lspace="0.222em" id="S3.SS2.p1.1.m1.1.2">√ó</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">4\times</annotation></semantics></math> temporal reduction via two convolutional subsampling layers. This is followed by a stack of Conformer layers with a non-streaming configuration using bidirectional attention. We use absolute sinusoidal positional encodings and chunk-wise attention¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> with a chunk size of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">8</annotation></semantics></math> seconds. In most experiments, we use RNN-T¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as a decoder for performing ASR, except for some ablation studies where we also utilize CTC¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">Our models have approximately 600M parameters, unless noted otherwise.
Specifically, we employ <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">24</annotation></semantics></math> Conformer layers with a hidden dimension of <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><cn type="integer" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">1024</annotation></semantics></math> and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><cn type="integer" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">8</annotation></semantics></math> attention heads. Our decoders use a vocabulary size of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><cn type="integer" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">2048</annotation></semantics></math> WordPiece¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> tokens by default. The model accepts 80-dimensional log-mel spectrograms as input.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training method</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We employ a two-stage training procedure, comprising self-supervised pre-training and fine-tuning.
<a href="#S3.F1" title="In 3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overall processing flow of the training procedure.
The pre-training stage leverages all unlabeled data to pre-condition the encoder. The fine-tuning stage appends a randomly initialized decoder on top of the pre-trained encoder and trains the entire model by using all labeled data, including the pseudo-labeled samples.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The pre-training and fine-tuning stages are detailed in Sections <a href="#S3.SS3.SSS1" title="3.3.1 Pre-training ‚Ä£ 3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a> and <a href="#S3.SS3.SSS2" title="3.3.2 Fine-tuning ‚Ä£ 3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>, respectively, below. Our implementation utilizes JAX¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and Flax¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and the training is performed on Google Cloud TPU v5e chips using fully-sharded data-parallel (FSDP) parallelism.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.09841/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Two-stage training procedure comprising self-supervised pre-training based on BEST-RQ followed by RNN-T fine-tuning.</figcaption>
</figure>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Pre-training</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">We utilize BEST-RQ¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, along with the modifications proposed in <cite class="ltx_cite ltx_citemacro_citet">Zhang et¬†al. [<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>,
for pre-training Conformer encoders. BEST-RQ offers advantages over other frequently used SSL methods that require additional representation learning, such as Hubert¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, particularly when dealing with large datasets. This advantage is realized by the use of a random projection quantizer and a randomly initialized codebook which remain frozen during training.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.15" class="ltx_p">Specifically, for a given audio sample, we select a random 32 second window and compute its log-mel spectrogram of shape <math id="S3.SS3.SSS1.p2.1.m1.2" class="ltx_Math" alttext="(n,D)" display="inline"><semantics id="S3.SS3.SSS1.p2.1.m1.2a"><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.2.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">n</mi><mo id="S3.SS3.SSS1.p2.1.m1.2.3.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p2.1.m1.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.2.cmml">D</mi><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.2.3" xref="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.2b"><interval closure="open" id="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.2"><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">ùëõ</ci><ci id="S3.SS3.SSS1.p2.1.m1.2.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.2">ùê∑</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.2c">(n,D)</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mi id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><ci id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">n</annotation></semantics></math> and <math id="S3.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">D</annotation></semantics></math> denote the sequence length and the feature dimensionality, respectively. We use a masking probability, <math id="S3.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="p_{\text{mask}}" display="inline"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><msub id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p2.4.m4.1.1.2" xref="S3.SS3.SSS1.p2.4.m4.1.1.2.cmml">p</mi><mtext id="S3.SS3.SSS1.p2.4.m4.1.1.3" xref="S3.SS3.SSS1.p2.4.m4.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><apply id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1.2">ùëù</ci><ci id="S3.SS3.SSS1.p2.4.m4.1.1.3a.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.4.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">p_{\text{mask}}</annotation></semantics></math>, of <math id="S3.SS3.SSS1.p2.5.m5.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S3.SS3.SSS1.p2.5.m5.1a"><mn id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.1b"><cn type="float" id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.1c">0.01</annotation></semantics></math> to obtain the number of masked regions as <math id="S3.SS3.SSS1.p2.6.m6.1" class="ltx_Math" alttext="n_{\text{masked}}=n\cdot p_{\text{mask}}" display="inline"><semantics id="S3.SS3.SSS1.p2.6.m6.1a"><mrow id="S3.SS3.SSS1.p2.6.m6.1.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.cmml"><msub id="S3.SS3.SSS1.p2.6.m6.1.1.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.2.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.2.cmml">n</mi><mtext id="S3.SS3.SSS1.p2.6.m6.1.1.2.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.3a.cmml">masked</mtext></msub><mo id="S3.SS3.SSS1.p2.6.m6.1.1.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.6.m6.1.1.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.3.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p2.6.m6.1.1.3.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.1.cmml">‚ãÖ</mo><msub id="S3.SS3.SSS1.p2.6.m6.1.1.3.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2.cmml">p</mi><mtext id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3a.cmml">mask</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.6.m6.1b"><apply id="S3.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1"><eq id="S3.SS3.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.1"></eq><apply id="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.2">ùëõ</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.2.3a.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.3">masked</mtext></ci></apply><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3"><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.1">‚ãÖ</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2">ùëõ</ci><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2">ùëù</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3a.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3">mask</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.6.m6.1c">n_{\text{masked}}=n\cdot p_{\text{mask}}</annotation></semantics></math>. Then, we use a uniform distribution over the sequence dimension to randomly determine the starting frame of each masked region. For each starting frame, we select a span of <math id="S3.SS3.SSS1.p2.7.m7.1" class="ltx_Math" alttext="n_{\text{span}}=10" display="inline"><semantics id="S3.SS3.SSS1.p2.7.m7.1a"><mrow id="S3.SS3.SSS1.p2.7.m7.1.1" xref="S3.SS3.SSS1.p2.7.m7.1.1.cmml"><msub id="S3.SS3.SSS1.p2.7.m7.1.1.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.7.m7.1.1.2.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.2.cmml">n</mi><mtext id="S3.SS3.SSS1.p2.7.m7.1.1.2.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.3a.cmml">span</mtext></msub><mo id="S3.SS3.SSS1.p2.7.m7.1.1.1" xref="S3.SS3.SSS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS1.p2.7.m7.1.1.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.7.m7.1b"><apply id="S3.SS3.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1"><eq id="S3.SS3.SSS1.p2.7.m7.1.1.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.1"></eq><apply id="S3.SS3.SSS1.p2.7.m7.1.1.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.2">ùëõ</ci><ci id="S3.SS3.SSS1.p2.7.m7.1.1.2.3a.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.3">span</mtext></ci></apply><cn type="integer" id="S3.SS3.SSS1.p2.7.m7.1.1.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.7.m7.1c">n_{\text{span}}=10</annotation></semantics></math> consecutive frames to be masked, allowing for overlaps between neighboring masked regions. This guarantees an upper limit of <math id="S3.SS3.SSS1.p2.8.m8.1" class="ltx_Math" alttext="n_{\text{masked}}\cdot p_{\text{mask}}\cdot n_{\text{span}}" display="inline"><semantics id="S3.SS3.SSS1.p2.8.m8.1a"><mrow id="S3.SS3.SSS1.p2.8.m8.1.1" xref="S3.SS3.SSS1.p2.8.m8.1.1.cmml"><msub id="S3.SS3.SSS1.p2.8.m8.1.1.2" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.8.m8.1.1.2.2" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.2.cmml">n</mi><mtext id="S3.SS3.SSS1.p2.8.m8.1.1.2.3" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.3a.cmml">masked</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p2.8.m8.1.1.1" xref="S3.SS3.SSS1.p2.8.m8.1.1.1.cmml">‚ãÖ</mo><msub id="S3.SS3.SSS1.p2.8.m8.1.1.3" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.8.m8.1.1.3.2" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.2.cmml">p</mi><mtext id="S3.SS3.SSS1.p2.8.m8.1.1.3.3" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.3a.cmml">mask</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS1.p2.8.m8.1.1.1a" xref="S3.SS3.SSS1.p2.8.m8.1.1.1.cmml">‚ãÖ</mo><msub id="S3.SS3.SSS1.p2.8.m8.1.1.4" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.cmml"><mi id="S3.SS3.SSS1.p2.8.m8.1.1.4.2" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.2.cmml">n</mi><mtext id="S3.SS3.SSS1.p2.8.m8.1.1.4.3" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.3a.cmml">span</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.8.m8.1b"><apply id="S3.SS3.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1"><ci id="S3.SS3.SSS1.p2.8.m8.1.1.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.1">‚ãÖ</ci><apply id="S3.SS3.SSS1.p2.8.m8.1.1.2.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.8.m8.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.8.m8.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.2">ùëõ</ci><ci id="S3.SS3.SSS1.p2.8.m8.1.1.2.3a.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.8.m8.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.3">masked</mtext></ci></apply><apply id="S3.SS3.SSS1.p2.8.m8.1.1.3.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.2">ùëù</ci><ci id="S3.SS3.SSS1.p2.8.m8.1.1.3.3a.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.3">mask</mtext></ci></apply><apply id="S3.SS3.SSS1.p2.8.m8.1.1.4.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.8.m8.1.1.4.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.4">subscript</csymbol><ci id="S3.SS3.SSS1.p2.8.m8.1.1.4.2.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.2">ùëõ</ci><ci id="S3.SS3.SSS1.p2.8.m8.1.1.4.3a.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.3"><mtext mathsize="70%" id="S3.SS3.SSS1.p2.8.m8.1.1.4.3.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.4.3">span</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.8.m8.1c">n_{\text{masked}}\cdot p_{\text{mask}}\cdot n_{\text{span}}</annotation></semantics></math> masked frames and introduces variability in the masked frame number and region length.
The features at the masked frames are replaced with random vectors sampled from <math id="S3.SS3.SSS1.p2.9.m9.2" class="ltx_Math" alttext="\mathcal{N}(\bm{0},\sigma\bm{I})" display="inline"><semantics id="S3.SS3.SSS1.p2.9.m9.2a"><mrow id="S3.SS3.SSS1.p2.9.m9.2.2" xref="S3.SS3.SSS1.p2.9.m9.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.9.m9.2.2.3" xref="S3.SS3.SSS1.p2.9.m9.2.2.3.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.9.m9.2.2.2" xref="S3.SS3.SSS1.p2.9.m9.2.2.2.cmml">‚Äã</mo><mrow id="S3.SS3.SSS1.p2.9.m9.2.2.1.1" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.2" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.2.cmml">(</mo><mn id="S3.SS3.SSS1.p2.9.m9.1.1" xref="S3.SS3.SSS1.p2.9.m9.1.1.cmml">ùüé</mn><mo id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.3" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.2.cmml">,</mo><mrow id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.2" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.2.cmml">œÉ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.1" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.3" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.3.cmml">ùë∞</mi></mrow><mo stretchy="false" id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.4" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.9.m9.2b"><apply id="S3.SS3.SSS1.p2.9.m9.2.2.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2"><times id="S3.SS3.SSS1.p2.9.m9.2.2.2.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.2"></times><ci id="S3.SS3.SSS1.p2.9.m9.2.2.3.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.3">ùí©</ci><interval closure="open" id="S3.SS3.SSS1.p2.9.m9.2.2.1.2.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1"><cn type="integer" id="S3.SS3.SSS1.p2.9.m9.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m9.1.1">0</cn><apply id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1"><times id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.1"></times><ci id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.2">ùúé</ci><ci id="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.9.m9.2.2.1.1.1.3">ùë∞</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.9.m9.2c">\mathcal{N}(\bm{0},\sigma\bm{I})</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p2.10.m10.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS3.SSS1.p2.10.m10.1a"><mi id="S3.SS3.SSS1.p2.10.m10.1.1" xref="S3.SS3.SSS1.p2.10.m10.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.10.m10.1b"><ci id="S3.SS3.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS3.SSS1.p2.10.m10.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.10.m10.1c">\sigma</annotation></semantics></math> = 0.1, and <math id="S3.SS3.SSS1.p2.11.m11.1" class="ltx_Math" alttext="\bm{0}" display="inline"><semantics id="S3.SS3.SSS1.p2.11.m11.1a"><mn id="S3.SS3.SSS1.p2.11.m11.1.1" xref="S3.SS3.SSS1.p2.11.m11.1.1.cmml">ùüé</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.11.m11.1b"><cn type="integer" id="S3.SS3.SSS1.p2.11.m11.1.1.cmml" xref="S3.SS3.SSS1.p2.11.m11.1.1">0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.11.m11.1c">\bm{0}</annotation></semantics></math> and <math id="S3.SS3.SSS1.p2.12.m12.1" class="ltx_Math" alttext="\bm{I}" display="inline"><semantics id="S3.SS3.SSS1.p2.12.m12.1a"><mi id="S3.SS3.SSS1.p2.12.m12.1.1" xref="S3.SS3.SSS1.p2.12.m12.1.1.cmml">ùë∞</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.12.m12.1b"><ci id="S3.SS3.SSS1.p2.12.m12.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.1.1">ùë∞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.12.m12.1c">\bm{I}</annotation></semantics></math> are a <math id="S3.SS3.SSS1.p2.13.m13.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS3.SSS1.p2.13.m13.1a"><mi id="S3.SS3.SSS1.p2.13.m13.1.1" xref="S3.SS3.SSS1.p2.13.m13.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.13.m13.1b"><ci id="S3.SS3.SSS1.p2.13.m13.1.1.cmml" xref="S3.SS3.SSS1.p2.13.m13.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.13.m13.1c">D</annotation></semantics></math>-dimensional zero vector and an identity matrix, respectively. After this step, the masked representations are fed through the encoder which is followed by a classifier with <math id="S3.SS3.SSS1.p2.14.m14.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS3.SSS1.p2.14.m14.1a"><mi id="S3.SS3.SSS1.p2.14.m14.1.1" xref="S3.SS3.SSS1.p2.14.m14.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.14.m14.1b"><ci id="S3.SS3.SSS1.p2.14.m14.1.1.cmml" xref="S3.SS3.SSS1.p2.14.m14.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.14.m14.1c">Q</annotation></semantics></math> classification heads, where the targets for each of the classification heads are obtained by using a separate BEST-RQ quantizer on the unmasked representations. In our experiments, we use <math id="S3.SS3.SSS1.p2.15.m15.1" class="ltx_Math" alttext="Q=8" display="inline"><semantics id="S3.SS3.SSS1.p2.15.m15.1a"><mrow id="S3.SS3.SSS1.p2.15.m15.1.1" xref="S3.SS3.SSS1.p2.15.m15.1.1.cmml"><mi id="S3.SS3.SSS1.p2.15.m15.1.1.2" xref="S3.SS3.SSS1.p2.15.m15.1.1.2.cmml">Q</mi><mo id="S3.SS3.SSS1.p2.15.m15.1.1.1" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS1.p2.15.m15.1.1.3" xref="S3.SS3.SSS1.p2.15.m15.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.15.m15.1b"><apply id="S3.SS3.SSS1.p2.15.m15.1.1.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1"><eq id="S3.SS3.SSS1.p2.15.m15.1.1.1.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.1"></eq><ci id="S3.SS3.SSS1.p2.15.m15.1.1.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.2">ùëÑ</ci><cn type="integer" id="S3.SS3.SSS1.p2.15.m15.1.1.3.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.15.m15.1c">Q=8</annotation></semantics></math> classification heads/quantization targets. For efficiency, we only quantize frames which have been masked as other positions do not influence the loss.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p">Note that the masking method described above slightly differs from the original BEST-RQ algorithm described in <cite class="ltx_cite ltx_citemacro_citet">Chiu et¬†al. [<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where
whether to start a masking region is determined for each frame independently with a fixed masking probability.
Our implementation guarantees static shapes, enabling more efficient implementation on TPUs. In our preliminary experiments, the two masking strategies showed almost the same convergence behaviors.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p"><a href="#S3.T2" title="In 3.3.1 Pre-training ‚Ä£ 3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a> shows training hyper-parameters for this Section and the next one. Further details about the pre-training process are provided in <a href="#A1" title="Appendix A Pre-training details ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Hyperparameters used for pre-training (PT) and fine-tuning (FT).</figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.4.5" class="ltx_tr">
<td id="S3.T2.4.5.1" class="ltx_td ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S3.T2.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Encoder PT</td>
<td id="S3.T2.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Encoder FT</td>
<td id="S3.T2.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Decoder FT</td>
</tr>
<tr id="S3.T2.4.6" class="ltx_tr">
<td id="S3.T2.4.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Batch size</td>
<td id="S3.T2.4.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S3.T2.4.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S3.T2.4.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
</tr>
<tr id="S3.T2.4.7" class="ltx_tr">
<td id="S3.T2.4.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Peak LR</td>
<td id="S3.T2.4.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4e-4</td>
<td id="S3.T2.4.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9e-4</td>
<td id="S3.T2.4.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">3e-3</td>
</tr>
<tr id="S3.T2.4.8" class="ltx_tr">
<td id="S3.T2.4.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Warm-up steps</td>
<td id="S3.T2.4.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">25k</td>
<td id="S3.T2.4.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">625</td>
<td id="S3.T2.4.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">187</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T2.3.3.4.1" class="ltx_text">Optimizer</span></td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AdamW, <math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><msub id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml"><mi id="S3.T2.1.1.1.m1.1.1.2.2" xref="S3.T2.1.1.1.m1.1.1.2.2.cmml">Œ≤</mi><mn id="S3.T2.1.1.1.m1.1.1.2.3" xref="S3.T2.1.1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.T2.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><eq id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1"></eq><apply id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.T2.1.1.1.m1.1.1.2.1.cmml" xref="S3.T2.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.T2.1.1.1.m1.1.1.2.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2.2">ùõΩ</ci><cn type="integer" id="S3.T2.1.1.1.m1.1.1.2.3.cmml" xref="S3.T2.1.1.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S3.T2.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S3.T2.2.2.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S3.T2.2.2.2.m2.1a"><mrow id="S3.T2.2.2.2.m2.1.1" xref="S3.T2.2.2.2.m2.1.1.cmml"><msub id="S3.T2.2.2.2.m2.1.1.2" xref="S3.T2.2.2.2.m2.1.1.2.cmml"><mi id="S3.T2.2.2.2.m2.1.1.2.2" xref="S3.T2.2.2.2.m2.1.1.2.2.cmml">Œ≤</mi><mn id="S3.T2.2.2.2.m2.1.1.2.3" xref="S3.T2.2.2.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.T2.2.2.2.m2.1.1.1" xref="S3.T2.2.2.2.m2.1.1.1.cmml">=</mo><mn id="S3.T2.2.2.2.m2.1.1.3" xref="S3.T2.2.2.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m2.1b"><apply id="S3.T2.2.2.2.m2.1.1.cmml" xref="S3.T2.2.2.2.m2.1.1"><eq id="S3.T2.2.2.2.m2.1.1.1.cmml" xref="S3.T2.2.2.2.m2.1.1.1"></eq><apply id="S3.T2.2.2.2.m2.1.1.2.cmml" xref="S3.T2.2.2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.T2.2.2.2.m2.1.1.2.1.cmml" xref="S3.T2.2.2.2.m2.1.1.2">subscript</csymbol><ci id="S3.T2.2.2.2.m2.1.1.2.2.cmml" xref="S3.T2.2.2.2.m2.1.1.2.2">ùõΩ</ci><cn type="integer" id="S3.T2.2.2.2.m2.1.1.2.3.cmml" xref="S3.T2.2.2.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S3.T2.2.2.2.m2.1.1.3.cmml" xref="S3.T2.2.2.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m2.1c">\beta_{2}=0.999</annotation></semantics></math>, <math id="S3.T2.3.3.3.m3.1" class="ltx_Math" alttext="\epsilon=10^{-8}" display="inline"><semantics id="S3.T2.3.3.3.m3.1a"><mrow id="S3.T2.3.3.3.m3.1.1" xref="S3.T2.3.3.3.m3.1.1.cmml"><mi id="S3.T2.3.3.3.m3.1.1.2" xref="S3.T2.3.3.3.m3.1.1.2.cmml">œµ</mi><mo id="S3.T2.3.3.3.m3.1.1.1" xref="S3.T2.3.3.3.m3.1.1.1.cmml">=</mo><msup id="S3.T2.3.3.3.m3.1.1.3" xref="S3.T2.3.3.3.m3.1.1.3.cmml"><mn id="S3.T2.3.3.3.m3.1.1.3.2" xref="S3.T2.3.3.3.m3.1.1.3.2.cmml">10</mn><mrow id="S3.T2.3.3.3.m3.1.1.3.3" xref="S3.T2.3.3.3.m3.1.1.3.3.cmml"><mo id="S3.T2.3.3.3.m3.1.1.3.3a" xref="S3.T2.3.3.3.m3.1.1.3.3.cmml">‚àí</mo><mn id="S3.T2.3.3.3.m3.1.1.3.3.2" xref="S3.T2.3.3.3.m3.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.m3.1b"><apply id="S3.T2.3.3.3.m3.1.1.cmml" xref="S3.T2.3.3.3.m3.1.1"><eq id="S3.T2.3.3.3.m3.1.1.1.cmml" xref="S3.T2.3.3.3.m3.1.1.1"></eq><ci id="S3.T2.3.3.3.m3.1.1.2.cmml" xref="S3.T2.3.3.3.m3.1.1.2">italic-œµ</ci><apply id="S3.T2.3.3.3.m3.1.1.3.cmml" xref="S3.T2.3.3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.T2.3.3.3.m3.1.1.3.1.cmml" xref="S3.T2.3.3.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S3.T2.3.3.3.m3.1.1.3.2.cmml" xref="S3.T2.3.3.3.m3.1.1.3.2">10</cn><apply id="S3.T2.3.3.3.m3.1.1.3.3.cmml" xref="S3.T2.3.3.3.m3.1.1.3.3"><minus id="S3.T2.3.3.3.m3.1.1.3.3.1.cmml" xref="S3.T2.3.3.3.m3.1.1.3.3"></minus><cn type="integer" id="S3.T2.3.3.3.m3.1.1.3.3.2.cmml" xref="S3.T2.3.3.3.m3.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.m3.1c">\epsilon=10^{-8}</annotation></semantics></math>,</td>
</tr>
<tr id="S3.T2.4.4" class="ltx_tr">
<td id="S3.T2.4.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">gradient clipping norm=1.0, weight decay=<math id="S3.T2.4.4.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.T2.4.4.1.m1.1a"><msup id="S3.T2.4.4.1.m1.1.1" xref="S3.T2.4.4.1.m1.1.1.cmml"><mn id="S3.T2.4.4.1.m1.1.1.2" xref="S3.T2.4.4.1.m1.1.1.2.cmml">10</mn><mrow id="S3.T2.4.4.1.m1.1.1.3" xref="S3.T2.4.4.1.m1.1.1.3.cmml"><mo id="S3.T2.4.4.1.m1.1.1.3a" xref="S3.T2.4.4.1.m1.1.1.3.cmml">‚àí</mo><mn id="S3.T2.4.4.1.m1.1.1.3.2" xref="S3.T2.4.4.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.1.m1.1b"><apply id="S3.T2.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.4.4.1.m1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T2.4.4.1.m1.1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.1.2">10</cn><apply id="S3.T2.4.4.1.m1.1.1.3.cmml" xref="S3.T2.4.4.1.m1.1.1.3"><minus id="S3.T2.4.4.1.m1.1.1.3.1.cmml" xref="S3.T2.4.4.1.m1.1.1.3"></minus><cn type="integer" id="S3.T2.4.4.1.m1.1.1.3.2.cmml" xref="S3.T2.4.4.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.1.m1.1c">10^{-4}</annotation></semantics></math>
</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Fine-tuning</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">During the fine-tuning stage,
the BEST-RQ pre-trained encoder is combined with a randomly-initialized CTC or RNN-T decoder to form an end-to-end ASR model. At the start of fine-tuning, gradients are noisy due to the randomly initialized decoder. To prevent catastrophic forgetting of the pre-trained encoder weights, we use
a lower peak learning rate and longer warm-up period for the encoder, as shown in <a href="#S3.T2" title="In 3.3.1 Pre-training ‚Ä£ 3.3 Training method ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">2</span></a>. We used float32 precision for fine-tuning, as bfloat16 caused loss spikes. Training is terminated after 75k steps.
Based on preliminary experiments and educated guesses about the data quality and diversity, we sampled 1.5 times more often from the supervised dataset than from the pseudo-labeled dataset when creating mini-batches.</p>
</div>
<section id="S3.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sequential transducer loss</h5>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.09841/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the changes made for the sequential transducer loss. Encoder outputs are unrolled over the Time(T) dimension and preventing the creation of a high memory lattice of shape <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="B\times V\times T\times U" display="inline"><semantics id="S3.F2.2.m1.1b"><mrow id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml"><mi id="S3.F2.2.m1.1.1.2" xref="S3.F2.2.m1.1.1.2.cmml">B</mi><mo lspace="0.222em" rspace="0.222em" id="S3.F2.2.m1.1.1.1" xref="S3.F2.2.m1.1.1.1.cmml">√ó</mo><mi id="S3.F2.2.m1.1.1.3" xref="S3.F2.2.m1.1.1.3.cmml">V</mi><mo lspace="0.222em" rspace="0.222em" id="S3.F2.2.m1.1.1.1b" xref="S3.F2.2.m1.1.1.1.cmml">√ó</mo><mi id="S3.F2.2.m1.1.1.4" xref="S3.F2.2.m1.1.1.4.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.F2.2.m1.1.1.1c" xref="S3.F2.2.m1.1.1.1.cmml">√ó</mo><mi id="S3.F2.2.m1.1.1.5" xref="S3.F2.2.m1.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><apply id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"><times id="S3.F2.2.m1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1"></times><ci id="S3.F2.2.m1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.2">ùêµ</ci><ci id="S3.F2.2.m1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.3">ùëâ</ci><ci id="S3.F2.2.m1.1.1.4.cmml" xref="S3.F2.2.m1.1.1.4">ùëá</ci><ci id="S3.F2.2.m1.1.1.5.cmml" xref="S3.F2.2.m1.1.1.5">ùëà</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">B\times V\times T\times U</annotation></semantics></math>.</figcaption>
</figure>
<div id="S3.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS2.Px1.p1.6" class="ltx_p">While CTC models can easily be trained with high speed on accelerators, training RNN-T models presents challenges when implemented naively. In RNN-T training, the transducer loss is computed over all possible alignments between an encoder output (padded to maximum length <math id="S3.SS3.SSS2.Px1.p1.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS2.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.1.m1.1c">T</annotation></semantics></math>) and a target transcript (padded to maximum length <math id="S3.SS3.SSS2.Px1.p1.2.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS2.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS2.Px1.p1.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS2.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.2.m2.1.1">ùëà</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.2.m2.1c">U</annotation></semantics></math>)¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. A straightforward implementation, like the one available in PyTorch¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, would perform the forward-backward algorithm completely in parallel. This requires a fully materialized lattice of output logits of shape <math id="S3.SS3.SSS2.Px1.p1.3.m3.1" class="ltx_Math" alttext="B\times T\times U\times V" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.3.m3.1a"><mrow id="S3.SS3.SSS2.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.2" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.2.cmml">B</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1.cmml">√ó</mo><mi id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.3" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.3.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1a" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1.cmml">√ó</mo><mi id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.4" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.4.cmml">U</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1b" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1.cmml">√ó</mo><mi id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.5" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.5.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.3.m3.1b"><apply id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1"><times id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.1"></times><ci id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.2">ùêµ</ci><ci id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.3">ùëá</ci><ci id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.4.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.4">ùëà</ci><ci id="S3.SS3.SSS2.Px1.p1.3.m3.1.1.5.cmml" xref="S3.SS3.SSS2.Px1.p1.3.m3.1.1.5">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.3.m3.1c">B\times T\times U\times V</annotation></semantics></math>, where <math id="S3.SS3.SSS2.Px1.p1.4.m4.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.4.m4.1a"><mi id="S3.SS3.SSS2.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS2.Px1.p1.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.4.m4.1b"><ci id="S3.SS3.SSS2.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.4.m4.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.4.m4.1c">B</annotation></semantics></math> and <math id="S3.SS3.SSS2.Px1.p1.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.5.m5.1a"><mi id="S3.SS3.SSS2.Px1.p1.5.m5.1.1" xref="S3.SS3.SSS2.Px1.p1.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.5.m5.1b"><ci id="S3.SS3.SSS2.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.5.m5.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.5.m5.1c">V</annotation></semantics></math> are the mini-batch and vocabulary sizes, respectively. In our case, this lattice alone would require <math id="S3.SS3.SSS2.Px1.p1.6.m6.1" class="ltx_Math" alttext="4096\cdot 800\cdot 256\cdot 2048\cdot 32\text{bits}=6.9\text{TB}" display="inline"><semantics id="S3.SS3.SSS2.Px1.p1.6.m6.1a"><mrow id="S3.SS3.SSS2.Px1.p1.6.m6.1.1" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.cmml"><mrow id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.cmml"><mrow id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.cmml"><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.2" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.2.cmml">4096</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1.cmml">‚ãÖ</mo><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.3" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.3.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1a" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1.cmml">‚ãÖ</mo><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.4" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.4.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1b" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1.cmml">‚ãÖ</mo><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.5" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.5.cmml">2048</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1c" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1.cmml">‚ãÖ</mo><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.6" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.6.cmml">32</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.1" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.1.cmml">‚Äã</mo><mtext id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3a.cmml">bits</mtext></mrow><mo id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.1" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.cmml"><mn id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.2" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.2.cmml">6.9</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.1" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.1.cmml">‚Äã</mo><mtext id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3a.cmml">TB</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p1.6.m6.1b"><apply id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1"><eq id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.1"></eq><apply id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2"><times id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.1"></times><apply id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2"><ci id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.1">‚ãÖ</ci><cn type="integer" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.2.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.2">4096</cn><cn type="integer" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.3.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.3">800</cn><cn type="integer" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.4.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.4">256</cn><cn type="integer" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.5.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.5">2048</cn><cn type="integer" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.6.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.2.6">32</cn></apply><ci id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3a.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3"><mtext id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.2.3">bits</mtext></ci></apply><apply id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3"><times id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.1"></times><cn type="float" id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.2">6.9</cn><ci id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3a.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3"><mtext id="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.SSS2.Px1.p1.6.m6.1.1.3.3">TB</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p1.6.m6.1c">4096\cdot 800\cdot 256\cdot 2048\cdot 32\text{bits}=6.9\text{TB}</annotation></semantics></math> of TPU memory, which is infeasible. This problem might be alleviated by computing the loss separately for each sample, as proposed in <cite class="ltx_cite ltx_citemacro_citet">Li et¬†al. [<a href="#bib.bib27" title="" class="ltx_ref">27</a>], Braun et¬†al. [<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S3.SS3.SSS2.Px1.p2" class="ltx_para">
<p id="S3.SS3.SSS2.Px1.p2.5" class="ltx_p">In our work, we compute the loss sequentially over time steps <math id="S3.SS3.SSS2.Px1.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.SSS2.Px1.p2.1.m1.1a"><mi id="S3.SS3.SSS2.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS2.Px1.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p2.1.m1.1b"><ci id="S3.SS3.SSS2.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p2.1.m1.1c">t</annotation></semantics></math> instead
to avoid negatively impacting JAX‚Äôs automated FSDP sharding. Our algorithm scans over the encoder output for each <math id="S3.SS3.SSS2.Px1.p2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.SSS2.Px1.p2.2.m2.1a"><mi id="S3.SS3.SSS2.Px1.p2.2.m2.1.1" xref="S3.SS3.SSS2.Px1.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p2.2.m2.1b"><ci id="S3.SS3.SSS2.Px1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p2.2.m2.1c">t</annotation></semantics></math>, computes the joiner outputs and the forward variables <math id="S3.SS3.SSS2.Px1.p2.3.m3.1" class="ltx_Math" alttext="\alpha_{t}" display="inline"><semantics id="S3.SS3.SSS2.Px1.p2.3.m3.1a"><msub id="S3.SS3.SSS2.Px1.p2.3.m3.1.1" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.2" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1.2.cmml">Œ±</mi><mi id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.3" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p2.3.m3.1b"><apply id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1.2">ùõº</ci><ci id="S3.SS3.SSS2.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.Px1.p2.3.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p2.3.m3.1c">\alpha_{t}</annotation></semantics></math>, which are carried forward as states for the next step. Since XLA would still automatically parallelize the computation of the gradients, we also use a custom gradient function to enforce sequential backpropagation through time (BPTT), passing the gradients from each step to the one before. This is functionally equivalent to computing the backward algorithm sequentially, although we leverage autodiff instead of explicit terms for <math id="S3.SS3.SSS2.Px1.p2.4.m4.1" class="ltx_Math" alttext="\beta_{t}" display="inline"><semantics id="S3.SS3.SSS2.Px1.p2.4.m4.1a"><msub id="S3.SS3.SSS2.Px1.p2.4.m4.1.1" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1.cmml"><mi id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.2" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1.2.cmml">Œ≤</mi><mi id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.3" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p2.4.m4.1b"><apply id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1.2">ùõΩ</ci><ci id="S3.SS3.SSS2.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS3.SSS2.Px1.p2.4.m4.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p2.4.m4.1c">\beta_{t}</annotation></semantics></math>. By itself, this sequentialization would slow down the loss computation compared to working on a fully materialized logit lattice. However, since it reduces the memory requirement by a factor of <math id="S3.SS3.SSS2.Px1.p2.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.SSS2.Px1.p2.5.m5.1a"><mi id="S3.SS3.SSS2.Px1.p2.5.m5.1.1" xref="S3.SS3.SSS2.Px1.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.Px1.p2.5.m5.1b"><ci id="S3.SS3.SSS2.Px1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS2.Px1.p2.5.m5.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.Px1.p2.5.m5.1c">T</annotation></semantics></math>, it allows us to fit a larger batch size, leading to more parallelization through the Conformer encoder and thus achieving a higher training throughput overall for a given amount of available TPU memory. We also unroll this scan loop for 50 time-steps, to get some parallelization. Other advanced approaches for transducer loss computation, such as <cite class="ltx_cite ltx_citemacro_citet">Bagby et¬†al. [<a href="#bib.bib2" title="" class="ltx_ref">2</a>], Sim et¬†al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, are not attempted in this paper.</p>
</div>
</section>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Inference</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this section, we outline our inference method for dealing with long-form audio input. At a high level, our inference method consists of three stages:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Audio segmentation</span>: Segment the input audio of arbitrary size into shorter chunks.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Decoding</span>: Batch forward each chunk through the model and perform greedy decoding to obtain output text and word-level timestamps.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Merging</span>: Merge the chunk-wise outputs and readjust time stamps to the original, pre-segmentation time scale.</p>
</div>
</li>
</ol>
<p id="S3.SS4.p1.2" class="ltx_p">Unlike the sequential decoding scheme employed by <cite class="ltx_cite ltx_citemacro_citet">Radford et¬†al. [<a href="#bib.bib36" title="" class="ltx_ref">36</a>], Peng et¬†al. [<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, this divide-and-conquer approach allows batched processing of long-form audio, significantly reducing inference latency.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Audio segmentation</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Long-form audio input is usually split into shorter chunks. This is performed to reduce the mismatch between the audio length to be decoded and those encountered by the model during training, where the training-time audio length is typically in the order of tens of seconds. Segmentation also helps reduce inference time when the computational complexity of the forward pass grows super-linearly, which is the case with vanilla Transformer models. For instance, Whisper and its follow-up models¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> sequentially transcribe 30-second audio chunks from the beginning of the input audio by shifting the 30-second window based on predicted timestamps.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">Our approach is to process individual audio chunks in parallel to optimize for reducing inference latency, i.e., turnaround time, for generating the output transcription for the entire audio. We split the input audio in a way that makes each chunk as long as possible within the maximum input length the model has been trained for during both the pre-training and fine-tuning stages, i.e., 32 seconds. We implement this by leveraging voice activity detection (VAD) to prevent the chunk boundary from occurring in the middle of a word. Specifically, a new chunk is created when a predefined minimum length is exceeded, and the VAD detects consecutive silence of 0.1 seconds or longer. The produced audio chunks are finally converted to log-mel spectrograms, padded to 32 seconds, and batched together up to a predefined maximum batch size. For VAD, we utilize WebRTC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Decoding</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">The batched audio frames from the segmentation stage are passed through the Conformer encoder of our model to obtain the temporally downsampled, encoded feature frames. These are passed to a batched greedy RNN-T decoding routine, which iteratively generates the output tokens and token-level timestamps for the entire batch in parallel. We limit the decoder to emit up to 5 tokens per feature frame. The timestamps produced at this stage are with respect to each chunk‚Äôs local reference frame, i.e., in the range from 0 to 32 seconds.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Merging</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">Finally, we take the output transcriptions and timestamps from all batches and concatenate them to generate a single output. This involves concatenating the individual transcriptions and applying per-chunk offsets to the predicted timestamps. That is, we convert the timestamp values from a local (i.e., within-chunk) time scale to a global one. Additional post-processing steps are applied, including timestamp bias correction (<a href="#S4.SS7" title="4.7 Timestamp estimation ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">4.7</span></a>) and text formatting (<a href="#A3" title="Appendix C Text formatting ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As <cite class="ltx_cite ltx_citemacro_citet">Radford et¬†al. [<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> put it succinctly, the goal of ASR should be to develop a single robust model that works reliably without the need for dataset-specific fine-tuning, which they call a ‚Äúzero-shot‚Äù
setting. Following this approach, we test our ASR models with a variety of test sets for both English and non-English languages by using a mix of publicly available datasets and independently sourced datasets representative of common ASR use cases.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For each test set, we calculate a micro-average word error rate (WER) as a performance evaluation metric. As an overall performance metric, we calculate the macro-average of the WERs of individual test sets. The full list of our test sets is shown in <a href="#A2" title="Appendix B Evaluation Datasets ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span>¬†<span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>English ASR</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><a href="#S4.T3" title="In 4.2 English ASR ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a> shows the WERs of our model, two representative open-source ASR models, and four commercial ASR providers. The open-source ASR models used are Whisper large-v3, the latest model of Whisper, and Canary-1B, the most recent powerful open-source model. Both models are based on an encoder-decoder model architecture. Whisper large-v3 consists of 1.55B parameters, with multilingual ASR and X-to-English translation capabilities. Canary-1B has 1B parameters and is trained to perform ASR and translation for English, Spanish, German, and French. For long-form ASR with Canary-1B, we utilized NVIDIA NeMo‚Äôs chunked inference code<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span><a target="_blank" href="https://github.com/NVIDIA/NeMo/blob/aee120aaa350c7e9709e6c48092d616f42ebc5cd/examples/asr/speech_multitask/speech_to_text_aed_chunked_infer.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo/blob/aee120aaa350c7e9709e6c48092d616f42ebc5cd/examples/asr/speech_multitask/speech_to_text_aed_chunked_infer.py</a></span></span></span>. All commercial ASR providers compared are queried using the default settings of their APIs as of April 1st, 2024.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The results demonstrate that, despite having only 600M parameters, our model performs comparably to the much bigger open-source models and outperforms other ASR providers. Unlike other systems, our model remains competitive across all test sets, showing its robustness. Our model performs particularly well for Noisy, consisting of noisy long-form real-world samples, outperforming the next best system by 7.6% relative.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>WER (%) of English ASR: Lower is better; best results highlighted in <span id="S4.T3.2.1" class="ltx_text ltx_font_bold">bold</span>. CV: CommonVoice, LS: LibriSpeech. Podcast, Broadcast, Telephony, and Noisy are our own internally sourced test sets.</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<td id="S4.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.2.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.2.2" class="ltx_text">
<span id="S4.T3.3.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Universal-1</span></span></span>
</span></span><span id="S4.T3.3.1.1.2.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.3.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.3.2" class="ltx_text">
<span id="S4.T3.3.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.3.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.3.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Canary-1B</span></span></span>
</span></span><span id="S4.T3.3.1.1.3.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.4.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.4.2" class="ltx_text">
<span id="S4.T3.3.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.4.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.4.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Whisper</span></span></span>
<span id="S4.T3.3.1.1.4.2.1.2" class="ltx_tr">
<span id="S4.T3.3.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Large-v3</span></span></span>
</span></span><span id="S4.T3.3.1.1.4.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.5.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.5.2" class="ltx_text">
<span id="S4.T3.3.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.5.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.5.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Azure</span></span></span>
<span id="S4.T3.3.1.1.5.2.1.2" class="ltx_tr">
<span id="S4.T3.3.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.5.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Batch v3.1</span></span></span>
</span></span><span id="S4.T3.3.1.1.5.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.6.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.6.2" class="ltx_text">
<span id="S4.T3.3.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.6.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.6.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Deepgram</span></span></span>
<span id="S4.T3.3.1.1.6.2.1.2" class="ltx_tr">
<span id="S4.T3.3.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.6.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Nova-2</span></span></span>
</span></span><span id="S4.T3.3.1.1.6.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.7.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.7.2" class="ltx_text">
<span id="S4.T3.3.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.7.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.7.2.1.1.1.1" class="ltx_text" style="font-size:80%;">AWS</span></span></span>
</span></span><span id="S4.T3.3.1.1.7.3" class="ltx_text"></span></th>
<th id="S4.T3.3.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T3.3.1.1.8.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.8.2" class="ltx_text">
<span id="S4.T3.3.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.8.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.8.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Google</span></span></span>
<span id="S4.T3.3.1.1.8.2.1.2" class="ltx_tr">
<span id="S4.T3.3.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.1.1.8.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Latest-long</span></span></span>
</span></span><span id="S4.T3.3.1.1.8.3" class="ltx_text"></span></th>
</tr>
<tr id="S4.T3.3.2.2" class="ltx_tr">
<td id="S4.T3.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t">CV V5.1</td>
<td id="S4.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.2.2.2.1" class="ltx_text ltx_font_bold">7.8</span></td>
<td id="S4.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">6.9</td>
<td id="S4.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">8.7</td>
<td id="S4.T3.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">9.2</td>
<td id="S4.T3.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t">11.9</td>
<td id="S4.T3.3.2.2.7" class="ltx_td ltx_align_center ltx_border_t">9.0</td>
<td id="S4.T3.3.2.2.8" class="ltx_td ltx_align_center ltx_border_t">16.9</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left">CORAAL</td>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center">12.9</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.3.1" class="ltx_text ltx_font_bold">9.4</span></td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center">12.5</td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center">12.7</td>
<td id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center">10.6</td>
<td id="S4.T3.3.3.3.7" class="ltx_td ltx_align_center">11.3</td>
<td id="S4.T3.3.3.3.8" class="ltx_td ltx_align_center">19.1</td>
</tr>
<tr id="S4.T3.3.4.4" class="ltx_tr">
<td id="S4.T3.3.4.4.1" class="ltx_td ltx_align_left">Earnings-21</td>
<td id="S4.T3.3.4.4.2" class="ltx_td ltx_align_center">9.8</td>
<td id="S4.T3.3.4.4.3" class="ltx_td ltx_align_center">11.2</td>
<td id="S4.T3.3.4.4.4" class="ltx_td ltx_align_center">9.6</td>
<td id="S4.T3.3.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.4.5.1" class="ltx_text ltx_font_bold">7.4</span></td>
<td id="S4.T3.3.4.4.6" class="ltx_td ltx_align_center">12.3</td>
<td id="S4.T3.3.4.4.7" class="ltx_td ltx_align_center">10.5</td>
<td id="S4.T3.3.4.4.8" class="ltx_td ltx_align_center">11.9</td>
</tr>
<tr id="S4.T3.3.5.5" class="ltx_tr">
<td id="S4.T3.3.5.5.1" class="ltx_td ltx_align_left">LS test-clean</td>
<td id="S4.T3.3.5.5.2" class="ltx_td ltx_align_center">1.8</td>
<td id="S4.T3.3.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.5.3.1" class="ltx_text ltx_font_bold">1.5</span></td>
<td id="S4.T3.3.5.5.4" class="ltx_td ltx_align_center">1.8</td>
<td id="S4.T3.3.5.5.5" class="ltx_td ltx_align_center">2.8</td>
<td id="S4.T3.3.5.5.6" class="ltx_td ltx_align_center">2.6</td>
<td id="S4.T3.3.5.5.7" class="ltx_td ltx_align_center">2.9</td>
<td id="S4.T3.3.5.5.8" class="ltx_td ltx_align_center">5.8</td>
</tr>
<tr id="S4.T3.3.6.6" class="ltx_tr">
<td id="S4.T3.3.6.6.1" class="ltx_td ltx_align_left">LS test-other</td>
<td id="S4.T3.3.6.6.2" class="ltx_td ltx_align_center">3.6</td>
<td id="S4.T3.3.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.6.3.1" class="ltx_text ltx_font_bold">3.0</span></td>
<td id="S4.T3.3.6.6.4" class="ltx_td ltx_align_center">3.6</td>
<td id="S4.T3.3.6.6.5" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T3.3.6.6.6" class="ltx_td ltx_align_center">5.7</td>
<td id="S4.T3.3.6.6.7" class="ltx_td ltx_align_center">6.6</td>
<td id="S4.T3.3.6.6.8" class="ltx_td ltx_align_center">12.6</td>
</tr>
<tr id="S4.T3.3.7.7" class="ltx_tr">
<td id="S4.T3.3.7.7.1" class="ltx_td ltx_align_left">Meanwhile</td>
<td id="S4.T3.3.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.7.7.2.1" class="ltx_text ltx_font_bold">5.2</span></td>
<td id="S4.T3.3.7.7.3" class="ltx_td ltx_align_center">6.0</td>
<td id="S4.T3.3.7.7.4" class="ltx_td ltx_align_center">9.7</td>
<td id="S4.T3.3.7.7.5" class="ltx_td ltx_align_center">6.2</td>
<td id="S4.T3.3.7.7.6" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T3.3.7.7.7" class="ltx_td ltx_align_center">7.3</td>
<td id="S4.T3.3.7.7.8" class="ltx_td ltx_align_center">10.0</td>
</tr>
<tr id="S4.T3.3.8.8" class="ltx_tr">
<td id="S4.T3.3.8.8.1" class="ltx_td ltx_align_left">TED-LIUM 3</td>
<td id="S4.T3.3.8.8.2" class="ltx_td ltx_align_center">7.6</td>
<td id="S4.T3.3.8.8.3" class="ltx_td ltx_align_center">7.8</td>
<td id="S4.T3.3.8.8.4" class="ltx_td ltx_align_center">7.4</td>
<td id="S4.T3.3.8.8.5" class="ltx_td ltx_align_center">9.7</td>
<td id="S4.T3.3.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.8.8.6.1" class="ltx_text ltx_font_bold">6.6</span></td>
<td id="S4.T3.3.8.8.7" class="ltx_td ltx_align_center">9.1</td>
<td id="S4.T3.3.8.8.8" class="ltx_td ltx_align_center">11.3</td>
</tr>
<tr id="S4.T3.3.9.9" class="ltx_tr">
<td id="S4.T3.3.9.9.1" class="ltx_td ltx_align_left">Podcast</td>
<td id="S4.T3.3.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.9.9.2.1" class="ltx_text ltx_font_bold">8.9</span></td>
<td id="S4.T3.3.9.9.3" class="ltx_td ltx_align_center">11.7</td>
<td id="S4.T3.3.9.9.4" class="ltx_td ltx_align_center">10.0</td>
<td id="S4.T3.3.9.9.5" class="ltx_td ltx_align_center">9.7</td>
<td id="S4.T3.3.9.9.6" class="ltx_td ltx_align_center">11.8</td>
<td id="S4.T3.3.9.9.7" class="ltx_td ltx_align_center">10.2</td>
<td id="S4.T3.3.9.9.8" class="ltx_td ltx_align_center">11.9</td>
</tr>
<tr id="S4.T3.3.10.10" class="ltx_tr">
<td id="S4.T3.3.10.10.1" class="ltx_td ltx_align_left">Broadcast</td>
<td id="S4.T3.3.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.10.10.2.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S4.T3.3.10.10.3" class="ltx_td ltx_align_center">5.4</td>
<td id="S4.T3.3.10.10.4" class="ltx_td ltx_align_center">4.8</td>
<td id="S4.T3.3.10.10.5" class="ltx_td ltx_align_center">6.0</td>
<td id="S4.T3.3.10.10.6" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T3.3.10.10.7" class="ltx_td ltx_align_center">5.9</td>
<td id="S4.T3.3.10.10.8" class="ltx_td ltx_align_center">8.2</td>
</tr>
<tr id="S4.T3.3.11.11" class="ltx_tr">
<td id="S4.T3.3.11.11.1" class="ltx_td ltx_align_left">Telephony</td>
<td id="S4.T3.3.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.11.11.2.1" class="ltx_text ltx_font_bold">10.7</span></td>
<td id="S4.T3.3.11.11.3" class="ltx_td ltx_align_center">13.1</td>
<td id="S4.T3.3.11.11.4" class="ltx_td ltx_align_center">12.8</td>
<td id="S4.T3.3.11.11.5" class="ltx_td ltx_align_center">16.4</td>
<td id="S4.T3.3.11.11.6" class="ltx_td ltx_align_center">15.1</td>
<td id="S4.T3.3.11.11.7" class="ltx_td ltx_align_center">16.0</td>
<td id="S4.T3.3.11.11.8" class="ltx_td ltx_align_center">20.4</td>
</tr>
<tr id="S4.T3.3.12.12" class="ltx_tr">
<td id="S4.T3.3.12.12.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T3.3.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.12.12.2.1" class="ltx_text ltx_font_bold">10.9</span></td>
<td id="S4.T3.3.12.12.3" class="ltx_td ltx_align_center">12.9</td>
<td id="S4.T3.3.12.12.4" class="ltx_td ltx_align_center">11.8</td>
<td id="S4.T3.3.12.12.5" class="ltx_td ltx_align_center">16.8</td>
<td id="S4.T3.3.12.12.6" class="ltx_td ltx_align_center">15.7</td>
<td id="S4.T3.3.12.12.7" class="ltx_td ltx_align_center">27.5</td>
<td id="S4.T3.3.12.12.8" class="ltx_td ltx_align_center">21.3</td>
</tr>
<tr id="S4.T3.3.13.13" class="ltx_tr">
<td id="S4.T3.3.13.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Average</td>
<td id="S4.T3.3.13.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.3.13.13.2.1" class="ltx_text ltx_font_bold">7.6</span></td>
<td id="S4.T3.3.13.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.1</td>
<td id="S4.T3.3.13.13.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.4</td>
<td id="S4.T3.3.13.13.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">9.4</td>
<td id="S4.T3.3.13.13.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">9.5</td>
<td id="S4.T3.3.13.13.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">10.6</td>
<td id="S4.T3.3.13.13.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13.6</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Multilingual ASR</h3>

<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>WER (%) of multilingual ASR: Lower is better; best results highlighted in <span id="S4.T4.2.1" class="ltx_text ltx_font_bold">bold</span>. LS: LibriSpeech. Private ES/DE/FR are our internally sourced test sets.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.2.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.2.2" class="ltx_text">
<span id="S4.T4.3.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Universal-1</span></span></span>
</span></span><span id="S4.T4.3.1.1.2.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.3.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.3.2" class="ltx_text">
<span id="S4.T4.3.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.3.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.3.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Canary-1B</span></span></span>
</span></span><span id="S4.T4.3.1.1.3.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.4.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.4.2" class="ltx_text">
<span id="S4.T4.3.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.4.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.4.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Whisper</span></span></span>
<span id="S4.T4.3.1.1.4.2.1.2" class="ltx_tr">
<span id="S4.T4.3.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Large-v3</span></span></span>
</span></span><span id="S4.T4.3.1.1.4.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.5.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.5.2" class="ltx_text">
<span id="S4.T4.3.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.5.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.5.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Azure</span></span></span>
<span id="S4.T4.3.1.1.5.2.1.2" class="ltx_tr">
<span id="S4.T4.3.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.5.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Batch v3.1</span></span></span>
</span></span><span id="S4.T4.3.1.1.5.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.6.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.6.2" class="ltx_text">
<span id="S4.T4.3.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.6.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.6.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Deepgram</span></span></span>
<span id="S4.T4.3.1.1.6.2.1.2" class="ltx_tr">
<span id="S4.T4.3.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.6.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Nova-2</span></span></span>
</span></span><span id="S4.T4.3.1.1.6.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.7.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.7.2" class="ltx_text">
<span id="S4.T4.3.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.7.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.7.2.1.1.1.1" class="ltx_text" style="font-size:80%;">AWS</span></span></span>
</span></span><span id="S4.T4.3.1.1.7.3" class="ltx_text"></span></th>
<th id="S4.T4.3.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T4.3.1.1.8.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.8.2" class="ltx_text">
<span id="S4.T4.3.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.8.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.8.2.1.1.1.1" class="ltx_text" style="font-size:80%;">Google</span></span></span>
<span id="S4.T4.3.1.1.8.2.1.2" class="ltx_tr">
<span id="S4.T4.3.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.1.1.8.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Latest-long</span></span></span>
</span></span><span id="S4.T4.3.1.1.8.3" class="ltx_text"></span></th>
</tr>
<tr id="S4.T4.3.2.2" class="ltx_tr">
<td id="S4.T4.3.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="8">Spanish</td>
</tr>
<tr id="S4.T4.3.3.3" class="ltx_tr">
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Fleurs</td>
<td id="S4.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">4.9</td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">7.1</td>
<td id="S4.T4.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.3.3.4.1" class="ltx_text ltx_font_bold">2.8</span></td>
<td id="S4.T4.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">4.9</td>
<td id="S4.T4.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t">7.0</td>
<td id="S4.T4.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">6.2</td>
<td id="S4.T4.3.3.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.0</td>
</tr>
<tr id="S4.T4.3.4.4" class="ltx_tr">
<td id="S4.T4.3.4.4.1" class="ltx_td ltx_align_left">Multilingual LS</td>
<td id="S4.T4.3.4.4.2" class="ltx_td ltx_align_center">3.4</td>
<td id="S4.T4.3.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.4.4.3.1" class="ltx_text ltx_font_bold">3.0</span></td>
<td id="S4.T4.3.4.4.4" class="ltx_td ltx_align_center">5.7</td>
<td id="S4.T4.3.4.4.5" class="ltx_td ltx_align_center">5.7</td>
<td id="S4.T4.3.4.4.6" class="ltx_td ltx_align_center">5.8</td>
<td id="S4.T4.3.4.4.7" class="ltx_td ltx_align_center">3.4</td>
<td id="S4.T4.3.4.4.8" class="ltx_td ltx_nopad_r ltx_align_center">7.2</td>
</tr>
<tr id="S4.T4.3.5.5" class="ltx_tr">
<td id="S4.T4.3.5.5.1" class="ltx_td ltx_align_left">Private ES</td>
<td id="S4.T4.3.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.5.5.2.1" class="ltx_text ltx_font_bold">4.3</span></td>
<td id="S4.T4.3.5.5.3" class="ltx_td ltx_align_center">11.3</td>
<td id="S4.T4.3.5.5.4" class="ltx_td ltx_align_center">6.6</td>
<td id="S4.T4.3.5.5.5" class="ltx_td ltx_align_center">8.9</td>
<td id="S4.T4.3.5.5.6" class="ltx_td ltx_align_center">8.2</td>
<td id="S4.T4.3.5.5.7" class="ltx_td ltx_align_center">6.2</td>
<td id="S4.T4.3.5.5.8" class="ltx_td ltx_nopad_r ltx_align_center">13.7</td>
</tr>
<tr id="S4.T4.3.6.6" class="ltx_tr">
<td id="S4.T4.3.6.6.1" class="ltx_td ltx_align_left">Voxpopuli</td>
<td id="S4.T4.3.6.6.2" class="ltx_td ltx_align_center">7.7</td>
<td id="S4.T4.3.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.6.6.3.1" class="ltx_text ltx_font_bold">7.1</span></td>
<td id="S4.T4.3.6.6.4" class="ltx_td ltx_align_center">9.6</td>
<td id="S4.T4.3.6.6.5" class="ltx_td ltx_align_center">8.8</td>
<td id="S4.T4.3.6.6.6" class="ltx_td ltx_align_center">9.3</td>
<td id="S4.T4.3.6.6.7" class="ltx_td ltx_align_center">8.6</td>
<td id="S4.T4.3.6.6.8" class="ltx_td ltx_nopad_r ltx_align_center">10.7</td>
</tr>
<tr id="S4.T4.3.7.7" class="ltx_tr">
<td id="S4.T4.3.7.7.1" class="ltx_td ltx_align_left">CommonVoice V9</td>
<td id="S4.T4.3.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.7.7.2.1" class="ltx_text ltx_font_bold">3.6</span></td>
<td id="S4.T4.3.7.7.3" class="ltx_td ltx_align_center">4.2</td>
<td id="S4.T4.3.7.7.4" class="ltx_td ltx_align_center">5.0</td>
<td id="S4.T4.3.7.7.5" class="ltx_td ltx_align_center">7.5</td>
<td id="S4.T4.3.7.7.6" class="ltx_td ltx_align_center">8.0</td>
<td id="S4.T4.3.7.7.7" class="ltx_td ltx_align_center">4.7</td>
<td id="S4.T4.3.7.7.8" class="ltx_td ltx_nopad_r ltx_align_center">7.1</td>
</tr>
<tr id="S4.T4.3.8.8" class="ltx_tr">
<td id="S4.T4.3.8.8.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Average</td>
<td id="S4.T4.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.8.8.2.1" class="ltx_text ltx_font_bold">4.8</span></td>
<td id="S4.T4.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">6.5</td>
<td id="S4.T4.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">6.0</td>
<td id="S4.T4.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t">7.2</td>
<td id="S4.T4.3.8.8.6" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
<td id="S4.T4.3.8.8.7" class="ltx_td ltx_align_center ltx_border_t">5.8</td>
<td id="S4.T4.3.8.8.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">8.7</td>
</tr>
<tr id="S4.T4.3.9.9" class="ltx_tr">
<td id="S4.T4.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t" colspan="8">German</td>
</tr>
<tr id="S4.T4.3.10.10" class="ltx_tr">
<td id="S4.T4.3.10.10.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Fleurs</td>
<td id="S4.T4.3.10.10.2" class="ltx_td ltx_align_center ltx_border_t">10.2</td>
<td id="S4.T4.3.10.10.3" class="ltx_td ltx_align_center ltx_border_t">9.2</td>
<td id="S4.T4.3.10.10.4" class="ltx_td ltx_align_center ltx_border_t">7.5</td>
<td id="S4.T4.3.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.10.10.5.1" class="ltx_text ltx_font_bold">7.2</span></td>
<td id="S4.T4.3.10.10.6" class="ltx_td ltx_align_center ltx_border_t">12.4</td>
<td id="S4.T4.3.10.10.7" class="ltx_td ltx_align_center ltx_border_t">9.0</td>
<td id="S4.T4.3.10.10.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">12.1</td>
</tr>
<tr id="S4.T4.3.11.11" class="ltx_tr">
<td id="S4.T4.3.11.11.1" class="ltx_td ltx_align_left">Multilingual LS</td>
<td id="S4.T4.3.11.11.2" class="ltx_td ltx_align_center">4.3</td>
<td id="S4.T4.3.11.11.3" class="ltx_td ltx_align_center">4.5</td>
<td id="S4.T4.3.11.11.4" class="ltx_td ltx_align_center">7.4</td>
<td id="S4.T4.3.11.11.5" class="ltx_td ltx_align_center">5.7</td>
<td id="S4.T4.3.11.11.6" class="ltx_td ltx_align_center">8.2</td>
<td id="S4.T4.3.11.11.7" class="ltx_td ltx_align_center"><span id="S4.T4.3.11.11.7.1" class="ltx_text ltx_font_bold">3.2</span></td>
<td id="S4.T4.3.11.11.8" class="ltx_td ltx_nopad_r ltx_align_center">12.0</td>
</tr>
<tr id="S4.T4.3.12.12" class="ltx_tr">
<td id="S4.T4.3.12.12.1" class="ltx_td ltx_align_left">Private DE</td>
<td id="S4.T4.3.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.12.12.2.1" class="ltx_text ltx_font_bold">7.5</span></td>
<td id="S4.T4.3.12.12.3" class="ltx_td ltx_align_center">11.9</td>
<td id="S4.T4.3.12.12.4" class="ltx_td ltx_align_center">8.7</td>
<td id="S4.T4.3.12.12.5" class="ltx_td ltx_align_center">9.5</td>
<td id="S4.T4.3.12.12.6" class="ltx_td ltx_align_center">11.1</td>
<td id="S4.T4.3.12.12.7" class="ltx_td ltx_align_center">9.7</td>
<td id="S4.T4.3.12.12.8" class="ltx_td ltx_nopad_r ltx_align_center">12.1</td>
</tr>
<tr id="S4.T4.3.13.13" class="ltx_tr">
<td id="S4.T4.3.13.13.1" class="ltx_td ltx_align_left">Voxpopuli</td>
<td id="S4.T4.3.13.13.2" class="ltx_td ltx_align_center">13.8</td>
<td id="S4.T4.3.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.13.13.3.1" class="ltx_text ltx_font_bold">9.5</span></td>
<td id="S4.T4.3.13.13.4" class="ltx_td ltx_align_center">12.6</td>
<td id="S4.T4.3.13.13.5" class="ltx_td ltx_align_center">14.7</td>
<td id="S4.T4.3.13.13.6" class="ltx_td ltx_align_center">15.1</td>
<td id="S4.T4.3.13.13.7" class="ltx_td ltx_align_center">16.8</td>
<td id="S4.T4.3.13.13.8" class="ltx_td ltx_nopad_r ltx_align_center">17.4</td>
</tr>
<tr id="S4.T4.3.14.14" class="ltx_tr">
<td id="S4.T4.3.14.14.1" class="ltx_td ltx_align_left">CommonVoice V9</td>
<td id="S4.T4.3.14.14.2" class="ltx_td ltx_align_center">4.7</td>
<td id="S4.T4.3.14.14.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.14.14.3.1" class="ltx_text ltx_font_bold">4.6</span></td>
<td id="S4.T4.3.14.14.4" class="ltx_td ltx_align_center">6.0</td>
<td id="S4.T4.3.14.14.5" class="ltx_td ltx_align_center">7.4</td>
<td id="S4.T4.3.14.14.6" class="ltx_td ltx_align_center">9.4</td>
<td id="S4.T4.3.14.14.7" class="ltx_td ltx_align_center">5.8</td>
<td id="S4.T4.3.14.14.8" class="ltx_td ltx_nopad_r ltx_align_center">10.1</td>
</tr>
<tr id="S4.T4.3.15.15" class="ltx_tr">
<td id="S4.T4.3.15.15.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Average</td>
<td id="S4.T4.3.15.15.2" class="ltx_td ltx_align_center ltx_border_t">8.1</td>
<td id="S4.T4.3.15.15.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.15.15.3.1" class="ltx_text ltx_font_bold">7.9</span></td>
<td id="S4.T4.3.15.15.4" class="ltx_td ltx_align_center ltx_border_t">8.4</td>
<td id="S4.T4.3.15.15.5" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="S4.T4.3.15.15.6" class="ltx_td ltx_align_center ltx_border_t">11.2</td>
<td id="S4.T4.3.15.15.7" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="S4.T4.3.15.15.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">12.7</td>
</tr>
<tr id="S4.T4.3.16.16" class="ltx_tr">
<td id="S4.T4.3.16.16.1" class="ltx_td ltx_align_center ltx_border_t" colspan="8">French</td>
</tr>
<tr id="S4.T4.3.17.17" class="ltx_tr">
<td id="S4.T4.3.17.17.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Fleurs</td>
<td id="S4.T4.3.17.17.2" class="ltx_td ltx_align_center ltx_border_t">6.9</td>
<td id="S4.T4.3.17.17.3" class="ltx_td ltx_align_center ltx_border_t">7.7</td>
<td id="S4.T4.3.17.17.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.17.17.4.1" class="ltx_text ltx_font_bold">5.6</span></td>
<td id="S4.T4.3.17.17.5" class="ltx_td ltx_align_center ltx_border_t">8.4</td>
<td id="S4.T4.3.17.17.6" class="ltx_td ltx_align_center ltx_border_t">9.7</td>
<td id="S4.T4.3.17.17.7" class="ltx_td ltx_align_center ltx_border_t">8.5</td>
<td id="S4.T4.3.17.17.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">12.4</td>
</tr>
<tr id="S4.T4.3.18.18" class="ltx_tr">
<td id="S4.T4.3.18.18.1" class="ltx_td ltx_align_left">Multilingual LS</td>
<td id="S4.T4.3.18.18.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.18.18.2.1" class="ltx_text ltx_font_bold">3.2</span></td>
<td id="S4.T4.3.18.18.3" class="ltx_td ltx_align_center">4.0</td>
<td id="S4.T4.3.18.18.4" class="ltx_td ltx_align_center">8.1</td>
<td id="S4.T4.3.18.18.5" class="ltx_td ltx_align_center">9.7</td>
<td id="S4.T4.3.18.18.6" class="ltx_td ltx_align_center">5.2</td>
<td id="S4.T4.3.18.18.7" class="ltx_td ltx_align_center">4.5</td>
<td id="S4.T4.3.18.18.8" class="ltx_td ltx_nopad_r ltx_align_center">15.2</td>
</tr>
<tr id="S4.T4.3.19.19" class="ltx_tr">
<td id="S4.T4.3.19.19.1" class="ltx_td ltx_align_left">Private FR</td>
<td id="S4.T4.3.19.19.2" class="ltx_td ltx_align_center">16.8</td>
<td id="S4.T4.3.19.19.3" class="ltx_td ltx_align_center">27.6</td>
<td id="S4.T4.3.19.19.4" class="ltx_td ltx_align_center">16.1</td>
<td id="S4.T4.3.19.19.5" class="ltx_td ltx_align_center">23.7</td>
<td id="S4.T4.3.19.19.6" class="ltx_td ltx_align_center">17.5</td>
<td id="S4.T4.3.19.19.7" class="ltx_td ltx_align_center"><span id="S4.T4.3.19.19.7.1" class="ltx_text ltx_font_bold">14.9</span></td>
<td id="S4.T4.3.19.19.8" class="ltx_td ltx_nopad_r ltx_align_center">17.6</td>
</tr>
<tr id="S4.T4.3.20.20" class="ltx_tr">
<td id="S4.T4.3.20.20.1" class="ltx_td ltx_align_left">Voxpopuli</td>
<td id="S4.T4.3.20.20.2" class="ltx_td ltx_align_center">9.5</td>
<td id="S4.T4.3.20.20.3" class="ltx_td ltx_align_center">10.1</td>
<td id="S4.T4.3.20.20.4" class="ltx_td ltx_align_center">11.2</td>
<td id="S4.T4.3.20.20.5" class="ltx_td ltx_align_center">11.4</td>
<td id="S4.T4.3.20.20.6" class="ltx_td ltx_align_center">11.2</td>
<td id="S4.T4.3.20.20.7" class="ltx_td ltx_align_center"><span id="S4.T4.3.20.20.7.1" class="ltx_text ltx_font_bold">8.7</span></td>
<td id="S4.T4.3.20.20.8" class="ltx_td ltx_nopad_r ltx_align_center">14.7</td>
</tr>
<tr id="S4.T4.3.21.21" class="ltx_tr">
<td id="S4.T4.3.21.21.1" class="ltx_td ltx_align_left">CommonVoice V9</td>
<td id="S4.T4.3.21.21.2" class="ltx_td ltx_align_center">8.0</td>
<td id="S4.T4.3.21.21.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.21.21.3.1" class="ltx_text ltx_font_bold">6.5</span></td>
<td id="S4.T4.3.21.21.4" class="ltx_td ltx_align_center">11.3</td>
<td id="S4.T4.3.21.21.5" class="ltx_td ltx_align_center">12.9</td>
<td id="S4.T4.3.21.21.6" class="ltx_td ltx_align_center">12.1</td>
<td id="S4.T4.3.21.21.7" class="ltx_td ltx_align_center">7.4</td>
<td id="S4.T4.3.21.21.8" class="ltx_td ltx_nopad_r ltx_align_center">17.5</td>
</tr>
<tr id="S4.T4.3.22.22" class="ltx_tr">
<td id="S4.T4.3.22.22.1" class="ltx_td ltx_align_left ltx_border_bb">
<span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>Average</td>
<td id="S4.T4.3.22.22.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.9</td>
<td id="S4.T4.3.22.22.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">11.2</td>
<td id="S4.T4.3.22.22.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">10.5</td>
<td id="S4.T4.3.22.22.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13.2</td>
<td id="S4.T4.3.22.22.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">11.1</td>
<td id="S4.T4.3.22.22.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.3.22.22.7.1" class="ltx_text ltx_font_bold">8.8</span></td>
<td id="S4.T4.3.22.22.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">15.5</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><a href="#S4.T4" title="In 4.3 Multilingual ASR ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> depicts the WERs of our model and the same six ASR systems, including two open-source models and four commercial ASR services, for Spanish, German, and French test sets. Our model achieved the lowest WER on average. Furthermore, our model was competitive even in test sets for which it did not achieve the best performance, despite its modest model size, which is beneficial for processing a large number of requests with low latency. We believe the robustness and consistently competitive performance across all test sets and languages achieved by our model result from the use of diverse training data and meticulous data filtering.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We would like to caution that there may have been data leakage for some of the considered systems for the FLEURS corpus. For FLEURS, we observed some instances of incorrect reference transcriptions being correctly predicted by some models.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Code-switching</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">One emergent capability in multilingual ASR models is the ability to handle code-switching speech. Code-switching occurs when a speaker switches back and forth between two or more languages in a single conversation, sometimes even within a single sentence. We have observed that our model, trained on multilingual training data using a multilingual tokenizer, exhibits the ability to handle code-switching speech to a certain extent. In this section, we explore this aspect by comparing our model with Whisper and Canary-1B, which also use multilingual tokenizers and are trained on multilingual data.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">To test the code-switching capability, we created datasets by combining audio clips from LibriSpeech test-clean and MLS corpora. They were chosen since the monolingual ASR accuracy for these datasets is sufficiently high to the extent that allows us to largely isolate the WER degradation caused by handling code-switching. We created 250 code-switching test samples per language pair as follows.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Randomly determine a target duration from the range between 30 and 180 seconds.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Randomly select a file from either MLS or LibriSpeech test-clean datasets.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Continue selecting files from these two datasets in an alternating fashion until the total audio duration exceeds the target length.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Concatenate all the selected audio files to generate a single audio file. The reference transcription is also created by concatenating all the corresponding reference transcriptions.</p>
</div>
</li>
</ol>
<p id="S4.SS4.p3.1" class="ltx_p">The duration range of 30‚Äì180 seconds in the first step was chosen to provide sufficient coverage with respect to the frequency of language alternations and the dominance of one language over the other in the concatenated files. Each test audio file created in this manner consisted of 3‚Äì19 files taken from the LibriSpeech and MLS corpora.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">We considered three language pairs: English-Spanish, English-French, and English-German, taking into account the fact that code-switching often occurs between English and another language in most real scenarios. The corresponding test sets are referred to as en-es, en-fr, and en-de.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2404.09841/assets/Figures/code-switching.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="355" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Code-switching experiment results using synthetic datasets, comparing our model and open-source models. The tags in the legend correspond to different ways of configuring the open-source models. ALD: Whisper‚Äôs automatic language detection was used to predict the language token for each sample. EN: English was specified. ML: The non-English language of each dataset was specified.</figcaption>
</figure>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">Unlike our model, which does not enforce a specific language during decoding, Whisper and Canary-1B require a user to specify a language by default. Therefore, for these models, we considered the following conditions:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Matched language (ml): For each dataset, we specify the language token of the non-English language.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">English (en): For each dataset, we specify the English language token.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Automatic Language Detection (ALD): For Whisper, we let the model predict the language token for each file independently. Canary-1B does not provide this option.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p"><a href="#S4.F3" title="In 4.4 Code-switching ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">3</span></a> shows the WERs obtained by the three models for each language pair. We can observe that our model handled the code-switching samples quite effectively compared to the other two models, irrespective of their language-code configurations, achieving the lowest WER by substantial margins. Upon manual inspection, we found that other models occasionally introduced disruptive artifacts, which seem to be the result of conditioning decoding on one language token or training for multiple tasks. Sometimes, these artifacts manifest themselves as significant deletion errors or translations, even though we specified the task as transcription. We hypothesize that the stability of our model with respect to code-switching can be attributed to the balanced diversity of our training datasets and the single-task nature of our training setup.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p">This benchmark was our first foray into quantifying code-switching, and we leave this open as an area for further work. Possible areas to explore include training on data created in a fashion similar to the benchmark we used and studying the trade-off between the use of explicit language tokens and code-switching performance.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Inference latency</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To investigate inference latency and throughput, we compared our Universal-1 model with Whisper large-v3 and Canary-1B. The latency of an ASR system is quantified using Real Time Factor (RTF), defined as the ratio of the aggregate wall clock time to process an entire dataset to the cumulative duration of the audio files within the dataset. In addition to the overall RTF analysis, we include detailed measurements of the inference times for both the encoder and decoder components. We utilized two distinct datasets, LibriSpeech test-other and TED-LIUM3, which serve as proxies for short-form and long-form audio benchmarking, respectively. The evaluation was conducted on an Nvidia T4 Graphics Processing Unit (GPU), equipped with 16 Gigabytes (GB) of Video Random Access Memory (VRAM).</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In the long-form ASR benchmarking, we enabled batched inference wherever possible. As described in Section <a href="#S3.SS4" title="3.4 Inference ‚Ä£ 3 Universal-1 ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, our model splits a long-form audio input signal and batch-processes the individual chunks. The official implementation of Whisper does not support batched inference. In our experiments, we utilized the batched inference method as implemented in <cite class="ltx_cite ltx_citemacro_citet">Bain et¬†al. [<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which also uses Faster-Whisper for efficient computation. For Canary-1B inference, we employed the same chunked inference script that was used in the accuracy benchmarking, as described in Section <a href="#S4.SS2" title="4.2 English ASR ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Since this implementation does not seem to take advantage of parallel batched computation, we only considered a single-sample batch computation for Canary-1B. For Whisper large-v3 and Universal-1, the maximum possible batch size within memory bounds was used for each model.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‚Ä£ 4.5 Inference latency ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the latency benchmarking experimental results. For the short-form audio clips, our model achieved 43% and 60% relative speed-up compared to Whisper large-v3 and Canary-1B, respectively. These speed gains can be attributed to the smaller parameter count, the use of chunk-wise attention in the encoder, and the relative compactness of a Transducer decoder compared to Transformer decoders.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">The inference efficiency of Universal-1 is further enhanced when transcribing long-form audio content. Even without batched inference, we observed 68% and 87% relative RTF reduction compared to Whisper large-v3 and Canary-1B, respectively. With batched inference, our model achieved a 5-fold RTF reduction in comparison to Whisper large-v3. The substantial RTF gain can be primarily attributed to the improved decoder speed, which greatly benefited from the parallelized inference. Notably, we observed an increase in encoder RTF with batched inference for both the Whisper large-v3 and Universal-1 models. We attribute this phenomenon to a computational bottleneck with the the T4 GPU hardware used in this experiment.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Inference latency of Universal-1, Whisper large-v3, and Canary-1B on short-form (LibriSpeech test-other) and long-form (TED-LIUM3) audio. Latency is measured in terms of Real Time Factor (RTF). Lower is better; best results are highlighted in <span id="S4.T5.4.1" class="ltx_text ltx_font_bold">bold</span>. For decoding short-form audio, batch size was always set to 1. For decoding long-form audio, the maximum batch size that fit into memory of the Nvidia T4 GPU was selected for each model.</figcaption>
<div id="S4.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:142.7pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.5pt,1.1pt) scale(0.984069708198537,0.984069708198537) ;">
<table id="S4.T5.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.2.2.3" class="ltx_tr">
<td id="S4.T5.2.2.3.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="3"><span id="S4.T5.2.2.3.1.1" class="ltx_text">Model</span></td>
<td id="S4.T5.2.2.3.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="3"><span id="S4.T5.2.2.3.2.1" class="ltx_text">
<span id="S4.T5.2.2.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.2.2.3.2.1.1.1" class="ltx_tr">
<span id="S4.T5.2.2.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Model</span></span>
<span id="S4.T5.2.2.3.2.1.1.2" class="ltx_tr">
<span id="S4.T5.2.2.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">size</span></span>
</span></span></td>
<td id="S4.T5.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">LibriSpeech test-other</td>
<td id="S4.T5.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">TED-LIUM3</td>
</tr>
<tr id="S4.T5.2.2.2" class="ltx_tr">
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3">RTF (<math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 10^{-3}" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T5.1.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.1.cmml">√ó</mo><msup id="S4.T5.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T5.1.1.1.1.m1.1.1.3.2" xref="S4.T5.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T5.1.1.1.1.m1.1.1.3.3" xref="S4.T5.1.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T5.1.1.1.1.m1.1.1.3.3a" xref="S4.T5.1.1.1.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.T5.1.1.1.1.m1.1.1.3.3.2" xref="S4.T5.1.1.1.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1"><times id="S4.T5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T5.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S4.T5.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T5.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T5.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T5.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3.3"><minus id="S4.T5.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T5.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\times 10^{-3}</annotation></semantics></math>)</td>
<td id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T5.2.2.2.3.1" class="ltx_text">
<span id="S4.T5.2.2.2.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.2.2.2.3.1.1.1" class="ltx_tr">
<span id="S4.T5.2.2.2.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Batch</span></span>
<span id="S4.T5.2.2.2.3.1.1.2" class="ltx_tr">
<span id="S4.T5.2.2.2.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">size</span></span>
</span></span></td>
<td id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3">RTF (<math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\times 10^{-3}" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><mrow id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml"><mi id="S4.T5.2.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T5.2.2.2.2.m1.1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.1.cmml">√ó</mo><msup id="S4.T5.2.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.2.m1.1.1.3.cmml"><mn id="S4.T5.2.2.2.2.m1.1.1.3.2" xref="S4.T5.2.2.2.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T5.2.2.2.2.m1.1.1.3.3" xref="S4.T5.2.2.2.2.m1.1.1.3.3.cmml"><mo id="S4.T5.2.2.2.2.m1.1.1.3.3a" xref="S4.T5.2.2.2.2.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.T5.2.2.2.2.m1.1.1.3.3.2" xref="S4.T5.2.2.2.2.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1"><times id="S4.T5.2.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T5.2.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.2.m1.1.1.2">absent</csymbol><apply id="S4.T5.2.2.2.2.m1.1.1.3.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T5.2.2.2.2.m1.1.1.3.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T5.2.2.2.2.m1.1.1.3.2.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3.2">10</cn><apply id="S4.T5.2.2.2.2.m1.1.1.3.3.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3.3"><minus id="S4.T5.2.2.2.2.m1.1.1.3.3.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T5.2.2.2.2.m1.1.1.3.3.2.cmml" xref="S4.T5.2.2.2.2.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\times 10^{-3}</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T5.2.2.4" class="ltx_tr">
<td id="S4.T5.2.2.4.1" class="ltx_td ltx_align_center ltx_border_t">Encoder</td>
<td id="S4.T5.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">Decoder</td>
<td id="S4.T5.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">Total</td>
<td id="S4.T5.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">Encoder</td>
<td id="S4.T5.2.2.4.5" class="ltx_td ltx_align_center ltx_border_t">Decoder</td>
<td id="S4.T5.2.2.4.6" class="ltx_td ltx_align_center ltx_border_t">Total</td>
</tr>
<tr id="S4.T5.2.2.5" class="ltx_tr">
<td id="S4.T5.2.2.5.1" class="ltx_td ltx_align_left ltx_border_t">Canary-1B</td>
<td id="S4.T5.2.2.5.2" class="ltx_td ltx_align_center ltx_border_t">1B</td>
<td id="S4.T5.2.2.5.3" class="ltx_td ltx_align_center ltx_border_t">14.8</td>
<td id="S4.T5.2.2.5.4" class="ltx_td ltx_align_center ltx_border_t">135.1</td>
<td id="S4.T5.2.2.5.5" class="ltx_td ltx_align_center ltx_border_t">149.9</td>
<td id="S4.T5.2.2.5.6" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T5.2.2.5.7" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
<td id="S4.T5.2.2.5.8" class="ltx_td ltx_align_center ltx_border_t">142.9</td>
<td id="S4.T5.2.2.5.9" class="ltx_td ltx_align_center ltx_border_t">149.6</td>
</tr>
<tr id="S4.T5.2.2.6" class="ltx_tr">
<td id="S4.T5.2.2.6.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S4.T5.2.2.6.1.1" class="ltx_text">Whisper large-v3</span></td>
<td id="S4.T5.2.2.6.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T5.2.2.6.2.1" class="ltx_text">1.55B</span></td>
<td id="S4.T5.2.2.6.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T5.2.2.6.3.1" class="ltx_text">43.3</span></td>
<td id="S4.T5.2.2.6.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T5.2.2.6.4.1" class="ltx_text">61.0</span></td>
<td id="S4.T5.2.2.6.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T5.2.2.6.5.1" class="ltx_text">104.3</span></td>
<td id="S4.T5.2.2.6.6" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T5.2.2.6.7" class="ltx_td ltx_align_center ltx_border_t">9.7</td>
<td id="S4.T5.2.2.6.8" class="ltx_td ltx_align_center ltx_border_t">50.2</td>
<td id="S4.T5.2.2.6.9" class="ltx_td ltx_align_center ltx_border_t">59.9</td>
</tr>
<tr id="S4.T5.2.2.7" class="ltx_tr">
<td id="S4.T5.2.2.7.1" class="ltx_td ltx_align_center">24</td>
<td id="S4.T5.2.2.7.2" class="ltx_td ltx_align_center">10.0</td>
<td id="S4.T5.2.2.7.3" class="ltx_td ltx_align_center">19.8</td>
<td id="S4.T5.2.2.7.4" class="ltx_td ltx_align_center">29.7</td>
</tr>
<tr id="S4.T5.2.2.8" class="ltx_tr">
<td id="S4.T5.2.2.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T5.2.2.8.1.1" class="ltx_text">Universal-1</span></td>
<td id="S4.T5.2.2.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T5.2.2.8.2.1" class="ltx_text">600M</span></td>
<td id="S4.T5.2.2.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T5.2.2.8.3.1" class="ltx_text ltx_font_bold">9.4</span></td>
<td id="S4.T5.2.2.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T5.2.2.8.4.1" class="ltx_text ltx_font_bold">50.6</span></td>
<td id="S4.T5.2.2.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T5.2.2.8.5.1" class="ltx_text ltx_font_bold">60.0</span></td>
<td id="S4.T5.2.2.8.6" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T5.2.2.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.2.8.7.1" class="ltx_text ltx_font_bold">2.3</span></td>
<td id="S4.T5.2.2.8.8" class="ltx_td ltx_align_center ltx_border_t">16.6</td>
<td id="S4.T5.2.2.8.9" class="ltx_td ltx_align_center ltx_border_t">18.9</td>
</tr>
<tr id="S4.T5.2.2.9" class="ltx_tr">
<td id="S4.T5.2.2.9.1" class="ltx_td ltx_align_center ltx_border_bb">64</td>
<td id="S4.T5.2.2.9.2" class="ltx_td ltx_align_center ltx_border_bb">3.2</td>
<td id="S4.T5.2.2.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.2.9.3.1" class="ltx_text ltx_font_bold">2.5</span></td>
<td id="S4.T5.2.2.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.2.2.9.4.1" class="ltx_text ltx_font_bold">5.7</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Hallucination analysis</h3>

<section id="S4.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.1 </span>Definition</h4>

<div id="S4.SS6.SSS1.p1" class="ltx_para">
<p id="S4.SS6.SSS1.p1.1" class="ltx_p">Due to the ongoing amalgamation of deep learning (DL) methods for audio and text processing, the phenomenon of hallucinations, originally discovered in NLP tasks, has become increasingly more common in the ASR field. Anecdotally, the Whisper large-v3 model has particularly been demonstrated to suffer from an increased propensity for hallucinations compared to its predecessors, despite substantial improvements in WER<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span><a target="_blank" href="https://github.com/openai/whisper/discussions/1762#discussioncomment-7557044" title="" class="ltx_ref ltx_href">https://github.com/openai/whisper/discussions/1762#discussioncomment-7557044</a></span></span></span>. However, there have been limited attempts thus far to study this emerging behaviour of modern ASR systems quantitatively. In this section, we propose evaluation metrics for quantitative analysis of ASR models with respect to their susceptibility to hallucinations.</p>
</div>
<div id="S4.SS6.SSS1.p2" class="ltx_para">
<p id="S4.SS6.SSS1.p2.1" class="ltx_p">In the context of ASR, hallucinations occur when the ASR model continuously produces words that are not grounded in the input audio or when it disregards long spoken segments. They can be measured by the number of consecutive transcription errors that the model makes in comparison to the reference transcripts. Following the edit operation concept in edit distance, such errors can be further divided into subgroups such as <span id="S4.SS6.SSS1.p2.1.1" class="ltx_text ltx_font_italic">fabrication</span> errors consisting of consecutive insertions or substitutions, and <span id="S4.SS6.SSS1.p2.1.2" class="ltx_text ltx_font_italic">omission</span> errors consisting of consecutive deletions. Based on this classification of consecutive errors, we define the corresponding consecutive error rates for a detailed analysis of hallucinations as follows:</p>
</div>
<div id="S4.SS6.SSS1.p3" class="ltx_para">
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.2" class="ltx_p">Fabrication rate (<math id="S4.I3.i1.p1.1.m1.1" class="ltx_Math" alttext="{F\!R}_{N}" display="inline"><semantics id="S4.I3.i1.p1.1.m1.1a"><mrow id="S4.I3.i1.p1.1.m1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.cmml"><mpadded width="0.612em"><mi id="S4.I3.i1.p1.1.m1.1.1.2" xref="S4.I3.i1.p1.1.m1.1.1.2.cmml">F</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.I3.i1.p1.1.m1.1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.I3.i1.p1.1.m1.1.1.3" xref="S4.I3.i1.p1.1.m1.1.1.3.cmml"><mi id="S4.I3.i1.p1.1.m1.1.1.3.2" xref="S4.I3.i1.p1.1.m1.1.1.3.2.cmml">R</mi><mi id="S4.I3.i1.p1.1.m1.1.1.3.3" xref="S4.I3.i1.p1.1.m1.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><apply id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1"><times id="S4.I3.i1.p1.1.m1.1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1"></times><ci id="S4.I3.i1.p1.1.m1.1.1.2.cmml" xref="S4.I3.i1.p1.1.m1.1.1.2">ùêπ</ci><apply id="S4.I3.i1.p1.1.m1.1.1.3.cmml" xref="S4.I3.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.I3.i1.p1.1.m1.1.1.3.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.I3.i1.p1.1.m1.1.1.3.2.cmml" xref="S4.I3.i1.p1.1.m1.1.1.3.2">ùëÖ</ci><ci id="S4.I3.i1.p1.1.m1.1.1.3.3.cmml" xref="S4.I3.i1.p1.1.m1.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">{F\!R}_{N}</annotation></semantics></math>): number of <math id="S4.I3.i1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I3.i1.p1.2.m2.1a"><mi id="S4.I3.i1.p1.2.m2.1.1" xref="S4.I3.i1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><ci id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">N</annotation></semantics></math> or more consecutive insertion or substitution errors observed per hour.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.2" class="ltx_p">Omission rate (<math id="S4.I3.i2.p1.1.m1.1" class="ltx_Math" alttext="{O\!R}_{N}" display="inline"><semantics id="S4.I3.i2.p1.1.m1.1a"><mrow id="S4.I3.i2.p1.1.m1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.cmml"><mpadded width="0.621em"><mi id="S4.I3.i2.p1.1.m1.1.1.2" xref="S4.I3.i2.p1.1.m1.1.1.2.cmml">O</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.I3.i2.p1.1.m1.1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.I3.i2.p1.1.m1.1.1.3" xref="S4.I3.i2.p1.1.m1.1.1.3.cmml"><mi id="S4.I3.i2.p1.1.m1.1.1.3.2" xref="S4.I3.i2.p1.1.m1.1.1.3.2.cmml">R</mi><mi id="S4.I3.i2.p1.1.m1.1.1.3.3" xref="S4.I3.i2.p1.1.m1.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><apply id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1"><times id="S4.I3.i2.p1.1.m1.1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1.1"></times><ci id="S4.I3.i2.p1.1.m1.1.1.2.cmml" xref="S4.I3.i2.p1.1.m1.1.1.2">ùëÇ</ci><apply id="S4.I3.i2.p1.1.m1.1.1.3.cmml" xref="S4.I3.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.I3.i2.p1.1.m1.1.1.3.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.I3.i2.p1.1.m1.1.1.3.2.cmml" xref="S4.I3.i2.p1.1.m1.1.1.3.2">ùëÖ</ci><ci id="S4.I3.i2.p1.1.m1.1.1.3.3.cmml" xref="S4.I3.i2.p1.1.m1.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">{O\!R}_{N}</annotation></semantics></math>): number of <math id="S4.I3.i2.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I3.i2.p1.2.m2.1a"><mi id="S4.I3.i2.p1.2.m2.1.1" xref="S4.I3.i2.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><ci id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">N</annotation></semantics></math> or more consecutive deletion errors observed per hour.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.2" class="ltx_p">Hallucination rate (<math id="S4.I3.i3.p1.1.m1.1" class="ltx_Math" alttext="{H\!R}_{N}" display="inline"><semantics id="S4.I3.i3.p1.1.m1.1a"><mrow id="S4.I3.i3.p1.1.m1.1.1" xref="S4.I3.i3.p1.1.m1.1.1.cmml"><mpadded width="0.742em"><mi id="S4.I3.i3.p1.1.m1.1.1.2" xref="S4.I3.i3.p1.1.m1.1.1.2.cmml">H</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.I3.i3.p1.1.m1.1.1.1" xref="S4.I3.i3.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.I3.i3.p1.1.m1.1.1.3" xref="S4.I3.i3.p1.1.m1.1.1.3.cmml"><mi id="S4.I3.i3.p1.1.m1.1.1.3.2" xref="S4.I3.i3.p1.1.m1.1.1.3.2.cmml">R</mi><mi id="S4.I3.i3.p1.1.m1.1.1.3.3" xref="S4.I3.i3.p1.1.m1.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i3.p1.1.m1.1b"><apply id="S4.I3.i3.p1.1.m1.1.1.cmml" xref="S4.I3.i3.p1.1.m1.1.1"><times id="S4.I3.i3.p1.1.m1.1.1.1.cmml" xref="S4.I3.i3.p1.1.m1.1.1.1"></times><ci id="S4.I3.i3.p1.1.m1.1.1.2.cmml" xref="S4.I3.i3.p1.1.m1.1.1.2">ùêª</ci><apply id="S4.I3.i3.p1.1.m1.1.1.3.cmml" xref="S4.I3.i3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.I3.i3.p1.1.m1.1.1.3.1.cmml" xref="S4.I3.i3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.I3.i3.p1.1.m1.1.1.3.2.cmml" xref="S4.I3.i3.p1.1.m1.1.1.3.2">ùëÖ</ci><ci id="S4.I3.i3.p1.1.m1.1.1.3.3.cmml" xref="S4.I3.i3.p1.1.m1.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i3.p1.1.m1.1c">{H\!R}_{N}</annotation></semantics></math>): number of <math id="S4.I3.i3.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I3.i3.p1.2.m2.1a"><mi id="S4.I3.i3.p1.2.m2.1.1" xref="S4.I3.i3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I3.i3.p1.2.m2.1b"><ci id="S4.I3.i3.p1.2.m2.1.1.cmml" xref="S4.I3.i3.p1.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i3.p1.2.m2.1c">N</annotation></semantics></math> or more consecutive insertion, substitution or deletions errors observed per hour.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.2 </span>Results on speech audio</h4>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2404.09841/assets/Figures/absolute_hallucination_rates.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="320" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Occurrences of five or more consecutive errors of each type per hour for Universal-1, Canary-1B and Whisper large-v3 models.</figcaption>
</figure>
<div id="S4.SS6.SSS2.p1" class="ltx_para">
<p id="S4.SS6.SSS2.p1.6" class="ltx_p"><a href="#S4.F4" title="In 4.6.2 Results on speech audio ‚Ä£ 4.6 Hallucination analysis ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> and¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.6.2 Results on speech audio ‚Ä£ 4.6 Hallucination analysis ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show our hallucination analysis results obtained from 146 hours of audio from a diverse set of English datasets<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>The datasets used largely overlap with our English test sets listed in Appendix <a href="#A2" title="Appendix B Evaluation Datasets ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></span></span>. We compared our Universal-1 model, Whisper large-v3, and Canary-1B. <a href="#S4.F4" title="In 4.6.2 Results on speech audio ‚Ä£ 4.6 Hallucination analysis ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> shows the rates of fabrications, omissions, and hallucinations with 5+ consecutive errors, i.e., <math id="S4.SS6.SSS2.p1.1.m1.1" class="ltx_Math" alttext="{F\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.1.m1.1a"><mrow id="S4.SS6.SSS2.p1.1.m1.1.1" xref="S4.SS6.SSS2.p1.1.m1.1.1.cmml"><mpadded width="0.612em"><mi id="S4.SS6.SSS2.p1.1.m1.1.1.2" xref="S4.SS6.SSS2.p1.1.m1.1.1.2.cmml">F</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.1.m1.1.1.1" xref="S4.SS6.SSS2.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.1.m1.1.1.3" xref="S4.SS6.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.1.m1.1.1.3.2" xref="S4.SS6.SSS2.p1.1.m1.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.1.m1.1.1.3.3" xref="S4.SS6.SSS2.p1.1.m1.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.1.m1.1b"><apply id="S4.SS6.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1"><times id="S4.SS6.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.1"></times><ci id="S4.SS6.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.2">ùêπ</ci><apply id="S4.SS6.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.1.m1.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.1.m1.1c">{F\!R}_{5}</annotation></semantics></math>, <math id="S4.SS6.SSS2.p1.2.m2.1" class="ltx_Math" alttext="{O\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.2.m2.1a"><mrow id="S4.SS6.SSS2.p1.2.m2.1.1" xref="S4.SS6.SSS2.p1.2.m2.1.1.cmml"><mpadded width="0.621em"><mi id="S4.SS6.SSS2.p1.2.m2.1.1.2" xref="S4.SS6.SSS2.p1.2.m2.1.1.2.cmml">O</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.2.m2.1.1.1" xref="S4.SS6.SSS2.p1.2.m2.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.2.m2.1.1.3" xref="S4.SS6.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS6.SSS2.p1.2.m2.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS6.SSS2.p1.2.m2.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.2.m2.1b"><apply id="S4.SS6.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1"><times id="S4.SS6.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.1"></times><ci id="S4.SS6.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.2">ùëÇ</ci><apply id="S4.SS6.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.2.m2.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.2.m2.1c">{O\!R}_{5}</annotation></semantics></math>, and <math id="S4.SS6.SSS2.p1.3.m3.1" class="ltx_Math" alttext="{H\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.3.m3.1a"><mrow id="S4.SS6.SSS2.p1.3.m3.1.1" xref="S4.SS6.SSS2.p1.3.m3.1.1.cmml"><mpadded width="0.742em"><mi id="S4.SS6.SSS2.p1.3.m3.1.1.2" xref="S4.SS6.SSS2.p1.3.m3.1.1.2.cmml">H</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.3.m3.1.1.1" xref="S4.SS6.SSS2.p1.3.m3.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.3.m3.1.1.3" xref="S4.SS6.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.3.m3.1.1.3.2" xref="S4.SS6.SSS2.p1.3.m3.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.3.m3.1.1.3.3" xref="S4.SS6.SSS2.p1.3.m3.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.3.m3.1b"><apply id="S4.SS6.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1"><times id="S4.SS6.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.1"></times><ci id="S4.SS6.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.2">ùêª</ci><apply id="S4.SS6.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.3.m3.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.3.m3.1c">{H\!R}_{5}</annotation></semantics></math>, respectively. Both Whisper large-v3 and Canary-1B turned out to be similarly susceptible to compounding insertion and substitution errors spanning 5 or more words. On the other hand, our model was much more robust to this type of error, showing a 41% relative reduction in <math id="S4.SS6.SSS2.p1.4.m4.1" class="ltx_Math" alttext="{F\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.4.m4.1a"><mrow id="S4.SS6.SSS2.p1.4.m4.1.1" xref="S4.SS6.SSS2.p1.4.m4.1.1.cmml"><mpadded width="0.612em"><mi id="S4.SS6.SSS2.p1.4.m4.1.1.2" xref="S4.SS6.SSS2.p1.4.m4.1.1.2.cmml">F</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.4.m4.1.1.1" xref="S4.SS6.SSS2.p1.4.m4.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.4.m4.1.1.3" xref="S4.SS6.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.4.m4.1.1.3.2" xref="S4.SS6.SSS2.p1.4.m4.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.4.m4.1.1.3.3" xref="S4.SS6.SSS2.p1.4.m4.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.4.m4.1b"><apply id="S4.SS6.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1"><times id="S4.SS6.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.1"></times><ci id="S4.SS6.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.2">ùêπ</ci><apply id="S4.SS6.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.4.m4.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.4.m4.1c">{F\!R}_{5}</annotation></semantics></math>. Additionally, it achieved a 21% relative reduction in <math id="S4.SS6.SSS2.p1.5.m5.1" class="ltx_Math" alttext="{O\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.5.m5.1a"><mrow id="S4.SS6.SSS2.p1.5.m5.1.1" xref="S4.SS6.SSS2.p1.5.m5.1.1.cmml"><mpadded width="0.621em"><mi id="S4.SS6.SSS2.p1.5.m5.1.1.2" xref="S4.SS6.SSS2.p1.5.m5.1.1.2.cmml">O</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.5.m5.1.1.1" xref="S4.SS6.SSS2.p1.5.m5.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.5.m5.1.1.3" xref="S4.SS6.SSS2.p1.5.m5.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.5.m5.1.1.3.2" xref="S4.SS6.SSS2.p1.5.m5.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.5.m5.1.1.3.3" xref="S4.SS6.SSS2.p1.5.m5.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.5.m5.1b"><apply id="S4.SS6.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1"><times id="S4.SS6.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.1"></times><ci id="S4.SS6.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.2">ùëÇ</ci><apply id="S4.SS6.SSS2.p1.5.m5.1.1.3.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.5.m5.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.5.m5.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.5.m5.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.5.m5.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.5.m5.1c">{O\!R}_{5}</annotation></semantics></math> compared to Whisper large-v3, while being 10% more prone to consecutive deletions relative to Canary-1B. Overall, we obtained 30% and 22% relative reductions in <math id="S4.SS6.SSS2.p1.6.m6.1" class="ltx_Math" alttext="{H\!R}_{5}" display="inline"><semantics id="S4.SS6.SSS2.p1.6.m6.1a"><mrow id="S4.SS6.SSS2.p1.6.m6.1.1" xref="S4.SS6.SSS2.p1.6.m6.1.1.cmml"><mpadded width="0.742em"><mi id="S4.SS6.SSS2.p1.6.m6.1.1.2" xref="S4.SS6.SSS2.p1.6.m6.1.1.2.cmml">H</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p1.6.m6.1.1.1" xref="S4.SS6.SSS2.p1.6.m6.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p1.6.m6.1.1.3" xref="S4.SS6.SSS2.p1.6.m6.1.1.3.cmml"><mi id="S4.SS6.SSS2.p1.6.m6.1.1.3.2" xref="S4.SS6.SSS2.p1.6.m6.1.1.3.2.cmml">R</mi><mn id="S4.SS6.SSS2.p1.6.m6.1.1.3.3" xref="S4.SS6.SSS2.p1.6.m6.1.1.3.3.cmml">5</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p1.6.m6.1b"><apply id="S4.SS6.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1"><times id="S4.SS6.SSS2.p1.6.m6.1.1.1.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.1"></times><ci id="S4.SS6.SSS2.p1.6.m6.1.1.2.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.2">ùêª</ci><apply id="S4.SS6.SSS2.p1.6.m6.1.1.3.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.3.2">ùëÖ</ci><cn type="integer" id="S4.SS6.SSS2.p1.6.m6.1.1.3.3.cmml" xref="S4.SS6.SSS2.p1.6.m6.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p1.6.m6.1c">{H\!R}_{5}</annotation></semantics></math> compared to Whisper large-v3 and Canary-1B, respectively.</p>
</div>
<div id="S4.SS6.SSS2.p2" class="ltx_para">
<p id="S4.SS6.SSS2.p2.8" class="ltx_p">In <a href="#S4.F5" title="In 4.6.2 Results on speech audio ‚Ä£ 4.6 Hallucination analysis ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">5</span></a>, we vary the consecutive error span, <math id="S4.SS6.SSS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS6.SSS2.p2.1.m1.1a"><mi id="S4.SS6.SSS2.p2.1.m1.1.1" xref="S4.SS6.SSS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.1.m1.1b"><ci id="S4.SS6.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS6.SSS2.p2.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.1.m1.1c">N</annotation></semantics></math>, from 1 to 9. For each <math id="S4.SS6.SSS2.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS6.SSS2.p2.2.m2.1a"><mi id="S4.SS6.SSS2.p2.2.m2.1.1" xref="S4.SS6.SSS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.2.m2.1b"><ci id="S4.SS6.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS6.SSS2.p2.2.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.2.m2.1c">N</annotation></semantics></math> value, we show the relative reduction rates in <math id="S4.SS6.SSS2.p2.3.m3.1" class="ltx_Math" alttext="{F\!R}_{N}" display="inline"><semantics id="S4.SS6.SSS2.p2.3.m3.1a"><mrow id="S4.SS6.SSS2.p2.3.m3.1.1" xref="S4.SS6.SSS2.p2.3.m3.1.1.cmml"><mpadded width="0.612em"><mi id="S4.SS6.SSS2.p2.3.m3.1.1.2" xref="S4.SS6.SSS2.p2.3.m3.1.1.2.cmml">F</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p2.3.m3.1.1.1" xref="S4.SS6.SSS2.p2.3.m3.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p2.3.m3.1.1.3" xref="S4.SS6.SSS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS6.SSS2.p2.3.m3.1.1.3.2" xref="S4.SS6.SSS2.p2.3.m3.1.1.3.2.cmml">R</mi><mi id="S4.SS6.SSS2.p2.3.m3.1.1.3.3" xref="S4.SS6.SSS2.p2.3.m3.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.3.m3.1b"><apply id="S4.SS6.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1"><times id="S4.SS6.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.1"></times><ci id="S4.SS6.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.2">ùêπ</ci><apply id="S4.SS6.SSS2.p2.3.m3.1.1.3.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.3.2">ùëÖ</ci><ci id="S4.SS6.SSS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS6.SSS2.p2.3.m3.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.3.m3.1c">{F\!R}_{N}</annotation></semantics></math>, <math id="S4.SS6.SSS2.p2.4.m4.1" class="ltx_Math" alttext="{O\!R}_{N}" display="inline"><semantics id="S4.SS6.SSS2.p2.4.m4.1a"><mrow id="S4.SS6.SSS2.p2.4.m4.1.1" xref="S4.SS6.SSS2.p2.4.m4.1.1.cmml"><mpadded width="0.621em"><mi id="S4.SS6.SSS2.p2.4.m4.1.1.2" xref="S4.SS6.SSS2.p2.4.m4.1.1.2.cmml">O</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p2.4.m4.1.1.1" xref="S4.SS6.SSS2.p2.4.m4.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p2.4.m4.1.1.3" xref="S4.SS6.SSS2.p2.4.m4.1.1.3.cmml"><mi id="S4.SS6.SSS2.p2.4.m4.1.1.3.2" xref="S4.SS6.SSS2.p2.4.m4.1.1.3.2.cmml">R</mi><mi id="S4.SS6.SSS2.p2.4.m4.1.1.3.3" xref="S4.SS6.SSS2.p2.4.m4.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.4.m4.1b"><apply id="S4.SS6.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1"><times id="S4.SS6.SSS2.p2.4.m4.1.1.1.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.1"></times><ci id="S4.SS6.SSS2.p2.4.m4.1.1.2.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.2">ùëÇ</ci><apply id="S4.SS6.SSS2.p2.4.m4.1.1.3.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.3.2">ùëÖ</ci><ci id="S4.SS6.SSS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS6.SSS2.p2.4.m4.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.4.m4.1c">{O\!R}_{N}</annotation></semantics></math>, and <math id="S4.SS6.SSS2.p2.5.m5.1" class="ltx_Math" alttext="{H\!R}_{N}" display="inline"><semantics id="S4.SS6.SSS2.p2.5.m5.1a"><mrow id="S4.SS6.SSS2.p2.5.m5.1.1" xref="S4.SS6.SSS2.p2.5.m5.1.1.cmml"><mpadded width="0.742em"><mi id="S4.SS6.SSS2.p2.5.m5.1.1.2" xref="S4.SS6.SSS2.p2.5.m5.1.1.2.cmml">H</mi></mpadded><mo lspace="0em" rspace="0em" id="S4.SS6.SSS2.p2.5.m5.1.1.1" xref="S4.SS6.SSS2.p2.5.m5.1.1.1.cmml">‚Äã</mo><msub id="S4.SS6.SSS2.p2.5.m5.1.1.3" xref="S4.SS6.SSS2.p2.5.m5.1.1.3.cmml"><mi id="S4.SS6.SSS2.p2.5.m5.1.1.3.2" xref="S4.SS6.SSS2.p2.5.m5.1.1.3.2.cmml">R</mi><mi id="S4.SS6.SSS2.p2.5.m5.1.1.3.3" xref="S4.SS6.SSS2.p2.5.m5.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.5.m5.1b"><apply id="S4.SS6.SSS2.p2.5.m5.1.1.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1"><times id="S4.SS6.SSS2.p2.5.m5.1.1.1.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.1"></times><ci id="S4.SS6.SSS2.p2.5.m5.1.1.2.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.2">ùêª</ci><apply id="S4.SS6.SSS2.p2.5.m5.1.1.3.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.SSS2.p2.5.m5.1.1.3.1.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS6.SSS2.p2.5.m5.1.1.3.2.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.3.2">ùëÖ</ci><ci id="S4.SS6.SSS2.p2.5.m5.1.1.3.3.cmml" xref="S4.SS6.SSS2.p2.5.m5.1.1.3.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.5.m5.1c">{H\!R}_{N}</annotation></semantics></math> achieved by Universal-1 in comparison to Whisper large-v3 and Canary-1B. For fabrication and hallucination errors, we observed consistent increases in the relative reduction rates with increasing <math id="S4.SS6.SSS2.p2.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS6.SSS2.p2.6.m6.1a"><mi id="S4.SS6.SSS2.p2.6.m6.1.1" xref="S4.SS6.SSS2.p2.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.6.m6.1b"><ci id="S4.SS6.SSS2.p2.6.m6.1.1.cmml" xref="S4.SS6.SSS2.p2.6.m6.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.6.m6.1c">N</annotation></semantics></math>. For very long consecutive errors spanning 9 or more words, Universal-1 achieved a 50+% relative reduction in the fabrication rate and an <math id="S4.SS6.SSS2.p2.7.m7.1" class="ltx_Math" alttext="\approx\!40\%" display="inline"><semantics id="S4.SS6.SSS2.p2.7.m7.1a"><mrow id="S4.SS6.SSS2.p2.7.m7.1.1" xref="S4.SS6.SSS2.p2.7.m7.1.1.cmml"><mi id="S4.SS6.SSS2.p2.7.m7.1.1.2" xref="S4.SS6.SSS2.p2.7.m7.1.1.2.cmml"></mi><mo rspace="0.108em" id="S4.SS6.SSS2.p2.7.m7.1.1.1" xref="S4.SS6.SSS2.p2.7.m7.1.1.1.cmml">‚âà</mo><mrow id="S4.SS6.SSS2.p2.7.m7.1.1.3" xref="S4.SS6.SSS2.p2.7.m7.1.1.3.cmml"><mn id="S4.SS6.SSS2.p2.7.m7.1.1.3.2" xref="S4.SS6.SSS2.p2.7.m7.1.1.3.2.cmml">40</mn><mo id="S4.SS6.SSS2.p2.7.m7.1.1.3.1" xref="S4.SS6.SSS2.p2.7.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.7.m7.1b"><apply id="S4.SS6.SSS2.p2.7.m7.1.1.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1"><approx id="S4.SS6.SSS2.p2.7.m7.1.1.1.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1.1"></approx><csymbol cd="latexml" id="S4.SS6.SSS2.p2.7.m7.1.1.2.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1.2">absent</csymbol><apply id="S4.SS6.SSS2.p2.7.m7.1.1.3.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1.3"><csymbol cd="latexml" id="S4.SS6.SSS2.p2.7.m7.1.1.3.1.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS6.SSS2.p2.7.m7.1.1.3.2.cmml" xref="S4.SS6.SSS2.p2.7.m7.1.1.3.2">40</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.7.m7.1c">\approx\!40\%</annotation></semantics></math> relative reduction in the hallucination rate over Whisper large-v3 and Canary-1B. Interestingly, while Canary-1B was more resilient to omissions than Universal-1 for shorter consecutive errors, this trend reversed for <math id="S4.SS6.SSS2.p2.8.m8.1" class="ltx_Math" alttext="N\geq 6" display="inline"><semantics id="S4.SS6.SSS2.p2.8.m8.1a"><mrow id="S4.SS6.SSS2.p2.8.m8.1.1" xref="S4.SS6.SSS2.p2.8.m8.1.1.cmml"><mi id="S4.SS6.SSS2.p2.8.m8.1.1.2" xref="S4.SS6.SSS2.p2.8.m8.1.1.2.cmml">N</mi><mo id="S4.SS6.SSS2.p2.8.m8.1.1.1" xref="S4.SS6.SSS2.p2.8.m8.1.1.1.cmml">‚â•</mo><mn id="S4.SS6.SSS2.p2.8.m8.1.1.3" xref="S4.SS6.SSS2.p2.8.m8.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p2.8.m8.1b"><apply id="S4.SS6.SSS2.p2.8.m8.1.1.cmml" xref="S4.SS6.SSS2.p2.8.m8.1.1"><geq id="S4.SS6.SSS2.p2.8.m8.1.1.1.cmml" xref="S4.SS6.SSS2.p2.8.m8.1.1.1"></geq><ci id="S4.SS6.SSS2.p2.8.m8.1.1.2.cmml" xref="S4.SS6.SSS2.p2.8.m8.1.1.2">ùëÅ</ci><cn type="integer" id="S4.SS6.SSS2.p2.8.m8.1.1.3.cmml" xref="S4.SS6.SSS2.p2.8.m8.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p2.8.m8.1c">N\geq 6</annotation></semantics></math>. As a result, we observed a 20+% relative reduction in the omission rate for 9 or more consecutive deletions in comparison to both Whisper large-v3 and Canary-1B.</p>
</div>
<div id="S4.SS6.SSS2.p3" class="ltx_para">
<p id="S4.SS6.SSS2.p3.1" class="ltx_p">We would attribute the reduced propensity for hallucinations of Universal-1 to the use of a Transducer-based model architecture, as opposed to an encoder-decoder architecture employed by Whisper and Canary-1B, and to our extensive data filtering pipelines for removing training samples with low-quality reference transcriptions. As discussed before in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, discriminative sequence models have different levels of susceptibility to the label bias problem, which is characterized by the overreliance on previously emitted labels while disregarding new evidence. We hypothesize that the hallucination problem in ASR is partly caused by label bias. Thus, RNN-T could be less susceptible to the aforementioned problem, since its auto-regressive decoder is smaller than in encoder-decoder models.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2404.09841/assets/Figures/relative_hallucination_rates.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="320" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Rel. reduction of <math id="S4.F5.3.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.F5.3.m1.1b"><mi id="S4.F5.3.m1.1.1" xref="S4.F5.3.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.F5.3.m1.1c"><ci id="S4.F5.3.m1.1.1.cmml" xref="S4.F5.3.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.3.m1.1d">N</annotation></semantics></math> or more consecutive errors of each type per hour of Universal-1 in comparison to Whisper large-v3 and Canary-1B models for different <math id="S4.F5.4.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.F5.4.m2.1b"><mi id="S4.F5.4.m2.1.1" xref="S4.F5.4.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.F5.4.m2.1c"><ci id="S4.F5.4.m2.1.1.cmml" xref="S4.F5.4.m2.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.4.m2.1d">N</annotation></semantics></math>s.</figcaption>
</figure>
</section>
<section id="S4.SS6.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.3 </span>Fabrication during ambient noise</h4>

<div id="S4.SS6.SSS3.p1" class="ltx_para">
<p id="S4.SS6.SSS3.p1.1" class="ltx_p">We further analyzed the behaviors of these models on ambient noise data. ASR systems are supposed to produce empty text when applied to ambient noise. Therefore, we focus on the fabrications generated by the ASR models.</p>
</div>
<div id="S4.SS6.SSS3.p2" class="ltx_para">
<p id="S4.SS6.SSS3.p2.1" class="ltx_p">We utilized the AudioSet¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and the DNC¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> datasets. Specifically, we obtained 200 audio samples from each dataset, while excluding any sound categories that might contain human speech. The filtering was performed based on the ontology provided by each dataset, removing categories like <em id="S4.SS6.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">singing</em> or <em id="S4.SS6.SSS3.p2.1.2" class="ltx_emph ltx_font_italic">conversation</em>. As shown in Table <a href="#S4.T6" title="Table 6 ‚Ä£ 4.6.3 Fabrication during ambient noise ‚Ä£ 4.6 Hallucination analysis ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Whisper large-v3 and Canary-1B, both based on an encoder-decoder architecture, almost always generated some text for these samples. In comparison, our model produced a blank output around 90% of the time. For Canary-1B, most of the generated text consisted of short words like ‚Äúand‚Äù, while Whisper would often output strings of repeated or unintelligible special symbols. The most severe cases‚Äîand the hardest ones to filter out with post-processing‚Äîare those where the models output long plausible sequences, which Whisper large-v3 suffered from most frequently. Our model‚Äôs faulty outputs tended to be short, as shown in the table.</p>
</div>
<div id="S4.SS6.SSS3.p3" class="ltx_para">
<p id="S4.SS6.SSS3.p3.1" class="ltx_p">While a separate VAD method may be applied to remove non-speech segments from the input audio beforehand, aggressive VAD could result in increased omissions.
Therefore, any lack of robustness against ambient noise could still pose a challenge in practice.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Fabrication comparison of three ASR models on ambient noise. Best results are highlighted in bold.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S4.T6.1.2.3.1" class="ltx_text">Dataset</span></td>
<td id="S4.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="3">Model</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.3.2" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Whisper large-v3</td>
<td id="S4.T6.1.3.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Canary-1B</td>
<td id="S4.T6.1.3.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Universal-1</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S4.T6.1.4.1.1" class="ltx_text">Non-blank response rate</span></td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">AudioSet</td>
<td id="S4.T6.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="S4.T6.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="S4.T6.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.4.6.1" class="ltx_text ltx_font_bold">10.5%</span></td>
</tr>
<tr id="S4.T6.1.5" class="ltx_tr">
<td id="S4.T6.1.5.1" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S4.T6.1.5.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">DNC</td>
<td id="S4.T6.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">99.5%</td>
<td id="S4.T6.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">100%</td>
<td id="S4.T6.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.5.5.1" class="ltx_text ltx_font_bold">10.0%</span></td>
</tr>
<tr id="S4.T6.1.6" class="ltx_tr">
<td id="S4.T6.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="6"><span id="S4.T6.1.6.1.1" class="ltx_text">
<span id="S4.T6.1.6.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.1.6.1.1.1.1" class="ltx_tr">
<span id="S4.T6.1.6.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Number of characters</span></span>
<span id="S4.T6.1.6.1.1.1.2" class="ltx_tr">
<span id="S4.T6.1.6.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">in non-blank responses</span></span>
</span></span></td>
<td id="S4.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S4.T6.1.6.2.1" class="ltx_text">Mean</span></td>
<td id="S4.T6.1.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">AudioSet</td>
<td id="S4.T6.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">16</td>
<td id="S4.T6.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.0</td>
<td id="S4.T6.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.6.6.1" class="ltx_text ltx_font_bold">3.6</span></td>
</tr>
<tr id="S4.T6.1.7" class="ltx_tr">
<td id="S4.T6.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">DNC</td>
<td id="S4.T6.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.6</td>
<td id="S4.T6.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">87.0</td>
<td id="S4.T6.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.7.4.1" class="ltx_text ltx_font_bold">2.0</span></td>
</tr>
<tr id="S4.T6.1.8" class="ltx_tr">
<td id="S4.T6.1.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S4.T6.1.8.1.1" class="ltx_text">Median</span></td>
<td id="S4.T6.1.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">AudioSet</td>
<td id="S4.T6.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3</td>
<td id="S4.T6.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3</td>
<td id="S4.T6.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.8.5.1" class="ltx_text ltx_font_bold">2</span></td>
</tr>
<tr id="S4.T6.1.9" class="ltx_tr">
<td id="S4.T6.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">DNC</td>
<td id="S4.T6.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3</td>
<td id="S4.T6.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3</td>
<td id="S4.T6.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.9.4.1" class="ltx_text ltx_font_bold">2</span></td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S4.T6.1.1.1.1" class="ltx_text"><math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\geq 10" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml"><mi id="S4.T6.1.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S4.T6.1.1.1.1.m1.1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.1.cmml">‚â•</mo><mn id="S4.T6.1.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1"><geq id="S4.T6.1.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1.1"></geq><csymbol cd="latexml" id="S4.T6.1.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T6.1.1.1.1.m1.1.1.3.cmml" xref="S4.T6.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\geq 10</annotation></semantics></math></span></td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">AudioSet</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">35%</td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.5%</td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.1.5.1" class="ltx_text ltx_font_bold">0.5%</span></td>
</tr>
<tr id="S4.T6.1.10" class="ltx_tr">
<td id="S4.T6.1.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">DNC</td>
<td id="S4.T6.1.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">39.5%</td>
<td id="S4.T6.1.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">14%</td>
<td id="S4.T6.1.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T6.1.10.4.1" class="ltx_text ltx_font_bold">0%</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Timestamp estimation</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">Many real-world scenarios require accurate word-level timestamps in addition to high-quality transcriptions to build applications with ASR systems. In this section, we evaluate the word-level timestamp performance of our Universal-1 model and several other open-source models.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">For the timestamp prediction accuracy evaluation, we used a diverse test set, as listed in Appendix <a href="#A2.SS3" title="B.3 Timestamp estimation datasets ‚Ä£ Appendix B Evaluation Datasets ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>. To create reference word-level timestamps for the test audio files, we performed forced alignment with the Montreal Forced Aligner toolkit¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on the audio signal and the reference transcription. For each tested model, the ASR output was first aligned with the reference transcription. We calculated the difference between the reference and predicted timestamps for all words that were matched successfully between the reference and ASR transcriptions.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2404.09841/assets/Figures/timestamp_accuracy.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Word-level timestamp estimation accuracy for different models as a function of estimation error tolerance. A curve closer to the upper left corner indicates higher accuracy.</figcaption>
</figure>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p"><a href="#S4.F6" title="In 4.7 Timestamp estimation ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a> shows the word-level timestamp estimation accuracy as a function of an estimation error tolerance threshold. For each x-axis value, the corresponding y-axis value shows the percentage of words whose estimated timestamp falls within this threshold when compared to the reference timestamp. A curve that approaches the upper-left corner indicates a model with more accurate timestamp estimation. Universal-1 model produced more accurate timestamps than Whisper large-v3 by a significant margin. Additionally, we included two powerful open-source models, NeMo‚Äôs Parakeet RNN-T and CTC models with 600M parameters. The results highlight the advantage of RNN-T in terms of timestamp estimation, while our model still substantially outperformed Parakeet RNN-T.</p>
</div>
<div id="S4.SS7.p4" class="ltx_para">
<p id="S4.SS7.p4.1" class="ltx_p">The superior performance achieved by our model is intriguing because an RNN-T loss does not explicitly include any terms related to the accuracy of audio-to-token alignments¬†<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Nevertheless, a few observations can be made to describe this unexpected effectiveness. First, the use of bidirectional context by the encoder is essential. In our other experiments using a streaming Conformer encoder, the timestamp prediction accuracy dropped drastically, with significant delays compared to the reference timestamps. In the streaming scenario, an additional mechanism, such as the one proposed in <cite class="ltx_cite ltx_citemacro_citet">Zhao et¬†al. [<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, is necessary to compensate for the delays. Second, greedy decoding may be more beneficial for timestamp accuracy than beam search, which optimizes for transcription accuracy. Finally, the use of chunk-wise attention in the encoder may also help constrain the audio-to-token alignment space. Further research into these aspects will allow us to draw more decisive conclusions.</p>
</div>
<div id="S4.SS7.p5" class="ltx_para">
<p id="S4.SS7.p5.1" class="ltx_p">A constant offset of -65ms was applied in our implementation as we found that the raw timestamp estimates generated by our model were slightly biased across test samples. The offsetting constant was obtained as the median value of the biases computed over a subset of the data. <a href="#S4.F7" title="In 4.7 Timestamp estimation ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">7</span></a> shows the effect of the bias offset.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2404.09841/assets/Figures/timestamp_offset.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Effect of bias offset in word-level timestamp estimation.</figcaption>
</figure>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Impact of Pre-training</h3>

<figure id="S4.F8" class="ltx_figure"><img src="/html/2404.09841/assets/x3.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Fine-tuning WER trajectory when initialized from various pre-training checkpoints</figcaption>
</figure>
<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">Finally, we studied the impact of the amount of pre-training data on the learning trajectory of the fine-tuning stage. To do so, we trained CTC models starting from different pre-training checkpoints. For efficiency in the investigation, for each pre-trained checkpoint, we used the first 12 layers of the encoder, added a CTC decoder on top, and fine-tuned the model for 10k steps to assess performance and convergence speed. Thus, each model size was approximately 300M parameters.</p>
</div>
<div id="S4.SS8.p2" class="ltx_para">
<p id="S4.SS8.p2.1" class="ltx_p"><a href="#S4.F8" title="In 4.8 Impact of Pre-training ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> shows the learning curves of the individual training runs, each starting from a distinct pre-training checkpoint. The validation set consists of 1024 randomly chosen utterances from LibriSpeech and MLS test sets. We can observe that longer BEST-RQ training considerably improves the WER, with diminishing returns around 0.8 epochs on our data, corresponding to 10M hours of audio. Further test WERs are provided in Table <a href="#S4.T7" title="Table 7 ‚Ä£ 4.8 Impact of Pre-training ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, showing a similar performance trend. We also note that, without pre-training, our end-to-end ASR training did not converge. This indicates the usefulness of pre-training in stabilizing training models without extensive hyperparameter tuning.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>WERs (%) after 10k fine-tuning steps, starting from different pre-training checkpoints</figcaption>
<p id="S4.T7.1" class="ltx_p ltx_align_center">.



<span id="S4.T7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T7.1.1.1" class="ltx_tr">
<span id="S4.T7.1.1.1.1" class="ltx_td ltx_border_tt"></span>
<span id="S4.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_5">Pre-training epochs</span></span>
<span id="S4.T7.1.1.2" class="ltx_tr">
<span id="S4.T7.1.1.2.1" class="ltx_td"></span>
<span id="S4.T7.1.1.2.2" class="ltx_td ltx_align_center">0.2</span>
<span id="S4.T7.1.1.2.3" class="ltx_td ltx_align_center">0.4</span>
<span id="S4.T7.1.1.2.4" class="ltx_td ltx_align_center">0.6</span>
<span id="S4.T7.1.1.2.5" class="ltx_td ltx_align_center">0.8</span>
<span id="S4.T7.1.1.2.6" class="ltx_td ltx_align_center">1</span></span>
<span id="S4.T7.1.1.3" class="ltx_tr">
<span id="S4.T7.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Podcast</span>
<span id="S4.T7.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">13.0</span>
<span id="S4.T7.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">12.4</span>
<span id="S4.T7.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">12.3</span>
<span id="S4.T7.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">12.1</span>
<span id="S4.T7.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">12.2</span></span>
<span id="S4.T7.1.1.4" class="ltx_tr">
<span id="S4.T7.1.1.4.1" class="ltx_td ltx_align_left">Noisy</span>
<span id="S4.T7.1.1.4.2" class="ltx_td ltx_align_center">16.5</span>
<span id="S4.T7.1.1.4.3" class="ltx_td ltx_align_center">15.6</span>
<span id="S4.T7.1.1.4.4" class="ltx_td ltx_align_center">15.4</span>
<span id="S4.T7.1.1.4.5" class="ltx_td ltx_align_center">14.9</span>
<span id="S4.T7.1.1.4.6" class="ltx_td ltx_align_center">15.0</span></span>
<span id="S4.T7.1.1.5" class="ltx_tr">
<span id="S4.T7.1.1.5.1" class="ltx_td ltx_align_left">LibriSpeech test-clean</span>
<span id="S4.T7.1.1.5.2" class="ltx_td ltx_align_center">4.2</span>
<span id="S4.T7.1.1.5.3" class="ltx_td ltx_align_center">4.1</span>
<span id="S4.T7.1.1.5.4" class="ltx_td ltx_align_center">4.1</span>
<span id="S4.T7.1.1.5.5" class="ltx_td ltx_align_center">4.1</span>
<span id="S4.T7.1.1.5.6" class="ltx_td ltx_align_center">4.0</span></span>
<span id="S4.T7.1.1.6" class="ltx_tr">
<span id="S4.T7.1.1.6.1" class="ltx_td ltx_align_left">LibriSpeech test-other</span>
<span id="S4.T7.1.1.6.2" class="ltx_td ltx_align_center">9.3</span>
<span id="S4.T7.1.1.6.3" class="ltx_td ltx_align_center">8.9</span>
<span id="S4.T7.1.1.6.4" class="ltx_td ltx_align_center">8.8</span>
<span id="S4.T7.1.1.6.5" class="ltx_td ltx_align_center">8.5</span>
<span id="S4.T7.1.1.6.6" class="ltx_td ltx_align_center">8.5</span></span>
<span id="S4.T7.1.1.7" class="ltx_tr">
<span id="S4.T7.1.1.7.1" class="ltx_td ltx_align_left">MLS Spanish</span>
<span id="S4.T7.1.1.7.2" class="ltx_td ltx_align_center">7.9</span>
<span id="S4.T7.1.1.7.3" class="ltx_td ltx_align_center">7.8</span>
<span id="S4.T7.1.1.7.4" class="ltx_td ltx_align_center">7.6</span>
<span id="S4.T7.1.1.7.5" class="ltx_td ltx_align_center">7.7</span>
<span id="S4.T7.1.1.7.6" class="ltx_td ltx_align_center">7.5</span></span>
<span id="S4.T7.1.1.8" class="ltx_tr">
<span id="S4.T7.1.1.8.1" class="ltx_td ltx_align_left">MLS French</span>
<span id="S4.T7.1.1.8.2" class="ltx_td ltx_align_center">9.7</span>
<span id="S4.T7.1.1.8.3" class="ltx_td ltx_align_center">9.6</span>
<span id="S4.T7.1.1.8.4" class="ltx_td ltx_align_center">9.4</span>
<span id="S4.T7.1.1.8.5" class="ltx_td ltx_align_center">9.4</span>
<span id="S4.T7.1.1.8.6" class="ltx_td ltx_align_center">9.4</span></span>
<span id="S4.T7.1.1.9" class="ltx_tr">
<span id="S4.T7.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb">MLS German</span>
<span id="S4.T7.1.1.9.2" class="ltx_td ltx_align_center ltx_border_bb">10.5</span>
<span id="S4.T7.1.1.9.3" class="ltx_td ltx_align_center ltx_border_bb">10.3</span>
<span id="S4.T7.1.1.9.4" class="ltx_td ltx_align_center ltx_border_bb">10.1</span>
<span id="S4.T7.1.1.9.5" class="ltx_td ltx_align_center ltx_border_bb">10.1</span>
<span id="S4.T7.1.1.9.6" class="ltx_td ltx_align_center ltx_border_bb">10.0</span></span>
</span></p>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this comprehensive study, we have presented Universal-1, a highly capable multilingual ASR system developed for English, Spanish, German, and French. Our investigation has revealed that the combination of a vast pre-training dataset, meticulously curated fine-tuning corpora, and the employment of a Conformer RNN-T model architecture results in remarkably robust performance across languages and test sets while achieving significantly faster inference than state-of-the-art open-source models.
This achievement underscores the effectiveness of scaling in ASR systems, aligning with the prevailing trend in AI research that emphasizes the importance of the quality and volume of the training data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Moreover, our analysis extends beyond a WER comparison, shedding light on practically relevant aspects, such as code-switching, resistance to hallucinations, timestamp estimation, and inference latency, as well as the impact of pre-training on fine-tuning efficiency. The insights derived from these observations could offer the speech community a deeper understanding of the nuanced dynamics within ASR systems.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In this work, we have emphasized a holistic, system-centric approach to the design and analysis of ASR systems. Rather than isolating individual components and investigating them by normalizing other variables, our methodology considers the ASR system in its entirety, focusing on real world use cases and aiming to make more nuanced and practically relevant observations. We believe this perspective is crucial for advancing the field, as the ASR technology matures and finds more applications in the real world.
Future research should aim to build publicly accessible, comprehensive benchmarks covering the aspects that we discussed in this paper as well as other critical aspects, such as measuring proper noun recognition.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et¬†al. [2020]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:12449‚Äì12460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagby et¬†al. [2018]</span>
<span class="ltx_bibblock">
Tom Bagby, Kanishka Rao, and Khe¬†Chai Sim.

</span>
<span class="ltx_bibblock">Efficient implementation of recurrent neural network transducer in tensorflow.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Spoken Language Technology Workshop (SLT)</em>, pages 506‚Äì512. IEEE, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et¬†al. [2023]</span>
<span class="ltx_bibblock">
Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Whisperx: Time-accurate speech transcription of long-form audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et¬†al. [2023]</span>
<span class="ltx_bibblock">
Lo√Øc Barrault, Yu-An Chung, Mariano¬†Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et¬†al.

</span>
<span class="ltx_bibblock">Seamlessm4t-massively multilingual &amp; multimodal machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11596</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradbury et¬†al. [2018]</span>
<span class="ltx_bibblock">
James Bradbury, Roy Frostig, Peter Hawkins, Matthew¬†James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.

</span>
<span class="ltx_bibblock">JAX: composable transformations of Python+NumPy programs, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/google/jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/jax</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun et¬†al. [2023]</span>
<span class="ltx_bibblock">
Stefan Braun, Erik McDermott, and Roger Hsiao.

</span>
<span class="ltx_bibblock">Neural transducer training: Reduced memory consumption with sample-wise computation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1‚Äì5. IEEE, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu et¬†al. [2022]</span>
<span class="ltx_bibblock">
Chung-Cheng Chiu, James Qin, Yu¬†Zhang, Jiahui Yu, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Self-supervised learning with random-projection quantizer for speech recognition.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, Le¬†Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 3915‚Äì3924. PMLR, 17‚Äì23 Jul 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et¬†al. [2023]</span>
<span class="ltx_bibblock">
Seamless Communication, Lo√Øc Barrault, Yu-An Chung, Mariano¬†Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik¬†Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji¬†El Hachem, Brian Ellis, Gabriel¬†Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta¬†R. Costa-juss√†, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzm√°n, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff
Wang, and Skyler Wang.

</span>
<span class="ltx_bibblock">Seamlessm4t: Massively multilingual &amp; multimodal machine translation, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et¬†al. [2023]</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin¬†F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark¬†Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi¬†Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetiƒá, Dustin Tran, Thomas Kipf, Mario Luƒçiƒá, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby.

</span>
<span class="ltx_bibblock">Scaling vision transformers to 22 billion parameters, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et¬†al. [2018]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et¬†al. [2019]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171‚Äì4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/N19-1423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/N19-1423</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemmeke et¬†al. [2017]</span>
<span class="ltx_bibblock">
Jort¬†F Gemmeke, Daniel¬†PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R¬†Channing Moore, Manoj Plakal, and Marvin Ritter.

</span>
<span class="ltx_bibblock">Audio set: An ontology and human-labeled dataset for audio events.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, pages 776‚Äì780. IEEE, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Webrtcs.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://webrtc.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://webrtc.org/</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves [2012]</span>
<span class="ltx_bibblock">
Alex Graves.

</span>
<span class="ltx_bibblock">Sequence transduction with recurrent neural networks, 2012.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et¬†al. [2006]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fern√°ndez, Faustino Gomez, and J√ºrgen Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Conference on Machine Learning</em>, ICML ‚Äô06, page 369‚Äì376, New York, NY, USA, 2006. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 1595933832.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/1143844.1143891</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/1143844.1143891" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1143844.1143891</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et¬†al. [2013]</span>
<span class="ltx_bibblock">
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Speech recognition with deep recurrent neural networks, 2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati et¬†al. [2020]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu¬†Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented Transformer for Speech Recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 5036‚Äì5040, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2020-3015</span>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hannun [2019]</span>
<span class="ltx_bibblock">
Awni Hannun.

</span>
<span class="ltx_bibblock">The label bias problem.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://awni.github.io/label-bias/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://awni.github.io/label-bias/</a>, 2019.

</span>
<span class="ltx_bibblock">Accessed: 2024/03/06.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heek et¬†al. [2023]</span>
<span class="ltx_bibblock">
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee.

</span>
<span class="ltx_bibblock">Flax: A neural network library and ecosystem for JAX, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://github.com/google/flax" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://github.com/google/flax</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et¬†al. [2021]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung¬†Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:3451‚Äì3460, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et¬†al. [2022a]</span>
<span class="ltx_bibblock">
Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil Siddhartha, Shefali Garg, David Qiu, Khe¬†Chai Sim, Trevor Strohman, Fran√ßoise Beaufays, and Yanzhang He.

</span>
<span class="ltx_bibblock">Large-scale asr domain adaptation using self-and semi-supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 6627‚Äì6631. IEEE, 2022a.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et¬†al. [2022b]</span>
<span class="ltx_bibblock">
Dongseong Hwang, Khe¬†Chai Sim, Zhouyuan Huo, and Trevor Strohman.

</span>
<span class="ltx_bibblock">Pseudo label is better than human label, 2022b.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et¬†al. [2022c]</span>
<span class="ltx_bibblock">
Dongseong Hwang, Khe¬†Chai Sim, Zhouyuan Huo, and Trevor Strohman.

</span>
<span class="ltx_bibblock">Pseudo label is better than human label.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.12668</em>, 2022c.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et¬†al. [2023]</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye¬†Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1‚Äì38, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et¬†al. [2020]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom¬†B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2001.08361, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2001.08361" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2001.08361</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koenecke et¬†al. [2024]</span>
<span class="ltx_bibblock">
Allison Koenecke, Anna Seo¬†Gyeong Choi, Katelyn Mei, Hilke Schellmann, and Mona Sloane.

</span>
<span class="ltx_bibblock">Careless whisper: Speech-to-text hallucination harms.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.08021</em>, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. [2019]</span>
<span class="ltx_bibblock">
Jinyu Li, Rui Zhao, Hu¬†Hu, and Yifan Gong.

</span>
<span class="ltx_bibblock">Improving rnn transducer modeling for end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 114‚Äì121. IEEE, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. [2022]</span>
<span class="ltx_bibblock">
Xinjian Li, Florian Metze, David¬†R Mortensen, Alan¬†W Black, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Asr2k: Speech recognition for around 2000 languages without audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.02842</em>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McAuliffe et¬†al. [2017]</span>
<span class="ltx_bibblock">
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.

</span>
<span class="ltx_bibblock">Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2017</em>, pages 498‚Äì502, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2017-1386</span>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et¬†al. [2024]</span>
<span class="ltx_bibblock">
Ashish Mittal, Rudra Murthy, Vishwajeet Kumar, and Riyaz Bhat.

</span>
<span class="ltx_bibblock">Towards understanding and mitigating the hallucinations in nlp and speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)</em>, pages 489‚Äì492, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohamed et¬†al. [2022]</span>
<span class="ltx_bibblock">
Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob¬†D Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maal√∏e, et¬†al.

</span>
<span class="ltx_bibblock">Self-supervised speech representation learning: A review.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 16(6):1179‚Äì1210, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et¬†al. [2017]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.

</span>
<span class="ltx_bibblock">Automatic differentiation in pytorch.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">NIPS-W</em>, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et¬†al. [2023]</span>
<span class="ltx_bibblock">
Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee weon Jung, Soumi Maiti, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Reproducing whisper-style training using an open-source toolkit and publicly available data, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et¬†al. [2024]</span>
<span class="ltx_bibblock">
Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee weon Jung, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Owsm v3.1: Better and faster open whisper-style speech models based on e-branchformer, 2024.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puvvada et¬†al. [2024]</span>
<span class="ltx_bibblock">
Krishna¬†C. Puvvada, Piotr ≈ªelasko, He¬†Huang, Oleksii Hrinchuk, Nithin¬†Rao Koluguri, Somshubra Majumdar, Elena Rastorgueva, Kunal Dhawan, Zhehuai Chen, Vitaly Larukhin, Jagadeesh Balam, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Nvidia nemo canary model pushes the frontier of speech recognition and translation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024/04/05.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et¬†al. [2023]</span>
<span class="ltx_bibblock">
Alec Radford, Jong¬†Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning (ICML)</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafael [2021]</span>
<span class="ltx_bibblock">
Zequeira¬†Jim√©nez Rafael.

</span>
<span class="ltx_bibblock">Dnc: Dataset for noise classification.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sim et¬†al. [2017]</span>
<span class="ltx_bibblock">
Khe¬†Chai Sim, Arun Narayanan, Tom Bagby, Tara¬†N Sainath, and Michiel Bacchiani.

</span>
<span class="ltx_bibblock">Improving the efficiency of forward-backward algorithm using batched computation in tensorflow.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 258‚Äì264. IEEE, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et¬†al. [2023]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew¬†M Dai, Anja Hauth, et¬†al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2021]</span>
<span class="ltx_bibblock">
Silero Team.

</span>
<span class="ltx_bibblock">Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/snakers4/silero-vad" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/snakers4/silero-vad</a>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et¬†al. [2022]</span>
<span class="ltx_bibblock">
Jinchuan Tian, Brian Yan, Jianwei Yu, Chao Weng, Dong Yu, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Bayes risk ctc: Controllable ctc alignment in sequence-to-sequence tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.07499</em>, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et¬†al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wortsman et¬†al. [2023]</span>
<span class="ltx_bibblock">
Mitchell Wortsman, Peter¬†J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John¬†D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.

</span>
<span class="ltx_bibblock">Small-scale proxies for large-scale transformer training instabilities, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et¬†al. [2020]</span>
<span class="ltx_bibblock">
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc¬†V. Le.

</span>
<span class="ltx_bibblock">Self-training with noisy student improves imagenet classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 10684‚Äì10695, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR42600.2020.01070</span>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeyer et¬†al. [2021]</span>
<span class="ltx_bibblock">
Albert Zeyer, Ralf Schl√ºter, and Hermann Ney.

</span>
<span class="ltx_bibblock">Why does ctc result in peaky behavior?

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.14849</em>, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2024]</span>
<span class="ltx_bibblock">
Kevin Zhang, Luka Chkhetiani, Francis¬†McCann Ramirez, Yash Khare, Andrea Vanzo, Michael Liang, Sergio¬†Ramirez Martin, Gabriel Oexle, Ruben Bousbib, Taufiquzzaman Peyash, Michael Nguyen, Dillon Pulliam, and Domenic Donato.

</span>
<span class="ltx_bibblock">Conformer-1: Robust asr via large-scale semisupervised bootstrapping, 2024.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2022]</span>
<span class="ltx_bibblock">
Yu¬†Zhang, Daniel¬†S. Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo¬†Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe¬†Chai Sim, Bhuvana Ramabhadran, Tara¬†N. Sainath, Fran√ßoise Beaufays, Zhifeng Chen, Quoc¬†V. Le, Chung-Cheng Chiu, Ruoming Pang, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 16(6):1519‚Äì1532, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2023a]</span>
<span class="ltx_bibblock">
Yu¬†Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo¬†Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke¬†Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel¬†S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran√ßoise Beaufays, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100 languages, 2023a.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. [2023b]</span>
<span class="ltx_bibblock">
Yu¬†Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo¬†Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke¬†Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel¬†S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran√ßoise Beaufays, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Google USM: Scaling automatic speech recognition beyond 100 languages, 2023b.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et¬†al. [2021]</span>
<span class="ltx_bibblock">
Rui Zhao, Jian Xue, Jinyu Li, Wenning Wei, Lei He, and Yifan Gong.

</span>
<span class="ltx_bibblock">On addressing practical challenges for rnn-transducer, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Pre-training details</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we provide additional details about model pre-training and encountered issues.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Model initialization</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">For pre-training, we initialize all linear and convolutional layer weights with a Kaiming uniform distribution, and linear biases with a uniform distribution.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Training divergence and stability</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.4" class="ltx_p">When scaling the pre-training to larger model sizes, especially exceeding 1B parameters, we observed sudden divergence. These divergent model training runs are characterized by sudden loss spikes and a collapse of the distribution of predicted labels. Following the methodology in <cite class="ltx_cite ltx_citemacro_citet">Wortsman et¬†al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, we were able to identify the AdamW <math id="A1.SS2.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mi id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><ci id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">\epsilon</annotation></semantics></math> value as the root cause of this issue. We could resume previously diverged training runs by lowering the <math id="A1.SS2.p1.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.SS2.p1.2.m2.1a"><mi id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><ci id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">\epsilon</annotation></semantics></math> value from <math id="A1.SS2.p1.3.m3.1" class="ltx_Math" alttext="10^{-8}" display="inline"><semantics id="A1.SS2.p1.3.m3.1a"><msup id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml"><mn id="A1.SS2.p1.3.m3.1.1.2" xref="A1.SS2.p1.3.m3.1.1.2.cmml">10</mn><mrow id="A1.SS2.p1.3.m3.1.1.3" xref="A1.SS2.p1.3.m3.1.1.3.cmml"><mo id="A1.SS2.p1.3.m3.1.1.3a" xref="A1.SS2.p1.3.m3.1.1.3.cmml">‚àí</mo><mn id="A1.SS2.p1.3.m3.1.1.3.2" xref="A1.SS2.p1.3.m3.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.1b"><apply id="A1.SS2.p1.3.m3.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.3.m3.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="A1.SS2.p1.3.m3.1.1.2.cmml" xref="A1.SS2.p1.3.m3.1.1.2">10</cn><apply id="A1.SS2.p1.3.m3.1.1.3.cmml" xref="A1.SS2.p1.3.m3.1.1.3"><minus id="A1.SS2.p1.3.m3.1.1.3.1.cmml" xref="A1.SS2.p1.3.m3.1.1.3"></minus><cn type="integer" id="A1.SS2.p1.3.m3.1.1.3.2.cmml" xref="A1.SS2.p1.3.m3.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.1c">10^{-8}</annotation></semantics></math> to <math id="A1.SS2.p1.4.m4.1" class="ltx_Math" alttext="10^{-15}" display="inline"><semantics id="A1.SS2.p1.4.m4.1a"><msup id="A1.SS2.p1.4.m4.1.1" xref="A1.SS2.p1.4.m4.1.1.cmml"><mn id="A1.SS2.p1.4.m4.1.1.2" xref="A1.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="A1.SS2.p1.4.m4.1.1.3" xref="A1.SS2.p1.4.m4.1.1.3.cmml"><mo id="A1.SS2.p1.4.m4.1.1.3a" xref="A1.SS2.p1.4.m4.1.1.3.cmml">‚àí</mo><mn id="A1.SS2.p1.4.m4.1.1.3.2" xref="A1.SS2.p1.4.m4.1.1.3.2.cmml">15</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.4.m4.1b"><apply id="A1.SS2.p1.4.m4.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.4.m4.1.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="A1.SS2.p1.4.m4.1.1.2.cmml" xref="A1.SS2.p1.4.m4.1.1.2">10</cn><apply id="A1.SS2.p1.4.m4.1.1.3.cmml" xref="A1.SS2.p1.4.m4.1.1.3"><minus id="A1.SS2.p1.4.m4.1.1.3.1.cmml" xref="A1.SS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="A1.SS2.p1.4.m4.1.1.3.2.cmml" xref="A1.SS2.p1.4.m4.1.1.3.2">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.4.m4.1c">10^{-15}</annotation></semantics></math>.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">In addition to the above, we observed more stable RNN-T training when starting from a pre-trained checkpoint. In other words, training runs that would diverge when training from scratch will converge when initializing the model from a pre-trained checkpoint. More details can be found in <a href="#S4.SS8" title="4.8 Impact of Pre-training ‚Ä£ 4 Experimental Results ‚Ä£ Anatomy of Industrial Scale Multilingual ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>¬†<span class="ltx_text ltx_ref_tag">4.8</span></a>.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation Datasets</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>English datasets</h3>

<div id="A2.SS1.p1" class="ltx_para">
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p">Common Voice V5.1: We used the English subset of the V5.1 dataset from the official website.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p">CORAAL: We used the version 2021.07 dataset from official sources and segmented according to the FairSpeech project.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p">TED-LIUM 3: We used 11 TED talks, following the Whisper‚Äôs TED-LIUM 3 long-form partition.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p">LibriSpeech: We used the test-clean and test-other splits from the LibriSpeech ASR corpus.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p">Earnings-21: We used the corpus of earnings calls from the speech-datasets repository from the 202206 version.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p">Meanwhile: We followed the dataset creation procedure from Whisper and downloaded the 64 segments from YouTube.</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p">Podcast: We used a 18.2-hour human-labeled dataset of podcasts.</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A2.I1.i8.p1" class="ltx_para">
<p id="A2.I1.i8.p1.1" class="ltx_p">Broadcast: We used a 7.5-hour human-labeled private dataset of news broadcasts.</p>
</div>
</li>
<li id="A2.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A2.I1.i9.p1" class="ltx_para">
<p id="A2.I1.i9.p1.1" class="ltx_p">Telephony: We used a 8.2-hour human-labeled private dataset of telephone conversations.</p>
</div>
</li>
<li id="A2.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A2.I1.i10.p1" class="ltx_para">
<p id="A2.I1.i10.p1.1" class="ltx_p">Noisy: We used a 7.2-hour human-labeled private dataset of noisy real world audio.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Multilingual datasets</h3>

<div id="A2.SS2.p1" class="ltx_para">
<ol id="A2.I2" class="ltx_enumerate">
<li id="A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I2.i1.p1" class="ltx_para">
<p id="A2.I2.i1.p1.1" class="ltx_p">Fleurs: We downloaded the test splits for each language from the HuggingFace distribution.</p>
</div>
</li>
<li id="A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I2.i2.p1" class="ltx_para">
<p id="A2.I2.i2.p1.1" class="ltx_p">MLS: We used the test split of each language in the Multilingual LibriSpeech (MLS) corpus.</p>
</div>
</li>
<li id="A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I2.i3.p1" class="ltx_para">
<p id="A2.I2.i3.p1.1" class="ltx_p">VoxPopuli: We downloaded and segmented the dataset according to the official instructions for each language.</p>
</div>
</li>
<li id="A2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A2.I2.i4.p1" class="ltx_para">
<p id="A2.I2.i4.p1.1" class="ltx_p">Common Voice V9: We downloaded the V9 dataset from the official website for each language.</p>
</div>
</li>
<li id="A2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A2.I2.i5.p1" class="ltx_para">
<p id="A2.I2.i5.p1.1" class="ltx_p">Private: We used a small 6‚Äì10 hour human-labeled dataset of real-world audio.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Timestamp estimation datasets</h3>

<div id="A2.SS3.p1" class="ltx_para">
<ol id="A2.I3" class="ltx_enumerate">
<li id="A2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I3.i1.p1" class="ltx_para">
<p id="A2.I3.i1.p1.1" class="ltx_p">LibriSpeech test: We merge both test-clean and test-other of the LibriSpeech ASR corpus.</p>
</div>
</li>
<li id="A2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I3.i2.p1" class="ltx_para">
<p id="A2.I3.i2.p1.1" class="ltx_p">Private: We use an internal test set consisting of audio across 4 audio domains (Telephony, Podcasts, Broadcast, and Webinars).</p>
</div>
</li>
<li id="A2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I3.i3.p1" class="ltx_para">
<p id="A2.I3.i3.p1.1" class="ltx_p">Challenge: We used a more difficult test set containing targeted real-world edge cases, such as long silences and multi-talker conversations, in addition to a subset of LibriSpeech and Private.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Text formatting</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The output from an ASR system typically consists of a sequence of words with no formatting (e.g., <span id="A3.p1.1.1" class="ltx_text ltx_font_italic">how much is the new phone you got i think something around five hundred dollars</span>). Even when the ASR output text represents the spoken words accurately, the absence of proper punctuation, truecasing, and other formatting artifacts makes the text challenging for humans to read. Therefore, in real-world applications, we apply a post-processing step to convert the ASR output into a formatted version that is more suitable for human readers (e.g., <span id="A3.p1.1.2" class="ltx_text ltx_font_italic">How much is the new phone you got? I think something around $500.</span>).</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">To achieve this, we employ a multi-step pipeline approach that takes the raw ASR output and formats it into written text by applying the following processing steps.</p>
<ol id="A3.I1" class="ltx_enumerate">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p"><span id="A3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Inverse Text Normalization</span>: This step converts certain spoken-format phrases into its written form (e.g., <span id="A3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">five hundred dollars</span> to <span id="A3.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">$500</span>).</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p"><span id="A3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Punctuation Restoration</span>: This step adds punctuation marks, such as periods, commas, and question marks.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p id="A3.I1.i3.p1.1" class="ltx_p"><span id="A3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Truecasing</span>: This step capitalizes letters where appropriate, such as the starts of sentences and proper nouns.</p>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.09840" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.09841" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.09841">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.09841" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.09842" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 15:06:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
