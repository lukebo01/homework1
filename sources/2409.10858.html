<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.10858] Speech Recognition for Analysis of Police Radio Communication</title><meta property="og:description" content="Police departments around the world use two-way radio for coordination. These broadcast police communications (BPC)
are a unique source of information about everyday police activity and emergency response. Yet BPC are …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Speech Recognition for Analysis of Police Radio Communication">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Speech Recognition for Analysis of Police Radio Communication">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.10858">

<!--Generated on Sun Oct  6 01:22:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.2" class="ltx_p"><sup id="p1.2.1" class="ltx_sup">1</sup>University of Chicago, Chicago, IL 
<br class="ltx_break"><sup id="p1.2.2" class="ltx_sup">2</sup>Toyota Technological Institute at Chicago, Chicago, IL
</p>
</div>
<h1 class="ltx_title ltx_title_document">Speech Recognition for Analysis of Police Radio Communication</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Police departments around the world use two-way radio for coordination. These broadcast police communications (BPC)
are a unique source of information about everyday police activity and emergency response. Yet BPC are not transcribed, and their naturalistic audio properties make automatic transcription challenging. We collect a corpus of roughly 62,000 manually transcribed radio transmissions (~46 hours of audio) to evaluate the feasibility of automatic speech recognition (ASR) using modern recognition models. We evaluate the performance of off-the-shelf speech recognizers, models fine-tuned on BPC data, and customized end-to-end models. We find that both human and machine transcription is challenging in this domain. Large off-the-shelf ASR models perform poorly, but fine-tuned models can reach the approximate range of human performance. Our work suggests directions for future work, including analysis of short utterances and potential miscommunication in police radio interactions. We make our corpus and data annotation pipeline available to other researchers, to enable further research on recognition and analysis of police communication.</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p2.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
speech recognition, police radio communication, naturalistic audio</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the last few years, there has been increasing interest in studying the language of police in the United States, due in part to public demand for better police accountability, especially surrounding questions of racial disparities in police encounters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. Recent work on the language of policing has focused on the speech recorded in body-worn police cameras during traffic stops <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>, <a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>. This work has found racial disparities in such interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>, including differences that appear near the very beginning of police stops <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the language used among policing professionals, outside of their direct interaction with community members, is less studied. One source of such data is two-way radio transmissions involving police officers and dispatchers. In these broadcast police communications (BPC), dispatchers communicate with police officers, and officers communicate with each other, about police incidents and the locations and people involved. The language used among policing professionals is a rich source of information about these events and may shed light on the preparation <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">leading up to</span> police interactions with the public. As with body worn cameras, recent work finds that racial disparities exist in BPC, where use of two-way radio poses unacknowledged privacy risks linked to disproportionate communication about members of specific groups (e.g., Black or African American males) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>, <a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>. However, this work has only been able to analyze a small fraction of available BPC due to the resource intensive nature of manual transcription.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Police radio transmissions are available in large quantities, but they are noisy and unannotated. A prerequisite for expanding the study of the language of police radio, therefore, is to develop transcription and related annotation pipelines. In this study, we take the first steps toward this goal by collecting, studying, and making available<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The corpus will be available at <a target="_blank" href="https://voices.uchicago.edu/p2r/bpc-cpd-corpus/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://voices.uchicago.edu/p2r/bpc-cpd-corpus/</a>under terms of use in line with US federal standards for human subjects research. Researchers seeking access to the data will be asked to agree to the terms of use
and to describe their intended research use case. Each use case will be manually reviewed for ethical considerations before access is granted.</span></span></span>
a corpus of BPC in Chicago, one of the largest police forces in the United States. Specifically, we investigate automatic speech recognition (ASR) model performance on a corpus of two-way radio transmissions between policing professionals in the city of Chicago to characterize challenges associated with this domain. Our main contributions are (1) the corpus itself and (2) our evaluation of ASR model performance, which provides a reference point for the current capabilities and limits of such models on this challenging domain.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Domain characteristics of police radio speech</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Police have used two-way radio transmissions to coordinate their activity since the 1930s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>.
A large number of these transmissions are short utterances involving a handshaking-like process where individuals confirm each other’s identity before transmitting information. These utterances often contain a unit number identifying the police beat (i.e., geographic region) the unit is assigned to patrol, and are therefore important for speaker identification and localization (e.g., “FOURTEEN TWELVE” is a call sign indicating the unit is patrolling beat number 1412). Beyond their use in call signs, numbers are an essential part of this domain since they are used for street addresses, ages of individuals encountered, and other information (e.g., “TEN FOUR” to acknowledge a transmission).
<span id="S2.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Representative utterances from our BPC corpus include “WE GOT FIRST FLOOR I THINK IT’S APARTMENT NUMBER F ONE OR L ONE” and “TWO TWENTY TWO ROBERT WE ARE ON OUR TRAFFIC CRASH ⟨UNINTELLIGIBLE⟩,” where ”⟨UNINTELLIGIBLE⟩” indicates a segment that the annotator could not understand (see Section <a href="#S3.SS1" title="3.1 Data collection and annotation process ‣ 3 Data ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</span> Finally, radio transmissions using the same frequency can interfere with each other. In practice, this means speakers engage in strict turn-taking.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Another feature of radio policing, like other police data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>, is the sharing of sensitive information about individuals, such as name and address. However, like other forms of policing data, BPC are often publicly accessible and may be observed and/or collected for research according to applicable local laws. Therefore, our data can be released to other researchers under applicable guidelines, and our research methods should be reproducible by other researchers aiming to analyze BPC data in other regions. In addition to the data, we will make our materials, such as annotation guidelines and data processing pipelines, available to the research community.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speech recognition for naturalistic audio</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, the research community has shifted from using curated benchmark corpora, such as LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, to training on as much accessible speech data as possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>, <a href="#bib.bibx12" title="" class="ltx_ref">12</a>, <a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>. Using large amounts of diverse data can help create general-purpose ASR systems that can be used in a zero-shot manner (i.e., without training on domain-specific data). However, domain differences can still degrade performance. For example, the Whisper large-v2 speech recognizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> degrades from 2.7% word error rate (WER) on LibriSpeech to 25.5% WER on data from the CHiME6 noisy speech challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> and 36.4% WER on multi-party meeting speech from the AMI corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>. On clean but accented speech, Whisper degrades to 19.7% WER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>.
Here, we focus on the domain of police radio communications, where unique challenges such as background noise, specialized terminology, and short utterances further compound the challenges of speech recognition in real-world conditions. In some ways this domain is similar to the air traffic control (ATC) domain, and there has been work on ASR for ATC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>, <a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>.
However, word error rates for ATC corpora tend to be much lower than for BPC (see Section <a href="#S5" title="5 Results and analysis ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), likely reflecting differences in audio quality despite the domain similarities.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">Our corpus, BPC-CPD, is composed of BPC radio transmissions involving the Chicago Police Department (CPD). BPC-CPD
contains <math id="S3.p1.1.m1.2" class="ltx_Math" alttext="62,080" display="inline"><semantics id="S3.p1.1.m1.2a"><mrow id="S3.p1.1.m1.2.3.2" xref="S3.p1.1.m1.2.3.1.cmml"><mn mathcolor="#000000" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">62</mn><mo mathcolor="#000000" id="S3.p1.1.m1.2.3.2.1" xref="S3.p1.1.m1.2.3.1.cmml">,</mo><mn mathcolor="#000000" id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.2b"><list id="S3.p1.1.m1.2.3.1.cmml" xref="S3.p1.1.m1.2.3.2"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">62</cn><cn type="integer" id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">080</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.2c">62,080</annotation></semantics></math> utterances (<math id="S3.p1.2.m2.1" class="ltx_Math" alttext="46.2" display="inline"><semantics id="S3.p1.2.m2.1a"><mn mathcolor="#000000" id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">46.2</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn type="float" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">46.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">46.2</annotation></semantics></math> hours of transcribed speech) labeled by a combination of researchers at the University of Chicago (<span id="S3.p1.2.1" class="ltx_text" style="color:#000000;">42.5%</span>) and a professional transcription service (<span id="S3.p1.2.2" class="ltx_text" style="color:#000000;">57.5%</span>). The professional service transcribed 510 minutes of “raw” audio (i.e., including silence between transmissions) so that these could be aligned with internal transcribers’ annotations for validation. The remaining professionally transcribed audio was sampled from all of the speech in the BPC recordings, as identified by a voice activity detection model. BPC-CPD includes data from 11 of Chicago’s 13 dispatch zones, distinct non-overlapping areas of the city each assigned one dispatcher to coordinate police activity in that area; data for the remaining two dispatch zones was not available.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">The training, development, and test sets consist of <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="44,664" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mn mathcolor="#000000" id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">44</mn><mo mathcolor="#000000" id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mn mathcolor="#000000" id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">664</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">44</cn><cn type="integer" id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">664</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">44,664</annotation></semantics></math> utterances (<span id="S3.p2.4.1" class="ltx_text" style="color:#000000;">33.0</span> hours), <math id="S3.p2.2.m2.2" class="ltx_Math" alttext="8,714" display="inline"><semantics id="S3.p2.2.m2.2a"><mrow id="S3.p2.2.m2.2.3.2" xref="S3.p2.2.m2.2.3.1.cmml"><mn mathcolor="#000000" id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">8</mn><mo mathcolor="#000000" id="S3.p2.2.m2.2.3.2.1" xref="S3.p2.2.m2.2.3.1.cmml">,</mo><mn mathcolor="#000000" id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">714</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2b"><list id="S3.p2.2.m2.2.3.1.cmml" xref="S3.p2.2.m2.2.3.2"><cn type="integer" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">8</cn><cn type="integer" id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">714</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2c">8,714</annotation></semantics></math> utterances (<span id="S3.p2.4.2" class="ltx_text" style="color:#000000;">6.6</span> hours), and <math id="S3.p2.3.m3.2" class="ltx_Math" alttext="8,702" display="inline"><semantics id="S3.p2.3.m3.2a"><mrow id="S3.p2.3.m3.2.3.2" xref="S3.p2.3.m3.2.3.1.cmml"><mn mathcolor="#000000" id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">8</mn><mo mathcolor="#000000" id="S3.p2.3.m3.2.3.2.1" xref="S3.p2.3.m3.2.3.1.cmml">,</mo><mn mathcolor="#000000" id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml">702</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.2b"><list id="S3.p2.3.m3.2.3.1.cmml" xref="S3.p2.3.m3.2.3.2"><cn type="integer" id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">8</cn><cn type="integer" id="S3.p2.3.m3.2.2.cmml" xref="S3.p2.3.m3.2.2">702</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.2c">8,702</annotation></semantics></math> utterances (<span id="S3.p2.4.3" class="ltx_text" style="color:#000000;">6.6</span> hours), respectively. Train (80%), dev (10%), and test (10%) splits were created through stratified sampling of transcribed utterances. To ensure even sampling of BPC across dispatch zones and day/night hours, we created 22 strata (11 zones <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.p2.4.m4.1a"><mo id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><times id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\times</annotation></semantics></math> 2 levels), since zone captures spatial variation in reported crimes and policing personnel, while the timing of police stops (before/after sunset) impacts police behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>. The utterances from two zones (5 and 6) were evenly split into dev/test sets, but excluded from train in order to assess robustness to unseen contexts. BPC associated with the remaining 9 zones are split between train, dev, and test as described above. Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Data ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the utterance length distribution for the train split, which is essentially identical to the dev and test distributions.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.10858/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="183" height="61" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Duration distribution of utterances in our training set, for utterances of duration up to 10 seconds (96.5% of the train set utterances).</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data collection and annotation process</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">BPC-CPD data were obtained from a publicly accessible web site (https://broadcastify.com) that maintains a rolling 6-month archive of recordings available under a CC-BY-3.0 license. Approximately 12 months (~80k hours) of BPC recordings were downloaded, of which <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="46.2" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">46.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="float" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">46.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">46.2</annotation></semantics></math> hours have been labeled thus far. The BPC-CPD corpus contains transcriptions for these 46.2 hours, including transcripts from multiple annotators for the same audio when available. Here, we use a deduplicated version of the corpus for development and testing, selecting the transcript produced by the annotator with the lowest median WER (see Section <a href="#S3.SS3" title="3.3 Inter-annotator agreement ‣ 3 Data ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Annotation guidelines were developed based on pilot transcription efforts by researchers at the University of Chicago, with researchers collectively discussing and addressing annotation challenges as part of protocol development. Internal annotators were provided 30-minute audio files and asked to identify and transcribe any speech found in the file, treating each radio transmission as a separate utterance and spelling out numbers as spoken. Annotators were instructed not to download audio, and annotation took place in a secure compute environment: Access to audio was restricted to either secure on-site computers or secure remote desktop applications. Annotators were made aware of the sensitive nature of BPC and advised to pause work if they experienced emotional distress given its content.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">If annotators were uncertain about a transcription, they used two types of in-line annotation: (a) enclose words they are 50%-90% sure are correct in square brackets (e.g., “[GOOD JOB]”); (b) indicate a segment of speech is unintelligible (i.e., <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><lt id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">&lt;</annotation></semantics></math>50% sure they correctly understood it) using “<math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\langle x\rangle" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.2.2" xref="S3.SS1.p3.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.2.2.1" xref="S3.SS1.p3.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS1.p3.2.m2.1.2.2.2" xref="S3.SS1.p3.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><csymbol cd="latexml" id="S3.SS1.p3.2.m2.1.2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\langle x\rangle</annotation></semantics></math>” (henceforth denoted “⟨<span id="S3.SS1.p3.2.1" class="ltx_text" style="color:#000000;">UNINTELLIGIBLE</span>⟩” in our analysis). This annotation protocol was provided to the professional transcribers, along with a list of Chicago street names and common CPD jargon.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Before further analysis and ASR experiments, we post-processed the annotations to correct typos and normalize the text for ASR.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Post-processing steps</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For purposes of ASR training and evaluation (and the human WER evaluation below), all square brackets indicating transcriber uncertainty were removed, all hyphens were replaced with whitespace, single quotes and backticks were converted to apostrophes, and all punctuation except for apostrophes was removed. In addition, certain annotator formatting errors were also manually corrected. Finally, annotation errors like misspellings were identified and, where possible, corrected following manual review.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">This process used a whitelist of tokens considered valid based on a combination of NLTK’s English words and Chicago street names <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. The manual review process leveraged both context and domain knowledge about policing in Chicago to correct tokens when (a) the error is clearly a typo, (b) the intended word or words are clear, given context, or (c) domain knowledge suggested a specific correction (e.g., “KEDZIE” is a street name that has many misspellings). If significant ambiguity persisted despite context, then no change was made.
</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Numbers were converted to their spoken word representation (e.g., “SIX” instead of “6”). First, as many numbers as possible (where there was no ambiguity) were corrected via the review process described above. For example, the number “<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="720" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn type="integer" id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">720</annotation></semantics></math>” would be corrected to “SEVEN TWENTY” since this is its conventional spoken word representation in BPC.
Second, for consistency the NeMo text normalizer was applied to remaining numbers whose spoken word representation was unclear or ambiguous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>, <a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite>. Finally, all text was upper-cased.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In the case of utterances that were transcribed by multiple annotators, for the training set the ground-truth transcript was selected randomly.
For the development and test set, the selection was done using annotator performance; specifically, the transcript from the annotator with the lowest median WER against other annotators was chosen. All post-processing steps described in this section are also applied to ASR model outputs for comparability.
</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">The released BPC-CPD corpus will include two versions of all transcriptions: (1) raw text as entered by annotators; (2) post-processed text, as used here, but including brackets to indicate annotator uncertainty. We do not make use of these in-line annotations in our current study, but we include them in the released corpus for use by others for robustness testing or other research uses. We will also release the complete details of our annotation and post-processing pipeline, to facilitate replication of our pipeline for BPC data collection in other regions.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Inter-annotator agreement</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">We report inter-annotator agreement using the WER between annotators for audio transcribed by multiple annotators. Since each annotator may have a different set of utterance boundaries, for each pair of annotators we identify audio files transcribed by both annotators and then concatenate all utterances from each file. Since we lack ground truth, each annotator’s transcript is treated first as reference and then as hypothesis. Across 23 annotators who contributed to BPC-CPD, we
have <span id="S3.SS3.p1.2.1" class="ltx_text" style="color:#000000;">468</span> triplets of the form (transcription from annotator <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">i</annotation></semantics></math>, transcription from annotator <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">j</annotation></semantics></math>, audio file), where WER for the triplet is calculated using the transcription from the first annotator as reference.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">According to this evaluation, the professional annotators have a median WER of 25.9% (when treating all other annotators as reference), while the remaining annotators have a median WER of 28.9%. These WERs provide a rough measure of human-level performance on our task. This measure is not directly comparable to the performance of our ASR systems, which are given a single utterance at a time without context, rather than the much longer audio that human transcribers were given. We also note that the true human WER is likely somewhat lower, since there is some noise introduced in the transcriptions through differences in spelling that our post-processing (see Section <a href="#S3.SS2" title="3.2 Post-processing steps ‣ 3 Data ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) may have missed.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Nevertheless, the relatively high annotator WERs suggest that BPC audio is challenging even for human listeners. We also note that roughly <span id="S3.SS3.p3.1.1" class="ltx_text" style="color:#000000;">13%</span> of the utterances in our corpus contain at least one audio segment marked unintelligible by a transcriber. This finding is consistent with interviews conducted with sworn officers about both BPC and interactions with youth, during which many asked for BPC audio clips to be repeated due to difficulty understanding what was spoken. While the most common confusions are the expected ones between short function words, the single most commonly disagreed-upon word is “⟨unintelligible⟩”.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.13.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.2.1" class="ltx_text" style="font-size:90%;">WER (%) for ASR models on the dev and test sets, as well as the substitution (S), deletion (D), and insertion (I) rates (%) for the dev set. We evaluate only the best models in each category on the test set, to avoid overfitting to the test set. [<sup id="S3.T1.2.1.1" class="ltx_sup">∗</sup>These models have fewer tuned parameters than the fused models due to projecting features to a lower dimensional space prior to inputting them to the encoder.]</span></figcaption>
<div id="S3.T1.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:506.5pt;height:166.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.9pt,153.6pt) scale(0.840838048876136,0.35112211408559) ;">
<table id="S3.T1.10.8" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.7.5.5" class="ltx_tr">
<td id="S3.T1.7.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Type</td>
<td id="S3.T1.7.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S3.T1.7.5.5.8" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T1.7.5.5.8.1" class="ltx_text"></span> <span id="S3.T1.7.5.5.8.2" class="ltx_text">
<span id="S3.T1.7.5.5.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.7.5.5.8.2.1.1" class="ltx_tr">
<span id="S3.T1.7.5.5.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">#</span></span>
<span id="S3.T1.7.5.5.8.2.1.2" class="ltx_tr">
<span id="S3.T1.7.5.5.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Parameters</span></span>
</span></span><span id="S3.T1.7.5.5.8.3" class="ltx_text"></span>
</td>
<td id="S3.T1.7.5.5.9" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T1.7.5.5.9.1" class="ltx_text"></span> <span id="S3.T1.7.5.5.9.2" class="ltx_text">
<span id="S3.T1.7.5.5.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.7.5.5.9.2.1.1" class="ltx_tr">
<span id="S3.T1.7.5.5.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"># Tuned</span></span>
<span id="S3.T1.7.5.5.9.2.1.2" class="ltx_tr">
<span id="S3.T1.7.5.5.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Parameters</span></span>
</span></span><span id="S3.T1.7.5.5.9.3" class="ltx_text"></span>
</td>
<td id="S3.T1.7.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S3.T1.7.5.5.10.1" class="ltx_text"></span> <span id="S3.T1.7.5.5.10.2" class="ltx_text">
<span id="S3.T1.7.5.5.10.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.7.5.5.10.2.1.1" class="ltx_tr">
<span id="S3.T1.7.5.5.10.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Language</span></span>
<span id="S3.T1.7.5.5.10.2.1.2" class="ltx_tr">
<span id="S3.T1.7.5.5.10.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Model?</span></span>
</span></span><span id="S3.T1.7.5.5.10.3" class="ltx_text"></span></td>
<td id="S3.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">WER<sub id="S3.T1.3.1.1.1.1" class="ltx_sub"><span id="S3.T1.3.1.1.1.1.1" class="ltx_text ltx_font_italic">dev</span></sub>
</td>
<td id="S3.T1.4.2.2.2" class="ltx_td ltx_align_right ltx_border_tt">S<sub id="S3.T1.4.2.2.2.1" class="ltx_sub"><span id="S3.T1.4.2.2.2.1.1" class="ltx_text ltx_font_italic">dev</span></sub>
</td>
<td id="S3.T1.5.3.3.3" class="ltx_td ltx_align_right ltx_border_tt">D<sub id="S3.T1.5.3.3.3.1" class="ltx_sub"><span id="S3.T1.5.3.3.3.1.1" class="ltx_text ltx_font_italic">dev</span></sub>
</td>
<td id="S3.T1.6.4.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">I<sub id="S3.T1.6.4.4.4.1" class="ltx_sub"><span id="S3.T1.6.4.4.4.1.1" class="ltx_text ltx_font_italic">dev</span></sub>
</td>
<td id="S3.T1.7.5.5.5" class="ltx_td ltx_align_center ltx_border_tt">WER<sub id="S3.T1.7.5.5.5.1" class="ltx_sub"><span id="S3.T1.7.5.5.5.1.1" class="ltx_text ltx_font_italic">test</span></sub>
</td>
</tr>
<tr id="S3.T1.10.8.9" class="ltx_tr">
<td id="S3.T1.10.8.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Whisper</td>
<td id="S3.T1.10.8.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_border_t">Whisper large-v2</td>
<td id="S3.T1.10.8.9.3" class="ltx_td ltx_align_right ltx_border_tt ltx_border_t">1.55B</td>
<td id="S3.T1.10.8.9.4" class="ltx_td ltx_align_right ltx_border_tt ltx_border_t">0</td>
<td id="S3.T1.10.8.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t">No</td>
<td id="S3.T1.10.8.9.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">57.4</td>
<td id="S3.T1.10.8.9.7" class="ltx_td ltx_align_right ltx_border_tt ltx_border_t">25.1</td>
<td id="S3.T1.10.8.9.8" class="ltx_td ltx_align_right ltx_border_tt ltx_border_t">15.8</td>
<td id="S3.T1.10.8.9.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt ltx_border_t">16.5</td>
<td id="S3.T1.10.8.9.10" class="ltx_td ltx_border_tt ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.10" class="ltx_tr">
<td id="S3.T1.10.8.10.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.10.2" class="ltx_td ltx_align_left ltx_border_r">Whisper large-v3</td>
<td id="S3.T1.10.8.10.3" class="ltx_td ltx_align_right">1.55B</td>
<td id="S3.T1.10.8.10.4" class="ltx_td ltx_align_right">0</td>
<td id="S3.T1.10.8.10.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.10.6" class="ltx_td ltx_align_center">51.4</td>
<td id="S3.T1.10.8.10.7" class="ltx_td ltx_align_right">26.2</td>
<td id="S3.T1.10.8.10.8" class="ltx_td ltx_align_right">11.2</td>
<td id="S3.T1.10.8.10.9" class="ltx_td ltx_align_right ltx_border_r">14.0</td>
<td id="S3.T1.10.8.10.10" class="ltx_td ltx_align_center">50.8</td>
</tr>
<tr id="S3.T1.10.8.11" class="ltx_tr">
<td id="S3.T1.10.8.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">NeMo</td>
<td id="S3.T1.10.8.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Conformer CTC</td>
<td id="S3.T1.10.8.11.3" class="ltx_td ltx_align_right ltx_border_tt">120M</td>
<td id="S3.T1.10.8.11.4" class="ltx_td ltx_align_right ltx_border_tt">0</td>
<td id="S3.T1.10.8.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">No</td>
<td id="S3.T1.10.8.11.6" class="ltx_td ltx_align_center ltx_border_tt">51.5</td>
<td id="S3.T1.10.8.11.7" class="ltx_td ltx_align_right ltx_border_tt">27.7</td>
<td id="S3.T1.10.8.11.8" class="ltx_td ltx_align_right ltx_border_tt">19.4</td>
<td id="S3.T1.10.8.11.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">4.4</td>
<td id="S3.T1.10.8.11.10" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S3.T1.10.8.12" class="ltx_tr">
<td id="S3.T1.10.8.12.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.12.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.12.3" class="ltx_td ltx_align_right">120M</td>
<td id="S3.T1.10.8.12.4" class="ltx_td ltx_align_right">120M</td>
<td id="S3.T1.10.8.12.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.12.6" class="ltx_td ltx_align_center">27.7</td>
<td id="S3.T1.10.8.12.7" class="ltx_td ltx_align_right">13.5</td>
<td id="S3.T1.10.8.12.8" class="ltx_td ltx_align_right">10.5</td>
<td id="S3.T1.10.8.12.9" class="ltx_td ltx_align_right ltx_border_r">3.6</td>
<td id="S3.T1.10.8.12.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.13" class="ltx_tr">
<td id="S3.T1.10.8.13.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.13.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.13.3" class="ltx_td ltx_align_right">616M</td>
<td id="S3.T1.10.8.13.4" class="ltx_td ltx_align_right">0</td>
<td id="S3.T1.10.8.13.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.13.6" class="ltx_td ltx_align_center">53.8</td>
<td id="S3.T1.10.8.13.7" class="ltx_td ltx_align_right">29.7</td>
<td id="S3.T1.10.8.13.8" class="ltx_td ltx_align_right">19.7</td>
<td id="S3.T1.10.8.13.9" class="ltx_td ltx_align_right ltx_border_r">4.4</td>
<td id="S3.T1.10.8.13.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.14" class="ltx_tr">
<td id="S3.T1.10.8.14.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.14.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.14.3" class="ltx_td ltx_align_right">616M</td>
<td id="S3.T1.10.8.14.4" class="ltx_td ltx_align_right">616M</td>
<td id="S3.T1.10.8.14.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.14.6" class="ltx_td ltx_align_center">27.0</td>
<td id="S3.T1.10.8.14.7" class="ltx_td ltx_align_right">13.8</td>
<td id="S3.T1.10.8.14.8" class="ltx_td ltx_align_right">9.2</td>
<td id="S3.T1.10.8.14.9" class="ltx_td ltx_align_right ltx_border_r">4.0</td>
<td id="S3.T1.10.8.14.10" class="ltx_td ltx_align_center"><span id="S3.T1.10.8.14.10.1" class="ltx_text ltx_font_bold">27.3</span></td>
</tr>
<tr id="S3.T1.10.8.15" class="ltx_tr">
<td id="S3.T1.10.8.15.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.15.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.15.3" class="ltx_td ltx_align_right">1.2B</td>
<td id="S3.T1.10.8.15.4" class="ltx_td ltx_align_right">0</td>
<td id="S3.T1.10.8.15.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.15.6" class="ltx_td ltx_align_center">53.1</td>
<td id="S3.T1.10.8.15.7" class="ltx_td ltx_align_right">27.3</td>
<td id="S3.T1.10.8.15.8" class="ltx_td ltx_align_right">21.7</td>
<td id="S3.T1.10.8.15.9" class="ltx_td ltx_align_right ltx_border_r">4.1</td>
<td id="S3.T1.10.8.15.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.16" class="ltx_tr">
<td id="S3.T1.10.8.16.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Conformer parakeet CTC</td>
<td id="S3.T1.10.8.16.3" class="ltx_td ltx_align_right ltx_border_t">616M</td>
<td id="S3.T1.10.8.16.4" class="ltx_td ltx_align_right ltx_border_t">0</td>
<td id="S3.T1.10.8.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.16.6" class="ltx_td ltx_align_center ltx_border_t">53.8</td>
<td id="S3.T1.10.8.16.7" class="ltx_td ltx_align_right ltx_border_t">28.6</td>
<td id="S3.T1.10.8.16.8" class="ltx_td ltx_align_right ltx_border_t">21.1</td>
<td id="S3.T1.10.8.16.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.1</td>
<td id="S3.T1.10.8.16.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.17" class="ltx_tr">
<td id="S3.T1.10.8.17.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.17.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.17.3" class="ltx_td ltx_align_right">1.2B</td>
<td id="S3.T1.10.8.17.4" class="ltx_td ltx_align_right">0</td>
<td id="S3.T1.10.8.17.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.17.6" class="ltx_td ltx_align_center">51.6</td>
<td id="S3.T1.10.8.17.7" class="ltx_td ltx_align_right">29.5</td>
<td id="S3.T1.10.8.17.8" class="ltx_td ltx_align_right">16.6</td>
<td id="S3.T1.10.8.17.9" class="ltx_td ltx_align_right ltx_border_r">5.5</td>
<td id="S3.T1.10.8.17.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.18" class="ltx_tr">
<td id="S3.T1.10.8.18.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Conformer parakeet Transducer</td>
<td id="S3.T1.10.8.18.3" class="ltx_td ltx_align_right ltx_border_t">616B</td>
<td id="S3.T1.10.8.18.4" class="ltx_td ltx_align_right ltx_border_t">0</td>
<td id="S3.T1.10.8.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.18.6" class="ltx_td ltx_align_center ltx_border_t">54.9</td>
<td id="S3.T1.10.8.18.7" class="ltx_td ltx_align_right ltx_border_t">18.6</td>
<td id="S3.T1.10.8.18.8" class="ltx_td ltx_align_right ltx_border_t">33.5</td>
<td id="S3.T1.10.8.18.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.8</td>
<td id="S3.T1.10.8.18.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.19" class="ltx_tr">
<td id="S3.T1.10.8.19.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.19.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.19.3" class="ltx_td ltx_align_right">1.2B</td>
<td id="S3.T1.10.8.19.4" class="ltx_td ltx_align_right">0</td>
<td id="S3.T1.10.8.19.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.19.6" class="ltx_td ltx_align_center">50.8</td>
<td id="S3.T1.10.8.19.7" class="ltx_td ltx_align_right">19.9</td>
<td id="S3.T1.10.8.19.8" class="ltx_td ltx_align_right">28.1</td>
<td id="S3.T1.10.8.19.9" class="ltx_td ltx_align_right ltx_border_r">2.8</td>
<td id="S3.T1.10.8.19.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.20" class="ltx_tr">
<td id="S3.T1.10.8.20.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Conformer parakeet TDT</td>
<td id="S3.T1.10.8.20.3" class="ltx_td ltx_align_right ltx_border_t">1.2B</td>
<td id="S3.T1.10.8.20.4" class="ltx_td ltx_align_right ltx_border_t">0</td>
<td id="S3.T1.10.8.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.20.6" class="ltx_td ltx_align_center ltx_border_t">48.2</td>
<td id="S3.T1.10.8.20.7" class="ltx_td ltx_align_right ltx_border_t">22.2</td>
<td id="S3.T1.10.8.20.8" class="ltx_td ltx_align_right ltx_border_t">21.9</td>
<td id="S3.T1.10.8.20.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.1</td>
<td id="S3.T1.10.8.20.10" class="ltx_td ltx_align_center ltx_border_t">47.5</td>
</tr>
<tr id="S3.T1.10.8.21" class="ltx_tr">
<td id="S3.T1.10.8.21.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">CTC-AED</td>
<td id="S3.T1.10.8.21.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">FBANK-Conformer</td>
<td id="S3.T1.10.8.21.3" class="ltx_td ltx_align_right ltx_border_tt">41M</td>
<td id="S3.T1.10.8.21.4" class="ltx_td ltx_align_right ltx_border_tt">41M</td>
<td id="S3.T1.10.8.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">No</td>
<td id="S3.T1.10.8.21.6" class="ltx_td ltx_align_center ltx_border_tt">51.9</td>
<td id="S3.T1.10.8.21.7" class="ltx_td ltx_align_right ltx_border_tt">33.6</td>
<td id="S3.T1.10.8.21.8" class="ltx_td ltx_align_right ltx_border_tt">9.0</td>
<td id="S3.T1.10.8.21.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.3</td>
<td id="S3.T1.10.8.21.10" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S3.T1.10.8.22" class="ltx_tr">
<td id="S3.T1.10.8.22.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.22.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.22.3" class="ltx_td ltx_align_right">41M</td>
<td id="S3.T1.10.8.22.4" class="ltx_td ltx_align_right">41M</td>
<td id="S3.T1.10.8.22.5" class="ltx_td ltx_align_center ltx_border_r">Yes</td>
<td id="S3.T1.10.8.22.6" class="ltx_td ltx_align_center">51.3</td>
<td id="S3.T1.10.8.22.7" class="ltx_td ltx_align_right">32.1</td>
<td id="S3.T1.10.8.22.8" class="ltx_td ltx_align_right">10.5</td>
<td id="S3.T1.10.8.22.9" class="ltx_td ltx_align_right ltx_border_r">8.7</td>
<td id="S3.T1.10.8.22.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.23" class="ltx_tr">
<td id="S3.T1.10.8.23.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.23.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">HuBERT-Conformer</td>
<td id="S3.T1.10.8.23.3" class="ltx_td ltx_align_right ltx_border_t">422M</td>
<td id="S3.T1.10.8.23.4" class="ltx_td ltx_align_right ltx_border_t">106M</td>
<td id="S3.T1.10.8.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.23.6" class="ltx_td ltx_align_center ltx_border_t">52.3</td>
<td id="S3.T1.10.8.23.7" class="ltx_td ltx_align_right ltx_border_t">33.9</td>
<td id="S3.T1.10.8.23.8" class="ltx_td ltx_align_right ltx_border_t">9.0</td>
<td id="S3.T1.10.8.23.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.4</td>
<td id="S3.T1.10.8.23.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.24" class="ltx_tr">
<td id="S3.T1.10.8.24.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.24.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.24.3" class="ltx_td ltx_align_right">422M</td>
<td id="S3.T1.10.8.24.4" class="ltx_td ltx_align_right">106M</td>
<td id="S3.T1.10.8.24.5" class="ltx_td ltx_align_center ltx_border_r">Yes</td>
<td id="S3.T1.10.8.24.6" class="ltx_td ltx_align_center">51.1</td>
<td id="S3.T1.10.8.24.7" class="ltx_td ltx_align_right">32.0</td>
<td id="S3.T1.10.8.24.8" class="ltx_td ltx_align_right">9.6</td>
<td id="S3.T1.10.8.24.9" class="ltx_td ltx_align_right ltx_border_r">9.5</td>
<td id="S3.T1.10.8.24.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.25" class="ltx_tr">
<td id="S3.T1.10.8.25.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.25.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">WavLM-Conformer</td>
<td id="S3.T1.10.8.25.3" class="ltx_td ltx_align_right ltx_border_t">421M</td>
<td id="S3.T1.10.8.25.4" class="ltx_td ltx_align_right ltx_border_t">106M</td>
<td id="S3.T1.10.8.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.25.6" class="ltx_td ltx_align_center ltx_border_t">42.3</td>
<td id="S3.T1.10.8.25.7" class="ltx_td ltx_align_right ltx_border_t">25.9</td>
<td id="S3.T1.10.8.25.8" class="ltx_td ltx_align_right ltx_border_t">6.5</td>
<td id="S3.T1.10.8.25.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.8</td>
<td id="S3.T1.10.8.25.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.26" class="ltx_tr">
<td id="S3.T1.10.8.26.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.26.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.26.3" class="ltx_td ltx_align_right">421M</td>
<td id="S3.T1.10.8.26.4" class="ltx_td ltx_align_right">106M</td>
<td id="S3.T1.10.8.26.5" class="ltx_td ltx_align_center ltx_border_r">Yes</td>
<td id="S3.T1.10.8.26.6" class="ltx_td ltx_align_center">40.6</td>
<td id="S3.T1.10.8.26.7" class="ltx_td ltx_align_right">23.9</td>
<td id="S3.T1.10.8.26.8" class="ltx_td ltx_align_right">7.0</td>
<td id="S3.T1.10.8.26.9" class="ltx_td ltx_align_right ltx_border_r">9.7</td>
<td id="S3.T1.10.8.26.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.8.6.6" class="ltx_tr">
<td id="S3.T1.8.6.6.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.8.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">HuBERT+WavLM-Conformer</td>
<td id="S3.T1.8.6.6.4" class="ltx_td ltx_align_right ltx_border_t">683M</td>
<td id="S3.T1.8.6.6.1" class="ltx_td ltx_align_right ltx_border_t">52M<sup id="S3.T1.8.6.6.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S3.T1.8.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.8.6.6.6" class="ltx_td ltx_align_center ltx_border_t">40.1</td>
<td id="S3.T1.8.6.6.7" class="ltx_td ltx_align_right ltx_border_t">24.4</td>
<td id="S3.T1.8.6.6.8" class="ltx_td ltx_align_right ltx_border_t">6.8</td>
<td id="S3.T1.8.6.6.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.8</td>
<td id="S3.T1.8.6.6.10" class="ltx_td ltx_align_center ltx_border_t">39.2</td>
</tr>
<tr id="S3.T1.9.7.7" class="ltx_tr">
<td id="S3.T1.9.7.7.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.9.7.7.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.9.7.7.4" class="ltx_td ltx_align_right">638M</td>
<td id="S3.T1.9.7.7.1" class="ltx_td ltx_align_right">52M<sup id="S3.T1.9.7.7.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S3.T1.9.7.7.5" class="ltx_td ltx_align_center ltx_border_r">Yes</td>
<td id="S3.T1.9.7.7.6" class="ltx_td ltx_align_center">39.9</td>
<td id="S3.T1.9.7.7.7" class="ltx_td ltx_align_right">23.5</td>
<td id="S3.T1.9.7.7.8" class="ltx_td ltx_align_right">7.4</td>
<td id="S3.T1.9.7.7.9" class="ltx_td ltx_align_right ltx_border_r">8.9</td>
<td id="S3.T1.9.7.7.10" class="ltx_td ltx_align_center">39.4</td>
</tr>
<tr id="S3.T1.10.8.27" class="ltx_tr">
<td id="S3.T1.10.8.27.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.27.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FBANK-Transformer</td>
<td id="S3.T1.10.8.27.3" class="ltx_td ltx_align_right ltx_border_t">7M</td>
<td id="S3.T1.10.8.27.4" class="ltx_td ltx_align_right ltx_border_t">7M</td>
<td id="S3.T1.10.8.27.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">No</td>
<td id="S3.T1.10.8.27.6" class="ltx_td ltx_align_center ltx_border_t">60.3</td>
<td id="S3.T1.10.8.27.7" class="ltx_td ltx_align_right ltx_border_t">38.3</td>
<td id="S3.T1.10.8.27.8" class="ltx_td ltx_align_right ltx_border_t">11.9</td>
<td id="S3.T1.10.8.27.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10.1</td>
<td id="S3.T1.10.8.27.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.10.8.28" class="ltx_tr">
<td id="S3.T1.10.8.28.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.28.2" class="ltx_td ltx_align_left ltx_border_r">HuBERT-Transformer</td>
<td id="S3.T1.10.8.28.3" class="ltx_td ltx_align_right">340M</td>
<td id="S3.T1.10.8.28.4" class="ltx_td ltx_align_right">23M</td>
<td id="S3.T1.10.8.28.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.28.6" class="ltx_td ltx_align_center">58.7</td>
<td id="S3.T1.10.8.28.7" class="ltx_td ltx_align_right">36.4</td>
<td id="S3.T1.10.8.28.8" class="ltx_td ltx_align_right">13.8</td>
<td id="S3.T1.10.8.28.9" class="ltx_td ltx_align_right ltx_border_r">8.4</td>
<td id="S3.T1.10.8.28.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.29" class="ltx_tr">
<td id="S3.T1.10.8.29.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T1.10.8.29.2" class="ltx_td ltx_align_left ltx_border_r">WavLM-Tranformer</td>
<td id="S3.T1.10.8.29.3" class="ltx_td ltx_align_right">339M</td>
<td id="S3.T1.10.8.29.4" class="ltx_td ltx_align_right">23M</td>
<td id="S3.T1.10.8.29.5" class="ltx_td ltx_align_center ltx_border_r">No</td>
<td id="S3.T1.10.8.29.6" class="ltx_td ltx_align_center">44.0</td>
<td id="S3.T1.10.8.29.7" class="ltx_td ltx_align_right">26.0</td>
<td id="S3.T1.10.8.29.8" class="ltx_td ltx_align_right">9.8</td>
<td id="S3.T1.10.8.29.9" class="ltx_td ltx_align_right ltx_border_r">8.1</td>
<td id="S3.T1.10.8.29.10" class="ltx_td"></td>
</tr>
<tr id="S3.T1.10.8.8" class="ltx_tr">
<td id="S3.T1.10.8.8.2" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="S3.T1.10.8.8.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">HuBERT+WavLM-Transformer</td>
<td id="S3.T1.10.8.8.4" class="ltx_td ltx_align_right ltx_border_bb">642M</td>
<td id="S3.T1.10.8.8.1" class="ltx_td ltx_align_right ltx_border_bb">10M<sup id="S3.T1.10.8.8.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S3.T1.10.8.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">No</td>
<td id="S3.T1.10.8.8.6" class="ltx_td ltx_align_center ltx_border_bb">42.2</td>
<td id="S3.T1.10.8.8.7" class="ltx_td ltx_align_right ltx_border_bb">25.4</td>
<td id="S3.T1.10.8.8.8" class="ltx_td ltx_align_right ltx_border_bb">8.1</td>
<td id="S3.T1.10.8.8.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">8.7</td>
<td id="S3.T1.10.8.8.10" class="ltx_td ltx_border_bb"></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Off-the-shelf ASR models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We start by evaluating how well off-the-shelf ASR models from the Whisper and NeMo families perform on the BPC domain without any fine-tuning. For the Whisper family, we use Whisper large-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> and Whisper large-v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>, which were trained on 680k hours and ~5M hours, respectively. They share the same architecture: 32 transformer layers with 20 attention heads per layer for both audio encoder and text decoder.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p">For the NeMo models, we test token-and-duration transducer (TDT)-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>, transducer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>, and connectionist temporal classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite> models, all of which were trained on 64k hours of English data. The architecture of these models is based on the fast conformer with linear attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>. NeMo models have 8 attention heads per layer, and come in several sizes: large (L, <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="17" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">17</annotation></semantics></math> layers and 120M parameters), extra large (XL, <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">24</annotation></semantics></math> layers and 616M parameters), and extra-extra large (XXL, <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="42" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">42</annotation></semantics></math> layers and 1.2B parameters).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Fine-tuned ASR models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.5" class="ltx_p">We fine-tune the pre-trained NeMo models to measure the extent to which optimizing parameters for the domain helps.
For fine-tuning experiments, we choose the CTC-based NeMo models because they have a good performance vs. speed tradeoff.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Based on the open ASR leaderboard: <a target="_blank" href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a>.</span></span></span>
We keep the original tokenizer when fine-tuning the models. We use the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite> optimizer with a weight decay of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="float" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">0.001</annotation></semantics></math>. We tune the learning rate over <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mn id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.2.m2.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.p1.2.m2.1.1.3.3a" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p1.2.m2.1.1.3.3.2" xref="S4.SS2.p1.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">2</cn><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"><minus id="S4.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">2\times 10^{-5}</annotation></semantics></math>, <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">×</mo><msup id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml"><mn id="S4.SS2.p1.3.m3.1.1.3.2" xref="S4.SS2.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.3.m3.1.1.3.3" xref="S4.SS2.p1.3.m3.1.1.3.3.cmml"><mo id="S4.SS2.p1.3.m3.1.1.3.3a" xref="S4.SS2.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p1.3.m3.1.1.3.3.2" xref="S4.SS2.p1.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">5</cn><apply id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.2">10</cn><apply id="S4.SS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3"><minus id="S4.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.p1.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">5\times 10^{-5}</annotation></semantics></math>, and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><msup id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml"><mo id="S4.SS2.p1.4.m4.1.1.3a" xref="S4.SS2.p1.4.m4.1.1.3.cmml">−</mo><mn id="S4.SS2.p1.4.m4.1.1.3.2" xref="S4.SS2.p1.4.m4.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">10</cn><apply id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3"><minus id="S4.SS2.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="S4.SS2.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.p1.4.m4.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">10^{-4}</annotation></semantics></math>, and select the best models based on WER on the dev set. We use cosine learning rate annealing with <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="5000" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mn id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><cn type="integer" id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">5000</annotation></semantics></math> warm-up steps.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.54.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:90%;">Top 10 most common ASR model confusions.
“X <math id="S4.T2.2.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.2.1.m1.1b"><mo stretchy="false" id="S4.T2.2.1.m1.1.1" xref="S4.T2.2.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><ci id="S4.T2.2.1.m1.1.1.cmml" xref="S4.T2.2.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">\longrightarrow</annotation></semantics></math> Y”
indicates that the ground-truth word “X” was misrecognized as the hypotheized word “Y”.</span></figcaption>
<table id="S4.T2.52" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.52.51" class="ltx_tr">
<td id="S4.T2.52.51.1" class="ltx_td ltx_align_center ltx_border_r" colspan="3">custom model, no LM</td>
<td id="S4.T2.52.51.2" class="ltx_td ltx_align_center ltx_border_r" colspan="3">custom model, LM</td>
<td id="S4.T2.52.51.3" class="ltx_td ltx_align_center ltx_border_r" colspan="3">NeMo, no fine-tune</td>
<td id="S4.T2.52.51.4" class="ltx_td ltx_align_center" colspan="3">NeMo, fine-tune</td>
<td id="S4.T2.52.51.5" class="ltx_td ltx_align_center" colspan="3">Whisper, large-v3</td>
</tr>
<tr id="S4.T2.7.5" class="ltx_tr">
<td id="S4.T2.7.5.6" class="ltx_td ltx_align_center ltx_border_t">the</td>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mo stretchy="false" id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><ci id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.7.5.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">a</td>
<td id="S4.T2.7.5.8" class="ltx_td ltx_align_center ltx_border_t">the</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.4.2.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.4.2.2.m1.1a"><mo stretchy="false" id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><ci id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.7.5.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">⟨unint⟩</td>
<td id="S4.T2.7.5.10" class="ltx_td ltx_align_center ltx_border_t">four</td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.5.3.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.5.3.3.m1.1a"><mo stretchy="false" id="S4.T2.5.3.3.m1.1.1" xref="S4.T2.5.3.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><ci id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.7.5.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">for</td>
<td id="S4.T2.7.5.12" class="ltx_td ltx_align_center ltx_border_t">one’s</td>
<td id="S4.T2.6.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.6.4.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.6.4.4.m1.1a"><mo stretchy="false" id="S4.T2.6.4.4.m1.1.1" xref="S4.T2.6.4.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.4.m1.1b"><ci id="S4.T2.6.4.4.m1.1.1.cmml" xref="S4.T2.6.4.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.7.5.13" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">one</td>
<td id="S4.T2.7.5.14" class="ltx_td ltx_align_center ltx_border_t">two</td>
<td id="S4.T2.7.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.7.5.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.7.5.5.m1.1a"><mo stretchy="false" id="S4.T2.7.5.5.m1.1.1" xref="S4.T2.7.5.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.5.5.m1.1b"><ci id="S4.T2.7.5.5.m1.1.1.cmml" xref="S4.T2.7.5.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.5.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.7.5.15" class="ltx_td ltx_align_right ltx_border_t">twenty</td>
</tr>
<tr id="S4.T2.12.10" class="ltx_tr">
<td id="S4.T2.12.10.6" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.8.6.1" class="ltx_td ltx_align_center"><math id="S4.T2.8.6.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.8.6.1.m1.1a"><mo stretchy="false" id="S4.T2.8.6.1.m1.1.1" xref="S4.T2.8.6.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.6.1.m1.1b"><ci id="S4.T2.8.6.1.m1.1.1.cmml" xref="S4.T2.8.6.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.6.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.12.10.7" class="ltx_td ltx_align_right ltx_border_r">the</td>
<td id="S4.T2.12.10.8" class="ltx_td ltx_align_center">four</td>
<td id="S4.T2.9.7.2" class="ltx_td ltx_align_center"><math id="S4.T2.9.7.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.9.7.2.m1.1a"><mo stretchy="false" id="S4.T2.9.7.2.m1.1.1" xref="S4.T2.9.7.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.7.2.m1.1b"><ci id="S4.T2.9.7.2.m1.1.1.cmml" xref="S4.T2.9.7.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.7.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.12.10.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.12.10.10" class="ltx_td ltx_align_center">two</td>
<td id="S4.T2.10.8.3" class="ltx_td ltx_align_center"><math id="S4.T2.10.8.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.10.8.3.m1.1a"><mo stretchy="false" id="S4.T2.10.8.3.m1.1.1" xref="S4.T2.10.8.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.8.3.m1.1b"><ci id="S4.T2.10.8.3.m1.1.1.cmml" xref="S4.T2.10.8.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.8.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.12.10.11" class="ltx_td ltx_align_right ltx_border_r">to</td>
<td id="S4.T2.12.10.12" class="ltx_td ltx_align_center">the</td>
<td id="S4.T2.11.9.4" class="ltx_td ltx_align_center"><math id="S4.T2.11.9.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.11.9.4.m1.1a"><mo stretchy="false" id="S4.T2.11.9.4.m1.1.1" xref="S4.T2.11.9.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.9.4.m1.1b"><ci id="S4.T2.11.9.4.m1.1.1.cmml" xref="S4.T2.11.9.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.9.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.12.10.13" class="ltx_td ltx_align_right ltx_border_r">a</td>
<td id="S4.T2.12.10.14" class="ltx_td ltx_align_center">three</td>
<td id="S4.T2.12.10.5" class="ltx_td ltx_align_center"><math id="S4.T2.12.10.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.12.10.5.m1.1a"><mo stretchy="false" id="S4.T2.12.10.5.m1.1.1" xref="S4.T2.12.10.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.10.5.m1.1b"><ci id="S4.T2.12.10.5.m1.1.1.cmml" xref="S4.T2.12.10.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.10.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.12.10.15" class="ltx_td ltx_align_right">thirty</td>
</tr>
<tr id="S4.T2.17.15" class="ltx_tr">
<td id="S4.T2.17.15.6" class="ltx_td ltx_align_center">oh</td>
<td id="S4.T2.13.11.1" class="ltx_td ltx_align_center"><math id="S4.T2.13.11.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.13.11.1.m1.1a"><mo stretchy="false" id="S4.T2.13.11.1.m1.1.1" xref="S4.T2.13.11.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.11.1.m1.1b"><ci id="S4.T2.13.11.1.m1.1.1.cmml" xref="S4.T2.13.11.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.11.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.17.15.7" class="ltx_td ltx_align_right ltx_border_r">zero</td>
<td id="S4.T2.17.15.8" class="ltx_td ltx_align_center">on</td>
<td id="S4.T2.14.12.2" class="ltx_td ltx_align_center"><math id="S4.T2.14.12.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.14.12.2.m1.1a"><mo stretchy="false" id="S4.T2.14.12.2.m1.1.1" xref="S4.T2.14.12.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.12.2.m1.1b"><ci id="S4.T2.14.12.2.m1.1.1.cmml" xref="S4.T2.14.12.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.12.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.17.15.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.17.15.10" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.15.13.3" class="ltx_td ltx_align_center"><math id="S4.T2.15.13.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.15.13.3.m1.1a"><mo stretchy="false" id="S4.T2.15.13.3.m1.1.1" xref="S4.T2.15.13.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.13.3.m1.1b"><ci id="S4.T2.15.13.3.m1.1.1.cmml" xref="S4.T2.15.13.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.13.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.17.15.11" class="ltx_td ltx_align_right ltx_border_r">the</td>
<td id="S4.T2.17.15.12" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.16.14.4" class="ltx_td ltx_align_center"><math id="S4.T2.16.14.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.16.14.4.m1.1a"><mo stretchy="false" id="S4.T2.16.14.4.m1.1.1" xref="S4.T2.16.14.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.14.4.m1.1b"><ci id="S4.T2.16.14.4.m1.1.1.cmml" xref="S4.T2.16.14.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.14.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.17.15.13" class="ltx_td ltx_align_right ltx_border_r">the</td>
<td id="S4.T2.17.15.14" class="ltx_td ltx_align_center">gonna</td>
<td id="S4.T2.17.15.5" class="ltx_td ltx_align_center"><math id="S4.T2.17.15.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.17.15.5.m1.1a"><mo stretchy="false" id="S4.T2.17.15.5.m1.1.1" xref="S4.T2.17.15.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.15.5.m1.1b"><ci id="S4.T2.17.15.5.m1.1.1.cmml" xref="S4.T2.17.15.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.15.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.17.15.15" class="ltx_td ltx_align_right">to</td>
</tr>
<tr id="S4.T2.22.20" class="ltx_tr">
<td id="S4.T2.22.20.6" class="ltx_td ltx_align_center">four</td>
<td id="S4.T2.18.16.1" class="ltx_td ltx_align_center"><math id="S4.T2.18.16.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.18.16.1.m1.1a"><mo stretchy="false" id="S4.T2.18.16.1.m1.1.1" xref="S4.T2.18.16.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.18.16.1.m1.1b"><ci id="S4.T2.18.16.1.m1.1.1.cmml" xref="S4.T2.18.16.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.16.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.22.20.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.22.20.8" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.19.17.2" class="ltx_td ltx_align_center"><math id="S4.T2.19.17.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.19.17.2.m1.1a"><mo stretchy="false" id="S4.T2.19.17.2.m1.1.1" xref="S4.T2.19.17.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.19.17.2.m1.1b"><ci id="S4.T2.19.17.2.m1.1.1.cmml" xref="S4.T2.19.17.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.17.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.22.20.9" class="ltx_td ltx_align_right ltx_border_r">the</td>
<td id="S4.T2.22.20.10" class="ltx_td ltx_align_center">gonna</td>
<td id="S4.T2.20.18.3" class="ltx_td ltx_align_center"><math id="S4.T2.20.18.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.20.18.3.m1.1a"><mo stretchy="false" id="S4.T2.20.18.3.m1.1.1" xref="S4.T2.20.18.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.20.18.3.m1.1b"><ci id="S4.T2.20.18.3.m1.1.1.cmml" xref="S4.T2.20.18.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.18.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.22.20.11" class="ltx_td ltx_align_right ltx_border_r">to</td>
<td id="S4.T2.22.20.12" class="ltx_td ltx_align_center">zero</td>
<td id="S4.T2.21.19.4" class="ltx_td ltx_align_center"><math id="S4.T2.21.19.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.21.19.4.m1.1a"><mo stretchy="false" id="S4.T2.21.19.4.m1.1.1" xref="S4.T2.21.19.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.19.4.m1.1b"><ci id="S4.T2.21.19.4.m1.1.1.cmml" xref="S4.T2.21.19.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.19.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.22.20.13" class="ltx_td ltx_align_right ltx_border_r">oh</td>
<td id="S4.T2.22.20.14" class="ltx_td ltx_align_center">zero</td>
<td id="S4.T2.22.20.5" class="ltx_td ltx_align_center"><math id="S4.T2.22.20.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.22.20.5.m1.1a"><mo stretchy="false" id="S4.T2.22.20.5.m1.1.1" xref="S4.T2.22.20.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.22.20.5.m1.1b"><ci id="S4.T2.22.20.5.m1.1.1.cmml" xref="S4.T2.22.20.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.20.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.22.20.15" class="ltx_td ltx_align_right">and</td>
</tr>
<tr id="S4.T2.27.25" class="ltx_tr">
<td id="S4.T2.27.25.6" class="ltx_td ltx_align_center">the</td>
<td id="S4.T2.23.21.1" class="ltx_td ltx_align_center"><math id="S4.T2.23.21.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.23.21.1.m1.1a"><mo stretchy="false" id="S4.T2.23.21.1.m1.1.1" xref="S4.T2.23.21.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.23.21.1.m1.1b"><ci id="S4.T2.23.21.1.m1.1.1.cmml" xref="S4.T2.23.21.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.21.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.27.25.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.27.25.8" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.24.22.2" class="ltx_td ltx_align_center"><math id="S4.T2.24.22.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.24.22.2.m1.1a"><mo stretchy="false" id="S4.T2.24.22.2.m1.1.1" xref="S4.T2.24.22.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.24.22.2.m1.1b"><ci id="S4.T2.24.22.2.m1.1.1.cmml" xref="S4.T2.24.22.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.22.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.27.25.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.27.25.10" class="ltx_td ltx_align_center">the</td>
<td id="S4.T2.25.23.3" class="ltx_td ltx_align_center"><math id="S4.T2.25.23.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.25.23.3.m1.1a"><mo stretchy="false" id="S4.T2.25.23.3.m1.1.1" xref="S4.T2.25.23.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.25.23.3.m1.1b"><ci id="S4.T2.25.23.3.m1.1.1.cmml" xref="S4.T2.25.23.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.23.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.27.25.11" class="ltx_td ltx_align_right ltx_border_r">a</td>
<td id="S4.T2.27.25.12" class="ltx_td ltx_align_center">ah</td>
<td id="S4.T2.26.24.4" class="ltx_td ltx_align_center"><math id="S4.T2.26.24.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.26.24.4.m1.1a"><mo stretchy="false" id="S4.T2.26.24.4.m1.1.1" xref="S4.T2.26.24.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.26.24.4.m1.1b"><ci id="S4.T2.26.24.4.m1.1.1.cmml" xref="S4.T2.26.24.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.24.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.27.25.13" class="ltx_td ltx_align_right ltx_border_r">uh</td>
<td id="S4.T2.27.25.14" class="ltx_td ltx_align_center">one</td>
<td id="S4.T2.27.25.5" class="ltx_td ltx_align_center"><math id="S4.T2.27.25.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.27.25.5.m1.1a"><mo stretchy="false" id="S4.T2.27.25.5.m1.1.1" xref="S4.T2.27.25.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.27.25.5.m1.1b"><ci id="S4.T2.27.25.5.m1.1.1.cmml" xref="S4.T2.27.25.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.25.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.27.25.15" class="ltx_td ltx_align_right">and</td>
</tr>
<tr id="S4.T2.32.30" class="ltx_tr">
<td id="S4.T2.32.30.6" class="ltx_td ltx_align_center">in</td>
<td id="S4.T2.28.26.1" class="ltx_td ltx_align_center"><math id="S4.T2.28.26.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.28.26.1.m1.1a"><mo stretchy="false" id="S4.T2.28.26.1.m1.1.1" xref="S4.T2.28.26.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.28.26.1.m1.1b"><ci id="S4.T2.28.26.1.m1.1.1.cmml" xref="S4.T2.28.26.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.26.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.32.30.7" class="ltx_td ltx_align_right ltx_border_r">and</td>
<td id="S4.T2.32.30.8" class="ltx_td ltx_align_center">and</td>
<td id="S4.T2.29.27.2" class="ltx_td ltx_align_center"><math id="S4.T2.29.27.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.29.27.2.m1.1a"><mo stretchy="false" id="S4.T2.29.27.2.m1.1.1" xref="S4.T2.29.27.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.29.27.2.m1.1b"><ci id="S4.T2.29.27.2.m1.1.1.cmml" xref="S4.T2.29.27.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.27.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.32.30.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.32.30.10" class="ltx_td ltx_align_center">two</td>
<td id="S4.T2.30.28.3" class="ltx_td ltx_align_center"><math id="S4.T2.30.28.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.30.28.3.m1.1a"><mo stretchy="false" id="S4.T2.30.28.3.m1.1.1" xref="S4.T2.30.28.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.30.28.3.m1.1b"><ci id="S4.T2.30.28.3.m1.1.1.cmml" xref="S4.T2.30.28.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.28.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.32.30.11" class="ltx_td ltx_align_right ltx_border_r">you</td>
<td id="S4.T2.32.30.12" class="ltx_td ltx_align_center">two</td>
<td id="S4.T2.31.29.4" class="ltx_td ltx_align_center"><math id="S4.T2.31.29.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.31.29.4.m1.1a"><mo stretchy="false" id="S4.T2.31.29.4.m1.1.1" xref="S4.T2.31.29.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.31.29.4.m1.1b"><ci id="S4.T2.31.29.4.m1.1.1.cmml" xref="S4.T2.31.29.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.29.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.32.30.13" class="ltx_td ltx_align_right ltx_border_r">six</td>
<td id="S4.T2.32.30.14" class="ltx_td ltx_align_center">six</td>
<td id="S4.T2.32.30.5" class="ltx_td ltx_align_center"><math id="S4.T2.32.30.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.32.30.5.m1.1a"><mo stretchy="false" id="S4.T2.32.30.5.m1.1.1" xref="S4.T2.32.30.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.32.30.5.m1.1b"><ci id="S4.T2.32.30.5.m1.1.1.cmml" xref="S4.T2.32.30.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.30.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.32.30.15" class="ltx_td ltx_align_right">sixty</td>
</tr>
<tr id="S4.T2.37.35" class="ltx_tr">
<td id="S4.T2.37.35.6" class="ltx_td ltx_align_center">on</td>
<td id="S4.T2.33.31.1" class="ltx_td ltx_align_center"><math id="S4.T2.33.31.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.33.31.1.m1.1a"><mo stretchy="false" id="S4.T2.33.31.1.m1.1.1" xref="S4.T2.33.31.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.33.31.1.m1.1b"><ci id="S4.T2.33.31.1.m1.1.1.cmml" xref="S4.T2.33.31.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.31.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.37.35.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.37.35.8" class="ltx_td ltx_align_center">the</td>
<td id="S4.T2.34.32.2" class="ltx_td ltx_align_center"><math id="S4.T2.34.32.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.34.32.2.m1.1a"><mo stretchy="false" id="S4.T2.34.32.2.m1.1.1" xref="S4.T2.34.32.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.34.32.2.m1.1b"><ci id="S4.T2.34.32.2.m1.1.1.cmml" xref="S4.T2.34.32.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.34.32.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.37.35.9" class="ltx_td ltx_align_right ltx_border_r">a</td>
<td id="S4.T2.37.35.10" class="ltx_td ltx_align_center">and</td>
<td id="S4.T2.35.33.3" class="ltx_td ltx_align_center"><math id="S4.T2.35.33.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.35.33.3.m1.1a"><mo stretchy="false" id="S4.T2.35.33.3.m1.1.1" xref="S4.T2.35.33.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.35.33.3.m1.1b"><ci id="S4.T2.35.33.3.m1.1.1.cmml" xref="S4.T2.35.33.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.33.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.37.35.11" class="ltx_td ltx_align_right ltx_border_r">in</td>
<td id="S4.T2.37.35.12" class="ltx_td ltx_align_center">⟨unint⟩</td>
<td id="S4.T2.36.34.4" class="ltx_td ltx_align_center"><math id="S4.T2.36.34.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.36.34.4.m1.1a"><mo stretchy="false" id="S4.T2.36.34.4.m1.1.1" xref="S4.T2.36.34.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.36.34.4.m1.1b"><ci id="S4.T2.36.34.4.m1.1.1.cmml" xref="S4.T2.36.34.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.34.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.37.35.13" class="ltx_td ltx_align_right ltx_border_r">uh</td>
<td id="S4.T2.37.35.14" class="ltx_td ltx_align_center">five</td>
<td id="S4.T2.37.35.5" class="ltx_td ltx_align_center"><math id="S4.T2.37.35.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.37.35.5.m1.1a"><mo stretchy="false" id="S4.T2.37.35.5.m1.1.1" xref="S4.T2.37.35.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.37.35.5.m1.1b"><ci id="S4.T2.37.35.5.m1.1.1.cmml" xref="S4.T2.37.35.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.35.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.37.35.15" class="ltx_td ltx_align_right">fifty</td>
</tr>
<tr id="S4.T2.42.40" class="ltx_tr">
<td id="S4.T2.42.40.6" class="ltx_td ltx_align_center">and</td>
<td id="S4.T2.38.36.1" class="ltx_td ltx_align_center"><math id="S4.T2.38.36.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.38.36.1.m1.1a"><mo stretchy="false" id="S4.T2.38.36.1.m1.1.1" xref="S4.T2.38.36.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.38.36.1.m1.1b"><ci id="S4.T2.38.36.1.m1.1.1.cmml" xref="S4.T2.38.36.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.36.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.42.40.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.42.40.8" class="ltx_td ltx_align_center">two</td>
<td id="S4.T2.39.37.2" class="ltx_td ltx_align_center"><math id="S4.T2.39.37.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.39.37.2.m1.1a"><mo stretchy="false" id="S4.T2.39.37.2.m1.1.1" xref="S4.T2.39.37.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.39.37.2.m1.1b"><ci id="S4.T2.39.37.2.m1.1.1.cmml" xref="S4.T2.39.37.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.37.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.42.40.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.42.40.10" class="ltx_td ltx_align_center">one’s</td>
<td id="S4.T2.40.38.3" class="ltx_td ltx_align_center"><math id="S4.T2.40.38.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.40.38.3.m1.1a"><mo stretchy="false" id="S4.T2.40.38.3.m1.1.1" xref="S4.T2.40.38.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.40.38.3.m1.1b"><ci id="S4.T2.40.38.3.m1.1.1.cmml" xref="S4.T2.40.38.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.38.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.42.40.11" class="ltx_td ltx_align_right ltx_border_r">one</td>
<td id="S4.T2.42.40.12" class="ltx_td ltx_align_center">seven</td>
<td id="S4.T2.41.39.4" class="ltx_td ltx_align_center"><math id="S4.T2.41.39.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.41.39.4.m1.1a"><mo stretchy="false" id="S4.T2.41.39.4.m1.1.1" xref="S4.T2.41.39.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.41.39.4.m1.1b"><ci id="S4.T2.41.39.4.m1.1.1.cmml" xref="S4.T2.41.39.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.39.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.42.40.13" class="ltx_td ltx_align_right ltx_border_r">eleven</td>
<td id="S4.T2.42.40.14" class="ltx_td ltx_align_center">four</td>
<td id="S4.T2.42.40.5" class="ltx_td ltx_align_center"><math id="S4.T2.42.40.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.42.40.5.m1.1a"><mo stretchy="false" id="S4.T2.42.40.5.m1.1.1" xref="S4.T2.42.40.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.42.40.5.m1.1b"><ci id="S4.T2.42.40.5.m1.1.1.cmml" xref="S4.T2.42.40.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.40.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.42.40.15" class="ltx_td ltx_align_right">forty</td>
</tr>
<tr id="S4.T2.47.45" class="ltx_tr">
<td id="S4.T2.47.45.6" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.43.41.1" class="ltx_td ltx_align_center"><math id="S4.T2.43.41.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.43.41.1.m1.1a"><mo stretchy="false" id="S4.T2.43.41.1.m1.1.1" xref="S4.T2.43.41.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.43.41.1.m1.1b"><ci id="S4.T2.43.41.1.m1.1.1.cmml" xref="S4.T2.43.41.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.41.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.47.45.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.47.45.8" class="ltx_td ltx_align_center">you</td>
<td id="S4.T2.44.42.2" class="ltx_td ltx_align_center"><math id="S4.T2.44.42.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.44.42.2.m1.1a"><mo stretchy="false" id="S4.T2.44.42.2.m1.1.1" xref="S4.T2.44.42.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.44.42.2.m1.1b"><ci id="S4.T2.44.42.2.m1.1.1.cmml" xref="S4.T2.44.42.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.44.42.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.47.45.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.47.45.10" class="ltx_td ltx_align_center">uh</td>
<td id="S4.T2.45.43.3" class="ltx_td ltx_align_center"><math id="S4.T2.45.43.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.45.43.3.m1.1a"><mo stretchy="false" id="S4.T2.45.43.3.m1.1.1" xref="S4.T2.45.43.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.45.43.3.m1.1b"><ci id="S4.T2.45.43.3.m1.1.1.cmml" xref="S4.T2.45.43.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.45.43.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.47.45.11" class="ltx_td ltx_align_right ltx_border_r">ah</td>
<td id="S4.T2.47.45.12" class="ltx_td ltx_align_center">three</td>
<td id="S4.T2.46.44.4" class="ltx_td ltx_align_center"><math id="S4.T2.46.44.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.46.44.4.m1.1a"><mo stretchy="false" id="S4.T2.46.44.4.m1.1.1" xref="S4.T2.46.44.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.46.44.4.m1.1b"><ci id="S4.T2.46.44.4.m1.1.1.cmml" xref="S4.T2.46.44.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.46.44.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.47.45.13" class="ltx_td ltx_align_right ltx_border_r">two</td>
<td id="S4.T2.47.45.14" class="ltx_td ltx_align_center">four</td>
<td id="S4.T2.47.45.5" class="ltx_td ltx_align_center"><math id="S4.T2.47.45.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.47.45.5.m1.1a"><mo stretchy="false" id="S4.T2.47.45.5.m1.1.1" xref="S4.T2.47.45.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.47.45.5.m1.1b"><ci id="S4.T2.47.45.5.m1.1.1.cmml" xref="S4.T2.47.45.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.47.45.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.47.45.15" class="ltx_td ltx_align_right">you</td>
</tr>
<tr id="S4.T2.52.50" class="ltx_tr">
<td id="S4.T2.52.50.6" class="ltx_td ltx_align_center">two</td>
<td id="S4.T2.48.46.1" class="ltx_td ltx_align_center"><math id="S4.T2.48.46.1.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.48.46.1.m1.1a"><mo stretchy="false" id="S4.T2.48.46.1.m1.1.1" xref="S4.T2.48.46.1.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.48.46.1.m1.1b"><ci id="S4.T2.48.46.1.m1.1.1.cmml" xref="S4.T2.48.46.1.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.48.46.1.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.52.50.7" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.52.50.8" class="ltx_td ltx_align_center">to</td>
<td id="S4.T2.49.47.2" class="ltx_td ltx_align_center"><math id="S4.T2.49.47.2.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.49.47.2.m1.1a"><mo stretchy="false" id="S4.T2.49.47.2.m1.1.1" xref="S4.T2.49.47.2.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.49.47.2.m1.1b"><ci id="S4.T2.49.47.2.m1.1.1.cmml" xref="S4.T2.49.47.2.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.49.47.2.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.52.50.9" class="ltx_td ltx_align_right ltx_border_r">⟨unint⟩</td>
<td id="S4.T2.52.50.10" class="ltx_td ltx_align_center">robert</td>
<td id="S4.T2.50.48.3" class="ltx_td ltx_align_center"><math id="S4.T2.50.48.3.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.50.48.3.m1.1a"><mo stretchy="false" id="S4.T2.50.48.3.m1.1.1" xref="S4.T2.50.48.3.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.50.48.3.m1.1b"><ci id="S4.T2.50.48.3.m1.1.1.cmml" xref="S4.T2.50.48.3.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.50.48.3.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.52.50.11" class="ltx_td ltx_align_right ltx_border_r">roberts</td>
<td id="S4.T2.52.50.12" class="ltx_td ltx_align_center">thirty</td>
<td id="S4.T2.51.49.4" class="ltx_td ltx_align_center"><math id="S4.T2.51.49.4.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.51.49.4.m1.1a"><mo stretchy="false" id="S4.T2.51.49.4.m1.1.1" xref="S4.T2.51.49.4.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.51.49.4.m1.1b"><ci id="S4.T2.51.49.4.m1.1.1.cmml" xref="S4.T2.51.49.4.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.51.49.4.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.52.50.13" class="ltx_td ltx_align_right ltx_border_r">three</td>
<td id="S4.T2.52.50.14" class="ltx_td ltx_align_center">a</td>
<td id="S4.T2.52.50.5" class="ltx_td ltx_align_center"><math id="S4.T2.52.50.5.m1.1" class="ltx_Math" alttext="\longrightarrow" display="inline"><semantics id="S4.T2.52.50.5.m1.1a"><mo stretchy="false" id="S4.T2.52.50.5.m1.1.1" xref="S4.T2.52.50.5.m1.1.1.cmml">⟶</mo><annotation-xml encoding="MathML-Content" id="S4.T2.52.50.5.m1.1b"><ci id="S4.T2.52.50.5.m1.1.1.cmml" xref="S4.T2.52.50.5.m1.1.1">⟶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.52.50.5.m1.1c">\longrightarrow</annotation></semantics></math></td>
<td id="S4.T2.52.50.15" class="ltx_td ltx_align_right">the</td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Customized E2E ASR models</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We implement our customized models using ESPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite> and S3PRL (for self-supervised learning (SSL) model support) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite>. These models test the value of adapting the parameters, token vocabulary and language model to the domain. We use hybrid CTC - attention-based encoder decoder (CTC-AED) models due to their good performance across many tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite>. We explore both transformer and conformer encoder architectures. Our transformer encoders have 12 layers with 8 attention heads per layer and an output dimension of 128. The conformer encoders consist of 12 layers with 4 attention heads per layer and output dimension of 256. For all experiments, the decoder is a 6-layer transformer. These hyperparameters were chosen based on tuning experiments on the validation set. We also integrate an RNN (specifically LSTM) language model (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite> trained on BPC-CPD, which has a perplexity of 11.6 on the dev set.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For each model, we test four feature extractors: log Mel-filterbank features (FBANK), HuBERT Large (300M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>, WavLM Large (300M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite>, and a feature fusion model, combining representations learned by HuBERT and WavLM. HuBERT and WavLM were chosen based on their consistently good performance in various benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>, <a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite>. As is common in recent benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite>, we use a weighted sum of frozen self-supervised model layers as input, with the weights learned during fine-tuning. For the feature fusion experiment, we follow the approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>, <a href="#bib.bibx36" title="" class="ltx_ref">36</a>, <a href="#bib.bibx37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and analysis</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>ASR performance</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Inter-annotator agreement ‣ 3 Data ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our main ASR results. Among the pre-trained ASR models, Whisper large-v3 outperforms Whisper large-v2 by a large margin, but is slightly worse than NeMo models in general. For NeMo models, the larger models sometimes but not always improve WER, suggesting that scaling up the model does not necessarily address domain differences. In terms of model type (CTC, transducer, and TDT), the transducer and TDT models have better performance in general. We leave fine-tuning these two models as future work.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">After fine-tuning the NeMo Fast-Conformer CTC model on BPC data, we see a dramatic improvement in WER, suggesting that fine-tuning can bridge much of the domain difference between the pre-trained model and the police radio domain. See Section <a href="#S5.SS2" title="5.2 Error analysis ‣ 5 Results and analysis ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> for further analysis.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">For the customized E2E models, conformer based models outperform their transformer counterparts. Interestingly, the performance of the FBANK-conformer model is similar to that of many of the pre-trained models in the NeMo and Whisper families, suggesting that off-the-shelf ASR can be replaced with smaller models if some domain data is available for training. The language model provides a small improvement to most of the ASR models, but the best-performing CTC-AED model shows the least effect from the use of the language model. Overall, the usefulness of a language model for this domain is unclear, and merits further study with different language models.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.4" class="ltx_p">Performance on the “unseen” Zone 5 (WER<math id="S5.SS1.p4.1.m1.1" class="ltx_math_unparsed" alttext="{}_{dev}=30\%" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1b"><mmultiscripts id="S5.SS1.p4.1.m1.1.1"><mo id="S5.SS1.p4.1.m1.1.1.2">=</mo><mprescripts id="S5.SS1.p4.1.m1.1.1a"></mprescripts><mrow id="S5.SS1.p4.1.m1.1.1.3"><mi id="S5.SS1.p4.1.m1.1.1.3.2">d</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.1.m1.1.1.3.1">​</mo><mi id="S5.SS1.p4.1.m1.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.1.m1.1.1.3.1a">​</mo><mi id="S5.SS1.p4.1.m1.1.1.3.4">v</mi></mrow><mrow id="S5.SS1.p4.1.m1.1.1b"></mrow></mmultiscripts><mn id="S5.SS1.p4.1.m1.1.2">30</mn><mo id="S5.SS1.p4.1.m1.1.3">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">{}_{dev}=30\%</annotation></semantics></math>, WER<math id="S5.SS1.p4.2.m2.1" class="ltx_math_unparsed" alttext="{}_{test}=31\%" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mrow id="S5.SS1.p4.2.m2.1b"><mmultiscripts id="S5.SS1.p4.2.m2.1.1"><mo id="S5.SS1.p4.2.m2.1.1.2">=</mo><mprescripts id="S5.SS1.p4.2.m2.1.1a"></mprescripts><mrow id="S5.SS1.p4.2.m2.1.1.3"><mi id="S5.SS1.p4.2.m2.1.1.3.2">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.2.m2.1.1.3.1">​</mo><mi id="S5.SS1.p4.2.m2.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.2.m2.1.1.3.1a">​</mo><mi id="S5.SS1.p4.2.m2.1.1.3.4">s</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.2.m2.1.1.3.1b">​</mo><mi id="S5.SS1.p4.2.m2.1.1.3.5">t</mi></mrow><mrow id="S5.SS1.p4.2.m2.1.1b"></mrow></mmultiscripts><mn id="S5.SS1.p4.2.m2.1.2">31</mn><mo id="S5.SS1.p4.2.m2.1.3">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">{}_{test}=31\%</annotation></semantics></math>) and Zone 6 (WER<sub id="S5.SS1.p4.4.1" class="ltx_sub"><span id="S5.SS1.p4.4.1.1" class="ltx_text ltx_font_italic">dev</span></sub> =WER<math id="S5.SS1.p4.4.m4.1" class="ltx_math_unparsed" alttext="{}_{test}=30\%" display="inline"><semantics id="S5.SS1.p4.4.m4.1a"><mrow id="S5.SS1.p4.4.m4.1b"><mmultiscripts id="S5.SS1.p4.4.m4.1.1"><mo id="S5.SS1.p4.4.m4.1.1.2">=</mo><mprescripts id="S5.SS1.p4.4.m4.1.1a"></mprescripts><mrow id="S5.SS1.p4.4.m4.1.1.3"><mi id="S5.SS1.p4.4.m4.1.1.3.2">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.4.m4.1.1.3.1">​</mo><mi id="S5.SS1.p4.4.m4.1.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.4.m4.1.1.3.1a">​</mo><mi id="S5.SS1.p4.4.m4.1.1.3.4">s</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.4.m4.1.1.3.1b">​</mo><mi id="S5.SS1.p4.4.m4.1.1.3.5">t</mi></mrow><mrow id="S5.SS1.p4.4.m4.1.1b"></mrow></mmultiscripts><mn id="S5.SS1.p4.4.m4.1.2">30</mn><mo id="S5.SS1.p4.4.m4.1.3">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">{}_{test}=30\%</annotation></semantics></math>)
is almost as good as on the previously seen zones.
This robustness check is especially useful since dispatch zones differ in terms of speakers, locations, and radio instrumentation (e.g., location/configuration of antenna).
</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2409.10858/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="183" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span id="S5.F2.3.2" class="ltx_text" style="font-size:90%;">WER vs. audio quality (SI-SDR) and utterance duration for NeMo FastConformer CTC (616M) on the dev set.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Error analysis</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Common errors</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:confusions</span> shows the most common ASR model confusions for five of our top-performing models. The off-the-shelf models have the most trouble with domain-specific words like digits and “robert” (code word for the night shift). The custom model, as well as the NeMo model after fine-tuning, do not suffer from as many of these domain-specific confusions, instead having more common stop words (“a”, “the”) among their top errors, as do human annotators. In addition, neither NeMo nor Whisper have the “unintelligible” token in their vocabularies, so they cannot label a speech segment as unintelligible. On the other hand, the custom models often predict the unintelligible token incorrectly, and it features prominently among their most common errors.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Effect of utterance length and audio quality on errors</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 ASR performance ‣ 5 Results and analysis ‣ Speech Recognition for Analysis of Police Radio Communication" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows scatterplots of the per-utterance WERs of NeMo FastConformer CTC (616M) for utterances of different lengths and estimated audio quality levels. We include results for both off-the-shelf and fine-tuned models, along with the Spearman correlation coefficient between WER and duration/audio quality.
Audio quality is estimated via a neural quality estimator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">38</a>]</cite> trained to predict SI-SDR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">39</a>]</cite>. Utterance duration exhibits a weak inverse relationship with WER, and fine-tuning appears to mitigate this relationship (i.e., the correlation between WER and duration is decreased). Audio quality also has a weak inverse relationship with WER, which also flattens after fine-tuning.
Overall, fine-tuning has the intended effect of both reducing WER (globally and among highly prevalent short utterances) <span id="S5.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">and</span> reducing the effect of audio quality on WER.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and future work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our study of automatic speech recognition on the BPC domain, which serves a functional role in police communication, is the first step toward better automated analysis of this form of police communication. Our findings provide a baseline assessment of how current speech recognition models perform in this challenging domain. We provide access to the BPC-CPD corpus so that others can develop and test novel solutions to the challenges associated with BPC, and to enable future analysis of policing practices through BPC.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Perhaps as expected given the domain, we have found that existing pre-trained models perform poorly (WER <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S6.p2.1.m1.1a"><mo id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><geq id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">\geq</annotation></semantics></math> ~50%).
Fine-tuning is effective, with our best performing model achieving a WER of 27.3%, which is roughly in the same range as inter-annotator agreement on our data. The relationship between WER and estimated audio quality (SI-SDR) is weak, suggesting that explicit denoising methods are unlikely to be of great help. We also found that fine-tuning mitigates some of the impact of noise.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">One limitation of this work is the location-specific source of BPC data. BPC from other police systems may exhibit different properties with fewer, different, or more challenges. However, there are also many shared properties across police radio domains in different locations, so we believe that our experience can help guide researchers studying this data domain elsewhere.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Considering the difficulty of annotation in this domain, future work includes approaches for effectively using unlabeled data in the domain, and analysis of how the quantity and quality of labeled data impacts performance. Further inter-annotator analysis may also allow us to assess, for example, whether audio marked unintelligible is linked to miscommunication.
Considering the prolific use of BPC in police departments around the world, this kind of exploration is needed to understand how BPC shapes officer decision-making.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgments</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Research supported by National Institute of Minority Health and Health Disparities of the National Institutes of Health under award number R01MD015064. This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Rebecca C Hetey et al.
</span>
<span class="ltx_bibblock">““When the Cruiser Lights Come On”: Using the Science of Bias &amp; Culture to Combat Racial Disparities in Policing”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Dædalus</em> <span id="bib.bibx1.2.2" class="ltx_text ltx_font_bold">153</span>, 2024
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Rob Voigt et al.
</span>
<span class="ltx_bibblock">“Language from police body camera footage shows racial disparities in officer respect”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">114</span>, 2017
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Anjalie Field et al.
</span>
<span class="ltx_bibblock">“Developing Speech Processing Pipelines for Police Accountability”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Eugenia H Rho et al.
</span>
<span class="ltx_bibblock">“Escalated police stops of Black men are linguistically and psychologically distinct in their earliest moments”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em> <span id="bib.bibx4.2.2" class="ltx_text ltx_font_bold">120.23</span>, 2023
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Robert Vargas, Kayla Preito-Hodge and Jeremy Christofferson
</span>
<span class="ltx_bibblock">“Digital Vulnerability: The Unequal Risk of E-Contact with the Criminal Justice System”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">RSF: The Russell Sage Foundation Journal of the Social Sciences</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">5</span>, 2019
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">P.N. Venkit et al.
</span>
<span class="ltx_bibblock">“Race and Privacy in Broadcast Police Communications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Proc. ACM CHI</em>, 2024
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Citizens’ Police Committee
</span>
<span class="ltx_bibblock">“Chicago police problems”
</span>
<span class="ltx_bibblock">Chicago, IL: University of Chicago Press, 1931
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Joseph A. Poli
</span>
<span class="ltx_bibblock">“Development and Present Trend of Police Radio Communications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Journal of Criminal Law and Criminology</em> <span id="bib.bibx8.2.2" class="ltx_text ltx_font_bold">33</span>, 1942
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Vinodkumar Prabhakaran et al.
</span>
<span class="ltx_bibblock">“Detecting institutional dialog acts in police traffic stops”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em> <span id="bib.bibx9.2.2" class="ltx_text ltx_font_bold">6</span>, 2018
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Vassil Panayotov et al.
</span>
<span class="ltx_bibblock">“Librispeech: An ASR Corpus based on public domain audio books”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2015
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Alec Radford et al.
</span>
<span class="ltx_bibblock">“Robust Speech Recognition via Large-Scale Weak Supervision”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Yifan Peng et al.
</span>
<span class="ltx_bibblock">“Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">Proc. ASRU</em>, 2023
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Wei Kang et al.
</span>
<span class="ltx_bibblock">“Libriheavy: A 50,000 Hours ASR Corpus with Punctuation Casing and Context”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2024
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP48485.2024.10447120" title="" class="ltx_ref ltx_href">10.1109/ICASSP48485.2024.10447120</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Shinji Watanabe et al.
</span>
<span class="ltx_bibblock">“CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv:2004.09249</em>, 2020
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Steve Renals, Thomas Hain and Hervé Bourlard
</span>
<span class="ltx_bibblock">“Recognition and understanding of meetings the AMI and AMIDA projects”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Proc. ASRU</em>, 2007
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Ramon Sanabria et al.
</span>
<span class="ltx_bibblock">“The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2023
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Thomas Pellegrini et al.
</span>
<span class="ltx_bibblock">“The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2019
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.21437/Interspeech.2019-1962" title="" class="ltx_ref ltx_href">10.21437/Interspeech.2019-1962</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">J. Zuluaga-Gomez et al.
</span>
<span class="ltx_bibblock">“How does pre-trained wav2vec 2.0 perform on domain-shifted ASR? An extensive benchmark on air traffic control communications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Proc. SLT</em>, 2023
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Emma Pierson et al.
</span>
<span class="ltx_bibblock">“A large-scale analysis of racial disparities in police stops across the United States”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Nature Human Behaviour</em>, 2020
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Steven Bird, Ewan Klein and Edward Loper
</span>
<span class="ltx_bibblock">“Natural language processing with Python”
</span>
<span class="ltx_bibblock">O’Reilly Media, Inc., 2009
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Yang Zhang, Evelina Bakhturina and Boris Ginsburg
</span>
<span class="ltx_bibblock">“NeMo (Inverse) Text Normalization: From Development to Production”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2021
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Evelina Bakhturina, Yang Zhang and Boris Ginsburg
</span>
<span class="ltx_bibblock">“Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“Whisper: Robust Speech Recognition via Large-Scale Weak Supervision”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">GitHub repository</em>
</span>
<span class="ltx_bibblock">GitHub, <a target="_blank" href="https://github.com/openai/whisper" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper</a>, 2022
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Hainan Xu et al.
</span>
<span class="ltx_bibblock">“Efficient Sequence Transduction by Jointly Predicting Tokens and Durations”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Alex Graves
</span>
<span class="ltx_bibblock">“Sequence Transduction with Recurrent Neural Networks”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, 2012
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Alex Graves et al.
</span>
<span class="ltx_bibblock">“Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2006
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Dima Rekesh et al.
</span>
<span class="ltx_bibblock">“Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">Proc. ASRU</em>, 2023
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Ilya Loshchilov and Frank Hutter
</span>
<span class="ltx_bibblock">“Decoupled Weight Decay Regularization”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2017
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Shinji Watanabe et al.
</span>
<span class="ltx_bibblock">“ESPnet: End-to-End Speech Processing Toolkit”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2018
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Shu-Wen Yang et al.
</span>
<span class="ltx_bibblock">“SUPERB: Speech processing Universal PERformance Benchmark”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2021
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Shinji Watanabe et al.
</span>
<span class="ltx_bibblock">“Hybrid CTC/Attention Architecture for End-to-End Speech Recognition”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 2017
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Tomáš Mikolov et al.
</span>
<span class="ltx_bibblock">“Recurrent neural network based language model”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2010
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Wei-Ning Hsu et al.
</span>
<span class="ltx_bibblock">“HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2021
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Sanyuan Chen et al.
</span>
<span class="ltx_bibblock">“WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 2021
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Dan Berrebbi et al.
</span>
<span class="ltx_bibblock">“Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Szu-Jui Chen, Jiamin Xie and John H.. Hansen
</span>
<span class="ltx_bibblock">“FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Tejes Srivastava et al.
</span>
<span class="ltx_bibblock">“EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Low Resource and Multilingual Scenarios”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2024
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Anurag Kumar et al.
</span>
<span class="ltx_bibblock">“Torchaudio-squim: Reference-less speech quality and intelligibility measures in torchaudio”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2023
</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Jonathan Le Roux et al.
</span>
<span class="ltx_bibblock">“SDR – Half-baked or Well Done?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Proc. ICAASP</em>, 2019
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.10857" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.10858" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.10858">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.10858" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.10859" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:22:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
