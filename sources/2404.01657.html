<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.01657] Release of Pre-Trained Models for the Japanese Language</title><meta property="og:description" content="AI democratization aims to create a world in which the average person can utilize AI techniques.
To achieve this goal, numerous research institutes have attempted to make their results accessible to the public.
In part…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Release of Pre-Trained Models for the Japanese Language">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Release of Pre-Trained Models for the Japanese Language">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.01657">

<!--Generated on Sun May  5 21:58:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Release of Pre-Trained Models for the Japanese Language</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">AI democratization aims to create a world in which the average person can utilize AI techniques.
To achieve this goal, numerous research institutes have attempted to make their results accessible to the public.
In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact.
However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly.
To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese.
By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI.
Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.


<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>AI democratization, pre-trained model, Japanese language</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Release of Pre-Trained Models for the Japanese Language</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui,</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Akio Kaga, Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda</span></td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">rinna Co., Ltd., Tokyo, Japan</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">keisawada@rinna.co.jp</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As AI technology advances, the idea of “AI democratization,” which aims to create a world where everyone can easily use AI, has become widely popular.
To contribute to AI democratization, many research institutions and companies are publicly releasing their latest methods, source codes, databases, and pre-trained models.
Such steps are essential for supporting the rapid development of AI technology in the future.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, methods using large-scale pre-trained models based on massive training data have achieved significant results and have become mainstream.
The advent of self-supervised learning, which generates pseudo-ground-truth labels from training data, coupled with the introduction of the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, which enables efficient and accurate model training from massive data, has made large-scale modeling possible.
The Generative Pre-trained Transformer (GPT, <cite class="ltx_cite ltx_citemacro_citet">Radford et al. <a href="#bib.bib16" title="" class="ltx_ref">2018</a></cite>) series engendered a breakthrough in text generation using self-supervised learning and Transformer architectures by discovering a scaling law suggesting that the performance improves as the model size, amount of training data, and computation increase <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.
As a result, the size of the pre-trained models has escalated dramatically in the text domain as well as the image and speech domains.
However, training high-performance pre-trained models incurs significant costs, such as creating training corpora and securing computational resources, making it infeasible for everyone to undertake pre-training easily.
Fortunately, there is an active trend of releasing pre-trained models on platforms such as Hugging Face, and such models are now available.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While there is vibrant activity in the publishing of pre-trained models, many pre-trained models targeting languages are specialized for English.
Consequently, AI democratization lags in non-English-speaking regions compared with English-speaking regions.
Research is underway on multilingual models that support several languages.
However, these multilingual models tend to have an increased number of parameters and often underperform compared with models specialized for a particular language given a fixed compute budget <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address this issue in Japanese, we released pre-trained models optimized for Japanese on Hugging Face.
By providing pre-trained models specialized for Japanese, we hope that users can freely access a model that aligns with Japanese cultural values but also ensures the identity of Japanese culture, leading to a more inclusive AI democratization that does not solely lean towards English-centric perspectives.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Japanese Pre-Trained Models</h2>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-trained model</span></th>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model size</span></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">License</span></td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Release date</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<th id="S2.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Language model</th>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<th id="S2.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.3.3.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-xsmall</span>
</th>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_right">37M</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_right">August 2021</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<th id="S2.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.4.4.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-small</span>
</th>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_right">110M</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_right">August 2021</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<th id="S2.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.5.5.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-medium</span>
</th>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_right">336M</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_right">April 2021</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<th id="S2.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.6.6.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-1b</span>
</th>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_right">1.3B</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_right">January 2022</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<th id="S2.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.7.7.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-small</span>
</th>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_right">110M</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_right">September 2022</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<th id="S2.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.8.8.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-3.6b</span>
</th>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_right">3.6B</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_right">May 2023</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<th id="S2.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.9.9.1.1" class="ltx_text ltx_font_italic">rinna/bilingual-gpt-neox-4b</span>
</th>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_right">4B</td>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_right">MIT</td>
<td id="S2.T1.1.9.9.4" class="ltx_td ltx_align_right">July 2023</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<th id="S2.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Language-image model</th>
<td id="S2.T1.1.10.10.2" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.10.10.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<th id="S2.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.11.11.1.1" class="ltx_text ltx_font_italic">rinna/japanese-clip-vit-b-16</span>
</th>
<td id="S2.T1.1.11.11.2" class="ltx_td ltx_align_right">197M</td>
<td id="S2.T1.1.11.11.3" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="S2.T1.1.11.11.4" class="ltx_td ltx_align_right">May 2022</td>
</tr>
<tr id="S2.T1.1.12.12" class="ltx_tr">
<th id="S2.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.12.12.1.1" class="ltx_text ltx_font_italic">rinna/japanese-cloob-vit-b-16</span>
</th>
<td id="S2.T1.1.12.12.2" class="ltx_td ltx_align_right">197M</td>
<td id="S2.T1.1.12.12.3" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="S2.T1.1.12.12.4" class="ltx_td ltx_align_right">May 2022</td>
</tr>
<tr id="S2.T1.1.13.13" class="ltx_tr">
<th id="S2.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span id="S2.T1.1.13.13.1.1" class="ltx_text ltx_font_italic">rinna/japanese-stable-diffusion</span>
</th>
<td id="S2.T1.1.13.13.2" class="ltx_td ltx_align_right">1.1B</td>
<td id="S2.T1.1.13.13.3" class="ltx_td ltx_align_right">CreativeML OpenRAIL M</td>
<td id="S2.T1.1.13.13.4" class="ltx_td ltx_align_right">September 2022</td>
</tr>
<tr id="S2.T1.1.14.14" class="ltx_tr">
<th id="S2.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Speech model</th>
<td id="S2.T1.1.14.14.2" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.14.14.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.14.14.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.15.15" class="ltx_tr">
<th id="S2.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   <span id="S2.T1.1.15.15.1.1" class="ltx_text ltx_font_italic">rinna/japanese-hubert-base</span>
</th>
<td id="S2.T1.1.15.15.2" class="ltx_td ltx_align_right ltx_border_bb">95M</td>
<td id="S2.T1.1.15.15.3" class="ltx_td ltx_align_right ltx_border_bb">Apache 2.0</td>
<td id="S2.T1.1.15.15.4" class="ltx_td ltx_align_right ltx_border_bb">April 2023</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Released pre-trained models in the Japanese language.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We built pre-trained models appropriate for the Japanese language and culture and released them in Hugging Face<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://huggingface.co/rinna" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/rinna</a></span></span></span>.
Table <a href="#S2.T1" title="Table 1 ‣ 2. Japanese Pre-Trained Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an overview of the released pre-trained models we have released by September 2023.
These models have fewer restrictive licenses, thereby allowing their wide use.
In fact, between April 2021 and September 2023, these models were downloaded over four million times from Hugging Face.
The details and specifics of these models are discussed in Sections <a href="#S3" title="3. Language Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to <a href="#S5" title="5. Speech Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Language Models</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   GPT</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.1.1.   Overview</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.3" class="ltx_p">The Generative Pre-trained Transformer (GPT, <cite class="ltx_cite ltx_citemacro_citet">Radford et al. <a href="#bib.bib16" title="" class="ltx_ref">2018</a></cite>) is an autoregressive language model composed of an input embedding layer, stacked Transformer layers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, and an output classification layer. It models <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="p(\mathbf{x})" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.1.m1.1.2.1" xref="S3.SS1.SSS1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS1.p1.1.m1.1.2.3.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.2.3.2.1" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.2.3.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2"><times id="S3.SS1.SSS1.p1.1.m1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.1"></times><ci id="S3.SS1.SSS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2">𝑝</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝐱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">p(\mathbf{x})</annotation></semantics></math>, the probability of a sequence of text tokens <math id="S3.SS1.SSS1.p1.2.m2.4" class="ltx_Math" alttext="\mathbf{x}=[x_{1},\cdots,x_{|\mathbf{x}|}]" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.4a"><mrow id="S3.SS1.SSS1.p1.2.m2.4.4" xref="S3.SS1.SSS1.p1.2.m2.4.4.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.4.4.4" xref="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml">𝐱</mi><mo id="S3.SS1.SSS1.p1.2.m2.4.4.3" xref="S3.SS1.SSS1.p1.2.m2.4.4.3.cmml">=</mo><mrow id="S3.SS1.SSS1.p1.2.m2.4.4.2.2" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.3" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml">[</mo><msub id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.2.cmml">x</mi><mn id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.4" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS1.p1.2.m2.2.2" xref="S3.SS1.SSS1.p1.2.m2.2.2.cmml">⋯</mi><mo id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.5" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml">,</mo><msub id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.2" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.2.cmml">x</mi><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.1.1.1.3.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.1.1.1.3.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msub><mo stretchy="false" id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.6" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.4b"><apply id="S3.SS1.SSS1.p1.2.m2.4.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4"><eq id="S3.SS1.SSS1.p1.2.m2.4.4.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.3"></eq><ci id="S3.SS1.SSS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.4">𝐱</ci><list id="S3.SS1.SSS1.p1.2.m2.4.4.2.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2"><apply id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.3.3.1.1.1.3">1</cn></apply><ci id="S3.SS1.SSS1.p1.2.m2.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.2.2">⋯</ci><apply id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.4.4.2.2.2.2">𝑥</ci><apply id="S3.SS1.SSS1.p1.2.m2.1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.3"><abs id="S3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.3.1"></abs><ci id="S3.SS1.SSS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.1.1">𝐱</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.4c">\mathbf{x}=[x_{1},\cdots,x_{|\mathbf{x}|}]</annotation></semantics></math>, as factorized token-level probabilities, and then pre-trains a GPT model to minimize the negative log-likelihood (NLL) <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{NLL}}" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><msub id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3a.cmml">NLL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2">ℒ</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">NLL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\mathcal{L}_{\text{NLL}}</annotation></semantics></math>.</p>
<table id="A0.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle p(\mathbf{x})" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mi id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">​</mo><mrow id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.2.3.2.1" xref="S3.E1.m1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.E1.m1.1.2.3.2.2" xref="S3.E1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><times id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></times><ci id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">𝑝</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle p(\mathbf{x})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.3" class="ltx_Math" alttext="\displaystyle=p(x_{1})p(x_{2}|x_{1})\cdots p(x_{|\mathbf{x}|}|x_{:|\mathbf{x}|-1})," display="inline"><semantics id="S3.E1.m2.3a"><mrow id="S3.E1.m2.3.3.1" xref="S3.E1.m2.3.3.1.1.cmml"><mrow id="S3.E1.m2.3.3.1.1" xref="S3.E1.m2.3.3.1.1.cmml"><mi id="S3.E1.m2.3.3.1.1.5" xref="S3.E1.m2.3.3.1.1.5.cmml"></mi><mo id="S3.E1.m2.3.3.1.1.4" xref="S3.E1.m2.3.3.1.1.4.cmml">=</mo><mrow id="S3.E1.m2.3.3.1.1.3" xref="S3.E1.m2.3.3.1.1.3.cmml"><mi id="S3.E1.m2.3.3.1.1.3.5" xref="S3.E1.m2.3.3.1.1.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mrow id="S3.E1.m2.3.3.1.1.1.1.1" xref="S3.E1.m2.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.3.3.1.1.1.1.1.2" xref="S3.E1.m2.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m2.3.3.1.1.1.1.1.1" xref="S3.E1.m2.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.3.3.1.1.1.1.1.1.2" xref="S3.E1.m2.3.3.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E1.m2.3.3.1.1.1.1.1.1.3" xref="S3.E1.m2.3.3.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E1.m2.3.3.1.1.1.1.1.3" xref="S3.E1.m2.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4a" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mi id="S3.E1.m2.3.3.1.1.3.6" xref="S3.E1.m2.3.3.1.1.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4b" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mrow id="S3.E1.m2.3.3.1.1.2.2.1" xref="S3.E1.m2.3.3.1.1.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.3.3.1.1.2.2.1.2" xref="S3.E1.m2.3.3.1.1.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m2.3.3.1.1.2.2.1.1" xref="S3.E1.m2.3.3.1.1.2.2.1.1.cmml"><msub id="S3.E1.m2.3.3.1.1.2.2.1.1.2" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2.cmml"><mi id="S3.E1.m2.3.3.1.1.2.2.1.1.2.2" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2.2.cmml">x</mi><mn id="S3.E1.m2.3.3.1.1.2.2.1.1.2.3" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2.3.cmml">2</mn></msub><mo fence="false" id="S3.E1.m2.3.3.1.1.2.2.1.1.1" xref="S3.E1.m2.3.3.1.1.2.2.1.1.1.cmml">|</mo><msub id="S3.E1.m2.3.3.1.1.2.2.1.1.3" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3.cmml"><mi id="S3.E1.m2.3.3.1.1.2.2.1.1.3.2" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3.2.cmml">x</mi><mn id="S3.E1.m2.3.3.1.1.2.2.1.1.3.3" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S3.E1.m2.3.3.1.1.2.2.1.3" xref="S3.E1.m2.3.3.1.1.2.2.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4c" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mi mathvariant="normal" id="S3.E1.m2.3.3.1.1.3.7" xref="S3.E1.m2.3.3.1.1.3.7.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4d" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mi id="S3.E1.m2.3.3.1.1.3.8" xref="S3.E1.m2.3.3.1.1.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.3.3.1.1.3.4e" xref="S3.E1.m2.3.3.1.1.3.4.cmml">​</mo><mrow id="S3.E1.m2.3.3.1.1.3.3.1" xref="S3.E1.m2.3.3.1.1.3.3.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.3.3.1.1.3.3.1.2" xref="S3.E1.m2.3.3.1.1.3.3.1.1.cmml">(</mo><mrow id="S3.E1.m2.3.3.1.1.3.3.1.1" xref="S3.E1.m2.3.3.1.1.3.3.1.1.cmml"><msub id="S3.E1.m2.3.3.1.1.3.3.1.1.2" xref="S3.E1.m2.3.3.1.1.3.3.1.1.2.cmml"><mi id="S3.E1.m2.3.3.1.1.3.3.1.1.2.2" xref="S3.E1.m2.3.3.1.1.3.3.1.1.2.2.cmml">x</mi><mrow id="S3.E1.m2.1.1.1.3" xref="S3.E1.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m2.1.1.1.3.1" xref="S3.E1.m2.1.1.1.2.1.cmml">|</mo><mi id="S3.E1.m2.1.1.1.1" xref="S3.E1.m2.1.1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.E1.m2.1.1.1.3.2" xref="S3.E1.m2.1.1.1.2.1.cmml">|</mo></mrow></msub><mo fence="false" id="S3.E1.m2.3.3.1.1.3.3.1.1.1" xref="S3.E1.m2.3.3.1.1.3.3.1.1.1.cmml">|</mo><msub id="S3.E1.m2.3.3.1.1.3.3.1.1.3" xref="S3.E1.m2.3.3.1.1.3.3.1.1.3.cmml"><mi id="S3.E1.m2.3.3.1.1.3.3.1.1.3.2" xref="S3.E1.m2.3.3.1.1.3.3.1.1.3.2.cmml">x</mi><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.cmml"><mi id="S3.E1.m2.2.2.1.3" xref="S3.E1.m2.2.2.1.3.cmml"></mi><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.2.cmml">:</mo><mrow id="S3.E1.m2.2.2.1.4" xref="S3.E1.m2.2.2.1.4.cmml"><mrow id="S3.E1.m2.2.2.1.4.2.2" xref="S3.E1.m2.2.2.1.4.2.1.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.4.2.2.1" xref="S3.E1.m2.2.2.1.4.2.1.1.cmml">|</mo><mi id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.E1.m2.2.2.1.4.2.2.2" xref="S3.E1.m2.2.2.1.4.2.1.1.cmml">|</mo></mrow><mo id="S3.E1.m2.2.2.1.4.1" xref="S3.E1.m2.2.2.1.4.1.cmml">−</mo><mn id="S3.E1.m2.2.2.1.4.3" xref="S3.E1.m2.2.2.1.4.3.cmml">1</mn></mrow></mrow></msub></mrow><mo stretchy="false" id="S3.E1.m2.3.3.1.1.3.3.1.3" xref="S3.E1.m2.3.3.1.1.3.3.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m2.3.3.1.2" xref="S3.E1.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.3b"><apply id="S3.E1.m2.3.3.1.1.cmml" xref="S3.E1.m2.3.3.1"><eq id="S3.E1.m2.3.3.1.1.4.cmml" xref="S3.E1.m2.3.3.1.1.4"></eq><csymbol cd="latexml" id="S3.E1.m2.3.3.1.1.5.cmml" xref="S3.E1.m2.3.3.1.1.5">absent</csymbol><apply id="S3.E1.m2.3.3.1.1.3.cmml" xref="S3.E1.m2.3.3.1.1.3"><times id="S3.E1.m2.3.3.1.1.3.4.cmml" xref="S3.E1.m2.3.3.1.1.3.4"></times><ci id="S3.E1.m2.3.3.1.1.3.5.cmml" xref="S3.E1.m2.3.3.1.1.3.5">𝑝</ci><apply id="S3.E1.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m2.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.3.3.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S3.E1.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.3.3.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.E1.m2.3.3.1.1.3.6.cmml" xref="S3.E1.m2.3.3.1.1.3.6">𝑝</ci><apply id="S3.E1.m2.3.3.1.1.2.2.1.1.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1"><csymbol cd="latexml" id="S3.E1.m2.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.1">conditional</csymbol><apply id="S3.E1.m2.3.3.1.1.2.2.1.1.2.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.1.1.2.2.1.1.2.1.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m2.3.3.1.1.2.2.1.1.2.2.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2.2">𝑥</ci><cn type="integer" id="S3.E1.m2.3.3.1.1.2.2.1.1.2.3.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.2.3">2</cn></apply><apply id="S3.E1.m2.3.3.1.1.2.2.1.1.3.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.1.1.2.2.1.1.3.1.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m2.3.3.1.1.2.2.1.1.3.2.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3.2">𝑥</ci><cn type="integer" id="S3.E1.m2.3.3.1.1.2.2.1.1.3.3.cmml" xref="S3.E1.m2.3.3.1.1.2.2.1.1.3.3">1</cn></apply></apply><ci id="S3.E1.m2.3.3.1.1.3.7.cmml" xref="S3.E1.m2.3.3.1.1.3.7">⋯</ci><ci id="S3.E1.m2.3.3.1.1.3.8.cmml" xref="S3.E1.m2.3.3.1.1.3.8">𝑝</ci><apply id="S3.E1.m2.3.3.1.1.3.3.1.1.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1"><csymbol cd="latexml" id="S3.E1.m2.3.3.1.1.3.3.1.1.1.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.1">conditional</csymbol><apply id="S3.E1.m2.3.3.1.1.3.3.1.1.2.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.1.1.3.3.1.1.2.1.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.2">subscript</csymbol><ci id="S3.E1.m2.3.3.1.1.3.3.1.1.2.2.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.2.2">𝑥</ci><apply id="S3.E1.m2.1.1.1.2.cmml" xref="S3.E1.m2.1.1.1.3"><abs id="S3.E1.m2.1.1.1.2.1.cmml" xref="S3.E1.m2.1.1.1.3.1"></abs><ci id="S3.E1.m2.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1">𝐱</ci></apply></apply><apply id="S3.E1.m2.3.3.1.1.3.3.1.1.3.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m2.3.3.1.1.3.3.1.1.3.1.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.3">subscript</csymbol><ci id="S3.E1.m2.3.3.1.1.3.3.1.1.3.2.cmml" xref="S3.E1.m2.3.3.1.1.3.3.1.1.3.2">𝑥</ci><apply id="S3.E1.m2.2.2.1.cmml" xref="S3.E1.m2.2.2.1"><ci id="S3.E1.m2.2.2.1.2.cmml" xref="S3.E1.m2.2.2.1.2">:</ci><csymbol cd="latexml" id="S3.E1.m2.2.2.1.3.cmml" xref="S3.E1.m2.2.2.1.3">absent</csymbol><apply id="S3.E1.m2.2.2.1.4.cmml" xref="S3.E1.m2.2.2.1.4"><minus id="S3.E1.m2.2.2.1.4.1.cmml" xref="S3.E1.m2.2.2.1.4.1"></minus><apply id="S3.E1.m2.2.2.1.4.2.1.cmml" xref="S3.E1.m2.2.2.1.4.2.2"><abs id="S3.E1.m2.2.2.1.4.2.1.1.cmml" xref="S3.E1.m2.2.2.1.4.2.2.1"></abs><ci id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1.1">𝐱</ci></apply><cn type="integer" id="S3.E1.m2.2.2.1.4.3.cmml" xref="S3.E1.m2.2.2.1.4.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.3c">\displaystyle=p(x_{1})p(x_{2}|x_{1})\cdots p(x_{|\mathbf{x}|}|x_{:|\mathbf{x}|-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{NLL}}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3a.cmml">NLL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ℒ</ci><ci id="S3.E2.m1.1.1.3a.cmml" xref="S3.E2.m1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">NLL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\mathcal{L}_{\text{NLL}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.2" class="ltx_Math" alttext="\displaystyle=-\log p(\mathbf{x})." display="inline"><semantics id="S3.E2.m2.2a"><mrow id="S3.E2.m2.2.2.1" xref="S3.E2.m2.2.2.1.1.cmml"><mrow id="S3.E2.m2.2.2.1.1" xref="S3.E2.m2.2.2.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.2" xref="S3.E2.m2.2.2.1.1.2.cmml"></mi><mo id="S3.E2.m2.2.2.1.1.1" xref="S3.E2.m2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E2.m2.2.2.1.1.3" xref="S3.E2.m2.2.2.1.1.3.cmml"><mo rspace="0.167em" id="S3.E2.m2.2.2.1.1.3a" xref="S3.E2.m2.2.2.1.1.3.cmml">−</mo><mrow id="S3.E2.m2.2.2.1.1.3.2" xref="S3.E2.m2.2.2.1.1.3.2.cmml"><mrow id="S3.E2.m2.2.2.1.1.3.2.2" xref="S3.E2.m2.2.2.1.1.3.2.2.cmml"><mi id="S3.E2.m2.2.2.1.1.3.2.2.1" xref="S3.E2.m2.2.2.1.1.3.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m2.2.2.1.1.3.2.2a" xref="S3.E2.m2.2.2.1.1.3.2.2.cmml">⁡</mo><mi id="S3.E2.m2.2.2.1.1.3.2.2.2" xref="S3.E2.m2.2.2.1.1.3.2.2.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m2.2.2.1.1.3.2.1" xref="S3.E2.m2.2.2.1.1.3.2.1.cmml">​</mo><mrow id="S3.E2.m2.2.2.1.1.3.2.3.2" xref="S3.E2.m2.2.2.1.1.3.2.cmml"><mo stretchy="false" id="S3.E2.m2.2.2.1.1.3.2.3.2.1" xref="S3.E2.m2.2.2.1.1.3.2.cmml">(</mo><mi id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">𝐱</mi><mo stretchy="false" id="S3.E2.m2.2.2.1.1.3.2.3.2.2" xref="S3.E2.m2.2.2.1.1.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m2.2.2.1.2" xref="S3.E2.m2.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.2b"><apply id="S3.E2.m2.2.2.1.1.cmml" xref="S3.E2.m2.2.2.1"><eq id="S3.E2.m2.2.2.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.E2.m2.2.2.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.2">absent</csymbol><apply id="S3.E2.m2.2.2.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.3"><minus id="S3.E2.m2.2.2.1.1.3.1.cmml" xref="S3.E2.m2.2.2.1.1.3"></minus><apply id="S3.E2.m2.2.2.1.1.3.2.cmml" xref="S3.E2.m2.2.2.1.1.3.2"><times id="S3.E2.m2.2.2.1.1.3.2.1.cmml" xref="S3.E2.m2.2.2.1.1.3.2.1"></times><apply id="S3.E2.m2.2.2.1.1.3.2.2.cmml" xref="S3.E2.m2.2.2.1.1.3.2.2"><log id="S3.E2.m2.2.2.1.1.3.2.2.1.cmml" xref="S3.E2.m2.2.2.1.1.3.2.2.1"></log><ci id="S3.E2.m2.2.2.1.1.3.2.2.2.cmml" xref="S3.E2.m2.2.2.1.1.3.2.2.2">𝑝</ci></apply><ci id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.2c">\displaystyle=-\log p(\mathbf{x}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">GPT-NeoX <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> is a GPT variant that uses a modified architecture for the Transformer layer and an alternative position encoding mechanism called rotary embedding <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> as a substitute for original learnable position embeddings.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">For the most capable models, we also released their instruction-following versions, which were trained using either Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF, <cite class="ltx_cite ltx_citemacro_citet">Ouyang et al. <a href="#bib.bib14" title="" class="ltx_ref">2022</a></cite>) via the Proximal Policy Optimization (PPO, <cite class="ltx_cite ltx_citemacro_citet">Schulman et al. <a href="#bib.bib19" title="" class="ltx_ref">2017</a></cite>) algorithm in addition to SFT.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.1.2.   Training Data</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">For Japanese-specific GPT models, we used Wikipedia, the CC-100 <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#biba.bib4" title="" class="ltx_ref">2020</a>)</cite>, and the mC4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#biba.bib12" title="" class="ltx_ref">2020</a>)</cite> datasets for pre-training. For bilingual English-Japanese GPT models, we additionally used the Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#biba.bib6" title="" class="ltx_ref">2020</a>)</cite> and Redpajama <cite class="ltx_cite ltx_citemacro_citep">(Computer, <a href="#biba.bib3" title="" class="ltx_ref">2023</a>)</cite> datasets. The instruction-following models were trained on the Japanese translation of the Anthropic HH <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a href="#biba.bib1" title="" class="ltx_ref">2022</a>)</cite>, the SHP <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh et al., <a href="#biba.bib5" title="" class="ltx_ref">2022</a>)</cite>, and the FLAN <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a href="#biba.bib15" title="" class="ltx_ref">2022</a>)</cite> datasets.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Tokenizers of the GPT models are trained via SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>.
Their vocabulary sizes vary from 32000 to 65536. While the tokenizers for Japanese-only models are trained from Japanese corpora, the tokenizer of <span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">bilingual-gpt-neox-4b</span> is trained from a mixture of Japanese and English corpora for a better coverage of English tokens.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.1.3.   Experiments</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">We conducted few-shot evaluations of the GPT models to assess their performance on Japanese tasks.
We used the JP Language Model Evaluation Harness<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable</a></span></span></span> benchmark for evaluation.
We conducted a comparison with <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">meta/llama-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib22" title="" class="ltx_ref">2023a</a>)</cite>, <span id="S3.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_italic">meta/llama2-7b</span>, and <span id="S3.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_italic">meta/llama2-7b-chat</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>, which were primarily trained using English data.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.1.3. Experiments ‣ 3.1. GPT ‣ 3. Language Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists the average scores for the jcommonsenseqa, jnli, marc-ja, jsquad <cite class="ltx_cite ltx_citemacro_citep">(Kurihara et al., <a href="#biba.bib8" title="" class="ltx_ref">2022</a>)</cite>, xwinograd <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al., <a href="#biba.bib10" title="" class="ltx_ref">2023</a>)</cite>, jaqket-v2<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://sites.google.com/view/project-aio/competition2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sites.google.com/view/project-aio/competition2</a></span></span></span>, xlsum-ja <cite class="ltx_cite ltx_citemacro_citep">(Hasan et al., <a href="#biba.bib7" title="" class="ltx_ref">2021</a>)</cite>, and mgsm <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a href="#biba.bib14" title="" class="ltx_ref">2023</a>)</cite> tasks.
The few-shot numbers were 3, 3, 3, 2, 0, 1, 1, and 5.
Our <span id="S3.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-3.6b</span> and <span id="S3.SS1.SSS3.p2.1.2" class="ltx_text ltx_font_italic">rinna/bilingual-gpt-neox-4b</span> pre-trained models outperformed <span id="S3.SS1.SSS3.p2.1.3" class="ltx_text ltx_font_italic">meta/llama-7b</span>, and instruction tuning via SFT or PPO significantly improved their capability.
By specializing in Japanese, good performance was achieved while keeping the number of parameters low.
We refer the readers to rinna’s language model benchmark<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://rinnakk.github.io/research/benchmarks/lm/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://rinnakk.github.io/research/benchmarks/lm/index.html</a> Due to the update of the evaluation code base, the latest benchmark adopts a different evaluation setting than that used in this paper.
The results presented in this paper can be found in the benchmark spreadsheet on the <span id="footnote4.1" class="ltx_text ltx_font_italic">20231031</span> tab.</span></span></span> for detailed benchmark results.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-trained model</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Average score</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.2.1.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-xsmall</span></td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">26.63</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.3.2.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-small</span></td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_right">27.33</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.4.3.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt2-medium</span></td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_right">28.33</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.5.4.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-1b</span></td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_right">32.21</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.6.5.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-small</span></td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_right">30.11</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<td id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.7.6.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-3.6b</span></td>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_right">36.60</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<td id="S3.T2.1.8.7.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.8.7.1.1" class="ltx_text ltx_font_italic">rinna/bilingual-gpt-neox-4b</span></td>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_right">38.29</td>
</tr>
<tr id="S3.T2.1.9.8" class="ltx_tr">
<td id="S3.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.9.8.1.1" class="ltx_text ltx_font_italic">meta/llama-7b</span></td>
<td id="S3.T2.1.9.8.2" class="ltx_td ltx_align_right ltx_border_t">33.28</td>
</tr>
<tr id="S3.T2.1.10.9" class="ltx_tr">
<td id="S3.T2.1.10.9.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.10.9.1.1" class="ltx_text ltx_font_italic">meta/llama2-7b</span></td>
<td id="S3.T2.1.10.9.2" class="ltx_td ltx_align_right">42.97</td>
</tr>
<tr id="S3.T2.1.11.10" class="ltx_tr">
<td id="S3.T2.1.11.10.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.11.10.1.1" class="ltx_text ltx_font_italic">meta/llama2-7b-chat</span></td>
<td id="S3.T2.1.11.10.2" class="ltx_td ltx_align_right">41.31</td>
</tr>
<tr id="S3.T2.1.12.11" class="ltx_tr">
<td id="S3.T2.1.12.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.12.11.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-3.6b-sft</span></td>
<td id="S3.T2.1.12.11.2" class="ltx_td ltx_align_right ltx_border_t">45.24</td>
</tr>
<tr id="S3.T2.1.13.12" class="ltx_tr">
<td id="S3.T2.1.13.12.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.13.12.1.1" class="ltx_text ltx_font_italic">rinna/japanese-gpt-neox-3.6b-ppo</span></td>
<td id="S3.T2.1.13.12.2" class="ltx_td ltx_align_right">46.37</td>
</tr>
<tr id="S3.T2.1.14.13" class="ltx_tr">
<td id="S3.T2.1.14.13.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.14.13.1.1" class="ltx_text ltx_font_italic">rinna/bilingual-gpt-neox-4b-sft</span></td>
<td id="S3.T2.1.14.13.2" class="ltx_td ltx_align_right">47.65</td>
</tr>
<tr id="S3.T2.1.15.14" class="ltx_tr">
<td id="S3.T2.1.15.14.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.1.15.14.1.1" class="ltx_text ltx_font_italic">rinna/bilingual-gpt-neox-4b-ppo</span></td>
<td id="S3.T2.1.15.14.2" class="ltx_td ltx_align_right ltx_border_bb">47.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Language model evaluation on the JP Language Model Evaluation Harness.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Language-Image Models</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   CLIP</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.1.1.   Overview</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Contrastive Language-Image Pre-training (CLIP, <cite class="ltx_cite ltx_citemacro_citet">Radford et al. <a href="#bib.bib15" title="" class="ltx_ref">2021</a></cite>) connects visual concepts with natural language in the embedding space.
It comprises a pair of text and image encoders and is trained by minimizing contrastive loss.
The Contrastive Leave One Out Boost (CLOOB, <cite class="ltx_cite ltx_citemacro_citet">Fürst et al. <a href="#bib.bib5" title="" class="ltx_ref">2022</a></cite>) demonstrated a better zero-shot performance than the original CLIP by introducing a novel loss function termed InfoLOOB.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">To train the Japanese-specific CLIP efficiently, we applied Locked-image Tuning (LiT, <cite class="ltx_cite ltx_citemacro_citet">Zhai et al. <a href="#bib.bib26" title="" class="ltx_ref">2022</a></cite>), in which both encoders were initialized using separate pre-trained models, and only the text encoder was trained.
We used the pre-trained 12-layer 16<math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mo id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><times id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\times</annotation></semantics></math>16-patch-size AugReg Vision Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Steiner et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> for the image encoder, and randomly initialized 12-layer Bidirectional Encoder Representations from Transformers (BERT, <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a></cite>) with a SentencePiece tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> for the text encoder.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.1.2.   Training Data</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Owing to the absence of a large-scale dataset with Japanese captions, we used CC12M <cite class="ltx_cite ltx_citemacro_citep">(Changpinyo et al., <a href="#biba.bib2" title="" class="ltx_ref">2021</a>)</cite>.
We translated all the English captions into Japanese.
For data augmentation, we generated captions using Bootstrapping Language-Image Pre-training (BLIP, <cite class="ltx_cite ltx_citemacro_citet">Li et al. <a href="#bib.bib12" title="" class="ltx_ref">2022</a></cite>) trained on an English dataset.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-trained model</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.2.1.1.1" class="ltx_text ltx_font_italic">laion/clip-base</span></th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">38.00</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.3.2.1.1" class="ltx_text ltx_font_italic">laion/clip-large</span></th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_right">53.09</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.4.3.1.1" class="ltx_text ltx_font_italic">rinna/japanese-clip-vit-b-16</span></th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_right ltx_border_t">50.69</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.5.4.1.1" class="ltx_text ltx_font_italic">rinna/japanese-cloob-vit-b-16</span></th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_right ltx_border_bb">54.64</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>ImageNet image classification accuracy in a zero-shot setting.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.1.3.   Experiments</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">We evaluated CLIP for ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib2" title="" class="ltx_ref">2009</a>)</cite> zero-shot image classification.
We used open-sourced Japanese class names<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://gist.github.com/PonDad/4dcb4b242b9358e524b4ddecbee385e9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://gist.github.com/PonDad/4dcb4b242b9358e524b4ddecbee385e9</a></span></span></span>.
Additionally, we created 37 Japanese templates from 80 English templates by deduplicating captions that had the same meaning in Japanese.
We compared our models with open-source multilingual CLIP models <cite class="ltx_cite ltx_citemacro_citep">(Ilharco et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> trained on full LAION-5B <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et al., <a href="#biba.bib13" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.1.2. Training Data ‣ 4.1. CLIP ‣ 4. Language-Image Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the top-1 accuracy for each model.
Our <span id="S4.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_italic">rinna/japanese-cloob-vit-b-16</span> performed the best and achieved state-of-the-art accuracy.
This is because, even with a limited amount of training data, the model can be efficiently trained by specializing in a specific language.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Stable Diffusion</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.1.   Overview</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Stable Diffusion (SD) facilitates high-quality image generation using simple text prompts.
It is based on the Latent Diffusion Model (LDM, <cite class="ltx_cite ltx_citemacro_citet">Rombach et al. <a href="#bib.bib17" title="" class="ltx_ref">2022</a></cite>), which comprises three main components: a Variational AutoEncoder (VAE, <cite class="ltx_cite ltx_citemacro_citet">Kingma and Welling <a href="#bib.bib10" title="" class="ltx_ref">2014</a></cite>), a text encoder, and U-Net <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">To train the Japanese-specific SD (JSD), we fine-tuned <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">CompVis/stable-diffusion-v1-4<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_upright">6</span></span><a target="_blank" href="https://huggingface.co/CompVis/stable-diffusion-v1-4" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://huggingface.co/CompVis/stable-diffusion-v1-4</a></span></span></span></span> trained on the English dataset.
We applied two training stages following the concept of Pretraining-based Image-To-Image translation (PITI, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. <a href="#bib.bib25" title="" class="ltx_ref">2022</a></cite>);
The text encoder was trained solely with U-Net fixed in the first stage and jointly trained in the second stage.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.2.   Training Data</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We used approximately 100 million images with Japanese captions, including the Japanese subset LAION-5B <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et al., <a href="#biba.bib13" title="" class="ltx_ref">2022</a>)</cite>.
To ensure data quality, we employed our <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">rinna/japanese-cloob-vit-b-16</span> introduced in Section <a href="#S4.SS1" title="4.1. CLIP ‣ 4. Language-Image Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> to calculate the similarity scores between images and their captions, and samples with scores below a certain threshold were removed.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">4.2.3.   Experiments</h4>

<figure id="S4.F1" class="ltx_figure">
<table id="S4.F1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F1.1.1" class="ltx_tr">
<th id="S4.F1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.F1.1.1.2.1" class="ltx_text" style="font-size:70%;">SD</span></th>
<th id="S4.F1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S4.F1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:199.2pt;"><img src="/html/2404.01657/assets/x1.png" id="S4.F1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="115" alt="Refer to caption">
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F1.2.2" class="ltx_tr">
<td id="S4.F1.2.2.2" class="ltx_td ltx_align_center">

<table id="S4.F1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F1.2.2.2.1.1" class="ltx_tr">
<td id="S4.F1.2.2.2.1.1.1" class="ltx_td ltx_align_center"><span id="S4.F1.2.2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">JSD</span></td>
</tr>
<tr id="S4.F1.2.2.2.1.2" class="ltx_tr">
<td id="S4.F1.2.2.2.1.2.1" class="ltx_td ltx_align_center"><span id="S4.F1.2.2.2.1.2.1.1" class="ltx_text" style="font-size:70%;">(1st)</span></td>
</tr>
</table>
</td>
<td id="S4.F1.2.2.1" class="ltx_td ltx_align_center">
<span id="S4.F1.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:199.2pt;"><img src="/html/2404.01657/assets/x2.png" id="S4.F1.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="115" alt="Refer to caption">
</span>
</td>
</tr>
<tr id="S4.F1.3.3" class="ltx_tr">
<td id="S4.F1.3.3.2" class="ltx_td ltx_align_center">

<table id="S4.F1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F1.3.3.2.1.1" class="ltx_tr">
<td id="S4.F1.3.3.2.1.1.1" class="ltx_td ltx_align_center"><span id="S4.F1.3.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">JSD</span></td>
</tr>
<tr id="S4.F1.3.3.2.1.2" class="ltx_tr">
<td id="S4.F1.3.3.2.1.2.1" class="ltx_td ltx_align_center"><span id="S4.F1.3.3.2.1.2.1.1" class="ltx_text" style="font-size:70%;">(2nd)</span></td>
</tr>
</table>
</td>
<td id="S4.F1.3.3.1" class="ltx_td ltx_align_center">
<span id="S4.F1.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:199.2pt;"><img src="/html/2404.01657/assets/x3.png" id="S4.F1.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="115" alt="Refer to caption">
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Outputs for the text prompt “salary man, oil painting”. For JSD, the translation in Japanese “サラリーマン 油絵” was used. </figcaption>
</figure>
<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">We used Japanglish “salary man”, which is commonly visualized as a man in a suit, as the text prompt for the evaluation.
Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2.3. Experiments ‣ 4.2. Stable Diffusion ‣ 4. Language-Image Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results.
The original SD failed to accurately interpret such distinctive Japanese terms.
In the first stage, JSD understood the prompt’s meaning, but the generated images depicted businessmen with Western features because U-Net had not been updated.
JSD at the second stage (<span id="S4.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">rinna/japanese-stable-diffusion</span>), JSD successfully generated images of businessmen with Japanese features.
Using images reflecting Japanese culture as the training data, we were able to construct a model consistent with Japanese cultural identity.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Speech Models</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.   HuBERT</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.1.1.   Overview</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">The Hidden-unit BERT (HuBERT, <cite class="ltx_cite ltx_citemacro_citet">Hsu et al. <a href="#bib.bib7" title="" class="ltx_ref">2021</a></cite>) is a pre-trained model that can learn self-supervised speech representations.
HuBERT comprises two main components: a convolutional waveform encoder and a BERT encoder <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>.
HuBERT is trained with a BERT-like masked prediction objective: a portion of the encoded speech feature sequence is randomly masked, and a label corresponding to the masked portion is predicted from the unmasked portion.
However, because speech signals, unlike text, are continuous-valued sequences, the model is trained by targeting discrete pseudo-labels obtained from speech using offline <math id="S5.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><mi id="S5.SS1.SSS1.p1.1.m1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.1.m1.1b"><ci id="S5.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.1.m1.1c">k</annotation></semantics></math>-means clustering.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.1.2.   Training Data</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">We used the ReazonSpeech corpus <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#biba.bib16" title="" class="ltx_ref">2023</a>)</cite>, a 19,000-hour speech corpus collected from Japanese TV programs with 16 kHz sampling.
To generate pseudo-labels, we ran <math id="S5.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.SSS2.p1.1.m1.1a"><mi id="S5.SS1.SSS2.p1.1.m1.1.1" xref="S5.SS1.SSS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.1.m1.1b"><ci id="S5.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.1.m1.1c">k</annotation></semantics></math>-means clustering with 100 clusters on 39-dimensional Mel-frequency cepstral coefficient features for the first iteration of HuBERT training and 500 clusters on the latent features extracted from the 6-th Transformer layers’ of the first iteration for the second iteration of HuBERT training.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.1.3.   Experiments</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">We evaluated the performance of the pre-trained HuBERT model for Japanese Automatic Speech Recognition (ASR).
We used Corpus of Spontaneous Japanese <cite class="ltx_cite ltx_citemacro_citep">(Maekawa et al., <a href="#biba.bib9" title="" class="ltx_ref">2000</a>)</cite>.
Two training subset sizes were prepared: core data only (approximately 32 h) and all data (approximately 552 h).
ASR fine-tuning using the Connectionist Temporal Classification (CTC, <cite class="ltx_cite ltx_citemacro_citet">Graves et al. <a href="#bib.bib6" title="" class="ltx_ref">2006</a></cite>) loss was performed as described in <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>.
The final projection layer was replaced with a softmax layer before ASR fine-tuning.
The target vocabulary included 40 Japanese phonemes, a word boundary symbol, and a special CTC blank symbol.
The public HuBERT model <span id="S5.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">meta/hubert-base-ls960<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span id="footnote7.1.1.1" class="ltx_text ltx_font_upright">7</span></span><a target="_blank" href="https://huggingface.co/facebook/hubert-base-ls960" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span></span>, pre-trained with 960 hours English speech from Librispeech <cite class="ltx_cite ltx_citemacro_citep">(Panayotov et al., <a href="#biba.bib11" title="" class="ltx_ref">2015</a>)</cite>, was used for comparison.
In this study, we used the beam search with a beam size of 20 without a language model.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-trained model</span></th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Eval1</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Eval2</span></td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S5.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Eval3</span></td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4"><span id="S5.T4.1.2.2.1.1" class="ltx_text ltx_font_italic">meta/hubert-base</span></th>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<th id="S5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   32-hour labeled</th>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_right">13.12</td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_right">10.33</td>
<td id="S5.T4.1.3.3.4" class="ltx_td ltx_align_right">10.66</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<th id="S5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   552-hour labeled</th>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_right">7.88</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_right">5.66</td>
<td id="S5.T4.1.4.4.4" class="ltx_td ltx_align_right">6.48</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<th id="S5.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4"><span id="S5.T4.1.5.5.1.1" class="ltx_text ltx_font_italic">rinna/japanese-hubert-base</span></th>
</tr>
<tr id="S5.T4.1.6.6" class="ltx_tr">
<th id="S5.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   32-hour labeled</th>
<td id="S5.T4.1.6.6.2" class="ltx_td ltx_align_right">9.30</td>
<td id="S5.T4.1.6.6.3" class="ltx_td ltx_align_right">7.07</td>
<td id="S5.T4.1.6.6.4" class="ltx_td ltx_align_right">6.87</td>
</tr>
<tr id="S5.T4.1.7.7" class="ltx_tr">
<th id="S5.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   552-hour labeled</th>
<td id="S5.T4.1.7.7.2" class="ltx_td ltx_align_right ltx_border_bb">5.72</td>
<td id="S5.T4.1.7.7.3" class="ltx_td ltx_align_right ltx_border_bb">4.45</td>
<td id="S5.T4.1.7.7.4" class="ltx_td ltx_align_right ltx_border_bb">4.73</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
The word error rates for the fine-tuned HuBERT models with the different sizes of data.
</figcaption>
</figure>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">The results are presented in Table <a href="#S5.T4" title="Table 4 ‣ 5.1.3. Experiments ‣ 5.1. HuBERT ‣ 5. Speech Models ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
For both sizes of labeled data, the <span id="S5.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_italic">rinna/japanese-hubert-base</span> outperformed the <span id="S5.SS1.SSS3.p2.1.2" class="ltx_text ltx_font_italic">meta/hubert-base-ls960</span>.
This result indicates that the pre-trained HuBERT model trained with a large Japanese speech corpus has the potential to provide better performance in Japanese speech-processing tasks.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Aiming to advance AI democratization, this paper discusses the released Japanese GPT, CLIP, Stable Diffusion, and HuBERT models.
Experiments with GPT, CLIP, and HuBERT showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.
Additionally, the Stable Diffusion results indicate that it handles Japanese input and produces output that reflects Japanese culture.
Pre-trained models are continuously refined, and technically challenging tasks for improving them have now become achievable.
We plan to continue releasing pre-trained models to further contribute to technological progress.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Bibliographical References</h2>

<div id="S7.p1" class="ltx_para">
<span id="S7.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al. (2022)</span>
<span class="ltx_bibblock">
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.bigscience-1.9/" title="" class="ltx_ref ltx_href">GPT-NeoX-20B: An open-source autoregressive language model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of BigScience Episode #5 – Workshop on Challenges &amp; Perspectives in Creating Large Language Models</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2009.5206848" title="" class="ltx_ref ltx_href">ImageNet: A large-scale hierarchical image database</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009)</em>, pages 248–255.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional Transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</em>, volume 1, pages 4171–4186.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth 16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR 2021)</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fürst et al. (2022)</span>
<span class="ltx_bibblock">
Andreas Fürst, Elisabeth Rumetshofer, Johannes Lehner, Viet T. Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto, and Sepp Hochreiter. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8078e76f913e31b8467e85b4c0f0d22b-Abstract-Conference.html" title="" class="ltx_ref ltx_href">CLOOB: Modern hopfield networks with InfoLOOB outperform CLIP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS 2022)</em>, volume 35, pages 20450–20468.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/1143844.1143891" title="" class="ltx_ref ltx_href">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Conference on Machine Learning (ICML 2006)</em>, pages 369–376.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TASLP.2021.3122291" title="" class="ltx_ref ltx_href">HuBERT: Self-supervised speech representation learning by masked prediction of hidden units</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:3451–3460.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et al. (2021)</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.5143773" title="" class="ltx_ref ltx_href">OpenCLIP</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Zenodo</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2001.08361" title="" class="ltx_ref ltx_href">Scaling laws for neural language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2001.08361</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2014)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Max Welling. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=33X9fd2-9FyZd" title="" class="ltx_ref ltx_href">Auto-encoding variational bayes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR 2014)</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-2012" title="" class="ltx_ref ltx_href">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/li22n.html" title="" class="ltx_ref ltx_href">BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning (ICML 2022)</em>, volume 162, pages 12888–12900.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.616" title="" class="ltx_ref ltx_href">Few-shot learning with multilingual generative language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)</em>, pages 9019–9052.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS 2022)</em>, volume 35, pages 27730–27744.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</em>, volume 139, pages 8748–8763.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.mikecaptain.com/resources/pdf/GPT-1.pdf" title="" class="ltx_ref ltx_href">Improving language understanding by generative pre-training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">OpenAI</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html" title="" class="ltx_ref ltx_href">High-resolution image synthesis with latent diffusion models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>, pages 10684–10695.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28" title="" class="ltx_ref ltx_href">U-Net: Convolutional networks for biomedical image segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer-Assisted Intervention (MICCAI 2015)</em>, pages 234–241.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1707.06347" title="" class="ltx_ref ltx_href">Proximal policy optimization algorithms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:1707.06347</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steiner et al. (2022)</span>
<span class="ltx_bibblock">
Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=4nPswr1KcP" title="" class="ltx_ref ltx_href">How to train your ViT? Data, augmentation, and regularization in Vision Transformers</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.09864" title="" class="ltx_ref ltx_href">RoFormer: Enhanced Transformer with rotary position embedding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2104.09864</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">LLaMA: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NIPS 2017)</em>, volume 30, pages 5998–6008.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2205.12952" title="" class="ltx_ref ltx_href">Pretraining is all you need for image-to-image translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2205.12952</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2022)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html" title="" class="ltx_ref ltx_href">LiT: Zero-shot transfer with locked-image text tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>, pages 18123–18133.

</span>
</li>
</ul>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Language Resource References</h2>

<div id="S8.p1" class="ltx_para">
<span id="S8.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="biba" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"> </h2>

<ul class="ltx_biblist">
<li id="biba.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2204.05862" title="" class="ltx_ref ltx_href">Training a helpful and harmless assistant with reinforcement learning from human feedback</a>.

</span>
<span class="ltx_bibblock"><em id="biba.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2204.05862</em>.

</span>
</li>
<li id="biba.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. (2021)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html" title="" class="ltx_ref ltx_href">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021)</em>, pages 3558–3568.

</span>
</li>
<li id="biba.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)</span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_href">RedPajama: An open source recipe to reproduce llama training dataset</a>.

</span>
<span class="ltx_bibblock"><em id="biba.bib3.1.1" class="ltx_emph ltx_font_italic">GitHub</em>.

</span>
</li>
<li id="biba.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="" class="ltx_ref ltx_href">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020)</em>, pages 8440–8451.

</span>
</li>
<li id="biba.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh et al. (2022)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/ethayarajh22a.html" title="" class="ltx_ref ltx_href">Understanding dataset difficulty with <math id="biba.bib5.1.1.m1.1" class="ltx_Math" alttext="\mathcal{V}" display="inline"><semantics id="biba.bib5.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="biba.bib5.1.1.m1.1.1" xref="biba.bib5.1.1.m1.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="biba.bib5.1.1.m1.1b"><ci id="biba.bib5.1.1.m1.1.1.cmml" xref="biba.bib5.1.1.m1.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="biba.bib5.1.1.m1.1c">\mathcal{V}</annotation></semantics></math>-usable information</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib5.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning (ICML 2022)</em>, volume 162, pages 5988–6008.

</span>
</li>
<li id="biba.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2101.00027" title="" class="ltx_ref ltx_href">The Pile: An 800GB dataset of diverse text for language modeling</a>.

</span>
<span class="ltx_bibblock"><em id="biba.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv, arXiv:2101.00027</em>.

</span>
</li>
<li id="biba.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al. (2021)</span>
<span class="ltx_bibblock">
Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-acl.413" title="" class="ltx_ref ltx_href">XL-sum: Large-scale multilingual abstractive summarization for 44 languages</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib7.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 4693–4703.

</span>
</li>
<li id="biba.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurihara et al. (2022)</span>
<span class="ltx_bibblock">
Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.lrec-1.317" title="" class="ltx_ref ltx_href">JGLUE: Japanese general language understanding evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC 2022)</em>, pages 2957–2966.

</span>
</li>
<li id="biba.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maekawa et al. (2000)</span>
<span class="ltx_bibblock">
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/L00-1200/" title="" class="ltx_ref ltx_href">Spontaneous speech corpus of Japanese</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Language Resources and Evaluation Conference (LREC 2000)</em>.

</span>
</li>
<li id="biba.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.891" title="" class="ltx_ref ltx_href">Crosslingual generalization through multitask finetuning</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)</em>, pages 15991–16111.

</span>
</li>
<li id="biba.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">Librispeech: An ASR corpus based on public domain audio books</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on acoustics, speech and signal processing (ICASSP 2015)</em>, pages 5206–5210.

</span>
</li>
<li id="biba.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the limits of transfer learning with a unified text-to-text Transformer</a>.

</span>
<span class="ltx_bibblock"><em id="biba.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li id="biba.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=M3Y74vmsMcY" title="" class="ltx_ref ltx_href">LAION-5B: An open large-scale dataset for training next generation image-text models</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib13.1.1" class="ltx_emph ltx_font_italic">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>.

</span>
</li>
<li id="biba.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=fR3wGCk-IXp" title="" class="ltx_ref ltx_href">Language models are multilingual chain-of-thought reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib14.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations (ICLR 2023)</em>.

</span>
</li>
<li id="biba.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=gEZrGCozdqR" title="" class="ltx_ref ltx_href">Finetuned language models are zero-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib15.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning Representations (ICLR 2022)</em>.

</span>
</li>
<li id="biba.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2023)</span>
<span class="ltx_bibblock">
Yue Yin, Daijiro Mori, and Seiji Fujimoto. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/A5-3.pdf" title="" class="ltx_ref ltx_href">ReazonSpeech: A free and massive corpus for Japanese ASR</a>.

</span>
<span class="ltx_bibblock">In <em id="biba.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-ninth Annual Meeting of the Association for Natural Language Processing</em>, pages 1134–1139.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A0.T5" class="ltx_table">
<table id="A0.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A0.T5.1.1.1" class="ltx_tr">
<td id="A0.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A0.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-trained model</span></td>
<td id="A0.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="A0.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">License</span></td>
<td id="A0.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="A0.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Release date</span></td>
<td id="A0.T5.1.1.1.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="A0.T5.1.2.2" class="ltx_tr">
<td id="A0.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Language model</td>
<td id="A0.T5.1.2.2.2" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.2.2.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A0.T5.1.3.3" class="ltx_tr">
<td id="A0.T5.1.3.3.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-roberta-base" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-roberta-base</a>
</td>
<td id="A0.T5.1.3.3.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.3.3.3" class="ltx_td ltx_align_right">August 2021</td>
<td id="A0.T5.1.3.3.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.4.4" class="ltx_tr">
<td id="A0.T5.1.4.4.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt2-xsmall" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt2-xsmall</a>
</td>
<td id="A0.T5.1.4.4.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.4.4.3" class="ltx_td ltx_align_right">August 2021</td>
<td id="A0.T5.1.4.4.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.5.5" class="ltx_tr">
<td id="A0.T5.1.5.5.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt2-small" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt2-small</a>
</td>
<td id="A0.T5.1.5.5.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.5.5.3" class="ltx_td ltx_align_right">August 2021</td>
<td id="A0.T5.1.5.5.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.6.6" class="ltx_tr">
<td id="A0.T5.1.6.6.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt2-medium" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt2-medium</a>
</td>
<td id="A0.T5.1.6.6.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.6.6.3" class="ltx_td ltx_align_right">April 2021</td>
<td id="A0.T5.1.6.6.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.7.7" class="ltx_tr">
<td id="A0.T5.1.7.7.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-1b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-1b</a>
</td>
<td id="A0.T5.1.7.7.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.7.7.3" class="ltx_td ltx_align_right">January 2022</td>
<td id="A0.T5.1.7.7.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.8.8" class="ltx_tr">
<td id="A0.T5.1.8.8.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-neox-small" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-neox-small</a>
</td>
<td id="A0.T5.1.8.8.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.8.8.3" class="ltx_td ltx_align_right">September 2022</td>
<td id="A0.T5.1.8.8.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.9.9" class="ltx_tr">
<td id="A0.T5.1.9.9.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-neox-3.6b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-neox-3.6b</a>
</td>
<td id="A0.T5.1.9.9.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.9.9.3" class="ltx_td ltx_align_right">May 2023</td>
<td id="A0.T5.1.9.9.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.10.10" class="ltx_tr">
<td id="A0.T5.1.10.10.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-neox-3.6b-instruction-sft</a>
</td>
<td id="A0.T5.1.10.10.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.10.10.3" class="ltx_td ltx_align_right">May 2023</td>
<td id="A0.T5.1.10.10.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.11.11" class="ltx_tr">
<td id="A0.T5.1.11.11.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-neox-3.6b-instruction-sft-v2</a>
</td>
<td id="A0.T5.1.11.11.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.11.11.3" class="ltx_td ltx_align_right">May 2023</td>
<td id="A0.T5.1.11.11.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.12.12" class="ltx_tr">
<td id="A0.T5.1.12.12.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-gpt-neox-3.6b-instruction-ppo</a>
</td>
<td id="A0.T5.1.12.12.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.12.12.3" class="ltx_td ltx_align_right">May 2023</td>
<td id="A0.T5.1.12.12.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.13.13" class="ltx_tr">
<td id="A0.T5.1.13.13.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/bilingual-gpt-neox-4b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/bilingual-gpt-neox-4b</a>
</td>
<td id="A0.T5.1.13.13.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.13.13.3" class="ltx_td ltx_align_right">July 2023</td>
<td id="A0.T5.1.13.13.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.14.14" class="ltx_tr">
<td id="A0.T5.1.14.14.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/bilingual-gpt-neox-4b-8k</a>
</td>
<td id="A0.T5.1.14.14.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.14.14.3" class="ltx_td ltx_align_right">July 2023</td>
<td id="A0.T5.1.14.14.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.15.15" class="ltx_tr">
<td id="A0.T5.1.15.15.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-sft" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/bilingual-gpt-neox-4b-instruction-sft</a>
</td>
<td id="A0.T5.1.15.15.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.15.15.3" class="ltx_td ltx_align_right">July 2023</td>
<td id="A0.T5.1.15.15.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.16.16" class="ltx_tr">
<td id="A0.T5.1.16.16.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/bilingual-gpt-neox-4b-instruction-ppo</a>
</td>
<td id="A0.T5.1.16.16.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.16.16.3" class="ltx_td ltx_align_right">August 2023</td>
<td id="A0.T5.1.16.16.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.17.17" class="ltx_tr">
<td id="A0.T5.1.17.17.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b</a>
</td>
<td id="A0.T5.1.17.17.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.17.17.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.17.17.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.18.18" class="ltx_tr">
<td id="A0.T5.1.18.18.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b-instruction" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b-instruction</a>
</td>
<td id="A0.T5.1.18.18.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.18.18.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.18.18.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.19.19" class="ltx_tr">
<td id="A0.T5.1.19.19.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b-chat" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b-chat</a>
</td>
<td id="A0.T5.1.19.19.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.19.19.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.19.19.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.20.20" class="ltx_tr">
<td id="A0.T5.1.20.20.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b-gptq" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b-gptq</a>
</td>
<td id="A0.T5.1.20.20.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.20.20.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.20.20.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.21.21" class="ltx_tr">
<td id="A0.T5.1.21.21.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b-instruction-gptq" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b-instruction-gptq</a>
</td>
<td id="A0.T5.1.21.21.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.21.21.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.21.21.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.22.22" class="ltx_tr">
<td id="A0.T5.1.22.22.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/youri-7b-chat-gptq" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/youri-7b-chat-gptq</a>
</td>
<td id="A0.T5.1.22.22.2" class="ltx_td ltx_align_right">LLAMA 2 Community</td>
<td id="A0.T5.1.22.22.3" class="ltx_td ltx_align_right">October 2023</td>
<td id="A0.T5.1.22.22.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.23.23" class="ltx_tr">
<td id="A0.T5.1.23.23.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-7b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-7b</a>
</td>
<td id="A0.T5.1.23.23.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.23.23.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.23.23.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.24.24" class="ltx_tr">
<td id="A0.T5.1.24.24.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-7b-instruction" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-7b-instruction</a>
</td>
<td id="A0.T5.1.24.24.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.24.24.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.24.24.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.25.25" class="ltx_tr">
<td id="A0.T5.1.25.25.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-7b-gguf" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-7b-gguf</a>
</td>
<td id="A0.T5.1.25.25.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.25.25.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.25.25.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.26.26" class="ltx_tr">
<td id="A0.T5.1.26.26.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-7b-instruction-gguf" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-7b-instruction-gguf</a>
</td>
<td id="A0.T5.1.26.26.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.26.26.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.26.26.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.27.27" class="ltx_tr">
<td id="A0.T5.1.27.27.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-14b" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-14b</a>
</td>
<td id="A0.T5.1.27.27.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.27.27.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.27.27.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.28.28" class="ltx_tr">
<td id="A0.T5.1.28.28.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-14b-instruction" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-14b-instruction</a>
</td>
<td id="A0.T5.1.28.28.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.28.28.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.28.28.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.29.29" class="ltx_tr">
<td id="A0.T5.1.29.29.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-14b-gguf" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-14b-gguf</a>
</td>
<td id="A0.T5.1.29.29.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.29.29.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.29.29.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.30.30" class="ltx_tr">
<td id="A0.T5.1.30.30.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/nekomata-14b-instruction-gguf" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nekomata-14b-instruction-gguf</a>
</td>
<td id="A0.T5.1.30.30.2" class="ltx_td ltx_align_right">Tongyi Qianwen</td>
<td id="A0.T5.1.30.30.3" class="ltx_td ltx_align_right">December 2023</td>
<td id="A0.T5.1.30.30.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.31.31" class="ltx_tr">
<td id="A0.T5.1.31.31.1" class="ltx_td ltx_align_left ltx_border_t">Language-image model</td>
<td id="A0.T5.1.31.31.2" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.31.31.3" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.31.31.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A0.T5.1.32.32" class="ltx_tr">
<td id="A0.T5.1.32.32.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-clip-vit-b-16" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-clip-vit-b-16</a>
</td>
<td id="A0.T5.1.32.32.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.32.32.3" class="ltx_td ltx_align_right">May 2022</td>
<td id="A0.T5.1.32.32.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.33.33" class="ltx_tr">
<td id="A0.T5.1.33.33.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-cloob-vit-b-16" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-cloob-vit-b-16</a>
</td>
<td id="A0.T5.1.33.33.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.33.33.3" class="ltx_td ltx_align_right">May 2022</td>
<td id="A0.T5.1.33.33.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.34.34" class="ltx_tr">
<td id="A0.T5.1.34.34.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-stable-diffusion" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-stable-diffusion</a>
</td>
<td id="A0.T5.1.34.34.2" class="ltx_td ltx_align_right">CreativeML OpenRAIL M</td>
<td id="A0.T5.1.34.34.3" class="ltx_td ltx_align_right">September 2022</td>
<td id="A0.T5.1.34.34.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.35.35" class="ltx_tr">
<td id="A0.T5.1.35.35.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/bilingual-gpt-neox-4b-minigpt4" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/bilingual-gpt-neox-4b-minigpt4</a>
</td>
<td id="A0.T5.1.35.35.2" class="ltx_td ltx_align_right">MIT</td>
<td id="A0.T5.1.35.35.3" class="ltx_td ltx_align_right">July 2023</td>
<td id="A0.T5.1.35.35.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.36.36" class="ltx_tr">
<td id="A0.T5.1.36.36.1" class="ltx_td ltx_align_left ltx_border_t">Language-speech model</td>
<td id="A0.T5.1.36.36.2" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.36.36.3" class="ltx_td ltx_border_t"></td>
<td id="A0.T5.1.36.36.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A0.T5.1.37.37" class="ltx_tr">
<td id="A0.T5.1.37.37.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-wav2vec2-base" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-wav2vec2-base</a>
</td>
<td id="A0.T5.1.37.37.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.37.37.3" class="ltx_td ltx_align_right">March 2024</td>
<td id="A0.T5.1.37.37.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.38.38" class="ltx_tr">
<td id="A0.T5.1.38.38.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-hubert-base" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-hubert-base</a>
</td>
<td id="A0.T5.1.38.38.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.38.38.3" class="ltx_td ltx_align_right">April 2023</td>
<td id="A0.T5.1.38.38.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.39.39" class="ltx_tr">
<td id="A0.T5.1.39.39.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-hubert-large" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-hubert-large</a>
</td>
<td id="A0.T5.1.39.39.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.39.39.3" class="ltx_td ltx_align_right">March 2024</td>
<td id="A0.T5.1.39.39.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.40.40" class="ltx_tr">
<td id="A0.T5.1.40.40.1" class="ltx_td ltx_align_left">   <a target="_blank" href="https://huggingface.co/rinna/japanese-data2vec-audio-base" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/japanese-data2vec-audio-base</a>
</td>
<td id="A0.T5.1.40.40.2" class="ltx_td ltx_align_right">Apache 2.0</td>
<td id="A0.T5.1.40.40.3" class="ltx_td ltx_align_right">March 2024</td>
<td id="A0.T5.1.40.40.4" class="ltx_td"></td>
</tr>
<tr id="A0.T5.1.41.41" class="ltx_tr">
<td id="A0.T5.1.41.41.1" class="ltx_td ltx_align_left ltx_border_bb">   <a target="_blank" href="https://huggingface.co/rinna/nue-asr" title="" class="ltx_ref ltx_href ltx_font_italic">rinna/nue-asr</a>
</td>
<td id="A0.T5.1.41.41.2" class="ltx_td ltx_align_right ltx_border_bb">Apache 2.0</td>
<td id="A0.T5.1.41.41.3" class="ltx_td ltx_align_right ltx_border_bb">December 2023</td>
<td id="A0.T5.1.41.41.4" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Based on our vision described in the Introduction <a href="#S1" title="1. Introduction ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Conclusion <a href="#S6" title="6. Conclusion ‣ Release of Pre-Trained Models for the Japanese Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we are continually releasing Japanese pre-trained models.
This table lists the models we have released by March 2024.</figcaption>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.01656" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.01657" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.01657">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.01657" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.01658" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 21:58:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
