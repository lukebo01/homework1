<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.02555] PhoWhisper: Automatic Speech Recognition for Vietnamese</title><meta property="og:description" content="We introduce PhoWhisper in five versions for Vietnamese automatic speech recognition. PhoWhisper’s robustness is achieved through fine-tuning the Whisper model on an 844-hour dataset that encompasses diverse Vietnamese…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PhoWhisper: Automatic Speech Recognition for Vietnamese">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PhoWhisper: Automatic Speech Recognition for Vietnamese">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.02555">

<!--Generated on Fri Jul  5 17:44:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PhoWhisper: 
<br class="ltx_break">Automatic Speech Recognition for Vietnamese</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thanh-Thien Le, Linh The Nguyen, Dat Quoc Nguyen 
<br class="ltx_break">VinAI Research, Vietnam
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{v.thienlt3, v.linhnt140, v.datnq9}@vinai.io</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We introduce <span id="id2.id1.1" class="ltx_text ltx_font_bold">PhoWhisper</span> in five versions for Vietnamese automatic speech recognition. PhoWhisper’s robustness is achieved through fine-tuning the Whisper model on an 844-hour dataset that encompasses diverse Vietnamese accents. Our experimental study demonstrates state-of-the-art performances of PhoWhisper on benchmark Vietnamese ASR datasets. We have open-sourced PhoWhisper at: <a target="_blank" href="https://github.com/VinAIResearch/PhoWhisper" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VinAIResearch/PhoWhisper</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Automatic speech recognition (ASR) technology, also referred to as speech-to-text, has experienced significant advancements <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Barrault et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Pratap et al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, expanding its applicability across a wide range of applications. The state-of-the-art ASR model, Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, has become extremely popular, being widely used in both academia and industry.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In this paper, we present an empirical study exploring Whisper for Vietnamese. Specifically, we further fine-tune the multilingual Whisper model on a large-scale ASR dataset that includes a diverse array of Vietnamese accents from different regions in Vietnam. This results in a fine-tuned model that we name PhoWhisper. Our empirical results demonstrate state-of-the-art performances of PhoWhisper, outperforming the previous best baselines on the Vietnamese Common Voice, VIVOS, VLSP 2020 Task-1 and VLSP 2020 Task-2 test sets.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">We publicly release PhoWhisper, which can be used with <span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">transformers</span> <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and <span id="S1.p3.1.2" class="ltx_text ltx_font_typewriter">openai-whisper</span> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. We hope that PhoWhisper can serve as a strong baseline for future Vietnamese ASR research and applications.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>PhoWhisper</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Our PhoWhisper has five versions, including PhoWhisper<sub id="S2.p1.1.1" class="ltx_sub">tiny</sub>, PhoWhisper<sub id="S2.p1.1.2" class="ltx_sub">base</sub>, PhoWhisper<sub id="S2.p1.1.3" class="ltx_sub">small</sub>, PhoWhisper<sub id="S2.p1.1.4" class="ltx_sub">medium</sub> and PhoWhisper<sub id="S2.p1.1.5" class="ltx_sub">large</sub>, using the same
architectures of the multilingual models Whisper<sub id="S2.p1.1.6" class="ltx_sub">tiny</sub>, Whisper<sub id="S2.p1.1.7" class="ltx_sub">base</sub>, Whisper<sub id="S2.p1.1.8" class="ltx_sub">small</sub>, Whisper<sub id="S2.p1.1.9" class="ltx_sub">medium</sub> and Whisper<sub id="S2.p1.1.10" class="ltx_sub">large-v2</sub>, respectively.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data statistics.</figcaption>
<table id="S2.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.5.6.1" class="ltx_tr">
<th id="S2.T1.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.5.6.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="S2.T1.5.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.5.6.1.2.1" class="ltx_text ltx_font_bold">Training size</span></td>
<td id="S2.T1.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.5.6.1.3.1" class="ltx_text ltx_font_bold">Validation size</span></td>
<td id="S2.T1.5.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.5.6.1.4.1" class="ltx_text ltx_font_bold">Test size</span></td>
<td id="S2.T1.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.5.6.1.5.1" class="ltx_text ltx_font_bold">#syllables in training set</span></td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">(hours)</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">(hours)</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">(hours)</td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center">(min – max <math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mo fence="false" stretchy="false" id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">|</annotation></semantics></math> average)</td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<th id="S2.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">CMV–Vi 14</th>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.04</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.41</td>
<td id="S2.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.35</td>
<td id="S2.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">1 – 14 <math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mo fence="false" stretchy="false" id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><ci id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">|</annotation></semantics></math> 7.55</td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<th id="S2.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VIVOS</th>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">13.94</td>
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">0.98</td>
<td id="S2.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_center">2 – 30 <math id="S2.T1.3.3.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S2.T1.3.3.1.m1.1a"><mo fence="false" stretchy="false" id="S2.T1.3.3.1.m1.1.1" xref="S2.T1.3.3.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.m1.1b"><ci id="S2.T1.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.m1.1c">|</annotation></semantics></math> 13.25</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<th id="S2.T1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S2.T1.4.4.2.1" class="ltx_ERROR undefined">\hdashline</span>VLSP 2020 Task-1</th>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">240.91</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">2.53</td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">7.50</td>
<td id="S2.T1.4.4.1" class="ltx_td ltx_align_center" rowspan="2"><span id="S2.T1.4.4.1.1" class="ltx_text">1 – 349 <math id="S2.T1.4.4.1.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S2.T1.4.4.1.1.m1.1a"><mo fence="false" stretchy="false" id="S2.T1.4.4.1.1.m1.1.1" xref="S2.T1.4.4.1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.1.m1.1b"><ci id="S2.T1.4.4.1.1.m1.1.1.cmml" xref="S2.T1.4.4.1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.1.m1.1c">|</annotation></semantics></math> 17.52</span></td>
</tr>
<tr id="S2.T1.5.7.2" class="ltx_tr">
<th id="S2.T1.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VLSP 2020 Task-2</th>
<td id="S2.T1.5.7.2.2" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S2.T1.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S2.T1.5.7.2.4" class="ltx_td ltx_align_center ltx_border_r">6.01</td>
</tr>
<tr id="S2.T1.5.5" class="ltx_tr">
<th id="S2.T1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S2.T1.5.5.2.1" class="ltx_ERROR undefined">\hdashline</span>Our private data</th>
<td id="S2.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">585.90</td>
<td id="S2.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S2.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S2.T1.5.5.1" class="ltx_td ltx_align_center">11 – 24 <math id="S2.T1.5.5.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S2.T1.5.5.1.m1.1a"><mo fence="false" stretchy="false" id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><ci id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">|</annotation></semantics></math> 16.90</td>
</tr>
<tr id="S2.T1.5.8.3" class="ltx_tr">
<th id="S2.T1.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Total</th>
<td id="S2.T1.5.8.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">843.79</td>
<td id="S2.T1.5.8.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3.92</td>
<td id="S2.T1.5.8.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15.61</td>
<td id="S2.T1.5.8.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">–</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">We fine-tune our models on a large-scale ASR training set consisting of 844 hours of audio collected from four different resources, including <span id="S2.p2.1.1" class="ltx_text ltx_font_bold">CMV–Vi</span>, the Vietnamese part of the Common Voice 14 <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, <span id="S2.p2.1.2" class="ltx_text ltx_font_bold">VIVOS</span> <cite class="ltx_cite ltx_citemacro_citep">(Luong &amp; Vu, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>, VLSP 2020 ASR challenge,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://vlsp.org.vn/vlsp2020/eval/asr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vlsp.org.vn/vlsp2020/eval/asr</a></span></span></span> and our private data, as shown in Table <a href="#S2.T1" title="Table 1 ‣ 2 PhoWhisper ‣ PhoWhisper: Automatic Speech Recognition for Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our “private data” is instrumental in providing the much-needed diversity of accents from 26K people spanning 63 provinces and municipalities, offering a profound understanding of the diverse ways in which Vietnamese is spoken. Finally, to enhance the robustness of our models against natural noises, we incorporate environmental sounds sourced from <cite class="ltx_cite ltx_citemacro_citet">Piczak (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> and leverage the <span id="S2.p2.1.3" class="ltx_text ltx_font_typewriter">audiomentations</span> library to add noise to half of the training set. That is, we randomly split the training set into two equal parts, A and B. We then augment part A with noise and combine the noise-augmented part A with the original part B to create the final training set of 844 hours of audio.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">For fine-tuning, we use <span id="S2.p3.1.1" class="ltx_text ltx_font_typewriter">transformers</span> <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, initializing PhoWhisper models from the corresponding multilingual Whisper models. We employ 8 A100 GPUs (40GB memory each) with a per-device batch size fixed at 4 and the number of gradient accumulation steps at 2 for all model versions, resulting in a global batch size of 64. The peak learning rates are set at 3.75e-5, 2.5e-5, 1.25e-5, 6.25e-6, and 5e-6 for PhoWhisper<sub id="S2.p3.1.2" class="ltx_sub">tiny</sub>, PhoWhisper<sub id="S2.p3.1.3" class="ltx_sub">base</sub>, PhoWhisper<sub id="S2.p3.1.4" class="ltx_sub">small</sub>, PhoWhisper<sub id="S2.p3.1.5" class="ltx_sub">medium</sub>, and PhoWhisper<sub id="S2.p3.1.6" class="ltx_sub">large</sub>, respectively. We perform a total of 48,000 updating steps, which is approximately equivalent to 5 epochs.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Empirical Results</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We compare our models with the previous state-of-the-art “wav2vec2”-based baselines from <cite class="ltx_cite ltx_citemacro_citet">Nguyen (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. These baselines are obtained by first pre-training Wav2Vec2.0 “base” and “large” models <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> on 13K hours of unlabeled Vietnamese YouTube audio and then fine-tuning them using 240+ hours of labeled training data from the VLSP 2020 ASR challenge.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on Vietnamese ASR benchmarks. “#paras” denotes the number of parameters.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">#paras</span></th>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="4"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Word Error Rate</span></td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">CMV–Vi</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VIVOS</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VLSP Task-1</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VLSP Task-2</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<th id="S3.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">wav2vec2-base-vietnamese-250h</th>
<th id="S3.T2.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">95M</th>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">102.04</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">10.83</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">21.02</td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">50.35</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<th id="S3.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">wav2vec2-base-vi-vlsp2020</th>
<th id="S3.T2.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">95M</th>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">103.71</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">9.90</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">16.82</td>
<td id="S3.T2.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.91</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<th id="S3.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">wav2vec2-large-vi-vlsp2020</th>
<th id="S3.T2.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">317M</th>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">101.41</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">8.61</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">15.18</td>
<td id="S3.T2.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.75</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<th id="S3.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S3.T2.1.6.6.1.1" class="ltx_ERROR undefined">\hdashline</span>PhoWhisper<sub id="S3.T2.1.6.6.1.2" class="ltx_sub">tiny</sub>
</th>
<th id="S3.T2.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">39M</th>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">19.05</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">10.41</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">20.74</td>
<td id="S3.T2.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">49.85</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<th id="S3.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PhoWhisper<sub id="S3.T2.1.7.7.1.1" class="ltx_sub">base</sub>
</th>
<th id="S3.T2.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">74M</th>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">16.19</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">8.46</td>
<td id="S3.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">19.70</td>
<td id="S3.T2.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.01</td>
</tr>
<tr id="S3.T2.1.8.8" class="ltx_tr">
<th id="S3.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PhoWhisper<sub id="S3.T2.1.8.8.1.1" class="ltx_sub">small</sub>
</th>
<th id="S3.T2.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">244M</th>
<td id="S3.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">11.08</td>
<td id="S3.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">6.33</td>
<td id="S3.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">15.93</td>
<td id="S3.T2.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.96</td>
</tr>
<tr id="S3.T2.1.9.9" class="ltx_tr">
<th id="S3.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PhoWhisper<sub id="S3.T2.1.9.9.1.1" class="ltx_sub">medium</sub>
</th>
<th id="S3.T2.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">769M</th>
<td id="S3.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.9.9.3.1" class="ltx_text ltx_framed ltx_framed_underline">8.27</span></td>
<td id="S3.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.9.9.4.1" class="ltx_text ltx_framed ltx_framed_underline">4.97</span></td>
<td id="S3.T2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.9.9.5.1" class="ltx_text ltx_framed ltx_framed_underline">14.12</span></td>
<td id="S3.T2.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.9.9.6.1" class="ltx_text ltx_framed ltx_framed_underline">26.85</span></td>
</tr>
<tr id="S3.T2.1.10.10" class="ltx_tr">
<th id="S3.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">PhoWhisper<sub id="S3.T2.1.10.10.1.1" class="ltx_sub">large</sub>
</th>
<th id="S3.T2.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">1.55B</th>
<td id="S3.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.10.10.3.1" class="ltx_text ltx_font_bold">8.14</span></td>
<td id="S3.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.10.10.4.1" class="ltx_text ltx_font_bold">4.67</span></td>
<td id="S3.T2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.10.10.5.1" class="ltx_text ltx_font_bold">13.75</span></td>
<td id="S3.T2.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S3.T2.1.10.10.6.1" class="ltx_text ltx_font_bold">26.68</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3 Empirical Results ‣ PhoWhisper: Automatic Speech Recognition for Vietnamese" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents our Word Error Rate (WER) results obtained for PhoWhisper and the baselines. Our PhoWhisper<sub id="S3.p2.1.1" class="ltx_sub">small</sub>, PhoWhisper<sub id="S3.p2.1.2" class="ltx_sub">medium</sub>, and PhoWhisper<sub id="S3.p2.1.3" class="ltx_sub">large</sub> outperform all the “wav2vec2”-based baselines. Meanwhile, the remaining PhoWhisper<sub id="S3.p2.1.4" class="ltx_sub">tiny</sub> and PhoWhisper<sub id="S3.p2.1.5" class="ltx_sub">base</sub> are competitive with “wav2vec2-base-vi-vlsp2020” and perform better than “wav2vec2-base-vietnamese-250h”. Here, PhoWhisper<sub id="S3.p2.1.6" class="ltx_sub">large</sub> establishes a new state-of-the-art WER score on each benchmark dataset.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this paper, we have presented an empirical study exploring Whisper-based models, specifically PhoWhisper, for Vietnamese ASR. Our experimental results showcase PhoWhisper’s state-of-the-art performance. We hope that our study and the public release of PhoWhisper will pave the way for further advancements and collaborations in this evolving field.</p>
</div>
<section id="S4.SS0.SSSx1" class="ltx_subsubsection">
<h3 class="ltx_title ltx_title_subsubsection">URM Statement</h3>

<div id="S4.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSSx1.p1.1" class="ltx_p">The authors acknowledge that at least one key author of this work meets the URM criteria of ICLR 2024 Tiny Papers Track.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2020)</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber.

</span>
<span class="ltx_bibblock">Common Voice: A Massively-Multilingual Speech Corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation</em>, pp.  4211–4215, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th Conference on Neural Information Processing Systems</em>, pp.  12449–12460, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2023)</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al.

</span>
<span class="ltx_bibblock">SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11596</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong &amp; Vu (2016)</span>
<span class="ltx_bibblock">
Hieu-Thi Luong and Hai-Quan Vu.

</span>
<span class="ltx_bibblock">A non-expert Kaldi recipe for Vietnamese Speech Recognition System.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd International Workshop on Worldwide Language Service Infrastructure and 2nd Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies</em>, pp.  51–55, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen (2021)</span>
<span class="ltx_bibblock">
Thai Binh Nguyen.

</span>
<span class="ltx_bibblock">Vietnamese end-to-end speech recognition using wav2vec 2.0, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/vietai/ASR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/vietai/ASR</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piczak (2015)</span>
<span class="ltx_bibblock">
Karol J. Piczak.

</span>
<span class="ltx_bibblock">ESC: Dataset for Environmental Sound Classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Annual ACM Conference on Multimedia</em>, pp.  1015–1018, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2023)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al.

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13516</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, pp.  28492–28518, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.02554" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.02555" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.02555">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.02555" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.02556" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 17:44:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
