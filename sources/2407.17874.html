<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.17874] Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions</title><meta property="og:description" content="End-to-end automatic speech recognition (E2E ASR) systems have significantly improved speech recognition through training on extensive datasets. Despite these advancements, they still struggle to accurately recognize d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.17874">

<!--Generated on Mon Aug  5 18:48:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=†]JiwonSuh
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=†]InjaeNa
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=]WoohwanJung








</p>
</div>
<h1 class="ltx_title ltx_title_document">Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">End-to-end automatic speech recognition (E2E ASR) systems have significantly improved speech recognition through training on extensive datasets. Despite these advancements, they still struggle to accurately recognize domain specific words, such as proper nouns and technical terminologies. To address this problem, we propose a method to utilize the state-of-the-art Whisper without modifying its architecture, preserving its generalization performance while enabling it to leverage descriptions effectively. Moreover, we propose two additional training techniques to improve the domain specific ASR: decoder fine-tuning, and context perturbation. We also propose a method to use a Large Language Model (LLM) to generate descriptions with simple metadata, when descriptions are unavailable. Our experiments demonstrate that proposed methods notably enhance domain-specific ASR accuracy on real-life datasets, with LLM-generated descriptions outperforming human-crafted ones in effectiveness.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>automatic speech recognition, contextual biasing, large language model
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><span id="footnotex1.1" class="ltx_text" style="font-size:70%;">Major in Bio Artificial Intelligence</span></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in end-to-end (E2E) automatic speech recognition (ASR) systems, such as Wav2Vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, have significantly improved the capabilities of speech recognition through extensive training on large datasets.
However, these systems often encounter difficulties in accurately identifying domain specific terms, such as proper nouns and technical jargon.
Consider a scenario where an ASR system is tasked with transcribing a lecture on mycology.
The pronunciation of `morel' ([mrel]
) and `moral' ([mrl]) can be very similar, especially in rapid speech, although these words have different meanings.
Since `moral' appears much more frequently in general speech than `morel', general-domain ASRs may misinterpret `morel' as `moral'.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Contextual biasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is widely used in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to improve the performance of speech recognition for domain-specific words.
In addition to the audio input, this method provides a biasing list that consists of words that do not appear frequently throughout the dataset or with a high error rate.
However, a notable limitation arises from the reliance on a single biasing list tailored for each specific dataset, such as Mathematics, Finance, and Chemistry.
Creating a comprehensive list that covers all potential domain-specific terms is inherently challenging. Thus, it often fails to recognize highly specific words in the current audio input (e.g., Creating a comprehensive list that covers all product names across various companies for earnings calls.).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">To alleviate the problem, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> introduce methods to incorporate more specific contextual information.
Some of the works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> utilize the preceding utterance as the contextual information.
However, this approach can suffer from the error propagation problem: an incorrect recognition in an utterance causes incorrect recognition in the next utterances.
In addition, they require an additional text encoder which should be trained on the domain specific dataset, demanding a substantial amount of training data that is frequently unavailable in domain-specific ASR.
The most relevant work to ours is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which utilizes human-written descriptions.
However, it also employs an additional model LLaMa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to utilize the textual description.
Since LLaMa (7B<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation></semantics></math>65B parameters) is much larger than typical ASR models like Whisper (0.04B <math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><csymbol cd="latexml" id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\sim</annotation></semantics></math> 1.6B parameters), it introduces a significant computational overhead.
Moreover, the human-written descriptions in real-world datasets often lack details and even are unavailable in many cases.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In response to these limitations, primarily due to the integration of additional models with pretrained ASR models,
We utilize the existing Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> framework without any additional module.
Since Whisper is already trained for ASR tasks, our approach minimizes the requirement for domain-specific training data.
We also introduce several techniques to efficiently fine-tune Whisper for domain-specific ASR with small domain specific data.
Moreover, for the cases without human-created descriptions, we propose a method to generate a description for each audio by using a Large Language Model (LLM).
The cost of utilizing an LLM is relatively small by generating a single description for each speech rather than for each utterance.
Notably, we empirically demonstrate that the LLM-generated description can surpass human-created ones in improving the accuracy of domain-specific ASRs, thanks to the detailed explanations of LLMs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we propose a method<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text" style="font-size:70%;">https://github.com/nickjw0205/Improving-ASR-with-LLM-Description</span></span></span></span> to improve domain-specific ASR by incorporating textual descriptions.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div id="S2.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:176.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-299.4pt,135.0pt) scale(0.394557544603631,0.394557544603631) ;"><img src="/html/2407.17874/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="1052" height="476" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of our method</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have explored improving ASR models with textual context typically by combining an acoustic model and a language model.
For instance, a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> leverages HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as the audio encoder and LLaMa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> as the language model.
Despite the effective utilization of textual information, this approach requires extensive domain-specific data for model training.
To address this challenge, we decide to utilize descriptions without modifying the architecture of an existing ASR model.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Conventional ASR models usually do not have a text encoder since they primarily focus on converting speech to text.
Thus, we cannot use such a model without additional modules.
However, Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, a state-of-the-art ASR model, intrinsically supports textual input.
Whisper is built on Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> architecture with an encoder and a decoder.
The encoder processes audio inputs while the decoder generates transcripts.
Given that the decoder is based on a Transformer, it can accept a prompt prior to transcript generation.
Thus, we input textual descriptions as prompts.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Method ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of our method which utilizes a pretrained Whisper model without modifying its original architecture.
The audio input is processed by the encoder, consistent with the original Whisper.
Note that the encoder remains frozen during fine-tuning, a topic further explored in Section <a href="#S2.SS2" title="2.2 Training the Proposed Model ‣ 2 Method ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
We give textual descriptions as prompts to the decoder.
Whisper has two special tokens <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">&lt;SOP&gt;</span> (Start of previous) and <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">&lt;SOT&gt;</span> (Start of transcript) which is to take the transcription of the preceding 30 seconds of audio segment as contextual information.
We adapt these tokens to input our textual descriptions in the format
<span id="S2.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">&lt;SOP&gt;</span> <span id="S2.SS1.p3.1.4" class="ltx_text ltx_font_typewriter">description</span> <span id="S2.SS1.p3.1.5" class="ltx_text ltx_font_typewriter">&lt;SOT&gt;</span>.
This strategy enables the model to utilize descriptions without an additional text encoder.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training the Proposed Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Our model can be fine-tuned in the same way as the original Whisper was trained since it uses the architecture of Whisper without any modification.
However, to improve the performance of the domain-specific ASR tasks with textual description, we need to consider its unique characteristics.
First, domain-specific datasets are generally much smaller than general-domain datasets.
Second, not every utterance directly relates to the provided textual description. We propose two methods to address the issues: decoder fine-tuning and context perturbation.
</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Decoder Fine-tuning</span>. Domain-specific ASR models suffer from the scarcity of training data with speech-text pairs.
To avoid overfitting and catastrophic forgetting, it is necessary to minimize the number of parameters to be trained.
The decoder of our model process descriptions as additional inputs, a feature not present during the pretraining phase of Whisper, mandating fine-tuning.
Conversely, fine-tuning the encoder is not essential since it solely processes audio, just as during pretraining.
The pretrained Whisper encoder has an excellent capability in featuring audio information even without fine-tuning.
According to a recent study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Whisper with a frozen encoder exhibits superior performance in various tasks, including intent classification, keyword spotting, and ASR, compared to the model where the encoder has been fine-tuned.
It implies that retraining the encoder may require a significant amount of data and risk losing its generalization performance.
Therefore, we opt to freeze the encoder and fine-tune the decoder to effectively leverage the textual description.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Context Perturbation</span>.
Although textual descriptions aid in recognizing domain specific words, not all utterances contain domain specific words.
For example, situations like greetings or humorous remarks for ice-breaking may not include domain-specific words.
In these instances, ASR models should be able to ignore the provided description.
Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>,
we occasionally input a randomly sampled description instead of the actual one with a probability of 5% in our experiments.
This approach trains the model to selectively utilize the description.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Generating Descriptions using LLM</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Utilizing textual descriptions can significantly improve ASR performance.
However, there are numerous situations where textual descriptions may not be available, such as personal recordings and live streams.
Moreover, even when textual descriptions are available, they often do not include necessary detail.
For example, consider the task of transcribing a university lecture through ASR.
While the syllabus may provide an overview of the course, it typically lacks specific details about individual lecture topics.
To overcome this challenge, we propose generating detailed descriptions through LLM.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We generate descriptions by using LLM with simple metadata about audio files.
To validate the effectiveness of LLM-generated descriptions with very simple metadata,
we use two datasets in our experiments: Earnings Call <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which contains discussions about financial results of particular public companies, and OCW (MIT OpenCourseWare), which includes video lectures.
For the Earnings Call dataset, the description of conference call is created by using the company name with a prompt “Explain about [<span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">company name</span>] in 2 sentences.”.
Due to the weakness of Whisper with processing long descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we limit the length of the description with the instruction "in 2 sentences".
For the OCW dataset, we use the prompt “Today’s lecture title is [<span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">lecture title</span>]. Please explain the academic field and content in 2 lines.”.
This technique, relying solely on simple metadata (lecture titles and company names), enables the extraction of essence-capturing descriptions, drawing on the extensive knowledge of LLM.
It is worth noting that the cost of using LLM is not high.
For example, the OCW dataset has 508 audio segments per lecture on average, indicating that the execution of ASR model is 508 times more frequent than that of LLM in the OCW dataset.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To evaluate the domain-specific ASR models with descriptions, we use the following two datasets.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Earnings Call <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>.
It consists of quarterly earning conference calls from S&amp;P 500 companies in 2017.
The Earnings Call dataset contains various domain-specific information, such as company and product names.
After removing duplicate calls from the same companies,
this dataset consists of 169 conference calls.
We split the dataset into 113, 28, and 28 calls for training, validation, and testing, respectively.
The total lengths of the audio in train/valid/test sets are approximately 40/10/10 hours, respectively.
The audio quality of Earnings Call is much lower than professionally recorded datasets due to factors like multiple speakers and fluctuating recording environments.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">OCW<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_medium">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_medium" style="font-size:70%;">https://github.com/nickjw0205/Improving-ASR-with-LLM-Description</span></span></span></span> </span>.
We collected a new dataset to validate the effectiveness of our model with academic words.
This dataset is collected from a MIT OpenCourseWare website<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text" style="font-size:70%;">https://ocw.mit.edu/, CC BY-NC-SA 4.0 license</span></span></span></span>
and includes the videos, English transcripts, and titles of 65 lectures on various academic fields such as linear algebra, biological chemistry, and cryptocurrency.
We split the dataset into train (44 lectures), valid (12 lectures), and test (9 lectures) sets to make their lengths are approximately 40 hours, 10 hours, and 10 hours, respectively.
We excluded all words from the transcript that were not actual utterances but were added to aid understanding.
Examples of such words include [No audio], [Not AUDIBLE], [Laugh], [DOOR CLOSES], etc.
Additionally, we performed preprocessing to ensure proper alignment between the audio and transcript by removing the speaker identifiers, such as `Speaker:', used to denote the speaker.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Setup</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We utilize the Whisper (base.en), a Transformer encoder-decoder model with 74 million parameters.
Notably, our method fine-tunes only the decoder which has 52 million parameters.
To ensure compatibility with Whisper, all audio files are resampled to 16kHz and transcriptions are normalized by converting to lowercase and removing punctuations.
To train Whisper, we use the HuggingFace Transformers library<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span id="footnote4.1" class="ltx_text" style="font-size:70%;">https://github.com/huggingface/transformers</span></span></span></span>.
Following the Whisper's training, we use AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> optimizer with a linear learning rate decay scheduler.
The training process was conducted with a batch size of 32, initiating learning rate decay following a 100-step warm-up period. We explored initial learning rates of 1e-4, 1e-5, and 1e-6, ultimately selecting 1e-4 for Earnings Call and 1e-6 for OCW based on performance.
Training durations are set at 10 epochs for Earnings Call and 15 epochs for OCW.
We use the cross entropy loss following Whisper.
Other settings follow the default configuration of the HuggingFace trainer.
All models are trained on an NVIDIA RTX 4090 GPU.
Unless otherwise specified, we use the descriptions generated by GPT-3.5 Turbo<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span id="footnote5.1" class="ltx_text" style="font-size:70%;">https://openai.com/product</span></span></span></span> as proposed in Section <a href="#S2.SS3" title="2.3 Generating Descriptions using LLM ‣ 2 Method ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
The evaluation metric is the Word Error Rate (WER).
Note that the training time was about 6 hours for Earnings Call, and 8 hours for OCW.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Main results</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:183.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.7pt,-28.6pt) scale(1.45387805507079,1.45387805507079) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2">Earnings Call</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2">OCW</td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Models</td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">20 h</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">40 h</td>
<td id="S3.T1.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">20 h</td>
<td id="S3.T1.1.1.2.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">40 h</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Whisper (Frozen)</td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.39%</td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.39%</td>
<td id="S3.T1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.98%</td>
<td id="S3.T1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.98%</td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">+ Full Fine-tuning</td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.38%</td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.64%</td>
<td id="S3.T1.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.41%</td>
<td id="S3.T1.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.94%</td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">+ Description</td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">20.63%</td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.70%</td>
<td id="S3.T1.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.81%</td>
<td id="S3.T1.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.72%</td>
</tr>
<tr id="S3.T1.1.1.6" class="ltx_tr">
<td id="S3.T1.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">+ Decoder Fine-tuning</td>
<td id="S3.T1.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">16.61%</td>
<td id="S3.T1.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">15.70%</td>
<td id="S3.T1.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T1.1.1.6.4.1" class="ltx_text ltx_font_bold">9.79%</span></td>
<td id="S3.T1.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T1.1.1.6.5.1" class="ltx_text ltx_font_bold">9.67%</span></td>
</tr>
<tr id="S3.T1.1.1.7" class="ltx_tr">
<td id="S3.T1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">+ Context Perturbation</td>
<td id="S3.T1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T1.1.1.7.2.1" class="ltx_text ltx_font_bold">16.24%</span></td>
<td id="S3.T1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T1.1.1.7.3.1" class="ltx_text ltx_font_bold">15.15</span>%</td>
<td id="S3.T1.1.1.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T1.1.1.7.4.1" class="ltx_text ltx_font_bold">9.79%</span></td>
<td id="S3.T1.1.1.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.68%</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of outputs with and without descriptions.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt">Edwards Lifesciences (Earnings Call)</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt">Assembly Language &amp; Computer Architecture (OCW)</td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Description</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.1.1" class="ltx_p" style="width:173.4pt;">Edwards Lifesciences is a global medical technology company that specializes in the development and manufacturing of heart valves … improve the lives of patients with <span id="S3.T2.1.2.2.1.1.1" class="ltx_text ltx_font_bold">cardiovascular</span> diseases</span>
</span>
</td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.3.1.1" class="ltx_p" style="width:173.4pt;">Assembly language is a low-level <span id="S3.T2.1.2.3.1.1.1" class="ltx_text ltx_font_bold">programming language</span> that communicates directly with the hardware of a computer system. Computer architecture is the study of how computers …</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_tt">Reference</td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T2.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1" class="ltx_p" style="width:173.4pt;">New treatments and therapies for <span id="S3.T2.1.3.2.1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">cardiovascular</span> care and heart muscle recovery</span>
</span>
</td>
<td id="S3.T2.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T2.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.1.1" class="ltx_p" style="width:173.4pt;">It's just a bunch of <span id="S3.T2.1.3.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">bytes</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.4.1.1" class="ltx_text">
<span id="S3.T2.1.4.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T2.1.4.1.1.1.1" class="ltx_p">Output</span>
<span id="S3.T2.1.4.1.1.1.2" class="ltx_p">w/ description</span>
</span></span></td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.2.1.1" class="ltx_p" style="width:173.4pt;">New treatments and therapies for <span id="S3.T2.1.4.2.1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">cardiovascular</span> care and heart muscle recovery</span>
</span>
</td>
<td id="S3.T2.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1" class="ltx_p" style="width:173.4pt;">It's just a bunch of <span id="S3.T2.1.4.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">bytes</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.5.1.1" class="ltx_text">
<span id="S3.T2.1.5.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T2.1.5.1.1.1.1" class="ltx_p">Output</span>
<span id="S3.T2.1.5.1.1.1.2" class="ltx_p">w/o description</span>
</span></span></td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T2.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.2.1.1" class="ltx_p" style="width:173.4pt;">New treatments and therapies for <span id="S3.T2.1.5.2.1.1.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">cardiovasular</span> care and heart missile recovery</span>
</span>
</td>
<td id="S3.T2.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T2.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.3.1.1" class="ltx_p" style="width:173.4pt;">It's just a bunch of <span id="S3.T2.1.5.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">bites</span></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Main Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To demonstrate the effectiveness of our proposed method we show the word error rates of ASR with different train data sizes (20 and 40 hours) in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Experimental Setup ‣ 3 Experiments ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We systematically investigate the effectiveness of the proposed components by incrementally introducing the following elements to the frozen Whisper: fine-tuning, utilization of descriptions, decoder fine-tuning (freezing encoder), and context perturbation.
Applying all proposed techniques result in the best ASR model performance, demonstrating the effectiveness of our methods.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The experimental results on Earnings Call (20 hours) are quite interesting.
When the Whisper model was fully fine-tuned with textual descriptions, there was a significant increase in WER by more than 4 percentage points, indicating a substantial performance decline.
This can be interpreted as a result of overfitting and catastrophic forgetting, which occurs when training on a small amount of data for domain-specific ASR tasks, as we previously mentioned.
However, even in such cases, our method, employing decoder fine-tuning and context perturbation, demonstrated its ability to overcome these issues.
This suggests that it is essential to perform decoder fine-tuning along with context perturbation to achieve robust results with a small amount of data and in conditions of low audio quality.
While the results from the OCW dataset are not as dramatic as those from the Earnings Call (20h), they still confirm the stable performance improvements by our methods.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples of collected and generated descriptions.</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Description</td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T3.1.2.1.1" class="ltx_text">
<span id="S3.T3.1.2.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:37.0pt;">
<span id="S3.T3.1.2.1.1.1.1" class="ltx_p">Collected</span>
</span></span></td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T3.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.1.1" class="ltx_p" style="width:316.5pt;">This course is an introduction to mathematical modeling of computational problems, as well as common algorithms, algorithmic paradigms, and data structures used to solve these problems. It emphasizes the relationship between algorithms and programming, and introduces basic performance measures and analysis techniques for these problems.</span>
</span>
</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T3.1.3.1.1" class="ltx_text">
<span id="S3.T3.1.3.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:37.0pt;">
<span id="S3.T3.1.3.1.1.1.1" class="ltx_p">LLM</span>
<span id="S3.T3.1.3.1.1.1.2" class="ltx_p ltx_align_center">Generated</span>
</span></span></td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T3.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.2.1.1" class="ltx_p" style="width:316.5pt;"><span id="S3.T3.1.3.2.1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">hashing</span> is a fundamental concept in computer science and cryptography that involves creating a unique, fixed-length representation of data using a mathematical algorithm. It is commonly used in data retrieval, password security, and ensuring data integrity.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Qualitative Analysis</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Experimental Setup ‣ 3 Experiments ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows examples of transcription, both with and without using descriptions.
The first example provides a description about Edward Lifesciences, a medical technology company, and transcription results for a sentence from an Edward Lifesciences earnings call.
The description incorporates domain-specific terminology such as cardiovascular and includes comprehensive information about the company.
By using this description, our method accurately transcribes the medical term `cardiovascular'.
However, the conventional speech recognition method struggles to accurately transcribe the domain-specific word `cardiovascular'.
In the OCW dataset, the description offers insights into "Assembly Language and Computer Architecture," facilitating the accurate identification of computer-related terms such as byte' based on this contextual information. Conversely, without utilizing the description, the term `byte' is misrecognized as its homophone `bite', because there is no computer-related context information during the transcription process. These indicate that descriptions provide not only domain-specific terms but also important contextual information to the ASR model. Therefore, incorporating descriptions into the ASR model can enhance the recognition accuracy of domain-specific words.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Comparison of LLM-Generated and Collected Descriptions</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p"><span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_bold">Collection of human-written descriptions</span>.
To evaluate the effectiveness of the LLM-generated descriptions, we also collect human-written descriptions for our dataset as a comparison.
We collect descriptions from Wikipedia for company information in the Earnings Call dataset, which are often overly detailed because they aim to provide comprehensive explanations. For the OCW dataset, we collect descriptions of the respective courses that provide a broad course overview, rather than specifics of individual lectures, reflecting their purpose of providing a general course summary.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p"><span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_bold">Description Quality</span>.
Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Main Results ‣ 3 Experiments ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents a LLM-generated description and using the lecture title ``Hashing" and a human-written one collected from the course description of ``Introduction to Algorithms".
The collected description offers an overview of the course, outlining the content to be learned at a comprehensive level.
In contrast, the description generated by LLM offers detailed information about the specific lecture `Hashing'.
This indicates that LLM can generate more specific and relevant descriptions than collected descriptions in real world applications.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results on generated and collected descriptions.</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Description</td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Earnings Call</td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">OCW</td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Collected</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">15.33%</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">10.30%</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_bb">LLM Generated</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T4.1.3.2.1" class="ltx_text ltx_font_bold">15.15</span>%</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T4.1.3.3.1" class="ltx_text ltx_font_bold">9.68</span>%</td>
</tr>
</table>
</figure>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<p id="S3.SS5.p3.1" class="ltx_p"><span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_bold">ASR Performance</span>.
In Table <a href="#S3.T4" title="Table 4 ‣ 3.5 Comparison of LLM-Generated and Collected Descriptions ‣ 3 Experiments ‣ Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare ASR performances with collected descriptions and LLM-generated descriptions.
For the Earnings Call, transcription results using LLM-generated descriptions slightly outperform those using collected descriptions by 0.18%p.
The LLM-generated descriptions contribute to this improvement by omitting unnecessary details and adopting more concise and central information.
In the OCW, transcriptions utilizing LLM-generated descriptions demonstrated a lower WER compared to those with collected descriptions by 0.62%p.
This difference is due to the clear disparity in content between the LLM-generated description and the collected one, as detailed in Table 3.
Collected descriptions provide only general information about the courses, failing to capture the specific details essential for each lecture audio.
However, the LLM-generated description is tailored to the lecture title ``hashing", providing detailed information relevant to the specific lecture.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose a domain-specific speech recognition model which leverages descriptions of speeches.
To address the data scarcity problem in domain-specific ASR, we propose a method to utilize the state-of-the-art Whisper without modifying its architecture.
Moreover, we propose decoder fine-tuning, and context perturbation to preserve the Whisper's generalization performance while utilizing descriptions.
The empirical studies show the effectiveness of our methods with two real datasets.
Furthermore, we find that LLM-generated descriptions can outperform human-written ones in domain-specific ASR with description.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 12449–12460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pp. 28492–28518, PMLR, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Aleksic, M. Ghodsi, A. Michaely, C. Allauzen, K. Hall, B. Roark, D. Rybach, and P. Moreno, ``Bringing contextual information to google speech recognition,'' in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2015</span>, pp. 468–472, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Le, M. Jain, G. Keren, S. Kim, Y. Shi, J. Mahadeokar, J. Chan, Y. Shangguan, C. Fuegen, O. Kalinli, Y. Saraf, and M. L. Seltzer, ``Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion,'' in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2021</span>, pp. 1772–1776, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Huang, A. Zhang, Z. Yang, P. Guo, B. Mu, T. Xu, and L. Xie, ``Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network,'' in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, pp. 4933–4937, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Xu, B. Liu, Q. Huang, X. Song, Z. Wu, S. Kang, and H. Meng, ``Cb-conformer: Contextual biasing conformer for biased word recognition,'' in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 1–5, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
G. Sun, X. Zheng, C. Zhang, and P. C. Woodland, ``Can Contextual Biasing Remain Effective with Whisper and GPT-2?,'' in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, pp. 1289–1293, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X. Yang, W. Kang, Z. Yao, Y. Yang, L. Guo, F. Kuang, L. Lin, and D. Povey, ``Promptasr for contextualized asr with controllable style,'' <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.07414</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.-Y. Chang, C. Zhang, T. N. Sainath, B. Li, and T. Strohman, ``Context-aware end-to-end asr using self-attentive embedding and tensor fusion,'' in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 1–5, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Duarte-Torres, A. Sen, A. Rana, L. Drude, A. Gomez-Alanis, A. Schwarz, L. Rädel, and V. Leutnant, ``Promptformer: Prompted conformer transducer for asr,'' <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.07360</span>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Li, Y. Wu, J. Li, and S. Liu, ``Prompting large language models for zero-shot domain adaptation in speech recognition,'' in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span>, pp. 1–8, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">et al.</span>, ``Llama: Open and efficient foundation language models,'' <span id="bib.bib12.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ``Attention is all you need,'' <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Yang, J. Zhao, G. Haffari, and E. Shareghi, ``Investigating Pre-trained Audio Encoders in the Low-Resource Condition,'' in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, pp. 1498–1502, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Qin and Y. Yang, ``What you say and how you say it matters: Predicting stock volatility using verbal and vocal cues,'' in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pp. 390–401, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L. Zhuo, R. Yuan, J. Pan, Y. Ma, Y. LI, G. Zhang, S. Liu, R. Dannenberg, J. Fu, C. Lin, <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">et al.</span>, ``Lyricwhiz: Robust multilingual zero-shot lyrics transcription by whispering to chatgpt,'' <span id="bib.bib17.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.17103</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Ilya and F. Hutter, ``Decoupled weight decay regularization,'' <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.05101</span>, 2017.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.17872" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.17874" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.17874">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.17874" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.17875" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:48:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
