<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.03059] Quantification of stylistic differences in human- and ASR-produced transcripts of African American English</title><meta property="og:description" content="Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs no…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Quantification of stylistic differences in human- and ASR-produced transcripts of African American English">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Quantification of stylistic differences in human- and ASR-produced transcripts of African American English">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.03059">

<!--Generated on Sun Oct  6 01:17:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]AnnikaHeuser
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]TylerKendall
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=3]Migueldel Rio
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=3]QuintenMcNamara
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=3]NishchalBhandari
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=3]CoreyMiller
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=3]MigüelJetté




</p>
</div>
<h1 class="ltx_title ltx_title_document">Quantification of stylistic differences in human- and ASR-produced transcripts of African American English</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for speech from underrepresented varieties, where the speech to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) speech. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training data’s human transcribers.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>automatic speech recognition, evaluation, transcription variation, bias, African American English, AAE
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Word error rate (WER) is the standard metric for automatic speech recognition (ASR) evaluation, widely used across industry and research. However, WER is readily affected by the properties of an ASR system’s training and test data. All the idiosyncrasies of the chosen reference transcript play a role in how the system is trained, refined, and assessed.
This means a system's performance could degrade if the reference doesn't reflect the training data's idiosyncrasies, many of which could be considered stylistic in nature.
Transcription styles are not a novel idea: in fact, companies like Rev<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.rev.com/blog/resources/verbatim-transcription" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.rev.com/blog/resources/verbatim-transcription</a></span></span></span> produce distinct verbatim and non-verbatim transcripts. Verbatim transcription includes filler words, such as ``um" and ``uh," false starts, and interjections, while non-verbatim transcription allows for light editing, still preserving the content of what was said in the audio.
Both are valid styles, but directly comparing a verbatim vs non-verbatim transcript would misleadingly highlight several ``errors", even between humans. To add to the complications, transcription companies commonly assign separate chunks of a single long file to different transcribers in order to maintain reasonable delivery times.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Deep Learning models are well known to capture their training data's distribution. We posit that during training, models acquire additional types of stylistic proclivities which can be explicitly observed, even beyond the verbatim vs non-verbatim axis of variation. It is precisely our limited knowledge about these proclivities that makes comparing different ASR models more challenging. Within the WER evaluation paradigm, a model would be penalized for not having a stylistic proclivity that another model and the reference transcript share. It is, however, critical that we can accurately compare different ASR models, in order to determine which architectures, training strategies, etc. are most effective.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We define transcription style as the collection of decisions made in contexts where there are multiple reasonable alternatives for how to transform the audio signal into an orthographic representation. Additionally, style can make a transcript better suited to its purpose. Not all the differences between two transcripts are stylistic in nature – for example: some might be perceptual disagreements, while others could of course be actual errors, like typos. Some of the differences might seem stylistic to one person but not another, because they do not agree on whether a given transcription choice was among the set of reasonable alternatives for the given context.
Bucholtz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, p. 1452]</cite> describes a situation where transcribers might be trying to do the ``original speaker a favor by `cleaning up'" their speech. However, some choices could be considered counter-productive or harmful because they might misrepresent the speech of an individual or community. Consequently, many researchers would not want ASR systems to replicate these.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The current project collects multiple transcripts of a subset of the sociolinguistic interviews contained in the Corpus of Regional African American Language (CORAAL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and characterizes differences between them. We collected multiple transcripts to demonstrate that professional transcripts of the same audio can differ substantially, with concomitant effect on WER. The original CORAAL transcripts were transcribed and corrected by multiple researchers familiar with AAE, but the other transcripts are still professional-grade. As an underrepresented variety of English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, AAE's orthography is not nearly as conventionalized as that of Standard American English (SAE).
We divide our transcripts into two groups: those produced by humans and those produced by ASR systems. We compare the distributions of differences within and across these two groups by categorizing the types of differences.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we examine three categories of transcription differences, which serve as hypotheses for the potential <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">sources</span> of the differences. For example, a verbatim vs non-verbatim category or hypothesis posits that any given difference between two transcripts could be due to the transcribers having different verbatim objectives. We then quantify what percentage of the time this hypothesis is true for any transcript pairing. In addition to 1) the verbatim vs non-verbatim hypothesis, we also test 2) whether morpho-syntactic features that differentiate AAE from SAE and 3) whether different reduction and contraction orthographic representations (e.g. ``going to" vs ``gonna" and ``she will" vs ``she'll") account for the differences.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The morpho-syntactic hypothesis allows us to investigate ASR bias against AAE, and potentially identify its source. The greater the percentage of transcript differences that are accounted for by the morpho-syntactic hypothesis, the more one transcript or the other might be transcribing AAE as SAE. By applying the same test to ASR output and human-produced transcripts from the same distribution as the training data, we can track the extent to which the ASR system is emulating the human rates of transcription decisions regarding AAE morpho-syntactic features.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">These 3 hypotheses serve as our metrics to quantify the stylistic differences across transcript versions. The 3 categories examined here are not meant to capture the full range of possible differences, but we hope they can contribute to a complete ontology of the axes of transcription variation, which is left for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Inter-transcriber variation, while under-explored in the context of ASR, has been examined in allied fields, such as phonetics, conversation analysis, and forensic linguistics. The analogy to verbatim vs non-verbatim in phonetics is narrow vs broad transcription. As might be expected since broad uses fewer symbols/distinctions than narrow, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> found that inter-transcriber reliability was higher for broad transcription. Nonetheless, broad phonetic transcripts are still much ``narrower" than word-level ASR transcripts. Conversation analysts often focus closely on transcriber decisions and agreement, in ways that are relevant to the interests of this paper, but focus on a wider-range of speech phenomena (such as pauses and intonation) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Forensic linguists are often concerned with content agreement between humans and also ASR transcriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Given the stakes of transcription in legal contexts, it is perhaps unsurprising that forensic linguists have considered categories of transcription differences. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> generated a difference ontology by manually examining eight transcripts of an audio recording produced by different linguistically trained transcribers. It consisted of 1) omitted/additional speech, 2) splitting of turns, 3) phonetic similarity, and 4) lexical variation. 1) corresponds to the verbatim vs non-verbatim distinction and 2) corresponds to speaker attribution/diarization differences as opposed to word-level phenomena. Like 1), 3) is a hypothesis of the origin of transcription differences, namely that they are the result of perceptual differences. Finally, 4) corresponds to the rest of the transcription differences.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Recent work, in both forensic linguistics and in ASR research, has investigated transcription accuracy on non-standard varieties of English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, particularly on AAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, work thus far has not investigated the categories underlying disagreements and inaccuracies across human- and ASR-produced transcripts of the same audio data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We selected 27 files from CORAAL, corresponding to about 10 hours of audio. For each file we produced 6 transcript versions (referred to simply as versions from this point forward).

<br class="ltx_break">
<br class="ltx_break"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Human Versions</span></p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">CORAAL</span>: The original transcript from the CORAAL corpus, produced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Rev</span>: Generated by soliciting a verbatim transcript through the web interface of Rev.com.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Rev (+AA tag)</span>: Generated exactly like the <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">Rev</span> transcript, but with the additional specification of ``Other - African American" in the accent information, which we expected to recruit transcribers more familiar with the variety.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Amberscript</span>: A verbatim transcript from Amberscript<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.amberscript.com/en/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.amberscript.com/en/</a></span></span></span>. We were helped by a salesperson who matched our audio with transcribers deemed well-suited.</p>
</div>
</li>
</ul>
<p id="S3.p1.2" class="ltx_p"><span id="S3.p1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Machine Versions</span></p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Rev ASR</span>: Generated using Rev.com's internal verbatim ASR model<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://docs.rev.ai/api/asynchronous/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.rev.ai/api/asynchronous/</a></span></span></span>, which is described in greater detail in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, Section 3.1]</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">OpenAI's <span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Whisper</span>: Generated using OpenAI's API<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://platform.openai.com/docs/guides/speech-to-text" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/guides/speech-to-text</a></span></span></span> to their large-v2 Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> model.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">It is important to clarify that the <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">CORAAL</span> transcript versions were developed by a team of linguistic researchers; each file passed through multiple stages of transcription and editing where a researcher had access to the whole audio file. The <span id="S3.p2.1.2" class="ltx_text ltx_font_bold">Rev</span>, <span id="S3.p2.1.3" class="ltx_text ltx_font_bold">Rev (+AA tag)</span>, and <span id="S3.p2.1.4" class="ltx_text ltx_font_bold">Amberscript</span> versions on the other hand were developed by professional transcribers who were only given a section of the audio to work with; each section could then have its quality verified and improved upon by a senior transcriber.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We used the open source
<span id="S4.p1.1.1" class="ltx_text ltx_font_italic">fstalign<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_upright">5</span></span><a target="_blank" href="https://github.com/revdotcom/fstalign/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://github.com/revdotcom/fstalign/</a></span></span></span></span> with default settings to align transcript pairs to produce alignments of every permutation of transcript pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
We generated tests<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/revdotcom/speech-datasets/tree/main/coraal-multi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/revdotcom/speech-datasets/tree/main/coraal-multi</a></span></span></span> for our three transcription difference source hypotheses: <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">morpho-syntactic</span>, <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">reductions</span>, and <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">verbatim</span>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Our <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">morpho-syntactic</span> tests are based on the features enumerated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
We could not translate all the features into potential transcription difference tests. For example, 19p in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> refers to stressed ``stay," but the stressed and unstressed versions cannot be differentiated in written form. Another example is 20d in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which describes the past participle form being used as the past tense. However, many common verbs have irregular participle or past forms (e.g. ``see"/``seen"/``saw" and ``run"/``run"/``ran"), making it difficult to algorithmically test for this alternation. Of the tests we were able to develop, some failed to capture any transcript differences. The <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">morpho-syntactic</span> hypothesis ultimately consisted of 17 tests.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">reductions</span> hypothesis consists of common contractions as well as a set of conventions used by CORAAL transcribers for reductions. The common contractions test checks for ``she'd/'s/'ve/'ll/'re/'t" contractions and their longer forms (e.g. ``she would/did/had," "she is/has," etc.). The CORAAL reduced form test checks for whether a substitution is made up of a full form and reduced form pairing listed in the table on pages 21-22 of the CORAAL user guide<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="http://lingtools.uoregon.edu/coraal/userguide/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://lingtools.uoregon.edu/coraal/userguide/</a></span></span></span>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Finally, our <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">verbatim</span> tests checked for filler deletions, filler substitution (e.g. transcript 1 has ``uh" while transcript 2 has ``um"), restart deletion or lack of restart indication (e.g. ``you-" vs ``you"), and repetition deletion.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">These hypotheses are in order of most to least indicative of speaker characteristics. AAE feature erasure captured by <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">morpho-syntactic</span> differences results in a more SAE-looking transcript which will potentially misrepresent the speech signal.
<span id="S4.p5.1.2" class="ltx_text ltx_font_italic">Reductions</span> are prevalent in both AAE and SAE, and they do not generally change the expression meaning, unlike some of the alternations caught by the morpho-syntactic tests. They do, however, have pragmatic consequences in that reduced forms are considered vernacular <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>; someone speaking in a more formal event, e.g. an interview or a trial, might prefer their reduced speech to be transcribed as the long forms. Finally, <span id="S4.p5.1.3" class="ltx_text ltx_font_italic">verbatim</span> differences do not change the speech content, and speakers are disfluent in every register, though the social context can impact how speakers are disfluent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Word error rates</h3>

<figure id="S5.F1" class="ltx_figure"><img src="/html/2409.03059/assets/humhum-final.png" id="S5.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>WER of human transcript pairs.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We first report WERs between the 4 human transcript versions. Though WER traditionally measures the error rate between an ASR hypothesis and a human reference, in this context we utilize the same WER mechanism to quantify the differences among humans by taking one human version as the reference and another as the hypothesis. We report the full WER as well as the individual rates of error that it is composed of, namely the rates of insertions (INS), deletions (DEL), and substitutions (SUB). As noted in <a href="#S5.F1" title="In 5.1 Word error rates ‣ 5 Results ‣ Quantification of stylistic differences in human- and ASR-produced transcripts of African American English" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the WERs range between 10% and 20%, demonstrating the importance of the reference transcript to evaluation – especially as many papers report traditional human vs machine WERs at much lower rates (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Unsurprisingly, the lowest WER is between the Rev and Rev (+AA tag) transcripts, likely because they were produced by a similar transcriber population. While the transcribers for the Rev (+AA tag) transcript may have been more familiar with AAE, they used the same style guide as the transcribers of the Rev transcript. It is even possible that there was overlap in the transcribers for the two sets of transcript versions. The Rev and Rev (+AA tag) versions also had relatively low WERs against the CORAAL transcript, suggesting similar stylistic proclivities. On the other hand, the greatest WER was between the CORAAL and Amberscript transcripts, most noticeably caused by the disproportionate amount of insertions.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">We turn to the WERs of the Rev ASR and Whisper models, reported in <a href="#S5.F2" title="In 5.1 Word error rates ‣ 5 Results ‣ Quantification of stylistic differences in human- and ASR-produced transcripts of African American English" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Rev's ASR performance is comparable between the CORAAL and both Rev transcript versions, but worse on Amberscript's version. Unexpectedly, we see a similar trend for the Whisper performance. We theorize that the higher deletion rate, compared to Rev, implies that the main difference between the models is likely where they fall on the verbatim to non-verbatim scale. We explore this hypothesis in the next section.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2409.03059/assets/asrhum-corrected.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>ASR WER against human transcript versions.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Difference source hypotheses</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2409.03059/assets/BucketPercentsNew.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The average percentage of total differences that each hypothesis accounted for across each transcript pairwise comparison. The error bars correspond to the standard deviation across the transcript version pairwise comparisons.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Looking across all transcripts, <a href="#S5.F3" title="In 5.2 Difference source hypotheses ‣ 5 Results ‣ Quantification of stylistic differences in human- and ASR-produced transcripts of African American English" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows that the biggest categories of differences are <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">verbatim</span> and <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">morpho-syntactic</span>, with <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">reductions</span> accounting for very few differences. We tease out the impact of each of these two categories of differences per each transcript version pair.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2409.03059/assets/VerbatimFullHeatMapNew.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The percentage of differences between transcript pairs for verbatim category.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><a href="#S5.F4" title="In 5.2 Difference source hypotheses ‣ 5 Results ‣ Quantification of stylistic differences in human- and ASR-produced transcripts of African American English" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> verifies our hypothesis that a higher percentage of the differences between the Whisper transcript version and all the other versions are related to verbatim style choices. In fact, over all pairs, the greatest percentage of verbatim differences is between the Whisper and CORAAL versions while the lowest is between the Rev ASR model and the Rev versions. We note that the verbatim percentage between the Rev ASR model and the Rev (+AA tag) version is particularly large, larger than the difference between the two ASR models' transcript versions. The addition of the AA tag could have resulted in the transcribers taking greater liberty with respect to many parts of the style guide, including the verbatim instructions.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2409.03059/assets/Morpho-syntacticFullHeatMap.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The percentage of differences between transcript pairs for morpho-syntactic category.</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Looking into the morpho-syntactic differences, <a href="#S5.F5" title="In 5.2 Difference source hypotheses ‣ 5 Results ‣ Quantification of stylistic differences in human- and ASR-produced transcripts of African American English" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows that the Rev ASR vs Rev transcript versions and the Rev ASR vs CORAAL transcript versions have the highest percentage of these differences. In contrast, Rev's ASR transcript version vs the Rev (+AA tag) transcript percentage is relatively low. Particularly confusing is that the percentage of morpho-syntactic differences between the individual Rev (ASR and both human) versions is nearly the same as the percentage between each Rev version vs the CORAAL version. Because the CORAAL version was produced by linguists who are familiar with AAE and its morpho-syntactic features, we expect that the CORAAL transcript will typically be a more accurate representation of the speech in the audio. We believe that Rev transcribers and the ASR model may have more standardizing transcription proclivities that are causing these differences. We consider whether the style guide used by Rev transcribers could explain this in the following section.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">In this work, we investigated how and the extent to which reference transcripts of the same audio can differ, especially on underrepresented speech. We collected 6 transcript versions, 4 human- and 2 ASR-produced, of the same 10 hours of CORAAL. We found that the human-produced transcripts could vary by WERs as low as <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.p1.1.m1.1a"><mo id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\sim</annotation></semantics></math>10% and as high as <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.p1.2.m2.1a"><mo id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><csymbol cd="latexml" id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">\sim</annotation></semantics></math>20%, and that ASR WER performance could increase or decrease by 5% depending on the reference transcript. We also found the Rev human- and ASR-produced transcripts to be the most similar to one another. This makes sense because the transcribers were all trained on the same style guide and the ASR was trained on data from this same population of transcribers. We next examined three hypotheses about sources of stylistic differences, in order of most to least potentially misrepresentative: 1) morpho-syntactic differences between AAE and SAE, 2) reduction differences, and 3) verbatim vs non-verbatim differences. The verbatim hypothesis accounted for the greatest percentage of the transcript differences, and the morpho-syntatic hypothesis for the second most. The Rev transcripts for the most part had fewer verbatim differences than the other transcript version pairwise comparisons, but they interestingly had more morpho-syntactic differences.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We might attribute this to the Rev style guide<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://cf-public.rev.com/styleguide/transcription/Transcription+Style+Guide+v5.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cf-public.rev.com/styleguide/transcription/Transcription+Style+Guide+v5.pdf</a></span></span></span>, which instructs transcribers to ``use English grammar conventions while maintaining the integrity of what was spoken. We are unable to cover and address specific guidelines regarding grammar. We expect you to have prior knowledge of, or be able to research, American English grammar, capitalization, and punctuation guidelines." This is ambiguous with respect to non-standard language varieties. Many AAE features that we tested for are often taught to be ``ungrammatical" in schools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. At the same time, those more familiar with AAE might deem them necessary to ``maintaining the integrity of what was spoken." Rev might consider clarifying this part of the style guide for underrepresented language varieties, as well as augmenting the customer-facing definition of verbatim vs non-verbatim, or introducing a new transcription variety option. The inclusion of examples could help as well. Then a user wanting to have audio of a non-standard variety transcribed could choose whether their variety's morpho-syntactic features are transcribed with the standard variety's constructions or not (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> makes a similar proposal for machine translation). Of course, greater awareness and education about underrepresented varieties would also help with this.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">With this work, we add to the ever more important research into bias in machine learning. We give more insight to similar discrepancies found by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and identify key categories of errors. Moreover, we come to the conclusion that a single reference transcript may not be sufficient to conclusively make claims about performance. Our findings indicate that different transcript versions may highlight distinct, yet equally valid, variations (e.g. verbatim vs non-verbatim) that must be considered for fair evaluation. We hope that by making our transcript versions and code available, we assist other research in addressing the important impact of human variation and bias.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Bucholtz, ``The politics of transcription,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Pragmatics</em>, vol. 32, no. 10, pp. 1439–1465, 2000.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Kendall and C. Farrington, ``The corpus of regional African American Language,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Version 2023.06</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Lanehart, <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">The Oxford Handbook of African American Language</em>.   Oxford University Press, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. D. Shriberg and G. L. Lof, ``Reliability studies in broad and narrow phonetic transcription,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Clinical Linguistics &amp; Phonetics</em>, vol. 5, no. 3, pp. 225–279, 1991.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. R. Patterson, N. C. Neupauer, P. A. Burant, S. C. Koehn, and A. T. Reed, ``A preliminary examination of conversation analytic techniques: Rates of inter-transcriber reliability,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Western Journal of Communication (includes Communication Reports)</em>, vol. 60, no. 1, pp. 76–91, 1996.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Loakes, ``Does automatic speech recognition (ASR) have a role in the transcription of indistinct covert recordings for forensic purposes?'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Frontiers in Communication</em>, vol. 7, p. 803452, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Love and D. Wright, ``Specifying challenges in transcribing covert recordings: Implications for forensic transcription,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Frontiers in Communication</em>, vol. 6, p. 797448, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Del Río, C. Miller, J. Profant, J. Drexler-Fox, Q. McNamara, N. Bhandari, N. Delworth, I. Pirkin, M. Jetté, S. Chandra <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Accents in speech recognition through the lens of a world englishes evaluation set,'' <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Research in Language</em>, pp. 225–244, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Jones, J. R. Kalbfeld, R. Hancock, and R. Clark, ``Testifying while black: An experimental study of court reporter accuracy in transcription of African American English,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Language</em>, vol. 95, no. 2, pp. e216–e252, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Koenecke, A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengesha, C. Toups, J. R. Rickford, D. Jurafsky, and S. Goel, ``Racial disparities in automated speech recognition,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, vol. 117, no. 14, pp. 7684–7689, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. B. Wassink, C. Gansen, and I. Bartholomew, ``Uneven success: automatic speech recognition and ethnicity-related dialects,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Speech Communication</em>, vol. 140, pp. 50–70, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. D. Fox and N. Delworth, ``Improving contextual recognition of rare words with an alternate spelling prediction model,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.01250</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Del Rio, N. Delworth, R. Westerman, M. Huang, N. Bhandari, J. Palakapilly, Q. McNamara, J. Dong, P. Zelasko, and M. Jetté, ``Earnings-21: A practical benchmark for asr in the wild,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.11348</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. R. Rickford, <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">African American Vernacular English</em>.   Blackwell Publishers, 1999, ch. Phonological and Grammatical Features of African American Vernacular English (AAVE).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. K. Spears, ``Rickford’s list of African American English grammatical features: an update,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">The Routledge companion to the work of John R. Rickford</em>.   Routledge, 2019, pp. 79–89.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Davydova and K. Hazen, ``The role of linguistic structure in the perceptions of vernacular speech: Evidence from L1 English and English as a foreign language,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">English World-Wide</em>, vol. 42, no. 3, pp. 273–298, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Harrington, R. W. Rhodes, and V. Hughes, ``Style variability in disfluency analysis for forensic speaker comparison,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Journal of Speech, Language and the Law</em>, pp. 31–58, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Fogel and L. C. Ehri, ``Teaching African American English forms to standard American English-speaking teachers: Effects on acquisition, attitudes, and responses to student use,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Journal of Teacher Education</em>, vol. 57, no. 5, pp. 464–480, 2006.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Miller, D. Silverman, V. Jurica, E. Richerson, R. Morris, and E. Mallard, ``Embedding register-aware MT into the CAT workflow,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)</em>, J. Campbell, A. Yanishevsky, J. Doyon, and D. Jones, Eds.   Boston, MA: Association for Machine Translation in the Americas, Mar. 2018, pp. 275–282.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.03057" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.03059" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.03059">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.03059" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.03060" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:17:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
