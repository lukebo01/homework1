<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.04051] FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title><meta property="og:description" content="This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multil…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.04051">

<!--Generated on Mon Aug  5 16:52:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tongyi SpeechTeam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Alibaba Group
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at <a target="_blank" href="https://fun-audio-llm.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fun-audio-llm.github.io</a>, and the code can be accessed at <a target="_blank" href="https://github.com/FunAudioLLM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FunAudioLLM</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the advancement in artificial intelligence (AI) has dramatically transformed how humans interact with machines, such as GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> and Gemini-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a href="#bib.bib43" title="" class="ltx_ref">2024</a>)</cite> and so on <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a href="#bib.bib3" title="" class="ltx_ref">2023b</a>; Chu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. This transformation is particularly evident in the realm of voice processing, where capabilities such as high-precision speech recognition <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite>, emotion recognition <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib33" title="" class="ltx_ref">2024b</a>)</cite>, and voice generation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>; Du et al., <a href="#bib.bib14" title="" class="ltx_ref">2024a</a>)</cite> are paving the way for more intuitive and human-like interactions. In this report, we introduce FunAudioLLM, an innovative framework designed to facilitate natural voice interactions between humans and large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>; Bai et al., <a href="#bib.bib2" title="" class="ltx_ref">2023a</a>; Touvron et al., <a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite>. At the core of FunAudioLLM are our two groundbreaking models: SenseVoice, for voice understanding, and CosyVoice, for voice generation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">SenseVoice is our state-of-the-art voice understanding model, which excels in multiple domains of voice processing. We offer both SenseVoice-Small and SenseVoice-Large variants. We have open-sourced SenseVoice-Small, which supports multilingual recognition in Chinese, English, Cantonese, Japanese, and Korean, delivering extremely low inference latency by employing a non-autoregressive end-to-end architecture. This design choice results in a performance that is more than 5 times faster than Whisper-small and more than 15 times faster than Whisper-large <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite>. On the other hand, SenseVoice-Large supports speech recognition in over 50 languages, with significant advantages in recognizing Chinese and Cantonese. In addition to speech recognition, SenseVoice offers state-of-the-art capabilities in emotion recognition and audio event detection <cite class="ltx_cite ltx_citemacro_citep">(Mesaros et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, making it an ideal choice for creating low-latency, human-like voice interaction systems.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our suite of applications is further enriched by CosyVoice <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib14" title="" class="ltx_ref">2024a</a>)</cite>, a family of fundamental speech generation models designed to produce natural-sounding voices for a variety of contexts. CosyVoice excels in generating multi-lingual voices tailored to specific speakers, zero-shot adaptation to new speakers <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>)</cite>, cross-lingual voice cloning <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2023</a>)</cite>, creating emotionally resonant voices <cite class="ltx_cite ltx_citemacro_citep">(Shin et al., <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>, and offering nuanced control over speech output through instructional text <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a href="#bib.bib24" title="" class="ltx_ref">2024</a>)</cite>. CosyVoice supports five languages: Chinese, English, Japanese, Cantonese, and Korean.
CosyVoice comes in three open-source models: CosyVoice-base-300M, which specializes in accurately representing speaker identity, zero-shot learning, and cross-lingual voice cloning; CosyVoice-instruct-300M, which focuses on generating emotionally expressive voices and allows for meticulous adjustments via instructional text, extending its capabilities to controllability over various aspects such as speaker identity <cite class="ltx_cite ltx_citemacro_citep">(Shimizu et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, speaking style <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a href="#bib.bib24" title="" class="ltx_ref">2024</a>)</cite>, and fine-grained paralinguistic features <cite class="ltx_cite ltx_citemacro_citep">(Kanda et al., <a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>; and CosyVoice-sft-300M, which has been fine-tuned on seven multilingual speakers and is ready for immediate deployment.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">By integrating SenseVoice, CosyVoice, and LLMs like Qwen <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>, FunAudioLLM offers a range of rich application demos. These include Speech-to-Speech Translation <cite class="ltx_cite ltx_citemacro_citep">(Berard et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, which allows users to speak in foreign languages using their own voice; Emotional Voice Chat <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a href="#bib.bib56" title="" class="ltx_ref">2024</a>)</cite>, which enables the model to understand and respond to emotions for more human-like interactions; Interactive Podcast <cite class="ltx_cite ltx_citemacro_citep">(Laban et al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, wherein users can engage in live discussions with multiple large models;
and AudioBook <cite class="ltx_cite ltx_citemacro_citep">(Chalamandaris et al., <a href="#bib.bib8" title="" class="ltx_ref">2014</a>)</cite>, allowing the model to perform expressive, multi-character narration for audiobooks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Overall, FunAudioLLM leverages the strengths of SenseVoice and CosyVoice to push the boundaries of voice interaction technology, enabling more natural and seamless communication between humans and large language models.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>FunAudioLLM Models</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of FunAudioLLM</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.04051/assets/img/overview-funaudiollm.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of our FunAudioLLM models for voice understanding and generation.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">FunAudioLLM consists of two foundation models for voice understanding and generation, named SenseVoice and CosyVoice, respectively. SenseVoice supports multi-lingual speech recognition, which is trained on over 300k hours. Specifically, SenseVoice-Small is efficient in inference, in which the recognition latency is less than 80ms and is more than 5 and 15 times faster than Whisper-Small and Whisper-large, respectively, and SenseVoice-Large supports high-precision ASR for over 50 languages. Furthermore, SenseVoice supports rich transcription, including state-of-the-art emotion recognition, audio event detection, inverse text normalization <cite class="ltx_cite ltx_citemacro_citep">(Pusateri et al., <a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite> and punctuation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Our voice generation model, CosyVoice, can generate multi-lingual speeches, which is trained on over 170k hours and five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). CosyVoice generated samples can achieve a WER of less 2% and speaker similarity of over 75%, which achieves the quality level of human parity.
CosyVoice supports zero-shot in-context learning, which enables voice cloning with a prompt speech of even 3 seconds. The timbre, emotion, prosody and style can be reproduced within or cross languages. We also released an instruction model, which can control speaker identity, speaking style (e.g., emotion) and other fine-grained paralinguistic features with natural textural instructions. An overview of FunAudioLLM models is shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview of FunAudioLLM ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Voice Understanding Model: SenseVoice</h3>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.04051/assets/img/overview-sensevoice.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>SenseVoice is a comprehensive speech foundation model designed to perform various speech understanding tasks, including Automatic Speech Recognition (ASR), Language Identification (LID), Speech Emotion Recognition (SER), and Audio Event Detection (AED). SenseVoice-Small [Top]: An encoder-only model optimized for rapid speech understanding. It offers high-speed processing while supporting 5 languages. SenseVoice-Large [Bottom]: An encoder-decoder model aimed at achieving more precise speech understanding across a broader range of languages. It excels in accuracy and supports an extensive set of language capabilities.</figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">SenseVoice is a speech foundation model with multiple voice understanding capabilities, including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event classification (AEC) or audio event detection (AED). Two models with different sizes and architectures are proposed to suit different requirements: SenseVoice-Small, an encoder-only speech foundation model for rapid speech understanding, and SenseVoice-Large, an encoder-decoder <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> speech foundation model for more accurate speech understanding with more languages supported, as illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Voice Understanding Model: SenseVoice ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.3" class="ltx_p"><span id="S2.SS2.p2.3.1" class="ltx_text ltx_font_bold">SenseVoice-Small</span> is a non-autoregressive encoder-only model for multi-lingual multi-style ASR and multiple speech understanding tasks. Given the input waveform, we first compute the 80-dimensional log-mel filter-bank, and then stack consecutive frames and down-sample them by a factor of 6. The extracted feature is then mapped to the dimension <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">D</annotation></semantics></math> of the encoder, denoted as <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="{\rm\bf X}_{\rm speech}\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mrow id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><msub id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2.2" xref="S2.SS2.p2.2.m2.1.1.2.2.cmml">𝐗</mi><mi id="S2.SS2.p2.2.m2.1.1.2.3" xref="S2.SS2.p2.2.m2.1.1.2.3.cmml">speech</mi></msub><mo id="S2.SS2.p2.2.m2.1.1.1" xref="S2.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml"><mi id="S2.SS2.p2.2.m2.1.1.3.2" xref="S2.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS2.p2.2.m2.1.1.3.3" xref="S2.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S2.SS2.p2.2.m2.1.1.3.3.2" xref="S2.SS2.p2.2.m2.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p2.2.m2.1.1.3.3.1" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.3" xref="S2.SS2.p2.2.m2.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><in id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1.1"></in><apply id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2.2">𝐗</ci><ci id="S2.SS2.p2.2.m2.1.1.2.3.cmml" xref="S2.SS2.p2.2.m2.1.1.2.3">speech</ci></apply><apply id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.3.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.3.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S2.SS2.p2.2.m2.1.1.3.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3"><times id="S2.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S2.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.2">𝑇</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">{\rm\bf X}_{\rm speech}\in\mathbb{R}^{T\times D}</annotation></semantics></math>, where <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">T</annotation></semantics></math> is the length of the down-sampled feature. The encoder is implemented as a memory-equipped self-attention network (SAN-M) <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>. To specify the task, we prepend four embeddings to the speech feature as the input to the encoder:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.5" class="ltx_Math" alttext="{\rm\bf X}={\rm concat}({\rm\bf e}_{\rm LID},{\rm\bf e}_{\rm SER},{\rm\bf e}_{\rm AEC},{\rm\bf e}_{\rm ITN/NoITN},{\rm X}_{\rm speech})" display="block"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mi id="S2.E1.m1.5.5.7" xref="S2.E1.m1.5.5.7.cmml">𝐗</mi><mo id="S2.E1.m1.5.5.6" xref="S2.E1.m1.5.5.6.cmml">=</mo><mrow id="S2.E1.m1.5.5.5" xref="S2.E1.m1.5.5.5.cmml"><mi id="S2.E1.m1.5.5.5.7" xref="S2.E1.m1.5.5.5.7.cmml">concat</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.5.6" xref="S2.E1.m1.5.5.5.6.cmml">​</mo><mrow id="S2.E1.m1.5.5.5.5.5" xref="S2.E1.m1.5.5.5.5.6.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.5.5.5.6" xref="S2.E1.m1.5.5.5.5.6.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">𝐞</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">LID</mi></msub><mo id="S2.E1.m1.5.5.5.5.5.7" xref="S2.E1.m1.5.5.5.5.6.cmml">,</mo><msub id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.2.cmml">𝐞</mi><mi id="S2.E1.m1.2.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.2.3.cmml">SER</mi></msub><mo id="S2.E1.m1.5.5.5.5.5.8" xref="S2.E1.m1.5.5.5.5.6.cmml">,</mo><msub id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.3.3.2.cmml">𝐞</mi><mi id="S2.E1.m1.3.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.3.cmml">AEC</mi></msub><mo id="S2.E1.m1.5.5.5.5.5.9" xref="S2.E1.m1.5.5.5.5.6.cmml">,</mo><msub id="S2.E1.m1.4.4.4.4.4.4" xref="S2.E1.m1.4.4.4.4.4.4.cmml"><mi id="S2.E1.m1.4.4.4.4.4.4.2" xref="S2.E1.m1.4.4.4.4.4.4.2.cmml">𝐞</mi><mrow id="S2.E1.m1.4.4.4.4.4.4.3" xref="S2.E1.m1.4.4.4.4.4.4.3.cmml"><mi id="S2.E1.m1.4.4.4.4.4.4.3.2" xref="S2.E1.m1.4.4.4.4.4.4.3.2.cmml">ITN</mi><mo id="S2.E1.m1.4.4.4.4.4.4.3.1" xref="S2.E1.m1.4.4.4.4.4.4.3.1.cmml">/</mo><mi id="S2.E1.m1.4.4.4.4.4.4.3.3" xref="S2.E1.m1.4.4.4.4.4.4.3.3.cmml">NoITN</mi></mrow></msub><mo id="S2.E1.m1.5.5.5.5.5.10" xref="S2.E1.m1.5.5.5.5.6.cmml">,</mo><msub id="S2.E1.m1.5.5.5.5.5.5" xref="S2.E1.m1.5.5.5.5.5.5.cmml"><mi mathvariant="normal" id="S2.E1.m1.5.5.5.5.5.5.2" xref="S2.E1.m1.5.5.5.5.5.5.2.cmml">X</mi><mi id="S2.E1.m1.5.5.5.5.5.5.3" xref="S2.E1.m1.5.5.5.5.5.5.3.cmml">speech</mi></msub><mo stretchy="false" id="S2.E1.m1.5.5.5.5.5.11" xref="S2.E1.m1.5.5.5.5.6.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.6.cmml" xref="S2.E1.m1.5.5.6"></eq><ci id="S2.E1.m1.5.5.7.cmml" xref="S2.E1.m1.5.5.7">𝐗</ci><apply id="S2.E1.m1.5.5.5.cmml" xref="S2.E1.m1.5.5.5"><times id="S2.E1.m1.5.5.5.6.cmml" xref="S2.E1.m1.5.5.5.6"></times><ci id="S2.E1.m1.5.5.5.7.cmml" xref="S2.E1.m1.5.5.5.7">concat</ci><vector id="S2.E1.m1.5.5.5.5.6.cmml" xref="S2.E1.m1.5.5.5.5.5"><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝐞</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">LID</ci></apply><apply id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.2">𝐞</ci><ci id="S2.E1.m1.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3">SER</ci></apply><apply id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.2">𝐞</ci><ci id="S2.E1.m1.3.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.3">AEC</ci></apply><apply id="S2.E1.m1.4.4.4.4.4.4.cmml" xref="S2.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.4.4.4.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4">subscript</csymbol><ci id="S2.E1.m1.4.4.4.4.4.4.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.2">𝐞</ci><apply id="S2.E1.m1.4.4.4.4.4.4.3.cmml" xref="S2.E1.m1.4.4.4.4.4.4.3"><divide id="S2.E1.m1.4.4.4.4.4.4.3.1.cmml" xref="S2.E1.m1.4.4.4.4.4.4.3.1"></divide><ci id="S2.E1.m1.4.4.4.4.4.4.3.2.cmml" xref="S2.E1.m1.4.4.4.4.4.4.3.2">ITN</ci><ci id="S2.E1.m1.4.4.4.4.4.4.3.3.cmml" xref="S2.E1.m1.4.4.4.4.4.4.3.3">NoITN</ci></apply></apply><apply id="S2.E1.m1.5.5.5.5.5.5.cmml" xref="S2.E1.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.5.5.5.1.cmml" xref="S2.E1.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S2.E1.m1.5.5.5.5.5.5.2.cmml" xref="S2.E1.m1.5.5.5.5.5.5.2">X</ci><ci id="S2.E1.m1.5.5.5.5.5.5.3.cmml" xref="S2.E1.m1.5.5.5.5.5.5.3">speech</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">{\rm\bf X}={\rm concat}({\rm\bf e}_{\rm LID},{\rm\bf e}_{\rm SER},{\rm\bf e}_{\rm AEC},{\rm\bf e}_{\rm ITN/NoITN},{\rm X}_{\rm speech})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="{\rm\bf P}={\rm Softmax}({\rm Linear}_{D\to|V^{\prime}|}({\rm Encoder}({\rm\bf X})))" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mi id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml">𝐏</mi><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mi id="S2.E2.m1.3.3.1.3" xref="S2.E2.m1.3.3.1.3.cmml">Softmax</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">Linear</mi><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml">D</mi><mo stretchy="false" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">→</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.1.cmml">|</mo><msup id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml">V</mi><mo id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">Encoder</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">𝐗</mi><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><ci id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3">𝐏</ci><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><times id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1.2"></times><ci id="S2.E2.m1.3.3.1.3.cmml" xref="S2.E2.m1.3.3.1.3">Softmax</ci><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">Linear</ci><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><ci id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2">→</ci><ci id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3">𝐷</ci><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1"><abs id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.2"></abs><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">′</ci></apply></apply></apply></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">Encoder</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">{\rm\bf P}={\rm Softmax}({\rm Linear}_{D\to|V^{\prime}|}({\rm Encoder}({\rm\bf X})))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.10" class="ltx_p"><math id="S2.SS2.p2.4.m1.1" class="ltx_Math" alttext="{\rm\bf X}\in\mathbb{R}^{(T+4)\times D}" display="inline"><semantics id="S2.SS2.p2.4.m1.1a"><mrow id="S2.SS2.p2.4.m1.1.2" xref="S2.SS2.p2.4.m1.1.2.cmml"><mi id="S2.SS2.p2.4.m1.1.2.2" xref="S2.SS2.p2.4.m1.1.2.2.cmml">𝐗</mi><mo id="S2.SS2.p2.4.m1.1.2.1" xref="S2.SS2.p2.4.m1.1.2.1.cmml">∈</mo><msup id="S2.SS2.p2.4.m1.1.2.3" xref="S2.SS2.p2.4.m1.1.2.3.cmml"><mi id="S2.SS2.p2.4.m1.1.2.3.2" xref="S2.SS2.p2.4.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S2.SS2.p2.4.m1.1.1.1" xref="S2.SS2.p2.4.m1.1.1.1.cmml"><mrow id="S2.SS2.p2.4.m1.1.1.1.1.1" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p2.4.m1.1.1.1.1.1.2" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.4.m1.1.1.1.1.1.1" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS2.p2.4.m1.1.1.1.1.1.1.2" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S2.SS2.p2.4.m1.1.1.1.1.1.1.1" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS2.p2.4.m1.1.1.1.1.1.1.3" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.3.cmml">4</mn></mrow><mo rspace="0.055em" stretchy="false" id="S2.SS2.p2.4.m1.1.1.1.1.1.3" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S2.SS2.p2.4.m1.1.1.1.2" xref="S2.SS2.p2.4.m1.1.1.1.2.cmml">×</mo><mi id="S2.SS2.p2.4.m1.1.1.1.3" xref="S2.SS2.p2.4.m1.1.1.1.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m1.1b"><apply id="S2.SS2.p2.4.m1.1.2.cmml" xref="S2.SS2.p2.4.m1.1.2"><in id="S2.SS2.p2.4.m1.1.2.1.cmml" xref="S2.SS2.p2.4.m1.1.2.1"></in><ci id="S2.SS2.p2.4.m1.1.2.2.cmml" xref="S2.SS2.p2.4.m1.1.2.2">𝐗</ci><apply id="S2.SS2.p2.4.m1.1.2.3.cmml" xref="S2.SS2.p2.4.m1.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.p2.4.m1.1.2.3.1.cmml" xref="S2.SS2.p2.4.m1.1.2.3">superscript</csymbol><ci id="S2.SS2.p2.4.m1.1.2.3.2.cmml" xref="S2.SS2.p2.4.m1.1.2.3.2">ℝ</ci><apply id="S2.SS2.p2.4.m1.1.1.1.cmml" xref="S2.SS2.p2.4.m1.1.1.1"><times id="S2.SS2.p2.4.m1.1.1.1.2.cmml" xref="S2.SS2.p2.4.m1.1.1.1.2"></times><apply id="S2.SS2.p2.4.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.4.m1.1.1.1.1.1"><plus id="S2.SS2.p2.4.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.1"></plus><ci id="S2.SS2.p2.4.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.2">𝑇</ci><cn type="integer" id="S2.SS2.p2.4.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.p2.4.m1.1.1.1.1.1.1.3">4</cn></apply><ci id="S2.SS2.p2.4.m1.1.1.1.3.cmml" xref="S2.SS2.p2.4.m1.1.1.1.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m1.1c">{\rm\bf X}\in\mathbb{R}^{(T+4)\times D}</annotation></semantics></math> and <math id="S2.SS2.p2.5.m2.2" class="ltx_Math" alttext="{\rm\bf P}\in\mathbb{R}^{(T+4)\times|V^{\prime}|}" display="inline"><semantics id="S2.SS2.p2.5.m2.2a"><mrow id="S2.SS2.p2.5.m2.2.3" xref="S2.SS2.p2.5.m2.2.3.cmml"><mi id="S2.SS2.p2.5.m2.2.3.2" xref="S2.SS2.p2.5.m2.2.3.2.cmml">𝐏</mi><mo id="S2.SS2.p2.5.m2.2.3.1" xref="S2.SS2.p2.5.m2.2.3.1.cmml">∈</mo><msup id="S2.SS2.p2.5.m2.2.3.3" xref="S2.SS2.p2.5.m2.2.3.3.cmml"><mi id="S2.SS2.p2.5.m2.2.3.3.2" xref="S2.SS2.p2.5.m2.2.3.3.2.cmml">ℝ</mi><mrow id="S2.SS2.p2.5.m2.2.2.2" xref="S2.SS2.p2.5.m2.2.2.2.cmml"><mrow id="S2.SS2.p2.5.m2.1.1.1.1.1" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p2.5.m2.1.1.1.1.1.2" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.5.m2.1.1.1.1.1.1" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.cmml"><mi id="S2.SS2.p2.5.m2.1.1.1.1.1.1.2" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.2.cmml">T</mi><mo id="S2.SS2.p2.5.m2.1.1.1.1.1.1.1" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS2.p2.5.m2.1.1.1.1.1.1.3" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.3.cmml">4</mn></mrow><mo rspace="0.055em" stretchy="false" id="S2.SS2.p2.5.m2.1.1.1.1.1.3" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S2.SS2.p2.5.m2.2.2.2.3" xref="S2.SS2.p2.5.m2.2.2.2.3.cmml">×</mo><mrow id="S2.SS2.p2.5.m2.2.2.2.2.1" xref="S2.SS2.p2.5.m2.2.2.2.2.2.cmml"><mo stretchy="false" id="S2.SS2.p2.5.m2.2.2.2.2.1.2" xref="S2.SS2.p2.5.m2.2.2.2.2.2.1.cmml">|</mo><msup id="S2.SS2.p2.5.m2.2.2.2.2.1.1" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1.cmml"><mi id="S2.SS2.p2.5.m2.2.2.2.2.1.1.2" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1.2.cmml">V</mi><mo id="S2.SS2.p2.5.m2.2.2.2.2.1.1.3" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S2.SS2.p2.5.m2.2.2.2.2.1.3" xref="S2.SS2.p2.5.m2.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m2.2b"><apply id="S2.SS2.p2.5.m2.2.3.cmml" xref="S2.SS2.p2.5.m2.2.3"><in id="S2.SS2.p2.5.m2.2.3.1.cmml" xref="S2.SS2.p2.5.m2.2.3.1"></in><ci id="S2.SS2.p2.5.m2.2.3.2.cmml" xref="S2.SS2.p2.5.m2.2.3.2">𝐏</ci><apply id="S2.SS2.p2.5.m2.2.3.3.cmml" xref="S2.SS2.p2.5.m2.2.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m2.2.3.3.1.cmml" xref="S2.SS2.p2.5.m2.2.3.3">superscript</csymbol><ci id="S2.SS2.p2.5.m2.2.3.3.2.cmml" xref="S2.SS2.p2.5.m2.2.3.3.2">ℝ</ci><apply id="S2.SS2.p2.5.m2.2.2.2.cmml" xref="S2.SS2.p2.5.m2.2.2.2"><times id="S2.SS2.p2.5.m2.2.2.2.3.cmml" xref="S2.SS2.p2.5.m2.2.2.2.3"></times><apply id="S2.SS2.p2.5.m2.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.5.m2.1.1.1.1.1"><plus id="S2.SS2.p2.5.m2.1.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.1"></plus><ci id="S2.SS2.p2.5.m2.1.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.2">𝑇</ci><cn type="integer" id="S2.SS2.p2.5.m2.1.1.1.1.1.1.3.cmml" xref="S2.SS2.p2.5.m2.1.1.1.1.1.1.3">4</cn></apply><apply id="S2.SS2.p2.5.m2.2.2.2.2.2.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1"><abs id="S2.SS2.p2.5.m2.2.2.2.2.2.1.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1.2"></abs><apply id="S2.SS2.p2.5.m2.2.2.2.2.1.1.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m2.2.2.2.2.1.1.1.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1">superscript</csymbol><ci id="S2.SS2.p2.5.m2.2.2.2.2.1.1.2.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1.2">𝑉</ci><ci id="S2.SS2.p2.5.m2.2.2.2.2.1.1.3.cmml" xref="S2.SS2.p2.5.m2.2.2.2.2.1.1.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m2.2c">{\rm\bf P}\in\mathbb{R}^{(T+4)\times|V^{\prime}|}</annotation></semantics></math>. <math id="S2.SS2.p2.6.m3.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S2.SS2.p2.6.m3.1a"><msup id="S2.SS2.p2.6.m3.1.1" xref="S2.SS2.p2.6.m3.1.1.cmml"><mi id="S2.SS2.p2.6.m3.1.1.2" xref="S2.SS2.p2.6.m3.1.1.2.cmml">V</mi><mo id="S2.SS2.p2.6.m3.1.1.3" xref="S2.SS2.p2.6.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m3.1b"><apply id="S2.SS2.p2.6.m3.1.1.cmml" xref="S2.SS2.p2.6.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m3.1.1.1.cmml" xref="S2.SS2.p2.6.m3.1.1">superscript</csymbol><ci id="S2.SS2.p2.6.m3.1.1.2.cmml" xref="S2.SS2.p2.6.m3.1.1.2">𝑉</ci><ci id="S2.SS2.p2.6.m3.1.1.3.cmml" xref="S2.SS2.p2.6.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m3.1c">V^{\prime}</annotation></semantics></math> is the vocabulary including tokens for ASR and other tasks. <math id="S2.SS2.p2.7.m4.1" class="ltx_Math" alttext="{\rm\bf e}_{\rm LID}" display="inline"><semantics id="S2.SS2.p2.7.m4.1a"><msub id="S2.SS2.p2.7.m4.1.1" xref="S2.SS2.p2.7.m4.1.1.cmml"><mi id="S2.SS2.p2.7.m4.1.1.2" xref="S2.SS2.p2.7.m4.1.1.2.cmml">𝐞</mi><mi id="S2.SS2.p2.7.m4.1.1.3" xref="S2.SS2.p2.7.m4.1.1.3.cmml">LID</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m4.1b"><apply id="S2.SS2.p2.7.m4.1.1.cmml" xref="S2.SS2.p2.7.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m4.1.1.1.cmml" xref="S2.SS2.p2.7.m4.1.1">subscript</csymbol><ci id="S2.SS2.p2.7.m4.1.1.2.cmml" xref="S2.SS2.p2.7.m4.1.1.2">𝐞</ci><ci id="S2.SS2.p2.7.m4.1.1.3.cmml" xref="S2.SS2.p2.7.m4.1.1.3">LID</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m4.1c">{\rm\bf e}_{\rm LID}</annotation></semantics></math>, <math id="S2.SS2.p2.8.m5.1" class="ltx_Math" alttext="{\rm\bf e}_{\rm SER}" display="inline"><semantics id="S2.SS2.p2.8.m5.1a"><msub id="S2.SS2.p2.8.m5.1.1" xref="S2.SS2.p2.8.m5.1.1.cmml"><mi id="S2.SS2.p2.8.m5.1.1.2" xref="S2.SS2.p2.8.m5.1.1.2.cmml">𝐞</mi><mi id="S2.SS2.p2.8.m5.1.1.3" xref="S2.SS2.p2.8.m5.1.1.3.cmml">SER</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m5.1b"><apply id="S2.SS2.p2.8.m5.1.1.cmml" xref="S2.SS2.p2.8.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m5.1.1.1.cmml" xref="S2.SS2.p2.8.m5.1.1">subscript</csymbol><ci id="S2.SS2.p2.8.m5.1.1.2.cmml" xref="S2.SS2.p2.8.m5.1.1.2">𝐞</ci><ci id="S2.SS2.p2.8.m5.1.1.3.cmml" xref="S2.SS2.p2.8.m5.1.1.3">SER</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m5.1c">{\rm\bf e}_{\rm SER}</annotation></semantics></math>, <math id="S2.SS2.p2.9.m6.1" class="ltx_Math" alttext="{\rm\bf e}_{\rm AEC}" display="inline"><semantics id="S2.SS2.p2.9.m6.1a"><msub id="S2.SS2.p2.9.m6.1.1" xref="S2.SS2.p2.9.m6.1.1.cmml"><mi id="S2.SS2.p2.9.m6.1.1.2" xref="S2.SS2.p2.9.m6.1.1.2.cmml">𝐞</mi><mi id="S2.SS2.p2.9.m6.1.1.3" xref="S2.SS2.p2.9.m6.1.1.3.cmml">AEC</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m6.1b"><apply id="S2.SS2.p2.9.m6.1.1.cmml" xref="S2.SS2.p2.9.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.9.m6.1.1.1.cmml" xref="S2.SS2.p2.9.m6.1.1">subscript</csymbol><ci id="S2.SS2.p2.9.m6.1.1.2.cmml" xref="S2.SS2.p2.9.m6.1.1.2">𝐞</ci><ci id="S2.SS2.p2.9.m6.1.1.3.cmml" xref="S2.SS2.p2.9.m6.1.1.3">AEC</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m6.1c">{\rm\bf e}_{\rm AEC}</annotation></semantics></math>, <math id="S2.SS2.p2.10.m7.1" class="ltx_Math" alttext="{\rm\bf e}_{\rm ITN/NoITN}" display="inline"><semantics id="S2.SS2.p2.10.m7.1a"><msub id="S2.SS2.p2.10.m7.1.1" xref="S2.SS2.p2.10.m7.1.1.cmml"><mi id="S2.SS2.p2.10.m7.1.1.2" xref="S2.SS2.p2.10.m7.1.1.2.cmml">𝐞</mi><mrow id="S2.SS2.p2.10.m7.1.1.3" xref="S2.SS2.p2.10.m7.1.1.3.cmml"><mi id="S2.SS2.p2.10.m7.1.1.3.2" xref="S2.SS2.p2.10.m7.1.1.3.2.cmml">ITN</mi><mo id="S2.SS2.p2.10.m7.1.1.3.1" xref="S2.SS2.p2.10.m7.1.1.3.1.cmml">/</mo><mi id="S2.SS2.p2.10.m7.1.1.3.3" xref="S2.SS2.p2.10.m7.1.1.3.3.cmml">NoITN</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m7.1b"><apply id="S2.SS2.p2.10.m7.1.1.cmml" xref="S2.SS2.p2.10.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.10.m7.1.1.1.cmml" xref="S2.SS2.p2.10.m7.1.1">subscript</csymbol><ci id="S2.SS2.p2.10.m7.1.1.2.cmml" xref="S2.SS2.p2.10.m7.1.1.2">𝐞</ci><apply id="S2.SS2.p2.10.m7.1.1.3.cmml" xref="S2.SS2.p2.10.m7.1.1.3"><divide id="S2.SS2.p2.10.m7.1.1.3.1.cmml" xref="S2.SS2.p2.10.m7.1.1.3.1"></divide><ci id="S2.SS2.p2.10.m7.1.1.3.2.cmml" xref="S2.SS2.p2.10.m7.1.1.3.2">ITN</ci><ci id="S2.SS2.p2.10.m7.1.1.3.3.cmml" xref="S2.SS2.p2.10.m7.1.1.3.3">NoITN</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m7.1c">{\rm\bf e}_{\rm ITN/NoITN}</annotation></semantics></math> are embeddings of four special tokens:</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.3" class="ltx_p"><math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\langle{\rm LID}\rangle" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.2.2" xref="S2.SS2.p3.1.m1.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p3.1.m1.1.2.2.1" xref="S2.SS2.p3.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">LID</mi><mo stretchy="false" id="S2.SS2.p3.1.m1.1.2.2.2" xref="S2.SS2.p3.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.2.1.cmml" xref="S2.SS2.p3.1.m1.1.2.2"><csymbol cd="latexml" id="S2.SS2.p3.1.m1.1.2.1.1.cmml" xref="S2.SS2.p3.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">LID</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\langle{\rm LID}\rangle</annotation></semantics></math> indicates the LID task. If <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="\langle{\rm LID}\rangle" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.2.2" xref="S2.SS2.p3.2.m2.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p3.2.m2.1.2.2.1" xref="S2.SS2.p3.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">LID</mi><mo stretchy="false" id="S2.SS2.p3.2.m2.1.2.2.2" xref="S2.SS2.p3.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.2.1.cmml" xref="S2.SS2.p3.2.m2.1.2.2"><csymbol cd="latexml" id="S2.SS2.p3.2.m2.1.2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">LID</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\langle{\rm LID}\rangle</annotation></semantics></math> is prepended, the model is trained to predict the language token, at the corresponding position of the output.
In the training stage, we randomly replace <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="\langle{\rm LID}\rangle" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mrow id="S2.SS2.p3.3.m3.1.2.2" xref="S2.SS2.p3.3.m3.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p3.3.m3.1.2.2.1" xref="S2.SS2.p3.3.m3.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml">LID</mi><mo stretchy="false" id="S2.SS2.p3.3.m3.1.2.2.2" xref="S2.SS2.p3.3.m3.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.2.1.cmml" xref="S2.SS2.p3.3.m3.1.2.2"><csymbol cd="latexml" id="S2.SS2.p3.3.m3.1.2.1.1.cmml" xref="S2.SS2.p3.3.m3.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">LID</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">\langle{\rm LID}\rangle</annotation></semantics></math> with the ground truth language token according to probability 0.8 so that the model can either predict the language token, or be configured with a specified language token in the inference stage.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.2" class="ltx_p"><math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="\langle{\rm SER}\rangle" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mrow id="S2.SS2.p4.1.m1.1.2.2" xref="S2.SS2.p4.1.m1.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p4.1.m1.1.2.2.1" xref="S2.SS2.p4.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">SER</mi><mo stretchy="false" id="S2.SS2.p4.1.m1.1.2.2.2" xref="S2.SS2.p4.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.2.1.cmml" xref="S2.SS2.p4.1.m1.1.2.2"><csymbol cd="latexml" id="S2.SS2.p4.1.m1.1.2.1.1.cmml" xref="S2.SS2.p4.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">SER</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\langle{\rm SER}\rangle</annotation></semantics></math> indicates the SER task. If <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="\langle{\rm SER}\rangle" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mrow id="S2.SS2.p4.2.m2.1.2.2" xref="S2.SS2.p4.2.m2.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p4.2.m2.1.2.2.1" xref="S2.SS2.p4.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">SER</mi><mo stretchy="false" id="S2.SS2.p4.2.m2.1.2.2.2" xref="S2.SS2.p4.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.2.1.cmml" xref="S2.SS2.p4.2.m2.1.2.2"><csymbol cd="latexml" id="S2.SS2.p4.2.m2.1.2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">SER</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">\langle{\rm SER}\rangle</annotation></semantics></math> is prepended, the model is trained to predict the speech emotion label, at the corresponding position of the output.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.2" class="ltx_p"><math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="\langle{\rm AEC}\rangle" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.2.2" xref="S2.SS2.p5.1.m1.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p5.1.m1.1.2.2.1" xref="S2.SS2.p5.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">AEC</mi><mo stretchy="false" id="S2.SS2.p5.1.m1.1.2.2.2" xref="S2.SS2.p5.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.2.1.cmml" xref="S2.SS2.p5.1.m1.1.2.2"><csymbol cd="latexml" id="S2.SS2.p5.1.m1.1.2.1.1.cmml" xref="S2.SS2.p5.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">AEC</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">\langle{\rm AEC}\rangle</annotation></semantics></math> indicates the AEC task. If <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="\langle{\rm AEC}\rangle" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.2.2" xref="S2.SS2.p5.2.m2.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p5.2.m2.1.2.2.1" xref="S2.SS2.p5.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml">AEC</mi><mo stretchy="false" id="S2.SS2.p5.2.m2.1.2.2.2" xref="S2.SS2.p5.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.2.2"><csymbol cd="latexml" id="S2.SS2.p5.2.m2.1.2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1">AEC</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">\langle{\rm AEC}\rangle</annotation></semantics></math> is prepended, the model is trained to predict the audio event label, at the corresponding position of the output.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.4" class="ltx_p"><math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="\langle{\rm ITN}\rangle" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><mrow id="S2.SS2.p6.1.m1.1.2.2" xref="S2.SS2.p6.1.m1.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p6.1.m1.1.2.2.1" xref="S2.SS2.p6.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml">ITN</mi><mo stretchy="false" id="S2.SS2.p6.1.m1.1.2.2.2" xref="S2.SS2.p6.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><apply id="S2.SS2.p6.1.m1.1.2.1.cmml" xref="S2.SS2.p6.1.m1.1.2.2"><csymbol cd="latexml" id="S2.SS2.p6.1.m1.1.2.1.1.cmml" xref="S2.SS2.p6.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1">ITN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">\langle{\rm ITN}\rangle</annotation></semantics></math> or <math id="S2.SS2.p6.2.m2.1" class="ltx_Math" alttext="\langle{\rm NoITN}\rangle" display="inline"><semantics id="S2.SS2.p6.2.m2.1a"><mrow id="S2.SS2.p6.2.m2.1.2.2" xref="S2.SS2.p6.2.m2.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p6.2.m2.1.2.2.1" xref="S2.SS2.p6.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p6.2.m2.1.1" xref="S2.SS2.p6.2.m2.1.1.cmml">NoITN</mi><mo stretchy="false" id="S2.SS2.p6.2.m2.1.2.2.2" xref="S2.SS2.p6.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.2.m2.1b"><apply id="S2.SS2.p6.2.m2.1.2.1.cmml" xref="S2.SS2.p6.2.m2.1.2.2"><csymbol cd="latexml" id="S2.SS2.p6.2.m2.1.2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p6.2.m2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1">NoITN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.2.m2.1c">\langle{\rm NoITN}\rangle</annotation></semantics></math> specify the transcription style. If <math id="S2.SS2.p6.3.m3.1" class="ltx_Math" alttext="\langle{\rm ITN}\rangle" display="inline"><semantics id="S2.SS2.p6.3.m3.1a"><mrow id="S2.SS2.p6.3.m3.1.2.2" xref="S2.SS2.p6.3.m3.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p6.3.m3.1.2.2.1" xref="S2.SS2.p6.3.m3.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p6.3.m3.1.1" xref="S2.SS2.p6.3.m3.1.1.cmml">ITN</mi><mo stretchy="false" id="S2.SS2.p6.3.m3.1.2.2.2" xref="S2.SS2.p6.3.m3.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.3.m3.1b"><apply id="S2.SS2.p6.3.m3.1.2.1.cmml" xref="S2.SS2.p6.3.m3.1.2.2"><csymbol cd="latexml" id="S2.SS2.p6.3.m3.1.2.1.1.cmml" xref="S2.SS2.p6.3.m3.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p6.3.m3.1.1.cmml" xref="S2.SS2.p6.3.m3.1.1">ITN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.3.m3.1c">\langle{\rm ITN}\rangle</annotation></semantics></math> is provided, the model is trained to transcript with inverse text normalization (ITN) and punctuation. If <math id="S2.SS2.p6.4.m4.1" class="ltx_Math" alttext="\langle{\rm NoITN}\rangle" display="inline"><semantics id="S2.SS2.p6.4.m4.1a"><mrow id="S2.SS2.p6.4.m4.1.2.2" xref="S2.SS2.p6.4.m4.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p6.4.m4.1.2.2.1" xref="S2.SS2.p6.4.m4.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p6.4.m4.1.1" xref="S2.SS2.p6.4.m4.1.1.cmml">NoITN</mi><mo stretchy="false" id="S2.SS2.p6.4.m4.1.2.2.2" xref="S2.SS2.p6.4.m4.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.4.m4.1b"><apply id="S2.SS2.p6.4.m4.1.2.1.cmml" xref="S2.SS2.p6.4.m4.1.2.2"><csymbol cd="latexml" id="S2.SS2.p6.4.m4.1.2.1.1.cmml" xref="S2.SS2.p6.4.m4.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p6.4.m4.1.1.cmml" xref="S2.SS2.p6.4.m4.1.1">NoITN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.4.m4.1c">\langle{\rm NoITN}\rangle</annotation></semantics></math> is provided, the model is trained to transcript without ITN and punctuation.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">In the training stage, the LID, SER, and AEC tasks are optimized using the cross-entropy loss. The ASR task is optimized using the CTC loss <cite class="ltx_cite ltx_citemacro_citep">(Graves et al., <a href="#bib.bib20" title="" class="ltx_ref">2006</a>)</cite>.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.3" class="ltx_p"><span id="S2.SS2.p8.3.1" class="ltx_text ltx_font_bold">SenseVoice-Large</span> is an autoregressive encoder-decoder model for multi-lingual ASR and multiple speech understanding tasks. Similar to Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite>, SenseVoice-Large specifies tasks by a sequence of input tokens to the decoder. Specifically, we specify whether to predict language, speech emotion, and audio events with timestamps by including <math id="S2.SS2.p8.1.m1.1" class="ltx_Math" alttext="\langle{\rm LID}\rangle" display="inline"><semantics id="S2.SS2.p8.1.m1.1a"><mrow id="S2.SS2.p8.1.m1.1.2.2" xref="S2.SS2.p8.1.m1.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p8.1.m1.1.2.2.1" xref="S2.SS2.p8.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml">LID</mi><mo stretchy="false" id="S2.SS2.p8.1.m1.1.2.2.2" xref="S2.SS2.p8.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><apply id="S2.SS2.p8.1.m1.1.2.1.cmml" xref="S2.SS2.p8.1.m1.1.2.2"><csymbol cd="latexml" id="S2.SS2.p8.1.m1.1.2.1.1.cmml" xref="S2.SS2.p8.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1">LID</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">\langle{\rm LID}\rangle</annotation></semantics></math>, <math id="S2.SS2.p8.2.m2.1" class="ltx_Math" alttext="\langle{\rm SER}\rangle" display="inline"><semantics id="S2.SS2.p8.2.m2.1a"><mrow id="S2.SS2.p8.2.m2.1.2.2" xref="S2.SS2.p8.2.m2.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p8.2.m2.1.2.2.1" xref="S2.SS2.p8.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p8.2.m2.1.1" xref="S2.SS2.p8.2.m2.1.1.cmml">SER</mi><mo stretchy="false" id="S2.SS2.p8.2.m2.1.2.2.2" xref="S2.SS2.p8.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.2.m2.1b"><apply id="S2.SS2.p8.2.m2.1.2.1.cmml" xref="S2.SS2.p8.2.m2.1.2.2"><csymbol cd="latexml" id="S2.SS2.p8.2.m2.1.2.1.1.cmml" xref="S2.SS2.p8.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p8.2.m2.1.1.cmml" xref="S2.SS2.p8.2.m2.1.1">SER</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.2.m2.1c">\langle{\rm SER}\rangle</annotation></semantics></math>, <math id="S2.SS2.p8.3.m3.1" class="ltx_Math" alttext="\langle{\rm AED}\rangle" display="inline"><semantics id="S2.SS2.p8.3.m3.1a"><mrow id="S2.SS2.p8.3.m3.1.2.2" xref="S2.SS2.p8.3.m3.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.p8.3.m3.1.2.2.1" xref="S2.SS2.p8.3.m3.1.2.1.1.cmml">⟨</mo><mi id="S2.SS2.p8.3.m3.1.1" xref="S2.SS2.p8.3.m3.1.1.cmml">AED</mi><mo stretchy="false" id="S2.SS2.p8.3.m3.1.2.2.2" xref="S2.SS2.p8.3.m3.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.3.m3.1b"><apply id="S2.SS2.p8.3.m3.1.2.1.cmml" xref="S2.SS2.p8.3.m3.1.2.2"><csymbol cd="latexml" id="S2.SS2.p8.3.m3.1.2.1.1.cmml" xref="S2.SS2.p8.3.m3.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S2.SS2.p8.3.m3.1.1.cmml" xref="S2.SS2.p8.3.m3.1.1">AED</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.3.m3.1c">\langle{\rm AED}\rangle</annotation></semantics></math> tokens respectively. Compared to SenseVoice-Small, the advantage of SenseVoice-Large is the transcription accuracy and supporting for a vast number of languages (50+).</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Voice Understanding Model: SenseVoice ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives examples of transcriptions of Whisper, SenseVoice-S, SenseVoice-L, and the ground truth of the ASR task.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:436.3pt;height:6981.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S2.T1.9.9" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.9.9.10" class="ltx_tr">
<td id="S2.T1.9.9.10.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S2.T1.9.9.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.9.9.10.1.1.1" class="ltx_p"><span id="S2.T1.9.9.10.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Whisper</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Absolute shock, but in a great way. Wow. That was awesome. That was awesome. What way to open a song. That was awesome. Awesome. </span><math id="S2.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">\cdots</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S2.T1.9.9.11" class="ltx_tr">
<td id="S2.T1.9.9.11.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T1.9.9.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.9.9.11.1.1.1" class="ltx_p"><span id="S2.T1.9.9.11.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SenseVoice-S</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4.4" class="ltx_tr">
<td id="S2.T1.4.4.4.3" class="ltx_td ltx_align_justify">
<span id="S2.T1.4.4.4.3.3" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.3.3.3" class="ltx_p"><math id="S2.T1.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="&lt;music&gt;" display="inline"><semantics id="S2.T1.2.2.2.1.1.1.m1.1a"><mrow id="S2.T1.2.2.2.1.1.1.m1.1.1.1" xref="S2.T1.2.2.2.1.1.1.m1.1.1.2.cmml"><mo fence="true" mathsize="80%" rspace="0em" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.2" xref="S2.T1.2.2.2.1.1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.2" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.3" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1a" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.4" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1b" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.5" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1c" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.6" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.6.cmml">c</mi></mrow><mo fence="true" lspace="0em" mathsize="80%" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.3" xref="S2.T1.2.2.2.1.1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.1.1.m1.1b"><apply id="S2.T1.2.2.2.1.1.1.m1.1.1.2.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1"><csymbol cd="latexml" id="S2.T1.2.2.2.1.1.1.m1.1.1.2.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.2">expectation</csymbol><apply id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1"><times id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.1"></times><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.2">𝑚</ci><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.3.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.3">𝑢</ci><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.4.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.4">𝑠</ci><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.5.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.5">𝑖</ci><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.6.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.1.1.6">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.1.1.m1.1c">&lt;music&gt;</annotation></semantics></math><span id="S2.T1.4.4.4.3.3.3.1" class="ltx_text" style="font-size:80%;"> Absolute shocked but in a great way my. </span><math id="S2.T1.3.3.3.2.2.2.m2.1" class="ltx_Math" alttext="&lt;happy&gt;" display="inline"><semantics id="S2.T1.3.3.3.2.2.2.m2.1a"><mrow id="S2.T1.3.3.3.2.2.2.m2.1.1.1" xref="S2.T1.3.3.3.2.2.2.m2.1.1.2.cmml"><mo fence="true" mathsize="80%" rspace="0em" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.2" xref="S2.T1.3.3.3.2.2.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.cmml"><mi mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.2" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.3" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1a" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.4" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1b" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.5" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1c" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.6" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.6.cmml">y</mi></mrow><mo fence="true" lspace="0em" mathsize="80%" id="S2.T1.3.3.3.2.2.2.m2.1.1.1.3" xref="S2.T1.3.3.3.2.2.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.2.2.2.m2.1b"><apply id="S2.T1.3.3.3.2.2.2.m2.1.1.2.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1"><csymbol cd="latexml" id="S2.T1.3.3.3.2.2.2.m2.1.1.2.1.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.2">expectation</csymbol><apply id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1"><times id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.1"></times><ci id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.2.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.2">ℎ</ci><ci id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.3.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.3">𝑎</ci><ci id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.4.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.4">𝑝</ci><ci id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.5.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.5">𝑝</ci><ci id="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.6.cmml" xref="S2.T1.3.3.3.2.2.2.m2.1.1.1.1.6">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.2.2.2.m2.1c">&lt;happy&gt;</annotation></semantics></math><span id="S2.T1.4.4.4.3.3.3.2" class="ltx_text" style="font-size:80%;"> That was awesome, that was awesome what way to open a song that was awesome, awesome, </span><math id="S2.T1.4.4.4.3.3.3.m3.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.T1.4.4.4.3.3.3.m3.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.4.4.4.3.3.3.m3.1.1" xref="S2.T1.4.4.4.3.3.3.m3.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.3.3.3.m3.1b"><ci id="S2.T1.4.4.4.3.3.3.m3.1.1.cmml" xref="S2.T1.4.4.4.3.3.3.m3.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.3.3.3.m3.1c">\cdots</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S2.T1.9.9.12" class="ltx_tr">
<td id="S2.T1.9.9.12.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T1.9.9.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.9.9.12.1.1.1" class="ltx_p"><span id="S2.T1.9.9.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SenseVoice-L</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.8.8.8" class="ltx_tr">
<td id="S2.T1.8.8.8.4" class="ltx_td ltx_align_justify">
<span id="S2.T1.8.8.8.4.4" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.8.8.8.4.4.4" class="ltx_p"><math id="S2.T1.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="&lt;music&gt;" display="inline"><semantics id="S2.T1.5.5.5.1.1.1.m1.1a"><mrow id="S2.T1.5.5.5.1.1.1.m1.1.1.1" xref="S2.T1.5.5.5.1.1.1.m1.1.1.2.cmml"><mo fence="true" mathsize="80%" rspace="0em" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.2" xref="S2.T1.5.5.5.1.1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.2" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.3" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1a" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.4" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1b" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.5" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1c" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.6" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.6.cmml">c</mi></mrow><mo fence="true" lspace="0em" mathsize="80%" id="S2.T1.5.5.5.1.1.1.m1.1.1.1.3" xref="S2.T1.5.5.5.1.1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.1.1.1.m1.1b"><apply id="S2.T1.5.5.5.1.1.1.m1.1.1.2.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1"><csymbol cd="latexml" id="S2.T1.5.5.5.1.1.1.m1.1.1.2.1.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.2">expectation</csymbol><apply id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1"><times id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.1"></times><ci id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.2">𝑚</ci><ci id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.3.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.3">𝑢</ci><ci id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.4.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.4">𝑠</ci><ci id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.5.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.5">𝑖</ci><ci id="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.6.cmml" xref="S2.T1.5.5.5.1.1.1.m1.1.1.1.1.6">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.1.1.1.m1.1c">&lt;music&gt;</annotation></semantics></math><span id="S2.T1.8.8.8.4.4.4.1" class="ltx_text" style="font-size:80%;"> Absolutely shocked but in a great way. That was awesome, </span><math id="S2.T1.6.6.6.2.2.2.m2.1" class="ltx_Math" alttext="&lt;music&gt;" display="inline"><semantics id="S2.T1.6.6.6.2.2.2.m2.1a"><mrow id="S2.T1.6.6.6.2.2.2.m2.1.1.1" xref="S2.T1.6.6.6.2.2.2.m2.1.1.2.cmml"><mo fence="true" mathsize="80%" rspace="0em" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.2" xref="S2.T1.6.6.6.2.2.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.cmml"><mi mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.2" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.3" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1a" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.4" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1b" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.5" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1c" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.6" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.6.cmml">c</mi></mrow><mo fence="true" lspace="0em" mathsize="80%" id="S2.T1.6.6.6.2.2.2.m2.1.1.1.3" xref="S2.T1.6.6.6.2.2.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.2.2.2.m2.1b"><apply id="S2.T1.6.6.6.2.2.2.m2.1.1.2.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1"><csymbol cd="latexml" id="S2.T1.6.6.6.2.2.2.m2.1.1.2.1.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.2">expectation</csymbol><apply id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1"><times id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.1"></times><ci id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.2.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.2">𝑚</ci><ci id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.3.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.3">𝑢</ci><ci id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.4.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.4">𝑠</ci><ci id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.5.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.5">𝑖</ci><ci id="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.6.cmml" xref="S2.T1.6.6.6.2.2.2.m2.1.1.1.1.6">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.2.2.2.m2.1c">&lt;music&gt;</annotation></semantics></math><span id="S2.T1.8.8.8.4.4.4.2" class="ltx_text" style="font-size:80%;"> that was awesome </span><math id="S2.T1.7.7.7.3.3.3.m3.1" class="ltx_Math" alttext="&lt;happy&gt;" display="inline"><semantics id="S2.T1.7.7.7.3.3.3.m3.1a"><mrow id="S2.T1.7.7.7.3.3.3.m3.1.1.1" xref="S2.T1.7.7.7.3.3.3.m3.1.1.2.cmml"><mo fence="true" mathsize="80%" rspace="0em" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.2" xref="S2.T1.7.7.7.3.3.3.m3.1.1.2.1.cmml">&lt;</mo><mrow id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.cmml"><mi mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.2" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.3" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1a" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.4" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1b" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.5" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1c" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.6" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.6.cmml">y</mi></mrow><mo fence="true" lspace="0em" mathsize="80%" id="S2.T1.7.7.7.3.3.3.m3.1.1.1.3" xref="S2.T1.7.7.7.3.3.3.m3.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.3.3.3.m3.1b"><apply id="S2.T1.7.7.7.3.3.3.m3.1.1.2.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1"><csymbol cd="latexml" id="S2.T1.7.7.7.3.3.3.m3.1.1.2.1.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.2">expectation</csymbol><apply id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1"><times id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.1"></times><ci id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.2.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.2">ℎ</ci><ci id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.3.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.3">𝑎</ci><ci id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.4.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.4">𝑝</ci><ci id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.5.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.5">𝑝</ci><ci id="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.6.cmml" xref="S2.T1.7.7.7.3.3.3.m3.1.1.1.1.6">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.3.3.3.m3.1c">&lt;happy&gt;</annotation></semantics></math><span id="S2.T1.8.8.8.4.4.4.3" class="ltx_text" style="font-size:80%;"> what way to open a song, that was awesome, awesome, </span><math id="S2.T1.8.8.8.4.4.4.m4.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.T1.8.8.8.4.4.4.m4.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.8.8.8.4.4.4.m4.1.1" xref="S2.T1.8.8.8.4.4.4.m4.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.4.4.4.m4.1b"><ci id="S2.T1.8.8.8.4.4.4.m4.1.1.cmml" xref="S2.T1.8.8.8.4.4.4.m4.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.4.4.4.m4.1c">\cdots</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S2.T1.9.9.13" class="ltx_tr">
<td id="S2.T1.9.9.13.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T1.9.9.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.9.9.13.1.1.1" class="ltx_p"><span id="S2.T1.9.9.13.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ground Truth</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.9.9.9" class="ltx_tr">
<td id="S2.T1.9.9.9.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S2.T1.9.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.9.9.9.1.1.1" class="ltx_p"><span id="S2.T1.9.9.9.1.1.1.1" class="ltx_text" style="font-size:80%;">Absolutely shocked, but in a great way. Who am I? Wow. That was awesome. That was awesome. What way to open a song. That was awesome. Awesome. </span><math id="S2.T1.9.9.9.1.1.1.m1.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.T1.9.9.9.1.1.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S2.T1.9.9.9.1.1.1.m1.1.1" xref="S2.T1.9.9.9.1.1.1.m1.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.1.1.1.m1.1b"><ci id="S2.T1.9.9.9.1.1.1.m1.1.1.cmml" xref="S2.T1.9.9.9.1.1.1.m1.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.1.1.1.m1.1c">\cdots</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of transcriptions of Whisper, SenseVoice-S, SenseVoice-L, and the ground truth.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Semantic Speech Tokenizer</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">A speech tokenizer transforms vocal signals into discrete tokens, enabling their modeling and prediction by autoregressive transformers for speech generation.
Our preliminary experiments indicated that the choice of speech tokenizer is pivotal for overall system performance as well as the requirements of both data quality and volume.
We evaluated three classes of speech tokenizers: 1) those based on residual quantization like SoundStream <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al., <a href="#bib.bib58" title="" class="ltx_ref">2022</a>)</cite>, Encodec <cite class="ltx_cite ltx_citemacro_citep">(Défossez et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> and FunCodec <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib15" title="" class="ltx_ref">2024b</a>)</cite>; 2) those utilizing multi-grouped quantization, such as HifiCodec <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>; and 3) “semantic” speech tokens, specifically HuBERT<cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>. All the above tokenizers are trained in the unsupervised or self-supervised manners.
Thus, their association to semantic content is often tenuous, contributing to an unstable synthesis process and a substantial demand for clean training data.
Moreover, unsupervised tokenizers are susceptible to data noise, necessitating meticulously curated clean data sets.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.2" class="ltx_p">Building on the success of SenseVoice models, we introduce a supervised semantic speech tokenizer, denoted as <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><msup id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml">𝒮</mi><mn id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">superscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib14" title="" class="ltx_ref">2024a</a>)</cite>.
Using the pre-trained SenseVoice-Large model as a foundation, we incorporate a vector quantizer subsequent to the encoder’s initial six layers, delineated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.3 Semantic Speech Tokenizer ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Importantly, the integration of an additional positional embedding post-quantization enhances temporal information.
The combination of <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="\textbf{Encoder}_{1}" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><msub id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS3.p2.2.m2.1.1.2" xref="S2.SS3.p2.2.m2.1.1.2a.cmml">Encoder</mtext><mn id="S2.SS3.p2.2.m2.1.1.3" xref="S2.SS3.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><apply id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.2.m2.1.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p2.2.m2.1.1.2a.cmml" xref="S2.SS3.p2.2.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.SS3.p2.2.m2.1.1.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2">Encoder</mtext></ci><cn type="integer" id="S2.SS3.p2.2.m2.1.1.3.cmml" xref="S2.SS3.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">\textbf{Encoder}_{1}</annotation></semantics></math> and vector quantizer is considered as the speech tokenizer, employing the index of the closest code vector as speech tokens.
The vector quantizer utilizes a solitary codebook with an expansive dictionary containing 4,096 entries.
The derived token sequence exhibits a frequency of 50 Hz, thereby reducing the computational load on text-to-token generation within language models.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Since the speech tokenizer is trained to minimize the recognition errors of rich text in an end-to-end manner, the extracted tokens have a strong semantic relationship to textual and paralinguistic information. Furthermore, our <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><msup id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml">𝒮</mi><mn id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S2.SS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> tokenizer benefits from supervised training, enhancing its robustness to data noise and reducing the reliance on pristine data collection. Consequently, a broader spectrum of data can be utilized for training the model.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2407.04051/assets/x1.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="138" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An illustration of our supervised semantic speech tokenizer.</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Voice Generation Model: CosyVoice</h3>

<figure id="S2.T2" class="ltx_table">
<div id="S2.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:418.6pt;height:169.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.0pt,4.5pt) scale(0.95,0.95) ;">
<table id="S2.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Projects</td>
<td id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Languages</td>
<td id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Zero-shot</td>
<td id="S2.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S2.T2.1.1.1.4.1" class="ltx_text"></span> <span id="S2.T2.1.1.1.4.2" class="ltx_text">
<span id="S2.T2.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S2.T2.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Style&amp;Speaker</span></span>
<span id="S2.T2.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S2.T2.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Control</span></span>
</span></span><span id="S2.T2.1.1.1.4.3" class="ltx_text"></span></td>
<td id="S2.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S2.T2.1.1.1.5.1" class="ltx_text"></span> <span id="S2.T2.1.1.1.5.2" class="ltx_text">
<span id="S2.T2.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S2.T2.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Fine-</span></span>
<span id="S2.T2.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S2.T2.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">grained</span></span>
</span></span><span id="S2.T2.1.1.1.5.3" class="ltx_text"></span></td>
<td id="S2.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">SFT</td>
<td id="S2.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Server</td>
</tr>
<tr id="S2.T2.1.1.2" class="ltx_tr">
<td id="S2.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Bark</td>
<td id="S2.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13</td>
<td id="S2.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.2.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.2.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.2.5.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.2.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.2.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S2.T2.1.1.3" class="ltx_tr">
<td id="S2.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">ChatTTS</td>
<td id="S2.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">en, zh</td>
<td id="S2.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.3.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.3.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.3.5.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.3.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">WebUI</td>
</tr>
<tr id="S2.T2.1.1.4" class="ltx_tr">
<td id="S2.T2.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">parler-tts</td>
<td id="S2.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">en</td>
<td id="S2.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.4.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.4.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.4.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.4.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.4.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S2.T2.1.1.5" class="ltx_tr">
<td id="S2.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">EmotiVoice</td>
<td id="S2.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">en, zh</td>
<td id="S2.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.5.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.5.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.5.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.5.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">WebUI</td>
</tr>
<tr id="S2.T2.1.1.6" class="ltx_tr">
<td id="S2.T2.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-SoVITS</td>
<td id="S2.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">en, zh, jp</td>
<td id="S2.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.6.3.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.6.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.6.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.6.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">WebUI</td>
</tr>
<tr id="S2.T2.1.1.7" class="ltx_tr">
<td id="S2.T2.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">OpenVoice</td>
<td id="S2.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S2.T2.1.1.7.2.1" class="ltx_text"></span> <span id="S2.T2.1.1.7.2.2" class="ltx_text">
<span id="S2.T2.1.1.7.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.1.1.7.2.2.1.1" class="ltx_tr">
<span id="S2.T2.1.1.7.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">en,sp,fr, zh,jp,kr</span></span>
</span></span><span id="S2.T2.1.1.7.2.3" class="ltx_text"></span></td>
<td id="S2.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.7.3.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.7.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.7.5.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.7.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S2.T2.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.7.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S2.T2.1.1.8" class="ltx_tr">
<td id="S2.T2.1.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">CosyVoice</td>
<td id="S2.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S2.T2.1.1.8.2.1" class="ltx_text"></span> <span id="S2.T2.1.1.8.2.2" class="ltx_text">
<span id="S2.T2.1.1.8.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.1.1.8.2.2.1.1" class="ltx_tr">
<span id="S2.T2.1.1.8.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">en, zh, jp, yue, kr</span></span>
</span></span><span id="S2.T2.1.1.8.2.3" class="ltx_text"></span></td>
<td id="S2.T2.1.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.8.3.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.8.4.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.8.5.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.1.1.8.6.1" class="ltx_text" style="color:#00FF00;">✓</span></td>
<td id="S2.T2.1.1.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S2.T2.1.1.8.7.1" class="ltx_text"></span> <span id="S2.T2.1.1.8.7.2" class="ltx_text">
<span id="S2.T2.1.1.8.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.1.1.8.7.2.1.1" class="ltx_tr">
<span id="S2.T2.1.1.8.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">WebUI, gRPC</span></span>
</span></span><span id="S2.T2.1.1.8.7.3" class="ltx_text"></span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison on released features between CosyVoice and other open-sourced projects.</figcaption>
</figure>
<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">CosyVoice, a family of fundamental speech generation models <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib14" title="" class="ltx_ref">2024a</a>)</cite>, utilizes <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><msup id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">𝒮</mi><mn id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> tokens to synthesize natural-sounding voices suitable for various applications.
As a versatile model, CosyVoice excels in tasks such as generating multi-lingual voices tailored to specific speakers, adapting to new speakers without training (zero-shot in-context learning), replicating voices across different languages (cross-lingual voice cloning), creating emotionally resonant voices, and offering nuanced influence over speech output through instructional text.
CosyVoice supports five languages, including Chinese (ZH), English (EN), Japanese (JP), Cantonese (Yue) and Korean (KO). We released three open-source models. The first, CosyVoice-base-300M, excels in accurately representing speaker identity, adapting to contexts without any finetuning, and cloning voices across languages. The second, CosyVoice-instruct-300M, is adept in generating emotionally expressive voices and allows for meticulous adjustments via instructional text. Lastly, CosyVoice-sft-300M has been fine-tuned on seven multi-lingual speakers and is ready for immediate deployment. All of them share the common model architecture and learning framework. Compared with other open-sourced projects, CosyVoice released a widest spectrum of supporting features as shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.4 Voice Generation Model: CosyVoice ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2407.04051/assets/x2.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A semantic diagram of CosyVoice models.</figcaption>
</figure>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>System Overview</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">CosyVoice incorporates an autoregressive Transformer-based language model (LM) to generate speech tokens for the input text.
An ordinary differential equation based (ODE-based) diffusion model, flow matching <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>, reconstructs Mel spectrum from the generated tokens. Subsequently, a HiFTNet-based vocoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> is followed to synthesize waveforms from the reconstructed Mel spectrum. Dashed models are optional for certain applications, such as cross-lingual cloning and speaker fine-tuned inference.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Model Training</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">At the training stage, the autoregressive language model (LM) is trained using a teacher-forcing paradigm.
In this process, tokenized text and a left-shifted version of the speech tokens are provided as input to predict the subsequent speech tokens.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.p2.6" class="ltx_p">The flow matching model is developed to estimate the conditional probabilities <math id="S2.SS4.SSS2.p2.1.m1.3" class="ltx_Math" alttext="P(S|X,v,S_{ref})" display="inline"><semantics id="S2.SS4.SSS2.p2.1.m1.3a"><mrow id="S2.SS4.SSS2.p2.1.m1.3.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.3.3.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.1.m1.3.3.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.cmml">​</mo><mrow id="S2.SS4.SSS2.p2.1.m1.3.3.1.1" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.3.cmml">S</mi><mo fence="false" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.2.cmml">|</mo><mrow id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.2.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.1.1" xref="S2.SS4.SSS2.p2.1.m1.1.1.cmml">X</mi><mo id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.2.cmml">,</mo><mi id="S2.SS4.SSS2.p2.1.m1.2.2" xref="S2.SS4.SSS2.p2.1.m1.2.2.cmml">v</mi><mo id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.2.cmml">,</mo><msub id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.2.cmml">S</mi><mrow id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1a" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.4" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.4.cmml">f</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.1.m1.3b"><apply id="S2.SS4.SSS2.p2.1.m1.3.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3"><times id="S2.SS4.SSS2.p2.1.m1.3.3.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2"></times><ci id="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.3">𝑃</ci><apply id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.2">conditional</csymbol><ci id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.3">𝑆</ci><list id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1"><ci id="S2.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1">𝑋</ci><ci id="S2.SS4.SSS2.p2.1.m1.2.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.2.2">𝑣</ci><apply id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.2">𝑆</ci><apply id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3"><times id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.1"></times><ci id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.2">𝑟</ci><ci id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.3">𝑒</ci><ci id="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.1.1.1.1.1.1.3.4">𝑓</ci></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.1.m1.3c">P(S|X,v,S_{ref})</annotation></semantics></math>, where <math id="S2.SS4.SSS2.p2.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS4.SSS2.p2.2.m2.1a"><mi id="S2.SS4.SSS2.p2.2.m2.1.1" xref="S2.SS4.SSS2.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.2.m2.1b"><ci id="S2.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.2.m2.1c">X</annotation></semantics></math> and <math id="S2.SS4.SSS2.p2.3.m3.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS4.SSS2.p2.3.m3.1a"><mi id="S2.SS4.SSS2.p2.3.m3.1.1" xref="S2.SS4.SSS2.p2.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.3.m3.1b"><ci id="S2.SS4.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.3.m3.1c">v</annotation></semantics></math> denote the speech tokens and speaker embeddings <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib54" title="" class="ltx_ref">2023b</a>)</cite>, respectively. <math id="S2.SS4.SSS2.p2.4.m4.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS4.SSS2.p2.4.m4.1a"><mi id="S2.SS4.SSS2.p2.4.m4.1.1" xref="S2.SS4.SSS2.p2.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.4.m4.1b"><ci id="S2.SS4.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p2.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.4.m4.1c">S</annotation></semantics></math> and <math id="S2.SS4.SSS2.p2.5.m5.1" class="ltx_Math" alttext="S_{ref}" display="inline"><semantics id="S2.SS4.SSS2.p2.5.m5.1a"><msub id="S2.SS4.SSS2.p2.5.m5.1.1" xref="S2.SS4.SSS2.p2.5.m5.1.1.cmml"><mi id="S2.SS4.SSS2.p2.5.m5.1.1.2" xref="S2.SS4.SSS2.p2.5.m5.1.1.2.cmml">S</mi><mrow id="S2.SS4.SSS2.p2.5.m5.1.1.3" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.cmml"><mi id="S2.SS4.SSS2.p2.5.m5.1.1.3.2" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.5.m5.1.1.3.1" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.5.m5.1.1.3.3" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.5.m5.1.1.3.1a" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.5.m5.1.1.3.4" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.5.m5.1b"><apply id="S2.SS4.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.5.m5.1.1.1.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.5.m5.1.1.2.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.2">𝑆</ci><apply id="S2.SS4.SSS2.p2.5.m5.1.1.3.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3"><times id="S2.SS4.SSS2.p2.5.m5.1.1.3.1.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.1"></times><ci id="S2.SS4.SSS2.p2.5.m5.1.1.3.2.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.2">𝑟</ci><ci id="S2.SS4.SSS2.p2.5.m5.1.1.3.3.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.3">𝑒</ci><ci id="S2.SS4.SSS2.p2.5.m5.1.1.3.4.cmml" xref="S2.SS4.SSS2.p2.5.m5.1.1.3.4">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.5.m5.1c">S_{ref}</annotation></semantics></math> represent the Mel spectrum of target and reference speech, respectively.
A convolutional Transformer U-Net <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> is employed to ascertain the vector field between the prior distribution and the desired one, which is derived from the optimal transport ODE. The straightforward nature of resolving the OT-ODE allows for a significantly reduced number of iterations during the inference stage, typically only five to ten iterations are required to produce a satisfactory Mel spectrogram. We also employ the classifier-free guidance (CFG) <cite class="ltx_cite ltx_citemacro_citep">(Ho &amp; Salimans, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> technique and mask out the 70%<math id="S2.SS4.SSS2.p2.6.m6.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS4.SSS2.p2.6.m6.1a"><mo id="S2.SS4.SSS2.p2.6.m6.1.1" xref="S2.SS4.SSS2.p2.6.m6.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.6.m6.1b"><csymbol cd="latexml" id="S2.SS4.SSS2.p2.6.m6.1.1.cmml" xref="S2.SS4.SSS2.p2.6.m6.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.6.m6.1c">\sim</annotation></semantics></math>100% proceeding feature conditions to boost the in-context learning ability.</p>
</div>
<div id="S2.SS4.SSS2.p3" class="ltx_para">
<p id="S2.SS4.SSS2.p3.1" class="ltx_p">For the synthesis of waveforms from the predicted Mel spectrograms, we utilize a vocoder based on HiFTNet <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. Modifications have been made on HiFTNet to support streaming generation, including the replacement and redesign of certain components. Complete details regarding these adjustments are available in our released code.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>Zero-shot In-context Learning</h4>

<figure id="S2.F5" class="ltx_figure"><img src="/html/2407.04051/assets/x3.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sequence construction for (a) zero-shot in-context learning and (b) cross-lingual voice cloning. LID represents language identifier.</figcaption>
</figure>
<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.2" class="ltx_p">CosyVoice models exhibit zero-shot in-context learning capabilities, allowing for the replication of an arbitrary voice with only a brief reference speech sample. This process entails the careful construction of input sequences for the token language model (LM), depicted in Figure <a href="#S2.F5" title="Figure 5 ‣ 2.4.3 Zero-shot In-context Learning ‣ 2.4 Voice Generation Model: CosyVoice ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
For prompt speech and input text in the same language, we merge them to form a unified input, treating the prompt speech tokens as pre-generated. With this input sequence, the autoregressive LM iteratively predicts subsequent tokens until it encounters the “end of sequence” token <svg id="S2.SS4.SSS3.p1.1.m1.1.1.pic1" class="ltx_picture ltx_markedasmath" height="15.8" overflow="visible" version="1.1" width="15.8"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.8) matrix(1 0 0 -1 0 0) translate(7.9,0) translate(0,7.9)"><path d="M 7.62 0 C 7.62 4.21 4.21 7.62 0 7.62 C -4.21 7.62 -7.62 4.21 -7.62 0 C -7.62 -4.21 -4.21 -7.62 0 -7.62 C 4.21 -7.62 7.62 -4.21 7.62 0 Z M 0 0" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -4.71 -4.73)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.42"><span id="S2.SS4.SSS3.p1.1.m1.1.1.pic1.1.1.1.1.1" class="ltx_text">E</span></foreignObject></g></g></svg>.
However, when the prompt speech and input text differ linguistically, we omit the text and tokens associated with the prompt to prevent prosodic characteristics of the original language from influencing the target language.
It is important to note that the prompt text, which corresponds to the prompt speech’s content, can be transcribed either through human annotation or ASR models, such as SenseVoice. Similar to the prompt text, the prompt tokens are extracted from the prompt speech with <math id="S2.SS4.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S2.SS4.SSS3.p1.2.m2.1a"><msup id="S2.SS4.SSS3.p1.2.m2.1.1" xref="S2.SS4.SSS3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.SSS3.p1.2.m2.1.1.2" xref="S2.SS4.SSS3.p1.2.m2.1.1.2.cmml">𝒮</mi><mn id="S2.SS4.SSS3.p1.2.m2.1.1.3" xref="S2.SS4.SSS3.p1.2.m2.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p1.2.m2.1b"><apply id="S2.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS3.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS4.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS3.p1.2.m2.1.1.2">𝒮</ci><cn type="integer" id="S2.SS4.SSS3.p1.2.m2.1.1.3.cmml" xref="S2.SS4.SSS3.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p1.2.m2.1c">\mathcal{S}^{3}</annotation></semantics></math> tokenizer.</p>
</div>
<div id="S2.SS4.SSS3.p2" class="ltx_para">
<p id="S2.SS4.SSS3.p2.1" class="ltx_p">After generating the speech tokens, they are appended after the prompt tokens, forming a composite condition for the flow-matching model. Additionally, the speaker embedding and the Mel spectrogram of the prompt speech are incorporated to further enhance timbre and environmental consistency.</p>
</div>
</section>
<section id="S2.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4 </span>Instruction Fine-tuning</h4>

<div id="S2.SS4.SSS4.p1" class="ltx_para">
<p id="S2.SS4.SSS4.p1.1" class="ltx_p">To enable further controllability on CosyVoice, we experiment with integrating additional instruction fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>.
CosyVoice-instruct extends CosyVoice-base with enhanced instruction-following capabilities. Specifically, it supports controllability over various aspects such as speaker identity (i.e., speaker’s characteristics), speaking style (including emotion, gender, speaking rate, and pitch), and fine-grained paralinguistic features. These features include the ability to insert laughter, breaths, speaking while laughing, and emphasizing certain words. Table <a href="#S2.T3" title="Table 3 ‣ 2.4.4 Instruction Fine-tuning ‣ 2.4 Voice Generation Model: CosyVoice ‣ 2 FunAudioLLM Models ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows some examples of speaker identity, speaking style, and fine-grained paralinguistic features.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<div id="S2.T3.20" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:436.3pt;height:10624pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S2.T3.20.20" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.20.20.21" class="ltx_tr">
<td id="S2.T3.20.20.21.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S2.T3.20.20.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.21.1.1.1" class="ltx_p"><span id="S2.T3.20.20.21.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Speaker Identity</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.2.2.2" class="ltx_tr">
<td id="S2.T3.2.2.2.2" class="ltx_td ltx_align_justify">
<span id="S2.T3.2.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.2.2.2.2" class="ltx_p"><span id="S2.T3.2.2.2.2.2.2.1" class="ltx_text" style="font-size:80%;">1. Selene ’Moonshade’, is a </span><span id="S2.T3.2.2.2.2.2.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">mysterious</span><span id="S2.T3.2.2.2.2.2.2.3" class="ltx_text" style="font-size:80%;">, </span><span id="S2.T3.2.2.2.2.2.2.4" class="ltx_text ltx_font_bold" style="font-size:80%;">elegant dancer</span><span id="S2.T3.2.2.2.2.2.2.5" class="ltx_text" style="font-size:80%;"> with a connection to the night. Her movements are both </span><span id="S2.T3.2.2.2.2.2.2.6" class="ltx_text ltx_font_bold" style="font-size:80%;">mesmerizing</span><span id="S2.T3.2.2.2.2.2.2.7" class="ltx_text" style="font-size:80%;"> and </span><span id="S2.T3.2.2.2.2.2.2.8" class="ltx_text ltx_font_bold" style="font-size:80%;">deadly</span><span id="S2.T3.2.2.2.2.2.2.9" class="ltx_text" style="font-size:80%;">.</span><math id="S2.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.1.1.1.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.1.1.1.1.1.1.m1.1.1" xref="S2.T3.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.1.1.1.1.1.1.m1.1b"><lt id="S2.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T3.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.1.1.1.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.2.2.2.2.2.2.10" class="ltx_text" style="font-size:80%;">endofprompt</span><math id="S2.T3.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.2.2.2.2.2.2.m2.1a"><mo mathsize="80%" id="S2.T3.2.2.2.2.2.2.m2.1.1" xref="S2.T3.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.2.2.2.2.2.2.m2.1b"><gt id="S2.T3.2.2.2.2.2.2.m2.1.1.cmml" xref="S2.T3.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.2.2.2.2.2.2.m2.1c">&gt;</annotation></semantics></math><span id="S2.T3.2.2.2.2.2.2.11" class="ltx_text" style="font-size:80%;">Hope is a good thing.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.4.4.4" class="ltx_tr">
<td id="S2.T3.4.4.4.2" class="ltx_td ltx_align_justify">
<span id="S2.T3.4.4.4.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.4.4.4.2.2.2" class="ltx_p"><span id="S2.T3.4.4.4.2.2.2.1" class="ltx_text" style="font-size:80%;">2. Theo ’Crimson’, is a </span><span id="S2.T3.4.4.4.2.2.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">fiery</span><span id="S2.T3.4.4.4.2.2.2.3" class="ltx_text" style="font-size:80%;">, </span><span id="S2.T3.4.4.4.2.2.2.4" class="ltx_text ltx_font_bold" style="font-size:80%;">passionate</span><span id="S2.T3.4.4.4.2.2.2.5" class="ltx_text" style="font-size:80%;"> rebel leader. Fights with fervor for justice, but struggles with </span><span id="S2.T3.4.4.4.2.2.2.6" class="ltx_text ltx_font_bold" style="font-size:80%;">impulsiveness</span><span id="S2.T3.4.4.4.2.2.2.7" class="ltx_text" style="font-size:80%;">.</span><math id="S2.T3.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.3.3.3.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.3.3.3.1.1.1.m1.1.1" xref="S2.T3.3.3.3.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.3.3.3.1.1.1.m1.1b"><lt id="S2.T3.3.3.3.1.1.1.m1.1.1.cmml" xref="S2.T3.3.3.3.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.3.3.3.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.4.4.4.2.2.2.8" class="ltx_text" style="font-size:80%;">endofprompt</span><math id="S2.T3.4.4.4.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.4.4.4.2.2.2.m2.1a"><mo mathsize="80%" id="S2.T3.4.4.4.2.2.2.m2.1.1" xref="S2.T3.4.4.4.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.4.4.4.2.2.2.m2.1b"><gt id="S2.T3.4.4.4.2.2.2.m2.1.1.cmml" xref="S2.T3.4.4.4.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.4.4.4.2.2.2.m2.1c">&gt;</annotation></semantics></math><span id="S2.T3.4.4.4.2.2.2.9" class="ltx_text" style="font-size:80%;">You don’t know about real loss.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.20.20.22" class="ltx_tr">
<td id="S2.T3.20.20.22.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T3.20.20.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.22.1.1.1" class="ltx_p"><span id="S2.T3.20.20.22.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Speaking Style</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.6.6.6" class="ltx_tr">
<td id="S2.T3.6.6.6.2" class="ltx_td ltx_align_justify">
<span id="S2.T3.6.6.6.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.6.6.6.2.2.2" class="ltx_p"><span id="S2.T3.6.6.6.2.2.2.1" class="ltx_text" style="font-size:80%;">1. A </span><span id="S2.T3.6.6.6.2.2.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">happy</span><span id="S2.T3.6.6.6.2.2.2.3" class="ltx_text" style="font-size:80%;"> </span><span id="S2.T3.6.6.6.2.2.2.4" class="ltx_text ltx_font_bold" style="font-size:80%;">girl</span><span id="S2.T3.6.6.6.2.2.2.5" class="ltx_text" style="font-size:80%;"> with </span><span id="S2.T3.6.6.6.2.2.2.6" class="ltx_text ltx_font_bold" style="font-size:80%;">high tone</span><span id="S2.T3.6.6.6.2.2.2.7" class="ltx_text" style="font-size:80%;"> and </span><span id="S2.T3.6.6.6.2.2.2.8" class="ltx_text ltx_font_bold" style="font-size:80%;">quick speech</span><span id="S2.T3.6.6.6.2.2.2.9" class="ltx_text" style="font-size:80%;">.</span><math id="S2.T3.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.5.5.5.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.5.5.5.1.1.1.m1.1.1" xref="S2.T3.5.5.5.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.5.5.5.1.1.1.m1.1b"><lt id="S2.T3.5.5.5.1.1.1.m1.1.1.cmml" xref="S2.T3.5.5.5.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.5.5.5.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.6.6.6.2.2.2.10" class="ltx_text" style="font-size:80%;">endofprompt</span><math id="S2.T3.6.6.6.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.6.6.6.2.2.2.m2.1a"><mo mathsize="80%" id="S2.T3.6.6.6.2.2.2.m2.1.1" xref="S2.T3.6.6.6.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.6.6.6.2.2.2.m2.1b"><gt id="S2.T3.6.6.6.2.2.2.m2.1.1.cmml" xref="S2.T3.6.6.6.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.6.6.6.2.2.2.m2.1c">&gt;</annotation></semantics></math><span id="S2.T3.6.6.6.2.2.2.11" class="ltx_text" style="font-size:80%;">The sun is shining brightly today.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.8.8.8" class="ltx_tr">
<td id="S2.T3.8.8.8.2" class="ltx_td ltx_align_justify">
<span id="S2.T3.8.8.8.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.8.8.8.2.2.2" class="ltx_p"><span id="S2.T3.8.8.8.2.2.2.1" class="ltx_text" style="font-size:80%;">2. A </span><span id="S2.T3.8.8.8.2.2.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">sad woman</span><span id="S2.T3.8.8.8.2.2.2.3" class="ltx_text" style="font-size:80%;"> with </span><span id="S2.T3.8.8.8.2.2.2.4" class="ltx_text ltx_font_bold" style="font-size:80%;">normal tone</span><span id="S2.T3.8.8.8.2.2.2.5" class="ltx_text" style="font-size:80%;"> and </span><span id="S2.T3.8.8.8.2.2.2.6" class="ltx_text ltx_font_bold" style="font-size:80%;">slow speaking speed</span><span id="S2.T3.8.8.8.2.2.2.7" class="ltx_text" style="font-size:80%;">.</span><math id="S2.T3.7.7.7.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.7.7.7.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.7.7.7.1.1.1.m1.1.1" xref="S2.T3.7.7.7.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.7.7.7.1.1.1.m1.1b"><lt id="S2.T3.7.7.7.1.1.1.m1.1.1.cmml" xref="S2.T3.7.7.7.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.7.7.7.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.8.8.8.2.2.2.8" class="ltx_text" style="font-size:80%;">endofprompt</span><math id="S2.T3.8.8.8.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.8.8.8.2.2.2.m2.1a"><mo mathsize="80%" id="S2.T3.8.8.8.2.2.2.m2.1.1" xref="S2.T3.8.8.8.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.8.8.8.2.2.2.m2.1b"><gt id="S2.T3.8.8.8.2.2.2.m2.1.1.cmml" xref="S2.T3.8.8.8.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.8.8.8.2.2.2.m2.1c">&gt;</annotation></semantics></math><span id="S2.T3.8.8.8.2.2.2.9" class="ltx_text" style="font-size:80%;">I failed my important exam.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.20.20.23" class="ltx_tr">
<td id="S2.T3.20.20.23.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T3.20.20.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.23.1.1.1" class="ltx_p"><span id="S2.T3.20.20.23.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fine-grained Paralinguistics</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.20.20.24" class="ltx_tr">
<td id="S2.T3.20.20.24.1" class="ltx_td ltx_align_justify">
<span id="S2.T3.20.20.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.24.1.1.1" class="ltx_p"><span id="S2.T3.20.20.24.1.1.1.1" class="ltx_text" style="font-size:80%;">1. Well that’s kind of scary </span><span id="S2.T3.20.20.24.1.1.1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">[laughter]</span><span id="S2.T3.20.20.24.1.1.1.3" class="ltx_text" style="font-size:80%;">.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.20.20.25" class="ltx_tr">
<td id="S2.T3.20.20.25.1" class="ltx_td ltx_align_justify">
<span id="S2.T3.20.20.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.25.1.1.1" class="ltx_p"><span id="S2.T3.20.20.25.1.1.1.1" class="ltx_text" style="font-size:80%;">2. I don’t think I over eat yeah </span><span id="S2.T3.20.20.25.1.1.1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">[breath]</span><span id="S2.T3.20.20.25.1.1.1.3" class="ltx_text" style="font-size:80%;"> and um I do exercise regularly.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.12.12.12" class="ltx_tr">
<td id="S2.T3.12.12.12.4" class="ltx_td ltx_align_justify">
<span id="S2.T3.12.12.12.4.4" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.12.12.12.4.4.4" class="ltx_p"><span id="S2.T3.12.12.12.4.4.4.4" class="ltx_text" style="font-size:80%;">3. Well that pretty much covers </span><math id="S2.T3.9.9.9.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.9.9.9.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.9.9.9.1.1.1.m1.1.1" xref="S2.T3.9.9.9.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.9.9.9.1.1.1.m1.1b"><lt id="S2.T3.9.9.9.1.1.1.m1.1.1.cmml" xref="S2.T3.9.9.9.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.9.9.9.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.12.12.12.4.4.4.3" class="ltx_text ltx_font_bold" style="font-size:80%;">laughter<math id="S2.T3.10.10.10.2.2.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.10.10.10.2.2.2.1.m1.1a"><mo id="S2.T3.10.10.10.2.2.2.1.m1.1.1" xref="S2.T3.10.10.10.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.10.10.10.2.2.2.1.m1.1b"><gt id="S2.T3.10.10.10.2.2.2.1.m1.1.1.cmml" xref="S2.T3.10.10.10.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.10.10.10.2.2.2.1.m1.1c">&gt;</annotation></semantics></math>the subject<math id="S2.T3.11.11.11.3.3.3.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.11.11.11.3.3.3.2.m2.1a"><mo id="S2.T3.11.11.11.3.3.3.2.m2.1.1" xref="S2.T3.11.11.11.3.3.3.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.11.11.11.3.3.3.2.m2.1b"><lt id="S2.T3.11.11.11.3.3.3.2.m2.1.1.cmml" xref="S2.T3.11.11.11.3.3.3.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.11.11.11.3.3.3.2.m2.1c">&lt;</annotation></semantics></math>/laughter<math id="S2.T3.12.12.12.4.4.4.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.12.12.12.4.4.4.3.m3.1a"><mo id="S2.T3.12.12.12.4.4.4.3.m3.1.1" xref="S2.T3.12.12.12.4.4.4.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.12.12.12.4.4.4.3.m3.1b"><gt id="S2.T3.12.12.12.4.4.4.3.m3.1.1.cmml" xref="S2.T3.12.12.12.4.4.4.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.12.12.12.4.4.4.3.m3.1c">&gt;</annotation></semantics></math></span><span id="S2.T3.12.12.12.4.4.4.5" class="ltx_text" style="font-size:80%;"> well thanks for calling me.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.20.20.20" class="ltx_tr">
<td id="S2.T3.20.20.20.8" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S2.T3.20.20.20.8.8" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.20.20.20.8.8.8" class="ltx_p"><span id="S2.T3.20.20.20.8.8.8.7" class="ltx_text" style="font-size:80%;">4. The team’s </span><math id="S2.T3.13.13.13.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.13.13.13.1.1.1.m1.1a"><mo mathsize="80%" id="S2.T3.13.13.13.1.1.1.m1.1.1" xref="S2.T3.13.13.13.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.13.13.13.1.1.1.m1.1b"><lt id="S2.T3.13.13.13.1.1.1.m1.1.1.cmml" xref="S2.T3.13.13.13.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.13.13.13.1.1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T3.16.16.16.4.4.4.3" class="ltx_text ltx_font_bold" style="font-size:80%;">strong<math id="S2.T3.14.14.14.2.2.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.14.14.14.2.2.2.1.m1.1a"><mo id="S2.T3.14.14.14.2.2.2.1.m1.1.1" xref="S2.T3.14.14.14.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.14.14.14.2.2.2.1.m1.1b"><gt id="S2.T3.14.14.14.2.2.2.1.m1.1.1.cmml" xref="S2.T3.14.14.14.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.14.14.14.2.2.2.1.m1.1c">&gt;</annotation></semantics></math>unity<math id="S2.T3.15.15.15.3.3.3.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.15.15.15.3.3.3.2.m2.1a"><mo id="S2.T3.15.15.15.3.3.3.2.m2.1.1" xref="S2.T3.15.15.15.3.3.3.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.15.15.15.3.3.3.2.m2.1b"><lt id="S2.T3.15.15.15.3.3.3.2.m2.1.1.cmml" xref="S2.T3.15.15.15.3.3.3.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.15.15.15.3.3.3.2.m2.1c">&lt;</annotation></semantics></math>/strong<math id="S2.T3.16.16.16.4.4.4.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.16.16.16.4.4.4.3.m3.1a"><mo id="S2.T3.16.16.16.4.4.4.3.m3.1.1" xref="S2.T3.16.16.16.4.4.4.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.16.16.16.4.4.4.3.m3.1b"><gt id="S2.T3.16.16.16.4.4.4.3.m3.1.1.cmml" xref="S2.T3.16.16.16.4.4.4.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.16.16.16.4.4.4.3.m3.1c">&gt;</annotation></semantics></math></span><span id="S2.T3.20.20.20.8.8.8.8" class="ltx_text" style="font-size:80%;"> and </span><math id="S2.T3.17.17.17.5.5.5.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.17.17.17.5.5.5.m2.1a"><mo mathsize="80%" id="S2.T3.17.17.17.5.5.5.m2.1.1" xref="S2.T3.17.17.17.5.5.5.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.17.17.17.5.5.5.m2.1b"><lt id="S2.T3.17.17.17.5.5.5.m2.1.1.cmml" xref="S2.T3.17.17.17.5.5.5.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.17.17.17.5.5.5.m2.1c">&lt;</annotation></semantics></math><span id="S2.T3.20.20.20.8.8.8.6" class="ltx_text ltx_font_bold" style="font-size:80%;">strong<math id="S2.T3.18.18.18.6.6.6.4.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.18.18.18.6.6.6.4.m1.1a"><mo id="S2.T3.18.18.18.6.6.6.4.m1.1.1" xref="S2.T3.18.18.18.6.6.6.4.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.18.18.18.6.6.6.4.m1.1b"><gt id="S2.T3.18.18.18.6.6.6.4.m1.1.1.cmml" xref="S2.T3.18.18.18.6.6.6.4.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.18.18.18.6.6.6.4.m1.1c">&gt;</annotation></semantics></math>resilience<math id="S2.T3.19.19.19.7.7.7.5.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T3.19.19.19.7.7.7.5.m2.1a"><mo id="S2.T3.19.19.19.7.7.7.5.m2.1.1" xref="S2.T3.19.19.19.7.7.7.5.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.19.19.19.7.7.7.5.m2.1b"><lt id="S2.T3.19.19.19.7.7.7.5.m2.1.1.cmml" xref="S2.T3.19.19.19.7.7.7.5.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.19.19.19.7.7.7.5.m2.1c">&lt;</annotation></semantics></math>/strong<math id="S2.T3.20.20.20.8.8.8.6.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T3.20.20.20.8.8.8.6.m3.1a"><mo id="S2.T3.20.20.20.8.8.8.6.m3.1.1" xref="S2.T3.20.20.20.8.8.8.6.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T3.20.20.20.8.8.8.6.m3.1b"><gt id="S2.T3.20.20.20.8.8.8.6.m3.1.1.cmml" xref="S2.T3.20.20.20.8.8.8.6.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.20.20.20.8.8.8.6.m3.1c">&gt;</annotation></semantics></math></span><span id="S2.T3.20.20.20.8.8.8.9" class="ltx_text" style="font-size:80%;"> helped them win the championship.</span></span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples of speaker identity, speaking style, and fine-grained paralinguistics. </figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training Set for SenseVoice</h3>

<figure id="S3.F6" class="ltx_figure"><img src="/html/2407.04051/assets/x4.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Hours of SenseVoice training data across languages (in log scale).</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Figure <a href="#S3.F6" title="Figure 6 ‣ 3.1 Training Set for SenseVoice ‣ 3 Dataset ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides an overview of the dataset utilized for training the SenseVoice models. The SenseVoice-Small model was trained on an extensive audio data corpus of approximately 300,000 hours, covering 5 languages including Chinese, Cantonese, English, Japanese, and Korean. To further enhance the multilingual ability of SenseVoice-Large, an additional 100,000 hours of diverse multilingual data were integrated into the training corpus.
To obtain rich transcription labels from speech data, we leveraged open-source models for audio event detection (AED) <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/qiuqiangkong/audioset_tagging_cnn/tree/master" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/qiuqiangkong/audioset_tagging_cnn/tree/master</a></span></span></span> and speech emotion recognition (SER) <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://modelscope.cn/models/iic/emotion2vec_plus_large" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://modelscope.cn/models/iic/emotion2vec_plus_large</a></span></span></span> to generate pseudo labels, yielding an extensive rich transcribe dataset. Specifically, the AED data amounted to 150 million entries, while the SER data comprised 30 million entries.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:165.1pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S3.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Language</td>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Duration (hr)</td>
</tr>
<tr id="S3.T4.1.1.2" class="ltx_tr">
<td id="S3.T4.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">ZH</td>
<td id="S3.T4.1.1.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">130,000</td>
</tr>
<tr id="S3.T4.1.1.3" class="ltx_tr">
<td id="S3.T4.1.1.3.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">EN</td>
<td id="S3.T4.1.1.3.2" class="ltx_td ltx_align_right" style="padding-left:15.0pt;padding-right:15.0pt;">30,000</td>
</tr>
<tr id="S3.T4.1.1.4" class="ltx_tr">
<td id="S3.T4.1.1.4.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Yue</td>
<td id="S3.T4.1.1.4.2" class="ltx_td ltx_align_right" style="padding-left:15.0pt;padding-right:15.0pt;">5,000</td>
</tr>
<tr id="S3.T4.1.1.5" class="ltx_tr">
<td id="S3.T4.1.1.5.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">JP</td>
<td id="S3.T4.1.1.5.2" class="ltx_td ltx_align_right" style="padding-left:15.0pt;padding-right:15.0pt;">4,600</td>
</tr>
<tr id="S3.T4.1.1.6" class="ltx_tr">
<td id="S3.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">KO</td>
<td id="S3.T4.1.1.6.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">2,200</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Hours of CosyVoice training data across languages.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Set for CosyVoice</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To train the CosyVoice models, we have amassed a considerable dataset comprising multiple languages. Throughout the collection process, we utilize specialized in-house tools for speech detection, signal-to-noise ratio (SNR) estimation, speaker diarization, and separation. Subsequently, pseudo text labels are generated using SenseVoice-Large and Paraformer. These labels undergo a refinement process with the aid of force-alignment (FA) models, which helps eliminate low-quality data and enhances the accuracy of punctuation. A comprehensive breakdown of the training data’s duration across various languages is presented in Table <a href="#S3.T4" title="Table 4 ‣ 3.1 Training Set for SenseVoice ‣ 3 Dataset ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For the CosyVoice-instruct model, we fine-tuned CosyVoice-base using instruction training data without incorporating speaker embedding in the autoregressive language model. Table <a href="#S3.T5" title="Table 5 ‣ 3.2 Training Set for CosyVoice ‣ 3 Dataset ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the duration of the training data for different types of instructions.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:245.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Type</td>
<td id="S3.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Duration (hr)</td>
</tr>
<tr id="S3.T5.1.1.2" class="ltx_tr">
<td id="S3.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">Speaker Identity</td>
<td id="S3.T5.1.1.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">101</td>
</tr>
<tr id="S3.T5.1.1.3" class="ltx_tr">
<td id="S3.T5.1.1.3.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Speaking Style</td>
<td id="S3.T5.1.1.3.2" class="ltx_td ltx_align_right" style="padding-left:15.0pt;padding-right:15.0pt;">407</td>
</tr>
<tr id="S3.T5.1.1.4" class="ltx_tr">
<td id="S3.T5.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">Fine-grained Paralinguistics</td>
<td id="S3.T5.1.1.4.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">48</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Duration statistics of instruction training data by type.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multilingual Speech Recognition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Metrics.</span> We use Character Error Rate (CER) to evaluate the models in five languages: Chinese, Cantonese, Japanese, Korean, and Thai, and use the Word Error Rate (WER) for all other languages. Both the ground truth transcriptions and the recognition outputs are standardized using text normalization before the error rate calculation, in alignment with the methodology used by Whisper. All Chinese characters were converted into the simplified Chinese version, together with an additional text normalization pipeline<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/speechio/chinese_text_normalization/blob/master/python/cn_tn.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/speechio/chinese_text_normalization/blob/master/python/cn_tn.py</a></span></span></span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Results in Table <a href="#S4.T6" title="Table 6 ‣ 4.1 Multilingual Speech Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> show the comparison of Whisper, SenseVoice and Paraformer <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Shi et al., <a href="#bib.bib45" title="" class="ltx_ref">2024</a>)</cite> on popular open speech recognition benchmark datasets, including AISHELL-1 <cite class="ltx_cite ltx_citemacro_citep">(Bu et al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>, AISHELL-2 <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>, WenetSpeech <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib60" title="" class="ltx_ref">2022</a>)</cite>, Librispeech <cite class="ltx_cite ltx_citemacro_citep">(Panayotov et al., <a href="#bib.bib38" title="" class="ltx_ref">2015</a>)</cite>, and Common Voice <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. It can be seen that SenseVoice-S and SenseVoice-L outperform their Whisper counterparts by a significant margin in most test sets except Librispeech.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Figure <a href="#S4.F7" title="Figure 7 ‣ 4.1 Multilingual Speech Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the comparative performance of SenseVoice-Large and Whisper-Large-V3 on a broader range of languages, with or without ground truth LID as input. While SenseVoice-Large performs comparably with Whisper-Large-V3 in general, SenseVoice-Large obtains significantly better performance in languages like Cantonese (Yue), Catalan (CA), and Marathi (MR).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The evaluation of inference efficiency is shown in Table <a href="#S4.T7" title="Table 7 ‣ 4.1 Multilingual Speech Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The Real-time factor (RTF, the ratio of the transcribing time to the audio length) and 10s Audio Latency (the average time cost when transcribing a 10s audio.) are benchmarked on an A800 machine, with a decoding batch size of 1. For the encoder-decoder-based model (Whipser-S, Whipser-L-V3, and SenseVoice-L), we perform beam search in decoding with a beam size of 5. Owing to its non-autoregressive architecture, SenseVoice-S obtains extremely low inference latency—more than 5 times faster compared to Whisper-small and more than 15 times faster compared to Whisper-L-V3. SenseVoice-L shows close performance with Whipser-L-V3.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2407.04051/assets/x5.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> Comparison of SenseVoice and Whisper on Common Voice, with or without LID</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:345.0pt;height:163.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-73.9pt,35.1pt) scale(0.7,0.7) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Whisper-S</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Whisper-L-V3</td>
<td id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">SenseVoice-S</td>
<td id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">SenseVoice-L</td>
<td id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Paraformer-zh</td>
</tr>
<tr id="S4.T6.1.1.2" class="ltx_tr">
<td id="S4.T6.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">AISHELL-1 test</td>
<td id="S4.T6.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">10.04</td>
<td id="S4.T6.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">5.14</td>
<td id="S4.T6.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">2.96</td>
<td id="S4.T6.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">2.09</td>
<td id="S4.T6.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.2.6.1" class="ltx_text ltx_font_bold">1.95</span></td>
</tr>
<tr id="S4.T6.1.1.3" class="ltx_tr">
<td id="S4.T6.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">AISHELL-2 test_ios</td>
<td id="S4.T6.1.1.3.2" class="ltx_td ltx_align_left ltx_border_t">8.78</td>
<td id="S4.T6.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">4.96</td>
<td id="S4.T6.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">3.80</td>
<td id="S4.T6.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">3.04</td>
<td id="S4.T6.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.3.6.1" class="ltx_text ltx_font_bold">2.85</span></td>
</tr>
<tr id="S4.T6.1.1.4" class="ltx_tr">
<td id="S4.T6.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">WenetSpeech test_meeting</td>
<td id="S4.T6.1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">25.62</td>
<td id="S4.T6.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">18.87</td>
<td id="S4.T6.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t">7.44</td>
<td id="S4.T6.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.4.5.1" class="ltx_text ltx_font_bold">6.73</span></td>
<td id="S4.T6.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t">6.97</td>
</tr>
<tr id="S4.T6.1.1.5" class="ltx_tr">
<td id="S4.T6.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">WenetSpeech test_net</td>
<td id="S4.T6.1.1.5.2" class="ltx_td ltx_align_left ltx_border_t">16.66</td>
<td id="S4.T6.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">10.48</td>
<td id="S4.T6.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">7.84</td>
<td id="S4.T6.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.5.5.1" class="ltx_text ltx_font_bold">6.01</span></td>
<td id="S4.T6.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">6.74</td>
</tr>
<tr id="S4.T6.1.1.6" class="ltx_tr">
<td id="S4.T6.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t">LibriSpeech test_clean</td>
<td id="S4.T6.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">3.13</td>
<td id="S4.T6.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.6.3.1" class="ltx_text ltx_font_bold">1.82</span></td>
<td id="S4.T6.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">3.15</td>
<td id="S4.T6.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">2.57</td>
<td id="S4.T6.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.7" class="ltx_tr">
<td id="S4.T6.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">LibriSpeech test_other</td>
<td id="S4.T6.1.1.7.2" class="ltx_td ltx_align_left ltx_border_t">7.37</td>
<td id="S4.T6.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.7.3.1" class="ltx_text ltx_font_bold">3.50</span></td>
<td id="S4.T6.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t">7.18</td>
<td id="S4.T6.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t">4.28</td>
<td id="S4.T6.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.8" class="ltx_tr">
<td id="S4.T6.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">CommonVoice zh-CN</td>
<td id="S4.T6.1.1.8.2" class="ltx_td ltx_align_left ltx_border_t">19.60</td>
<td id="S4.T6.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t">12.55</td>
<td id="S4.T6.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t">10.78</td>
<td id="S4.T6.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.8.5.1" class="ltx_text ltx_font_bold">7.68</span></td>
<td id="S4.T6.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t">10.30</td>
</tr>
<tr id="S4.T6.1.1.9" class="ltx_tr">
<td id="S4.T6.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">CommonVoice en</td>
<td id="S4.T6.1.1.9.2" class="ltx_td ltx_align_left ltx_border_t">14.85</td>
<td id="S4.T6.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">9.39</td>
<td id="S4.T6.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">14.71</td>
<td id="S4.T6.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.9.5.1" class="ltx_text ltx_font_bold">9.00</span></td>
<td id="S4.T6.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.10" class="ltx_tr">
<td id="S4.T6.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">CommonVoice yue</td>
<td id="S4.T6.1.1.10.2" class="ltx_td ltx_align_left ltx_border_t">38.97</td>
<td id="S4.T6.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t">10.41</td>
<td id="S4.T6.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t">7.09</td>
<td id="S4.T6.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.10.5.1" class="ltx_text ltx_font_bold">6.78</span></td>
<td id="S4.T6.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.11" class="ltx_tr">
<td id="S4.T6.1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">CommonVoice ja</td>
<td id="S4.T6.1.1.11.2" class="ltx_td ltx_align_left ltx_border_t">19.51</td>
<td id="S4.T6.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t">10.34</td>
<td id="S4.T6.1.1.11.4" class="ltx_td ltx_align_center ltx_border_t">11.96</td>
<td id="S4.T6.1.1.11.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.11.5.1" class="ltx_text ltx_font_bold">9.19</span></td>
<td id="S4.T6.1.1.11.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.12" class="ltx_tr">
<td id="S4.T6.1.1.12.1" class="ltx_td ltx_align_left ltx_border_t">CommonVoice ko</td>
<td id="S4.T6.1.1.12.2" class="ltx_td ltx_align_left ltx_border_t">10.48</td>
<td id="S4.T6.1.1.12.3" class="ltx_td ltx_align_center ltx_border_t">5.59</td>
<td id="S4.T6.1.1.12.4" class="ltx_td ltx_align_center ltx_border_t">8.28</td>
<td id="S4.T6.1.1.12.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.1.12.5.1" class="ltx_text ltx_font_bold">5.21</span></td>
<td id="S4.T6.1.1.12.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.1.13" class="ltx_tr">
<td id="S4.T6.1.1.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">CommonVoice 5 lang. Average</td>
<td id="S4.T6.1.1.13.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">20.68</td>
<td id="S4.T6.1.1.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">9.66</td>
<td id="S4.T6.1.1.13.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">10.56</td>
<td id="S4.T6.1.1.13.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T6.1.1.13.5.1" class="ltx_text ltx_font_bold">7.57</span></td>
<td id="S4.T6.1.1.13.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance comparisons among different models on Chinese and English Open Corpus.</figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<div id="S4.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:380.0pt;height:86.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.5pt,10.8pt) scale(0.8,0.8) ;">
<table id="S4.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T7.1.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Framework</td>
<td id="S4.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Parameters</td>
<td id="S4.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Support Language</td>
<td id="S4.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">RTF</td>
<td id="S4.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">10s Audio Latency(ms)</td>
</tr>
<tr id="S4.T7.1.1.2" class="ltx_tr">
<td id="S4.T7.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Whisper-S</td>
<td id="S4.T7.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Autoregressive</td>
<td id="S4.T7.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">224M</td>
<td id="S4.T7.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">50+</td>
<td id="S4.T7.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.042</td>
<td id="S4.T7.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">518</td>
</tr>
<tr id="S4.T7.1.1.3" class="ltx_tr">
<td id="S4.T7.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Whisper-L-V3</td>
<td id="S4.T7.1.1.3.2" class="ltx_td ltx_align_left ltx_border_t">Autoregressive</td>
<td id="S4.T7.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">1550M</td>
<td id="S4.T7.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">50+</td>
<td id="S4.T7.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.111</td>
<td id="S4.T7.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">1281</td>
</tr>
<tr id="S4.T7.1.1.4" class="ltx_tr">
<td id="S4.T7.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">Paraformer-zh</td>
<td id="S4.T7.1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">Non-autoregressive</td>
<td id="S4.T7.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">220M</td>
<td id="S4.T7.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t">zh</td>
<td id="S4.T7.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">0.009</td>
<td id="S4.T7.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t">100</td>
</tr>
<tr id="S4.T7.1.1.5" class="ltx_tr">
<td id="S4.T7.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">SenseVoice-S</td>
<td id="S4.T7.1.1.5.2" class="ltx_td ltx_align_left ltx_border_t">Non-autoregressive</td>
<td id="S4.T7.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">234M</td>
<td id="S4.T7.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">zh,yue,en,ja,ko</td>
<td id="S4.T7.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">0.007</td>
<td id="S4.T7.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">70</td>
</tr>
<tr id="S4.T7.1.1.6" class="ltx_tr">
<td id="S4.T7.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">SenseVoice-L</td>
<td id="S4.T7.1.1.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Autoregressive</td>
<td id="S4.T7.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1587M</td>
<td id="S4.T7.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50+</td>
<td id="S4.T7.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.110</td>
<td id="S4.T7.1.1.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1623</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison of model architecture, parameter scale, supported languages, and inference efficiency of SenseVoice, Paraformer, and Whisper.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Speech Emotion Recognition</h3>

<figure id="S4.T8" class="ltx_table">
<div id="S4.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:402.3pt;height:121.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.1pt,20.3pt) scale(0.75,0.75) ;">
<table id="S4.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T8.1.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">EmoBox</td>
<td id="S4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Emo-Superb</td>
<td id="S4.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">MerBench</td>
<td id="S4.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">SenseVoice-L</td>
<td id="S4.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">SenseVoice-S</td>
</tr>
<tr id="S4.T8.1.1.2" class="ltx_tr">
<td id="S4.T8.1.1.2.1" class="ltx_td ltx_align_left">Test set</td>
<td id="S4.T8.1.1.2.2" class="ltx_td ltx_align_center">UA</td>
<td id="S4.T8.1.1.2.3" class="ltx_td ltx_align_center">WA</td>
<td id="S4.T8.1.1.2.4" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T8.1.1.2.5" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T8.1.1.2.6" class="ltx_td ltx_align_center">WF1</td>
<td id="S4.T8.1.1.2.7" class="ltx_td ltx_align_center">UA</td>
<td id="S4.T8.1.1.2.8" class="ltx_td ltx_align_center">WA</td>
<td id="S4.T8.1.1.2.9" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T8.1.1.2.10" class="ltx_td ltx_align_center">WF1</td>
<td id="S4.T8.1.1.2.11" class="ltx_td ltx_align_center">UA</td>
<td id="S4.T8.1.1.2.12" class="ltx_td ltx_align_center">WA</td>
<td id="S4.T8.1.1.2.13" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T8.1.1.2.14" class="ltx_td ltx_align_center">WF1</td>
</tr>
<tr id="S4.T8.1.1.3" class="ltx_tr">
<td id="S4.T8.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">CASIA</td>
<td id="S4.T8.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">59.6</td>
<td id="S4.T8.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">59.6</td>
<td id="S4.T8.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">56.3</td>
<td id="S4.T8.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T8.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S4.T8.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">96.0</td>
<td id="S4.T8.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">96.0</td>
<td id="S4.T8.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">95.5</td>
<td id="S4.T8.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">95.5</td>
<td id="S4.T8.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="S4.T8.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="S4.T8.1.1.3.13" class="ltx_td ltx_align_center ltx_border_t">70.3</td>
<td id="S4.T8.1.1.3.14" class="ltx_td ltx_align_center ltx_border_t">70.3</td>
</tr>
<tr id="S4.T8.1.1.4" class="ltx_tr">
<td id="S4.T8.1.1.4.1" class="ltx_td ltx_align_left">CREMA-D</td>
<td id="S4.T8.1.1.4.2" class="ltx_td ltx_align_center">76.8</td>
<td id="S4.T8.1.1.4.3" class="ltx_td ltx_align_center">76.5</td>
<td id="S4.T8.1.1.4.4" class="ltx_td ltx_align_center">76.6</td>
<td id="S4.T8.1.1.4.5" class="ltx_td ltx_align_center">67.7</td>
<td id="S4.T8.1.1.4.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.4.7" class="ltx_td ltx_align_center">90.1</td>
<td id="S4.T8.1.1.4.8" class="ltx_td ltx_align_center">90.4</td>
<td id="S4.T8.1.1.4.9" class="ltx_td ltx_align_center">89.8</td>
<td id="S4.T8.1.1.4.10" class="ltx_td ltx_align_center">89.9</td>
<td id="S4.T8.1.1.4.11" class="ltx_td ltx_align_center">73.1</td>
<td id="S4.T8.1.1.4.12" class="ltx_td ltx_align_center">74.0</td>
<td id="S4.T8.1.1.4.13" class="ltx_td ltx_align_center">65.7</td>
<td id="S4.T8.1.1.4.14" class="ltx_td ltx_align_center">65.7</td>
</tr>
<tr id="S4.T8.1.1.5" class="ltx_tr">
<td id="S4.T8.1.1.5.1" class="ltx_td ltx_align_left">ESD</td>
<td id="S4.T8.1.1.5.2" class="ltx_td ltx_align_center">84.6</td>
<td id="S4.T8.1.1.5.3" class="ltx_td ltx_align_center">84.6</td>
<td id="S4.T8.1.1.5.4" class="ltx_td ltx_align_center">84.3</td>
<td id="S4.T8.1.1.5.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.5.6" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.5.7" class="ltx_td ltx_align_center">93.2</td>
<td id="S4.T8.1.1.5.8" class="ltx_td ltx_align_center">93.2</td>
<td id="S4.T8.1.1.5.9" class="ltx_td ltx_align_center">92.2</td>
<td id="S4.T8.1.1.5.10" class="ltx_td ltx_align_center">92.2</td>
<td id="S4.T8.1.1.5.11" class="ltx_td ltx_align_center">85.5</td>
<td id="S4.T8.1.1.5.12" class="ltx_td ltx_align_center">85.5</td>
<td id="S4.T8.1.1.5.13" class="ltx_td ltx_align_center">81.0</td>
<td id="S4.T8.1.1.5.14" class="ltx_td ltx_align_center">81.0</td>
</tr>
<tr id="S4.T8.1.1.6" class="ltx_tr">
<td id="S4.T8.1.1.6.1" class="ltx_td ltx_align_left">IEMOCAP</td>
<td id="S4.T8.1.1.6.2" class="ltx_td ltx_align_center">73.5</td>
<td id="S4.T8.1.1.6.3" class="ltx_td ltx_align_center">72.9</td>
<td id="S4.T8.1.1.6.4" class="ltx_td ltx_align_center">73.1</td>
<td id="S4.T8.1.1.6.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.6.6" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T8.1.1.6.7" class="ltx_td ltx_align_center">73.9</td>
<td id="S4.T8.1.1.6.8" class="ltx_td ltx_align_center">75.3</td>
<td id="S4.T8.1.1.6.9" class="ltx_td ltx_align_center">73.2</td>
<td id="S4.T8.1.1.6.10" class="ltx_td ltx_align_center">72.8</td>
<td id="S4.T8.1.1.6.11" class="ltx_td ltx_align_center">70.5</td>
<td id="S4.T8.1.1.6.12" class="ltx_td ltx_align_center">65.7</td>
<td id="S4.T8.1.1.6.13" class="ltx_td ltx_align_center">67.9</td>
<td id="S4.T8.1.1.6.14" class="ltx_td ltx_align_center">67.8</td>
</tr>
<tr id="S4.T8.1.1.7" class="ltx_tr">
<td id="S4.T8.1.1.7.1" class="ltx_td ltx_align_left">MELD</td>
<td id="S4.T8.1.1.7.2" class="ltx_td ltx_align_center">31.5</td>
<td id="S4.T8.1.1.7.3" class="ltx_td ltx_align_center">51.9</td>
<td id="S4.T8.1.1.7.4" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T8.1.1.7.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.7.6" class="ltx_td ltx_align_center">46.5</td>
<td id="S4.T8.1.1.7.7" class="ltx_td ltx_align_center">58.7</td>
<td id="S4.T8.1.1.7.8" class="ltx_td ltx_align_center">63.1</td>
<td id="S4.T8.1.1.7.9" class="ltx_td ltx_align_center">50.9</td>
<td id="S4.T8.1.1.7.10" class="ltx_td ltx_align_center">65.7</td>
<td id="S4.T8.1.1.7.11" class="ltx_td ltx_align_center">50.8</td>
<td id="S4.T8.1.1.7.12" class="ltx_td ltx_align_center">57.8</td>
<td id="S4.T8.1.1.7.13" class="ltx_td ltx_align_center">44.6</td>
<td id="S4.T8.1.1.7.14" class="ltx_td ltx_align_center">57.7</td>
</tr>
<tr id="S4.T8.1.1.8" class="ltx_tr">
<td id="S4.T8.1.1.8.1" class="ltx_td ltx_align_left">MER2023</td>
<td id="S4.T8.1.1.8.2" class="ltx_td ltx_align_center">61.2</td>
<td id="S4.T8.1.1.8.3" class="ltx_td ltx_align_center">65.2</td>
<td id="S4.T8.1.1.8.4" class="ltx_td ltx_align_center">62.3</td>
<td id="S4.T8.1.1.8.5" class="ltx_td ltx_align_center">–</td>
<td id="S4.T8.1.1.8.6" class="ltx_td ltx_align_center">67.5</td>
<td id="S4.T8.1.1.8.7" class="ltx_td ltx_align_center">70.9</td>
<td id="S4.T8.1.1.8.8" class="ltx_td ltx_align_center">69.2</td>
<td id="S4.T8.1.1.8.9" class="ltx_td ltx_align_center">55.6</td>
<td id="S4.T8.1.1.8.10" class="ltx_td ltx_align_center">57.4</td>
<td id="S4.T8.1.1.8.11" class="ltx_td ltx_align_center">69.0</td>
<td id="S4.T8.1.1.8.12" class="ltx_td ltx_align_center">68.3</td>
<td id="S4.T8.1.1.8.13" class="ltx_td ltx_align_center">52.8</td>
<td id="S4.T8.1.1.8.14" class="ltx_td ltx_align_center">56.6</td>
</tr>
<tr id="S4.T8.1.1.9" class="ltx_tr">
<td id="S4.T8.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb">MSPPodcast</td>
<td id="S4.T8.1.1.9.2" class="ltx_td ltx_align_center ltx_border_bb">21.4</td>
<td id="S4.T8.1.1.9.3" class="ltx_td ltx_align_center ltx_border_bb">43.4</td>
<td id="S4.T8.1.1.9.4" class="ltx_td ltx_align_center ltx_border_bb">21.5</td>
<td id="S4.T8.1.1.9.5" class="ltx_td ltx_align_center ltx_border_bb">38.4</td>
<td id="S4.T8.1.1.9.6" class="ltx_td ltx_align_center ltx_border_bb">–</td>
<td id="S4.T8.1.1.9.7" class="ltx_td ltx_align_center ltx_border_bb">46.0</td>
<td id="S4.T8.1.1.9.8" class="ltx_td ltx_align_center ltx_border_bb">61.7</td>
<td id="S4.T8.1.1.9.9" class="ltx_td ltx_align_center ltx_border_bb">45.0</td>
<td id="S4.T8.1.1.9.10" class="ltx_td ltx_align_center ltx_border_bb">58.9</td>
<td id="S4.T8.1.1.9.11" class="ltx_td ltx_align_center ltx_border_bb">49.4</td>
<td id="S4.T8.1.1.9.12" class="ltx_td ltx_align_center ltx_border_bb">64.1</td>
<td id="S4.T8.1.1.9.13" class="ltx_td ltx_align_center ltx_border_bb">46.4</td>
<td id="S4.T8.1.1.9.14" class="ltx_td ltx_align_center ltx_border_bb">63.1</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>SER performance comparisons on different evaluation benchmarks.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate the SER ability of the SenseVoice on 7 popular emotion recognition datasets, including CREMA-D<cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib7" title="" class="ltx_ref">2014</a>)</cite>, MELD<cite class="ltx_cite ltx_citemacro_citep">(Poria et al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, IEMOCAP<cite class="ltx_cite ltx_citemacro_citep">(Busso et al., <a href="#bib.bib6" title="" class="ltx_ref">2008</a>)</cite>, MSP-Podcast<cite class="ltx_cite ltx_citemacro_citep">(Martinez-Lucas et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, CASIA<cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Jia, <a href="#bib.bib61" title="" class="ltx_ref">2008</a>)</cite>, MER2023<cite class="ltx_cite ltx_citemacro_citep">(Lian et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> and ESD<cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>. These corpora cover both Chinese and English, and scenarios like acts, TV dramas, and daily conversation. We report unweighted average accuracy (UA), weighted average accuracy (WA), macro F1 Score (F1), and weighted average F1 (WF1), and compare them with some recently published SER benchmarks (EmoBox <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib32" title="" class="ltx_ref">2024a</a>)</cite>, Emo-Superb<cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib55" title="" class="ltx_ref">2024</a>)</cite> and MerBench <cite class="ltx_cite ltx_citemacro_citep">(Lian et al., <a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>) from literature in Table <a href="#S4.T8" title="Table 8 ‣ 4.2 Speech Emotion Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. We show that SenseVoice achieves a good performance on all test sets and all metrics even without fine-tuning on the target domain.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2407.04051/assets/x6.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="465" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Weighted Average Accuracy (WA(%)) comparison with other open source SER models.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We further compare SenseVoice with some open-sourced SER models. Results are shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2 Speech Emotion Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. XLSR-SER is the most popular SER model on HuggingFace<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition</a></span></span></span>, and Qwen-Audio<cite class="ltx_cite ltx_citemacro_citep">(Chu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and SALMONN<cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a href="#bib.bib49" title="" class="ltx_ref">2024</a>)</cite> are two Audio-LLM models which can recognize speech emotion with natural language prompt. Results from EmoBox are also involved in the figure as references.
SenseVoice-Large achieves the best results on almost all datasets while the SenseVoice-Small also outperforms other baseline models on the majority datasets.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Audio Event Detection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Both SenseVoice-Small and SenseVoice-Large models can classify the audio event in the speech, including music, applause, and laughter. The SenseVoice-L can further predict the start and end position of the audio event, while the SenseVoice-Small can only predict what happened in the audio, with at most one event per utterance. SenseVoice-Small can detect more kinds of events, such as coughing, sneezing, breathing, and crying which could occur during human-machine interaction.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:190.8pt;">
<p id="S4.F9.1.1" class="ltx_p ltx_align_center"><span id="S4.F9.1.1.1" class="ltx_text"><img src="/html/2407.04051/assets/x7.png" id="S4.F9.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="664" height="381" alt="Refer to caption"></span></p>
<p id="S4.F9.1.2" class="ltx_p ltx_align_center"><span id="S4.F9.1.2.1" class="ltx_text">ESC-50</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:190.8pt;">
<p id="S4.F9.2.1" class="ltx_p ltx_align_center"><span id="S4.F9.2.1.1" class="ltx_text"><img src="/html/2407.04051/assets/x8.png" id="S4.F9.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="664" height="381" alt="Refer to caption"></span></p>
<p id="S4.F9.2.2" class="ltx_p ltx_align_center"><span id="S4.F9.2.2.1" class="ltx_text">Baby Cry Detection</span></p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:190.8pt;">
<p id="S4.F9.3.1" class="ltx_p ltx_align_center"><span id="S4.F9.3.1.1" class="ltx_text"><img src="/html/2407.04051/assets/x9.png" id="S4.F9.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="664" height="381" alt="Refer to caption"></span></p>
<p id="S4.F9.3.2" class="ltx_p ltx_align_center"><span id="S4.F9.3.2.1" class="ltx_text">Coswara</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F9.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:190.8pt;">
<p id="S4.F9.4.1" class="ltx_p ltx_align_center"><span id="S4.F9.4.1.1" class="ltx_text"><img src="/html/2407.04051/assets/x10.png" id="S4.F9.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="664" height="381" alt="Refer to caption"></span></p>
<p id="S4.F9.4.2" class="ltx_p ltx_align_center"><span id="S4.F9.4.2.1" class="ltx_text">Inhome Talkshow</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>F1(%) Score comparison of the SenseVoice with the audio event detection models BEATS and PANNs on different audio event detection tasks.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We compare SenseVoice with the SOTA audio event detection models BEATs<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2023a</a>)</cite> and PANNs<cite class="ltx_cite ltx_citemacro_citep">(Kong et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> on different tasks, including environment sound classification (ESC-50)<cite class="ltx_cite ltx_citemacro_citep">(Piczak, <a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite>, baby cry/laugh detection<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/giulbia/baby_cry_detection/tree/master" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/giulbia/baby_cry_detection/tree/master</a></span></span></span>, coughing detection (Coswara)<cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite> <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/iiscleap/Coswara-Data/tree/master" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/iiscleap/Coswara-Data/tree/master</a></span></span></span> and in-home talkshow event detection. As SenseVoice only predicts the event of our interest, which may not include event categories in other models, we use the F1 score on each event for evaluation. Qwen-audio is also evaluated for comparison.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We find that SenseVoice serves as a good audio event classification or detection model, though BEATs and PANNs may have better F1 scores, which may be attributed to two reasons. Firstly, BETAS and PANNs can modify the detection threshold to trade-off the accuracy and recall rate to obtain a higher F1 score, but threshold modification is much more difficult for SenseVoice and Qwen-Audio (An interesting discovery is that SenseVoice and Qwen-Audio always have a much higher accuracy than the recall rate, which could be more friendly for the human-machine interaction). Secondly, SenseVoice is trained with ASR data with AED pseudo labeling rather than AED-specific data.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Preserving Semantic Information by <math id="S4.SS4.1.m1.1" class="ltx_Math" alttext="S^{3}" display="inline"><semantics id="S4.SS4.1.m1.1b"><msup id="S4.SS4.1.m1.1.1" xref="S4.SS4.1.m1.1.1.cmml"><mi id="S4.SS4.1.m1.1.1.2" xref="S4.SS4.1.m1.1.1.2.cmml">S</mi><mn id="S4.SS4.1.m1.1.1.3" xref="S4.SS4.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.1.m1.1c"><apply id="S4.SS4.1.m1.1.1.cmml" xref="S4.SS4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.1.m1.1.1.1.cmml" xref="S4.SS4.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.1.m1.1.1.2.cmml" xref="S4.SS4.1.m1.1.1.2">𝑆</ci><cn type="integer" id="S4.SS4.1.m1.1.1.3.cmml" xref="S4.SS4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.1.m1.1d">S^{3}</annotation></semantics></math> Tokenizer</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To assess the <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><msup id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">𝒮</mi><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> tokenizer’s ability to preserve semantic information, we compared the recognition performance of the quantizer-augmented SenseVoice-L against its original version and the Whisper-Large V3 model.
The models underwent evaluation using the Common Voice zh-CN and en benchmarks, with the findings detailed in Table <a href="#S4.T9" title="Table 9 ‣ 4.4 Preserving Semantic Information by 𝑆³ Tokenizer ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.4" class="ltx_p">From the table, we can see that our <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><msup id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">𝒮</mi><mn id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> tokens demonstrate robust recognition performance in both the Chinese and English test sets. Notably, on the common_voice_zh-CN set, <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><msup id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">𝒮</mi><mn id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">𝒮</ci><cn type="integer" id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\mathcal{S}^{3}</annotation></semantics></math> tokens surpass the performance of the Whisper-Large V3 model, achieving a 4.14% relative reduction in error rate. This suggests a substantial correlation between <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><msup id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">𝒮</mi><mn id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">superscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">𝒮</ci><cn type="integer" id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">\mathcal{S}^{3}</annotation></semantics></math> tokens and semantic content.
It is worth noting that there is only a single codebook in the <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><msup id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">𝒮</mi><mn id="S4.SS4.p2.4.m4.1.1.3" xref="S4.SS4.p2.4.m4.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">superscript</csymbol><ci id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">𝒮</ci><cn type="integer" id="S4.SS4.p2.4.m4.1.1.3.cmml" xref="S4.SS4.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">\mathcal{S}^{3}</annotation></semantics></math> tokenizer with a dictionary size of 4,096 entries.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<div id="S4.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:363.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T9.1.1.1" class="ltx_tr">
<td id="S4.T9.1.1.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Whisper-L-V3</td>
<td id="S4.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">SenseVoice-L</td>
<td id="S4.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S4.T9.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.T9.1.1.1.1.m1.1a"><msup id="S4.T9.1.1.1.1.m1.1.1" xref="S4.T9.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T9.1.1.1.1.m1.1.1.2" xref="S4.T9.1.1.1.1.m1.1.1.2.cmml">𝒮</mi><mn id="S4.T9.1.1.1.1.m1.1.1.3" xref="S4.T9.1.1.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.1.m1.1b"><apply id="S4.T9.1.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T9.1.1.1.1.m1.1.1.1.cmml" xref="S4.T9.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T9.1.1.1.1.m1.1.1.2.cmml" xref="S4.T9.1.1.1.1.m1.1.1.2">𝒮</ci><cn type="integer" id="S4.T9.1.1.1.1.m1.1.1.3.cmml" xref="S4.T9.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.1.m1.1c">\mathcal{S}^{3}</annotation></semantics></math> tokens</td>
</tr>
<tr id="S4.T9.1.1.2" class="ltx_tr">
<td id="S4.T9.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r">Test set</td>
<td id="S4.T9.1.1.2.2" class="ltx_td ltx_align_center">w/o lid</td>
<td id="S4.T9.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r">w/ lid</td>
<td id="S4.T9.1.1.2.4" class="ltx_td ltx_align_center">w/o lid</td>
<td id="S4.T9.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r">w/ lid</td>
<td id="S4.T9.1.1.2.6" class="ltx_td ltx_align_center">w/o lid</td>
<td id="S4.T9.1.1.2.7" class="ltx_td ltx_align_center">w/ lid</td>
</tr>
<tr id="S4.T9.1.1.3" class="ltx_tr">
<td id="S4.T9.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">common_voice_zh-CN</td>
<td id="S4.T9.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">12.82</td>
<td id="S4.T9.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.55</td>
<td id="S4.T9.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">8.76</td>
<td id="S4.T9.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.68</td>
<td id="S4.T9.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">12.24</td>
<td id="S4.T9.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">12.06</td>
</tr>
<tr id="S4.T9.1.1.4" class="ltx_tr">
<td id="S4.T9.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">common_voice_en</td>
<td id="S4.T9.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb">13.55</td>
<td id="S4.T9.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">9.39</td>
<td id="S4.T9.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb">9.79</td>
<td id="S4.T9.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">9.77</td>
<td id="S4.T9.1.1.4.6" class="ltx_td ltx_align_center ltx_border_bb">15.43</td>
<td id="S4.T9.1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb">15.38</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>The evaluation on <math id="S4.T9.3.m1.1" class="ltx_Math" alttext="\mathcal{S}^{3}" display="inline"><semantics id="S4.T9.3.m1.1b"><msup id="S4.T9.3.m1.1.1" xref="S4.T9.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T9.3.m1.1.1.2" xref="S4.T9.3.m1.1.1.2.cmml">𝒮</mi><mn id="S4.T9.3.m1.1.1.3" xref="S4.T9.3.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T9.3.m1.1c"><apply id="S4.T9.3.m1.1.1.cmml" xref="S4.T9.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T9.3.m1.1.1.1.cmml" xref="S4.T9.3.m1.1.1">superscript</csymbol><ci id="S4.T9.3.m1.1.1.2.cmml" xref="S4.T9.3.m1.1.1.2">𝒮</ci><cn type="integer" id="S4.T9.3.m1.1.1.3.cmml" xref="S4.T9.3.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.m1.1d">\mathcal{S}^{3}</annotation></semantics></math> tokens’ capability to preserve semantic information. We employ character and word error rates for zh-CN and en languages on the Common Voice benchmarks. Please note that the SenseVoice-L model in this table is an intermediate version, and is not identical to the one presented in Table <a href="#S4.T6" title="Table 6 ‣ 4.1 Multilingual Speech Recognition ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Evaluation on Generation Quality of CosyVoice</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We evaluate the quality of CosyVoice’s speech synthesis by examining content consistency and speaker similarity.
The “test-clean” subset of LibriTTS <cite class="ltx_cite ltx_citemacro_citep">(Zen et al., <a href="#bib.bib59" title="" class="ltx_ref">2019</a>)</cite> and the test set of AISHELL-3 <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite> are employed to construct evaluation set for English and Chinese, respectively. For each text in these sets, we randomly select a prompt speech. Content consistency was evaluated using Whisper-Large V3 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite> for English and Paraformer <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> for Chinese recognition. Speaker similarity was quantified by calculating the cosine similarity between speaker embeddings of the generated and prompt speeches, extracted using ERes2Net <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib11" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Similar to other autoregressive language models, we employ a random sampling decoding strategy for our token LM and assessed the synthesis process using five different random seed values: 0, 7, 42, 123, and 1,337. The resultant evaluation metrics were averaged to determine the mean and standard deviation. Additionally, we conducted an ASR re-ranking to demonstrate potential performance improvements in offline mode.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Tables <a href="#S4.T10" title="Table 10 ‣ 4.5 Evaluation on Generation Quality of CosyVoice ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and <a href="#S4.T11" title="Table 11 ‣ 4.5 Evaluation on Generation Quality of CosyVoice ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> present the results for English and Chinese, respectively. On the English dataset, CosyVoice attained human-level performance with similar content recognition and higher speaker similarity. ASR re-ranking notably enhanced content consistency, yielding a reduced word error rate (WER) of 1.51%. CosyVoice outperformed ChatTTS in WER and the number of insertion and deletion errors, indicating superior content consistency. We did not assess speaker similarity for ChatTTS as it doesn’t release voice cloning capabilities.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<div id="S4.T10.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:347.2pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T10.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.4.4.5" class="ltx_tr">
<td id="S4.T10.4.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Model</td>
<td id="S4.T10.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">WER (%)</td>
<td id="S4.T10.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">#Ins.&amp;Del.</td>
<td id="S4.T10.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">SS</td>
</tr>
<tr id="S4.T10.4.4.6" class="ltx_tr">
<td id="S4.T10.4.4.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">Original</td>
<td id="S4.T10.4.4.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">2.66</td>
<td id="S4.T10.4.4.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">92</td>
<td id="S4.T10.4.4.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">69.67</td>
</tr>
<tr id="S4.T10.4.4.7" class="ltx_tr">
<td id="S4.T10.4.4.7.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">ChatTTS</td>
<td id="S4.T10.4.4.7.2" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">8.32</td>
<td id="S4.T10.4.4.7.3" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">441</td>
<td id="S4.T10.4.4.7.4" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">-</td>
</tr>
<tr id="S4.T10.3.3.3" class="ltx_tr">
<td id="S4.T10.3.3.3.4" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">CosyVoice</td>
<td id="S4.T10.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">2.89<math id="S4.T10.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T10.1.1.1.1.m1.1a"><mo id="S4.T10.1.1.1.1.m1.1.1" xref="S4.T10.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T10.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T10.1.1.1.1.m1.1.1.cmml" xref="S4.T10.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.18</td>
<td id="S4.T10.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">88.60<math id="S4.T10.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T10.2.2.2.2.m1.1a"><mo id="S4.T10.2.2.2.2.m1.1.1" xref="S4.T10.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T10.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T10.2.2.2.2.m1.1.1.cmml" xref="S4.T10.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.2.2.2.2.m1.1c">\pm</annotation></semantics></math>3.88</td>
<td id="S4.T10.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">74.30<math id="S4.T10.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T10.3.3.3.3.m1.1a"><mo id="S4.T10.3.3.3.3.m1.1.1" xref="S4.T10.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T10.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T10.3.3.3.3.m1.1.1.cmml" xref="S4.T10.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.3.3.3.3.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S4.T10.4.4.4" class="ltx_tr">
<td id="S4.T10.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">       + 5<math id="S4.T10.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T10.4.4.4.1.m1.1a"><mo id="S4.T10.4.4.4.1.m1.1.1" xref="S4.T10.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T10.4.4.4.1.m1.1b"><times id="S4.T10.4.4.4.1.m1.1.1.cmml" xref="S4.T10.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.4.4.4.1.m1.1c">\times</annotation></semantics></math> re-ranking</td>
<td id="S4.T10.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">1.51</td>
<td id="S4.T10.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">47</td>
<td id="S4.T10.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">74.30</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>The comparison of original and CosyVoice generated speeches on the LibriTTS test-clean set in terms of word error rate (WER) and speaker similarity (SS). “<math id="S4.T10.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T10.6.m1.1b"><mo id="S4.T10.6.m1.1.1" xref="S4.T10.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T10.6.m1.1c"><csymbol cd="latexml" id="S4.T10.6.m1.1.1.cmml" xref="S4.T10.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.6.m1.1d">\pm</annotation></semantics></math>” joins the mean and standard deviation for each evaluation metric.</figcaption>
</figure>
<figure id="S4.T11" class="ltx_table">
<div id="S4.T11.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.6pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T11.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T11.4.4.5" class="ltx_tr">
<td id="S4.T11.4.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Model</td>
<td id="S4.T11.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">CER (%)</td>
<td id="S4.T11.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">#Ins.&amp;Del.</td>
<td id="S4.T11.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">SS</td>
</tr>
<tr id="S4.T11.4.4.6" class="ltx_tr">
<td id="S4.T11.4.4.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">Original</td>
<td id="S4.T11.4.4.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">2.52</td>
<td id="S4.T11.4.4.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">25</td>
<td id="S4.T11.4.4.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">74.15</td>
</tr>
<tr id="S4.T11.4.4.7" class="ltx_tr">
<td id="S4.T11.4.4.7.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">ChatTTS</td>
<td id="S4.T11.4.4.7.2" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">3.87</td>
<td id="S4.T11.4.4.7.3" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">111</td>
<td id="S4.T11.4.4.7.4" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">-</td>
</tr>
<tr id="S4.T11.3.3.3" class="ltx_tr">
<td id="S4.T11.3.3.3.4" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">CosyVoice</td>
<td id="S4.T11.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">3.82<math id="S4.T11.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T11.1.1.1.1.m1.1a"><mo id="S4.T11.1.1.1.1.m1.1.1" xref="S4.T11.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T11.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T11.1.1.1.1.m1.1.1.cmml" xref="S4.T11.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T11.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T11.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">24.4<math id="S4.T11.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T11.2.2.2.2.m1.1a"><mo id="S4.T11.2.2.2.2.m1.1.1" xref="S4.T11.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T11.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T11.2.2.2.2.m1.1.1.cmml" xref="S4.T11.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T11.2.2.2.2.m1.1c">\pm</annotation></semantics></math>2.24</td>
<td id="S4.T11.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:15.0pt;padding-right:15.0pt;">81.58<math id="S4.T11.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T11.3.3.3.3.m1.1a"><mo id="S4.T11.3.3.3.3.m1.1.1" xref="S4.T11.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T11.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T11.3.3.3.3.m1.1.1.cmml" xref="S4.T11.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T11.3.3.3.3.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
<tr id="S4.T11.4.4.4" class="ltx_tr">
<td id="S4.T11.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">       + 5<math id="S4.T11.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T11.4.4.4.1.m1.1a"><mo id="S4.T11.4.4.4.1.m1.1.1" xref="S4.T11.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T11.4.4.4.1.m1.1b"><times id="S4.T11.4.4.4.1.m1.1.1.cmml" xref="S4.T11.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T11.4.4.4.1.m1.1c">\times</annotation></semantics></math> re-ranking</td>
<td id="S4.T11.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">1.84</td>
<td id="S4.T11.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">11</td>
<td id="S4.T11.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">81.58</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>The comparison of original and CosyVoice generated speeches on the AISHELL-3 test set in terms of character error rate (CER) and speaker similarity (SS). “<math id="S4.T11.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T11.6.m1.1b"><mo id="S4.T11.6.m1.1.1" xref="S4.T11.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T11.6.m1.1c"><csymbol cd="latexml" id="S4.T11.6.m1.1.1.cmml" xref="S4.T11.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T11.6.m1.1d">\pm</annotation></semantics></math>” joins the mean and standard deviation for each evaluation metric.</figcaption>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">As for the results on Chinese, the generated utterances of CosyVoice achieves a comparable CER as well as the errors of insertion and deletion compared with the original utterances. It seems that ChatTTS has a better generation ability on Chinese than English in terms of CER. Although ChatTTS and CosyVoice achieves a similar CER, ChatTTS produces more insertion and deletion errors, This is due to the problem of speaker leaking, where modal particles of another speaker is generated unexpectedly. On the contrary, CosyVoice doesn’t suffer this problem with much less insertion and deletion errors. With ASR re-ranking, CosyVoice reached a remarkably low CER of 1.84%. As seen with English, CosyVoice also exhibited greater speaker similarity than the original utterances, showcasing its effective voice-cloning proficiency.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Evaluation on Emotion Controllability of CosyVoice</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">To verify the emotion controllability, we use the public speech emotion recognition model emo2vec<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://modelscope.cn/models/iic/emotion2vec_base_finetuned" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://modelscope.cn/models/iic/emotion2vec_base_finetuned</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib33" title="" class="ltx_ref">2024b</a>)</cite>. We generate and evaluate 100 English utterances for each of the six emotions: happy, angry, sad, surprised, fearful, and disgusted. The content of the synthesized text is designed to match the target emotion. We then measure the accuracy of the predicted emotions from the synthesized speech for each emotion.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.2" class="ltx_p">Table <a href="#S4.T12" title="Table 12 ‣ 4.6 Evaluation on Emotion Controllability of CosyVoice ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the comparison of emotion control accuracy between CosyVoice-base and CosyVoice-instruct. For CosyVoice-instruct, the input consists of content text accompanied by a speaking style instruction (e.g., “Happy.<math id="S4.SS6.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS6.p2.1.m1.1a"><mo id="S4.SS6.p2.1.m1.1.1" xref="S4.SS6.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><lt id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">&lt;</annotation></semantics></math>endofprompt<math id="S4.SS6.p2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS6.p2.2.m2.1a"><mo id="S4.SS6.p2.2.m2.1.1" xref="S4.SS6.p2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.1b"><gt id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.1c">&gt;</annotation></semantics></math>Content Text”). In contrast, CosyVoice-base only receives the content text as input.
The results indicate that CosyVoice-instruct with emotional instructions demonstrates a significant improvement over both CosyVoice-base and CosyVoice-instruct without emotional instructions.</p>
</div>
<figure id="S4.T12" class="ltx_table">
<div id="S4.T12.18" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:392.5pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.8pt,3.6pt) scale(0.9,0.9) ;">
<table id="S4.T12.18.18" class="ltx_tabular ltx_align_middle">
<tr id="S4.T12.18.18.19" class="ltx_tr">
<td id="S4.T12.18.18.19.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S4.T12.18.18.19.2" class="ltx_td ltx_align_center ltx_border_tt">Happy</td>
<td id="S4.T12.18.18.19.3" class="ltx_td ltx_align_center ltx_border_tt">Sad</td>
<td id="S4.T12.18.18.19.4" class="ltx_td ltx_align_center ltx_border_tt">Angry</td>
<td id="S4.T12.18.18.19.5" class="ltx_td ltx_align_center ltx_border_tt">Surprised</td>
<td id="S4.T12.18.18.19.6" class="ltx_td ltx_align_center ltx_border_tt">Fearful</td>
<td id="S4.T12.18.18.19.7" class="ltx_td ltx_align_center ltx_border_tt">Disgusted</td>
</tr>
<tr id="S4.T12.6.6.6" class="ltx_tr">
<td id="S4.T12.6.6.6.7" class="ltx_td ltx_align_left ltx_border_t">CosyVoice-base</td>
<td id="S4.T12.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">1.00<math id="S4.T12.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.1.1.1.1.m1.1a"><mo id="S4.T12.1.1.1.1.m1.1.1" xref="S4.T12.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T12.1.1.1.1.m1.1.1.cmml" xref="S4.T12.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="S4.T12.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.45<math id="S4.T12.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.2.2.2.2.m1.1a"><mo id="S4.T12.2.2.2.2.m1.1.1" xref="S4.T12.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T12.2.2.2.2.m1.1.1.cmml" xref="S4.T12.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.05</td>
<td id="S4.T12.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.59<math id="S4.T12.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.3.3.3.3.m1.1a"><mo id="S4.T12.3.3.3.3.m1.1.1" xref="S4.T12.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T12.3.3.3.3.m1.1.1.cmml" xref="S4.T12.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.3.3.3.3.m1.1c">\pm</annotation></semantics></math>0.03</td>
<td id="S4.T12.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.26<math id="S4.T12.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.4.4.4.4.m1.1a"><mo id="S4.T12.4.4.4.4.m1.1.1" xref="S4.T12.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S4.T12.4.4.4.4.m1.1.1.cmml" xref="S4.T12.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.4.4.4.4.m1.1c">\pm</annotation></semantics></math>0.02</td>
<td id="S4.T12.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.88<math id="S4.T12.5.5.5.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.5.5.5.5.m1.1a"><mo id="S4.T12.5.5.5.5.m1.1.1" xref="S4.T12.5.5.5.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.5.5.5.5.m1.1b"><csymbol cd="latexml" id="S4.T12.5.5.5.5.m1.1.1.cmml" xref="S4.T12.5.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.5.5.5.5.m1.1c">\pm</annotation></semantics></math>0.01</td>
<td id="S4.T12.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t">0.46<math id="S4.T12.6.6.6.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.6.6.6.6.m1.1a"><mo id="S4.T12.6.6.6.6.m1.1.1" xref="S4.T12.6.6.6.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.6.6.6.6.m1.1b"><csymbol cd="latexml" id="S4.T12.6.6.6.6.m1.1.1.cmml" xref="S4.T12.6.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.6.6.6.6.m1.1c">\pm</annotation></semantics></math>0.06</td>
</tr>
<tr id="S4.T12.12.12.12" class="ltx_tr">
<td id="S4.T12.12.12.12.7" class="ltx_td ltx_align_left">CosyVoice-instruct</td>
<td id="S4.T12.7.7.7.1" class="ltx_td ltx_align_center">1.00<math id="S4.T12.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.7.7.7.1.m1.1a"><mo id="S4.T12.7.7.7.1.m1.1.1" xref="S4.T12.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T12.7.7.7.1.m1.1.1.cmml" xref="S4.T12.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.7.7.7.1.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="S4.T12.8.8.8.2" class="ltx_td ltx_align_center">0.98<math id="S4.T12.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.8.8.8.2.m1.1a"><mo id="S4.T12.8.8.8.2.m1.1.1" xref="S4.T12.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T12.8.8.8.2.m1.1.1.cmml" xref="S4.T12.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.8.8.8.2.m1.1c">\pm</annotation></semantics></math>0.02</td>
<td id="S4.T12.9.9.9.3" class="ltx_td ltx_align_center">0.83<math id="S4.T12.9.9.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.9.9.9.3.m1.1a"><mo id="S4.T12.9.9.9.3.m1.1.1" xref="S4.T12.9.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.9.9.9.3.m1.1b"><csymbol cd="latexml" id="S4.T12.9.9.9.3.m1.1.1.cmml" xref="S4.T12.9.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.9.9.9.3.m1.1c">\pm</annotation></semantics></math>0.04</td>
<td id="S4.T12.10.10.10.4" class="ltx_td ltx_align_center">0.64<math id="S4.T12.10.10.10.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.10.10.10.4.m1.1a"><mo id="S4.T12.10.10.10.4.m1.1.1" xref="S4.T12.10.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.10.10.10.4.m1.1b"><csymbol cd="latexml" id="S4.T12.10.10.10.4.m1.1.1.cmml" xref="S4.T12.10.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.10.10.10.4.m1.1c">\pm</annotation></semantics></math>0.03</td>
<td id="S4.T12.11.11.11.5" class="ltx_td ltx_align_center">0.87<math id="S4.T12.11.11.11.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.11.11.11.5.m1.1a"><mo id="S4.T12.11.11.11.5.m1.1.1" xref="S4.T12.11.11.11.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.11.11.11.5.m1.1b"><csymbol cd="latexml" id="S4.T12.11.11.11.5.m1.1.1.cmml" xref="S4.T12.11.11.11.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.11.11.11.5.m1.1c">\pm</annotation></semantics></math>0.03</td>
<td id="S4.T12.12.12.12.6" class="ltx_td ltx_align_center">0.93<math id="S4.T12.12.12.12.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.12.12.12.6.m1.1a"><mo id="S4.T12.12.12.12.6.m1.1.1" xref="S4.T12.12.12.12.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.12.12.12.6.m1.1b"><csymbol cd="latexml" id="S4.T12.12.12.12.6.m1.1.1.cmml" xref="S4.T12.12.12.12.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.12.12.12.6.m1.1c">\pm</annotation></semantics></math>0.02</td>
</tr>
<tr id="S4.T12.18.18.18" class="ltx_tr">
<td id="S4.T12.18.18.18.7" class="ltx_td ltx_align_left ltx_border_bb">w/o instruction</td>
<td id="S4.T12.13.13.13.1" class="ltx_td ltx_align_center ltx_border_bb">0.98<math id="S4.T12.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.13.13.13.1.m1.1a"><mo id="S4.T12.13.13.13.1.m1.1.1" xref="S4.T12.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T12.13.13.13.1.m1.1.1.cmml" xref="S4.T12.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.13.13.13.1.m1.1c">\pm</annotation></semantics></math>0.01</td>
<td id="S4.T12.14.14.14.2" class="ltx_td ltx_align_center ltx_border_bb">0.77<math id="S4.T12.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.14.14.14.2.m1.1a"><mo id="S4.T12.14.14.14.2.m1.1.1" xref="S4.T12.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T12.14.14.14.2.m1.1.1.cmml" xref="S4.T12.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.14.14.14.2.m1.1c">\pm</annotation></semantics></math>0.04</td>
<td id="S4.T12.15.15.15.3" class="ltx_td ltx_align_center ltx_border_bb">0.49<math id="S4.T12.15.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.15.15.15.3.m1.1a"><mo id="S4.T12.15.15.15.3.m1.1.1" xref="S4.T12.15.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.15.15.15.3.m1.1b"><csymbol cd="latexml" id="S4.T12.15.15.15.3.m1.1.1.cmml" xref="S4.T12.15.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.15.15.15.3.m1.1c">\pm</annotation></semantics></math>0.12</td>
<td id="S4.T12.16.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">0.28<math id="S4.T12.16.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.16.16.16.4.m1.1a"><mo id="S4.T12.16.16.16.4.m1.1.1" xref="S4.T12.16.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.16.16.16.4.m1.1b"><csymbol cd="latexml" id="S4.T12.16.16.16.4.m1.1.1.cmml" xref="S4.T12.16.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.16.16.16.4.m1.1c">\pm</annotation></semantics></math>0.06</td>
<td id="S4.T12.17.17.17.5" class="ltx_td ltx_align_center ltx_border_bb">0.83<math id="S4.T12.17.17.17.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.17.17.17.5.m1.1a"><mo id="S4.T12.17.17.17.5.m1.1.1" xref="S4.T12.17.17.17.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.17.17.17.5.m1.1b"><csymbol cd="latexml" id="S4.T12.17.17.17.5.m1.1.1.cmml" xref="S4.T12.17.17.17.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.17.17.17.5.m1.1c">\pm</annotation></semantics></math>0.04</td>
<td id="S4.T12.18.18.18.6" class="ltx_td ltx_align_center ltx_border_bb">0.45<math id="S4.T12.18.18.18.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.18.18.18.6.m1.1a"><mo id="S4.T12.18.18.18.6.m1.1.1" xref="S4.T12.18.18.18.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.18.18.18.6.m1.1b"><csymbol cd="latexml" id="S4.T12.18.18.18.6.m1.1.1.cmml" xref="S4.T12.18.18.18.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.18.18.18.6.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Comparison of emotion control accuracy between CosyVoice-base and CosyVoice-instruct. “<math id="S4.T12.20.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T12.20.m1.1b"><mo id="S4.T12.20.m1.1.1" xref="S4.T12.20.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T12.20.m1.1c"><csymbol cd="latexml" id="S4.T12.20.m1.1.1.cmml" xref="S4.T12.20.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T12.20.m1.1d">\pm</annotation></semantics></math>” joins the mean and standard deviation for each evaluation metric.</figcaption>
</figure>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>CosyVoice as a Data Generator</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">A straightforward application of CosyVoice is as a data generator to augment the training data of other tasks, such as ASR, speech-to-speech translation (S2ST). Taking the ASR task an example, we conduct an experiment on the Librispeech corpus to evaluate CosyVoice’s capability in generating high-quality data. The experimental results are shown in Table <a href="#S4.T13" title="Table 13 ‣ 4.7 CosyVoice as a Data Generator ‣ 4 Experimental Results ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, where “Librispeech” denotes the original 960-hour data. “Syn on LS text” and “Syn on LS text” denote the generated data with the text from Librispeech and MLS training sets, respectively. From the table, we can see that only training on the synthesized data, the ASR model can achieve a comparable result than the original Librispeech training set. Upon integration of them, a notable enhancement in recognition accuracy is observed. An interesting finding is that involving the synthesized data on the MLS text significantly improve the recognition performance. This may indicates that the text diversity is more critical for ASR task than the duration of speech itself. This improvement can be attributed to the varied linguistic content introduced by CosyVoice synthesized samples. The findings from our evaluation underscore the high quality of the samples generated by CosyVoice.</p>
</div>
<figure id="S4.T13" class="ltx_table">
<div id="S4.T13.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:336.4pt;height:97.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.7pt,5.4pt) scale(0.9,0.9) ;">
<table id="S4.T13.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T13.1.1.2" class="ltx_tr">
<td id="S4.T13.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Training Data</td>
<td id="S4.T13.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">dev_clean</td>
<td id="S4.T13.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">dev_other</td>
<td id="S4.T13.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">test_clean</td>
<td id="S4.T13.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">test_other</td>
</tr>
<tr id="S4.T13.1.1.3" class="ltx_tr">
<td id="S4.T13.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Librispeech</td>
<td id="S4.T13.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.77</td>
<td id="S4.T13.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5.84</td>
<td id="S4.T13.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.79</td>
<td id="S4.T13.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5.97</td>
</tr>
<tr id="S4.T13.1.1.4" class="ltx_tr">
<td id="S4.T13.1.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Syn on LS text</td>
<td id="S4.T13.1.1.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.79</td>
<td id="S4.T13.1.1.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.37</td>
<td id="S4.T13.1.1.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.00</td>
<td id="S4.T13.1.1.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.59</td>
</tr>
<tr id="S4.T13.1.1.5" class="ltx_tr">
<td id="S4.T13.1.1.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Librispeech + Syn on LS text</td>
<td id="S4.T13.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.44</td>
<td id="S4.T13.1.1.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.52</td>
<td id="S4.T13.1.1.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.56</td>
<td id="S4.T13.1.1.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.68</td>
</tr>
<tr id="S4.T13.1.1.1" class="ltx_tr">
<td id="S4.T13.1.1.1.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Librispeech + Syn on LS text <math id="S4.T13.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S4.T13.1.1.1.1.m1.1a"><mrow id="S4.T13.1.1.1.1.m1.1.1" xref="S4.T13.1.1.1.1.m1.1.1.cmml"><mi id="S4.T13.1.1.1.1.m1.1.1.2" xref="S4.T13.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T13.1.1.1.1.m1.1.1.1" xref="S4.T13.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T13.1.1.1.1.m1.1.1.3" xref="S4.T13.1.1.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T13.1.1.1.1.m1.1b"><apply id="S4.T13.1.1.1.1.m1.1.1.cmml" xref="S4.T13.1.1.1.1.m1.1.1"><times id="S4.T13.1.1.1.1.m1.1.1.1.cmml" xref="S4.T13.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T13.1.1.1.1.m1.1.1.2.cmml" xref="S4.T13.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T13.1.1.1.1.m1.1.1.3.cmml" xref="S4.T13.1.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T13.1.1.1.1.m1.1c">\times 2</annotation></semantics></math>
</td>
<td id="S4.T13.1.1.1.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.51</td>
<td id="S4.T13.1.1.1.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.23</td>
<td id="S4.T13.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.68</td>
<td id="S4.T13.1.1.1.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.26</td>
</tr>
<tr id="S4.T13.1.1.6" class="ltx_tr">
<td id="S4.T13.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">Librispeech + Syn on LS, MLS text</td>
<td id="S4.T13.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T13.1.1.6.2.1" class="ltx_text ltx_font_bold">1.93</span></td>
<td id="S4.T13.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T13.1.1.6.3.1" class="ltx_text ltx_font_bold">4.43</span></td>
<td id="S4.T13.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T13.1.1.6.4.1" class="ltx_text ltx_font_bold">2.04</span></td>
<td id="S4.T13.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T13.1.1.6.5.1" class="ltx_text ltx_font_bold">4.53</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Evaluation on CosyVoice generation quality by treating it as a data generator. Word error rates (%) on the human-uttered test sets are employed as the evaluation metrics.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The FunAudioLLM is an innovative framework designed to facilitate natural voice interactions between humans and large language models (LLMs). By integrating SenseVoice, CosyVoice, and LLMs, FunAudioLLM offers a variety of rich application demos, including speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration. The demos are available at <a target="_blank" href="https://fun-audio-llm.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fun-audio-llm.github.io</a>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">By combining SenseVoice, LLMs, and CosyVoice, we can effortlessly perform speech-to-speech translation (S2ST), as illustrated in Figure <a href="#S5.F10" title="Figure 10 ‣ 5 Applications ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. SenseVoice is used to recognize the input speech in its original language, the LLM translates the source language to the target language, and CosyVoice synthesizes the target speech with cross-lingual voice cloning. This allows users to speak in foreign languages using their own voice.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2407.04051/assets/img/S2ST.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>A diagram of Speech-to-Speech Translation.</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">By integrating SenseVoice, LLMs, and CosyVoice, we can develop an Emotional Voice Chat application, as depicted in Figure <a href="#S5.F11" title="Figure 11 ‣ 5 Applications ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. SenseVoice recognizes the input speech and its emotion and audio event, the LLM generates the response content with a speaking style description, and CosyVoice produces emotional speech following the given speaking style description.</p>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2407.04051/assets/img/EmotionalVoiceChat.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>A diagram of Emotional Voice Chat.</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">By leveraging SenseVoice, an LLM-based multi-agent system with real-time world knowledge, and CosyVoice, we can create an interactive podcast, as shown in Figure <a href="#S5.F12" title="Figure 12 ‣ 5 Applications ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We can use an LLM plugin to fetch real-time daily knowledge, which a content-generation agent then transforms into a podcast script. The Multi-Agent system matches podcast roles, and CosyVoice synthesizes the voices. Users can also insert themselves into the podcast for interactive dialogues with the Multi-Agent system.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2407.04051/assets/img/InteractivePodcast.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A diagram of Interactive Podcast.</figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Through the analytical capabilities of LLMs to structure and identify emotions within books, and synthesizing this with CosyVoice, we achieve audiobooks with enhanced expressiveness, as illustrated in Figure <a href="#S5.F13" title="Figure 13 ‣ 5 Applications ‣ FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. The LLM is used for narrative and dialogue analysis, character analysis, and fine-grained sentiment analysis, while CosyVoice synthesizes the speech with enhanced expressiveness.</p>
</div>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2407.04051/assets/img/AudioBook.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A diagram of Expressive Audiobook.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">SenseVoice has certain limitations that need to be addressed. Firstly, the ASR performance generally remains much lower for under-resourced languages. Secondly, SenseVoice is not designed for streaming transcription. Therefore, future work may focus on developing streamable voice understanding models based on SenseVoice.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">CosyVoice also has several limitations. Firstly, it supports a limited number of languages. While it can express emotions and speaking styles based on explicit instructions, it cannot infer the appropriate emotion or style based on the semantic content of the text. Additionally, CosyVoice does not perform well when tasked with singing. There’s still room for improvement in achieving expressive emotional changes while maintaining the original timbre of the voice.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Another limitation is that the two innovative models within FunAudioLLM are not trained end-to-end with LLMs. This pipeline approach may introduce error propagation, which could affect overall performance.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Authors (alphabetical order of family name)</h2>

<div class="ltx_pagination ltx_role_start_3_columns"></div>
<div id="S7.p1" class="ltx_para">
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Keyu An</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Qian Chen</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">Chong Deng</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p">Zhihao Du</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p">Changfeng Gao</p>
</div>
</li>
<li id="S7.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i6.p1" class="ltx_para">
<p id="S7.I1.i6.p1.1" class="ltx_p">Zhifu Gao</p>
</div>
</li>
<li id="S7.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i7.p1" class="ltx_para">
<p id="S7.I1.i7.p1.1" class="ltx_p">Yue Gu</p>
</div>
</li>
<li id="S7.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i8.p1" class="ltx_para">
<p id="S7.I1.i8.p1.1" class="ltx_p">Ting He</p>
</div>
</li>
<li id="S7.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i9.p1" class="ltx_para">
<p id="S7.I1.i9.p1.1" class="ltx_p">Hangrui Hu</p>
</div>
</li>
<li id="S7.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i10.p1" class="ltx_para">
<p id="S7.I1.i10.p1.1" class="ltx_p">Kai Hu</p>
</div>
</li>
<li id="S7.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i11.p1" class="ltx_para">
<p id="S7.I1.i11.p1.1" class="ltx_p">Shengpeng Ji</p>
</div>
</li>
<li id="S7.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i12.p1" class="ltx_para">
<p id="S7.I1.i12.p1.1" class="ltx_p">Yabin Li</p>
</div>
</li>
<li id="S7.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i13.p1" class="ltx_para">
<p id="S7.I1.i13.p1.1" class="ltx_p">Zerui Li</p>
</div>
</li>
<li id="S7.I1.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i14.p1" class="ltx_para">
<p id="S7.I1.i14.p1.1" class="ltx_p">Heng Lu</p>
</div>
</li>
<li id="S7.I1.i15" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i15.p1" class="ltx_para">
<p id="S7.I1.i15.p1.1" class="ltx_p">Haoneng Luo</p>
</div>
</li>
<li id="S7.I1.i16" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i16.p1" class="ltx_para">
<p id="S7.I1.i16.p1.1" class="ltx_p">Xiang Lv</p>
</div>
</li>
<li id="S7.I1.i17" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i17.p1" class="ltx_para">
<p id="S7.I1.i17.p1.1" class="ltx_p">Bin Ma</p>
</div>
</li>
<li id="S7.I1.i18" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i18.p1" class="ltx_para">
<p id="S7.I1.i18.p1.1" class="ltx_p">Ziyang Ma</p>
</div>
</li>
<li id="S7.I1.i19" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i19.p1" class="ltx_para">
<p id="S7.I1.i19.p1.1" class="ltx_p">Chongjia Ni</p>
</div>
</li>
<li id="S7.I1.i20" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i20.p1" class="ltx_para">
<p id="S7.I1.i20.p1.1" class="ltx_p">Changhe Song</p>
</div>
</li>
<li id="S7.I1.i21" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i21.p1" class="ltx_para">
<p id="S7.I1.i21.p1.1" class="ltx_p">Jiaqi Shi</p>
</div>
</li>
<li id="S7.I1.i22" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i22.p1" class="ltx_para">
<p id="S7.I1.i22.p1.1" class="ltx_p">Xian Shi</p>
</div>
</li>
<li id="S7.I1.i23" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i23.p1" class="ltx_para">
<p id="S7.I1.i23.p1.1" class="ltx_p">Hao Wang</p>
</div>
</li>
<li id="S7.I1.i24" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i24.p1" class="ltx_para">
<p id="S7.I1.i24.p1.1" class="ltx_p">Wen Wang</p>
</div>
</li>
<li id="S7.I1.i25" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i25.p1" class="ltx_para">
<p id="S7.I1.i25.p1.1" class="ltx_p">Yuxuan Wang</p>
</div>
</li>
<li id="S7.I1.i26" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i26.p1" class="ltx_para">
<p id="S7.I1.i26.p1.1" class="ltx_p">Zhangyu Xiao</p>
</div>
</li>
<li id="S7.I1.i27" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i27.p1" class="ltx_para">
<p id="S7.I1.i27.p1.1" class="ltx_p">Zhijie Yan</p>
</div>
</li>
<li id="S7.I1.i28" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i28.p1" class="ltx_para">
<p id="S7.I1.i28.p1.1" class="ltx_p">Yexin Yang</p>
</div>
</li>
<li id="S7.I1.i29" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i29.p1" class="ltx_para">
<p id="S7.I1.i29.p1.1" class="ltx_p">Bin Zhang</p>
</div>
</li>
<li id="S7.I1.i30" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i30.p1" class="ltx_para">
<p id="S7.I1.i30.p1.1" class="ltx_p">Qinglin Zhang</p>
</div>
</li>
<li id="S7.I1.i31" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i31.p1" class="ltx_para">
<p id="S7.I1.i31.p1.1" class="ltx_p">Shiliang Zhang</p>
</div>
</li>
<li id="S7.I1.i32" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i32.p1" class="ltx_para">
<p id="S7.I1.i32.p1.1" class="ltx_p">Nan Zhao</p>
</div>
</li>
<li id="S7.I1.i33" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i33.p1" class="ltx_para">
<p id="S7.I1.i33.p1.1" class="ltx_p">Siqi Zheng</p>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_3_columns"></div>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgment</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We extend our heartfelt appreciation to the developers and contributors of the following open-source projects: FunASR, FunCodec, Whisper, ESPNet, WeNet, SLAM-LLM, Matcha-TTS, and Tortoise. Their innovative efforts and valuable code contributions have significantly inspired our work and facilitated our research. We are also grateful to numerous other projects not explicitly mentioned here, which have equally provided considerable assistance and played an instrumental role in the success of our endeavors.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2019)</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,
Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor
Weber.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06670</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023a)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,
Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.16609, 2023a.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023b)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang
Lin, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-vl: A frontier large vision-language model with versatile
abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.12966, 2023b.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berard et al. (2018)</span>
<span class="ltx_bibblock">
Alexandre Berard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier
Pietquin.

</span>
<span class="ltx_bibblock">End-to-end automatic speech translation of audiobooks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, pp.  6224–6228. IEEE, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bu et al. (2017)</span>
<span class="ltx_bibblock">
Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng.

</span>
<span class="ltx_bibblock">Aishell-1: An open-source mandarin speech corpus and a speech
recognition baseline.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2017 20th conference of the oriental chapter of the
international coordinating committee on speech databases and speech I/O
systems and assessment (O-COCOSDA)</em>, pp.  1–5. IEEE, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busso et al. (2008)</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe) Kazemzadeh,
Emily Mower Provost, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and
Shrikanth S. Narayanan.

</span>
<span class="ltx_bibblock">Iemocap: interactive emotional dyadic motion capture database.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, 42:335–359,
2008.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2014)</span>
<span class="ltx_bibblock">
Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur, Ani Nenkova,
and Ragini Verma.

</span>
<span class="ltx_bibblock">Crema-d: Crowd-sourced emotional multimodal actors dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, 5(4):377–390, 2014.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TAFFC.2014.2336244</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chalamandaris et al. (2014)</span>
<span class="ltx_bibblock">
Aimilios Chalamandaris, Pirros Tsiakoulis, Sotiris Karabetsos, and Spyros
Raptis.

</span>
<span class="ltx_bibblock">Using audio books for training a text-to-speech system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">LREC</em>, pp.  3076–3080. European Language Resources
Association (ELRA), 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Qian Chen, Mengzhe Chen, Bo Li, and Wen Wang.

</span>
<span class="ltx_bibblock">Controllable time-delay transformer for real-time punctuation
prediction and disfluency detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, pp.  8069–8073. IEEE, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen,
Wanxiang Che, Xiangzhan Yu, and Furu Wei.

</span>
<span class="ltx_bibblock">Beats: Audio pre-training with acoustic tokenizers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 202 of <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pp.  5178–5193. PMLR, 2023a.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, and Jiajun Qi.

</span>
<span class="ltx_bibblock">An enhanced res2net with local and global feature fusion for speaker
verification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>. ISCA, 2023b.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2023)</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang
Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified
large-scale audio-language models, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2018)</span>
<span class="ltx_bibblock">
Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu.

</span>
<span class="ltx_bibblock">Aishell-2: Transforming mandarin asr research into industrial scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.10583</em>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2024a)</span>
<span class="ltx_bibblock">
Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Yue Gu,
Ziyang Ma, and Zhijie Yan.

</span>
<span class="ltx_bibblock">Cosyvoice: A scalable multilingual zero-shot text-to-speech
synthesizer based on supervised semantic tokens.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arxiv</em>, 2024a.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2024b)</span>
<span class="ltx_bibblock">
Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.

</span>
<span class="ltx_bibblock">Funcodec: A fundamental, reproducible and integrable open-source
toolkit for neural speech codec.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, 2024b.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Défossez et al. (2022)</span>
<span class="ltx_bibblock">
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi.

</span>
<span class="ltx_bibblock">High fidelity neural audio compression.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv:2210.13438</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Zhifu Gao, Shiliang Zhang, Ming Lei, and Ian McLoughlin.

</span>
<span class="ltx_bibblock">SAN-M: memory equipped self-attention for end-to-end speech
recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">21st Annual Conference of the International Speech
Communication Association, Interspeech 2020, Virtual Event, Shanghai, China,
October 25-29, 2020</em>, pp.  6–10. ISCA, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan.

</span>
<span class="ltx_bibblock">Paraformer: Fast and accurate parallel transformer for
non-autoregressive end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, pp.  2063–2067. ISCA, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin
Li, Lingyun Zuo, Zhihao Du, Zhangyu Xiao, and Shiliang Zhang.

</span>
<span class="ltx_bibblock">Funasr: A fundamental end-to-end speech recognition toolkit.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino J. Gomez, and Jürgen
Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Machine Learning, Proceedings of the Twenty-Third
International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June
25-29, 2006</em>, volume 148, pp.  369–376. ACM, 2006.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho &amp; Salimans (2022)</span>
<span class="ltx_bibblock">
Jonathan Ho and Tim Salimans.

</span>
<span class="ltx_bibblock">Classifier-free diffusion guidance.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2207.12598, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
Salakhutdinov, and Abdelrahman Mohamed.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked
prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE ACM Trans. Audio Speech Lang. Process.</em>, 29:3451–3460, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Shengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan,
Baoxing Huai, and Zhou Zhao.

</span>
<span class="ltx_bibblock">Textrolspeech: A text style control speech corpus with codec
language text-to-speech models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.14430, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2024)</span>
<span class="ltx_bibblock">
Shengpeng Ji, Jialong Zuo, Minghui Fang, Siqi Zheng, Qian Chen, Wen Wang, Ziyue
Jiang, Hai Huang, Xize Cheng, Rongjie Huang, and Zhou Zhao.

</span>
<span class="ltx_bibblock">Controlspeech: Towards simultaneous zero-shot speaker cloning and
zero-shot language style control with decoupled codec.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2406.01205" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2406.01205</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanda et al. (2024)</span>
<span class="ltx_bibblock">
Naoyuki Kanda, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Hemin Yang,
Zirun Zhu, Min Tang, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Yufei Xia,
Jinzhu Li, Yanqing Liu, Sheng Zhao, and Michael Zeng.

</span>
<span class="ltx_bibblock">Making flow-matching-based zero-shot text-to-speech laugh as you
like.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2402.07383, 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. (2020)</span>
<span class="ltx_bibblock">
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D.
Plumbley.

</span>
<span class="ltx_bibblock">Panns: Large-scale pretrained audio neural networks for audio pattern
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 28:2880–2894, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TASLP.2020.3030497</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laban et al. (2022)</span>
<span class="ltx_bibblock">
Philippe Laban, Elicia Ye, Srujay Korlakunta, John F. Canny, and Marti A.
Hearst.

</span>
<span class="ltx_bibblock">Newspod: Automatic and interactive news podcasts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IUI</em>, pp.  691–706. ACM, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yinghao Aaron Li, Cong Han, Xilin Jiang, and Nima Mesgarani.

</span>
<span class="ltx_bibblock">Hiftnet: A fast high-quality neural vocoder with
harmonic-plus-noise filter and inverse short time fourier transform.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.09493, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al. (2023)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu,
Yu He, Ying Li, Jinming Zhao, Ye Liu, Bin Liu, Jiangyan Yi, Meng Wang, Erik
Cambria, Guoying Zhao, Björn W. Schuller, and Jianhua Tao.

</span>
<span class="ltx_bibblock">Mer 2023: Multi-label learning, modality robustness, and
semi-supervised learning, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al. (2024)</span>
<span class="ltx_bibblock">
Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and
Jianhua Tao.

</span>
<span class="ltx_bibblock">Merbench: A unified evaluation benchmark for multimodal emotion
recognition, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipman et al. (2023)</span>
<span class="ltx_bibblock">
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew
Le.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICLR</em>. OpenReview.net, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024a)</span>
<span class="ltx_bibblock">
Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li,
Jiaxin Ye, Xie Chen, and Thomas Hain.

</span>
<span class="ltx_bibblock">Emobox: Multilingual multi-corpus speech emotion recognition toolkit
and benchmark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2024a.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024b)</span>
<span class="ltx_bibblock">
Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang,
and Xie Chen.

</span>
<span class="ltx_bibblock">emotion2vec: Self-supervised pre-training for speech emotion
representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. ACL Findings</em>, 2024b.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez-Lucas et al. (2020)</span>
<span class="ltx_bibblock">
Luz Martinez-Lucas, Mohammed Abdelwahab, and Carlos Busso.

</span>
<span class="ltx_bibblock">The MSP-Conversation Corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pp.  1823–1827, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2023)</span>
<span class="ltx_bibblock">
Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, and Gustav Eje
Henter.

</span>
<span class="ltx_bibblock">Matcha-tts: A fast TTS architecture with conditional flow
matching.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.03199, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mesaros et al. (2021)</span>
<span class="ltx_bibblock">
Annamaria Mesaros, Toni Heittola, Tuomas Virtanen, and Mark D. Plumbley.

</span>
<span class="ltx_bibblock">Sound event detection: A tutorial.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Process. Mag.</em>, 38(5):67–83,
2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pp.  5206–5210. IEEE, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piczak (2015)</span>
<span class="ltx_bibblock">
Karol J. Piczak.

</span>
<span class="ltx_bibblock">ESC: dataset for environmental sound classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia</em>, pp.  1015–1018. ACM, 2015.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al. (2019)</span>
<span class="ltx_bibblock">
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik
Cambria, and Rada Mihalcea.

</span>
<span class="ltx_bibblock">MELD: A multimodal multi-party dataset for emotion recognition in
conversations.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez (eds.),
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pp.  527–536, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pusateri et al. (2017)</span>
<span class="ltx_bibblock">
Ernest Pusateri, Bharat Ram Ambati, Elizabeth Brooks, Ondrej Plátek,
Donald McAllaster, and Venki Nagesha.

</span>
<span class="ltx_bibblock">A mostly data-driven approach to inverse text normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, pp.  2784–2788. ISCA, 2017.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 202 of <em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pp.  28492–28518. PMLR, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P.
Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian
Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault
Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James
Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy,
Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica
Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen
Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand,
Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav
Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and
et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2403.05530, 2024.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2020)</span>
<span class="ltx_bibblock">
Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji, Srikanth Raj
Chetupalli, Nirmala R., Prasanta Kumar Ghosh, and Sriram Ganapathy.

</span>
<span class="ltx_bibblock">Coswara — a database of breathing, cough, and voice sounds for
covid-19 diagnosis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Interspeech 2020</em>. ISCA, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2024)</span>
<span class="ltx_bibblock">
Xian Shi, Yexin Yang, Zerui Li, Yanni Chen, Zhifu Gao, and Shiliang Zhang.

</span>
<span class="ltx_bibblock">Seaco-paraformer: A non-autoregressive asr system with flexible and
effective hotword customization ability.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pp.  10346–10350. IEEE, 2024.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2021)</span>
<span class="ltx_bibblock">
Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li.

</span>
<span class="ltx_bibblock">AISHELL-3: A multi-speaker mandarin TTS corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, pp.  2756–2760. ISCA, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimizu et al. (2023)</span>
<span class="ltx_bibblock">
Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi,
Tatsuya Komatsu, and Kentaro Tachibana.

</span>
<span class="ltx_bibblock">Prompttts++: Controlling speaker identity in prompt-based
text-to-speech using natural language descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.08140, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2022)</span>
<span class="ltx_bibblock">
Yookyung Shin, Younggun Lee, Suhee Jo, Yeongtae Hwang, and Taesu Kim.

</span>
<span class="ltx_bibblock">Text-driven emotional style control and cross-speaker style transfer
in neural TTS.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, pp.  2313–2317. ISCA, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu,
Zejun MA, and Chao Zhang.

</span>
<span class="ltx_bibblock">SALMONN: Towards generic hearing abilities for large language
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pp.  5998–6008, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo
Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei.

</span>
<span class="ltx_bibblock">Neural codec language models are zero-shot text to speech
synthesizers.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.02111, 2023a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, and Qian Chen.

</span>
<span class="ltx_bibblock">CAM++: A fast and efficient network for speaker verification
using context-aware masking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, pp.  5301–5305. ISCA,
2023b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du,
Jyh-Shing Roger Jang, Chi-Chun Lee, and Hung-Yi Lee.

</span>
<span class="ltx_bibblock">Emo-superb: An in-depth look at speech emotion recognition, 2024.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2024)</span>
<span class="ltx_bibblock">
Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Mengzhe Chen, Qian Chen,
and Lei Xie.

</span>
<span class="ltx_bibblock">E-chat: Emotion-sensitive spoken dialogue system with large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2401.00475, 2024.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and
Yuexian Zou.

</span>
<span class="ltx_bibblock">Hifi-codec: Group-residual vector quantization for high fidelity
audio codec.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.02765, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al. (2022)</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco
Tagliasacchi.

</span>
<span class="ltx_bibblock">Soundstream: An end-to-end neural audio codec.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE ACM Trans. Audio Speech Lang. Process.</em>, 30:495–507, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zen et al. (2019)</span>
<span class="ltx_bibblock">
Heiga Zen, Viet Dang, Rob Clark, and et al.

</span>
<span class="ltx_bibblock">Libritts: A corpus derived from librispeech for text-to-speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv:1904.02882</em>, 2019.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu,
Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al.

</span>
<span class="ltx_bibblock">Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech
recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pp.  6182–6186. IEEE, 2022.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Jia (2008)</span>
<span class="ltx_bibblock">
JTFLM Zhang and Huibin Jia.

</span>
<span class="ltx_bibblock">Design of speech corpus for mandarin text to speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">The blizzard challenge 2008 workshop</em>, 2008.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo
Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei.

</span>
<span class="ltx_bibblock">Speak foreign languages with your own voice: Cross-lingual neural
codec language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.03926, 2023.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2021)</span>
<span class="ltx_bibblock">
Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li.

</span>
<span class="ltx_bibblock">Seen and unseen emotional style transfer for voice conversion with a
new emotional speech dataset, 2021.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Auxiliary Results of SenseVoice on Common Voice.</h2>

<figure id="A1.T14" class="ltx_table">
<div id="A1.T14.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:226.0pt;height:576pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="A1.T14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T14.1.1.1" class="ltx_tr">
<td id="A1.T14.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T14.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="A1.T14.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Whisper-L-V3</td>
<td id="A1.T14.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SenseVoice-L</td>
</tr>
<tr id="A1.T14.1.1.2" class="ltx_tr">
<td id="A1.T14.1.1.2.1" class="ltx_td ltx_align_left">Language</td>
<td id="A1.T14.1.1.2.2" class="ltx_td"></td>
<td id="A1.T14.1.1.2.3" class="ltx_td ltx_align_center">w/o lid</td>
<td id="A1.T14.1.1.2.4" class="ltx_td ltx_align_center">w lid</td>
<td id="A1.T14.1.1.2.5" class="ltx_td ltx_align_center">w/o lid</td>
<td id="A1.T14.1.1.2.6" class="ltx_td ltx_align_center">w lid</td>
</tr>
<tr id="A1.T14.1.1.3" class="ltx_tr">
<td id="A1.T14.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">zh-CN</td>
<td id="A1.T14.1.1.3.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">12.82</td>
<td id="A1.T14.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">12.55</td>
<td id="A1.T14.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">7.92</td>
<td id="A1.T14.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">7.68</td>
</tr>
<tr id="A1.T14.1.1.4" class="ltx_tr">
<td id="A1.T14.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">en</td>
<td id="A1.T14.1.1.4.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">13.55</td>
<td id="A1.T14.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t">9.39</td>
<td id="A1.T14.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">14.30</td>
<td id="A1.T14.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t">9.00</td>
</tr>
<tr id="A1.T14.1.1.5" class="ltx_tr">
<td id="A1.T14.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">yue</td>
<td id="A1.T14.1.1.5.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">40.42</td>
<td id="A1.T14.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">10.51</td>
<td id="A1.T14.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">7.08</td>
<td id="A1.T14.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">6.78</td>
</tr>
<tr id="A1.T14.1.1.6" class="ltx_tr">
<td id="A1.T14.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t">ja</td>
<td id="A1.T14.1.1.6.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">11.18</td>
<td id="A1.T14.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">10.34</td>
<td id="A1.T14.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">9.58</td>
<td id="A1.T14.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t">9.19</td>
</tr>
<tr id="A1.T14.1.1.7" class="ltx_tr">
<td id="A1.T14.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">ko</td>
<td id="A1.T14.1.1.7.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t">5.59</td>
<td id="A1.T14.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t">5.59</td>
<td id="A1.T14.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t">5.23</td>
<td id="A1.T14.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t">5.21</td>
</tr>
<tr id="A1.T14.1.1.8" class="ltx_tr">
<td id="A1.T14.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">fr</td>
<td id="A1.T14.1.1.8.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t">11.13</td>
<td id="A1.T14.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t">10.77</td>
<td id="A1.T14.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t">8.67</td>
<td id="A1.T14.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t">8.45</td>
</tr>
<tr id="A1.T14.1.1.9" class="ltx_tr">
<td id="A1.T14.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">es</td>
<td id="A1.T14.1.1.9.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">5.00</td>
<td id="A1.T14.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">4.74</td>
<td id="A1.T14.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">5.37</td>
<td id="A1.T14.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">4.63</td>
</tr>
<tr id="A1.T14.1.1.10" class="ltx_tr">
<td id="A1.T14.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">it</td>
<td id="A1.T14.1.1.10.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t">5.93</td>
<td id="A1.T14.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t">5.46</td>
<td id="A1.T14.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t">5.74</td>
<td id="A1.T14.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t">5.16</td>
</tr>
<tr id="A1.T14.1.1.11" class="ltx_tr">
<td id="A1.T14.1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">ru</td>
<td id="A1.T14.1.1.11.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t">6.16</td>
<td id="A1.T14.1.1.11.4" class="ltx_td ltx_align_center ltx_border_t">5.67</td>
<td id="A1.T14.1.1.11.5" class="ltx_td ltx_align_center ltx_border_t">6.60</td>
<td id="A1.T14.1.1.11.6" class="ltx_td ltx_align_center ltx_border_t">5.23</td>
</tr>
<tr id="A1.T14.1.1.12" class="ltx_tr">
<td id="A1.T14.1.1.12.1" class="ltx_td ltx_align_left ltx_border_t">id</td>
<td id="A1.T14.1.1.12.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.12.3" class="ltx_td ltx_align_center ltx_border_t">8.98</td>
<td id="A1.T14.1.1.12.4" class="ltx_td ltx_align_center ltx_border_t">7.22</td>
<td id="A1.T14.1.1.12.5" class="ltx_td ltx_align_center ltx_border_t">12.80</td>
<td id="A1.T14.1.1.12.6" class="ltx_td ltx_align_center ltx_border_t">6.97</td>
</tr>
<tr id="A1.T14.1.1.13" class="ltx_tr">
<td id="A1.T14.1.1.13.1" class="ltx_td ltx_align_left ltx_border_t">th</td>
<td id="A1.T14.1.1.13.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.13.3" class="ltx_td ltx_align_center ltx_border_t">9.73</td>
<td id="A1.T14.1.1.13.4" class="ltx_td ltx_align_center ltx_border_t">5.80</td>
<td id="A1.T14.1.1.13.5" class="ltx_td ltx_align_center ltx_border_t">4.36</td>
<td id="A1.T14.1.1.13.6" class="ltx_td ltx_align_center ltx_border_t">4.12</td>
</tr>
<tr id="A1.T14.1.1.14" class="ltx_tr">
<td id="A1.T14.1.1.14.1" class="ltx_td ltx_align_left ltx_border_t">de</td>
<td id="A1.T14.1.1.14.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.14.3" class="ltx_td ltx_align_center ltx_border_t">6.06</td>
<td id="A1.T14.1.1.14.4" class="ltx_td ltx_align_center ltx_border_t">5.70</td>
<td id="A1.T14.1.1.14.5" class="ltx_td ltx_align_center ltx_border_t">6.94</td>
<td id="A1.T14.1.1.14.6" class="ltx_td ltx_align_center ltx_border_t">6.57</td>
</tr>
<tr id="A1.T14.1.1.15" class="ltx_tr">
<td id="A1.T14.1.1.15.1" class="ltx_td ltx_align_left ltx_border_t">ca</td>
<td id="A1.T14.1.1.15.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.15.3" class="ltx_td ltx_align_center ltx_border_t">16.76</td>
<td id="A1.T14.1.1.15.4" class="ltx_td ltx_align_center ltx_border_t">13.20</td>
<td id="A1.T14.1.1.15.5" class="ltx_td ltx_align_center ltx_border_t">5.90</td>
<td id="A1.T14.1.1.15.6" class="ltx_td ltx_align_center ltx_border_t">5.62</td>
</tr>
<tr id="A1.T14.1.1.16" class="ltx_tr">
<td id="A1.T14.1.1.16.1" class="ltx_td ltx_align_left ltx_border_t">nl</td>
<td id="A1.T14.1.1.16.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.16.3" class="ltx_td ltx_align_center ltx_border_t">5.51</td>
<td id="A1.T14.1.1.16.4" class="ltx_td ltx_align_center ltx_border_t">4.28</td>
<td id="A1.T14.1.1.16.5" class="ltx_td ltx_align_center ltx_border_t">6.65</td>
<td id="A1.T14.1.1.16.6" class="ltx_td ltx_align_center ltx_border_t">5.23</td>
</tr>
<tr id="A1.T14.1.1.17" class="ltx_tr">
<td id="A1.T14.1.1.17.1" class="ltx_td ltx_align_left ltx_border_t">pt</td>
<td id="A1.T14.1.1.17.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.17.3" class="ltx_td ltx_align_center ltx_border_t">6.90</td>
<td id="A1.T14.1.1.17.4" class="ltx_td ltx_align_center ltx_border_t">5.92</td>
<td id="A1.T14.1.1.17.5" class="ltx_td ltx_align_center ltx_border_t">9.05</td>
<td id="A1.T14.1.1.17.6" class="ltx_td ltx_align_center ltx_border_t">6.88</td>
</tr>
<tr id="A1.T14.1.1.18" class="ltx_tr">
<td id="A1.T14.1.1.18.1" class="ltx_td ltx_align_left ltx_border_t">pl</td>
<td id="A1.T14.1.1.18.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.18.3" class="ltx_td ltx_align_center ltx_border_t">7.26</td>
<td id="A1.T14.1.1.18.4" class="ltx_td ltx_align_center ltx_border_t">5.95</td>
<td id="A1.T14.1.1.18.5" class="ltx_td ltx_align_center ltx_border_t">10.01</td>
<td id="A1.T14.1.1.18.6" class="ltx_td ltx_align_center ltx_border_t">7.47</td>
</tr>
<tr id="A1.T14.1.1.19" class="ltx_tr">
<td id="A1.T14.1.1.19.1" class="ltx_td ltx_align_left ltx_border_t">cs</td>
<td id="A1.T14.1.1.19.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.19.3" class="ltx_td ltx_align_center ltx_border_t">10.99</td>
<td id="A1.T14.1.1.19.4" class="ltx_td ltx_align_center ltx_border_t">9.04</td>
<td id="A1.T14.1.1.19.5" class="ltx_td ltx_align_center ltx_border_t">11.45</td>
<td id="A1.T14.1.1.19.6" class="ltx_td ltx_align_center ltx_border_t">9.70</td>
</tr>
<tr id="A1.T14.1.1.20" class="ltx_tr">
<td id="A1.T14.1.1.20.1" class="ltx_td ltx_align_left ltx_border_t">hi</td>
<td id="A1.T14.1.1.20.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.20.3" class="ltx_td ltx_align_center ltx_border_t">46.17</td>
<td id="A1.T14.1.1.20.4" class="ltx_td ltx_align_center ltx_border_t">16.88</td>
<td id="A1.T14.1.1.20.5" class="ltx_td ltx_align_center ltx_border_t">48.85</td>
<td id="A1.T14.1.1.20.6" class="ltx_td ltx_align_center ltx_border_t">10.06</td>
</tr>
<tr id="A1.T14.1.1.21" class="ltx_tr">
<td id="A1.T14.1.1.21.1" class="ltx_td ltx_align_left ltx_border_t">tr</td>
<td id="A1.T14.1.1.21.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.21.3" class="ltx_td ltx_align_center ltx_border_t">14.65</td>
<td id="A1.T14.1.1.21.4" class="ltx_td ltx_align_center ltx_border_t">12.04</td>
<td id="A1.T14.1.1.21.5" class="ltx_td ltx_align_center ltx_border_t">14.10</td>
<td id="A1.T14.1.1.21.6" class="ltx_td ltx_align_center ltx_border_t">11.09</td>
</tr>
<tr id="A1.T14.1.1.22" class="ltx_tr">
<td id="A1.T14.1.1.22.1" class="ltx_td ltx_align_left ltx_border_t">ro</td>
<td id="A1.T14.1.1.22.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.22.3" class="ltx_td ltx_align_center ltx_border_t">13.43</td>
<td id="A1.T14.1.1.22.4" class="ltx_td ltx_align_center ltx_border_t">10.84</td>
<td id="A1.T14.1.1.22.5" class="ltx_td ltx_align_center ltx_border_t">18.21</td>
<td id="A1.T14.1.1.22.6" class="ltx_td ltx_align_center ltx_border_t">12.01</td>
</tr>
<tr id="A1.T14.1.1.23" class="ltx_tr">
<td id="A1.T14.1.1.23.1" class="ltx_td ltx_align_left ltx_border_t">hu</td>
<td id="A1.T14.1.1.23.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.23.3" class="ltx_td ltx_align_center ltx_border_t">13.89</td>
<td id="A1.T14.1.1.23.4" class="ltx_td ltx_align_center ltx_border_t">13.40</td>
<td id="A1.T14.1.1.23.5" class="ltx_td ltx_align_center ltx_border_t">12.53</td>
<td id="A1.T14.1.1.23.6" class="ltx_td ltx_align_center ltx_border_t">12.27</td>
</tr>
<tr id="A1.T14.1.1.24" class="ltx_tr">
<td id="A1.T14.1.1.24.1" class="ltx_td ltx_align_left ltx_border_t">da</td>
<td id="A1.T14.1.1.24.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.24.3" class="ltx_td ltx_align_center ltx_border_t">15.59</td>
<td id="A1.T14.1.1.24.4" class="ltx_td ltx_align_center ltx_border_t">12.49</td>
<td id="A1.T14.1.1.24.5" class="ltx_td ltx_align_center ltx_border_t">17.41</td>
<td id="A1.T14.1.1.24.6" class="ltx_td ltx_align_center ltx_border_t">13.23</td>
</tr>
<tr id="A1.T14.1.1.25" class="ltx_tr">
<td id="A1.T14.1.1.25.1" class="ltx_td ltx_align_left ltx_border_t">bg</td>
<td id="A1.T14.1.1.25.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.25.3" class="ltx_td ltx_align_center ltx_border_t">17.05</td>
<td id="A1.T14.1.1.25.4" class="ltx_td ltx_align_center ltx_border_t">14.24</td>
<td id="A1.T14.1.1.25.5" class="ltx_td ltx_align_center ltx_border_t">18.56</td>
<td id="A1.T14.1.1.25.6" class="ltx_td ltx_align_center ltx_border_t">13.25</td>
</tr>
<tr id="A1.T14.1.1.26" class="ltx_tr">
<td id="A1.T14.1.1.26.1" class="ltx_td ltx_align_left ltx_border_t">mr</td>
<td id="A1.T14.1.1.26.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.26.3" class="ltx_td ltx_align_center ltx_border_t">38.14</td>
<td id="A1.T14.1.1.26.4" class="ltx_td ltx_align_center ltx_border_t">31.13</td>
<td id="A1.T14.1.1.26.5" class="ltx_td ltx_align_center ltx_border_t">20.80</td>
<td id="A1.T14.1.1.26.6" class="ltx_td ltx_align_center ltx_border_t">13.51</td>
</tr>
<tr id="A1.T14.1.1.27" class="ltx_tr">
<td id="A1.T14.1.1.27.1" class="ltx_td ltx_align_left ltx_border_t">el</td>
<td id="A1.T14.1.1.27.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.27.3" class="ltx_td ltx_align_center ltx_border_t">15.58</td>
<td id="A1.T14.1.1.27.4" class="ltx_td ltx_align_center ltx_border_t">13.73</td>
<td id="A1.T14.1.1.27.5" class="ltx_td ltx_align_center ltx_border_t">25.39</td>
<td id="A1.T14.1.1.27.6" class="ltx_td ltx_align_center ltx_border_t">16.98</td>
</tr>
<tr id="A1.T14.1.1.28" class="ltx_tr">
<td id="A1.T14.1.1.28.1" class="ltx_td ltx_align_left ltx_border_t">uk</td>
<td id="A1.T14.1.1.28.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.28.3" class="ltx_td ltx_align_center ltx_border_t">15.89</td>
<td id="A1.T14.1.1.28.4" class="ltx_td ltx_align_center ltx_border_t">11.60</td>
<td id="A1.T14.1.1.28.5" class="ltx_td ltx_align_center ltx_border_t">12.43</td>
<td id="A1.T14.1.1.28.6" class="ltx_td ltx_align_center ltx_border_t">20.75</td>
</tr>
<tr id="A1.T14.1.1.29" class="ltx_tr">
<td id="A1.T14.1.1.29.1" class="ltx_td ltx_align_left ltx_border_t">az</td>
<td id="A1.T14.1.1.29.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.29.3" class="ltx_td ltx_align_center ltx_border_t">36.32</td>
<td id="A1.T14.1.1.29.4" class="ltx_td ltx_align_center ltx_border_t">25.21</td>
<td id="A1.T14.1.1.29.5" class="ltx_td ltx_align_center ltx_border_t">72.65</td>
<td id="A1.T14.1.1.29.6" class="ltx_td ltx_align_center ltx_border_t">28.63</td>
</tr>
<tr id="A1.T14.1.1.30" class="ltx_tr">
<td id="A1.T14.1.1.30.1" class="ltx_td ltx_align_left ltx_border_t">sw</td>
<td id="A1.T14.1.1.30.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.30.3" class="ltx_td ltx_align_center ltx_border_t">54.10</td>
<td id="A1.T14.1.1.30.4" class="ltx_td ltx_align_center ltx_border_t">50.43</td>
<td id="A1.T14.1.1.30.5" class="ltx_td ltx_align_center ltx_border_t">26.21</td>
<td id="A1.T14.1.1.30.6" class="ltx_td ltx_align_center ltx_border_t">25.85</td>
</tr>
<tr id="A1.T14.1.1.31" class="ltx_tr">
<td id="A1.T14.1.1.31.1" class="ltx_td ltx_align_left ltx_border_t">fa</td>
<td id="A1.T14.1.1.31.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T14.1.1.31.3" class="ltx_td ltx_align_center ltx_border_t">37.44</td>
<td id="A1.T14.1.1.31.4" class="ltx_td ltx_align_center ltx_border_t">34.86</td>
<td id="A1.T14.1.1.31.5" class="ltx_td ltx_align_center ltx_border_t">32.40</td>
<td id="A1.T14.1.1.31.6" class="ltx_td ltx_align_center ltx_border_t">40.33</td>
</tr>
<tr id="A1.T14.1.1.32" class="ltx_tr">
<td id="A1.T14.1.1.32.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">bn</td>
<td id="A1.T14.1.1.32.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="A1.T14.1.1.32.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">42.25</td>
<td id="A1.T14.1.1.32.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">40.15</td>
<td id="A1.T14.1.1.32.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">44.10</td>
<td id="A1.T14.1.1.32.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">43.80</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Performance comparisons among different models with and without language id.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.04050" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.04051" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.04051">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.04051" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.04052" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 16:52:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
