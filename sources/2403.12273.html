<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.12273] Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models</title><meta property="og:description" content="In this paper, we extended the method proposed in (Linus and Elmar, 2024) to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.12273">

<!--Generated on Fri Apr  5 18:08:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Human-robot interaction,  LLMs,  VLMs,  ROS,  foundation models,  natural language interaction,  vocal conversation.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linus Nwankwo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:linus.nwankwo@unileoben.ac.at">linus.nwankwo@unileoben.ac.at</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Chair of Cyber-Physical Systems, Montanuniversität</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_streetaddress">Franz Josef-Straße 18, 8700 Leoben</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_city">Leoben</span><span id="id7.4.id4" class="ltx_text ltx_affiliation_country">Austria</span><span id="id8.5.id5" class="ltx_text ltx_affiliation_postcode">8700</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elmar Rueckert
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Chair of Cyber-Physical Systems, Montanuniversität</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_streetaddress">Franz Josef-Straße 18, 8700 Leoben</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_city">Leoben</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_country">Austria</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id3.3" class="ltx_p">In this paper, we extended the method proposed in <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite> to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and speech recognition (SR) models to decode the high-level natural language conversations and semantic understanding of the robot’s task environment, and abstract them to the robot’s actionable commands or queries. We performed a quantitative evaluation of our framework’s natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved <math id="id1.1.m1.1" class="ltx_Math" alttext="87.55\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">87.55</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="float" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">87.55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">87.55\%</annotation></semantics></math> vocal commands decoding accuracy, <math id="id2.2.m2.1" class="ltx_Math" alttext="86.27\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">86.27</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn type="float" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">86.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">86.27\%</annotation></semantics></math> commands execution success, and an average latency of <math id="id3.3.m3.1" class="ltx_Math" alttext="0.89" display="inline"><semantics id="id3.3.m3.1a"><mn id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">0.89</mn><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><cn type="float" id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">0.89</cn></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">0.89</annotation></semantics></math> seconds from receiving the participants’ vocal chat commands to initiating the robot’s actual physical action. The video demonstrations of this paper can be found at <a target="_blank" href="https://linusnep.github.io/MTCC-IRoNL/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://linusnep.github.io/MTCC-IRoNL/</a>.</p>
</div>
<div class="ltx_keywords">Human-robot interaction, LLMs, VLMs, ROS, foundation models, natural language interaction, vocal conversation.
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Workshop of the 2024 ACM/IEEE International Conference on Human-Robot Interaction; March 11–14, 2024; Boulder, CO, USA</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Workshop of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI ’24 Workshop), March 11–14, 2024, Boulder, CO, USA</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/24/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Human computer interaction (HCI)</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interaction paradigms</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Natural language</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Existing approaches for interacting with autonomous robots in the real world have been dominated by complex teleoperation controllers <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> and rigid command protocols <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>, where the robots execute predefined tasks based on specialized programming languages. As the challenges we present to these robots become more intricate and the environments they operate in grow more unpredictable <cite class="ltx_cite ltx_citemacro_citep">(Nwankwo and Rueckert, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, there arises an unmistakable need for more natural and intuitive interaction mechanisms.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The last few years have witnessed tremendous advancement in generative AI and natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib14" title="" class="ltx_ref">2023e</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Murugesan and Cherukuri, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>. These advancements driven primarily by foundation models, specifically transformer-based large language models (LLMs) like OpenAI GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, Google BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, Meta AI LLaMA <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib10" title="" class="ltx_ref">2023d</a>)</cite>, and multimodal visual language models (VLMs) e.g., CLIP <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib6" title="" class="ltx_ref">2021a</a>)</cite>, DALL-E <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib13" title="" class="ltx_ref">2021b</a>)</cite>, and their successors, has opened new avenues for human-robot interaction (HRI) <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>. The inherent abilities of these models to understand language patterns, and structure and generate human-like responses as well as visual observations have led to several interesting robotic applications, such as <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib7" title="" class="ltx_ref">2023c</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib3" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we exploit the inherent natural language capabilities of the pre-trained foundation models, as well as a speech recognition (SR) model to enable humans to interact naturally with autonomous agents through both spoken and textual dialogues.
As demonstrated in the video at the project website<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://linusnep.github.io/MTCC-IRoNL/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://linusnep.github.io/MTCC-IRoNL/</a></span></span></span>, our framework aims to realize a new approach to human-robot interactions—one where the vocal or textual conversation is the command (refer to Section <a href="#S3" title="3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for more details).
Therefore, our contributions are twofold:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">we introduced a dual-modality framework that can leverage independent pre-trained LLMs, VLMs, and SR models to enable humans to interact with real-world autonomous robots or other entities through spoken or textual conversations.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">we performed real-world experiments with our developed framework to ensure that the robot’s actions are always aligned with the user’s spoken or textual instructions.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Prior works such as <cite class="ltx_cite ltx_citemacro_citep">(Ferrari et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citep">(Kodur et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> have explored the incorporation of vocal instructions into robotic systems. However, while these works are exceptional, they have relied primarily on a direct speech-to-action (STA) approach, where the robot’s actions are dependent upon the accurate transcription of the vocal commands by the SR model employed in the respective works. In most noisy real-world scenarios, their approach may introduce stochastic behaviour in the robot’s actions due to vulnerability to acoustic distortions present in real-world environments.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Further, L. Nwankwo et al. <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite> incorporated text-based interaction techniques into autonomous robots leveraging LLMs and VLMs. Nonetheless, the framework lacks complete naturalness due to the absence of a mechanism to understand vocal instructions. In this work, we build upon the foundation provided in <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>. Instead of relying on the accuracy of the SR model to plan the robot’s actions or depend on the text-based approach as a standalone, we propose a dual-modality approach that synergizes both the textual and vocal modalities. We leveraged the LLMs and the SR models’ abilities to maintain robustness in diverse environments. In environments where ambient noise levels may compromise the accuracy of the SR model’s vocal instructions decoding, our framework provides the flexibility to revert to the text-based interaction method. Conversely, in a quieter environment, the user can leverage the vocal modality pipeline for more natural and seamless interaction.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">With our proposed dual-modality approach, we aim to provide the user with the autonomy to select the mode of interaction most suited to the prevailing conditions. Specifically, our framework mitigates the risk of misinterpretations and erroneous robotic actions that may have arisen due to sole dependence on the STA method, thereby ensuring consistent and reliable HRI <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> in the real world.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the architectural overview of our proposed framework.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2403.12273/assets/figs/method.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of our framework’s architecture. The area enclosed with the red dotted line decodes the textual-based natural language conversations and visual understanding. In the SRNode, we employed Google’s SR model <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> to decode the vocal conversation from humans and abstract them to the textual representations required by the ChatGUI to interact with the LLMNode.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The proposed model contains five main components: the LLMNode to decode the high-level textual-based conversations from humans, the CLIPNode to provide a visual and semantic understanding of the robot’s task environment, the REM node to abstract the high-level understanding from the LLMNode to actual robot actions, the ChatGUI to serve as the user’s primary textual-based interaction point, and the SRNode to provide vocal or auditory commands understanding.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In this section, we provide details on the incorporation of the vocal conversation understanding pipeline. For details about the implementation of the first four sections (the area enclosed in red dotted line in Figure <a href="#S3.F1" title="Figure 1 ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and how the pre-trained LLMs and VLMs are prompted to generate the actions used by the REM node, we refer the reader to <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Vocal Conversation Decoding</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">In order to decode the vocal natural language conversation and abstract them to the robot’s actions, we developed the SRNode. The SRNode employs Google’s SR model <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> to capture the high-level auditory input from a microphone device, transcribing the auditory inputs to textual representations. The textual representation is subsequently used by the ChatGUI to establish communication between the LLMNode and the rest of the interfaces of Figure <a href="#S3.F1" title="Figure 1 ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> within ROS ecosystem <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib12" title="" class="ltx_ref">2009</a>)</cite>.
Formally, given a vocal command <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{V}_{c}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">𝒱</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝒱</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{V}_{c}</annotation></semantics></math> e.g., task descriptions, queries captured by the microphone device, we developed a function <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="f_{STT}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">f</mi><mrow id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.3.1" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.3.1a" xref="S3.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.3.4" xref="S3.SS1.p1.2.m2.1.1.3.4.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑓</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><times id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">𝑇</ci><ci id="S3.SS1.p1.2.m2.1.1.3.4.cmml" xref="S3.SS1.p1.2.m2.1.1.3.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">f_{STT}</annotation></semantics></math> employing the Google’s SR model <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> such that <math id="S3.SS1.p1.3.m3.4" class="ltx_Math" alttext="\mathcal{V}_{c}=\{v_{1},v_{2},...,v_{n}\}" display="inline"><semantics id="S3.SS1.p1.3.m3.4a"><mrow id="S3.SS1.p1.3.m3.4.4" xref="S3.SS1.p1.3.m3.4.4.cmml"><msub id="S3.SS1.p1.3.m3.4.4.5" xref="S3.SS1.p1.3.m3.4.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.4.4.5.2" xref="S3.SS1.p1.3.m3.4.4.5.2.cmml">𝒱</mi><mi id="S3.SS1.p1.3.m3.4.4.5.3" xref="S3.SS1.p1.3.m3.4.4.5.3.cmml">c</mi></msub><mo id="S3.SS1.p1.3.m3.4.4.4" xref="S3.SS1.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.4.4.3.3" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.4.4.3.3.4" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S3.SS1.p1.3.m3.2.2.1.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2.cmml">v</mi><mn id="S3.SS1.p1.3.m3.2.2.1.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.3.m3.4.4.3.3.5" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.3.m3.3.3.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.3.m3.3.3.2.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.2.cmml">v</mi><mn id="S3.SS1.p1.3.m3.3.3.2.2.2.3" xref="S3.SS1.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.3.m3.4.4.3.3.6" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.p1.3.m3.4.4.3.3.7" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.3.m3.4.4.3.3.3" xref="S3.SS1.p1.3.m3.4.4.3.3.3.cmml"><mi id="S3.SS1.p1.3.m3.4.4.3.3.3.2" xref="S3.SS1.p1.3.m3.4.4.3.3.3.2.cmml">v</mi><mi id="S3.SS1.p1.3.m3.4.4.3.3.3.3" xref="S3.SS1.p1.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS1.p1.3.m3.4.4.3.3.8" xref="S3.SS1.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.4b"><apply id="S3.SS1.p1.3.m3.4.4.cmml" xref="S3.SS1.p1.3.m3.4.4"><eq id="S3.SS1.p1.3.m3.4.4.4.cmml" xref="S3.SS1.p1.3.m3.4.4.4"></eq><apply id="S3.SS1.p1.3.m3.4.4.5.cmml" xref="S3.SS1.p1.3.m3.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.4.4.5.1.cmml" xref="S3.SS1.p1.3.m3.4.4.5">subscript</csymbol><ci id="S3.SS1.p1.3.m3.4.4.5.2.cmml" xref="S3.SS1.p1.3.m3.4.4.5.2">𝒱</ci><ci id="S3.SS1.p1.3.m3.4.4.5.3.cmml" xref="S3.SS1.p1.3.m3.4.4.5.3">𝑐</ci></apply><set id="S3.SS1.p1.3.m3.4.4.3.4.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3"><apply id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.2">𝑣</ci><cn type="integer" id="S3.SS1.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2.2">𝑣</ci><cn type="integer" id="S3.SS1.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">…</ci><apply id="S3.SS1.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.4.4.3.3.3.1.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.4.4.3.3.3.2.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.3.2">𝑣</ci><ci id="S3.SS1.p1.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.4c">\mathcal{V}_{c}=\{v_{1},v_{2},...,v_{n}\}</annotation></semantics></math> is transcribed to a textual natural language representation <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{L}</annotation></semantics></math> as depicted in Eq. <a href="#S3.E1" title="In 3.1. Vocal Conversation Decoding ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The elements <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">v</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑣</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">v_{i}</annotation></semantics></math>, <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="i\in n" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">i</mi><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">∈</mo><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><in id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></in><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑖</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">i\in n</annotation></semantics></math> represent distinct vocal commands or instructions given to the SR model through speech, e.g., “Hello robot, can you move forward?”, “What is your current location?”, “Navigate to the kitchen area”, etc.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\mathcal{L}_{v}=f_{STT}(\mathcal{V}_{c})=\{l_{1},l_{2},...,l_{n}\},\quad l_{i}\in\mathcal{L}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.3.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.6" xref="S3.E1.m1.2.2.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.6.2" xref="S3.E1.m1.2.2.1.1.6.2.cmml">ℒ</mi><mi id="S3.E1.m1.2.2.1.1.6.3" xref="S3.E1.m1.2.2.1.1.6.3.cmml">v</mi></msub><mo id="S3.E1.m1.2.2.1.1.7" xref="S3.E1.m1.2.2.1.1.7.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">f</mi><mrow id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.3.1a" xref="S3.E1.m1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3.4" xref="S3.E1.m1.2.2.1.1.1.3.3.4.cmml">T</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">𝒱</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">c</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.8" xref="S3.E1.m1.2.2.1.1.8.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.4.3" xref="S3.E1.m1.2.2.1.1.4.4.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.4.3.4" xref="S3.E1.m1.2.2.1.1.4.4.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.2.cmml">l</mi><mn id="S3.E1.m1.2.2.1.1.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.2.2.1.1.4.3.5" xref="S3.E1.m1.2.2.1.1.4.4.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.3.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2.2.2" xref="S3.E1.m1.2.2.1.1.3.2.2.2.cmml">l</mi><mn id="S3.E1.m1.2.2.1.1.3.2.2.3" xref="S3.E1.m1.2.2.1.1.3.2.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.2.2.1.1.4.3.6" xref="S3.E1.m1.2.2.1.1.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">…</mi><mo id="S3.E1.m1.2.2.1.1.4.3.7" xref="S3.E1.m1.2.2.1.1.4.4.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.4.3.3" xref="S3.E1.m1.2.2.1.1.4.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.4.3.3.2" xref="S3.E1.m1.2.2.1.1.4.3.3.2.cmml">l</mi><mi id="S3.E1.m1.2.2.1.1.4.3.3.3" xref="S3.E1.m1.2.2.1.1.4.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.4.3.8" xref="S3.E1.m1.2.2.1.1.4.4.cmml">}</mo></mrow></mrow><mo rspace="1.167em" id="S3.E1.m1.3.3.2.3" xref="S3.E1.m1.3.3.3a.cmml">,</mo><mrow id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml"><msub id="S3.E1.m1.3.3.2.2.2" xref="S3.E1.m1.3.3.2.2.2.cmml"><mi id="S3.E1.m1.3.3.2.2.2.2" xref="S3.E1.m1.3.3.2.2.2.2.cmml">l</mi><mi id="S3.E1.m1.3.3.2.2.2.3" xref="S3.E1.m1.3.3.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.2.2.1" xref="S3.E1.m1.3.3.2.2.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.2.2.3" xref="S3.E1.m1.3.3.2.2.3.cmml">ℒ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3a.cmml" xref="S3.E1.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><and id="S3.E1.m1.2.2.1.1a.cmml" xref="S3.E1.m1.2.2.1.1"></and><apply id="S3.E1.m1.2.2.1.1b.cmml" xref="S3.E1.m1.2.2.1.1"><eq id="S3.E1.m1.2.2.1.1.7.cmml" xref="S3.E1.m1.2.2.1.1.7"></eq><apply id="S3.E1.m1.2.2.1.1.6.cmml" xref="S3.E1.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.6.1.cmml" xref="S3.E1.m1.2.2.1.1.6">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.6.2.cmml" xref="S3.E1.m1.2.2.1.1.6.2">ℒ</ci><ci id="S3.E1.m1.2.2.1.1.6.3.cmml" xref="S3.E1.m1.2.2.1.1.6.3">𝑣</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">𝑓</ci><apply id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3"><times id="S3.E1.m1.2.2.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.1"></times><ci id="S3.E1.m1.2.2.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.2">𝑆</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.3">𝑇</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.4.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.4">𝑇</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝒱</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.1.1c.cmml" xref="S3.E1.m1.2.2.1.1"><eq id="S3.E1.m1.2.2.1.1.8.cmml" xref="S3.E1.m1.2.2.1.1.8"></eq><share href="#S3.E1.m1.2.2.1.1.1.cmml" id="S3.E1.m1.2.2.1.1d.cmml" xref="S3.E1.m1.2.2.1.1"></share><set id="S3.E1.m1.2.2.1.1.4.4.cmml" xref="S3.E1.m1.2.2.1.1.4.3"><apply id="S3.E1.m1.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.2">𝑙</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2.2">𝑙</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.2.2.3">2</cn></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">…</ci><apply id="S3.E1.m1.2.2.1.1.4.3.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.4.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.4.3.3.2">𝑙</ci><ci id="S3.E1.m1.2.2.1.1.4.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3.3.3">𝑛</ci></apply></set></apply></apply><apply id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2"><in id="S3.E1.m1.3.3.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.1"></in><apply id="S3.E1.m1.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2.2">𝑙</ci><ci id="S3.E1.m1.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.2.3">𝑖</ci></apply><ci id="S3.E1.m1.3.3.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.3">ℒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathcal{L}_{v}=f_{STT}(\mathcal{V}_{c})=\{l_{1},l_{2},...,l_{n}\},\quad l_{i}\in\mathcal{L}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.8" class="ltx_p">where <math id="S3.SS1.p1.7.m1.1" class="ltx_Math" alttext="l_{i}" display="inline"><semantics id="S3.SS1.p1.7.m1.1a"><msub id="S3.SS1.p1.7.m1.1.1" xref="S3.SS1.p1.7.m1.1.1.cmml"><mi id="S3.SS1.p1.7.m1.1.1.2" xref="S3.SS1.p1.7.m1.1.1.2.cmml">l</mi><mi id="S3.SS1.p1.7.m1.1.1.3" xref="S3.SS1.p1.7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m1.1b"><apply id="S3.SS1.p1.7.m1.1.1.cmml" xref="S3.SS1.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m1.1.1.1.cmml" xref="S3.SS1.p1.7.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m1.1.1.2.cmml" xref="S3.SS1.p1.7.m1.1.1.2">𝑙</ci><ci id="S3.SS1.p1.7.m1.1.1.3.cmml" xref="S3.SS1.p1.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m1.1c">l_{i}</annotation></semantics></math> denotes the transcribed natural language command from the set <math id="S3.SS1.p1.8.m2.1" class="ltx_Math" alttext="\mathcal{L}_{v}" display="inline"><semantics id="S3.SS1.p1.8.m2.1a"><msub id="S3.SS1.p1.8.m2.1.1" xref="S3.SS1.p1.8.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.8.m2.1.1.2" xref="S3.SS1.p1.8.m2.1.1.2.cmml">ℒ</mi><mi id="S3.SS1.p1.8.m2.1.1.3" xref="S3.SS1.p1.8.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m2.1b"><apply id="S3.SS1.p1.8.m2.1.1.cmml" xref="S3.SS1.p1.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m2.1.1.1.cmml" xref="S3.SS1.p1.8.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m2.1.1.2.cmml" xref="S3.SS1.p1.8.m2.1.1.2">ℒ</ci><ci id="S3.SS1.p1.8.m2.1.1.3.cmml" xref="S3.SS1.p1.8.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m2.1c">\mathcal{L}_{v}</annotation></semantics></math>. We sent the resulting output from Eq.<a href="#S3.E1" title="In 3.1. Vocal Conversation Decoding ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as input to the ChatGUI. We then used the LLMNode to handle the incoming natural language inputs from the ChatGUI by first passing them through the pre-trained LLM <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. The resultant output from the LLMNode is then mapped to the robot’s actionable commands or information request by the robot’s execution mechanism (REM) node, consisting of the ROS<cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib12" title="" class="ltx_ref">2009</a>)</cite> navigation planner packages shown at the lower bottom-left of Figure <a href="#S3.F1" title="Figure 1 ‣ 3. Method ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conducted both real-world and simulated experiments to validate the performance of our framework. In simulation, we utilised the Unitree Go1 ROS &amp; Gazebo packages<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/unitreerobotics/unitree_guide" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/unitreerobotics/unitree_guide</a></span></span></span>
and a ROS-based open-source mobile robot adapted from <cite class="ltx_cite ltx_citemacro_citep">(Nwankwo et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. We ran all the simulations with a ground station PC with Nvidia Geforce RTX 3060 Ti GPU, 8GB memory running Ubuntu 20.04, ROS Noetic distribution.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.3" class="ltx_p">In the real-world experiments, we used a Lenovo ThinkBook Intel Core i7 with Intel iRIS Graphics running Ubuntu 20.04, ROS Noetic distribution. Segway RMP Lite 220 mobile robot was used. The robot is equipped with an RGB-D camera and a <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="2D~{}\text{RP}" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mn id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">D</mi><mo lspace="0.330em" rspace="0em" id="S4.p2.1.m1.1.1.1a" xref="S4.p2.1.m1.1.1.1.cmml">​</mo><mtext id="S4.p2.1.m1.1.1.4" xref="S4.p2.1.m1.1.1.4a.cmml">RP</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">2</cn><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">𝐷</ci><ci id="S4.p2.1.m1.1.1.4a.cmml" xref="S4.p2.1.m1.1.1.4"><mtext id="S4.p2.1.m1.1.1.4.cmml" xref="S4.p2.1.m1.1.1.4">RP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">2D~{}\text{RP}</annotation></semantics></math>LiDAR for both visual and spatial observations of the task environment. We used our PC’s inbuilt microphone and a plug-and-play AmazonBasics Pro Gaming Headset with a microphone function in all the experiments. We performed all the real-world experiments in our laboratory office (11 rooms) and outside corridor environment, measuring approximately <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="18\times 20\;m" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mrow id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml"><mn id="S4.p2.2.m2.1.1.2.2" xref="S4.p2.2.m2.1.1.2.2.cmml">18</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.2.m2.1.1.2.1" xref="S4.p2.2.m2.1.1.2.1.cmml">×</mo><mn id="S4.p2.2.m2.1.1.2.3" xref="S4.p2.2.m2.1.1.2.3.cmml">20</mn></mrow><mo lspace="0.280em" rspace="0em" id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><times id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></times><apply id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2"><times id="S4.p2.2.m2.1.1.2.1.cmml" xref="S4.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S4.p2.2.m2.1.1.2.2.cmml" xref="S4.p2.2.m2.1.1.2.2">18</cn><cn type="integer" id="S4.p2.2.m2.1.1.2.3.cmml" xref="S4.p2.2.m2.1.1.2.3">20</cn></apply><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">18\times 20\;m</annotation></semantics></math> and <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="6\times 120\;m" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mrow id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml"><mn id="S4.p2.3.m3.1.1.2.2" xref="S4.p2.3.m3.1.1.2.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.3.m3.1.1.2.1" xref="S4.p2.3.m3.1.1.2.1.cmml">×</mo><mn id="S4.p2.3.m3.1.1.2.3" xref="S4.p2.3.m3.1.1.2.3.cmml">120</mn></mrow><mo lspace="0.280em" rspace="0em" id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><times id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></times><apply id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2"><times id="S4.p2.3.m3.1.1.2.1.cmml" xref="S4.p2.3.m3.1.1.2.1"></times><cn type="integer" id="S4.p2.3.m3.1.1.2.2.cmml" xref="S4.p2.3.m3.1.1.2.2">6</cn><cn type="integer" id="S4.p2.3.m3.1.1.2.3.cmml" xref="S4.p2.3.m3.1.1.2.3">120</cn></apply><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">6\times 120\;m</annotation></semantics></math> respectively.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We experimented with OpenAI GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, Google BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, and Meta AI LLaMA <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib10" title="" class="ltx_ref">2023d</a>)</cite>. OpenAI GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> are also adaptable to our framework. However, due to their API access limitations, we mostly utilised the open-access and free versions of the LLMs (GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> specifically) in our experiments.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Preliminary Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.4" class="ltx_p">We invited <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">5</annotation></semantics></math> participants (average age of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="27\ (\pm 3)" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">27</mn><mo lspace="0.500em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S4.SS1.p1.2.m2.1.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.2.m2.1.1.1.1.2" xref="S4.SS1.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.2.m2.1.1.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.1.1.cmml"><mo id="S4.SS1.p1.2.m2.1.1.1.1.1a" xref="S4.SS1.p1.2.m2.1.1.1.1.1.cmml">±</mo><mn id="S4.SS1.p1.2.m2.1.1.1.1.1.2" xref="S4.SS1.p1.2.m2.1.1.1.1.1.2.cmml">3</mn></mrow><mo stretchy="false" id="S4.SS1.p1.2.m2.1.1.1.1.3" xref="S4.SS1.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><times id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2"></times><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">27</cn><apply id="S4.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.SS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">27\ (\pm 3)</annotation></semantics></math> and gender distribution, <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">80</mn><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">80\%</annotation></semantics></math> male and <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">20</mn><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">20\%</annotation></semantics></math> female) with different English language accents to interact with the robot via natural vocal conversation. We logged the interaction data i.e., the SR models’ transcription of the participants’ spoken words, the LLMNode predicted labels, the true action labels, etc.
We used the logged interaction data to quantitatively evaluate the performance of our framework.
We defined vocal commands understanding accuracy (VCUA) metric in addition to the navigation success rate (NSR), object identification accuracy (OIA), and average response time (ART) metrics utilised in <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite> to assess our framework’s performance. With the VCUA, we assess how accurately the LLMNode predicts the commands based on the transcribed vocal instructions from the SRNode. We computed the accuracy as the percentage proportion of the correctly transcribed instruction to the generated instructions (from LLMNode) fed to the REM node for the actual robot’s execution.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1. Preliminary Results ‣ 4. Experiments ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the statistical results that we obtained from the interaction data analysis.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.12273/assets/figs/metrics.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="303" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Quantitative evaluation results illustrating VCUA, NSR, OIA, and ART based on the logged interaction data.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F2.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F2.2" class="ltx_p ltx_figure_panel ltx_align_center">xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</p>
</div>
</div>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p">The top-left figure shows the VCUA, NSR, and OIA metrics for selected labels. We achieved <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="87.55\%" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">87.55</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">87.55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">87.55\%</annotation></semantics></math> VCUA and <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="86.27\%" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mn id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">86.27</mn><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">86.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">86.27\%</annotation></semantics></math> NSR, which indicates a good level of accuracy in the vocal commands decoding. In comparison to the results obtained in the textual-based method <cite class="ltx_cite ltx_citemacro_citep">(Linus and Elmar, <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>, we observed a slight difference in the command recognition accuracy, with VCUA achieving about <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="12\%" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">12</mn><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">12\%</annotation></semantics></math> less than the textual-based CRA (<math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="99.13\%" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">99.13</mn><mo id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">99.13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">99.13\%</annotation></semantics></math>) as well as a <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="11.69\%" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mn id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">11.69</mn><mo id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2">11.69</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">11.69\%</annotation></semantics></math> reduction in the NSR. This is expected because of the ambient environmental noise and variation in the participant’s accents, which affect the vocal transcription from the SRNode, as could be seen in the confusion matrix (bottom-left of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1. Preliminary Results ‣ 4. Experiments ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Further, the ART (right column of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1. Preliminary Results ‣ 4. Experiments ‣ Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) across all the selected commands is approximately <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="0.89" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mn id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">0.89</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><cn type="float" id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">0.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">0.89</annotation></semantics></math> seconds. This indicates that, on average, the robot takes less than a second from receiving a vocal chat command to initiating the robot’s actual physical action, which suggests a relatively quick response time for our framework.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduced a framework that leverages the inherent capabilities of LLMs, VLMs, and SR models to enhance human-robot interaction through natural vocal and textual conversations. Our evaluation from logged human interaction data achieved high vocal command understanding accuracy and effective task execution. This shows that our framework can enhance the intuitiveness and naturalness of human-robot interaction in the real world.
In our future work, we aim to refine our framework to resist the impact of environmental noise. We intend to incorporate adaptive noise-cancellation algorithms and context-aware speech recognition techniques to mitigate the impacts of random noise.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">North American Chapter of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:52967399" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:52967399</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023a)</span>
<span class="ltx_bibblock">
Anthony Brohan et al. 2023a.

</span>
<span class="ltx_bibblock">RT-1: Robotics Transformer for Real-World Control at Scale.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.06817 [cs.RO]

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023b)</span>
<span class="ltx_bibblock">
Anthony Brohan et al. 2023b.

</span>
<span class="ltx_bibblock">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.15818 [cs.RO]

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford et al. 2019.

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask Learners.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:160025533" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:160025533</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2021a)</span>
<span class="ltx_bibblock">
Alec Radford et al. 2021a.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision. In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:231591445" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:231591445</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023c)</span>
<span class="ltx_bibblock">
Brohan Anthony et al. 2023c.

</span>
<span class="ltx_bibblock">Do as i can, not as i say: Grounding language in robotic affordances. In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 6th Conference on Robot Learning</em> <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">(Proceedings of Machine Learning Research, Vol. 205)</em>, Karen Liu, Dana Kulic, and Jeff Ichnowski (Eds.). PMLR, 287–318.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.mlr.press/v205/ichter23a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v205/ichter23a.html</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2020)</span>
<span class="ltx_bibblock">
Brown Tom et al. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners. In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2018)</span>
<span class="ltx_bibblock">
Chiu Chung-Cheng et al. 2018.

</span>
<span class="ltx_bibblock">State-of-the-Art Speech Recognition with Sequence-to-Sequence Models. In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (Calgary, AB, Canada). IEEE Press, 4774–4778.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP.2018.8462105" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP.2018.8462105</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023d)</span>
<span class="ltx_bibblock">
Hugo Touvron et al. 2023d.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2302.13971 [cs.CL]

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2022)</span>
<span class="ltx_bibblock">
Matthew Marge et al. 2022.

</span>
<span class="ltx_bibblock">Spoken language interaction with robots: Recommendations for future research.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em> 71 (2022), 101255.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.csl.2021.101255" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.csl.2021.101255</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2009)</span>
<span class="ltx_bibblock">
Quigley Morgan et al. 2009.

</span>
<span class="ltx_bibblock">ROS: an open-source Robot Operating System.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICRA Workshop on Open Source Software</em> 3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2021b)</span>
<span class="ltx_bibblock">
Ramesh Aditya et al. 2021b.

</span>
<span class="ltx_bibblock">Zero-Shot Text-to-Image Generation. In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning</em> <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">(Proceedings of Machine Learning Research, Vol. 139)</em>, Marina Meila and Tong Zhang (Eds.). PMLR, 8821–8831.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.mlr.press/v139/ramesh21a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v139/ramesh21a.html</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023e)</span>
<span class="ltx_bibblock">
Staphord Bengesi et al. 2023e.

</span>
<span class="ltx_bibblock">Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.10242 [cs.LG]

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrari et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Davide Ferrari, Filippo Alberi, and Cristian Secchi. 2023.

</span>
<span class="ltx_bibblock">Facilitating Human-Robot Collaboration through Natural Vocal Conversations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.13973 [cs.RO]

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kodur et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Krishna Kodur, Manizheh Zand, and Maria Kyrarini. 2023.

</span>
<span class="ltx_bibblock">Towards Robot Learning from Spoken Language. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction</em> (Stockholm, Sweden) <em id="bib.bib16.4.2" class="ltx_emph ltx_font_italic">(HRI ’23)</em>. Association for Computing Machinery, New York, NY, USA, 112–116.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3568294.3580053" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3568294.3580053</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tsung-Chi Lin, Achyuthan Unni Krishnan, and Zhi Li. 2023.

</span>
<span class="ltx_bibblock">Perception-Motion Coupling in Active Telepresence: Human Behavior and Teleoperation Interface Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">J. Hum.-Robot Interact.</em> 12, 3, Article 31 (mar 2023), 24 pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3571599" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3571599</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Linus and Elmar (2024)</span>
<span class="ltx_bibblock">
Nwankwo Linus and Rueckert Elmar. 2024.

</span>
<span class="ltx_bibblock">The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language.

</span>
<span class="ltx_bibblock">arXiv:2401.11838 [cs.RO]

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Debasmita Mukherjee, Kashish Gupta, and Homayoun Najjaran. 2022.

</span>
<span class="ltx_bibblock">A Critical Analysis of Industrial Human-Robot Communication and Its Quest for Naturalness Through the Lens of Complexity Theory.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Frontiers in Robotics and AI</em> 9 (2022).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/frobt.2022.870477" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/frobt.2022.870477</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murugesan and Cherukuri (2023)</span>
<span class="ltx_bibblock">
San Murugesan and Aswani Kumar Cherukuri. 2023.

</span>
<span class="ltx_bibblock">The Rise of Generative Artificial Intelligence and Its Impact on Education: The Promises and Perils.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Computer</em> 56, 5 (2023), 116–121.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/MC.2023.3253292" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MC.2023.3253292</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nwankwo et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Linus Nwankwo, Clemens Fritze, Konrad Bartsch, and Elmar Rueckert. 2023.

</span>
<span class="ltx_bibblock">ROMR: A ROS-based open-source mobile robot.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">HardwareX</em> 14 (2023), e00426.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.ohx.2023.e00426" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.ohx.2023.e00426</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nwankwo and Rueckert (2023)</span>
<span class="ltx_bibblock">
Linus Nwankwo and Elmar Rueckert. 2023.

</span>
<span class="ltx_bibblock">Understanding Why SLAM Algorithms Fail in Modern Indoor Environments. In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Service and Industrial Robotics</em>, Tadej Petrič, Aleš Ude, and Leon Žlajpah (Eds.). Springer Nature Switzerland, Cham, 186–194.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-031-32606-6_22" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-031-32606-6_22</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hang Su, Wen Qi, Jiahao Chen, Chenguang Yang, Juan Sandoval, and Med Amine Laribi. 2023.

</span>
<span class="ltx_bibblock">Recent advancements in multimodal human–robot interaction.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Frontiers in Neurorobotics</em> 17 (2023).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/fnbot.2023.1084000" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/fnbot.2023.1084000</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.12272" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.12273" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.12273">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.12273" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.12274" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 18:08:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
