<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.11214] Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text</title><meta property="og:description" content="Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) an…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.11214">

<!--Generated on Sat Oct  5 23:42:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Multilingual Speech-to-Text,  Dual Multilingual Encoders,  Large Language Models.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongfei Xue<sup id="id6.3.id1" class="ltx_sup">∗</sup>
</span><span class="ltx_author_notes"><sup id="id7.4.id1" class="ltx_sup">∗</sup>Equal contribution.
<span class="ltx_contact ltx_role_affiliation"><span id="id8.5.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>hfxue@mail.nwpu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Ren<sup id="id9.2.id1" class="ltx_sup">∗</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.3.id1" class="ltx_text ltx_font_italic">Chongqing Changan Automobile Co., Ltd
<br class="ltx_break"></span>agnanren@tinnove.com.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuelong Geng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>xlgeng@mail.nwpu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kun Wei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id12.1.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>ethanwei@mail.nwpu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Longhao Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>lhli@mail.nwpu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qijie Shao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id14.1.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>qjshao@npu-aslp.org
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linju Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_font_italic">Chongqing Changan Automobile Co., Ltd
<br class="ltx_break"></span>louisyang@tinnove.com.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kai Diao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_font_italic">Chongqing Changan Automobile Co., Ltd
<br class="ltx_break"></span>diaokai@changan.com.cn 
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Xie<sup id="id17.3.id1" class="ltx_sup"><span id="id17.3.id1.1" class="ltx_text ltx_font_italic">†</span></sup>
</span><span class="ltx_author_notes"><sup id="id18.4.id1" class="ltx_sup">†</sup>Corresponding author.
<span class="ltx_contact ltx_role_affiliation"> <span id="id19.5.id1" class="ltx_text ltx_font_italic">Northwestern Polytechnical University
<br class="ltx_break"></span>lxie@nwpu.edu.cn
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST). However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically. By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations. Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language. Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Multilingual Speech-to-Text, Dual Multilingual Encoders, Large Language Models.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Text-based Large Language Models (LLM) have demonstrated significant influence in the field of artificial intelligence due to their powerful natural language understanding and generation capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Recently, researchers have explored integrating audio encoders with LLMs through connectors, enabling LLMs to process and understand audio modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. By training with audio-text paired data, the connector aligns the output space of the audio encoder with the input space of the LLM, playing a crucial role in audio understanding tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Multilingual speech-to-text (S2T) is a vital task in audio understanding, encompassing both multilingual automatic speech recognition (ASR) and automatic speech translation (AST). Existing studies have shown that integrating speech encoders and LLMs with a connector significantly enhances performance compared to traditional end-to-end models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. For example, one approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> employs a connectionist temporal classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> trained encoder to process speech sequences, which are then fed into an LLM decoder through a projection layer. This method outperforms end-to-end models in multilingual ASR tasks. Similarly, Speech-LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> uses a CTC compressor and a simple audio encoder to map compressed acoustic features into the continuous semantic space of the LLM, achieving superior performance on several AST test sets.
Moreover, approaches leveraging Whisper encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> or self-supervised learning (SSL) encoders have demonstrated superior improvements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Qwen-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> leverage a fine-tuned Whisper encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to extract speech representations, leading to significant advancements in both multilingual ASR and AST tasks. Seed-ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> has achieved state-of-the-art (SOTA) results in multilingual, multi-domain ASR test sets by feeding continuous speech representations and contextual information into the LLM, fully exploiting the LLM’s capabilities.
Additionally, a recent study<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> employs W2v-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> as a speech encoder, inputs it into the LLM via a length adapter, and incorporates Chain of Thought (COT) to achieve SOTA results on the CommonVoice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and CovoST2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> test sets for AST.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite these advancements, existing studies often focus on adding multilingual data rather than adequately considering language adaptation. In terms of connectors, the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> delves into various connector architectures, revealing that Q-former <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> surpasses both linear layers and multi-head cross-attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Similarly, the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> employs HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to extract speech representations and utilizes a Transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as a connector to map these representations into the LLM’s space, achieving excellent results on several Mandarin test sets. However, these connectors primarily rely on decoder loss for optimization and lack language adaptation, potentially limiting their effectiveness in accurately mapping the multilingual representation space.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.11214/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
The overall framework of the proposed Ideal-LLM model.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While existing models have leveraged the powerful text comprehension capabilities of LLMs to enhance multilingual S2T tasks, aligning the speech feature space of each language to the LLM remains insufficient due to inherent linguistic differences. On one hand, the features extracted by the encoder may not be sufficiently adapted for multiple languages. On the other hand, existing connectors cannot specifically align two representation spaces for every language.
To address this problem, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich the language information contained in speech features and utilizes a language-adapted connector to target the adaptation of each language specifically. We leverage Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and MMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, two popular and robust models trained on extensive multilingual data using weakly-supervised and self-supervised learning, respectively. These models’ representations complement each other due to their distinct pre-training methods on various language distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. While Salmonn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and WavLLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> also employ dual encoders, they focus on extracting semantic and acoustic information separately rather than addressing the critical aspect of language adaptation. Additionally, our language-adapted connector facilitates modal alignment through CTC loss and integrates dual encoder representations via a language weight selector. The experimental results indicate that our model is more effective at distinguishing languages and aligning the multilingual embedding space. Specifically, our approach significantly enhances ASR performance, achieving a 32.6% relative reduction in average word error rates (WER) compared to Whisper encoder integrated with LLMs. In AST task, our method yields an average BLEU score of 36.78, surpassing the performance of Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Architecture</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our model comprises dual encoders, a language-adapted connector, and a text decoder. An illustration of the overall architecture is shown in Fig <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p"><span id="S2.SS1.p2.2.1" class="ltx_text ltx_font_bold">Dual Encoders</span>
Our dual speech encoders are based on Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and MMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which are robust models trained on large multilingual datasets using weakly-supervised and self-supervised learning, respectively. We process the input speech through two paths. First, we convert the speech signal to an 80-channel log-magnitude Mel spectrogram representation, which is input to the Whisper encoder to obtain the speech feature <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="F_{w}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msub id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">F</mi><mi id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝐹</ci><ci id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">F_{w}</annotation></semantics></math>. Second, the speech signal is directly input to the MMS encoder to obtain the speech feature <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="F_{m}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msub id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">F</mi><mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">𝐹</ci><ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">F_{m}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.9" class="ltx_p"><span id="S2.SS1.p3.9.1" class="ltx_text ltx_font_bold">Language-adapted Connector</span>
Since the dual encoders have been trained on different distributions of language data, we design a connector to perform a language-dependent fusion of the dual encoders’ features and transform them into the embedding space of the LLM. First, the speech features <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="F_{w}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><msub id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">F</mi><mi id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">𝐹</ci><ci id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">F_{w}</annotation></semantics></math> and <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="F_{m}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msub id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">F</mi><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">𝐹</ci><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">F_{m}</annotation></semantics></math> are transformed into hidden representations by Whisper and MMS adapter, which are transformer encoder networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, resulting in <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="H_{w}" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><msub id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.2" xref="S2.SS1.p3.3.m3.1.1.2.cmml">H</mi><mi id="S2.SS1.p3.3.m3.1.1.3" xref="S2.SS1.p3.3.m3.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m3.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p3.3.m3.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.2">𝐻</ci><ci id="S2.SS1.p3.3.m3.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">H_{w}</annotation></semantics></math> and <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="H_{m}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><msub id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml"><mi id="S2.SS1.p3.4.m4.1.1.2" xref="S2.SS1.p3.4.m4.1.1.2.cmml">H</mi><mi id="S2.SS1.p3.4.m4.1.1.3" xref="S2.SS1.p3.4.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.2.cmml" xref="S2.SS1.p3.4.m4.1.1.2">𝐻</ci><ci id="S2.SS1.p3.4.m4.1.1.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">H_{m}</annotation></semantics></math>, respectively. Then, based on the Weight Selector, <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="H_{w}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msub id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">H</mi><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">𝐻</ci><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">H_{w}</annotation></semantics></math> and <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="H_{m}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">H</mi><mi id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">𝐻</ci><ci id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">H_{m}</annotation></semantics></math> are mixed with different weights to form <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="H_{mix}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><msub id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">H</mi><mrow id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml"><mi id="S2.SS1.p3.7.m7.1.1.3.2" xref="S2.SS1.p3.7.m7.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.7.m7.1.1.3.1" xref="S2.SS1.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.7.m7.1.1.3.3" xref="S2.SS1.p3.7.m7.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.7.m7.1.1.3.1a" xref="S2.SS1.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.7.m7.1.1.3.4" xref="S2.SS1.p3.7.m7.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">𝐻</ci><apply id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3"><times id="S2.SS1.p3.7.m7.1.1.3.1.cmml" xref="S2.SS1.p3.7.m7.1.1.3.1"></times><ci id="S2.SS1.p3.7.m7.1.1.3.2.cmml" xref="S2.SS1.p3.7.m7.1.1.3.2">𝑚</ci><ci id="S2.SS1.p3.7.m7.1.1.3.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3.3">𝑖</ci><ci id="S2.SS1.p3.7.m7.1.1.3.4.cmml" xref="S2.SS1.p3.7.m7.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">H_{mix}</annotation></semantics></math>. If the frame lengths differ, they are aligned by adding blank frames. Finally, the fused hidden representations <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="H_{mix}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><msub id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1.2" xref="S2.SS1.p3.8.m8.1.1.2.cmml">H</mi><mrow id="S2.SS1.p3.8.m8.1.1.3" xref="S2.SS1.p3.8.m8.1.1.3.cmml"><mi id="S2.SS1.p3.8.m8.1.1.3.2" xref="S2.SS1.p3.8.m8.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.3.1" xref="S2.SS1.p3.8.m8.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.8.m8.1.1.3.3" xref="S2.SS1.p3.8.m8.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.3.1a" xref="S2.SS1.p3.8.m8.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.8.m8.1.1.3.4" xref="S2.SS1.p3.8.m8.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.2">𝐻</ci><apply id="S2.SS1.p3.8.m8.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3"><times id="S2.SS1.p3.8.m8.1.1.3.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3.1"></times><ci id="S2.SS1.p3.8.m8.1.1.3.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2">𝑚</ci><ci id="S2.SS1.p3.8.m8.1.1.3.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.3">𝑖</ci><ci id="S2.SS1.p3.8.m8.1.1.3.4.cmml" xref="S2.SS1.p3.8.m8.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">H_{mix}</annotation></semantics></math> are sequentially downsampled through the convolutional layer, and the projection layer maps them to <math id="S2.SS1.p3.9.m9.1" class="ltx_Math" alttext="E_{speech}" display="inline"><semantics id="S2.SS1.p3.9.m9.1a"><msub id="S2.SS1.p3.9.m9.1.1" xref="S2.SS1.p3.9.m9.1.1.cmml"><mi id="S2.SS1.p3.9.m9.1.1.2" xref="S2.SS1.p3.9.m9.1.1.2.cmml">E</mi><mrow id="S2.SS1.p3.9.m9.1.1.3" xref="S2.SS1.p3.9.m9.1.1.3.cmml"><mi id="S2.SS1.p3.9.m9.1.1.3.2" xref="S2.SS1.p3.9.m9.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.1.1.3.1" xref="S2.SS1.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.9.m9.1.1.3.3" xref="S2.SS1.p3.9.m9.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.1.1.3.1a" xref="S2.SS1.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.9.m9.1.1.3.4" xref="S2.SS1.p3.9.m9.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.1.1.3.1b" xref="S2.SS1.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.9.m9.1.1.3.5" xref="S2.SS1.p3.9.m9.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.1.1.3.1c" xref="S2.SS1.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.9.m9.1.1.3.6" xref="S2.SS1.p3.9.m9.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.1.1.3.1d" xref="S2.SS1.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p3.9.m9.1.1.3.7" xref="S2.SS1.p3.9.m9.1.1.3.7.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.1b"><apply id="S2.SS1.p3.9.m9.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.1.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1">subscript</csymbol><ci id="S2.SS1.p3.9.m9.1.1.2.cmml" xref="S2.SS1.p3.9.m9.1.1.2">𝐸</ci><apply id="S2.SS1.p3.9.m9.1.1.3.cmml" xref="S2.SS1.p3.9.m9.1.1.3"><times id="S2.SS1.p3.9.m9.1.1.3.1.cmml" xref="S2.SS1.p3.9.m9.1.1.3.1"></times><ci id="S2.SS1.p3.9.m9.1.1.3.2.cmml" xref="S2.SS1.p3.9.m9.1.1.3.2">𝑠</ci><ci id="S2.SS1.p3.9.m9.1.1.3.3.cmml" xref="S2.SS1.p3.9.m9.1.1.3.3">𝑝</ci><ci id="S2.SS1.p3.9.m9.1.1.3.4.cmml" xref="S2.SS1.p3.9.m9.1.1.3.4">𝑒</ci><ci id="S2.SS1.p3.9.m9.1.1.3.5.cmml" xref="S2.SS1.p3.9.m9.1.1.3.5">𝑒</ci><ci id="S2.SS1.p3.9.m9.1.1.3.6.cmml" xref="S2.SS1.p3.9.m9.1.1.3.6">𝑐</ci><ci id="S2.SS1.p3.9.m9.1.1.3.7.cmml" xref="S2.SS1.p3.9.m9.1.1.3.7">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.1c">E_{speech}</annotation></semantics></math> in the LLM embedding space.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.5" class="ltx_p">In the Weight Selector, we initialize trainable parameters for each language and apply a sigmoid function to generate weights. When the LID Adapter predicts a specific language, it selects the parameters of the specified language for weighting. This process is learned through backpropagation with decoder loss and LID loss, guiding the model to prefer certain encoders. The specific formula is as follows:</p>
<table id="S4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle l" display="inline"><semantics id="S2.E1.m1.1a"><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle l</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2.2" class="ltx_Math" alttext="\displaystyle=\text{LID Adapter}(L)," display="inline"><semantics id="S2.E1.m2.2a"><mrow id="S2.E1.m2.2.2.1" xref="S2.E1.m2.2.2.1.1.cmml"><mrow id="S2.E1.m2.2.2.1.1" xref="S2.E1.m2.2.2.1.1.cmml"><mi id="S2.E1.m2.2.2.1.1.2" xref="S2.E1.m2.2.2.1.1.2.cmml"></mi><mo id="S2.E1.m2.2.2.1.1.1" xref="S2.E1.m2.2.2.1.1.1.cmml">=</mo><mrow id="S2.E1.m2.2.2.1.1.3" xref="S2.E1.m2.2.2.1.1.3.cmml"><mtext id="S2.E1.m2.2.2.1.1.3.2" xref="S2.E1.m2.2.2.1.1.3.2a.cmml">LID Adapter</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m2.2.2.1.1.3.1" xref="S2.E1.m2.2.2.1.1.3.1.cmml">​</mo><mrow id="S2.E1.m2.2.2.1.1.3.3.2" xref="S2.E1.m2.2.2.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m2.2.2.1.1.3.3.2.1" xref="S2.E1.m2.2.2.1.1.3.cmml">(</mo><mi id="S2.E1.m2.1.1" xref="S2.E1.m2.1.1.cmml">L</mi><mo stretchy="false" id="S2.E1.m2.2.2.1.1.3.3.2.2" xref="S2.E1.m2.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m2.2.2.1.2" xref="S2.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m2.2b"><apply id="S2.E1.m2.2.2.1.1.cmml" xref="S2.E1.m2.2.2.1"><eq id="S2.E1.m2.2.2.1.1.1.cmml" xref="S2.E1.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S2.E1.m2.2.2.1.1.2.cmml" xref="S2.E1.m2.2.2.1.1.2">absent</csymbol><apply id="S2.E1.m2.2.2.1.1.3.cmml" xref="S2.E1.m2.2.2.1.1.3"><times id="S2.E1.m2.2.2.1.1.3.1.cmml" xref="S2.E1.m2.2.2.1.1.3.1"></times><ci id="S2.E1.m2.2.2.1.1.3.2a.cmml" xref="S2.E1.m2.2.2.1.1.3.2"><mtext id="S2.E1.m2.2.2.1.1.3.2.cmml" xref="S2.E1.m2.2.2.1.1.3.2">LID Adapter</mtext></ci><ci id="S2.E1.m2.1.1.cmml" xref="S2.E1.m2.1.1">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.2c">\displaystyle=\text{LID Adapter}(L),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\displaystyle w" display="inline"><semantics id="S2.E2.m1.1a"><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle w</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m2.3" class="ltx_Math" alttext="\displaystyle=\text{Weight Selector}(l,W)," display="inline"><semantics id="S2.E2.m2.3a"><mrow id="S2.E2.m2.3.3.1" xref="S2.E2.m2.3.3.1.1.cmml"><mrow id="S2.E2.m2.3.3.1.1" xref="S2.E2.m2.3.3.1.1.cmml"><mi id="S2.E2.m2.3.3.1.1.2" xref="S2.E2.m2.3.3.1.1.2.cmml"></mi><mo id="S2.E2.m2.3.3.1.1.1" xref="S2.E2.m2.3.3.1.1.1.cmml">=</mo><mrow id="S2.E2.m2.3.3.1.1.3" xref="S2.E2.m2.3.3.1.1.3.cmml"><mtext id="S2.E2.m2.3.3.1.1.3.2" xref="S2.E2.m2.3.3.1.1.3.2a.cmml">Weight Selector</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m2.3.3.1.1.3.1" xref="S2.E2.m2.3.3.1.1.3.1.cmml">​</mo><mrow id="S2.E2.m2.3.3.1.1.3.3.2" xref="S2.E2.m2.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.E2.m2.3.3.1.1.3.3.2.1" xref="S2.E2.m2.3.3.1.1.3.3.1.cmml">(</mo><mi id="S2.E2.m2.1.1" xref="S2.E2.m2.1.1.cmml">l</mi><mo id="S2.E2.m2.3.3.1.1.3.3.2.2" xref="S2.E2.m2.3.3.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m2.2.2" xref="S2.E2.m2.2.2.cmml">W</mi><mo stretchy="false" id="S2.E2.m2.3.3.1.1.3.3.2.3" xref="S2.E2.m2.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m2.3.3.1.2" xref="S2.E2.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m2.3b"><apply id="S2.E2.m2.3.3.1.1.cmml" xref="S2.E2.m2.3.3.1"><eq id="S2.E2.m2.3.3.1.1.1.cmml" xref="S2.E2.m2.3.3.1.1.1"></eq><csymbol cd="latexml" id="S2.E2.m2.3.3.1.1.2.cmml" xref="S2.E2.m2.3.3.1.1.2">absent</csymbol><apply id="S2.E2.m2.3.3.1.1.3.cmml" xref="S2.E2.m2.3.3.1.1.3"><times id="S2.E2.m2.3.3.1.1.3.1.cmml" xref="S2.E2.m2.3.3.1.1.3.1"></times><ci id="S2.E2.m2.3.3.1.1.3.2a.cmml" xref="S2.E2.m2.3.3.1.1.3.2"><mtext id="S2.E2.m2.3.3.1.1.3.2.cmml" xref="S2.E2.m2.3.3.1.1.3.2">Weight Selector</mtext></ci><interval closure="open" id="S2.E2.m2.3.3.1.1.3.3.1.cmml" xref="S2.E2.m2.3.3.1.1.3.3.2"><ci id="S2.E2.m2.1.1.cmml" xref="S2.E2.m2.1.1">𝑙</ci><ci id="S2.E2.m2.2.2.cmml" xref="S2.E2.m2.2.2">𝑊</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m2.3c">\displaystyle=\text{Weight Selector}(l,W),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\displaystyle w^{\prime}" display="inline"><semantics id="S2.E3.m1.1a"><msup id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mi id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml">w</mi><mo id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1">superscript</csymbol><ci id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2">𝑤</ci><ci id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\displaystyle w^{\prime}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E3.m2.2" class="ltx_Math" alttext="\displaystyle=\text{sigmoid}(w)," display="inline"><semantics id="S2.E3.m2.2a"><mrow id="S2.E3.m2.2.2.1" xref="S2.E3.m2.2.2.1.1.cmml"><mrow id="S2.E3.m2.2.2.1.1" xref="S2.E3.m2.2.2.1.1.cmml"><mi id="S2.E3.m2.2.2.1.1.2" xref="S2.E3.m2.2.2.1.1.2.cmml"></mi><mo id="S2.E3.m2.2.2.1.1.1" xref="S2.E3.m2.2.2.1.1.1.cmml">=</mo><mrow id="S2.E3.m2.2.2.1.1.3" xref="S2.E3.m2.2.2.1.1.3.cmml"><mtext id="S2.E3.m2.2.2.1.1.3.2" xref="S2.E3.m2.2.2.1.1.3.2a.cmml">sigmoid</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m2.2.2.1.1.3.1" xref="S2.E3.m2.2.2.1.1.3.1.cmml">​</mo><mrow id="S2.E3.m2.2.2.1.1.3.3.2" xref="S2.E3.m2.2.2.1.1.3.cmml"><mo stretchy="false" id="S2.E3.m2.2.2.1.1.3.3.2.1" xref="S2.E3.m2.2.2.1.1.3.cmml">(</mo><mi id="S2.E3.m2.1.1" xref="S2.E3.m2.1.1.cmml">w</mi><mo stretchy="false" id="S2.E3.m2.2.2.1.1.3.3.2.2" xref="S2.E3.m2.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m2.2.2.1.2" xref="S2.E3.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m2.2b"><apply id="S2.E3.m2.2.2.1.1.cmml" xref="S2.E3.m2.2.2.1"><eq id="S2.E3.m2.2.2.1.1.1.cmml" xref="S2.E3.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S2.E3.m2.2.2.1.1.2.cmml" xref="S2.E3.m2.2.2.1.1.2">absent</csymbol><apply id="S2.E3.m2.2.2.1.1.3.cmml" xref="S2.E3.m2.2.2.1.1.3"><times id="S2.E3.m2.2.2.1.1.3.1.cmml" xref="S2.E3.m2.2.2.1.1.3.1"></times><ci id="S2.E3.m2.2.2.1.1.3.2a.cmml" xref="S2.E3.m2.2.2.1.1.3.2"><mtext id="S2.E3.m2.2.2.1.1.3.2.cmml" xref="S2.E3.m2.2.2.1.1.3.2">sigmoid</mtext></ci><ci id="S2.E3.m2.1.1.cmml" xref="S2.E3.m2.1.1">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m2.2c">\displaystyle=\text{sigmoid}(w),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E4.m1.1" class="ltx_Math" alttext="\displaystyle H_{mix}" display="inline"><semantics id="S2.E4.m1.1a"><msub id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mi id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml">H</mi><mrow id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.3.2" xref="S2.E4.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.1" xref="S2.E4.m1.1.1.3.1.cmml">​</mo><mi id="S2.E4.m1.1.1.3.3" xref="S2.E4.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.1a" xref="S2.E4.m1.1.1.3.1.cmml">​</mo><mi id="S2.E4.m1.1.1.3.4" xref="S2.E4.m1.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2">𝐻</ci><apply id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3"><times id="S2.E4.m1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.3.1"></times><ci id="S2.E4.m1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.3.2">𝑚</ci><ci id="S2.E4.m1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.3.3">𝑖</ci><ci id="S2.E4.m1.1.1.3.4.cmml" xref="S2.E4.m1.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\displaystyle H_{mix}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E4.m2.1" class="ltx_Math" alttext="\displaystyle=H_{w}\cdot(1-w^{\prime})+H_{m}\cdot w^{\prime}," display="inline"><semantics id="S2.E4.m2.1a"><mrow id="S2.E4.m2.1.1.1" xref="S2.E4.m2.1.1.1.1.cmml"><mrow id="S2.E4.m2.1.1.1.1" xref="S2.E4.m2.1.1.1.1.cmml"><mi id="S2.E4.m2.1.1.1.1.3" xref="S2.E4.m2.1.1.1.1.3.cmml"></mi><mo id="S2.E4.m2.1.1.1.1.2" xref="S2.E4.m2.1.1.1.1.2.cmml">=</mo><mrow id="S2.E4.m2.1.1.1.1.1" xref="S2.E4.m2.1.1.1.1.1.cmml"><mrow id="S2.E4.m2.1.1.1.1.1.1" xref="S2.E4.m2.1.1.1.1.1.1.cmml"><msub id="S2.E4.m2.1.1.1.1.1.1.3" xref="S2.E4.m2.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m2.1.1.1.1.1.1.3.2" xref="S2.E4.m2.1.1.1.1.1.1.3.2.cmml">H</mi><mi id="S2.E4.m2.1.1.1.1.1.1.3.3" xref="S2.E4.m2.1.1.1.1.1.1.3.3.cmml">w</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m2.1.1.1.1.1.1.2" xref="S2.E4.m2.1.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S2.E4.m2.1.1.1.1.1.1.1.1" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m2.1.1.1.1.1.1.1.1.2" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m2.1.1.1.1.1.1.1.1.1" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.cmml"><mn id="S2.E4.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S2.E4.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.2.cmml">w</mi><mo id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S2.E4.m2.1.1.1.1.1.1.1.1.3" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m2.1.1.1.1.1.2" xref="S2.E4.m2.1.1.1.1.1.2.cmml">+</mo><mrow id="S2.E4.m2.1.1.1.1.1.3" xref="S2.E4.m2.1.1.1.1.1.3.cmml"><msub id="S2.E4.m2.1.1.1.1.1.3.2" xref="S2.E4.m2.1.1.1.1.1.3.2.cmml"><mi id="S2.E4.m2.1.1.1.1.1.3.2.2" xref="S2.E4.m2.1.1.1.1.1.3.2.2.cmml">H</mi><mi id="S2.E4.m2.1.1.1.1.1.3.2.3" xref="S2.E4.m2.1.1.1.1.1.3.2.3.cmml">m</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m2.1.1.1.1.1.3.1" xref="S2.E4.m2.1.1.1.1.1.3.1.cmml">⋅</mo><msup id="S2.E4.m2.1.1.1.1.1.3.3" xref="S2.E4.m2.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m2.1.1.1.1.1.3.3.2" xref="S2.E4.m2.1.1.1.1.1.3.3.2.cmml">w</mi><mo id="S2.E4.m2.1.1.1.1.1.3.3.3" xref="S2.E4.m2.1.1.1.1.1.3.3.3.cmml">′</mo></msup></mrow></mrow></mrow><mo id="S2.E4.m2.1.1.1.2" xref="S2.E4.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m2.1b"><apply id="S2.E4.m2.1.1.1.1.cmml" xref="S2.E4.m2.1.1.1"><eq id="S2.E4.m2.1.1.1.1.2.cmml" xref="S2.E4.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S2.E4.m2.1.1.1.1.3.cmml" xref="S2.E4.m2.1.1.1.1.3">absent</csymbol><apply id="S2.E4.m2.1.1.1.1.1.cmml" xref="S2.E4.m2.1.1.1.1.1"><plus id="S2.E4.m2.1.1.1.1.1.2.cmml" xref="S2.E4.m2.1.1.1.1.1.2"></plus><apply id="S2.E4.m2.1.1.1.1.1.1.cmml" xref="S2.E4.m2.1.1.1.1.1.1"><ci id="S2.E4.m2.1.1.1.1.1.1.2.cmml" xref="S2.E4.m2.1.1.1.1.1.1.2">⋅</ci><apply id="S2.E4.m2.1.1.1.1.1.1.3.cmml" xref="S2.E4.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m2.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m2.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m2.1.1.1.1.1.1.3.2">𝐻</ci><ci id="S2.E4.m2.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m2.1.1.1.1.1.1.3.3">𝑤</ci></apply><apply id="S2.E4.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1"><minus id="S2.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S2.E4.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.2">𝑤</ci><ci id="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m2.1.1.1.1.1.1.1.1.1.3.3">′</ci></apply></apply></apply><apply id="S2.E4.m2.1.1.1.1.1.3.cmml" xref="S2.E4.m2.1.1.1.1.1.3"><ci id="S2.E4.m2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m2.1.1.1.1.1.3.1">⋅</ci><apply id="S2.E4.m2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m2.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m2.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m2.1.1.1.1.1.3.2.2">𝐻</ci><ci id="S2.E4.m2.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m2.1.1.1.1.1.3.2.3">𝑚</ci></apply><apply id="S2.E4.m2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m2.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m2.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m2.1.1.1.1.1.3.3">superscript</csymbol><ci id="S2.E4.m2.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m2.1.1.1.1.1.3.3.2">𝑤</ci><ci id="S2.E4.m2.1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m2.1.1.1.1.1.3.3.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m2.1c">\displaystyle=H_{w}\cdot(1-w^{\prime})+H_{m}\cdot w^{\prime},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p4.4" class="ltx_p">where the Weight Selector contains a set of learnable weights <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">W</annotation></semantics></math> that are selected according to a specific language <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">l</annotation></semantics></math> from the LID Adapter in <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">L</annotation></semantics></math>, corresponding to the weights <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><mi id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><ci id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">w</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">Text Decoder</span>
The text decoder is built upon the phi-3-mini model <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/microsoft/Phi-3-mini-4k-instruct</span></span></span>, a language model with 3.8 billion parameters trained on 3.3 trillion tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. This model demonstrates robust overall performance. The prompt and text labels are represented by the tokenizer embedding layer, which is then concatenated with <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="E_{speech}" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><msub id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml"><mi id="S2.SS1.p5.1.m1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.2.cmml">E</mi><mrow id="S2.SS1.p5.1.m1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.3.cmml"><mi id="S2.SS1.p5.1.m1.1.1.3.2" xref="S2.SS1.p5.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.3.1" xref="S2.SS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p5.1.m1.1.1.3.3" xref="S2.SS1.p5.1.m1.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.3.1a" xref="S2.SS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p5.1.m1.1.1.3.4" xref="S2.SS1.p5.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.3.1b" xref="S2.SS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p5.1.m1.1.1.3.5" xref="S2.SS1.p5.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.3.1c" xref="S2.SS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p5.1.m1.1.1.3.6" xref="S2.SS1.p5.1.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p5.1.m1.1.1.3.1d" xref="S2.SS1.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p5.1.m1.1.1.3.7" xref="S2.SS1.p5.1.m1.1.1.3.7.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2">𝐸</ci><apply id="S2.SS1.p5.1.m1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3"><times id="S2.SS1.p5.1.m1.1.1.3.1.cmml" xref="S2.SS1.p5.1.m1.1.1.3.1"></times><ci id="S2.SS1.p5.1.m1.1.1.3.2.cmml" xref="S2.SS1.p5.1.m1.1.1.3.2">𝑠</ci><ci id="S2.SS1.p5.1.m1.1.1.3.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3.3">𝑝</ci><ci id="S2.SS1.p5.1.m1.1.1.3.4.cmml" xref="S2.SS1.p5.1.m1.1.1.3.4">𝑒</ci><ci id="S2.SS1.p5.1.m1.1.1.3.5.cmml" xref="S2.SS1.p5.1.m1.1.1.3.5">𝑒</ci><ci id="S2.SS1.p5.1.m1.1.1.3.6.cmml" xref="S2.SS1.p5.1.m1.1.1.3.6">𝑐</ci><ci id="S2.SS1.p5.1.m1.1.1.3.7.cmml" xref="S2.SS1.p5.1.m1.1.1.3.7">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">E_{speech}</annotation></semantics></math> from the language-adapted connector. These embeddings are then fed into the text decoder, with the output target being the text labels.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Multi-task Training</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Our multi-task training includes Cross-Entropy (CE) loss for the decoder, CTC loss and LID loss for the language-adapted connector. An illustration of the overall architecture is shown in Fig <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">CE Loss</span>
The CE loss is used in the LLM to optimize the model’s final recognition or translation results.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">CTC Loss</span>
To facilitate the transformation of speech representations into the LLM’s textual representations in the language-adapted connector, we employ an additional CTC loss in <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="H_{mix}" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><msub id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">H</mi><mrow id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.m1.1.1.3.2" xref="S2.SS2.p3.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.1.1.3.1" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.1.m1.1.1.3.3" xref="S2.SS2.p3.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.1.1.3.1a" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.1.m1.1.1.3.4" xref="S2.SS2.p3.1.m1.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">𝐻</ci><apply id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3"><times id="S2.SS2.p3.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.m1.1.1.3.1"></times><ci id="S2.SS2.p3.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.m1.1.1.3.2">𝑚</ci><ci id="S2.SS2.p3.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3.3">𝑖</ci><ci id="S2.SS2.p3.1.m1.1.1.3.4.cmml" xref="S2.SS2.p3.1.m1.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">H_{mix}</annotation></semantics></math> to impose constraints.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.2" class="ltx_p"><span id="S2.SS2.p4.2.1" class="ltx_text ltx_font_bold">LID Loss</span>
We incorporate an LID prediction loss to select appropriate weights based on specific languages to enhance representation fusion. Specifically, the LID Adapter performs pooling operations to reduce <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="H_{m}" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><msub id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml"><mi id="S2.SS2.p4.1.m1.1.1.2" xref="S2.SS2.p4.1.m1.1.1.2.cmml">H</mi><mi id="S2.SS2.p4.1.m1.1.1.3" xref="S2.SS2.p4.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.1.m1.1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p4.1.m1.1.1.2.cmml" xref="S2.SS2.p4.1.m1.1.1.2">𝐻</ci><ci id="S2.SS2.p4.1.m1.1.1.3.cmml" xref="S2.SS2.p4.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">H_{m}</annotation></semantics></math> and <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="H_{w}" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><msub id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.2" xref="S2.SS2.p4.2.m2.1.1.2.cmml">H</mi><mi id="S2.SS2.p4.2.m2.1.1.3" xref="S2.SS2.p4.2.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.2.m2.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p4.2.m2.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.2">𝐻</ci><ci id="S2.SS2.p4.2.m2.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">H_{w}</annotation></semantics></math> to one-dimensional representations, which are then summed to form LID representations. The LID loss optimizes these representations for language prediction. Once a specific language is predicted, the Weight Selector gets the corresponding weight, which is sent to the mixing component for weighted fusion.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>WER (%) results on Multilingual Librispeech for different methods. For the Baseline and Ideal-LLM Base models, we use 10 kh English, while all other models are 44 kh.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:81.1pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.6pt,22.8pt) scale(0.638696024198318,0.638696024198318) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Model</th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Training Step (k)</th>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Trainable Params (B)</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.4.1" class="ltx_text ltx_font_italic">en</span></td>
<td id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.5.1" class="ltx_text ltx_font_italic">de</span></td>
<td id="S2.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.6.1" class="ltx_text ltx_font_italic">nl</span></td>
<td id="S2.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.7.1" class="ltx_text ltx_font_italic">fr</span></td>
<td id="S2.T1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.8.1" class="ltx_text ltx_font_italic">es</span></td>
<td id="S2.T1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.9.1" class="ltx_text ltx_font_italic">it</span></td>
<td id="S2.T1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.10.1" class="ltx_text ltx_font_italic">pt</span></td>
<td id="S2.T1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.11.1" class="ltx_text ltx_font_italic">pl</span></td>
<td id="S2.T1.1.1.1.1.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Avg</td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<th id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Duration (kh)</th>
<th id="S2.T1.1.1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">10 / 44</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">1.97</td>
<td id="S2.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">1.55</td>
<td id="S2.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">1.08</td>
<td id="S2.T1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S2.T1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S2.T1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">0.16</td>
<td id="S2.T1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">0.10</td>
<td id="S2.T1.1.1.2.2.12" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<th id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MMS CTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<th id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">50</th>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">1.0</td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T1.1.1.3.3.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">8.7</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<th id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA with ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<th id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">250</th>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_center">0.240</td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center">6.2</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.4.4.5.1" class="ltx_text ltx_font_bold">6.7</span></td>
<td id="S2.T1.1.1.4.4.6" class="ltx_td ltx_align_center">11.3</td>
<td id="S2.T1.1.1.4.4.7" class="ltx_td ltx_align_center">5.5</td>
<td id="S2.T1.1.1.4.4.8" class="ltx_td ltx_align_center">5.2</td>
<td id="S2.T1.1.1.4.4.9" class="ltx_td ltx_align_center">10.8</td>
<td id="S2.T1.1.1.4.4.10" class="ltx_td ltx_align_center">16.2</td>
<td id="S2.T1.1.1.4.4.11" class="ltx_td ltx_align_center">15.9</td>
<td id="S2.T1.1.1.4.4.12" class="ltx_td ltx_nopad_r ltx_align_center">9.73</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<th id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<th id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">26</th>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.075</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">7.42</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">9.55</td>
<td id="S2.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">14.05</td>
<td id="S2.T1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">8.18</td>
<td id="S2.T1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">7.51</td>
<td id="S2.T1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">14.64</td>
<td id="S2.T1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">12.69</td>
<td id="S2.T1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">14.60</td>
<td id="S2.T1.1.1.5.5.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">11.59</td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<th id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ideal-LLM Base</th>
<th id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">26</th>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_center">0.172</td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center">7.44</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_center">8.25</td>
<td id="S2.T1.1.1.6.6.6" class="ltx_td ltx_align_center">12.47</td>
<td id="S2.T1.1.1.6.6.7" class="ltx_td ltx_align_center">6.71</td>
<td id="S2.T1.1.1.6.6.8" class="ltx_td ltx_align_center">5.47</td>
<td id="S2.T1.1.1.6.6.9" class="ltx_td ltx_align_center">11.84</td>
<td id="S2.T1.1.1.6.6.10" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.6.6.10.1" class="ltx_text ltx_font_bold">10.87</span></td>
<td id="S2.T1.1.1.6.6.11" class="ltx_td ltx_align_center">9.38</td>
<td id="S2.T1.1.1.6.6.12" class="ltx_td ltx_nopad_r ltx_align_center">9.05</td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<th id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ideal-LLM Large</th>
<th id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">50</th>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.303</td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.4.1" class="ltx_text ltx_font_bold">6.15</span></td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb">7.12</td>
<td id="S2.T1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.6.1" class="ltx_text ltx_font_bold">11.23</span></td>
<td id="S2.T1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.7.1" class="ltx_text ltx_font_bold">5.40</span></td>
<td id="S2.T1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.8.1" class="ltx_text ltx_font_bold">4.26</span></td>
<td id="S2.T1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.9.1" class="ltx_text ltx_font_bold">9.93</span></td>
<td id="S2.T1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_bb">12.41</td>
<td id="S2.T1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.11.1" class="ltx_text ltx_font_bold">6.02</span></td>
<td id="S2.T1.1.1.7.7.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.7.7.12.1" class="ltx_text ltx_font_bold">7.81</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">The total loss is formulated as follows:</p>
<table id="S4.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E5.m1.1" class="ltx_Math" alttext="\displaystyle L_{\text{all}}=(1-\alpha)\cdot L_{\text{decoder}}+\alpha\cdot L_{\text{CTC}}+\beta\cdot L_{\text{LID}}" display="inline"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><msub id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.3.2" xref="S2.E5.m1.1.1.3.2.cmml">L</mi><mtext id="S2.E5.m1.1.1.3.3" xref="S2.E5.m1.1.1.3.3a.cmml">all</mtext></msub><mo id="S2.E5.m1.1.1.2" xref="S2.E5.m1.1.1.2.cmml">=</mo><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml"><mn id="S2.E5.m1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S2.E5.m1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S2.E5.m1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo rspace="0.055em" stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml">⋅</mo><msub id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.3.2.cmml">L</mi><mtext id="S2.E5.m1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.3.3a.cmml">decoder</mtext></msub></mrow><mo id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">+</mo><mrow id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.3.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m1.1.1.1.3.1" xref="S2.E5.m1.1.1.1.3.1.cmml">⋅</mo><msub id="S2.E5.m1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.3.3.cmml"><mi id="S2.E5.m1.1.1.1.3.3.2" xref="S2.E5.m1.1.1.1.3.3.2.cmml">L</mi><mtext id="S2.E5.m1.1.1.1.3.3.3" xref="S2.E5.m1.1.1.1.3.3.3a.cmml">CTC</mtext></msub></mrow><mo id="S2.E5.m1.1.1.1.2a" xref="S2.E5.m1.1.1.1.2.cmml">+</mo><mrow id="S2.E5.m1.1.1.1.4" xref="S2.E5.m1.1.1.1.4.cmml"><mi id="S2.E5.m1.1.1.1.4.2" xref="S2.E5.m1.1.1.1.4.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m1.1.1.1.4.1" xref="S2.E5.m1.1.1.1.4.1.cmml">⋅</mo><msub id="S2.E5.m1.1.1.1.4.3" xref="S2.E5.m1.1.1.1.4.3.cmml"><mi id="S2.E5.m1.1.1.1.4.3.2" xref="S2.E5.m1.1.1.1.4.3.2.cmml">L</mi><mtext id="S2.E5.m1.1.1.1.4.3.3" xref="S2.E5.m1.1.1.1.4.3.3a.cmml">LID</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><eq id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1.2"></eq><apply id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.3.2">𝐿</ci><ci id="S2.E5.m1.1.1.3.3a.cmml" xref="S2.E5.m1.1.1.3.3"><mtext mathsize="70%" id="S2.E5.m1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.3.3">all</mtext></ci></apply><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><plus id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></plus><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1"><ci id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2">⋅</ci><apply id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1"><minus id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S2.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S2.E5.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.2">𝐿</ci><ci id="S2.E5.m1.1.1.1.1.3.3a.cmml" xref="S2.E5.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S2.E5.m1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3">decoder</mtext></ci></apply></apply><apply id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3"><ci id="S2.E5.m1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.3.1">⋅</ci><ci id="S2.E5.m1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.3.2">𝛼</ci><apply id="S2.E5.m1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.3.3.1.cmml" xref="S2.E5.m1.1.1.1.3.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.3.3.2.cmml" xref="S2.E5.m1.1.1.1.3.3.2">𝐿</ci><ci id="S2.E5.m1.1.1.1.3.3.3a.cmml" xref="S2.E5.m1.1.1.1.3.3.3"><mtext mathsize="70%" id="S2.E5.m1.1.1.1.3.3.3.cmml" xref="S2.E5.m1.1.1.1.3.3.3">CTC</mtext></ci></apply></apply><apply id="S2.E5.m1.1.1.1.4.cmml" xref="S2.E5.m1.1.1.1.4"><ci id="S2.E5.m1.1.1.1.4.1.cmml" xref="S2.E5.m1.1.1.1.4.1">⋅</ci><ci id="S2.E5.m1.1.1.1.4.2.cmml" xref="S2.E5.m1.1.1.1.4.2">𝛽</ci><apply id="S2.E5.m1.1.1.1.4.3.cmml" xref="S2.E5.m1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.4.3.1.cmml" xref="S2.E5.m1.1.1.1.4.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.4.3.2.cmml" xref="S2.E5.m1.1.1.1.4.3.2">𝐿</ci><ci id="S2.E5.m1.1.1.1.4.3.3a.cmml" xref="S2.E5.m1.1.1.1.4.3.3"><mtext mathsize="70%" id="S2.E5.m1.1.1.1.4.3.3.cmml" xref="S2.E5.m1.1.1.1.4.3.3">LID</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">\displaystyle L_{\text{all}}=(1-\alpha)\cdot L_{\text{decoder}}+\alpha\cdot L_{\text{CTC}}+\beta\cdot L_{\text{LID}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p5.2" class="ltx_p">This training strategy ensures the model learns to perform recognition and translation while accurately weighting and fusing language-specific representations.
We will open source the training code once the paper is accepted.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Multilingual ASR</span>
For the multilingual ASR task, we use the Multilingual LibriSpeech (MLS) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, following prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This dataset is a 50,000-hour ASR corpus derived from reading audiobooks on LibriVox. It comprises eight languages: English (<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">en</span>), German (<span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">de</span>), Dutch (<span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">nl</span>), French (<span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">fr</span>), Spanish (<span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_italic">es</span>), Italian (<span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_italic">it</span>), Portuguese (<span id="S3.SS1.p1.1.8" class="ltx_text ltx_font_italic">pt</span>), and Polish (<span id="S3.SS1.p1.1.9" class="ltx_text ltx_font_italic">pl</span>). The dataset predominantly consists of English recordings, with 44,500 hours dedicated to this language. We evaluated performance using the WER metric.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Multilingual AST</span>
For the multilingual AST task, we use the CovoST2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, following Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To utilize the model obtained from the ASR task, we select the intersection of the languages in MLS: English to German (<span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">en-de</span>), German to English (<span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">de-en</span>), French to English (<span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_italic">fr-en</span>), Spanish to English (<span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_italic">es-en</span>), and Italian to English (<span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_italic">it-en</span>). Performance is evaluated using BLEU scores, employing the sacrebleu tool for comparison <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/mjpost/sacrebleu</span></span></span>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Experiment Setup</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Baseline</span>
Ideal-LLM integrates dual encoders and a language-adapted LLM, making the most relevant baseline is a model that integrates an encoder with an LLM.
For the baseline model, we use a combination of the Whisper encoder and the phi-3-mini model, where the Whisper encoder is from Whisper Large-v3 <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/openai/whisper-large-v3</span></span></span>. The connector consists of only the Whisper adapter, convolutional, and projector layers. The Whisper Adapter is a 4-layer Transformer encoder. The convolutional layer performs 2x downsampling, and the projector is a linear layer that maps feature dimensions to the LLM embedding dimensions. We use 10k hours of English data and the full data for the other seven languages during ASR task training for convenience. We also applied a data balancing strategy as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The Adam optimizer has a peak learning rate of 5e-4 and a warmup of 2k steps for 26k training steps. We use 8 NVIDIA 4090 GPUs with 24GB of memory each, with gradient accumulation equivalent to about 400s of data per GPU. During training, only the connector is trainable. For the AST task, training is initialized with parameters from the ASR model, using a peak learning rate of 1e-4, a warmup of 2k steps, and a total of 10k steps. The prompt for the ASR task is <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">“Transcribe the speech to text,”</span> and for the AST task, it is <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">“Translate the speech to </span>{<span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">language</span>}<span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">.”</span>, where the language is English or German.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p"><span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_bold">Ideal-LLM Base</span>
In the proposed base model setup, the Whisper encoder remains the same as in the baseline model, and the MMS encoder is the 300M version <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/facebook/mms-300m</span></span></span>. Both Whisper and MMS adapters are 4-layer Transformer encoders. The LID Adapter is a linear layer that maps feature dimensions to 8 languages. During training, only the language-adapted connector is trainable. For multi-task training, we set <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\alpha</annotation></semantics></math> to 0.1 and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\beta</annotation></semantics></math> to 0.05. The experimental data and training strategy are the same as the baseline model.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Ideal-LLM Large</span>
For the large model, the MMS encoder uses the 1B version <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/facebook/mms-1b</span></span></span>. The Whisper Adapter and the MMS Adapter are 9-layer Transformer encoders, and the rest of the configurations are the same as for the Ideal-LLM base model. We use the full MLS dataset for training, which increases the amount of data mainly in English compared to the baseline and Ideal-LLM base models. The Adam optimizer has a peak learning rate of 2e-4, a warmup of 2000 steps, and 50k training steps. We use 8 NVIDIA A6000 GPUs with 48GB of memory each, with gradient accumulation equivalent to about 800s data per GPU. For inference, we use the best five models for average decoding. Training for the AST task is initialized with parameters from the ASR task model, using a peak learning rate of 5e-5, a warmup of 2k steps, and a total of 10k training steps.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>BLEU scores (%) for the AST task on the CoVoST2 dataset.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:169.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(9.1pt,-3.5pt) scale(1.04368713640176,1.04368713640176) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.2.1" class="ltx_text ltx_font_italic">en-de</span></td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.3.1" class="ltx_text ltx_font_italic">de-en</span></td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.4.1" class="ltx_text ltx_font_italic">es-en</span></td>
<td id="S3.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.5.1" class="ltx_text ltx_font_italic">fr-en</span></td>
<td id="S3.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.6.1" class="ltx_text ltx_font_italic">it-en</span></td>
<td id="S3.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Avg</td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<th id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Duration (h)</th>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">364</td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">119</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">97</td>
<td id="S3.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">180</td>
<td id="S3.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">28</td>
<td id="S3.T2.1.1.2.2.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<th id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Speech-LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">27.1</td>
<td id="S3.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">27.9</td>
<td id="S3.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">25.2</td>
<td id="S3.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">25.9</td>
<td id="S3.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<th id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.4.4.2.1" class="ltx_text ltx_font_bold">29.9</span></td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_center">35.2</td>
<td id="S3.T2.1.1.4.4.4" class="ltx_td ltx_align_center">40.0</td>
<td id="S3.T2.1.1.4.4.5" class="ltx_td ltx_align_center">38.5</td>
<td id="S3.T2.1.1.4.4.6" class="ltx_td ltx_align_center">36.3</td>
<td id="S3.T2.1.1.4.4.7" class="ltx_td ltx_align_center">35.98</td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<th id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">22.7</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">34.1</td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">38.6</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">36.1</td>
<td id="S3.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">34.7</td>
<td id="S3.T2.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">33.24</td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<th id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ideal-LLM Base</th>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_center">23.3</td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_center">34.4</td>
<td id="S3.T2.1.1.6.6.4" class="ltx_td ltx_align_center">39.3</td>
<td id="S3.T2.1.1.6.6.5" class="ltx_td ltx_align_center">37.1</td>
<td id="S3.T2.1.1.6.6.6" class="ltx_td ltx_align_center">34.4</td>
<td id="S3.T2.1.1.6.6.7" class="ltx_td ltx_align_center">33.70</td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<th id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">      + COT</th>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_center">24.6</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_center">37.8</td>
<td id="S3.T2.1.1.7.7.4" class="ltx_td ltx_align_center">40.9</td>
<td id="S3.T2.1.1.7.7.5" class="ltx_td ltx_align_center">38.7</td>
<td id="S3.T2.1.1.7.7.6" class="ltx_td ltx_align_center">37.4</td>
<td id="S3.T2.1.1.7.7.7" class="ltx_td ltx_align_center">35.88</td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<th id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ideal-LLM Large</th>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_center">23.7</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_center">34.9</td>
<td id="S3.T2.1.1.8.8.4" class="ltx_td ltx_align_center">39.9</td>
<td id="S3.T2.1.1.8.8.5" class="ltx_td ltx_align_center">37.8</td>
<td id="S3.T2.1.1.8.8.6" class="ltx_td ltx_align_center">35.2</td>
<td id="S3.T2.1.1.8.8.7" class="ltx_td ltx_align_center">34.30</td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<th id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">      + COT</th>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb">25.9</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.9.9.3.1" class="ltx_text ltx_font_bold">38.5</span></td>
<td id="S3.T2.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.9.9.4.1" class="ltx_text ltx_font_bold">41.5</span></td>
<td id="S3.T2.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.9.9.5.1" class="ltx_text ltx_font_bold">40.0</span></td>
<td id="S3.T2.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.9.9.6.1" class="ltx_text ltx_font_bold">38.0</span></td>
<td id="S3.T2.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.9.9.7.1" class="ltx_text ltx_font_bold">36.78</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Main Results</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">ASR Results</span>
Table <a href="#S2.T1" title="TABLE I ‣ II-B Multi-task Training ‣ II Method ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> presents the WER results for the ASR task on the MLS dataset, using two existing works for reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. MMS CTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> uses the full MLS dataset with CTC training on a 1B MMS encoder and adds an n-gram language model for inference. LLaMA with ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> employs an encoder trained with CTC loss on MLS data to generate speech features and uses LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> as the text decoder with LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
The experimental results show that the Ideal-LLM Base significantly decreases WER across all languages compared to the baseline, achieving a relative decrease of 19.3% in average WER. This indicates that our proposed model better utilizes multilingual speech representation. The Ideal-LLM large model, which includes more data, a larger encoder and an increased number of trainable parameters (0.13B), shows a relative decrease of 16.3% in average WER, resulting in a 32.6% decrease compared to the baseline model. Despite a slight regression in WER for Portuguese, we think this is due to the data imbalance caused by the addition of a large amount of English data and Portuguese is more sensitive to the imbalance. The Ideal-LLM large model also outperforms existing works, demonstrating its potential for multilingual ASR.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>WER (%) results on 8 languages of MLS in proposed base model for ablation study.</figcaption>
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:111.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(35.0pt,-10.5pt) scale(1.23419695282266,1.23419695282266) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S3.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Trainable params (B)</th>
<th id="S3.T3.1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.2.1" class="ltx_tr">
<th id="S3.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Ideal-LLM Base</th>
<td id="S3.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.172</td>
<td id="S3.T3.1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S3.T3.1.1.2.1.3.1" class="ltx_text ltx_font_bold">9.05</span></td>
</tr>
<tr id="S3.T3.1.1.3.2" class="ltx_tr">
<th id="S3.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">      - Weight Selector</th>
<td id="S3.T3.1.1.3.2.2" class="ltx_td ltx_align_center">0.172</td>
<td id="S3.T3.1.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">9.30</td>
</tr>
<tr id="S3.T3.1.1.4.3" class="ltx_tr">
<th id="S3.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">           - Dual Encoders</th>
<td id="S3.T3.1.1.4.3.2" class="ltx_td ltx_align_center">0.116</td>
<td id="S3.T3.1.1.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center">10.52</td>
</tr>
<tr id="S3.T3.1.1.5.4" class="ltx_tr">
<th id="S3.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">              - CTC Loss</th>
<td id="S3.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.075</td>
<td id="S3.T3.1.1.5.4.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">11.59</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">AST Results</span>
Table <a href="#S3.T2" title="TABLE II ‣ III-B Experiment Setup ‣ III Experiments ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the BLEU scores for the AST task on the CoVoST2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, referencing two existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Speech-LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> combines a simple encoder with LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, while Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> utilizes a fine-tuned Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Qwen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> decoder. Both baseline and Ideal-LLM models are configured as described in the previous section. We also employ a COT prompt to enhance performance. The prompt is: <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">“First transcribe the speech to text, and then translate the speech to language.”</span>, where language can be either English or German.
The experimental results reveal that both the Ideal-LLM base and large models increase BLEU scores compared to the baseline, with the large model achieving an increase of 1 point in average BLEU scores. The inclusion of the COT prompt further improves the BLEU scores of the Ideal-LLM models. Ideal-LLM Large with COT outperforms Qwen2-Audio, showcasing its potential for multilingual AST.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Analysis</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">Ablation Study</span>
We conduct ablation experiments on the dual encoder structure and the language-adapted connector proposed in the paper. The language-adapted connector comprises the CTC loss function for converting the speech feature to the text space and the Weight Selector module for learning language-specific weights.
From the experimental results shown in Table <a href="#S3.T3" title="TABLE III ‣ III-C Main Results ‣ III Experiments ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, removing the Weight Selector structure increases the average WER from 9.05 to 9.30. Additionally, removing the dual-encoder structure leads to an 11.6% relative increase in WER, highlighting the dual-encoder’s critical role in providing richer multilingual representations. Furthermore, eliminating the CTC loss function results in a 9.2% relative increase in WER, indicating that the inclusion of the CTC loss in the connector effectively aids the transformation of speech representation.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Weight Selector</span>
We analyze whether Weight Selector learns based on the pre-training methods and datasets of Whisper and MMS. As shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-D Analysis ‣ III Experiments ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the blue and green lines represent the weights of the MMS encoder for the Ideal-LLM base and large models, respectively.
When comparing these two lines, we observe that the overall weight of Ideal-LLM large is approximately 0.06 higher than the base model. We attribute this to the stronger SSL capability of the MMS 1B encoder, demonstrating that the fusion is correlated with the pre-training methods. The overall trend for the two lines is similar, likely due to the same data distribution used during pre-training of MMS 1B and 300M.
The weight for <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">en</span> reveals that since Whisper pre-trains with a significantly larger amount of <span id="S3.SS4.p2.1.3" class="ltx_text ltx_font_italic">en</span> data than MMS, it is weighted substantially lower than other languages. For languages such as <span id="S3.SS4.p2.1.4" class="ltx_text ltx_font_italic">nl</span>, <span id="S3.SS4.p2.1.5" class="ltx_text ltx_font_italic">it</span>, and <span id="S3.SS4.p2.1.6" class="ltx_text ltx_font_italic">pl</span>, Whisper is pre-trained with only 2k hours of data compared to approximately 20k for MMS, resulting in weights skewed in favor of MMS for these languages compared to the others.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Speech Embedding</span>
We conduct a T-SNE analysis of the speech embedding obtained from the Baseline model and the Ideal-LLM model. To verify the robustness of the models, we randomly selected 900 sentences for each language from the test set of CommonVoice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, an out-of-domain dataset. As illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-D Analysis ‣ III Experiments ‣ Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="E_{speech}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">E</mi><mrow id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1a" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.4" xref="S3.SS4.p3.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1b" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.5" xref="S3.SS4.p3.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1c" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.6" xref="S3.SS4.p3.1.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1d" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.7" xref="S3.SS4.p3.1.m1.1.1.3.7.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝐸</ci><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><times id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3">𝑝</ci><ci id="S3.SS4.p3.1.m1.1.1.3.4.cmml" xref="S3.SS4.p3.1.m1.1.1.3.4">𝑒</ci><ci id="S3.SS4.p3.1.m1.1.1.3.5.cmml" xref="S3.SS4.p3.1.m1.1.1.3.5">𝑒</ci><ci id="S3.SS4.p3.1.m1.1.1.3.6.cmml" xref="S3.SS4.p3.1.m1.1.1.3.6">𝑐</ci><ci id="S3.SS4.p3.1.m1.1.1.3.7.cmml" xref="S3.SS4.p3.1.m1.1.1.3.7">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">E_{speech}</annotation></semantics></math> from the Baseline model exhibits overlap among different languages, indicating its inability to effectively separate the languages during the conversion process. In contrast, the Ideal-LLM model demonstrates a marked reduction in overlap. This suggests that by utilizing a language-adapted connector, our proposed model successfully differentiates between languages during the embedding conversion process. This distinction is crucial for ensuring the accuracy and reliability of multilingual S2T tasks.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">Future Work</span>
In our experiment, Ideal-LLM has demonstrated superior performance over several previous works. However, there remains a gap when compared to some SOTA methods. For instance, in the AST task, our results still need to catch up to those achieved by SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and the method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. We attribute this discrepancy primarily to the limited amount of supervised data available for Ideal-LLM, which cannot match the scale and efficacy of SOTA approaches.
In the future, we plan to leverage larger-scale multilingual datasets to enhance the training of our model.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.11214/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Weight distribution of MMS Encoder (<math id="S3.F2.2.m1.1" class="ltx_Math" alttext="w^{\prime}" display="inline"><semantics id="S3.F2.2.m1.1b"><msup id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml"><mi id="S3.F2.2.m1.1.1.2" xref="S3.F2.2.m1.1.1.2.cmml">w</mi><mo id="S3.F2.2.m1.1.1.3" xref="S3.F2.2.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><apply id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.2.m1.1.1.1.cmml" xref="S3.F2.2.m1.1.1">superscript</csymbol><ci id="S3.F2.2.m1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.2">𝑤</ci><ci id="S3.F2.2.m1.1.1.3.cmml" xref="S3.F2.2.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">w^{\prime}</annotation></semantics></math>) in the Ideal-LLM base and large models across different languages.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.11214/assets/image3_joint_commonvoice.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>T-SNE analysis for <math id="S3.F3.2.m1.1" class="ltx_Math" alttext="E_{speech}" display="inline"><semantics id="S3.F3.2.m1.1b"><msub id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml"><mi id="S3.F3.2.m1.1.1.2" xref="S3.F3.2.m1.1.1.2.cmml">E</mi><mrow id="S3.F3.2.m1.1.1.3" xref="S3.F3.2.m1.1.1.3.cmml"><mi id="S3.F3.2.m1.1.1.3.2" xref="S3.F3.2.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.3.1" xref="S3.F3.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F3.2.m1.1.1.3.3" xref="S3.F3.2.m1.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.3.1b" xref="S3.F3.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F3.2.m1.1.1.3.4" xref="S3.F3.2.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.3.1c" xref="S3.F3.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F3.2.m1.1.1.3.5" xref="S3.F3.2.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.3.1d" xref="S3.F3.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F3.2.m1.1.1.3.6" xref="S3.F3.2.m1.1.1.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.F3.2.m1.1.1.3.1e" xref="S3.F3.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.F3.2.m1.1.1.3.7" xref="S3.F3.2.m1.1.1.3.7.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><apply id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.2.m1.1.1.1.cmml" xref="S3.F3.2.m1.1.1">subscript</csymbol><ci id="S3.F3.2.m1.1.1.2.cmml" xref="S3.F3.2.m1.1.1.2">𝐸</ci><apply id="S3.F3.2.m1.1.1.3.cmml" xref="S3.F3.2.m1.1.1.3"><times id="S3.F3.2.m1.1.1.3.1.cmml" xref="S3.F3.2.m1.1.1.3.1"></times><ci id="S3.F3.2.m1.1.1.3.2.cmml" xref="S3.F3.2.m1.1.1.3.2">𝑠</ci><ci id="S3.F3.2.m1.1.1.3.3.cmml" xref="S3.F3.2.m1.1.1.3.3">𝑝</ci><ci id="S3.F3.2.m1.1.1.3.4.cmml" xref="S3.F3.2.m1.1.1.3.4">𝑒</ci><ci id="S3.F3.2.m1.1.1.3.5.cmml" xref="S3.F3.2.m1.1.1.3.5">𝑒</ci><ci id="S3.F3.2.m1.1.1.3.6.cmml" xref="S3.F3.2.m1.1.1.3.6">𝑐</ci><ci id="S3.F3.2.m1.1.1.3.7.cmml" xref="S3.F3.2.m1.1.1.3.7">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">E_{speech}</annotation></semantics></math> of 900 utterances in each of the eight languages.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we propose a novel approach, Ideal-LLM, to multilingual speech-to-text tasks by integrating dual encoders with a language-adapted LLM. Our method leverages the complementary strengths of the Whisper and MMS encoders, optimizing their fusion through a CTC loss function and Weight Selector mechanism. The experimental results demonstrate significant improvements in WER across multiple languages, with a 32.6% relative decrease in average WER compared to the baseline. Additionally, our ablation studies underscore the critical role of the dual-encoder structure and the CTC loss in enhancing performance. This work highlights the potential of combining dual multilingual speech encoders with LLMs to achieve robust and adaptive multilingual speech-to-text, paving the way for more effective and inclusive language processing technologies. We will utilize larger-scale data to train our model in the future.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
OpenAI,

</span>
<span class="ltx_bibblock">“Introducing chatgpt,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">URL https://openai.com/blog/chatgpt</span>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
OpenAI,

</span>
<span class="ltx_bibblock">“Gpt-4 technical report,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.11276</span>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.,

</span>
<span class="ltx_bibblock">“Language models are few-shot learners,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al.,

</span>
<span class="ltx_bibblock">“Palm 2 technical report,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10403</span>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample,

</span>
<span class="ltx_bibblock">“Llama: Open and efficient foundation language models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass,

</span>
<span class="ltx_bibblock">“Listen, think, and understand,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10790</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang,

</span>
<span class="ltx_bibblock">“Salmonn: Towards generic hearing abilities for large language models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.13289</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou,

</span>
<span class="ltx_bibblock">“Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.07919</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al.,

</span>
<span class="ltx_bibblock">“Wavllm: Towards robust and adaptive speech large language model,”

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.00656</span>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou,

</span>
<span class="ltx_bibblock">“Qwen2-audio technical report,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2407.10759</span>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Qian Chen, and Lei Xie,

</span>
<span class="ltx_bibblock">“E-chat: Emotion-sensitive spoken dialogue system with large language models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.00475</span>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al.,

</span>
<span class="ltx_bibblock">“Prompting large language models with speech recognition abilities,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. IEEE, 2024, pp. 13351–13355.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, and Yu Wu,

</span>
<span class="ltx_bibblock">“On decoder-only architecture for speech-to-text and large language model integration,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ASRU</span>. 2023, pp. 1–8, IEEE.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, et al.,

</span>
<span class="ltx_bibblock">“An embarrassingly simple approach for llm with strong asr capacity,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.08846</span>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ye Bai, Jingping Chen, Jitong Chen, Wei Chen, Zhuo Chen, Chen Ding, Linhao Dong, Qianqian Dong, Yujiao Du, Kepan Gao, et al.,

</span>
<span class="ltx_bibblock">“Seed-asr: Understanding diverse speech and contexts with llm-based speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2407.04675</span>, 2024.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Chao-Wei Huang, Hui Lu, Hongyu Gong, Hirofumi Inaguma, Ilia Kulikov, Ruslan Mavlyutov, and Sravya Popuri,

</span>
<span class="ltx_bibblock">“Investigating decoder-only large language models for speech-to-text translation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2024, ISCA.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino J. Gomez, and Jürgen Schmidhuber,

</span>
<span class="ltx_bibblock">“Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICML</span>. 2006, vol. 148, pp. 369–376, ACM.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever,

</span>
<span class="ltx_bibblock">“Robust speech recognition via large-scale weak supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2023, vol. 202, pp. 28492–28518.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu,

</span>
<span class="ltx_bibblock">“w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">ASRU</span>. 2021, pp. 244–250, IEEE.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber,

</span>
<span class="ltx_bibblock">“Common voice: A massively-multilingual speech corpus,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">LREC</span>. 2020, pp. 4218–4222, European Language Resources Association.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino,

</span>
<span class="ltx_bibblock">“Covost 2 and massively multilingual speech translation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2021, pp. 2247–2251, ISCA.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang,

</span>
<span class="ltx_bibblock">“Connecting speech encoder and large language model for ASR,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. 2024, pp. 12637–12641, IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi,

</span>
<span class="ltx_bibblock">“BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ICML</span>. 2023, vol. 202, pp. 19730–19742, PMLR.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin,

</span>
<span class="ltx_bibblock">“Attention is all you need,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Xuelong Geng, Tianyi Xu, Kun Wei, Bingsheng Mu, Hongfei Xue, He Wang, Yangze Li, Pengcheng Guo, Yuhang Dai, Longhao Li, et al.,

</span>
<span class="ltx_bibblock">“Unveiling the potential of llm-based asr on chinese open-source datasets,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">ISCSLP</span>. 2024, ISCA.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed,

</span>
<span class="ltx_bibblock">“Hubert: Self-supervised speech representation learning by masked prediction of hidden units,”

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE ACM Trans. Audio Speech Lang. Process.</span>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al.,

</span>
<span class="ltx_bibblock">“Scaling speech technology to 1,000+ languages,”

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13516</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Andrew Rouditchenko, Sameer Khurana, Samuel Thomas, Rogério Feris, Leonid Karlinsky, Hilde Kuehne, David Harwath, Brian Kingsbury, and James R. Glass,

</span>
<span class="ltx_bibblock">“Comparison of multilingual self-supervised and weakly-supervised speech pre-training for adaptation to unseen languages,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2023, pp. 2268–2272, ISCA.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.,

</span>
<span class="ltx_bibblock">“Phi-3 technical report: A highly capable language model locally on your phone,”

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.14219</span>, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert,

</span>
<span class="ltx_bibblock">“MLS: A large-scale multilingual dataset for speech research,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2020, pp. 2757–2761, ISCA.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli,

</span>
<span class="ltx_bibblock">“Unsupervised cross-lingual representation learning for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2021, pp. 2426–2430, ISCA.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen,

</span>
<span class="ltx_bibblock">“Lora: Low-rank adaptation of large language models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">ICLR</span>. 2022, OpenReview.net.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.,

</span>
<span class="ltx_bibblock">“Qwen technical report,”

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.16609</span>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al.,

</span>
<span class="ltx_bibblock">“Seamlessm4t-massively multilingual &amp; multimodal machine translation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.11596</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.11213" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.11214" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.11214">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.11214" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.11215" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 23:42:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
