<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.08801] Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation</title><meta property="og:description" content="The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits.
This research delves into the complexities of synchron…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.08801">

<!--Generated on Fri Jul  5 18:55:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mingwang Xu<sup id="id14.14.id1" class="ltx_sup"><span id="id14.14.id1.1" class="ltx_text ltx_font_italic">1∗</span></sup>  Hui Li<sup id="id15.15.id2" class="ltx_sup"><span id="id15.15.id2.1" class="ltx_text ltx_font_italic">1∗</span></sup>  Qingkun Su<sup id="id16.16.id3" class="ltx_sup"><span id="id16.16.id3.1" class="ltx_text ltx_font_italic">1∗</span></sup>  Hanlin Shang<sup id="id17.17.id4" class="ltx_sup">1</sup>  Liwei Zhang<sup id="id18.18.id5" class="ltx_sup">1</sup>  <span id="id6.6.1" class="ltx_text ltx_font_bold">Ce Liu<sup id="id6.6.1.1" class="ltx_sup"><span id="id6.6.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span> 
<br class="ltx_break"><span id="id7.7.2" class="ltx_text ltx_font_bold">Jingdong Wang<sup id="id7.7.2.1" class="ltx_sup"><span id="id7.7.2.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>  <span id="id8.8.3" class="ltx_text ltx_font_bold">Yao Yao<sup id="id8.8.3.1" class="ltx_sup"><span id="id8.8.3.1.1" class="ltx_text ltx_font_medium">4</span></sup></span>  <span id="id9.9.4" class="ltx_text ltx_font_bold">Siyu Zhu<sup id="id9.9.4.1" class="ltx_sup"><span id="id9.9.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span> 
<br class="ltx_break"><sup id="id19.19.id6" class="ltx_sup">1</sup>Fudan University  <sup id="id20.20.id7" class="ltx_sup">2</sup>Baidu Inc  <sup id="id21.21.id8" class="ltx_sup">3</sup>ETH Zurich  <sup id="id22.22.id9" class="ltx_sup">4</sup>Nanjing University
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id23.id1" class="ltx_p">The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits.
This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies.
Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion.
Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network.
The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities.
Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity.
Further visualization and access to the source code can be found at: <a target="_blank" href="https://fudan-generative-vision.github.io/hallo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fudan-generative-vision.github.io/hallo</a>.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup">∗</sup> indicates equal contribution.</span></span></span>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/best_visual_results.jpg" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="462" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">
The proposed methodology aims to generate portrait image animations that are temporally consistent and visually high-fidelity.
This is achieved by utilizing a reference image, an audio sequence, and optionally, the visual synthesis weight in conjunction with a diffusion model based on the hierarchical audio-driven visual synthesis approach.
The results of this method showcase improved fidelity and visual quality in comparison to previous approaches that rely on intermediate facial representations.
Furthermore, the proposed methodology enhances the accuracy of lip synchronization measurement and enhances the control over motion diversity.
</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Portrait image animation, also known as talking head image animation, aims to generate a speaking portrait from a single static image and a corresponding speech audio.
This technology holds substantial value across various domains such as video gaming and virtual reality, film and television production, social media and digital marketing, online education and training, as well as human-computer interaction and virtual assistants.
Significant advancements in this field are exemplified by works like Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and DiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which involve the gradual incorporation of noise into the training data in the latent space, followed by a reverse process that progressively reconstructs the signal from this noise.
By leveraging parametric or implicit representations of lip movements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, facial expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and head poses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, combined with diffusion techniques in the latent space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, it is possible to generate high-quality, lifelike dynamic portraits in an end-to-end manner.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This study focuses on two primary challenges in portrait image animation:
(1) The synchronization and coordination of lip movements, facial expressions, and head poses driven by the audio input.
(2) The creation of high-fidelity animations that are visually appealing and maintain temporal consistency.
Addressing the first challenge, parametric model-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> initially rely on input audio to derive sequences of facial landmarks or parametric motion coefficients, such as 3DMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, FLAME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and HeadNeRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, as intermediate representations, which are subsequently used for visual generation.
However, the effectiveness of these methods is constrained by the accuracy and expressiveness of the intermediate representations.
In contrast, another approach involves decoupled representation learning in latent space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, which segregates facial features into identity and non-identity components within a latent space.
These components are independently learned and integrated across frames.
The main obstacle in this approach is effectively disentangling the various portrait factors while encompassing static and dynamic facial attributes comprehensively.
Moreover, the introduction of intermediary representations or decoupled representations in the latent space of the aforementioned methods undermine the production of temporally consistent visual outputs with high realism and diversity.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To generate visually appealing and temporally consistent high-quality animations, the utilization of end-to-end diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> leverages recent advancements in diffusion models for synthesizing portrait videos directly from images and audio clips.
This approach eliminates the need for intermediate representations or complex preprocessing steps, enabling the production of high-resolution, detailed, and diverse visual outputs.
However, achieving precise alignment between audio and the generated facial visual synthesis, including lip movements, expressions, and poses, still remains a challenge.
In this study, we adhere to the framework of end-to-end diffusion models with the objective of addressing the alignment issue while incorporating the ability to control the diversity of expressions and poses.
To tackle this challenge, we introduce a hierarchical audio-driven visual synthesis module that employs cross-attention mechanisms to establish correspondences between audio and visual features associated with lips, expressions, and poses.
Subsequently, these cross-attentions are fused using adaptive weighting.
Building upon this hierarchical audio-driven visual synthesis module, we propose a network architecture that integrates a diffusion-based generative model with a UNet-based denoiser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, temporal alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for sequence coherence, and a ReferenceNet to guide visual generation.
As shown in Figure <a href="#S0.F1" title="Figure 1 ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this integrated approach enhances lip and expression movements to synchronize effectively with the audio signal and offers adaptive control over the granularity of expressions and poses, thereby ensuring consistency and realism across various visual identities.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In the experimental section, our proposed approach is comprehensively evaluated from both qualitative and quantitative perspectives, encompassing assessments of image and video quality, lip synchronization, and motion diversity.
Specifically, our method demonstrates a significant enhancement in image and video quality compared to previous methodologies, as quantified by the FID and FVD metrics.
Moreover, our approach exhibits a promising advancement in lip synchronization when contrasted with previous diffusion-based techniques.
Furthermore, our method offers the flexibility to adjust the diversity of expressions and poses according to specific requirements.
It is noteworthy that we are dedicated to the dissemination of our source code and sample data to the open-source community, with the aim of facilitating further research in related fields.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Diffusion-Based Video Generation.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Recent advancements in diffusion-based video generation have shown promising results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, leveraging the foundational principles of text-to-image diffusion models.
Notable efforts include Video Diffusion Models (VDM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which utilize a space-time factorized U-Net for simultaneous image and video data training, and ImagenVideo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which enhances VDM with cascaded diffusion models for high-definition outputs.
Make-A-Video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and MagicVideo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> extend these concepts to facilitate seamless text-to-video transformations.
For video editing, techniques like VideoP2P <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and vid2vid-zero <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> manipulate cross-attention maps to refine outputs, while Dreamix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> employs image-video mixed fine-tuning.
Additionally, Gen-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> integrates structural guidance through depth maps and cross-attention, whereas MCDiff <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and LaMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> focus on motion-guided video generation, enhancing the realism of human actions.
VideoComposer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, AnimateDiff <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and VideoCrafter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> further explore the synthesis of image-to-video generation by conditioning the diffusion process on images and blending image latents.
Given the remarkable efficacy of diffusion models in achieving high-fidelity and temporally consistent visual generation outcomes, an end-to-end diffusion model is adopted in this study.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Facial Representation Learning.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Learning facial representations in the latent space, particularly disentangled from identity-related appearances and non-identity-related motions such as lip movements, expressions, and poses, represents a significant challenge in the field of computer vision.
Representation learning related to this challenge generally falls into two categories: explicit and implicit methods.
Explicit methods often employ facial landmarks, which are key points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> on the face used for localization and representation of critical regions such as the mouth, eyes, eyebrows, nose, and jawline.
Additionally, work involving 3D parametric models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, like the 3D morphable Model (3DMM), captures the variability in human faces through a statistical shape model coupled with a texture model.
However, these explicit methods are limited by their expressive capabilities and the precision of reconstructions.
Conversely, implicit methods aim to learn disentangled representations in 2D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or 3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> latent spaces, focusing on aspects such as appearance identity, facial dynamics, and head pose.
While these approaches have yielded promising results in expressive facial representations, they face similar challenges to explicit methods, notably the accurate and effective disentanglement of various facial factors remains a considerable challenge.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Portrait Image Animation.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">In recent years, the field of portrait image animation has made substantial progress, particularly in generating realistic and expressive talking head animations from static images paired with audio inputs.
The advancement began with LipSyncExpert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which improved lip synchronization to speech segments, achieving high accuracy for static images and videos of specific identities.
This was followed by developments such as SadTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and DiffTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which integrated 3D information and control mechanisms to enhance the naturalism of head movements and expressions.
A major shift occurred with the introduction of diffusion models, notably Diffused Heads <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and GAIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which facilitated the creation of high-fidelity talking head videos featuring natural head motion and facial expressions without reliance on additional reference videos.
These models demonstrated robust capabilities for personalized and generalized synthesis across various identities.
Continuing this evolution, DreamTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and VividTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> utilized diffusion models to produce expressive, high-quality audio-driven facial animations, emphasizing improved lip synchronization and diverse speaking styles.
Further advancements were made by Vlogger <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and AniPortrait <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, which introduced spatial and temporal controls to accommodate variable video lengths and customizable character representations, respectively.
Recent innovations such as VASA-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and EMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> have developed frameworks for creating emotionally expressive and realistic talking faces from static images and audio, capturing a broad spectrum of facial nuances and head movements.
Lastly, AniTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> introduced a groundbreaking framework focused on generating detailed and realistic facial movements through a universal motion representation, minimizing the need for labeled data and highlighting the potential for dynamic avatar creation in practical applications.
In this study, we adopt the latent diffusion formulation as espoused in EMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and introduce a hierarchical cross-attention mechanism to augment the correlation between audio inputs and non-identity-related motions such as lip movements, expressions, and poses.
Such a formulation not only affords adaptive control over expression and pose diversity but also enhances the overall coherence and naturalness of the generated animations.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/halo.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">
The overview of the proposed pipeline.
Specifically, we integrates a reference image containing a portrait with corresponding audio input to drive portrait animation.
Optional visual synthesis weights can be used to balance lip, expression, and pose weights.
ReferenceNet encodes global visual texture information for consistent and controllable character animation.
Face and audio encoders generate high-fidelity portrait identity features and encode audio as motion information respectively.
The module of hierarchical audio-driven visual synthesis establishes relationships between audio and visual components (lips, expression, pose), with a UNet denoiser used in the diffusion process.
</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Latent Diffusion Models.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.3" class="ltx_p">Latent diffusion models represent a class of diffusion models that operate within the encoded latent space produced by an autoencoder, formalized as <math id="S3.SS1.SSS0.Px1.p1.1.m1.2" class="ltx_Math" alttext="\mathcal{D}(\mathcal{E}(\cdot))" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.2a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.3.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2.cmml">ℰ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3.2.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.2b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2"><times id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.2"></times><ci id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.3">𝒟</ci><apply id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1"><times id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.1"></times><ci id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2">ℰ</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">⋅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.2c">\mathcal{D}(\mathcal{E}(\cdot))</annotation></semantics></math>.
A prominent instance of an latent diffusion model is Stable Diffusion, which integrates a VQ-VAE autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and a time-conditioned U-Net for noise estimation.
Moreover, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> employs a CLIP ViT-L/14 text encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> to transform the input text prompt into a corresponding text embedding, which serves as a condition during the diffusion process.
During the training phase, given an image <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="I\in\mathbb{R}^{H_{I}\times W_{I}\times 3}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">I</mi><mo id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.cmml"><msub id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.2.cmml">H</mi><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.3.cmml">I</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.3.cmml">I</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1a" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.4" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><in id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1"></in><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝐼</ci><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3"><times id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.1"></times><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.2.3">𝐼</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.2">𝑊</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.3.3">𝐼</ci></apply><cn type="integer" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.4.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">I\in\mathbb{R}^{H_{I}\times W_{I}\times 3}</annotation></semantics></math> and its associated text condition <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="c_{\text{embed}}\in\mathbb{R}^{D_{c}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">c</mi><mtext id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3a.cmml">embed</mtext></msub><mo id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.3.cmml">c</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1"><in id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1"></in><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2">𝑐</ci><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3">embed</mtext></ci></apply><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">c_{\text{embed}}\in\mathbb{R}^{D_{c}}</annotation></semantics></math>, the latent representation:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="z_{0}=\mathcal{E}(I)\in\mathbb{R}^{H_{z}\times W_{z}\times D_{z}}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><msub id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml"><mi id="S3.E1.m1.1.2.2.2" xref="S3.E1.m1.1.2.2.2.cmml">z</mi><mn id="S3.E1.m1.1.2.2.3" xref="S3.E1.m1.1.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml">=</mo><mrow id="S3.E1.m1.1.2.4" xref="S3.E1.m1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.2.4.2" xref="S3.E1.m1.1.2.4.2.cmml">ℰ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.2.4.1" xref="S3.E1.m1.1.2.4.1.cmml">​</mo><mrow id="S3.E1.m1.1.2.4.3.2" xref="S3.E1.m1.1.2.4.cmml"><mo stretchy="false" id="S3.E1.m1.1.2.4.3.2.1" xref="S3.E1.m1.1.2.4.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E1.m1.1.2.4.3.2.2" xref="S3.E1.m1.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.2.5" xref="S3.E1.m1.1.2.5.cmml">∈</mo><msup id="S3.E1.m1.1.2.6" xref="S3.E1.m1.1.2.6.cmml"><mi id="S3.E1.m1.1.2.6.2" xref="S3.E1.m1.1.2.6.2.cmml">ℝ</mi><mrow id="S3.E1.m1.1.2.6.3" xref="S3.E1.m1.1.2.6.3.cmml"><msub id="S3.E1.m1.1.2.6.3.2" xref="S3.E1.m1.1.2.6.3.2.cmml"><mi id="S3.E1.m1.1.2.6.3.2.2" xref="S3.E1.m1.1.2.6.3.2.2.cmml">H</mi><mi id="S3.E1.m1.1.2.6.3.2.3" xref="S3.E1.m1.1.2.6.3.2.3.cmml">z</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.2.6.3.1" xref="S3.E1.m1.1.2.6.3.1.cmml">×</mo><msub id="S3.E1.m1.1.2.6.3.3" xref="S3.E1.m1.1.2.6.3.3.cmml"><mi id="S3.E1.m1.1.2.6.3.3.2" xref="S3.E1.m1.1.2.6.3.3.2.cmml">W</mi><mi id="S3.E1.m1.1.2.6.3.3.3" xref="S3.E1.m1.1.2.6.3.3.3.cmml">z</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.2.6.3.1a" xref="S3.E1.m1.1.2.6.3.1.cmml">×</mo><msub id="S3.E1.m1.1.2.6.3.4" xref="S3.E1.m1.1.2.6.3.4.cmml"><mi id="S3.E1.m1.1.2.6.3.4.2" xref="S3.E1.m1.1.2.6.3.4.2.cmml">D</mi><mi id="S3.E1.m1.1.2.6.3.4.3" xref="S3.E1.m1.1.2.6.3.4.3.cmml">z</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><and id="S3.E1.m1.1.2a.cmml" xref="S3.E1.m1.1.2"></and><apply id="S3.E1.m1.1.2b.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"></eq><apply id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.2.1.cmml" xref="S3.E1.m1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.2.2.cmml" xref="S3.E1.m1.1.2.2.2">𝑧</ci><cn type="integer" id="S3.E1.m1.1.2.2.3.cmml" xref="S3.E1.m1.1.2.2.3">0</cn></apply><apply id="S3.E1.m1.1.2.4.cmml" xref="S3.E1.m1.1.2.4"><times id="S3.E1.m1.1.2.4.1.cmml" xref="S3.E1.m1.1.2.4.1"></times><ci id="S3.E1.m1.1.2.4.2.cmml" xref="S3.E1.m1.1.2.4.2">ℰ</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci></apply></apply><apply id="S3.E1.m1.1.2c.cmml" xref="S3.E1.m1.1.2"><in id="S3.E1.m1.1.2.5.cmml" xref="S3.E1.m1.1.2.5"></in><share href="#S3.E1.m1.1.2.4.cmml" id="S3.E1.m1.1.2d.cmml" xref="S3.E1.m1.1.2"></share><apply id="S3.E1.m1.1.2.6.cmml" xref="S3.E1.m1.1.2.6"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.6.1.cmml" xref="S3.E1.m1.1.2.6">superscript</csymbol><ci id="S3.E1.m1.1.2.6.2.cmml" xref="S3.E1.m1.1.2.6.2">ℝ</ci><apply id="S3.E1.m1.1.2.6.3.cmml" xref="S3.E1.m1.1.2.6.3"><times id="S3.E1.m1.1.2.6.3.1.cmml" xref="S3.E1.m1.1.2.6.3.1"></times><apply id="S3.E1.m1.1.2.6.3.2.cmml" xref="S3.E1.m1.1.2.6.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.6.3.2.1.cmml" xref="S3.E1.m1.1.2.6.3.2">subscript</csymbol><ci id="S3.E1.m1.1.2.6.3.2.2.cmml" xref="S3.E1.m1.1.2.6.3.2.2">𝐻</ci><ci id="S3.E1.m1.1.2.6.3.2.3.cmml" xref="S3.E1.m1.1.2.6.3.2.3">𝑧</ci></apply><apply id="S3.E1.m1.1.2.6.3.3.cmml" xref="S3.E1.m1.1.2.6.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.6.3.3.1.cmml" xref="S3.E1.m1.1.2.6.3.3">subscript</csymbol><ci id="S3.E1.m1.1.2.6.3.3.2.cmml" xref="S3.E1.m1.1.2.6.3.3.2">𝑊</ci><ci id="S3.E1.m1.1.2.6.3.3.3.cmml" xref="S3.E1.m1.1.2.6.3.3.3">𝑧</ci></apply><apply id="S3.E1.m1.1.2.6.3.4.cmml" xref="S3.E1.m1.1.2.6.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.6.3.4.1.cmml" xref="S3.E1.m1.1.2.6.3.4">subscript</csymbol><ci id="S3.E1.m1.1.2.6.3.4.2.cmml" xref="S3.E1.m1.1.2.6.3.4.2">𝐷</ci><ci id="S3.E1.m1.1.2.6.3.4.3.cmml" xref="S3.E1.m1.1.2.6.3.4.3">𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">z_{0}=\mathcal{E}(I)\in\mathbb{R}^{H_{z}\times W_{z}\times D_{z}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px1.p1.5" class="ltx_p">undergoes a diffusion process across <math id="S3.SS1.SSS0.Px1.p1.4.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.4.m1.1a"><mi id="S3.SS1.SSS0.Px1.p1.4.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m1.1b"><ci id="S3.SS1.SSS0.Px1.p1.4.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m1.1c">T</annotation></semantics></math> timesteps.
This process is modeled as a deterministic Gaussian process, culminating in <math id="S3.SS1.SSS0.Px1.p1.5.m2.2" class="ltx_Math" alttext="z_{T}\sim\mathcal{N}(0,I)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.5.m2.2a"><mrow id="S3.SS1.SSS0.Px1.p1.5.m2.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.cmml"><msub id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.3.cmml">T</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.1" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.1.cmml">∼</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.1" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.2.1" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.SSS0.Px1.p1.5.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m2.1.1.cmml">0</mn><mo id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.5.m2.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.2.cmml">I</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m2.2b"><apply id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.1">similar-to</csymbol><apply id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.2">𝑧</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.2.3">𝑇</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3"><times id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.1"></times><ci id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.3.3.3.2"><cn type="integer" id="S3.SS1.SSS0.Px1.p1.5.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.1.1">0</cn><ci id="S3.SS1.SSS0.Px1.p1.5.m2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m2.2.2">𝐼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m2.2c">z_{T}\sim\mathcal{N}(0,I)</annotation></semantics></math>.
The training objective for Stable Diffusion is encapsulated by the following loss function:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.8" class="ltx_Math" alttext="L=\mathbb{E}_{\mathcal{E}(I),c_{\text{embed}},\epsilon\sim\mathcal{N}(0,1),t}\left[\lVert\epsilon-\epsilon_{\theta}(z_{t},t,c_{\text{embed}})\rVert_{2}^{2}\right]," display="block"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1" xref="S3.E2.m1.8.8.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.3" xref="S3.E2.m1.8.8.1.1.3.cmml">L</mi><mo id="S3.E2.m1.8.8.1.1.2" xref="S3.E2.m1.8.8.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.8.8.1.1.1" xref="S3.E2.m1.8.8.1.1.1.cmml"><msub id="S3.E2.m1.8.8.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.3.cmml"><mi id="S3.E2.m1.8.8.1.1.1.3.2" xref="S3.E2.m1.8.8.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E2.m1.6.6.6.6" xref="S3.E2.m1.6.6.6.7.cmml"><mrow id="S3.E2.m1.6.6.6.6.1" xref="S3.E2.m1.6.6.6.6.1.cmml"><mrow id="S3.E2.m1.6.6.6.6.1.2.2" xref="S3.E2.m1.6.6.6.6.1.2.3.cmml"><mrow id="S3.E2.m1.6.6.6.6.1.1.1.1" xref="S3.E2.m1.6.6.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.6.6.1.1.1.1.2.cmml">ℰ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.6.6.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.6.6.6.6.1.1.1.1.3.2" xref="S3.E2.m1.6.6.6.6.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.6.6.1.1.1.1.3.2.1" xref="S3.E2.m1.6.6.6.6.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E2.m1.6.6.6.6.1.1.1.1.3.2.2" xref="S3.E2.m1.6.6.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.6.6.1.2.2.3" xref="S3.E2.m1.6.6.6.6.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.6.6.1.2.2.2" xref="S3.E2.m1.6.6.6.6.1.2.2.2.cmml"><mi id="S3.E2.m1.6.6.6.6.1.2.2.2.2" xref="S3.E2.m1.6.6.6.6.1.2.2.2.2.cmml">c</mi><mtext id="S3.E2.m1.6.6.6.6.1.2.2.2.3" xref="S3.E2.m1.6.6.6.6.1.2.2.2.3a.cmml">embed</mtext></msub><mo id="S3.E2.m1.6.6.6.6.1.2.2.4" xref="S3.E2.m1.6.6.6.6.1.2.3.cmml">,</mo><mi id="S3.E2.m1.4.4.4.4" xref="S3.E2.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E2.m1.6.6.6.6.1.3" xref="S3.E2.m1.6.6.6.6.1.3.cmml">∼</mo><mrow id="S3.E2.m1.6.6.6.6.1.4" xref="S3.E2.m1.6.6.6.6.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.6.6.1.4.2" xref="S3.E2.m1.6.6.6.6.1.4.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.6.6.1.4.1" xref="S3.E2.m1.6.6.6.6.1.4.1.cmml">​</mo><mrow id="S3.E2.m1.6.6.6.6.1.4.3.2" xref="S3.E2.m1.6.6.6.6.1.4.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.6.6.1.4.3.2.1" xref="S3.E2.m1.6.6.6.6.1.4.3.1.cmml">(</mo><mn id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">0</mn><mo id="S3.E2.m1.6.6.6.6.1.4.3.2.2" xref="S3.E2.m1.6.6.6.6.1.4.3.1.cmml">,</mo><mn id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml">1</mn><mo stretchy="false" id="S3.E2.m1.6.6.6.6.1.4.3.2.3" xref="S3.E2.m1.6.6.6.6.1.4.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.6.6.2" xref="S3.E2.m1.6.6.6.7a.cmml">,</mo><mi id="S3.E2.m1.5.5.5.5" xref="S3.E2.m1.5.5.5.5.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.2.cmml"><mo id="S3.E2.m1.8.8.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.2.1.cmml">[</mo><msubsup id="S3.E2.m1.8.8.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml">∥</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.4.cmml">ϵ</mi><mo id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.2.cmml">ϵ</mi><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml">t</mi><mo id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">c</mi><mtext id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3a.cmml">embed</mtext></msub><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.6" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo fence="true" lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.E2.m1.8.8.1.1.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E2.m1.8.8.1.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S3.E2.m1.8.8.1.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E2.m1.8.8.1.2" xref="S3.E2.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.1.1.cmml" xref="S3.E2.m1.8.8.1"><eq id="S3.E2.m1.8.8.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2"></eq><ci id="S3.E2.m1.8.8.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.3">𝐿</ci><apply id="S3.E2.m1.8.8.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1"><times id="S3.E2.m1.8.8.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.2"></times><apply id="S3.E2.m1.8.8.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.3.1.cmml" xref="S3.E2.m1.8.8.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.3.2.cmml" xref="S3.E2.m1.8.8.1.1.1.3.2">𝔼</ci><apply id="S3.E2.m1.6.6.6.7.cmml" xref="S3.E2.m1.6.6.6.6"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.6.7a.cmml" xref="S3.E2.m1.6.6.6.6.2">formulae-sequence</csymbol><apply id="S3.E2.m1.6.6.6.6.1.cmml" xref="S3.E2.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.6.6.1.3.cmml" xref="S3.E2.m1.6.6.6.6.1.3">similar-to</csymbol><list id="S3.E2.m1.6.6.6.6.1.2.3.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2"><apply id="S3.E2.m1.6.6.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.6.6.1.1.1.1"><times id="S3.E2.m1.6.6.6.6.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.6.6.1.1.1.1.1"></times><ci id="S3.E2.m1.6.6.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.6.6.1.1.1.1.2">ℰ</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝐼</ci></apply><apply id="S3.E2.m1.6.6.6.6.1.2.2.2.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.6.6.1.2.2.2.1.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.6.6.1.2.2.2.2.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2.2.2">𝑐</ci><ci id="S3.E2.m1.6.6.6.6.1.2.2.2.3a.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2.2.3"><mtext mathsize="50%" id="S3.E2.m1.6.6.6.6.1.2.2.2.3.cmml" xref="S3.E2.m1.6.6.6.6.1.2.2.2.3">embed</mtext></ci></apply><ci id="S3.E2.m1.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E2.m1.6.6.6.6.1.4.cmml" xref="S3.E2.m1.6.6.6.6.1.4"><times id="S3.E2.m1.6.6.6.6.1.4.1.cmml" xref="S3.E2.m1.6.6.6.6.1.4.1"></times><ci id="S3.E2.m1.6.6.6.6.1.4.2.cmml" xref="S3.E2.m1.6.6.6.6.1.4.2">𝒩</ci><interval closure="open" id="S3.E2.m1.6.6.6.6.1.4.3.1.cmml" xref="S3.E2.m1.6.6.6.6.1.4.3.2"><cn type="integer" id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">0</cn><cn type="integer" id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">1</cn></interval></apply></apply><ci id="S3.E2.m1.5.5.5.5.cmml" xref="S3.E2.m1.5.5.5.5">𝑡</ci></apply></apply><apply id="S3.E2.m1.8.8.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.8.8.1.1.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.3"></minus><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.4">italic-ϵ</ci><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.3"></times><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.2">italic-ϵ</ci><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.4.3">𝜃</ci></apply><vector id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7">𝑡</ci><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2">𝑐</ci><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3a.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3">embed</mtext></ci></apply></vector></apply></apply></apply><cn type="integer" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E2.m1.8.8.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">L=\mathbb{E}_{\mathcal{E}(I),c_{\text{embed}},\epsilon\sim\mathcal{N}(0,1),t}\left[\lVert\epsilon-\epsilon_{\theta}(z_{t},t,c_{\text{embed}})\rVert_{2}^{2}\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px1.p1.10" class="ltx_p">with <math id="S3.SS1.SSS0.Px1.p1.6.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.6.m1.1a"><mi id="S3.SS1.SSS0.Px1.p1.6.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m1.1b"><ci id="S3.SS1.SSS0.Px1.p1.6.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m1.1c">t</annotation></semantics></math> uniformly sampled from <math id="S3.SS1.SSS0.Px1.p1.7.m2.3" class="ltx_Math" alttext="\{1,...,T\}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.7.m2.3a"><mrow id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2.1" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml">{</mo><mn id="S3.SS1.SSS0.Px1.p1.7.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.7.m2.1.1.cmml">1</mn><mo id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS0.Px1.p1.7.m2.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m2.2.2.cmml">…</mi><mo id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2.3" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.7.m2.3.3" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.3.cmml">T</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2.4" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.7.m2.3b"><set id="S3.SS1.SSS0.Px1.p1.7.m2.3.4.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.4.2"><cn type="integer" id="S3.SS1.SSS0.Px1.p1.7.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m2.1.1">1</cn><ci id="S3.SS1.SSS0.Px1.p1.7.m2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m2.2.2">…</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m2.3.3">𝑇</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.7.m2.3c">\{1,...,T\}</annotation></semantics></math>. Here, <math id="S3.SS1.SSS0.Px1.p1.8.m3.1" class="ltx_Math" alttext="\epsilon_{\theta}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.8.m3.1a"><msub id="S3.SS1.SSS0.Px1.p1.8.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1.2.cmml">ϵ</mi><mi id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.8.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1.2">italic-ϵ</ci><ci id="S3.SS1.SSS0.Px1.p1.8.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.8.m3.1c">\epsilon_{\theta}</annotation></semantics></math> denotes the trainable components of the model, which include a denoising U-Net equipped with Residual Blocks and layers that facilitate both self-attention and cross-attention.
These components are tasked with processing the noisy latent variables <math id="S3.SS1.SSS0.Px1.p1.9.m4.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.9.m4.1a"><msub id="S3.SS1.SSS0.Px1.p1.9.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.9.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px1.p1.9.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.9.m4.1c">z_{t}</annotation></semantics></math> and the conditional embeddings <math id="S3.SS1.SSS0.Px1.p1.10.m5.1" class="ltx_Math" alttext="c_{\text{embed}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.10.m5.1a"><msub id="S3.SS1.SSS0.Px1.p1.10.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.2.cmml">c</mi><mtext id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.10.m5.1b"><apply id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.2">𝑐</ci><ci id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m5.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.10.m5.1c">c_{\text{embed}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.3" class="ltx_p">Upon the completion of training, the original latent <math id="S3.SS1.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.1.m1.1c">z_{t}</annotation></semantics></math> is reconstructed using deterministic sampling methods such as denoising diffusion implicit models (DDIM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
The latent <math id="S3.SS1.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.2.m2.1c">z_{t}</annotation></semantics></math> is then decoded by the decoder <math id="S3.SS1.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.3.m3.1c">\mathcal{D}</annotation></semantics></math> to generate the final image output.
This methodology not only preserves the fidelity of the generated images but also ensures that they are contextually aligned with the input auxiliary conditions, demonstrating the efficacy of integrating conditioned processes in generative models.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Cross Attention as Motion Guidance.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.2" class="ltx_p">Cross attention is a critical component of the latent diffusion model framework, significantly enhancing the generative process by directing the flow and focus of information.
This mechanism allows the model to concentrate on specific motion guidance and latent image representations, thus improving the semantic coherence between the generated videos and the motion condition.
In general video generation tasks, motion conditions may include textual prompts or dense motion flows. In more specialized domains such as human image animation, motion conditions may involve skeletons, semantic maps, and 3D parametric flows.
For portrait image animation tasks, audio is typically used as the motion condition, with cross attention employed to integrate the relevant conditions effectively.
Mathematically, cross attention is implemented using attention layers that process both the embedding of the motion condition, denoted as <math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="c_{\text{embed}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">c</mi><mtext id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml">embed</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3">embed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">c_{\text{embed}}</annotation></semantics></math>, and the noisy latent variables <math id="S3.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">z_{t}</annotation></semantics></math>.
These layers calculate attention scores that determine the model’s focus level on various input aspects.
The scores are then utilized to weight the contributions of each component, creating a weighted sum that serves as the input for subsequent layers.
The cross attention mechanism can be expressed as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\text{CrossAttn}(z_{t},c_{\text{embed}})=\text{Attention}(Q,K,V)." display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml"><mtext id="S3.E3.m1.4.4.1.1.2.4" xref="S3.E3.m1.4.4.1.1.2.4a.cmml">CrossAttn</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.2.3" xref="S3.E3.m1.4.4.1.1.2.3.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.2.2.2.3" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">(</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E3.m1.4.4.1.1.2.2.2.4" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.2.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.2.2.2.2.2" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2.cmml">c</mi><mtext id="S3.E3.m1.4.4.1.1.2.2.2.2.3" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3a.cmml">embed</mtext></msub><mo stretchy="false" id="S3.E3.m1.4.4.1.1.2.2.2.5" xref="S3.E3.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.4" xref="S3.E3.m1.4.4.1.1.4.cmml"><mtext id="S3.E3.m1.4.4.1.1.4.2" xref="S3.E3.m1.4.4.1.1.4.2a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.4.1" xref="S3.E3.m1.4.4.1.1.4.1.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.4.3.2" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.4.3.2.1" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Q</mi><mo id="S3.E3.m1.4.4.1.1.4.3.2.2" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">K</mi><mo id="S3.E3.m1.4.4.1.1.4.3.2.3" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">,</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E3.m1.4.4.1.1.4.3.2.4" xref="S3.E3.m1.4.4.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"></eq><apply id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"><times id="S3.E3.m1.4.4.1.1.2.3.cmml" xref="S3.E3.m1.4.4.1.1.2.3"></times><ci id="S3.E3.m1.4.4.1.1.2.4a.cmml" xref="S3.E3.m1.4.4.1.1.2.4"><mtext id="S3.E3.m1.4.4.1.1.2.4.cmml" xref="S3.E3.m1.4.4.1.1.2.4">CrossAttn</mtext></ci><interval closure="open" id="S3.E3.m1.4.4.1.1.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2"><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E3.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.2">𝑐</ci><ci id="S3.E3.m1.4.4.1.1.2.2.2.2.3a.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S3.E3.m1.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.2.2.2.2.3">embed</mtext></ci></apply></interval></apply><apply id="S3.E3.m1.4.4.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.4"><times id="S3.E3.m1.4.4.1.1.4.1.cmml" xref="S3.E3.m1.4.4.1.1.4.1"></times><ci id="S3.E3.m1.4.4.1.1.4.2a.cmml" xref="S3.E3.m1.4.4.1.1.4.2"><mtext id="S3.E3.m1.4.4.1.1.4.2.cmml" xref="S3.E3.m1.4.4.1.1.4.2">Attention</mtext></ci><vector id="S3.E3.m1.4.4.1.1.4.3.1.cmml" xref="S3.E3.m1.4.4.1.1.4.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑄</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝐾</ci><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">𝑉</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\text{CrossAttn}(z_{t},c_{\text{embed}})=\text{Attention}(Q,K,V).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px2.p1.3" class="ltx_p">Here, <math id="S3.SS1.SSS0.Px2.p1.3.m1.2" class="ltx_Math" alttext="\text{CrossAttn}(z_{t},c_{\text{embed}})" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.3.m1.2a"><mrow id="S3.SS1.SSS0.Px2.p1.3.m1.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.cmml"><mtext id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4a.cmml">CrossAttn</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.3.cmml">​</mo><mrow id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.3.cmml">(</mo><msub id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.4" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.2.cmml">c</mi><mtext id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3a.cmml">embed</mtext></msub><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.5" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m1.2b"><apply id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2"><times id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.3"></times><ci id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4a.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4"><mtext id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.4">CrossAttn</mtext></ci><interval closure="open" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2"><apply id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.2">𝑐</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3a.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m1.2.2.2.2.2.3">embed</mtext></ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m1.2c">\text{CrossAttn}(z_{t},c_{\text{embed}})</annotation></semantics></math> computes the attention scores using:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle Q" display="inline"><semantics id="S3.E4.m1.1a"><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle Q</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.2" class="ltx_Math" alttext="\displaystyle=W_{Q}\cdot z_{t},\;\;W_{Q}\in\mathbb{R}^{D_{e}\times D_{z}}" display="inline"><semantics id="S3.E4.m2.2a"><mrow id="S3.E4.m2.2.2.2" xref="S3.E4.m2.2.2.3.cmml"><mrow id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.2.cmml"></mi><mo id="S3.E4.m2.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.3.cmml"><msub id="S3.E4.m2.1.1.1.1.3.2" xref="S3.E4.m2.1.1.1.1.3.2.cmml"><mi id="S3.E4.m2.1.1.1.1.3.2.2" xref="S3.E4.m2.1.1.1.1.3.2.2.cmml">W</mi><mi id="S3.E4.m2.1.1.1.1.3.2.3" xref="S3.E4.m2.1.1.1.1.3.2.3.cmml">Q</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m2.1.1.1.1.3.1" xref="S3.E4.m2.1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E4.m2.1.1.1.1.3.3" xref="S3.E4.m2.1.1.1.1.3.3.cmml"><mi id="S3.E4.m2.1.1.1.1.3.3.2" xref="S3.E4.m2.1.1.1.1.3.3.2.cmml">z</mi><mi id="S3.E4.m2.1.1.1.1.3.3.3" xref="S3.E4.m2.1.1.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><mo rspace="0.727em" id="S3.E4.m2.2.2.2.3" xref="S3.E4.m2.2.2.3a.cmml">,</mo><mrow id="S3.E4.m2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.cmml"><msub id="S3.E4.m2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.2.cmml">W</mi><mi id="S3.E4.m2.2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.2.3.cmml">Q</mi></msub><mo id="S3.E4.m2.2.2.2.2.1" xref="S3.E4.m2.2.2.2.2.1.cmml">∈</mo><msup id="S3.E4.m2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.3.cmml"><mi id="S3.E4.m2.2.2.2.2.3.2" xref="S3.E4.m2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.E4.m2.2.2.2.2.3.3" xref="S3.E4.m2.2.2.2.2.3.3.cmml"><msub id="S3.E4.m2.2.2.2.2.3.3.2" xref="S3.E4.m2.2.2.2.2.3.3.2.cmml"><mi id="S3.E4.m2.2.2.2.2.3.3.2.2" xref="S3.E4.m2.2.2.2.2.3.3.2.2.cmml">D</mi><mi id="S3.E4.m2.2.2.2.2.3.3.2.3" xref="S3.E4.m2.2.2.2.2.3.3.2.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m2.2.2.2.2.3.3.1" xref="S3.E4.m2.2.2.2.2.3.3.1.cmml">×</mo><msub id="S3.E4.m2.2.2.2.2.3.3.3" xref="S3.E4.m2.2.2.2.2.3.3.3.cmml"><mi id="S3.E4.m2.2.2.2.2.3.3.3.2" xref="S3.E4.m2.2.2.2.2.3.3.3.2.cmml">D</mi><mi id="S3.E4.m2.2.2.2.2.3.3.3.3" xref="S3.E4.m2.2.2.2.2.3.3.3.3.cmml">z</mi></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.2b"><apply id="S3.E4.m2.2.2.3.cmml" xref="S3.E4.m2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.3a.cmml" xref="S3.E4.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m2.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1"><eq id="S3.E4.m2.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.E4.m2.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.2">absent</csymbol><apply id="S3.E4.m2.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.3"><ci id="S3.E4.m2.1.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.1.3.1">⋅</ci><apply id="S3.E4.m2.1.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.3.2.1.cmml" xref="S3.E4.m2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.3.2.2.cmml" xref="S3.E4.m2.1.1.1.1.3.2.2">𝑊</ci><ci id="S3.E4.m2.1.1.1.1.3.2.3.cmml" xref="S3.E4.m2.1.1.1.1.3.2.3">𝑄</ci></apply><apply id="S3.E4.m2.1.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.3.3.1.cmml" xref="S3.E4.m2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.3.3.2.cmml" xref="S3.E4.m2.1.1.1.1.3.3.2">𝑧</ci><ci id="S3.E4.m2.1.1.1.1.3.3.3.cmml" xref="S3.E4.m2.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S3.E4.m2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2"><in id="S3.E4.m2.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.2.2.1"></in><apply id="S3.E4.m2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2.2">𝑊</ci><ci id="S3.E4.m2.2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.2.3">𝑄</ci></apply><apply id="S3.E4.m2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.3.1.cmml" xref="S3.E4.m2.2.2.2.2.3">superscript</csymbol><ci id="S3.E4.m2.2.2.2.2.3.2.cmml" xref="S3.E4.m2.2.2.2.2.3.2">ℝ</ci><apply id="S3.E4.m2.2.2.2.2.3.3.cmml" xref="S3.E4.m2.2.2.2.2.3.3"><times id="S3.E4.m2.2.2.2.2.3.3.1.cmml" xref="S3.E4.m2.2.2.2.2.3.3.1"></times><apply id="S3.E4.m2.2.2.2.2.3.3.2.cmml" xref="S3.E4.m2.2.2.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.3.3.2.1.cmml" xref="S3.E4.m2.2.2.2.2.3.3.2">subscript</csymbol><ci id="S3.E4.m2.2.2.2.2.3.3.2.2.cmml" xref="S3.E4.m2.2.2.2.2.3.3.2.2">𝐷</ci><ci id="S3.E4.m2.2.2.2.2.3.3.2.3.cmml" xref="S3.E4.m2.2.2.2.2.3.3.2.3">𝑒</ci></apply><apply id="S3.E4.m2.2.2.2.2.3.3.3.cmml" xref="S3.E4.m2.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.3.3.3.1.cmml" xref="S3.E4.m2.2.2.2.2.3.3.3">subscript</csymbol><ci id="S3.E4.m2.2.2.2.2.3.3.3.2.cmml" xref="S3.E4.m2.2.2.2.2.3.3.3.2">𝐷</ci><ci id="S3.E4.m2.2.2.2.2.3.3.3.3.cmml" xref="S3.E4.m2.2.2.2.2.3.3.3.3">𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.2c">\displaystyle=W_{Q}\cdot z_{t},\;\;W_{Q}\in\mathbb{R}^{D_{e}\times D_{z}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle K" display="inline"><semantics id="S3.E5.m1.1a"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle K</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.2" class="ltx_Math" alttext="\displaystyle=W_{K}\cdot c_{\text{embed}},\;\;W_{K}\in\mathbb{R}^{D_{e}\times D_{c}}" display="inline"><semantics id="S3.E5.m2.2a"><mrow id="S3.E5.m2.2.2.2" xref="S3.E5.m2.2.2.3.cmml"><mrow id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.2.cmml"></mi><mo id="S3.E5.m2.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.E5.m2.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.3.cmml"><msub id="S3.E5.m2.1.1.1.1.3.2" xref="S3.E5.m2.1.1.1.1.3.2.cmml"><mi id="S3.E5.m2.1.1.1.1.3.2.2" xref="S3.E5.m2.1.1.1.1.3.2.2.cmml">W</mi><mi id="S3.E5.m2.1.1.1.1.3.2.3" xref="S3.E5.m2.1.1.1.1.3.2.3.cmml">K</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m2.1.1.1.1.3.1" xref="S3.E5.m2.1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E5.m2.1.1.1.1.3.3" xref="S3.E5.m2.1.1.1.1.3.3.cmml"><mi id="S3.E5.m2.1.1.1.1.3.3.2" xref="S3.E5.m2.1.1.1.1.3.3.2.cmml">c</mi><mtext id="S3.E5.m2.1.1.1.1.3.3.3" xref="S3.E5.m2.1.1.1.1.3.3.3a.cmml">embed</mtext></msub></mrow></mrow><mo rspace="0.727em" id="S3.E5.m2.2.2.2.3" xref="S3.E5.m2.2.2.3a.cmml">,</mo><mrow id="S3.E5.m2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.cmml"><msub id="S3.E5.m2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.cmml"><mi id="S3.E5.m2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.2.cmml">W</mi><mi id="S3.E5.m2.2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.2.3.cmml">K</mi></msub><mo id="S3.E5.m2.2.2.2.2.1" xref="S3.E5.m2.2.2.2.2.1.cmml">∈</mo><msup id="S3.E5.m2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.3.cmml"><mi id="S3.E5.m2.2.2.2.2.3.2" xref="S3.E5.m2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.E5.m2.2.2.2.2.3.3" xref="S3.E5.m2.2.2.2.2.3.3.cmml"><msub id="S3.E5.m2.2.2.2.2.3.3.2" xref="S3.E5.m2.2.2.2.2.3.3.2.cmml"><mi id="S3.E5.m2.2.2.2.2.3.3.2.2" xref="S3.E5.m2.2.2.2.2.3.3.2.2.cmml">D</mi><mi id="S3.E5.m2.2.2.2.2.3.3.2.3" xref="S3.E5.m2.2.2.2.2.3.3.2.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m2.2.2.2.2.3.3.1" xref="S3.E5.m2.2.2.2.2.3.3.1.cmml">×</mo><msub id="S3.E5.m2.2.2.2.2.3.3.3" xref="S3.E5.m2.2.2.2.2.3.3.3.cmml"><mi id="S3.E5.m2.2.2.2.2.3.3.3.2" xref="S3.E5.m2.2.2.2.2.3.3.3.2.cmml">D</mi><mi id="S3.E5.m2.2.2.2.2.3.3.3.3" xref="S3.E5.m2.2.2.2.2.3.3.3.3.cmml">c</mi></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.2b"><apply id="S3.E5.m2.2.2.3.cmml" xref="S3.E5.m2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3a.cmml" xref="S3.E5.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1"><eq id="S3.E5.m2.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.2">absent</csymbol><apply id="S3.E5.m2.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.3"><ci id="S3.E5.m2.1.1.1.1.3.1.cmml" xref="S3.E5.m2.1.1.1.1.3.1">⋅</ci><apply id="S3.E5.m2.1.1.1.1.3.2.cmml" xref="S3.E5.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.3.2.1.cmml" xref="S3.E5.m2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.3.2.2.cmml" xref="S3.E5.m2.1.1.1.1.3.2.2">𝑊</ci><ci id="S3.E5.m2.1.1.1.1.3.2.3.cmml" xref="S3.E5.m2.1.1.1.1.3.2.3">𝐾</ci></apply><apply id="S3.E5.m2.1.1.1.1.3.3.cmml" xref="S3.E5.m2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.3.3.1.cmml" xref="S3.E5.m2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.3.3.2.cmml" xref="S3.E5.m2.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E5.m2.1.1.1.1.3.3.3a.cmml" xref="S3.E5.m2.1.1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E5.m2.1.1.1.1.3.3.3.cmml" xref="S3.E5.m2.1.1.1.1.3.3.3">embed</mtext></ci></apply></apply></apply><apply id="S3.E5.m2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2"><in id="S3.E5.m2.2.2.2.2.1.cmml" xref="S3.E5.m2.2.2.2.2.1"></in><apply id="S3.E5.m2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.2.1.cmml" xref="S3.E5.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.E5.m2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2">𝑊</ci><ci id="S3.E5.m2.2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.2.3">𝐾</ci></apply><apply id="S3.E5.m2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.3.1.cmml" xref="S3.E5.m2.2.2.2.2.3">superscript</csymbol><ci id="S3.E5.m2.2.2.2.2.3.2.cmml" xref="S3.E5.m2.2.2.2.2.3.2">ℝ</ci><apply id="S3.E5.m2.2.2.2.2.3.3.cmml" xref="S3.E5.m2.2.2.2.2.3.3"><times id="S3.E5.m2.2.2.2.2.3.3.1.cmml" xref="S3.E5.m2.2.2.2.2.3.3.1"></times><apply id="S3.E5.m2.2.2.2.2.3.3.2.cmml" xref="S3.E5.m2.2.2.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.3.3.2.1.cmml" xref="S3.E5.m2.2.2.2.2.3.3.2">subscript</csymbol><ci id="S3.E5.m2.2.2.2.2.3.3.2.2.cmml" xref="S3.E5.m2.2.2.2.2.3.3.2.2">𝐷</ci><ci id="S3.E5.m2.2.2.2.2.3.3.2.3.cmml" xref="S3.E5.m2.2.2.2.2.3.3.2.3">𝑒</ci></apply><apply id="S3.E5.m2.2.2.2.2.3.3.3.cmml" xref="S3.E5.m2.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.3.3.3.1.cmml" xref="S3.E5.m2.2.2.2.2.3.3.3">subscript</csymbol><ci id="S3.E5.m2.2.2.2.2.3.3.3.2.cmml" xref="S3.E5.m2.2.2.2.2.3.3.3.2">𝐷</ci><ci id="S3.E5.m2.2.2.2.2.3.3.3.3.cmml" xref="S3.E5.m2.2.2.2.2.3.3.3.3">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.2c">\displaystyle=W_{K}\cdot c_{\text{embed}},\;\;W_{K}\in\mathbb{R}^{D_{e}\times D_{c}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle V" display="inline"><semantics id="S3.E6.m1.1a"><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle V</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6.m2.2" class="ltx_Math" alttext="\displaystyle=W_{V}\cdot c_{\text{embed}},\;\;W_{V}\in\mathbb{R}^{D_{e}\times D_{c}}" display="inline"><semantics id="S3.E6.m2.2a"><mrow id="S3.E6.m2.2.2.2" xref="S3.E6.m2.2.2.3.cmml"><mrow id="S3.E6.m2.1.1.1.1" xref="S3.E6.m2.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.2.cmml"></mi><mo id="S3.E6.m2.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m2.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.3.cmml"><msub id="S3.E6.m2.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.3.2.cmml"><mi id="S3.E6.m2.1.1.1.1.3.2.2" xref="S3.E6.m2.1.1.1.1.3.2.2.cmml">W</mi><mi id="S3.E6.m2.1.1.1.1.3.2.3" xref="S3.E6.m2.1.1.1.1.3.2.3.cmml">V</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m2.1.1.1.1.3.1" xref="S3.E6.m2.1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E6.m2.1.1.1.1.3.3" xref="S3.E6.m2.1.1.1.1.3.3.cmml"><mi id="S3.E6.m2.1.1.1.1.3.3.2" xref="S3.E6.m2.1.1.1.1.3.3.2.cmml">c</mi><mtext id="S3.E6.m2.1.1.1.1.3.3.3" xref="S3.E6.m2.1.1.1.1.3.3.3a.cmml">embed</mtext></msub></mrow></mrow><mo rspace="0.727em" id="S3.E6.m2.2.2.2.3" xref="S3.E6.m2.2.2.3a.cmml">,</mo><mrow id="S3.E6.m2.2.2.2.2" xref="S3.E6.m2.2.2.2.2.cmml"><msub id="S3.E6.m2.2.2.2.2.2" xref="S3.E6.m2.2.2.2.2.2.cmml"><mi id="S3.E6.m2.2.2.2.2.2.2" xref="S3.E6.m2.2.2.2.2.2.2.cmml">W</mi><mi id="S3.E6.m2.2.2.2.2.2.3" xref="S3.E6.m2.2.2.2.2.2.3.cmml">V</mi></msub><mo id="S3.E6.m2.2.2.2.2.1" xref="S3.E6.m2.2.2.2.2.1.cmml">∈</mo><msup id="S3.E6.m2.2.2.2.2.3" xref="S3.E6.m2.2.2.2.2.3.cmml"><mi id="S3.E6.m2.2.2.2.2.3.2" xref="S3.E6.m2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.E6.m2.2.2.2.2.3.3" xref="S3.E6.m2.2.2.2.2.3.3.cmml"><msub id="S3.E6.m2.2.2.2.2.3.3.2" xref="S3.E6.m2.2.2.2.2.3.3.2.cmml"><mi id="S3.E6.m2.2.2.2.2.3.3.2.2" xref="S3.E6.m2.2.2.2.2.3.3.2.2.cmml">D</mi><mi id="S3.E6.m2.2.2.2.2.3.3.2.3" xref="S3.E6.m2.2.2.2.2.3.3.2.3.cmml">e</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m2.2.2.2.2.3.3.1" xref="S3.E6.m2.2.2.2.2.3.3.1.cmml">×</mo><msub id="S3.E6.m2.2.2.2.2.3.3.3" xref="S3.E6.m2.2.2.2.2.3.3.3.cmml"><mi id="S3.E6.m2.2.2.2.2.3.3.3.2" xref="S3.E6.m2.2.2.2.2.3.3.3.2.cmml">D</mi><mi id="S3.E6.m2.2.2.2.2.3.3.3.3" xref="S3.E6.m2.2.2.2.2.3.3.3.3.cmml">c</mi></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.2b"><apply id="S3.E6.m2.2.2.3.cmml" xref="S3.E6.m2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.2.2.3a.cmml" xref="S3.E6.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E6.m2.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1"><eq id="S3.E6.m2.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.E6.m2.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.2">absent</csymbol><apply id="S3.E6.m2.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.3"><ci id="S3.E6.m2.1.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.1.3.1">⋅</ci><apply id="S3.E6.m2.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.3.2.1.cmml" xref="S3.E6.m2.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.3.2.2.cmml" xref="S3.E6.m2.1.1.1.1.3.2.2">𝑊</ci><ci id="S3.E6.m2.1.1.1.1.3.2.3.cmml" xref="S3.E6.m2.1.1.1.1.3.2.3">𝑉</ci></apply><apply id="S3.E6.m2.1.1.1.1.3.3.cmml" xref="S3.E6.m2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.3.3.1.cmml" xref="S3.E6.m2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.3.3.2.cmml" xref="S3.E6.m2.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E6.m2.1.1.1.1.3.3.3a.cmml" xref="S3.E6.m2.1.1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E6.m2.1.1.1.1.3.3.3.cmml" xref="S3.E6.m2.1.1.1.1.3.3.3">embed</mtext></ci></apply></apply></apply><apply id="S3.E6.m2.2.2.2.2.cmml" xref="S3.E6.m2.2.2.2.2"><in id="S3.E6.m2.2.2.2.2.1.cmml" xref="S3.E6.m2.2.2.2.2.1"></in><apply id="S3.E6.m2.2.2.2.2.2.cmml" xref="S3.E6.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.2.2.2.2.2.1.cmml" xref="S3.E6.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.E6.m2.2.2.2.2.2.2.cmml" xref="S3.E6.m2.2.2.2.2.2.2">𝑊</ci><ci id="S3.E6.m2.2.2.2.2.2.3.cmml" xref="S3.E6.m2.2.2.2.2.2.3">𝑉</ci></apply><apply id="S3.E6.m2.2.2.2.2.3.cmml" xref="S3.E6.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m2.2.2.2.2.3.1.cmml" xref="S3.E6.m2.2.2.2.2.3">superscript</csymbol><ci id="S3.E6.m2.2.2.2.2.3.2.cmml" xref="S3.E6.m2.2.2.2.2.3.2">ℝ</ci><apply id="S3.E6.m2.2.2.2.2.3.3.cmml" xref="S3.E6.m2.2.2.2.2.3.3"><times id="S3.E6.m2.2.2.2.2.3.3.1.cmml" xref="S3.E6.m2.2.2.2.2.3.3.1"></times><apply id="S3.E6.m2.2.2.2.2.3.3.2.cmml" xref="S3.E6.m2.2.2.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E6.m2.2.2.2.2.3.3.2.1.cmml" xref="S3.E6.m2.2.2.2.2.3.3.2">subscript</csymbol><ci id="S3.E6.m2.2.2.2.2.3.3.2.2.cmml" xref="S3.E6.m2.2.2.2.2.3.3.2.2">𝐷</ci><ci id="S3.E6.m2.2.2.2.2.3.3.2.3.cmml" xref="S3.E6.m2.2.2.2.2.3.3.2.3">𝑒</ci></apply><apply id="S3.E6.m2.2.2.2.2.3.3.3.cmml" xref="S3.E6.m2.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m2.2.2.2.2.3.3.3.1.cmml" xref="S3.E6.m2.2.2.2.2.3.3.3">subscript</csymbol><ci id="S3.E6.m2.2.2.2.2.3.3.3.2.cmml" xref="S3.E6.m2.2.2.2.2.3.3.3.2">𝐷</ci><ci id="S3.E6.m2.2.2.2.2.3.3.3.3.cmml" xref="S3.E6.m2.2.2.2.2.3.3.3.3">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.2c">\displaystyle=W_{V}\cdot c_{\text{embed}},\;\;W_{V}\in\mathbb{R}^{D_{e}\times D_{c}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px2.p1.6" class="ltx_p">where <math id="S3.SS1.SSS0.Px2.p1.4.m1.1" class="ltx_Math" alttext="W_{Q}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.4.m1.1a"><msub id="S3.SS1.SSS0.Px2.p1.4.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1.2">𝑊</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m1.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m1.1c">W_{Q}</annotation></semantics></math>, <math id="S3.SS1.SSS0.Px2.p1.5.m2.1" class="ltx_Math" alttext="W_{K}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.5.m2.1a"><msub id="S3.SS1.SSS0.Px2.p1.5.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1.2">𝑊</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m2.1c">W_{K}</annotation></semantics></math>, and <math id="S3.SS1.SSS0.Px2.p1.6.m3.1" class="ltx_Math" alttext="W_{V}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.6.m3.1a"><msub id="S3.SS1.SSS0.Px2.p1.6.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1.2.cmml">W</mi><mi id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.6.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1.2">𝑊</ci><ci id="S3.SS1.SSS0.Px2.p1.6.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.6.m3.1c">W_{V}</annotation></semantics></math> are learnable projection matrices.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">The output is a weighted representation that captures the most pertinent aspects of the input condition and latent image at the current stage of the diffusion process.
By incorporating cross attention, the latent diffusion model dynamically adjusts its focus based on the evolving state of the latent variables and the specified motion condition.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/hierarchical.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Visualization of the hierarchical audio-driven visual synthesis and a comparative analysis of the audio-visual cross attention between the original full method and our proposed hierarchical audio-visual cross attention.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hierarchical Audio-Driven Visual Synthesis.</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">As demonstrated in Figure <a href="#S2.F2" title="Figure 2 ‣ Portrait Image Animation. ‣ 2 Related Work ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our task is to generate a video with <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">S</annotation></semantics></math> frames given a face image and a clip of audio. For each frame <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">s</annotation></semantics></math>, we denote the corresponding latent representation at the diffusion time step <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">t</annotation></semantics></math> as <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="z^{(s)}_{t}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msubsup id="S3.SS2.p1.4.m4.1.2" xref="S3.SS2.p1.4.m4.1.2.cmml"><mi id="S3.SS2.p1.4.m4.1.2.2.2" xref="S3.SS2.p1.4.m4.1.2.2.2.cmml">z</mi><mi id="S3.SS2.p1.4.m4.1.2.3" xref="S3.SS2.p1.4.m4.1.2.3.cmml">t</mi><mrow id="S3.SS2.p1.4.m4.1.1.1.3" xref="S3.SS2.p1.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.4.m4.1.1.1.3.1" xref="S3.SS2.p1.4.m4.1.2.cmml">(</mo><mi id="S3.SS2.p1.4.m4.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS2.p1.4.m4.1.1.1.3.2" xref="S3.SS2.p1.4.m4.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.2.cmml" xref="S3.SS2.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2">subscript</csymbol><apply id="S3.SS2.p1.4.m4.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.2.2.1.cmml" xref="S3.SS2.p1.4.m4.1.2">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.2.2.2.cmml" xref="S3.SS2.p1.4.m4.1.2.2.2">𝑧</ci><ci id="S3.SS2.p1.4.m4.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1">𝑠</ci></apply><ci id="S3.SS2.p1.4.m4.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.2.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">z^{(s)}_{t}</annotation></semantics></math>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Face Embedding.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.3" class="ltx_p">The objective of face embedding is to produce high-fidelity portrait identity features.
In contrast to previous approaches utilizing CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> for generic visual feature encoding by jointly training on a large dataset of images and textual descriptions, our method employs a pre-trained face encoder to extract identity features <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="c_{\text{exp}}\in\mathbb{R}^{D_{f}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">c</mi><mtext id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3a.cmml">exp</mtext></msub><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml">f</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><in id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></in><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2">𝑐</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3">exp</mtext></ci></apply><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">c_{\text{exp}}\in\mathbb{R}^{D_{f}}</annotation></semantics></math>.
These features are fed into a diffusion network’s cross-attention module, i.e., CrossAttn(<math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="z_{t}^{(s)}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><msubsup id="S3.SS2.SSS0.Px1.p1.2.m2.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.2.cmml">z</mi><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.3.cmml">t</mi><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.2">𝑧</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.2.2.3">𝑡</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">z_{t}^{(s)}</annotation></semantics></math>, <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="c_{\text{exp}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">c</mi><mtext id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝑐</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">c_{\text{exp}}</annotation></semantics></math>), to interact with latents, generating portrait animations faithful to the input character features.
This approach not only ensures generalization in facial feature extraction but also accurately preserves and reproduces individual identity traits such as facial expressions, age, and gender.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Audio Embedding.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.4" class="ltx_p">To enhance the encoding of audio for motion-driven information in animation, we employed wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as the audio feature encoder.
Specifically, we concatenated the audio embeddings from the final 12 layers of the wav2vec network to capture richer semantic information across different audio layers.
Given the contextual influence on sequential audio data, we extracted the corresponding 5-second audio segment for the <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">S</annotation></semantics></math> frames.
Utilizing three simple linear layers, we transform the pre-trained model’s audio embeddings to <math id="S3.SS2.SSS0.Px2.p1.2.m2.2" class="ltx_Math" alttext="\{c_{\text{audio}}^{(s)}\}_{s=1}^{S}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.2a"><msubsup id="S3.SS2.SSS0.Px2.p1.2.m2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.2.cmml">c</mi><mtext id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3a.cmml">audio</mtext><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.2.cmml">s</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml">S</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.2b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2">superscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2">subscript</csymbol><set id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.2">𝑐</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3a.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.1.2.3">audio</mtext></ci></apply><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1">𝑠</ci></apply></set><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3"><eq id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.1"></eq><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.2">𝑠</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.2c">\{c_{\text{audio}}^{(s)}\}_{s=1}^{S}</annotation></semantics></math>, where <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="c_{\text{audio}}^{(s)}\in\mathbb{R}^{D_{a}}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.cmml"><msubsup id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.2.cmml">c</mi><mtext id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3a.cmml">audio</mtext><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.2.cmml">ℝ</mi><msub id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.3.cmml">a</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2"><in id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.1"></in><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2">superscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.2">𝑐</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3a.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.2.2.3">audio</mtext></ci></apply><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.1">𝑠</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.2">ℝ</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.2">𝐷</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.2.3.3.3">𝑎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">c_{\text{audio}}^{(s)}\in\mathbb{R}^{D_{a}}</annotation></semantics></math> is the audio embedding for the <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">s</annotation></semantics></math> frame.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Hierarchical Audio-Visual Cross Attention.</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Cross Attention as Motion Guidance. ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we introduce a hierarchical audio-visual cross attention mechanism to facilitate the model’s learning of relationships between audio and visual components such as lips, expression and pose.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p2.4" class="ltx_p">First, we pre-process the face image to obtain masks <math id="S3.SS2.SSS0.Px3.p2.1.m1.5" class="ltx_Math" alttext="M_{\text{lip}},M_{\text{exp}},M_{\text{pose}}\in\{0,1\}^{H_{z}\times W_{z}}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.1.m1.5a"><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.5.5" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.4.cmml"><msub id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.2.cmml">M</mi><mtext id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3a.cmml">lip</mtext></msub><mo id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.4" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.2.cmml">M</mi><mtext id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3a.cmml">exp</mtext></msub><mo id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.5" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.2.cmml">M</mi><mtext id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3a.cmml">pose</mtext></msub></mrow><mo id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.4" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.4.cmml">∈</mo><msup id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.2.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.1.cmml">{</mo><mn id="S3.SS2.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.1.cmml">,</mo><mn id="S3.SS2.SSS0.Px3.p2.1.m1.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.2.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.1.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.cmml"><msub id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.2.cmml">H</mi><mi id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.3.cmml">z</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.1.cmml">×</mo><msub id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.3.cmml">z</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.1.m1.5b"><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5"><in id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.4.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.4"></in><list id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.4.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3"><apply id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.2">𝑀</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3a.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.3.3.1.1.1.3">lip</mtext></ci></apply><apply id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.2">𝑀</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3a.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.4.4.2.2.2.3">exp</mtext></ci></apply><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.2">𝑀</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3a.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.3.3.3.3">pose</mtext></ci></apply></list><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5">superscript</csymbol><set id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.2.2"><cn type="integer" id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.SSS0.Px3.p2.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.2.2">1</cn></set><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3"><times id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.1"></times><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.2">𝐻</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.2.3">𝑧</ci></apply><apply id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.2">𝑊</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.5.5.5.3.3.3">𝑧</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.1.m1.5c">M_{\text{lip}},M_{\text{exp}},M_{\text{pose}}\in\{0,1\}^{H_{z}\times W_{z}}</annotation></semantics></math> representing the lip, expression and pose areas respectively. Specifically, we use the off-the-shelf toolbox MediaPipe to predict landmarks from the face image <math id="S3.SS2.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.2.m2.1a"><mi id="S3.SS2.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.2.m2.1c">I</annotation></semantics></math>. The MediaPipe predicts the coordinates of multiple landmarks for lip and expression. The bounding box masks <math id="S3.SS2.SSS0.Px3.p2.3.m3.1" class="ltx_Math" alttext="Y_{\text{lip}}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.3.m3.1a"><msub id="S3.SS2.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.2" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.2.cmml">Y</mi><mtext id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3a.cmml">lip</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.3.m3.1b"><apply id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.2">𝑌</ci><ci id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.3">lip</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.3.m3.1c">Y_{\text{lip}}</annotation></semantics></math>, <math id="S3.SS2.SSS0.Px3.p2.4.m4.2" class="ltx_Math" alttext="Y_{\text{exp}}\in\{0,1\}^{H_{z}\times W_{z}}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.4.m4.2a"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.2.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.cmml"><msub id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.2.cmml">Y</mi><mtext id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3a.cmml">exp</mtext></msub><mo id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.2.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.1.cmml">{</mo><mn id="S3.SS2.SSS0.Px3.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.cmml">0</mn><mo id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.2.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.SSS0.Px3.p2.4.m4.2.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.2.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.1.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.cmml"><msub id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.cmml"><mi id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.2.cmml">H</mi><mi id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.3.cmml">z</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.1.cmml">×</mo><msub id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.2.cmml">W</mi><mi id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.3.cmml">z</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.4.m4.2b"><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3"><in id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.1"></in><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.2">𝑌</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3a.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.2.3">exp</mtext></ci></apply><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3">superscript</csymbol><set id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.2.2"><cn type="integer" id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1">0</cn><cn type="integer" id="S3.SS2.SSS0.Px3.p2.4.m4.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.2">1</cn></set><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3"><times id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.1"></times><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.2">𝐻</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.2.3">𝑧</ci></apply><apply id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.2">𝑊</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.2.3.3.3.3.3">𝑧</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.4.m4.2c">Y_{\text{exp}}\in\{0,1\}^{H_{z}\times W_{z}}</annotation></semantics></math> are obtained by</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.8" class="ltx_Math" alttext="Y_{\text{r}}(i,j)=\begin{cases}1&amp;\text{if $(i,j)$ is inside $\text{BoundingBox(r)}$}\\
0&amp;\text{otherwise}\end{cases}," display="block"><semantics id="S3.E7.m1.8a"><mrow id="S3.E7.m1.8.8.1" xref="S3.E7.m1.8.8.1.1.cmml"><mrow id="S3.E7.m1.8.8.1.1" xref="S3.E7.m1.8.8.1.1.cmml"><mrow id="S3.E7.m1.8.8.1.1.2" xref="S3.E7.m1.8.8.1.1.2.cmml"><msub id="S3.E7.m1.8.8.1.1.2.2" xref="S3.E7.m1.8.8.1.1.2.2.cmml"><mi id="S3.E7.m1.8.8.1.1.2.2.2" xref="S3.E7.m1.8.8.1.1.2.2.2.cmml">Y</mi><mtext id="S3.E7.m1.8.8.1.1.2.2.3" xref="S3.E7.m1.8.8.1.1.2.2.3a.cmml">r</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.1.1.2.1" xref="S3.E7.m1.8.8.1.1.2.1.cmml">​</mo><mrow id="S3.E7.m1.8.8.1.1.2.3.2" xref="S3.E7.m1.8.8.1.1.2.3.1.cmml"><mo stretchy="false" id="S3.E7.m1.8.8.1.1.2.3.2.1" xref="S3.E7.m1.8.8.1.1.2.3.1.cmml">(</mo><mi id="S3.E7.m1.6.6" xref="S3.E7.m1.6.6.cmml">i</mi><mo id="S3.E7.m1.8.8.1.1.2.3.2.2" xref="S3.E7.m1.8.8.1.1.2.3.1.cmml">,</mo><mi id="S3.E7.m1.7.7" xref="S3.E7.m1.7.7.cmml">j</mi><mo stretchy="false" id="S3.E7.m1.8.8.1.1.2.3.2.3" xref="S3.E7.m1.8.8.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.8.1.1.1" xref="S3.E7.m1.8.8.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.5.5" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mo id="S3.E7.m1.5.5.6" xref="S3.E7.m1.8.8.1.1.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E7.m1.5.5.5" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mtr id="S3.E7.m1.5.5.5a" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E7.m1.5.5.5b" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mn id="S3.E7.m1.3.3.3.3.3.1" xref="S3.E7.m1.3.3.3.3.3.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E7.m1.5.5.5c" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mrow id="S3.E7.m1.2.2.2.2.2.2" xref="S3.E7.m1.2.2.2.2.2.2d.cmml"><mtext id="S3.E7.m1.2.2.2.2.2.2a" xref="S3.E7.m1.2.2.2.2.2.2d.cmml">if </mtext><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.cmml">(</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml">i</mi><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.cmml">,</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.cmml">)</mo></mrow><mtext id="S3.E7.m1.2.2.2.2.2.2b" xref="S3.E7.m1.2.2.2.2.2.2d.cmml"> is inside </mtext><mtext id="S3.E7.m1.2.2.2.2.2.2c" xref="S3.E7.m1.2.2.2.2.2.2d.cmml">BoundingBox(r)</mtext></mrow></mtd></mtr><mtr id="S3.E7.m1.5.5.5d" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E7.m1.5.5.5e" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mn id="S3.E7.m1.4.4.4.4.1.1" xref="S3.E7.m1.4.4.4.4.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E7.m1.5.5.5f" xref="S3.E7.m1.8.8.1.1.3.1.cmml"><mtext id="S3.E7.m1.5.5.5.5.2.1" xref="S3.E7.m1.5.5.5.5.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo id="S3.E7.m1.8.8.1.2" xref="S3.E7.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.8b"><apply id="S3.E7.m1.8.8.1.1.cmml" xref="S3.E7.m1.8.8.1"><eq id="S3.E7.m1.8.8.1.1.1.cmml" xref="S3.E7.m1.8.8.1.1.1"></eq><apply id="S3.E7.m1.8.8.1.1.2.cmml" xref="S3.E7.m1.8.8.1.1.2"><times id="S3.E7.m1.8.8.1.1.2.1.cmml" xref="S3.E7.m1.8.8.1.1.2.1"></times><apply id="S3.E7.m1.8.8.1.1.2.2.cmml" xref="S3.E7.m1.8.8.1.1.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.1.1.2.2.1.cmml" xref="S3.E7.m1.8.8.1.1.2.2">subscript</csymbol><ci id="S3.E7.m1.8.8.1.1.2.2.2.cmml" xref="S3.E7.m1.8.8.1.1.2.2.2">𝑌</ci><ci id="S3.E7.m1.8.8.1.1.2.2.3a.cmml" xref="S3.E7.m1.8.8.1.1.2.2.3"><mtext mathsize="70%" id="S3.E7.m1.8.8.1.1.2.2.3.cmml" xref="S3.E7.m1.8.8.1.1.2.2.3">r</mtext></ci></apply><interval closure="open" id="S3.E7.m1.8.8.1.1.2.3.1.cmml" xref="S3.E7.m1.8.8.1.1.2.3.2"><ci id="S3.E7.m1.6.6.cmml" xref="S3.E7.m1.6.6">𝑖</ci><ci id="S3.E7.m1.7.7.cmml" xref="S3.E7.m1.7.7">𝑗</ci></interval></apply><apply id="S3.E7.m1.8.8.1.1.3.1.cmml" xref="S3.E7.m1.5.5"><csymbol cd="latexml" id="S3.E7.m1.8.8.1.1.3.1.1.cmml" xref="S3.E7.m1.5.5.6">cases</csymbol><cn type="integer" id="S3.E7.m1.3.3.3.3.3.1.cmml" xref="S3.E7.m1.3.3.3.3.3.1">1</cn><ci id="S3.E7.m1.2.2.2.2.2.2d.cmml" xref="S3.E7.m1.2.2.2.2.2.2"><mrow id="S3.E7.m1.2.2.2.2.2.2.cmml" xref="S3.E7.m1.2.2.2.2.2.2"><mtext id="S3.E7.m1.2.2.2.2.2.2a.cmml" xref="S3.E7.m1.2.2.2.2.2.2">if </mtext><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2">(</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.1.1">i</mi><mo id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2">,</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.2">j</mi><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.m1.2.3.2">)</mo></mrow><mtext id="S3.E7.m1.2.2.2.2.2.2b.cmml" xref="S3.E7.m1.2.2.2.2.2.2"> is inside </mtext><mtext id="S3.E7.m1.2.2.2.2.2.2c.cmml" xref="S3.E7.m1.2.2.2.2.2.2">BoundingBox(r)</mtext></mrow></ci><cn type="integer" id="S3.E7.m1.4.4.4.4.1.1.cmml" xref="S3.E7.m1.4.4.4.4.1.1">0</cn><ci id="S3.E7.m1.5.5.5.5.2.1a.cmml" xref="S3.E7.m1.5.5.5.5.2.1"><mtext id="S3.E7.m1.5.5.5.5.2.1.cmml" xref="S3.E7.m1.5.5.5.5.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.8c">Y_{\text{r}}(i,j)=\begin{cases}1&amp;\text{if $(i,j)$ is inside $\text{BoundingBox(r)}$}\\
0&amp;\text{otherwise}\end{cases},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px3.p2.7" class="ltx_p">where <math id="S3.SS2.SSS0.Px3.p2.5.m1.2" class="ltx_Math" alttext="r\in\{\text{lip},\text{exp}\}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.5.m1.2a"><mrow id="S3.SS2.SSS0.Px3.p2.5.m1.2.3" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.cmml"><mi id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.2" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.2.cmml">r</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.1" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.2" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.2.1" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.1.cmml">{</mo><mtext id="S3.SS2.SSS0.Px3.p2.5.m1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m1.1.1a.cmml">lip</mtext><mo id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.2.2" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.1.cmml">,</mo><mtext id="S3.SS2.SSS0.Px3.p2.5.m1.2.2" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.2a.cmml">exp</mtext><mo stretchy="false" id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.2.3" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.5.m1.2b"><apply id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3"><in id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.1"></in><ci id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.2">𝑟</ci><set id="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.3.3.2"><ci id="S3.SS2.SSS0.Px3.p2.5.m1.1.1a.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.1.1"><mtext id="S3.SS2.SSS0.Px3.p2.5.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.1.1">lip</mtext></ci><ci id="S3.SS2.SSS0.Px3.p2.5.m1.2.2a.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.2"><mtext id="S3.SS2.SSS0.Px3.p2.5.m1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m1.2.2">exp</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.5.m1.2c">r\in\{\text{lip},\text{exp}\}</annotation></semantics></math>, and <span id="S3.SS2.SSS0.Px3.p2.7.1" class="ltx_text ltx_markedasmath">BoundingBox(r)</span> denotes the bounding box of all the landmarks for <span id="S3.SS2.SSS0.Px3.p2.7.2" class="ltx_text ltx_markedasmath">r</span>.
We compute</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E8.m1.1" class="ltx_Math" alttext="\displaystyle M_{\text{lip}}" display="inline"><semantics id="S3.E8.m1.1a"><msub id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><mi id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">M</mi><mtext id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3a.cmml">lip</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1">subscript</csymbol><ci id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2">𝑀</ci><ci id="S3.E8.m1.1.1.3a.cmml" xref="S3.E8.m1.1.1.3"><mtext mathsize="70%" id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3">lip</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\displaystyle M_{\text{lip}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E8.m2.1" class="ltx_Math" alttext="\displaystyle=Y_{\text{lip}}," display="inline"><semantics id="S3.E8.m2.1a"><mrow id="S3.E8.m2.1.1.1" xref="S3.E8.m2.1.1.1.1.cmml"><mrow id="S3.E8.m2.1.1.1.1" xref="S3.E8.m2.1.1.1.1.cmml"><mi id="S3.E8.m2.1.1.1.1.2" xref="S3.E8.m2.1.1.1.1.2.cmml"></mi><mo id="S3.E8.m2.1.1.1.1.1" xref="S3.E8.m2.1.1.1.1.1.cmml">=</mo><msub id="S3.E8.m2.1.1.1.1.3" xref="S3.E8.m2.1.1.1.1.3.cmml"><mi id="S3.E8.m2.1.1.1.1.3.2" xref="S3.E8.m2.1.1.1.1.3.2.cmml">Y</mi><mtext id="S3.E8.m2.1.1.1.1.3.3" xref="S3.E8.m2.1.1.1.1.3.3a.cmml">lip</mtext></msub></mrow><mo id="S3.E8.m2.1.1.1.2" xref="S3.E8.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m2.1b"><apply id="S3.E8.m2.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1"><eq id="S3.E8.m2.1.1.1.1.1.cmml" xref="S3.E8.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.E8.m2.1.1.1.1.2.cmml" xref="S3.E8.m2.1.1.1.1.2">absent</csymbol><apply id="S3.E8.m2.1.1.1.1.3.cmml" xref="S3.E8.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m2.1.1.1.1.3.1.cmml" xref="S3.E8.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.E8.m2.1.1.1.1.3.2.cmml" xref="S3.E8.m2.1.1.1.1.3.2">𝑌</ci><ci id="S3.E8.m2.1.1.1.1.3.3a.cmml" xref="S3.E8.m2.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E8.m2.1.1.1.1.3.3.cmml" xref="S3.E8.m2.1.1.1.1.3.3">lip</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m2.1c">\displaystyle=Y_{\text{lip}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1.1" class="ltx_Math" alttext="\displaystyle M_{\text{exp}}" display="inline"><semantics id="S3.E9.m1.1a"><msub id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><mi id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml">M</mi><mtext id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1">subscript</csymbol><ci id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2">𝑀</ci><ci id="S3.E9.m1.1.1.3a.cmml" xref="S3.E9.m1.1.1.3"><mtext mathsize="70%" id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\displaystyle M_{\text{exp}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m2.1" class="ltx_Math" alttext="\displaystyle=(1-M_{\text{lip}})\odot Y_{\text{exp}}," display="inline"><semantics id="S3.E9.m2.1a"><mrow id="S3.E9.m2.1.1.1" xref="S3.E9.m2.1.1.1.1.cmml"><mrow id="S3.E9.m2.1.1.1.1" xref="S3.E9.m2.1.1.1.1.cmml"><mi id="S3.E9.m2.1.1.1.1.3" xref="S3.E9.m2.1.1.1.1.3.cmml"></mi><mo id="S3.E9.m2.1.1.1.1.2" xref="S3.E9.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.E9.m2.1.1.1.1.1" xref="S3.E9.m2.1.1.1.1.1.cmml"><mrow id="S3.E9.m2.1.1.1.1.1.1.1" xref="S3.E9.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m2.1.1.1.1.1.1.1.2" xref="S3.E9.m2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m2.1.1.1.1.1.1.1.1" xref="S3.E9.m2.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E9.m2.1.1.1.1.1.1.1.1.2" xref="S3.E9.m2.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E9.m2.1.1.1.1.1.1.1.1.1" xref="S3.E9.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E9.m2.1.1.1.1.1.1.1.1.3" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m2.1.1.1.1.1.1.1.1.3.2" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.2.cmml">M</mi><mtext id="S3.E9.m2.1.1.1.1.1.1.1.1.3.3" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.3a.cmml">lip</mtext></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E9.m2.1.1.1.1.1.1.1.3" xref="S3.E9.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E9.m2.1.1.1.1.1.2" xref="S3.E9.m2.1.1.1.1.1.2.cmml">⊙</mo><msub id="S3.E9.m2.1.1.1.1.1.3" xref="S3.E9.m2.1.1.1.1.1.3.cmml"><mi id="S3.E9.m2.1.1.1.1.1.3.2" xref="S3.E9.m2.1.1.1.1.1.3.2.cmml">Y</mi><mtext id="S3.E9.m2.1.1.1.1.1.3.3" xref="S3.E9.m2.1.1.1.1.1.3.3a.cmml">exp</mtext></msub></mrow></mrow><mo id="S3.E9.m2.1.1.1.2" xref="S3.E9.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.1b"><apply id="S3.E9.m2.1.1.1.1.cmml" xref="S3.E9.m2.1.1.1"><eq id="S3.E9.m2.1.1.1.1.2.cmml" xref="S3.E9.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E9.m2.1.1.1.1.3.cmml" xref="S3.E9.m2.1.1.1.1.3">absent</csymbol><apply id="S3.E9.m2.1.1.1.1.1.cmml" xref="S3.E9.m2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E9.m2.1.1.1.1.1.2.cmml" xref="S3.E9.m2.1.1.1.1.1.2">direct-product</csymbol><apply id="S3.E9.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1"><minus id="S3.E9.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E9.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E9.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.2">𝑀</ci><ci id="S3.E9.m2.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E9.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m2.1.1.1.1.1.1.1.1.3.3">lip</mtext></ci></apply></apply><apply id="S3.E9.m2.1.1.1.1.1.3.cmml" xref="S3.E9.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m2.1.1.1.1.1.3.1.cmml" xref="S3.E9.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m2.1.1.1.1.1.3.2.cmml" xref="S3.E9.m2.1.1.1.1.1.3.2">𝑌</ci><ci id="S3.E9.m2.1.1.1.1.1.3.3a.cmml" xref="S3.E9.m2.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E9.m2.1.1.1.1.1.3.3.cmml" xref="S3.E9.m2.1.1.1.1.1.3.3">exp</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.1c">\displaystyle=(1-M_{\text{lip}})\odot Y_{\text{exp}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E10.m1.1" class="ltx_Math" alttext="\displaystyle M_{\text{pose}}" display="inline"><semantics id="S3.E10.m1.1a"><msub id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml"><mi id="S3.E10.m1.1.1.2" xref="S3.E10.m1.1.1.2.cmml">M</mi><mtext id="S3.E10.m1.1.1.3" xref="S3.E10.m1.1.1.3a.cmml">pose</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E10.m1.1b"><apply id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.cmml" xref="S3.E10.m1.1.1">subscript</csymbol><ci id="S3.E10.m1.1.1.2.cmml" xref="S3.E10.m1.1.1.2">𝑀</ci><ci id="S3.E10.m1.1.1.3a.cmml" xref="S3.E10.m1.1.1.3"><mtext mathsize="70%" id="S3.E10.m1.1.1.3.cmml" xref="S3.E10.m1.1.1.3">pose</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.1c">\displaystyle M_{\text{pose}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E10.m2.1" class="ltx_Math" alttext="\displaystyle=1-M_{\text{exp}}," display="inline"><semantics id="S3.E10.m2.1a"><mrow id="S3.E10.m2.1.1.1" xref="S3.E10.m2.1.1.1.1.cmml"><mrow id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.1.cmml"><mi id="S3.E10.m2.1.1.1.1.2" xref="S3.E10.m2.1.1.1.1.2.cmml"></mi><mo id="S3.E10.m2.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.E10.m2.1.1.1.1.3" xref="S3.E10.m2.1.1.1.1.3.cmml"><mn id="S3.E10.m2.1.1.1.1.3.2" xref="S3.E10.m2.1.1.1.1.3.2.cmml">1</mn><mo id="S3.E10.m2.1.1.1.1.3.1" xref="S3.E10.m2.1.1.1.1.3.1.cmml">−</mo><msub id="S3.E10.m2.1.1.1.1.3.3" xref="S3.E10.m2.1.1.1.1.3.3.cmml"><mi id="S3.E10.m2.1.1.1.1.3.3.2" xref="S3.E10.m2.1.1.1.1.3.3.2.cmml">M</mi><mtext id="S3.E10.m2.1.1.1.1.3.3.3" xref="S3.E10.m2.1.1.1.1.3.3.3a.cmml">exp</mtext></msub></mrow></mrow><mo id="S3.E10.m2.1.1.1.2" xref="S3.E10.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.1b"><apply id="S3.E10.m2.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1"><eq id="S3.E10.m2.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.E10.m2.1.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1.2">absent</csymbol><apply id="S3.E10.m2.1.1.1.1.3.cmml" xref="S3.E10.m2.1.1.1.1.3"><minus id="S3.E10.m2.1.1.1.1.3.1.cmml" xref="S3.E10.m2.1.1.1.1.3.1"></minus><cn type="integer" id="S3.E10.m2.1.1.1.1.3.2.cmml" xref="S3.E10.m2.1.1.1.1.3.2">1</cn><apply id="S3.E10.m2.1.1.1.1.3.3.cmml" xref="S3.E10.m2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E10.m2.1.1.1.1.3.3.1.cmml" xref="S3.E10.m2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E10.m2.1.1.1.1.3.3.2.cmml" xref="S3.E10.m2.1.1.1.1.3.3.2">𝑀</ci><ci id="S3.E10.m2.1.1.1.1.3.3.3a.cmml" xref="S3.E10.m2.1.1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E10.m2.1.1.1.1.3.3.3.cmml" xref="S3.E10.m2.1.1.1.1.3.3.3">exp</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.1c">\displaystyle=1-M_{\text{exp}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px3.p2.8" class="ltx_p">where <math id="S3.SS2.SSS0.Px3.p2.8.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.8.m1.1a"><mo id="S3.SS2.SSS0.Px3.p2.8.m1.1.1" xref="S3.SS2.SSS0.Px3.p2.8.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.8.m1.1b"><csymbol cd="latexml" id="S3.SS2.SSS0.Px3.p2.8.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.8.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.8.m1.1c">\odot</annotation></semantics></math> is the Hadamard product.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p3.1" class="ltx_p">Next, we apply the cross-attention mechanism between the latent representations and the audio embeddings:</p>
<table id="S3.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E11.m1.4" class="ltx_Math" alttext="o_{t}^{(s)}=\text{CrossAttn}(z_{t}^{(s)},c_{\text{audio}}^{(s)})." display="block"><semantics id="S3.E11.m1.4a"><mrow id="S3.E11.m1.4.4.1" xref="S3.E11.m1.4.4.1.1.cmml"><mrow id="S3.E11.m1.4.4.1.1" xref="S3.E11.m1.4.4.1.1.cmml"><msubsup id="S3.E11.m1.4.4.1.1.4" xref="S3.E11.m1.4.4.1.1.4.cmml"><mi id="S3.E11.m1.4.4.1.1.4.2.2" xref="S3.E11.m1.4.4.1.1.4.2.2.cmml">o</mi><mi id="S3.E11.m1.4.4.1.1.4.2.3" xref="S3.E11.m1.4.4.1.1.4.2.3.cmml">t</mi><mrow id="S3.E11.m1.1.1.1.3" xref="S3.E11.m1.4.4.1.1.4.cmml"><mo stretchy="false" id="S3.E11.m1.1.1.1.3.1" xref="S3.E11.m1.4.4.1.1.4.cmml">(</mo><mi id="S3.E11.m1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E11.m1.1.1.1.3.2" xref="S3.E11.m1.4.4.1.1.4.cmml">)</mo></mrow></msubsup><mo id="S3.E11.m1.4.4.1.1.3" xref="S3.E11.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E11.m1.4.4.1.1.2" xref="S3.E11.m1.4.4.1.1.2.cmml"><mtext id="S3.E11.m1.4.4.1.1.2.4" xref="S3.E11.m1.4.4.1.1.2.4a.cmml">CrossAttn</mtext><mo lspace="0em" rspace="0em" id="S3.E11.m1.4.4.1.1.2.3" xref="S3.E11.m1.4.4.1.1.2.3.cmml">​</mo><mrow id="S3.E11.m1.4.4.1.1.2.2.2" xref="S3.E11.m1.4.4.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E11.m1.4.4.1.1.2.2.2.3" xref="S3.E11.m1.4.4.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E11.m1.4.4.1.1.1.1.1.1" xref="S3.E11.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E11.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.E11.m1.4.4.1.1.1.1.1.1.2.2.cmml">z</mi><mi id="S3.E11.m1.4.4.1.1.1.1.1.1.2.3" xref="S3.E11.m1.4.4.1.1.1.1.1.1.2.3.cmml">t</mi><mrow id="S3.E11.m1.2.2.1.3" xref="S3.E11.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E11.m1.2.2.1.3.1" xref="S3.E11.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E11.m1.2.2.1.1" xref="S3.E11.m1.2.2.1.1.cmml">s</mi><mo stretchy="false" id="S3.E11.m1.2.2.1.3.2" xref="S3.E11.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E11.m1.4.4.1.1.2.2.2.4" xref="S3.E11.m1.4.4.1.1.2.2.3.cmml">,</mo><msubsup id="S3.E11.m1.4.4.1.1.2.2.2.2" xref="S3.E11.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E11.m1.4.4.1.1.2.2.2.2.2.2" xref="S3.E11.m1.4.4.1.1.2.2.2.2.2.2.cmml">c</mi><mtext id="S3.E11.m1.4.4.1.1.2.2.2.2.2.3" xref="S3.E11.m1.4.4.1.1.2.2.2.2.2.3a.cmml">audio</mtext><mrow id="S3.E11.m1.3.3.1.3" xref="S3.E11.m1.4.4.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S3.E11.m1.3.3.1.3.1" xref="S3.E11.m1.4.4.1.1.2.2.2.2.cmml">(</mo><mi id="S3.E11.m1.3.3.1.1" xref="S3.E11.m1.3.3.1.1.cmml">s</mi><mo stretchy="false" id="S3.E11.m1.3.3.1.3.2" xref="S3.E11.m1.4.4.1.1.2.2.2.2.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.E11.m1.4.4.1.1.2.2.2.5" xref="S3.E11.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E11.m1.4.4.1.2" xref="S3.E11.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m1.4b"><apply id="S3.E11.m1.4.4.1.1.cmml" xref="S3.E11.m1.4.4.1"><eq id="S3.E11.m1.4.4.1.1.3.cmml" xref="S3.E11.m1.4.4.1.1.3"></eq><apply id="S3.E11.m1.4.4.1.1.4.cmml" xref="S3.E11.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.4.1.cmml" xref="S3.E11.m1.4.4.1.1.4">superscript</csymbol><apply id="S3.E11.m1.4.4.1.1.4.2.cmml" xref="S3.E11.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.4.2.1.cmml" xref="S3.E11.m1.4.4.1.1.4">subscript</csymbol><ci id="S3.E11.m1.4.4.1.1.4.2.2.cmml" xref="S3.E11.m1.4.4.1.1.4.2.2">𝑜</ci><ci id="S3.E11.m1.4.4.1.1.4.2.3.cmml" xref="S3.E11.m1.4.4.1.1.4.2.3">𝑡</ci></apply><ci id="S3.E11.m1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1">𝑠</ci></apply><apply id="S3.E11.m1.4.4.1.1.2.cmml" xref="S3.E11.m1.4.4.1.1.2"><times id="S3.E11.m1.4.4.1.1.2.3.cmml" xref="S3.E11.m1.4.4.1.1.2.3"></times><ci id="S3.E11.m1.4.4.1.1.2.4a.cmml" xref="S3.E11.m1.4.4.1.1.2.4"><mtext id="S3.E11.m1.4.4.1.1.2.4.cmml" xref="S3.E11.m1.4.4.1.1.2.4">CrossAttn</mtext></ci><interval closure="open" id="S3.E11.m1.4.4.1.1.2.2.3.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2"><apply id="S3.E11.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E11.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E11.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1.2.2">𝑧</ci><ci id="S3.E11.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.E11.m1.4.4.1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.E11.m1.2.2.1.1.cmml" xref="S3.E11.m1.2.2.1.1">𝑠</ci></apply><apply id="S3.E11.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E11.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E11.m1.4.4.1.1.2.2.2.2.2.1.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E11.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2.2.2">𝑐</ci><ci id="S3.E11.m1.4.4.1.1.2.2.2.2.2.3a.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2.2.3"><mtext mathsize="70%" id="S3.E11.m1.4.4.1.1.2.2.2.2.2.3.cmml" xref="S3.E11.m1.4.4.1.1.2.2.2.2.2.3">audio</mtext></ci></apply><ci id="S3.E11.m1.3.3.1.1.cmml" xref="S3.E11.m1.3.3.1.1">𝑠</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m1.4c">o_{t}^{(s)}=\text{CrossAttn}(z_{t}^{(s)},c_{\text{audio}}^{(s)}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px3.p3.2" class="ltx_p">We apply masks and obtain different scaled latent representations by</p>
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E12.m1.1" class="ltx_Math" alttext="\displaystyle b_{t}^{(s)}" display="inline"><semantics id="S3.E12.m1.1a"><msubsup id="S3.E12.m1.1.2" xref="S3.E12.m1.1.2.cmml"><mi id="S3.E12.m1.1.2.2.2" xref="S3.E12.m1.1.2.2.2.cmml">b</mi><mi id="S3.E12.m1.1.2.2.3" xref="S3.E12.m1.1.2.2.3.cmml">t</mi><mrow id="S3.E12.m1.1.1.1.3" xref="S3.E12.m1.1.2.cmml"><mo stretchy="false" id="S3.E12.m1.1.1.1.3.1" xref="S3.E12.m1.1.2.cmml">(</mo><mi id="S3.E12.m1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E12.m1.1.1.1.3.2" xref="S3.E12.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E12.m1.1b"><apply id="S3.E12.m1.1.2.cmml" xref="S3.E12.m1.1.2"><csymbol cd="ambiguous" id="S3.E12.m1.1.2.1.cmml" xref="S3.E12.m1.1.2">superscript</csymbol><apply id="S3.E12.m1.1.2.2.cmml" xref="S3.E12.m1.1.2"><csymbol cd="ambiguous" id="S3.E12.m1.1.2.2.1.cmml" xref="S3.E12.m1.1.2">subscript</csymbol><ci id="S3.E12.m1.1.2.2.2.cmml" xref="S3.E12.m1.1.2.2.2">𝑏</ci><ci id="S3.E12.m1.1.2.2.3.cmml" xref="S3.E12.m1.1.2.2.3">𝑡</ci></apply><ci id="S3.E12.m1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12.m1.1c">\displaystyle b_{t}^{(s)}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E12.m2.2" class="ltx_Math" alttext="\displaystyle=o_{t}^{(s)}\odot M_{\text{pose}}," display="inline"><semantics id="S3.E12.m2.2a"><mrow id="S3.E12.m2.2.2.1" xref="S3.E12.m2.2.2.1.1.cmml"><mrow id="S3.E12.m2.2.2.1.1" xref="S3.E12.m2.2.2.1.1.cmml"><mi id="S3.E12.m2.2.2.1.1.2" xref="S3.E12.m2.2.2.1.1.2.cmml"></mi><mo id="S3.E12.m2.2.2.1.1.1" xref="S3.E12.m2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E12.m2.2.2.1.1.3" xref="S3.E12.m2.2.2.1.1.3.cmml"><msubsup id="S3.E12.m2.2.2.1.1.3.2" xref="S3.E12.m2.2.2.1.1.3.2.cmml"><mi id="S3.E12.m2.2.2.1.1.3.2.2.2" xref="S3.E12.m2.2.2.1.1.3.2.2.2.cmml">o</mi><mi id="S3.E12.m2.2.2.1.1.3.2.2.3" xref="S3.E12.m2.2.2.1.1.3.2.2.3.cmml">t</mi><mrow id="S3.E12.m2.1.1.1.3" xref="S3.E12.m2.2.2.1.1.3.2.cmml"><mo stretchy="false" id="S3.E12.m2.1.1.1.3.1" xref="S3.E12.m2.2.2.1.1.3.2.cmml">(</mo><mi id="S3.E12.m2.1.1.1.1" xref="S3.E12.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E12.m2.1.1.1.3.2" xref="S3.E12.m2.2.2.1.1.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E12.m2.2.2.1.1.3.1" xref="S3.E12.m2.2.2.1.1.3.1.cmml">⊙</mo><msub id="S3.E12.m2.2.2.1.1.3.3" xref="S3.E12.m2.2.2.1.1.3.3.cmml"><mi id="S3.E12.m2.2.2.1.1.3.3.2" xref="S3.E12.m2.2.2.1.1.3.3.2.cmml">M</mi><mtext id="S3.E12.m2.2.2.1.1.3.3.3" xref="S3.E12.m2.2.2.1.1.3.3.3a.cmml">pose</mtext></msub></mrow></mrow><mo id="S3.E12.m2.2.2.1.2" xref="S3.E12.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E12.m2.2b"><apply id="S3.E12.m2.2.2.1.1.cmml" xref="S3.E12.m2.2.2.1"><eq id="S3.E12.m2.2.2.1.1.1.cmml" xref="S3.E12.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.E12.m2.2.2.1.1.2.cmml" xref="S3.E12.m2.2.2.1.1.2">absent</csymbol><apply id="S3.E12.m2.2.2.1.1.3.cmml" xref="S3.E12.m2.2.2.1.1.3"><csymbol cd="latexml" id="S3.E12.m2.2.2.1.1.3.1.cmml" xref="S3.E12.m2.2.2.1.1.3.1">direct-product</csymbol><apply id="S3.E12.m2.2.2.1.1.3.2.cmml" xref="S3.E12.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E12.m2.2.2.1.1.3.2.1.cmml" xref="S3.E12.m2.2.2.1.1.3.2">superscript</csymbol><apply id="S3.E12.m2.2.2.1.1.3.2.2.cmml" xref="S3.E12.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E12.m2.2.2.1.1.3.2.2.1.cmml" xref="S3.E12.m2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E12.m2.2.2.1.1.3.2.2.2.cmml" xref="S3.E12.m2.2.2.1.1.3.2.2.2">𝑜</ci><ci id="S3.E12.m2.2.2.1.1.3.2.2.3.cmml" xref="S3.E12.m2.2.2.1.1.3.2.2.3">𝑡</ci></apply><ci id="S3.E12.m2.1.1.1.1.cmml" xref="S3.E12.m2.1.1.1.1">𝑠</ci></apply><apply id="S3.E12.m2.2.2.1.1.3.3.cmml" xref="S3.E12.m2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E12.m2.2.2.1.1.3.3.1.cmml" xref="S3.E12.m2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.E12.m2.2.2.1.1.3.3.2.cmml" xref="S3.E12.m2.2.2.1.1.3.3.2">𝑀</ci><ci id="S3.E12.m2.2.2.1.1.3.3.3a.cmml" xref="S3.E12.m2.2.2.1.1.3.3.3"><mtext mathsize="70%" id="S3.E12.m2.2.2.1.1.3.3.3.cmml" xref="S3.E12.m2.2.2.1.1.3.3.3">pose</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12.m2.2c">\displaystyle=o_{t}^{(s)}\odot M_{\text{pose}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
<tbody id="S3.E13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E13.m1.1" class="ltx_Math" alttext="\displaystyle f_{t}^{(s)}" display="inline"><semantics id="S3.E13.m1.1a"><msubsup id="S3.E13.m1.1.2" xref="S3.E13.m1.1.2.cmml"><mi id="S3.E13.m1.1.2.2.2" xref="S3.E13.m1.1.2.2.2.cmml">f</mi><mi id="S3.E13.m1.1.2.2.3" xref="S3.E13.m1.1.2.2.3.cmml">t</mi><mrow id="S3.E13.m1.1.1.1.3" xref="S3.E13.m1.1.2.cmml"><mo stretchy="false" id="S3.E13.m1.1.1.1.3.1" xref="S3.E13.m1.1.2.cmml">(</mo><mi id="S3.E13.m1.1.1.1.1" xref="S3.E13.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E13.m1.1.1.1.3.2" xref="S3.E13.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E13.m1.1b"><apply id="S3.E13.m1.1.2.cmml" xref="S3.E13.m1.1.2"><csymbol cd="ambiguous" id="S3.E13.m1.1.2.1.cmml" xref="S3.E13.m1.1.2">superscript</csymbol><apply id="S3.E13.m1.1.2.2.cmml" xref="S3.E13.m1.1.2"><csymbol cd="ambiguous" id="S3.E13.m1.1.2.2.1.cmml" xref="S3.E13.m1.1.2">subscript</csymbol><ci id="S3.E13.m1.1.2.2.2.cmml" xref="S3.E13.m1.1.2.2.2">𝑓</ci><ci id="S3.E13.m1.1.2.2.3.cmml" xref="S3.E13.m1.1.2.2.3">𝑡</ci></apply><ci id="S3.E13.m1.1.1.1.1.cmml" xref="S3.E13.m1.1.1.1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E13.m1.1c">\displaystyle f_{t}^{(s)}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E13.m2.2" class="ltx_Math" alttext="\displaystyle=o_{t}^{(s)}\odot M_{\text{exp}}," display="inline"><semantics id="S3.E13.m2.2a"><mrow id="S3.E13.m2.2.2.1" xref="S3.E13.m2.2.2.1.1.cmml"><mrow id="S3.E13.m2.2.2.1.1" xref="S3.E13.m2.2.2.1.1.cmml"><mi id="S3.E13.m2.2.2.1.1.2" xref="S3.E13.m2.2.2.1.1.2.cmml"></mi><mo id="S3.E13.m2.2.2.1.1.1" xref="S3.E13.m2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E13.m2.2.2.1.1.3" xref="S3.E13.m2.2.2.1.1.3.cmml"><msubsup id="S3.E13.m2.2.2.1.1.3.2" xref="S3.E13.m2.2.2.1.1.3.2.cmml"><mi id="S3.E13.m2.2.2.1.1.3.2.2.2" xref="S3.E13.m2.2.2.1.1.3.2.2.2.cmml">o</mi><mi id="S3.E13.m2.2.2.1.1.3.2.2.3" xref="S3.E13.m2.2.2.1.1.3.2.2.3.cmml">t</mi><mrow id="S3.E13.m2.1.1.1.3" xref="S3.E13.m2.2.2.1.1.3.2.cmml"><mo stretchy="false" id="S3.E13.m2.1.1.1.3.1" xref="S3.E13.m2.2.2.1.1.3.2.cmml">(</mo><mi id="S3.E13.m2.1.1.1.1" xref="S3.E13.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E13.m2.1.1.1.3.2" xref="S3.E13.m2.2.2.1.1.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E13.m2.2.2.1.1.3.1" xref="S3.E13.m2.2.2.1.1.3.1.cmml">⊙</mo><msub id="S3.E13.m2.2.2.1.1.3.3" xref="S3.E13.m2.2.2.1.1.3.3.cmml"><mi id="S3.E13.m2.2.2.1.1.3.3.2" xref="S3.E13.m2.2.2.1.1.3.3.2.cmml">M</mi><mtext id="S3.E13.m2.2.2.1.1.3.3.3" xref="S3.E13.m2.2.2.1.1.3.3.3a.cmml">exp</mtext></msub></mrow></mrow><mo id="S3.E13.m2.2.2.1.2" xref="S3.E13.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E13.m2.2b"><apply id="S3.E13.m2.2.2.1.1.cmml" xref="S3.E13.m2.2.2.1"><eq id="S3.E13.m2.2.2.1.1.1.cmml" xref="S3.E13.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.E13.m2.2.2.1.1.2.cmml" xref="S3.E13.m2.2.2.1.1.2">absent</csymbol><apply id="S3.E13.m2.2.2.1.1.3.cmml" xref="S3.E13.m2.2.2.1.1.3"><csymbol cd="latexml" id="S3.E13.m2.2.2.1.1.3.1.cmml" xref="S3.E13.m2.2.2.1.1.3.1">direct-product</csymbol><apply id="S3.E13.m2.2.2.1.1.3.2.cmml" xref="S3.E13.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E13.m2.2.2.1.1.3.2.1.cmml" xref="S3.E13.m2.2.2.1.1.3.2">superscript</csymbol><apply id="S3.E13.m2.2.2.1.1.3.2.2.cmml" xref="S3.E13.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E13.m2.2.2.1.1.3.2.2.1.cmml" xref="S3.E13.m2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E13.m2.2.2.1.1.3.2.2.2.cmml" xref="S3.E13.m2.2.2.1.1.3.2.2.2">𝑜</ci><ci id="S3.E13.m2.2.2.1.1.3.2.2.3.cmml" xref="S3.E13.m2.2.2.1.1.3.2.2.3">𝑡</ci></apply><ci id="S3.E13.m2.1.1.1.1.cmml" xref="S3.E13.m2.1.1.1.1">𝑠</ci></apply><apply id="S3.E13.m2.2.2.1.1.3.3.cmml" xref="S3.E13.m2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E13.m2.2.2.1.1.3.3.1.cmml" xref="S3.E13.m2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.E13.m2.2.2.1.1.3.3.2.cmml" xref="S3.E13.m2.2.2.1.1.3.3.2">𝑀</ci><ci id="S3.E13.m2.2.2.1.1.3.3.3a.cmml" xref="S3.E13.m2.2.2.1.1.3.3.3"><mtext mathsize="70%" id="S3.E13.m2.2.2.1.1.3.3.3.cmml" xref="S3.E13.m2.2.2.1.1.3.3.3">exp</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E13.m2.2c">\displaystyle=o_{t}^{(s)}\odot M_{\text{exp}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
<tbody id="S3.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E14.m1.1" class="ltx_Math" alttext="\displaystyle l_{t}^{(s)}" display="inline"><semantics id="S3.E14.m1.1a"><msubsup id="S3.E14.m1.1.2" xref="S3.E14.m1.1.2.cmml"><mi id="S3.E14.m1.1.2.2.2" xref="S3.E14.m1.1.2.2.2.cmml">l</mi><mi id="S3.E14.m1.1.2.2.3" xref="S3.E14.m1.1.2.2.3.cmml">t</mi><mrow id="S3.E14.m1.1.1.1.3" xref="S3.E14.m1.1.2.cmml"><mo stretchy="false" id="S3.E14.m1.1.1.1.3.1" xref="S3.E14.m1.1.2.cmml">(</mo><mi id="S3.E14.m1.1.1.1.1" xref="S3.E14.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E14.m1.1.1.1.3.2" xref="S3.E14.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E14.m1.1b"><apply id="S3.E14.m1.1.2.cmml" xref="S3.E14.m1.1.2"><csymbol cd="ambiguous" id="S3.E14.m1.1.2.1.cmml" xref="S3.E14.m1.1.2">superscript</csymbol><apply id="S3.E14.m1.1.2.2.cmml" xref="S3.E14.m1.1.2"><csymbol cd="ambiguous" id="S3.E14.m1.1.2.2.1.cmml" xref="S3.E14.m1.1.2">subscript</csymbol><ci id="S3.E14.m1.1.2.2.2.cmml" xref="S3.E14.m1.1.2.2.2">𝑙</ci><ci id="S3.E14.m1.1.2.2.3.cmml" xref="S3.E14.m1.1.2.2.3">𝑡</ci></apply><ci id="S3.E14.m1.1.1.1.1.cmml" xref="S3.E14.m1.1.1.1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E14.m1.1c">\displaystyle l_{t}^{(s)}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E14.m2.2" class="ltx_Math" alttext="\displaystyle=o_{t}^{(s)}\odot M_{\text{lip}}." display="inline"><semantics id="S3.E14.m2.2a"><mrow id="S3.E14.m2.2.2.1" xref="S3.E14.m2.2.2.1.1.cmml"><mrow id="S3.E14.m2.2.2.1.1" xref="S3.E14.m2.2.2.1.1.cmml"><mi id="S3.E14.m2.2.2.1.1.2" xref="S3.E14.m2.2.2.1.1.2.cmml"></mi><mo id="S3.E14.m2.2.2.1.1.1" xref="S3.E14.m2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E14.m2.2.2.1.1.3" xref="S3.E14.m2.2.2.1.1.3.cmml"><msubsup id="S3.E14.m2.2.2.1.1.3.2" xref="S3.E14.m2.2.2.1.1.3.2.cmml"><mi id="S3.E14.m2.2.2.1.1.3.2.2.2" xref="S3.E14.m2.2.2.1.1.3.2.2.2.cmml">o</mi><mi id="S3.E14.m2.2.2.1.1.3.2.2.3" xref="S3.E14.m2.2.2.1.1.3.2.2.3.cmml">t</mi><mrow id="S3.E14.m2.1.1.1.3" xref="S3.E14.m2.2.2.1.1.3.2.cmml"><mo stretchy="false" id="S3.E14.m2.1.1.1.3.1" xref="S3.E14.m2.2.2.1.1.3.2.cmml">(</mo><mi id="S3.E14.m2.1.1.1.1" xref="S3.E14.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E14.m2.1.1.1.3.2" xref="S3.E14.m2.2.2.1.1.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E14.m2.2.2.1.1.3.1" xref="S3.E14.m2.2.2.1.1.3.1.cmml">⊙</mo><msub id="S3.E14.m2.2.2.1.1.3.3" xref="S3.E14.m2.2.2.1.1.3.3.cmml"><mi id="S3.E14.m2.2.2.1.1.3.3.2" xref="S3.E14.m2.2.2.1.1.3.3.2.cmml">M</mi><mtext id="S3.E14.m2.2.2.1.1.3.3.3" xref="S3.E14.m2.2.2.1.1.3.3.3a.cmml">lip</mtext></msub></mrow></mrow><mo lspace="0em" id="S3.E14.m2.2.2.1.2" xref="S3.E14.m2.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E14.m2.2b"><apply id="S3.E14.m2.2.2.1.1.cmml" xref="S3.E14.m2.2.2.1"><eq id="S3.E14.m2.2.2.1.1.1.cmml" xref="S3.E14.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.E14.m2.2.2.1.1.2.cmml" xref="S3.E14.m2.2.2.1.1.2">absent</csymbol><apply id="S3.E14.m2.2.2.1.1.3.cmml" xref="S3.E14.m2.2.2.1.1.3"><csymbol cd="latexml" id="S3.E14.m2.2.2.1.1.3.1.cmml" xref="S3.E14.m2.2.2.1.1.3.1">direct-product</csymbol><apply id="S3.E14.m2.2.2.1.1.3.2.cmml" xref="S3.E14.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E14.m2.2.2.1.1.3.2.1.cmml" xref="S3.E14.m2.2.2.1.1.3.2">superscript</csymbol><apply id="S3.E14.m2.2.2.1.1.3.2.2.cmml" xref="S3.E14.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E14.m2.2.2.1.1.3.2.2.1.cmml" xref="S3.E14.m2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E14.m2.2.2.1.1.3.2.2.2.cmml" xref="S3.E14.m2.2.2.1.1.3.2.2.2">𝑜</ci><ci id="S3.E14.m2.2.2.1.1.3.2.2.3.cmml" xref="S3.E14.m2.2.2.1.1.3.2.2.3">𝑡</ci></apply><ci id="S3.E14.m2.1.1.1.1.cmml" xref="S3.E14.m2.1.1.1.1">𝑠</ci></apply><apply id="S3.E14.m2.2.2.1.1.3.3.cmml" xref="S3.E14.m2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E14.m2.2.2.1.1.3.3.1.cmml" xref="S3.E14.m2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.E14.m2.2.2.1.1.3.3.2.cmml" xref="S3.E14.m2.2.2.1.1.3.3.2">𝑀</ci><ci id="S3.E14.m2.2.2.1.1.3.3.3a.cmml" xref="S3.E14.m2.2.2.1.1.3.3.3"><mtext mathsize="70%" id="S3.E14.m2.2.2.1.1.3.3.3.cmml" xref="S3.E14.m2.2.2.1.1.3.3.3">lip</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E14.m2.2c">\displaystyle=o_{t}^{(s)}\odot M_{\text{lip}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS0.Px3.p4" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p4.1" class="ltx_p">In the end, to effectively merge these outputs, we add a adaptive module to handle hierarchical audio guidance. Specifically, the output is obtained by the convolution layer:</p>
<table id="S3.E15" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E15.m1.6" class="ltx_Math" alttext="\sum_{u}\text{Conv}_{u}(u),\;u\in\{a_{t}^{(s)},f_{t}^{(s)},l_{t}^{(s)}\}." display="block"><semantics id="S3.E15.m1.6a"><mrow id="S3.E15.m1.6.6.1" xref="S3.E15.m1.6.6.1.1.cmml"><mrow id="S3.E15.m1.6.6.1.1" xref="S3.E15.m1.6.6.1.1.cmml"><mrow id="S3.E15.m1.6.6.1.1.1.1" xref="S3.E15.m1.6.6.1.1.1.2.cmml"><mrow id="S3.E15.m1.6.6.1.1.1.1.1" xref="S3.E15.m1.6.6.1.1.1.1.1.cmml"><munder id="S3.E15.m1.6.6.1.1.1.1.1.1" xref="S3.E15.m1.6.6.1.1.1.1.1.1.cmml"><mo movablelimits="false" id="S3.E15.m1.6.6.1.1.1.1.1.1.2" xref="S3.E15.m1.6.6.1.1.1.1.1.1.2.cmml">∑</mo><mi id="S3.E15.m1.6.6.1.1.1.1.1.1.3" xref="S3.E15.m1.6.6.1.1.1.1.1.1.3.cmml">u</mi></munder><mrow id="S3.E15.m1.6.6.1.1.1.1.1.2" xref="S3.E15.m1.6.6.1.1.1.1.1.2.cmml"><msub id="S3.E15.m1.6.6.1.1.1.1.1.2.2" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.cmml"><mtext id="S3.E15.m1.6.6.1.1.1.1.1.2.2.2" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.2a.cmml">Conv</mtext><mi id="S3.E15.m1.6.6.1.1.1.1.1.2.2.3" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.3.cmml">u</mi></msub><mo lspace="0em" rspace="0em" id="S3.E15.m1.6.6.1.1.1.1.1.2.1" xref="S3.E15.m1.6.6.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E15.m1.6.6.1.1.1.1.1.2.3.2" xref="S3.E15.m1.6.6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E15.m1.6.6.1.1.1.1.1.2.3.2.1" xref="S3.E15.m1.6.6.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E15.m1.4.4" xref="S3.E15.m1.4.4.cmml">u</mi><mo stretchy="false" id="S3.E15.m1.6.6.1.1.1.1.1.2.3.2.2" xref="S3.E15.m1.6.6.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.447em" id="S3.E15.m1.6.6.1.1.1.1.2" xref="S3.E15.m1.6.6.1.1.1.2.cmml">,</mo><mi id="S3.E15.m1.5.5" xref="S3.E15.m1.5.5.cmml">u</mi></mrow><mo id="S3.E15.m1.6.6.1.1.5" xref="S3.E15.m1.6.6.1.1.5.cmml">∈</mo><mrow id="S3.E15.m1.6.6.1.1.4.3" xref="S3.E15.m1.6.6.1.1.4.4.cmml"><mo stretchy="false" id="S3.E15.m1.6.6.1.1.4.3.4" xref="S3.E15.m1.6.6.1.1.4.4.cmml">{</mo><msubsup id="S3.E15.m1.6.6.1.1.2.1.1" xref="S3.E15.m1.6.6.1.1.2.1.1.cmml"><mi id="S3.E15.m1.6.6.1.1.2.1.1.2.2" xref="S3.E15.m1.6.6.1.1.2.1.1.2.2.cmml">a</mi><mi id="S3.E15.m1.6.6.1.1.2.1.1.2.3" xref="S3.E15.m1.6.6.1.1.2.1.1.2.3.cmml">t</mi><mrow id="S3.E15.m1.1.1.1.3" xref="S3.E15.m1.6.6.1.1.2.1.1.cmml"><mo stretchy="false" id="S3.E15.m1.1.1.1.3.1" xref="S3.E15.m1.6.6.1.1.2.1.1.cmml">(</mo><mi id="S3.E15.m1.1.1.1.1" xref="S3.E15.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S3.E15.m1.1.1.1.3.2" xref="S3.E15.m1.6.6.1.1.2.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E15.m1.6.6.1.1.4.3.5" xref="S3.E15.m1.6.6.1.1.4.4.cmml">,</mo><msubsup id="S3.E15.m1.6.6.1.1.3.2.2" xref="S3.E15.m1.6.6.1.1.3.2.2.cmml"><mi id="S3.E15.m1.6.6.1.1.3.2.2.2.2" xref="S3.E15.m1.6.6.1.1.3.2.2.2.2.cmml">f</mi><mi id="S3.E15.m1.6.6.1.1.3.2.2.2.3" xref="S3.E15.m1.6.6.1.1.3.2.2.2.3.cmml">t</mi><mrow id="S3.E15.m1.2.2.1.3" xref="S3.E15.m1.6.6.1.1.3.2.2.cmml"><mo stretchy="false" id="S3.E15.m1.2.2.1.3.1" xref="S3.E15.m1.6.6.1.1.3.2.2.cmml">(</mo><mi id="S3.E15.m1.2.2.1.1" xref="S3.E15.m1.2.2.1.1.cmml">s</mi><mo stretchy="false" id="S3.E15.m1.2.2.1.3.2" xref="S3.E15.m1.6.6.1.1.3.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.E15.m1.6.6.1.1.4.3.6" xref="S3.E15.m1.6.6.1.1.4.4.cmml">,</mo><msubsup id="S3.E15.m1.6.6.1.1.4.3.3" xref="S3.E15.m1.6.6.1.1.4.3.3.cmml"><mi id="S3.E15.m1.6.6.1.1.4.3.3.2.2" xref="S3.E15.m1.6.6.1.1.4.3.3.2.2.cmml">l</mi><mi id="S3.E15.m1.6.6.1.1.4.3.3.2.3" xref="S3.E15.m1.6.6.1.1.4.3.3.2.3.cmml">t</mi><mrow id="S3.E15.m1.3.3.1.3" xref="S3.E15.m1.6.6.1.1.4.3.3.cmml"><mo stretchy="false" id="S3.E15.m1.3.3.1.3.1" xref="S3.E15.m1.6.6.1.1.4.3.3.cmml">(</mo><mi id="S3.E15.m1.3.3.1.1" xref="S3.E15.m1.3.3.1.1.cmml">s</mi><mo stretchy="false" id="S3.E15.m1.3.3.1.3.2" xref="S3.E15.m1.6.6.1.1.4.3.3.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.E15.m1.6.6.1.1.4.3.7" xref="S3.E15.m1.6.6.1.1.4.4.cmml">}</mo></mrow></mrow><mo lspace="0em" id="S3.E15.m1.6.6.1.2" xref="S3.E15.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E15.m1.6b"><apply id="S3.E15.m1.6.6.1.1.cmml" xref="S3.E15.m1.6.6.1"><in id="S3.E15.m1.6.6.1.1.5.cmml" xref="S3.E15.m1.6.6.1.1.5"></in><list id="S3.E15.m1.6.6.1.1.1.2.cmml" xref="S3.E15.m1.6.6.1.1.1.1"><apply id="S3.E15.m1.6.6.1.1.1.1.1.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1"><apply id="S3.E15.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.1">subscript</csymbol><sum id="S3.E15.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.1.2"></sum><ci id="S3.E15.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.1.3">𝑢</ci></apply><apply id="S3.E15.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2"><times id="S3.E15.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.1"></times><apply id="S3.E15.m1.6.6.1.1.1.1.1.2.2.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.1.1.1.2.2.1.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E15.m1.6.6.1.1.1.1.1.2.2.2a.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.2"><mtext id="S3.E15.m1.6.6.1.1.1.1.1.2.2.2.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.2">Conv</mtext></ci><ci id="S3.E15.m1.6.6.1.1.1.1.1.2.2.3.cmml" xref="S3.E15.m1.6.6.1.1.1.1.1.2.2.3">𝑢</ci></apply><ci id="S3.E15.m1.4.4.cmml" xref="S3.E15.m1.4.4">𝑢</ci></apply></apply><ci id="S3.E15.m1.5.5.cmml" xref="S3.E15.m1.5.5">𝑢</ci></list><set id="S3.E15.m1.6.6.1.1.4.4.cmml" xref="S3.E15.m1.6.6.1.1.4.3"><apply id="S3.E15.m1.6.6.1.1.2.1.1.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.2.1.1.1.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1">superscript</csymbol><apply id="S3.E15.m1.6.6.1.1.2.1.1.2.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.2.1.1.2.1.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1">subscript</csymbol><ci id="S3.E15.m1.6.6.1.1.2.1.1.2.2.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1.2.2">𝑎</ci><ci id="S3.E15.m1.6.6.1.1.2.1.1.2.3.cmml" xref="S3.E15.m1.6.6.1.1.2.1.1.2.3">𝑡</ci></apply><ci id="S3.E15.m1.1.1.1.1.cmml" xref="S3.E15.m1.1.1.1.1">𝑠</ci></apply><apply id="S3.E15.m1.6.6.1.1.3.2.2.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.3.2.2.1.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2">superscript</csymbol><apply id="S3.E15.m1.6.6.1.1.3.2.2.2.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.3.2.2.2.1.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2">subscript</csymbol><ci id="S3.E15.m1.6.6.1.1.3.2.2.2.2.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2.2.2">𝑓</ci><ci id="S3.E15.m1.6.6.1.1.3.2.2.2.3.cmml" xref="S3.E15.m1.6.6.1.1.3.2.2.2.3">𝑡</ci></apply><ci id="S3.E15.m1.2.2.1.1.cmml" xref="S3.E15.m1.2.2.1.1">𝑠</ci></apply><apply id="S3.E15.m1.6.6.1.1.4.3.3.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.4.3.3.1.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3">superscript</csymbol><apply id="S3.E15.m1.6.6.1.1.4.3.3.2.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3"><csymbol cd="ambiguous" id="S3.E15.m1.6.6.1.1.4.3.3.2.1.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3">subscript</csymbol><ci id="S3.E15.m1.6.6.1.1.4.3.3.2.2.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3.2.2">𝑙</ci><ci id="S3.E15.m1.6.6.1.1.4.3.3.2.3.cmml" xref="S3.E15.m1.6.6.1.1.4.3.3.2.3">𝑡</ci></apply><ci id="S3.E15.m1.3.3.1.1.cmml" xref="S3.E15.m1.3.3.1.1">𝑠</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E15.m1.6c">\sum_{u}\text{Conv}_{u}(u),\;u\in\{a_{t}^{(s)},f_{t}^{(s)},l_{t}^{(s)}\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Network Architecture</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Finally, we present the network architecture of the diffusion-based generative model with the hierarchical audio-visual cross-attention proposed in this paper.
We employed a standard visual generative model for video generation based on Stable Diffusion, utilizing ReferenceNet to guide the generation process using existing reference images.
Temporal alignment enhances coherence and consistency in the generated video sequence over time, in addition to the hierarchical audio-driven visual synthesis module that establishes refined mappings between audio and lip, expression, and pose, as previously described.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Diffusion Backbone.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">Stable Diffusion 1.5 is built upon a latent diffusion model that consists of three primary components: Vector Quantised Variational AutoEncoder (VQ-VAE), Unet-based denoising model, and a conditioning module.
In the context of text-to-image applications, latent image inputs are generated from random initialization and processed by the diffusion model in collaboration with the conditioning module to generate new latent image outputs.
In this study, text features are excluded from the conditioning in Stable Diffusion, as audio signals are employed as the primary driver for motion.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ReferenceNet.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">ReferenceNet is designed to guide the generation process by referencing existing images to enhance the quality of generated videos, including visual texture information of portraits and backgrounds.
It is a Unet-based Stable Diffusion network with the same number of layers as the denoising model network.
The feature maps generated by these structures at specific layers are likely to exhibit similarities, aiding in the integration of extracted features by ReferenceNet into the diffusion backbone.
These structures generate feature maps at specific layers with the same spatial resolution and potentially similar semantic features.
This integration enhances the quality of generated videos, including visual texture information of portraits and backgrounds.
During model training, the first frame of the video clip serves as the reference image.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Temporal Alignment.</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">In video generation tasks based on diffusion models, temporal alignment plays a crucial role in ensuring the coherence and consistency of the generated video sequences over time.
More precisely, a subset of frames (we use 2 in our implementation) from the preceding inference step are designated as motion frames, which are then concatenated with the latent noise and manipulated along the temporal axis.
This temporal manipulation is facilitated through multiple self-attention blocks, each adept at processing the temporal sequence elements of the video frames.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:229.8pt;">
<img src="/html/2406.08801/assets/fig_tab/data_statistics.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="208" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F4.3" class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer" style="width:173.4pt;height:244.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(139.3pt,-68.4pt) scale(2.26692123358036,2.26692123358036) ;">
<table id="S3.F4.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.F4.3.1.1.1" class="ltx_tr">
<th id="S3.F4.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S3.F4.3.1.1.1.1.1" class="ltx_text">Datasets</span></th>
<th id="S3.F4.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Raw</th>
<th id="S3.F4.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Filtered</th>
</tr>
<tr id="S3.F4.3.1.2.2" class="ltx_tr">
<th id="S3.F4.3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#IDs</th>
<th id="S3.F4.3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#Hours</th>
<th id="S3.F4.3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#IDs</th>
<th id="S3.F4.3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#Hours</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.F4.3.1.3.1" class="ltx_tr">
<td id="S3.F4.3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">HDTF</td>
<td id="S3.F4.3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">190</td>
<td id="S3.F4.3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">8.42</td>
<td id="S3.F4.3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">188</td>
<td id="S3.F4.3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">6.47</td>
</tr>
<tr id="S3.F4.3.1.4.2" class="ltx_tr">
<td id="S3.F4.3.1.4.2.1" class="ltx_td ltx_align_center">Bilibili</td>
<td id="S3.F4.3.1.4.2.2" class="ltx_td ltx_align_center">402</td>
<td id="S3.F4.3.1.4.2.3" class="ltx_td ltx_align_center">18.41</td>
<td id="S3.F4.3.1.4.2.4" class="ltx_td ltx_align_center">311</td>
<td id="S3.F4.3.1.4.2.5" class="ltx_td ltx_align_center">8.66</td>
</tr>
<tr id="S3.F4.3.1.5.3" class="ltx_tr">
<td id="S3.F4.3.1.5.3.1" class="ltx_td ltx_align_center">Youtube</td>
<td id="S3.F4.3.1.5.3.2" class="ltx_td ltx_align_center">1617</td>
<td id="S3.F4.3.1.5.3.3" class="ltx_td ltx_align_center">137.49</td>
<td id="S3.F4.3.1.5.3.4" class="ltx_td ltx_align_center">1324</td>
<td id="S3.F4.3.1.5.3.5" class="ltx_td ltx_align_center">93.73</td>
</tr>
<tr id="S3.F4.3.1.6.4" class="ltx_tr">
<td id="S3.F4.3.1.6.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Total</td>
<td id="S3.F4.3.1.6.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">2209</td>
<td id="S3.F4.3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">164.32</td>
<td id="S3.F4.3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1823</td>
<td id="S3.F4.3.1.6.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">108.86</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.5.2" class="ltx_text" style="font-size:90%;">Statistics of the dataset for training and inference.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training and Inference</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training.</h4>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">The training process consists of two distinct stages:
(1) In the first training phase, individual video frames are generated by utilizing reference image and target video frame pairs.
The parameters of VAE encoder and decoder, along with the facial image encoder, are fixed, while allowing the weights of the spatial cross-attention modules of ReferenceNet and denoising UNet to be optimized to improve single-frame generation capability.
Video clips containing 14 frames are extracted for input data, with a random frame from the facial video clip chosen as the reference frame and another frame from the same video as the target image.
(2) In the second training phase, video sequences are trained using reference images, input audio, and target video data.
The spatial modules of ReferenceNet and denoising UNet remain static, focusing on enhancing video sequence generation capability.
This phase predominantly focuses on training hierarchical audio-visual cross-attention to establish the relationship between audio as motion guidance and the visual information of lip, expression, and pose.
Additionally, motion modules are introduced to improve model temporal coherence and smoothness, initialized with pre-existing weights from AnimateDiff <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
One frame from a video clip is randomly selected as the reference image during this phase.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Inference.</h4>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">During the inference stage, the network takes a single reference image and driving audio as input, producing a video sequence that animates the reference image based on the corresponding audio.
To produce visually consistent long videos, we utilize the last 2 frames of the previous video clip as the initial <math id="S3.SS4.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS4.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p1.1.m1.1c">k</annotation></semantics></math> frames of the next clip, enabling incremental inference for video clip generation.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.5.5" class="ltx_tr">
<th id="S3.T1.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">FVD<math id="S3.T1.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.2.2.2.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-C<math id="S3.T1.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.3.3.3.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S3.T1.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Sync-D<math id="S3.T1.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.4.4.4.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S3.T1.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S3.T1.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.5.5.5.m1.1a"><mo stretchy="false" id="S3.T1.5.5.5.m1.1.1" xref="S3.T1.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.6.1" class="ltx_tr">
<td id="S3.T1.5.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SadTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S3.T1.5.6.1.2" class="ltx_td ltx_align_center ltx_border_t">22.340</td>
<td id="S3.T1.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">203.860</td>
<td id="S3.T1.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">7.885</td>
<td id="S3.T1.5.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.545</td>
<td id="S3.T1.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">9.776</td>
</tr>
<tr id="S3.T1.5.7.2" class="ltx_tr">
<td id="S3.T1.5.7.2.1" class="ltx_td ltx_align_center ltx_border_r">Audio2Head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S3.T1.5.7.2.2" class="ltx_td ltx_align_center">37.776</td>
<td id="S3.T1.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r">239.860</td>
<td id="S3.T1.5.7.2.4" class="ltx_td ltx_align_center"><span id="S3.T1.5.7.2.4.1" class="ltx_text ltx_font_bold">8.024</span></td>
<td id="S3.T1.5.7.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.5.7.2.5.1" class="ltx_text ltx_font_bold">7.145</span></td>
<td id="S3.T1.5.7.2.6" class="ltx_td ltx_align_center">17.103</td>
</tr>
<tr id="S3.T1.5.8.3" class="ltx_tr">
<td id="S3.T1.5.8.3.1" class="ltx_td ltx_align_center ltx_border_r">DreamTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S3.T1.5.8.3.2" class="ltx_td ltx_align_center">78.147</td>
<td id="S3.T1.5.8.3.3" class="ltx_td ltx_align_center ltx_border_r">790.660</td>
<td id="S3.T1.5.8.3.4" class="ltx_td ltx_align_center">6.376</td>
<td id="S3.T1.5.8.3.5" class="ltx_td ltx_align_center ltx_border_r">8.364</td>
<td id="S3.T1.5.8.3.6" class="ltx_td ltx_align_center">15.696</td>
</tr>
<tr id="S3.T1.5.9.4" class="ltx_tr">
<td id="S3.T1.5.9.4.1" class="ltx_td ltx_align_center ltx_border_r">AniPortrait <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S3.T1.5.9.4.2" class="ltx_td ltx_align_center">26.561</td>
<td id="S3.T1.5.9.4.3" class="ltx_td ltx_align_center ltx_border_r">234.666</td>
<td id="S3.T1.5.9.4.4" class="ltx_td ltx_align_center">4.015</td>
<td id="S3.T1.5.9.4.5" class="ltx_td ltx_align_center ltx_border_r">10.548</td>
<td id="S3.T1.5.9.4.6" class="ltx_td ltx_align_center">13.754</td>
</tr>
<tr id="S3.T1.5.10.5" class="ltx_tr">
<td id="S3.T1.5.10.5.1" class="ltx_td ltx_align_center ltx_border_r">Ours</td>
<td id="S3.T1.5.10.5.2" class="ltx_td ltx_align_center"><span id="S3.T1.5.10.5.2.1" class="ltx_text ltx_font_bold">20.545</span></td>
<td id="S3.T1.5.10.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.5.10.5.3.1" class="ltx_text ltx_font_bold">173.497</span></td>
<td id="S3.T1.5.10.5.4" class="ltx_td ltx_align_center">7.750</td>
<td id="S3.T1.5.10.5.5" class="ltx_td ltx_align_center ltx_border_r">7.659</td>
<td id="S3.T1.5.10.5.6" class="ltx_td ltx_align_center"><span id="S3.T1.5.10.5.6.1" class="ltx_text ltx_font_bold">7.951</span></td>
</tr>
<tr id="S3.T1.5.11.6" class="ltx_tr">
<td id="S3.T1.5.11.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Real video</td>
<td id="S3.T1.5.11.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
<td id="S3.T1.5.11.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S3.T1.5.11.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">8.700</td>
<td id="S3.T1.5.11.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.597</td>
<td id="S3.T1.5.11.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.8.2" class="ltx_text" style="font-size:90%;">
The quantitative comparisons with the existed portrait image animation approaches on the HDTF data-set.
Our proposed method excels in generating high-quality, temporally coherent talking head animations with superior lip synchronization performance.
</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/open_methods.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison with existing approaches on HDTF data-set.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setups</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation Details.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Experiments encompassing both training and inference were carried out on a computational platform featuring 8 NVIDIA A100 GPUs.
The initial and subsequent stages comprised 30,000 training steps each, with a batch size of 4 and video dimensions set at <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">512</cn><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">512\times 512</annotation></semantics></math>.
Each training instance in the second stage produced 14 video frames, with the motion module’s latents concatenated with the first 2 ground truth frames for video continuity.
In both training stages, a learning rate of 1e-5 was employed, and the motion module was initialized with weights from Animatediff.
To enhance video generation, the reference image, guidance audio, and motion frames were dropped with a 0.05 probability during training.
In inference, continuity across sequences was ensured by concatenating noisy latents with feature maps of the last 2 motion frames from the previous step within the motion module.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Temporal Alignment. ‣ 3.3 Network Architecture ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our dataset comprises HDTF (190 clips, 8.42 hours) and additional Internet-sourced data (2019 clips, 155.90 hours).
As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Temporal Alignment. ‣ 3.3 Network Architecture ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, these videos are consisted of individuals of diverse ages, ethnicities, and genders in half-body or close-up shots against varied indoor and outdoor backgrounds.
To ensure high-quality training data, we underwent a data cleaning process that focused on retaining single-person speaking videos exhibiting strong lip and audio consistency, while excluding videos with scene changes, significant camera movements, excessive facial motion, and fully side-facing shots.
Mediapipe was utilized to determine expression and lip activity ranges in training videos, forging the basis for constructing expression and lip masks employed in both training and inference.
Following data cleaning, our refined training dataset includes HDTF (188 clips, 6.47 hours) and Internet-sourced data (1635 clips, 102.39 hours), with each training video clip comprising 15 frames at a resolution of <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">512</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">512\times 512</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Metrics.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The evaluation metrics utilized in the portrait image animation approach include Fréchet Inception Distance (FID), Fréchet Video Distance (FVD), Synchronization-C (Sync-C), Synchronization-D (Sync-D) and E-FID.
Specifically, FID and FVD measure the similarity between generated images and real data, with lower values indicating better performance and thus more realistic outputs.
Sync-C and Sync-D evaluate the lip synchronization of generated videos in terms of content and dynamics, with higher Sync-C and lower Sync-D scores denoting better alignment with the audio.
E-FID assesses the quality of generated images based on the features extracted from the Inception network, offering a refined evaluation of fidelity.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.5" class="ltx_tr">
<th id="S4.T2.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">FID<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">FVD<math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Sync-C<math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Sync-D<math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S4.T2.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.5.5.5.m1.1a"><mo stretchy="false" id="S4.T2.5.5.5.m1.1.1" xref="S4.T2.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.5.6.1" class="ltx_tr">
<td id="S4.T2.5.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SadTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S4.T2.5.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.015</td>
<td id="S4.T2.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">471.163</td>
<td id="S4.T2.5.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.922</td>
<td id="S4.T2.5.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.5.6.1.5.1" class="ltx_text ltx_font_bold">7.921</span></td>
<td id="S4.T2.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">95.194</td>
</tr>
<tr id="S4.T2.5.7.2" class="ltx_tr">
<td id="S4.T2.5.7.2.1" class="ltx_td ltx_align_center ltx_border_r">Audio2Head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S4.T2.5.7.2.2" class="ltx_td ltx_align_center ltx_border_r">84.793</td>
<td id="S4.T2.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r">457.499</td>
<td id="S4.T2.5.7.2.4" class="ltx_td ltx_align_center ltx_border_r">6.518</td>
<td id="S4.T2.5.7.2.5" class="ltx_td ltx_align_center ltx_border_r">8.143</td>
<td id="S4.T2.5.7.2.6" class="ltx_td ltx_align_center">153.618</td>
</tr>
<tr id="S4.T2.5.8.3" class="ltx_tr">
<td id="S4.T2.5.8.3.1" class="ltx_td ltx_align_center ltx_border_r">DreamTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.T2.5.8.3.2" class="ltx_td ltx_align_center ltx_border_r">109.011</td>
<td id="S4.T2.5.8.3.3" class="ltx_td ltx_align_center ltx_border_r">988.539</td>
<td id="S4.T2.5.8.3.4" class="ltx_td ltx_align_center ltx_border_r">5.709</td>
<td id="S4.T2.5.8.3.5" class="ltx_td ltx_align_center ltx_border_r">8.743</td>
<td id="S4.T2.5.8.3.6" class="ltx_td ltx_align_center">153.450</td>
</tr>
<tr id="S4.T2.5.9.4" class="ltx_tr">
<td id="S4.T2.5.9.4.1" class="ltx_td ltx_align_center ltx_border_r">AniPortrait <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S4.T2.5.9.4.2" class="ltx_td ltx_align_center ltx_border_r">46.915</td>
<td id="S4.T2.5.9.4.3" class="ltx_td ltx_align_center ltx_border_r">477.179</td>
<td id="S4.T2.5.9.4.4" class="ltx_td ltx_align_center ltx_border_r">2.853</td>
<td id="S4.T2.5.9.4.5" class="ltx_td ltx_align_center ltx_border_r">11.709</td>
<td id="S4.T2.5.9.4.6" class="ltx_td ltx_align_center">88.986</td>
</tr>
<tr id="S4.T2.5.10.5" class="ltx_tr">
<td id="S4.T2.5.10.5.1" class="ltx_td ltx_align_center ltx_border_r">Ours</td>
<td id="S4.T2.5.10.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.5.10.5.2.1" class="ltx_text ltx_font_bold">44.578</span></td>
<td id="S4.T2.5.10.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.5.10.5.3.1" class="ltx_text ltx_font_bold">377.117</span></td>
<td id="S4.T2.5.10.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.5.10.5.4.1" class="ltx_text ltx_font_bold">7.191</span></td>
<td id="S4.T2.5.10.5.5" class="ltx_td ltx_align_center ltx_border_r">7.984</td>
<td id="S4.T2.5.10.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.5.10.5.6.1" class="ltx_text ltx_font_bold">78.495</span></td>
</tr>
<tr id="S4.T2.5.11.6" class="ltx_tr">
<td id="S4.T2.5.11.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Real video</td>
<td id="S4.T2.5.11.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.5.11.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.5.11.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7.372</td>
<td id="S4.T2.5.11.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7.518</td>
<td id="S4.T2.5.11.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.7.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.8.2" class="ltx_text" style="font-size:90%;">The quantitative comparisons with the existing portrait image animation approaches on the CelebV data-set.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/comparison_celebv.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison with existing approaches on CelebV data-set.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baseline.</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">In our quantitative experiments, we conducted a comparative analysis of our proposed method against publicly available implementations of SadTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, DreamTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Audio2Head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and AniPortrait <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
The evaluation was performed on the HDTF, CelebV and the proposed dataset, utilizing a training and testing split where 90% of the identity data was allocated for training purposes.
The qualitative comparison encompassed an evaluation of our method against these selected approaches, taking into consideration reference images, audio inputs, and the resultant animated outputs provided by each respective method.
This qualitative assessment aimed to provide insights into the performance and capabilities of our method in generating realistic and expressive talking head animations.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.5" class="ltx_tr">
<th id="S4.T3.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID<math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">FVD<math id="S4.T3.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-C<math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Sync-D<math id="S4.T3.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.4.4.4.m1.1a"><mo stretchy="false" id="S4.T3.4.4.4.m1.1.1" xref="S4.T3.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S4.T3.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.5.5.5.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.m1.1.1" xref="S4.T3.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.6.1" class="ltx_tr">
<td id="S4.T3.5.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SadTalker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S4.T3.5.6.1.2" class="ltx_td ltx_align_center ltx_border_t">24.212</td>
<td id="S4.T3.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">249.786</td>
<td id="S4.T3.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">6.613</td>
<td id="S4.T3.5.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.099</td>
<td id="S4.T3.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">37.324</td>
</tr>
<tr id="S4.T3.5.7.2" class="ltx_tr">
<td id="S4.T3.5.7.2.1" class="ltx_td ltx_align_center ltx_border_r">Audio2Head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S4.T3.5.7.2.2" class="ltx_td ltx_align_center">61.510</td>
<td id="S4.T3.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r">383.178</td>
<td id="S4.T3.5.7.2.4" class="ltx_td ltx_align_center">5.719</td>
<td id="S4.T3.5.7.2.5" class="ltx_td ltx_align_center ltx_border_r">8.585</td>
<td id="S4.T3.5.7.2.6" class="ltx_td ltx_align_center">66.116</td>
</tr>
<tr id="S4.T3.5.8.3" class="ltx_tr">
<td id="S4.T3.5.8.3.1" class="ltx_td ltx_align_center ltx_border_r">DreamTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.T3.5.8.3.2" class="ltx_td ltx_align_center">128.423</td>
<td id="S4.T3.5.8.3.3" class="ltx_td ltx_align_center ltx_border_r">964.088</td>
<td id="S4.T3.5.8.3.4" class="ltx_td ltx_align_center">5.925</td>
<td id="S4.T3.5.8.3.5" class="ltx_td ltx_align_center ltx_border_r">8.596</td>
<td id="S4.T3.5.8.3.6" class="ltx_td ltx_align_center">58.180</td>
</tr>
<tr id="S4.T3.5.9.4" class="ltx_tr">
<td id="S4.T3.5.9.4.1" class="ltx_td ltx_align_center ltx_border_r">AniPortrait <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S4.T3.5.9.4.2" class="ltx_td ltx_align_center">24.118</td>
<td id="S4.T3.5.9.4.3" class="ltx_td ltx_align_center ltx_border_r">250.770</td>
<td id="S4.T3.5.9.4.4" class="ltx_td ltx_align_center">3.043</td>
<td id="S4.T3.5.9.4.5" class="ltx_td ltx_align_center ltx_border_r">10.997</td>
<td id="S4.T3.5.9.4.6" class="ltx_td ltx_align_center">37.806</td>
</tr>
<tr id="S4.T3.5.10.5" class="ltx_tr">
<td id="S4.T3.5.10.5.1" class="ltx_td ltx_align_center ltx_border_r">Ours</td>
<td id="S4.T3.5.10.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.5.10.5.2.1" class="ltx_text ltx_font_bold">23.266</span></td>
<td id="S4.T3.5.10.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.10.5.3.1" class="ltx_text ltx_font_bold">239.647</span></td>
<td id="S4.T3.5.10.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.10.5.4.1" class="ltx_text ltx_font_bold">6.924</span></td>
<td id="S4.T3.5.10.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.10.5.5.1" class="ltx_text ltx_font_bold">7.969</span></td>
<td id="S4.T3.5.10.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.5.10.5.6.1" class="ltx_text ltx_font_bold">34.731</span></td>
</tr>
<tr id="S4.T3.5.11.6" class="ltx_tr">
<td id="S4.T3.5.11.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Real video</td>
<td id="S4.T3.5.11.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
<td id="S4.T3.5.11.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T3.5.11.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">7.011</td>
<td id="S4.T3.5.11.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7.606</td>
<td id="S4.T3.5.11.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.7.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.8.2" class="ltx_text" style="font-size:90%;">The quantitative comparisons with the existed portrait image animation approaches on the proposed “wild” data-set.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/portrait_styles.jpg" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="565" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Video generation results of the proposed approach given different portrait styles.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Results</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison on HDTF Dataset.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ Inference. ‣ 3.4 Training and Inference ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comprehensive quantitative evaluation of various portrait image animation techniques on the HDTF dataset.
Our proposed method demonstrates superior performance across several metrics, notably achieving the lowest FID (20.545), FVD (173.497), and E-FID (7.951).
These results underscore the high quality and temporal coherence of the generated talking head animations.
Additionally, our method exhibits commendable lip synchronization capabilities, as evidenced by the Sync-C (7.750) and Sync-D (7.659) metrics, which closely align with real video benchmarks.
These achievements highlight the effectiveness of our approach in enhancing lip synchronization while maintaining high-fidelity visual generation and temporal consistency.
To supplement these findings, Figure <a href="#S3.F5" title="Figure 5 ‣ Inference. ‣ 3.4 Training and Inference ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> visualizes the comparative performance of the different methods.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/audio_style.jpg" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Video generation results of the proposed approach given different audio styles.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2406.08801/assets/x1.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison of head and expression motion diversity between existing methodologies and our proposed approach.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison on CelebV Dataset.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">The quantitative evaluations presented in Table <a href="#S4.T2" title="Table 2 ‣ Evaluation Metrics. ‣ 4.1 Experimental Setups ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provide a comparative analysis of various portrait image animation techniques using the CelebV dataset.
Our proposed approach achieves superior performance, demonstrating the lowest FID (44.578) and FVD (377.117) scores, as well as the highest Sync-C score (7.191).
Additionally, our method secures a competitive Sync-D value (7.984) and the lowest E-FID score (78.495), indicating high-quality animations with notable temporal coherence and precise lip synchronization.
These results attest to the robustness and efficacy of our technique in generating realistic animations. To offer a more comprehensive analysis, a visual comparative examination is presented in Figure <a href="#S4.F6" title="Figure 6 ‣ Evaluation Metrics. ‣ 4.1 Experimental Setups ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison on the Proposed “Wild” Dataset.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">The quantitative evaluations depicted in Table <a href="#S4.T3" title="Table 3 ‣ Baseline. ‣ 4.1 Experimental Setups ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> offer a comparative analysis of various portrait image animation techniques on the proposed “wild” dataset (as shown in statistics of Figure <a href="#S3.F4" title="Figure 4 ‣ Temporal Alignment. ‣ 3.3 Network Architecture ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> except for HDTF).
Our methodology demonstrates outstanding performance, achieving the lowest FID (23.266) and FVD (239.647) scores, as well as the highest Sync-C score (6.924).
Additionally, our approach attains the lowest E-FID score (34.731) and a competitive Sync-D value (7.969), closely approximating benchmarks set by real video data.
These results highlight the robustness and efficacy of our technique in generating high-quality animations characterized by temporal coherence and precise lip synchronization under diverse and challenging conditions.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/id.jpg" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.3.2" class="ltx_text" style="font-size:90%;">Visualization of identity fine-tuning personalization.
Enhancing identity-specific expression and pose features through personalized data fine-tuning facilitates the generation of animations closely resembling the targeted identity.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Results</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different Portrait Styles.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">The study depicted in Figures <a href="#S4.F7" title="Figure 7 ‣ Baseline. ‣ 4.1 Experimental Setups ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> delves into the impact of various portrait styles, including but not limited to sketching, painting, AI generated images, sculpture, and more, on video synthesis using the proposed methodology.
The results underscore the versatility and robustness of the approach in generating a diverse range of visual and auditory outputs.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different Audio Styles.</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">Figure <a href="#S4.F8" title="Figure 8 ‣ Comparison on HDTF Dataset. ‣ 4.2 Quantitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> presents qualitative outcomes concerning distinct audio styles.
The findings demonstrate that our method can efficiently handle a range of audio inputs to generate high-fidelity and visually coherent videos that align seamlessly with the audio content.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Audio-Visual Cross Attention.</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ Cross Attention as Motion Guidance. ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the comparison of audio-visual cross attention between the original full and the proposed hierarchical audio-visual cross attention.
It is observed that the proposed hierarchical audio-visual cross attention effectively achieves fine-grained and precise alignment between audio, lip motion, and facial expressions.
This alignment enhances the relevance during training and enables more precise motion control based on audio in the inference process.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Head and Expression Motion Diversity.</h4>

<div id="S4.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px4.p1.1" class="ltx_p">Figure <a href="#S4.F9" title="Figure 9 ‣ Comparison on HDTF Dataset. ‣ 4.2 Quantitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> presents a qualitative comparison of head and expression motion diversity between existing methodologies and our proposed approach.
The analysis demonstrates that our method excels in generating animations characterized by enhanced diversity in expression and pose motion.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Identity Fine-tuning Personalization.</h4>

<div id="S4.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px5.p1.1" class="ltx_p">Figure <a href="#S4.F10" title="Figure 10 ‣ Comparison on the Proposed “Wild” Dataset. ‣ 4.2 Quantitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> presents the outcomes of fine-tuning diverse identities using personalized data specific to each identity.
The analysis reveals that the proposed hierarchical audio-driven visual synthesis adeptly captures the distinctive features of individual identities, leading to personalized animations that closely reflect the unique traits of each identity following the fine-tuning procedure.</p>
</div>
<figure id="S4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2406.08801/assets/x2.png" id="S4.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F11.1.2.2" class="ltx_text" style="font-size:90%;">Expression control by adjusting hierarchical weights.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2406.08801/assets/x3.png" id="S4.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F11.2.2.2" class="ltx_text" style="font-size:90%;">Qualitative comparison with applying higher coefficient on expression.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2406.08801/assets/x4.png" id="S4.F11.3.g1" class="ltx_graphics ltx_img_landscape" width="415" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F11.3.2.2" class="ltx_text" style="font-size:90%;">Qualitative comparison with applying higher coefficient on pose.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.5.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.6.2" class="ltx_text" style="font-size:90%;">Motion control (pose, expression and lip) by adjusting hierarchical weights.
he statistical analysis of these dynamics is conducted exclusively on the HDTF dataset to ensure consistency in the evaluation framework.</span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.5" class="ltx_tr">
<th id="S4.T4.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Full</th>
<th id="S4.T4.5.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Lip</th>
<th id="S4.T4.5.5.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Exp.</th>
<th id="S4.T4.5.5.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Pose</th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID<math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FVD<math id="S4.T4.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.2.2.2.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-C<math id="S4.T4.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.3.3.3.m1.1a"><mo stretchy="false" id="S4.T4.3.3.3.m1.1.1" xref="S4.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-D<math id="S4.T4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S4.T4.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.5.5.5.m1.1a"><mo stretchy="false" id="S4.T4.5.5.5.m1.1.1" xref="S4.T4.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.6.6" class="ltx_tr">
<th id="S4.T4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><math id="S4.T4.6.6.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.6.6.1.m1.1a"><mi mathvariant="normal" id="S4.T4.6.6.1.m1.1.1" xref="S4.T4.6.6.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.1.m1.1b"><ci id="S4.T4.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.1.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.6.6.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T4.6.6.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T4.6.6.4" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T4.6.6.5" class="ltx_td ltx_align_center ltx_border_t">20.581</td>
<td id="S4.T4.6.6.6" class="ltx_td ltx_align_center ltx_border_t">193.062</td>
<td id="S4.T4.6.6.7" class="ltx_td ltx_align_center ltx_border_t">6.499</td>
<td id="S4.T4.6.6.8" class="ltx_td ltx_align_center ltx_border_t">8.691</td>
<td id="S4.T4.6.6.9" class="ltx_td ltx_align_center ltx_border_t">9.133</td>
</tr>
<tr id="S4.T4.8.8" class="ltx_tr">
<th id="S4.T4.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T4.7.7.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.7.7.1.m1.1a"><mi mathvariant="normal" id="S4.T4.7.7.1.m1.1.1" xref="S4.T4.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.1.m1.1b"><ci id="S4.T4.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.1.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T4.8.8.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.8.8.2.m1.1a"><mi mathvariant="normal" id="S4.T4.8.8.2.m1.1.1" xref="S4.T4.8.8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.2.m1.1b"><ci id="S4.T4.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.2.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.8.8.3" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.8.8.4" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T4.8.8.5" class="ltx_td ltx_align_center">24.605</td>
<td id="S4.T4.8.8.6" class="ltx_td ltx_align_center">217.417</td>
<td id="S4.T4.8.8.7" class="ltx_td ltx_align_center">7.187</td>
<td id="S4.T4.8.8.8" class="ltx_td ltx_align_center">8.002</td>
<td id="S4.T4.8.8.9" class="ltx_td ltx_align_center">8.334</td>
</tr>
<tr id="S4.T4.10.10" class="ltx_tr">
<th id="S4.T4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T4.9.9.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.9.9.1.m1.1a"><mi mathvariant="normal" id="S4.T4.9.9.1.m1.1.1" xref="S4.T4.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.1.m1.1b"><ci id="S4.T4.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.1.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.10.10.3" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T4.10.10.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.10.10.2.m1.1a"><mi mathvariant="normal" id="S4.T4.10.10.2.m1.1.1" xref="S4.T4.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.2.m1.1b"><ci id="S4.T4.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.2.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.10.10.4" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T4.10.10.5" class="ltx_td ltx_align_center">24.003</td>
<td id="S4.T4.10.10.6" class="ltx_td ltx_align_center">207.352</td>
<td id="S4.T4.10.10.7" class="ltx_td ltx_align_center">7.072</td>
<td id="S4.T4.10.10.8" class="ltx_td ltx_align_center">8.127</td>
<td id="S4.T4.10.10.9" class="ltx_td ltx_align_center">8.282</td>
</tr>
<tr id="S4.T4.12.12" class="ltx_tr">
<th id="S4.T4.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><math id="S4.T4.11.11.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.11.11.1.m1.1a"><mi mathvariant="normal" id="S4.T4.11.11.1.m1.1.1" xref="S4.T4.11.11.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.1.m1.1b"><ci id="S4.T4.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.1.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.12.12.3" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.12.12.4" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T4.12.12.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.12.12.2.m1.1a"><mi mathvariant="normal" id="S4.T4.12.12.2.m1.1.1" xref="S4.T4.12.12.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.2.m1.1b"><ci id="S4.T4.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.2.m1.1c">\checkmark</annotation></semantics></math></th>
<td id="S4.T4.12.12.5" class="ltx_td ltx_align_center">23.452</td>
<td id="S4.T4.12.12.6" class="ltx_td ltx_align_center">205.636</td>
<td id="S4.T4.12.12.7" class="ltx_td ltx_align_center">6.436</td>
<td id="S4.T4.12.12.8" class="ltx_td ltx_align_center">8.502</td>
<td id="S4.T4.12.12.9" class="ltx_td ltx_align_center">8.375</td>
</tr>
<tr id="S4.T4.15.15" class="ltx_tr">
<th id="S4.T4.15.15.4" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_t"></th>
<th id="S4.T4.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t"><math id="S4.T4.13.13.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.13.13.1.m1.1a"><mi mathvariant="normal" id="S4.T4.13.13.1.m1.1.1" xref="S4.T4.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.1.m1.1b"><ci id="S4.T4.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.1.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t"><math id="S4.T4.14.14.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.14.14.2.m1.1a"><mi mathvariant="normal" id="S4.T4.14.14.2.m1.1.1" xref="S4.T4.14.14.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.2.m1.1b"><ci id="S4.T4.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.2.m1.1c">\checkmark</annotation></semantics></math></th>
<th id="S4.T4.15.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T4.15.15.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T4.15.15.3.m1.1a"><mi mathvariant="normal" id="S4.T4.15.15.3.m1.1.1" xref="S4.T4.15.15.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.3.m1.1b"><ci id="S4.T4.15.15.3.m1.1.1.cmml" xref="S4.T4.15.15.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.3.m1.1c">\checkmark</annotation></semantics></math></th>
<td id="S4.T4.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.15.15.5.1" class="ltx_text ltx_font_bold">20.545</span></td>
<td id="S4.T4.15.15.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.15.15.6.1" class="ltx_text ltx_font_bold">173.497</span></td>
<td id="S4.T4.15.15.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.15.15.7.1" class="ltx_text ltx_font_bold">7.750</span></td>
<td id="S4.T4.15.15.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.15.15.8.1" class="ltx_text ltx_font_bold">7.659</span></td>
<td id="S4.T4.15.15.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.15.15.9.1" class="ltx_text ltx_font_bold">7.951</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.17.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.18.2" class="ltx_text" style="font-size:90%;">Ablation study of hierarchical audio-visual (lip, expression, and pose) cross attention.
The designation “Full” denotes the baseline configuration of full visual-audio cross attention.
Subsequently, the study incrementally incorporates regional cross attention between audio and visual modalities, introducing lip, expression, and pose features individually. In our experimental setup, we adhere to the configuration specified in the last row of the table.
</span></figcaption>
</figure>
<figure id="S4.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F12.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2406.08801/assets/fig_tab/hierc_qual_compare.jpg" id="S4.F12.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="104" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F12.1.2.2" class="ltx_text" style="font-size:90%;">Qualitative study of hierarchical audio-visual (lip, expression, and pose) cross attention.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F12.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2406.08801/assets/fig_tab/att_weight.jpg" id="S4.F12.2.g1" class="ltx_graphics ltx_img_landscape" width="538" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F12.2.2.2" class="ltx_text" style="font-size:90%;">Comparison of different attention weighting mechanisms in hierarchical audio-driven visual synthesis.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.4.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S4.F12.5.2" class="ltx_text" style="font-size:90%;">Qualitative ablation study of (1) hierarchical audio-visual (lip, expression, and pose) cross attention; (2) different attention weighting mechanism in hierarchical audio-driven visual synthesis.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Hierarchical Audio-Visual Cross Attention.</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">The ablation study presented in Table <a href="#S4.T4" title="Table 4 ‣ Identity Fine-tuning Personalization. ‣ 4.3 Qualitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> meticulously examines the impact of hierarchical audio-visual cross attention by systematically integrating lip, expression, and pose features.
Analysis of the results reveals that the incorporation of lip features in isolation leads to enhancements in specific metrics, albeit with varying degrees of improvement across evaluation criteria.
Furthermore, the combined inclusion of facial features with lip features yields mixed results but notably enhances synchronization confidence and reduces E-FID. Notably, the incorporation of all three modalities—lip, expression, and pose—exhibits the most substantial overall performance improvement, elevating the quality and coherence of audio-visual synthesis.
This iterative refinement underscores the efficacy of hierarchical cross-attention across multiple features in achieving enhanced audio-visual integration.
Figure <a href="#S4.F12" title="Figure 12 ‣ Identity Fine-tuning Personalization. ‣ 4.3 Qualitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>(a) complements this assessment with further qualitative insights into hierarchical audio-visual (lip, expression, and pose) cross attention.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different Attention Weighting Mechanism.</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ Different Attention Weighting Mechanism. ‣ 4.4 Ablation Study ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a quantitative assessment of various attention weighting mechanisms utilized in the hierarchical audio-driven visual synthesis module.
Among the mechanisms examined, the “Direct addition” approach stands out with the lowest FID score of 19.580, indicating exceptional image quality.
Remarkably, our “Zero convolution” strategy surpasses all others, demonstrating superior performance across critical metrics.
Specifically, it achieves the highest FVD (173.497), Sync-C (7.750), Sync-D (7.659), and E-FID (7.951) scores, showcasing its robustness in generating synchronized, temporally coherent animations with utmost fidelity.
Figure <a href="#S4.F12" title="Figure 12 ‣ Identity Fine-tuning Personalization. ‣ 4.3 Qualitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>(b) complements this analysis by providing additional qualitative comparisons of different attention weighting mechanisms in hierarchical audio-driven visual synthesis.</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2406.08801/assets/fig_tab/CFG-comparison.jpg" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="392" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S4.F13.3.2" class="ltx_text" style="font-size:90%;">Qualitative study of audio and image CFG scales.</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.5.5" class="ltx_tr">
<th id="S4.T5.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Mechanism</th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">FVD<math id="S4.T5.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.2.2.2.m1.1a"><mo stretchy="false" id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-C<math id="S4.T5.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.3.3.3.m1.1a"><mo stretchy="false" id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T5.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Sync-D<math id="S4.T5.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.4.4.4.m1.1a"><mo stretchy="false" id="S4.T5.4.4.4.m1.1.1" xref="S4.T5.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.m1.1b"><ci id="S4.T5.4.4.4.m1.1.1.cmml" xref="S4.T5.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S4.T5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T5.5.5.5.m1.1.1" xref="S4.T5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.m1.1b"><ci id="S4.T5.5.5.5.m1.1.1.cmml" xref="S4.T5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.5.6.1" class="ltx_tr">
<td id="S4.T5.5.6.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Direct addition</td>
<td id="S4.T5.5.6.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.5.6.1.2.1" class="ltx_text ltx_font_bold">19.580</span></td>
<td id="S4.T5.5.6.1.3" class="ltx_td ltx_align_left ltx_border_t">177.715</td>
<td id="S4.T5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">7.087</td>
<td id="S4.T5.5.6.1.5" class="ltx_td ltx_align_left ltx_border_t">8.147</td>
<td id="S4.T5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">8.063</td>
</tr>
<tr id="S4.T5.5.7.2" class="ltx_tr">
<td id="S4.T5.5.7.2.1" class="ltx_td ltx_align_center ltx_border_r">Self attention</td>
<td id="S4.T5.5.7.2.2" class="ltx_td ltx_align_center">20.984</td>
<td id="S4.T5.5.7.2.3" class="ltx_td ltx_align_left">183.113</td>
<td id="S4.T5.5.7.2.4" class="ltx_td ltx_align_center">7.568</td>
<td id="S4.T5.5.7.2.5" class="ltx_td ltx_align_left">7.947</td>
<td id="S4.T5.5.7.2.6" class="ltx_td ltx_align_center">8.292</td>
</tr>
<tr id="S4.T5.5.8.3" class="ltx_tr">
<td id="S4.T5.5.8.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Zero convolution (ours)</td>
<td id="S4.T5.5.8.3.2" class="ltx_td ltx_align_center ltx_border_b">20.545</td>
<td id="S4.T5.5.8.3.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T5.5.8.3.3.1" class="ltx_text ltx_font_bold">173.497</span></td>
<td id="S4.T5.5.8.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.5.8.3.4.1" class="ltx_text ltx_font_bold">7.750</span></td>
<td id="S4.T5.5.8.3.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T5.5.8.3.5.1" class="ltx_text ltx_font_bold">7.659</span></td>
<td id="S4.T5.5.8.3.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.5.8.3.6.1" class="ltx_text ltx_font_bold">7.951</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.7.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.8.2" class="ltx_text" style="font-size:90%;">Quantitative comparison of different attention weighting mechanism in the module of hierarchical audio-driven visual synthesis.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Expression Control by Adjusting Hierarchical Weights.</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S4.F11" title="Figure 11 ‣ Identity Fine-tuning Personalization. ‣ 4.3 Qualitative Results ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> illustrates the qualitative outcomes of expression control achieved through the manipulation of hierarchical weights.
The evaluation of various synthesis weight settings reveals a discernible trend: augmenting the lip weight leads to improved lip-sync accuracy while slightly compromising overall fidelity and video quality.
In contrast, elevating the expression weight marginally enhances image and video quality but may have a slight adverse effect on synchronization metrics.
Consequently, the selection of weight settings necessitates a judicious balance between fidelity, video quality, and synchronization, guided by the specific requirements of the application.
For this study, we adhere to the default setting determined through the adaptive weighting process.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.5.5" class="ltx_tr">
<th id="S4.T6.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">audio</th>
<th id="S4.T6.5.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">image</th>
<th id="S4.T6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID<math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mo stretchy="false" id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FVD<math id="S4.T6.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.2.2.2.m1.1a"><mo stretchy="false" id="S4.T6.2.2.2.m1.1.1" xref="S4.T6.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T6.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-C<math id="S4.T6.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.3.3.3.m1.1a"><mo stretchy="false" id="S4.T6.3.3.3.m1.1.1" xref="S4.T6.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.m1.1b"><ci id="S4.T6.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T6.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Sync-D<math id="S4.T6.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.4.4.4.m1.1a"><mo stretchy="false" id="S4.T6.4.4.4.m1.1.1" xref="S4.T6.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.m1.1b"><ci id="S4.T6.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T6.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">E-FID<math id="S4.T6.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.5.5.5.m1.1a"><mo stretchy="false" id="S4.T6.5.5.5.m1.1.1" xref="S4.T6.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.m1.1b"><ci id="S4.T6.5.5.5.m1.1.1.cmml" xref="S4.T6.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.7.7" class="ltx_tr">
<th id="S4.T6.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<math id="S4.T6.6.6.1.m1.1" class="ltx_Math" alttext="\lambda_{a}" display="inline"><semantics id="S4.T6.6.6.1.m1.1a"><msub id="S4.T6.6.6.1.m1.1.1" xref="S4.T6.6.6.1.m1.1.1.cmml"><mi id="S4.T6.6.6.1.m1.1.1.2" xref="S4.T6.6.6.1.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.6.6.1.m1.1.1.3" xref="S4.T6.6.6.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.1.m1.1b"><apply id="S4.T6.6.6.1.m1.1.1.cmml" xref="S4.T6.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.6.6.1.m1.1.1.1.cmml" xref="S4.T6.6.6.1.m1.1.1">subscript</csymbol><ci id="S4.T6.6.6.1.m1.1.1.2.cmml" xref="S4.T6.6.6.1.m1.1.1.2">𝜆</ci><ci id="S4.T6.6.6.1.m1.1.1.3.cmml" xref="S4.T6.6.6.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.1.m1.1c">\lambda_{a}</annotation></semantics></math> = 1.0</th>
<th id="S4.T6.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<math id="S4.T6.7.7.2.m1.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.T6.7.7.2.m1.1a"><msub id="S4.T6.7.7.2.m1.1.1" xref="S4.T6.7.7.2.m1.1.1.cmml"><mi id="S4.T6.7.7.2.m1.1.1.2" xref="S4.T6.7.7.2.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.7.7.2.m1.1.1.3" xref="S4.T6.7.7.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.7.7.2.m1.1b"><apply id="S4.T6.7.7.2.m1.1.1.cmml" xref="S4.T6.7.7.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.7.7.2.m1.1.1.1.cmml" xref="S4.T6.7.7.2.m1.1.1">subscript</csymbol><ci id="S4.T6.7.7.2.m1.1.1.2.cmml" xref="S4.T6.7.7.2.m1.1.1.2">𝜆</ci><ci id="S4.T6.7.7.2.m1.1.1.3.cmml" xref="S4.T6.7.7.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.7.2.m1.1c">\lambda_{i}</annotation></semantics></math> = 1.0</th>
<td id="S4.T6.7.7.3" class="ltx_td ltx_align_center ltx_border_t">33.320</td>
<td id="S4.T6.7.7.4" class="ltx_td ltx_align_center ltx_border_t">270.925</td>
<td id="S4.T6.7.7.5" class="ltx_td ltx_align_center ltx_border_t">5.983</td>
<td id="S4.T6.7.7.6" class="ltx_td ltx_align_center ltx_border_t">9.532</td>
<td id="S4.T6.7.7.7" class="ltx_td ltx_align_center ltx_border_t">12.579</td>
</tr>
<tr id="S4.T6.9.9" class="ltx_tr">
<th id="S4.T6.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<math id="S4.T6.8.8.1.m1.1" class="ltx_Math" alttext="\lambda_{a}" display="inline"><semantics id="S4.T6.8.8.1.m1.1a"><msub id="S4.T6.8.8.1.m1.1.1" xref="S4.T6.8.8.1.m1.1.1.cmml"><mi id="S4.T6.8.8.1.m1.1.1.2" xref="S4.T6.8.8.1.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.8.8.1.m1.1.1.3" xref="S4.T6.8.8.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.8.8.1.m1.1b"><apply id="S4.T6.8.8.1.m1.1.1.cmml" xref="S4.T6.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.8.8.1.m1.1.1.1.cmml" xref="S4.T6.8.8.1.m1.1.1">subscript</csymbol><ci id="S4.T6.8.8.1.m1.1.1.2.cmml" xref="S4.T6.8.8.1.m1.1.1.2">𝜆</ci><ci id="S4.T6.8.8.1.m1.1.1.3.cmml" xref="S4.T6.8.8.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.8.1.m1.1c">\lambda_{a}</annotation></semantics></math> = 1.0</th>
<th id="S4.T6.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<math id="S4.T6.9.9.2.m1.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.T6.9.9.2.m1.1a"><msub id="S4.T6.9.9.2.m1.1.1" xref="S4.T6.9.9.2.m1.1.1.cmml"><mi id="S4.T6.9.9.2.m1.1.1.2" xref="S4.T6.9.9.2.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.9.9.2.m1.1.1.3" xref="S4.T6.9.9.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.9.9.2.m1.1b"><apply id="S4.T6.9.9.2.m1.1.1.cmml" xref="S4.T6.9.9.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.9.9.2.m1.1.1.1.cmml" xref="S4.T6.9.9.2.m1.1.1">subscript</csymbol><ci id="S4.T6.9.9.2.m1.1.1.2.cmml" xref="S4.T6.9.9.2.m1.1.1.2">𝜆</ci><ci id="S4.T6.9.9.2.m1.1.1.3.cmml" xref="S4.T6.9.9.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.9.2.m1.1c">\lambda_{i}</annotation></semantics></math> = 3.5</th>
<td id="S4.T6.9.9.3" class="ltx_td ltx_align_center">23.210</td>
<td id="S4.T6.9.9.4" class="ltx_td ltx_align_center">194.295</td>
<td id="S4.T6.9.9.5" class="ltx_td ltx_align_center">5.742</td>
<td id="S4.T6.9.9.6" class="ltx_td ltx_align_center">9.435</td>
<td id="S4.T6.9.9.7" class="ltx_td ltx_align_center">8.565</td>
</tr>
<tr id="S4.T6.11.11" class="ltx_tr">
<th id="S4.T6.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<math id="S4.T6.10.10.1.m1.1" class="ltx_Math" alttext="\lambda_{a}" display="inline"><semantics id="S4.T6.10.10.1.m1.1a"><msub id="S4.T6.10.10.1.m1.1.1" xref="S4.T6.10.10.1.m1.1.1.cmml"><mi id="S4.T6.10.10.1.m1.1.1.2" xref="S4.T6.10.10.1.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.10.10.1.m1.1.1.3" xref="S4.T6.10.10.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.10.10.1.m1.1b"><apply id="S4.T6.10.10.1.m1.1.1.cmml" xref="S4.T6.10.10.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.10.10.1.m1.1.1.1.cmml" xref="S4.T6.10.10.1.m1.1.1">subscript</csymbol><ci id="S4.T6.10.10.1.m1.1.1.2.cmml" xref="S4.T6.10.10.1.m1.1.1.2">𝜆</ci><ci id="S4.T6.10.10.1.m1.1.1.3.cmml" xref="S4.T6.10.10.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.10.1.m1.1c">\lambda_{a}</annotation></semantics></math> = 1.0</th>
<th id="S4.T6.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<math id="S4.T6.11.11.2.m1.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.T6.11.11.2.m1.1a"><msub id="S4.T6.11.11.2.m1.1.1" xref="S4.T6.11.11.2.m1.1.1.cmml"><mi id="S4.T6.11.11.2.m1.1.1.2" xref="S4.T6.11.11.2.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.11.11.2.m1.1.1.3" xref="S4.T6.11.11.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.11.11.2.m1.1b"><apply id="S4.T6.11.11.2.m1.1.1.cmml" xref="S4.T6.11.11.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.11.11.2.m1.1.1.1.cmml" xref="S4.T6.11.11.2.m1.1.1">subscript</csymbol><ci id="S4.T6.11.11.2.m1.1.1.2.cmml" xref="S4.T6.11.11.2.m1.1.1.2">𝜆</ci><ci id="S4.T6.11.11.2.m1.1.1.3.cmml" xref="S4.T6.11.11.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.11.11.2.m1.1c">\lambda_{i}</annotation></semantics></math> = 6.0</th>
<td id="S4.T6.11.11.3" class="ltx_td ltx_align_center">23.154</td>
<td id="S4.T6.11.11.4" class="ltx_td ltx_align_center">204.625</td>
<td id="S4.T6.11.11.5" class="ltx_td ltx_align_center">5.583</td>
<td id="S4.T6.11.11.6" class="ltx_td ltx_align_center">9.589</td>
<td id="S4.T6.11.11.7" class="ltx_td ltx_align_center">8.759</td>
</tr>
<tr id="S4.T6.13.13" class="ltx_tr">
<th id="S4.T6.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<math id="S4.T6.12.12.1.m1.1" class="ltx_Math" alttext="\lambda_{a}" display="inline"><semantics id="S4.T6.12.12.1.m1.1a"><msub id="S4.T6.12.12.1.m1.1.1" xref="S4.T6.12.12.1.m1.1.1.cmml"><mi id="S4.T6.12.12.1.m1.1.1.2" xref="S4.T6.12.12.1.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.12.12.1.m1.1.1.3" xref="S4.T6.12.12.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.12.12.1.m1.1b"><apply id="S4.T6.12.12.1.m1.1.1.cmml" xref="S4.T6.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.12.12.1.m1.1.1.1.cmml" xref="S4.T6.12.12.1.m1.1.1">subscript</csymbol><ci id="S4.T6.12.12.1.m1.1.1.2.cmml" xref="S4.T6.12.12.1.m1.1.1.2">𝜆</ci><ci id="S4.T6.12.12.1.m1.1.1.3.cmml" xref="S4.T6.12.12.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.12.12.1.m1.1c">\lambda_{a}</annotation></semantics></math> = 3.5</th>
<th id="S4.T6.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<math id="S4.T6.13.13.2.m1.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.T6.13.13.2.m1.1a"><msub id="S4.T6.13.13.2.m1.1.1" xref="S4.T6.13.13.2.m1.1.1.cmml"><mi id="S4.T6.13.13.2.m1.1.1.2" xref="S4.T6.13.13.2.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.13.13.2.m1.1.1.3" xref="S4.T6.13.13.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.13.13.2.m1.1b"><apply id="S4.T6.13.13.2.m1.1.1.cmml" xref="S4.T6.13.13.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.13.13.2.m1.1.1.1.cmml" xref="S4.T6.13.13.2.m1.1.1">subscript</csymbol><ci id="S4.T6.13.13.2.m1.1.1.2.cmml" xref="S4.T6.13.13.2.m1.1.1.2">𝜆</ci><ci id="S4.T6.13.13.2.m1.1.1.3.cmml" xref="S4.T6.13.13.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.13.2.m1.1c">\lambda_{i}</annotation></semantics></math> = 3.5</th>
<td id="S4.T6.13.13.3" class="ltx_td ltx_align_center">23.167</td>
<td id="S4.T6.13.13.4" class="ltx_td ltx_align_center">195.179</td>
<td id="S4.T6.13.13.5" class="ltx_td ltx_align_center">7.658</td>
<td id="S4.T6.13.13.6" class="ltx_td ltx_align_center">7.894</td>
<td id="S4.T6.13.13.7" class="ltx_td ltx_align_center">7.951</td>
</tr>
<tr id="S4.T6.15.15" class="ltx_tr">
<th id="S4.T6.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">
<math id="S4.T6.14.14.1.m1.1" class="ltx_Math" alttext="\lambda_{a}" display="inline"><semantics id="S4.T6.14.14.1.m1.1a"><msub id="S4.T6.14.14.1.m1.1.1" xref="S4.T6.14.14.1.m1.1.1.cmml"><mi id="S4.T6.14.14.1.m1.1.1.2" xref="S4.T6.14.14.1.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.14.14.1.m1.1.1.3" xref="S4.T6.14.14.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.14.14.1.m1.1b"><apply id="S4.T6.14.14.1.m1.1.1.cmml" xref="S4.T6.14.14.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.14.14.1.m1.1.1.1.cmml" xref="S4.T6.14.14.1.m1.1.1">subscript</csymbol><ci id="S4.T6.14.14.1.m1.1.1.2.cmml" xref="S4.T6.14.14.1.m1.1.1.2">𝜆</ci><ci id="S4.T6.14.14.1.m1.1.1.3.cmml" xref="S4.T6.14.14.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.14.14.1.m1.1c">\lambda_{a}</annotation></semantics></math> = 6.0</th>
<th id="S4.T6.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">
<math id="S4.T6.15.15.2.m1.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.T6.15.15.2.m1.1a"><msub id="S4.T6.15.15.2.m1.1.1" xref="S4.T6.15.15.2.m1.1.1.cmml"><mi id="S4.T6.15.15.2.m1.1.1.2" xref="S4.T6.15.15.2.m1.1.1.2.cmml">λ</mi><mi id="S4.T6.15.15.2.m1.1.1.3" xref="S4.T6.15.15.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T6.15.15.2.m1.1b"><apply id="S4.T6.15.15.2.m1.1.1.cmml" xref="S4.T6.15.15.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.15.15.2.m1.1.1.1.cmml" xref="S4.T6.15.15.2.m1.1.1">subscript</csymbol><ci id="S4.T6.15.15.2.m1.1.1.2.cmml" xref="S4.T6.15.15.2.m1.1.1.2">𝜆</ci><ci id="S4.T6.15.15.2.m1.1.1.3.cmml" xref="S4.T6.15.15.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.15.15.2.m1.1c">\lambda_{i}</annotation></semantics></math> = 3.5</th>
<td id="S4.T6.15.15.3" class="ltx_td ltx_align_center ltx_border_b">25.016</td>
<td id="S4.T6.15.15.4" class="ltx_td ltx_align_center ltx_border_b">229.128</td>
<td id="S4.T6.15.15.5" class="ltx_td ltx_align_center ltx_border_b">7.952</td>
<td id="S4.T6.15.15.6" class="ltx_td ltx_align_center ltx_border_b">7.742</td>
<td id="S4.T6.15.15.7" class="ltx_td ltx_align_center ltx_border_b">9.024</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.21.3.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.19.2" class="ltx_text" style="font-size:90%;">Quantitative study of audio and image CFG scales. In our implementation, we adopt the setting (<math id="S4.T6.18.1.m1.1" class="ltx_Math" alttext="\lambda_{a}=3.5" display="inline"><semantics id="S4.T6.18.1.m1.1b"><mrow id="S4.T6.18.1.m1.1.1" xref="S4.T6.18.1.m1.1.1.cmml"><msub id="S4.T6.18.1.m1.1.1.2" xref="S4.T6.18.1.m1.1.1.2.cmml"><mi id="S4.T6.18.1.m1.1.1.2.2" xref="S4.T6.18.1.m1.1.1.2.2.cmml">λ</mi><mi id="S4.T6.18.1.m1.1.1.2.3" xref="S4.T6.18.1.m1.1.1.2.3.cmml">a</mi></msub><mo id="S4.T6.18.1.m1.1.1.1" xref="S4.T6.18.1.m1.1.1.1.cmml">=</mo><mn id="S4.T6.18.1.m1.1.1.3" xref="S4.T6.18.1.m1.1.1.3.cmml">3.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.18.1.m1.1c"><apply id="S4.T6.18.1.m1.1.1.cmml" xref="S4.T6.18.1.m1.1.1"><eq id="S4.T6.18.1.m1.1.1.1.cmml" xref="S4.T6.18.1.m1.1.1.1"></eq><apply id="S4.T6.18.1.m1.1.1.2.cmml" xref="S4.T6.18.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T6.18.1.m1.1.1.2.1.cmml" xref="S4.T6.18.1.m1.1.1.2">subscript</csymbol><ci id="S4.T6.18.1.m1.1.1.2.2.cmml" xref="S4.T6.18.1.m1.1.1.2.2">𝜆</ci><ci id="S4.T6.18.1.m1.1.1.2.3.cmml" xref="S4.T6.18.1.m1.1.1.2.3">𝑎</ci></apply><cn type="float" id="S4.T6.18.1.m1.1.1.3.cmml" xref="S4.T6.18.1.m1.1.1.3">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.18.1.m1.1d">\lambda_{a}=3.5</annotation></semantics></math> and <math id="S4.T6.19.2.m2.1" class="ltx_Math" alttext="\lambda_{i}=3.5" display="inline"><semantics id="S4.T6.19.2.m2.1b"><mrow id="S4.T6.19.2.m2.1.1" xref="S4.T6.19.2.m2.1.1.cmml"><msub id="S4.T6.19.2.m2.1.1.2" xref="S4.T6.19.2.m2.1.1.2.cmml"><mi id="S4.T6.19.2.m2.1.1.2.2" xref="S4.T6.19.2.m2.1.1.2.2.cmml">λ</mi><mi id="S4.T6.19.2.m2.1.1.2.3" xref="S4.T6.19.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S4.T6.19.2.m2.1.1.1" xref="S4.T6.19.2.m2.1.1.1.cmml">=</mo><mn id="S4.T6.19.2.m2.1.1.3" xref="S4.T6.19.2.m2.1.1.3.cmml">3.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.19.2.m2.1c"><apply id="S4.T6.19.2.m2.1.1.cmml" xref="S4.T6.19.2.m2.1.1"><eq id="S4.T6.19.2.m2.1.1.1.cmml" xref="S4.T6.19.2.m2.1.1.1"></eq><apply id="S4.T6.19.2.m2.1.1.2.cmml" xref="S4.T6.19.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.T6.19.2.m2.1.1.2.1.cmml" xref="S4.T6.19.2.m2.1.1.2">subscript</csymbol><ci id="S4.T6.19.2.m2.1.1.2.2.cmml" xref="S4.T6.19.2.m2.1.1.2.2">𝜆</ci><ci id="S4.T6.19.2.m2.1.1.2.3.cmml" xref="S4.T6.19.2.m2.1.1.2.3">𝑖</ci></apply><cn type="float" id="S4.T6.19.2.m2.1.1.3.cmml" xref="S4.T6.19.2.m2.1.1.3">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.19.2.m2.1d">\lambda_{i}=3.5</annotation></semantics></math>) for the balance of visual fidelity and motion diversity.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Audio and Image CFG Scales.</h4>

<div id="S4.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px4.p1.2" class="ltx_p">Table <a href="#S4.T6" title="Table 6 ‣ Expression Control by Adjusting Hierarchical Weights. ‣ 4.4 Ablation Study ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents a quantitative analysis of the generated videos under various configurations of audio and image CFG scales.
Among the configurations, the setting of <math id="S4.SS4.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\lambda_{a}=3.5" display="inline"><semantics id="S4.SS4.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS4.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml"><msub id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">λ</mi><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">a</mi></msub><mo id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml">3.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1"><eq id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2">𝜆</ci><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3">𝑎</ci></apply><cn type="float" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.1.m1.1c">\lambda_{a}=3.5</annotation></semantics></math> and <math id="S4.SS4.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="\lambda_{i}=3.5" display="inline"><semantics id="S4.SS4.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS4.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml"><msub id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">λ</mi><mi id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml">3.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"><eq id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.2">𝜆</ci><ci id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.3">𝑖</ci></apply><cn type="float" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.2.m2.1c">\lambda_{i}=3.5</annotation></semantics></math> achieves a balanced performance, with competitive FID (23.167), FVD (195.179), Sync-C (7.658), Sync-D (7.894), and E-FID (7.951) scores.
This balance between visual fidelity and motion diversity underscores the effectiveness of this configuration in producing high-quality, synchronized videos.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.3.1" class="ltx_tr">
<th id="S4.T7.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T7.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GPU memory (GB)</th>
<th id="S4.T7.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Time (sec)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.4.1" class="ltx_tr">
<th id="S4.T7.2.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Inference w. HADVS</th>
<td id="S4.T7.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">9.77</td>
<td id="S4.T7.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">1.63</td>
</tr>
<tr id="S4.T7.2.5.2" class="ltx_tr">
<th id="S4.T7.2.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Inference w.o. HADVS</th>
<td id="S4.T7.2.5.2.2" class="ltx_td ltx_align_center">9.76</td>
<td id="S4.T7.2.5.2.3" class="ltx_td ltx_align_center">1.63</td>
</tr>
<tr id="S4.T7.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Inference (<math id="S4.T7.1.1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.T7.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.m1.1.1" xref="S4.T7.1.1.1.m1.1.1.cmml"><mn id="S4.T7.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.1.1.1.m1.1.1.1" xref="S4.T7.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1"><times id="S4.T7.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T7.1.1.1.m1.1.1.3.cmml" xref="S4.T7.1.1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.m1.1c">256\times 256</annotation></semantics></math>)</th>
<td id="S4.T7.1.1.2" class="ltx_td ltx_align_center">6.62</td>
<td id="S4.T7.1.1.3" class="ltx_td ltx_align_center">0.46</td>
</tr>
<tr id="S4.T7.2.2" class="ltx_tr">
<th id="S4.T7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Inference (<math id="S4.T7.2.2.1.m1.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S4.T7.2.2.1.m1.1a"><mrow id="S4.T7.2.2.1.m1.1.1" xref="S4.T7.2.2.1.m1.1.1.cmml"><mn id="S4.T7.2.2.1.m1.1.1.2" xref="S4.T7.2.2.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.2.2.1.m1.1.1.1" xref="S4.T7.2.2.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.2.2.1.m1.1.1.3" xref="S4.T7.2.2.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.1.m1.1b"><apply id="S4.T7.2.2.1.m1.1.1.cmml" xref="S4.T7.2.2.1.m1.1.1"><times id="S4.T7.2.2.1.m1.1.1.1.cmml" xref="S4.T7.2.2.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.2.2.1.m1.1.1.2.cmml" xref="S4.T7.2.2.1.m1.1.1.2">1024</cn><cn type="integer" id="S4.T7.2.2.1.m1.1.1.3.cmml" xref="S4.T7.2.2.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.1.m1.1c">1024\times 1024</annotation></semantics></math>)</th>
<td id="S4.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_b">20.66</td>
<td id="S4.T7.2.2.3" class="ltx_td ltx_align_center ltx_border_b">10.29</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.8.3.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.6.2" class="ltx_text" style="font-size:90%;">Efficiency analysis of different steps and setting of the proposed approach.
“HADVS” denotes the proposed hierarchical audio-driven visual synthesis.
“<math id="S4.T7.5.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.T7.5.1.m1.1b"><mrow id="S4.T7.5.1.m1.1.1" xref="S4.T7.5.1.m1.1.1.cmml"><mn id="S4.T7.5.1.m1.1.1.2" xref="S4.T7.5.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.5.1.m1.1.1.1" xref="S4.T7.5.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.5.1.m1.1.1.3" xref="S4.T7.5.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.5.1.m1.1c"><apply id="S4.T7.5.1.m1.1.1.cmml" xref="S4.T7.5.1.m1.1.1"><times id="S4.T7.5.1.m1.1.1.1.cmml" xref="S4.T7.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T7.5.1.m1.1.1.2.cmml" xref="S4.T7.5.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T7.5.1.m1.1.1.3.cmml" xref="S4.T7.5.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.1.m1.1d">256\times 256</annotation></semantics></math>” and “<math id="S4.T7.6.2.m2.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S4.T7.6.2.m2.1b"><mrow id="S4.T7.6.2.m2.1.1" xref="S4.T7.6.2.m2.1.1.cmml"><mn id="S4.T7.6.2.m2.1.1.2" xref="S4.T7.6.2.m2.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.6.2.m2.1.1.1" xref="S4.T7.6.2.m2.1.1.1.cmml">×</mo><mn id="S4.T7.6.2.m2.1.1.3" xref="S4.T7.6.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.6.2.m2.1c"><apply id="S4.T7.6.2.m2.1.1.cmml" xref="S4.T7.6.2.m2.1.1"><times id="S4.T7.6.2.m2.1.1.1.cmml" xref="S4.T7.6.2.m2.1.1.1"></times><cn type="integer" id="S4.T7.6.2.m2.1.1.2.cmml" xref="S4.T7.6.2.m2.1.1.2">1024</cn><cn type="integer" id="S4.T7.6.2.m2.1.1.3.cmml" xref="S4.T7.6.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.2.m2.1d">1024\times 1024</annotation></semantics></math>” represent the inference video resoluton.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Efficiency Analysis.</h4>

<div id="S4.SS4.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px5.p1.2" class="ltx_p">Table <a href="#S4.T7" title="Table 7 ‣ Audio and Image CFG Scales. ‣ 4.4 Ablation Study ‣ 4 Experiment ‣ Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents an efficiency analysis of various configurations of our proposed approach.
The inference with hierarchical audio-driven visual synthesis requires 9.77 GB of GPU memory and takes 1.63 seconds. In contrast, without hierarchical audio-driven visual synthesis, the GPU memory usage is slightly reduced to 9.76 GB while maintaining the same inference time of 1.63 seconds.
Variations in video resolution significantly affect both GPU memory usage and inference time; processing at <math id="S4.SS4.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS4.SSS0.Px5.p1.1.m1.1a"><mrow id="S4.SS4.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.cmml"><mn id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.2" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.1" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.3" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px5.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1"><times id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS4.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS0.Px5.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px5.p1.1.m1.1c">256\times 256</annotation></semantics></math> resolution requires 6.62 GB of GPU memory and 0.46 seconds, whereas <math id="S4.SS4.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S4.SS4.SSS0.Px5.p1.2.m2.1a"><mrow id="S4.SS4.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.cmml"><mn id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.1" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px5.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1"><times id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.2">1024</cn><cn type="integer" id="S4.SS4.SSS0.Px5.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px5.p1.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px5.p1.2.m2.1c">1024\times 1024</annotation></semantics></math> resolution necessitates 20.66 GB of GPU memory and 10.29 seconds.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Limitation and Future Work</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Despite the advancements proposed in this study regarding portrait image animation, there exist several limitations that necessitate further exploration and consideration.
These limitations underscore areas where future research endeavors can contribute to the refinement and augmentation of the presented methodology:
<span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">(1) Enhanced visual-audio synchronization.</span>
Future investigations could delve into more advanced synchronization techniques, potentially integrating sophisticated audio analysis methods or harnessing deeper cross-modal learning strategies.
These advancements have the potential to refine the alignment of facial movements with audio inputs, particularly in contexts characterized by intricate speech patterns or nuanced emotional expressions.
<span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_bold">(2) Robust temporal coherence.</span>
There is a need for further exploration of advanced temporal coherence mechanisms to address inconsistencies in sequences featuring rapid or intricate movements.
Developing robust alignment strategies, possibly guided by long-term dependencies in sequential data or leveraging recurrent neural network models, could bolster frame-to-frame stability.
<span id="S4.SS5.p1.1.3" class="ltx_text ltx_font_bold">(3) Computational efficiency.</span>
It is imperative to optimize the computational efficiency of the diffusion-based generative model and UNet-based denoiser.
Research into lightweight architectures, model pruning, or efficient parallelization techniques holds promise in rendering the approach more viable for real-time applications while minimizing resource utilization.
<span id="S4.SS5.p1.1.4" class="ltx_text ltx_font_bold">(4) Improved diversity control.</span>
The balance between the diversity of expressions and poses and the preservation of visual identity integrity remains a critical challenge.
Future endeavors could concentrate on refining adaptive control mechanisms, potentially through the integration of more nuanced control parameters or sophisticated diversity metrics.
Such refinements would ensure a more natural and diverse animation output while upholding the authenticity of facial identities.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Social Risks and Mitigations</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">In the context of the research presented in the paper, there are social risks associated with the development and implementation of portrait image animation technologies driven by audio inputs.
One potential risk is the ethical implications of creating highly realistic and dynamic portraits that could potentially be misused for deceptive or malicious purposes, such as deepfakes.
To mitigate this risk, it is essential to establish ethical guidelines and responsible use practices for the technology.
Additionally, there may be concerns regarding privacy and consent related to the use of individuals’ images and voices in the creation of animated portraits.
Mitigating these concerns involves ensuring transparent data usage policies, obtaining informed consent, and safeguarding the privacy rights of individuals.
By addressing these social risks and implementing appropriate mitigations, the research aims to promote the responsible and ethical development of portrait image animation technologies within the societal context.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper introduces a novel method for portrait image animation using end-to-end diffusion models, addressing challenges in audio-driven facial dynamics synchronization and high-quality animation generation with temporal consistency.
The proposed hierarchical audio-driven visual synthesis module enhances audio-visual alignment through cross-attention mechanisms and adaptive weighting.
By integrating diffusion-based generative modeling, UNet denoising, temporal alignment, and ReferenceNet, the method improves animation quality and realism.
Experimental evaluations demonstrate superior image and video quality, enhanced lip synchronization, and increased motion diversity, validated by superior FID and FVD metrics.
The method allows flexible control over expression and pose diversity to accommodate diverse visual identities.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanz and Vetter [2003]</span>
<span class="ltx_bibblock">
Volker Blanz and Thomas
Vetter. 2003.

</span>
<span class="ltx_bibblock">Face recognition based on fitting a 3D morphable
model.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on pattern analysis and
machine intelligence</em> 25, 9
(2003), 1063–1074.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin
Rombach, Huan Ling, Tim Dockhorn,
Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. 2023.

</span>
<span class="ltx_bibblock">Align your latents: High-resolution video synthesis
with latent diffusion models. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
22563–22575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burkov et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> [2020]</span>
<span class="ltx_bibblock">
Egor Burkov, Igor
Pasechnik, Artur Grigorev, and Victor
Lempitsky. 2020.

</span>
<span class="ltx_bibblock">Neural head reenactment with latent pose
descriptors. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
13786–13795.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> [2023b]</span>
<span class="ltx_bibblock">
Haoxin Chen, Menghan Xia,
Yingqing He, Yong Zhang,
Xiaodong Cun, Shaoshu Yang,
Jinbo Xing, Yaofang Liu,
Qifeng Chen, Xintao Wang,
et al<span id="bib.bib5.3.1" class="ltx_text">.</span> 2023b.

</span>
<span class="ltx_bibblock">Videocrafter1: Open diffusion models for
high-quality video generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.19512</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> [2023a]</span>
<span class="ltx_bibblock">
Tsai-Shien Chen,
Chieh Hubert Lin, Hung-Yu Tseng,
Tsung-Yi Lin, and Ming-Hsuan Yang.
2023a.

</span>
<span class="ltx_bibblock">Motion-conditioned diffusion model for controllable
video synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14404</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corona et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Enric Corona, Andrei
Zanfir, Eduard Gabriel Bazavan, Nikos
Kolotouros, Thiemo Alldieck, and
Cristian Sminchisescu. 2024.

</span>
<span class="ltx_bibblock">VLOGGER: Multimodal Diffusion for Embodied Avatar
Synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.08764</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Zuozhuo Dai, Zhenghao
Zhang, Yao Yao, Bingxue Qiu,
Siyu Zhu, Long Qin, and
Weizhi Wang. 2023.

</span>
<span class="ltx_bibblock">AnimateAnything: Fine-Grained Open Domain Image
Animation with Motion Guidance.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.12886</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drobyshev et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
Nikita Drobyshev, Jenya
Chelishev, Taras Khakhulin, Aleksei
Ivakhnenko, Victor Lempitsky, and Egor
Zakharov. 2022.

</span>
<span class="ltx_bibblock">Megaportraits: One-shot megapixel neural head
avatars. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM
International Conference on Multimedia (ACM MM)</em>.
2663–2671.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Patrick Esser, Johnathan
Chiu, Parmida Atighehchian, Jonathan
Granskog, and Anastasis Germanidis.
2023.

</span>
<span class="ltx_bibblock">Structure and content-guided video synthesis with
diffusion models. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
7346–7356.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Yue Gao, Yuan Zhou,
Jinglu Wang, Xiao Li,
Xiang Ming, and Yan Lu.
2023.

</span>
<span class="ltx_bibblock">High-fidelity and freely controllable talking head
video generation. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
5609–5619.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Yuwei Guo, Ceyuan Yang,
Anyi Rao, Zhengyang Liang,
Yaohui Wang, Yu Qiao,
Maneesh Agrawala, Dahua Lin, and
Bo Dai. 2023.

</span>
<span class="ltx_bibblock">AnimateDiff: Animate Your Personalized
Text-to-Image Diffusion Models without Specific Tuning. In
<em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harvey et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
William Harvey, Saeid
Naderiparizi, Vaden Masrani, Christian
Weilbach, and Frank Wood.
2022.

</span>
<span class="ltx_bibblock">Flexible diffusion modeling of long videos.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems (NeurIPS)</em> 35 (2022),
27953–27965.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Tianyu He, Junliang Guo,
Runyi Yu, Yuchi Wang,
Jialiang Zhu, Kaikai An,
Leyi Li, Xu Tan, Chunyu
Wang, Han Hu, et al<span id="bib.bib14.3.1" class="ltx_text">.</span>
2023.

</span>
<span class="ltx_bibblock">GAIA: Zero-shot Talking Avatar Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.15230</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> [2022a]</span>
<span class="ltx_bibblock">
Jonathan Ho, William
Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko,
Diederik P Kingma, Ben Poole,
Mohammad Norouzi, David J Fleet,
et al<span id="bib.bib15.3.1" class="ltx_text">.</span> 2022a.

</span>
<span class="ltx_bibblock">Imagen video: High definition video generation with
diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02303</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> [2022b]</span>
<span class="ltx_bibblock">
Jonathan Ho, Tim
Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J
Fleet. 2022b.

</span>
<span class="ltx_bibblock">Video diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems (NeurIPS)</em> 35 (2022),
8633–8646.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
Yang Hong, Bo Peng,
Haiyao Xiao, Ligang Liu, and
Juyong Zhang. 2022.

</span>
<span class="ltx_bibblock">Headnerf: A real-time nerf-based parametric head
model. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR)</em>.
20374–20384.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Yaosi Hu, Zhenzhong Chen,
and Chong Luo. 2023.

</span>
<span class="ltx_bibblock">Lamd: Latent motion diffusion for video
generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.11603</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khachatryan et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Levon Khachatryan,
Andranik Movsisyan, Vahram Tadevosyan,
Roberto Henschel, Zhangyang Wang,
Shant Navasardyan, and Humphrey Shi.
2023.

</span>
<span class="ltx_bibblock">Text2video-zero: Text-to-image diffusion models are
zero-shot video generators. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
15954–15964.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> [2017]</span>
<span class="ltx_bibblock">
Tianye Li, Timo Bolkart,
Michael J Black, Hao Li, and
Javier Romero. 2017.

</span>
<span class="ltx_bibblock">Learning a model of facial shape and expression
from 4D scans.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">ACM Transactions Graphics.</em>
36, 6 (2017),
194–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
Borong Liang, Yan Pan,
Zhizhi Guo, Hang Zhou,
Zhibin Hong, Xiaoguang Han,
Junyu Han, Jingtuo Liu,
Errui Ding, and Jingdong Wang.
2022.

</span>
<span class="ltx_bibblock">Expressive talking head generation with granular
audio-visual control. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
3387–3396.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Shaoteng Liu, Yuechen
Zhang, Wenbo Li, Zhe Lin, and
Jiaya Jia. 2023.

</span>
<span class="ltx_bibblock">Video-p2p: Video editing with cross-attention
control.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04761</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Tao Liu, Feilong Chen,
Shuai Fan, Chenpeng Du,
Qi Chen, Xie Chen, and
Kai Yu. 2024.

</span>
<span class="ltx_bibblock">AniTalker: Animate Vivid and Diverse Talking Faces
through Identity-Decoupled Facial Motion Encoding.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.03121</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Zhengxiong Luo, Dayou
Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen,
Deli Zhao, Jingren Zhou, and
Tieniu Tan. 2023.

</span>
<span class="ltx_bibblock">Videofusion: Decomposed diffusion models for
high-quality video generation. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10209–10218.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Yifeng Ma, Shiwei Zhang,
Jiayu Wang, Xiang Wang,
Yingya Zhang, and Zhidong Deng.
2023.

</span>
<span class="ltx_bibblock">Dreamtalk: When expressive talking head generation
meets diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.09767</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Molad et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Eyal Molad, Eliahu
Horwitz, Dani Valevski, Alex Rav Acha,
Yossi Matias, Yael Pritch,
Yaniv Leviathan, and Yedid Hoshen.
2023.

</span>
<span class="ltx_bibblock">Dreamix: Video diffusion models are general video
editors.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.01329</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Youxin Pang, Yong Zhang,
Weize Quan, Yanbo Fan,
Xiaodong Cun, Ying Shan, and
Dong-ming Yan. 2023.

</span>
<span class="ltx_bibblock">Dpe: Disentanglement of pose and expression for
general video portrait editing. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
427–436.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles and Xie [2023]</span>
<span class="ltx_bibblock">
William Peebles and
Saining Xie. 2023.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers. In
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV)</em>. 4195–4205.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> [2020]</span>
<span class="ltx_bibblock">
KR Prajwal, Rudrabha
Mukhopadhyay, Vinay P Namboodiri, and
CV Jawahar. 2020.

</span>
<span class="ltx_bibblock">A lip sync expert is all you need for speech to lip
generation in the wild. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th
ACM international conference on multimedia (ACM MM)</em>.
484–492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook
Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark,
et al<span id="bib.bib30.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural
language supervision. In <em id="bib.bib30.4.1" class="ltx_emph ltx_font_italic">International conference
on machine learning (ICML)</em>. 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> [2021]</span>
<span class="ltx_bibblock">
Yurui Ren, Ge Li,
Yuanqi Chen, Thomas H Li, and
Shan Liu. 2021.

</span>
<span class="ltx_bibblock">Pirenderer: Controllable portrait image generation
via semantic neural rendering. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV)</em>.
13759–13768.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> [2022a]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas
Blattmann, Dominik Lorenz, Patrick
Esser, and Björn Ommer.
2022a.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent
diffusion models. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> [2022b]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas
Blattmann, Dominik Lorenz, Patrick
Esser, and Björn Ommer.
2022b.

</span>
<span class="ltx_bibblock">High-resolution Image Synthesis with Latent
Diffusion Models. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneider et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> [2019]</span>
<span class="ltx_bibblock">
Steffen Schneider, Alexei
Baevski, Ronan Collobert, and Michael
Auli. 2019.

</span>
<span class="ltx_bibblock">wav2vec: Unsupervised pre-training for speech
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Shuai Shen, Wenliang
Zhao, Zibin Meng, Wanhua Li,
Zheng Zhu, Jie Zhou, and
Jiwen Lu. 2023.

</span>
<span class="ltx_bibblock">Difftalk: Crafting diffusion models for generalized
audio-driven portraits animation. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR)</em>. 1982–1991.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siarohin et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> [2019]</span>
<span class="ltx_bibblock">
Aliaksandr Siarohin,
Stéphane Lathuilière, Sergey
Tulyakov, Elisa Ricci, and Nicu Sebe.
2019.

</span>
<span class="ltx_bibblock">First order motion model for image animation.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems (NeurIPS)</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
Uriel Singer, Adam
Polyak, Thomas Hayes, Xi Yin,
Jie An, Songyang Zhang,
Qiyuan Hu, Harry Yang,
Oron Ashual, Oran Gafni, et al<span id="bib.bib37.3.1" class="ltx_text">.</span>
2022.

</span>
<span class="ltx_bibblock">Make-A-Video: Text-to-Video Generation without
Text-Video Data. In <em id="bib.bib37.4.1" class="ltx_emph ltx_font_italic">The Eleventh International
Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stypułkowski et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Michał Stypułkowski,
Konstantinos Vougioukas, Sen He,
Maciej Zięba, Stavros Petridis,
and Maja Pantic. 2024.

</span>
<span class="ltx_bibblock">Diffused heads: Diffusion models beat gans on
talking-face generation. In <em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>.
5091–5100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Xusen Sun, Longhao Zhang,
Hao Zhu, Peng Zhang,
Bang Zhang, Xinya Ji,
Kangneng Zhou, Daiheng Gao,
Liefeng Bo, and Xun Cao.
2023.

</span>
<span class="ltx_bibblock">Vividtalk: One-shot audio-driven talking head
generation based on 3d hybrid prior.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.01841</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Linrui Tian, Qi Wang,
Bang Zhang, and Liefeng Bo.
2024.

</span>
<span class="ltx_bibblock">EMO: Emote Portrait Alive-Generating Expressive
Portrait Videos with Audio2Video Diffusion Model under Weak Conditions.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.17485</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Den Oord et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> [2017]</span>
<span class="ltx_bibblock">
Aaron Van Den Oord, Oriol
Vinyals, et al<span id="bib.bib41.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.4.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems (NeurIPS)</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> [2023a]</span>
<span class="ltx_bibblock">
Duomin Wang, Yu Deng,
Zixin Yin, Heung-Yeung Shum, and
Baoyuan Wang. 2023a.

</span>
<span class="ltx_bibblock">Progressive disentangled representation learning
for fine-grained controllable talking head synthesis. In
<em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>. 17979–17989.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> [2021a]</span>
<span class="ltx_bibblock">
S Wang, L Li,
Y Ding, C Fan, and X
Yu. 2021a.

</span>
<span class="ltx_bibblock">Audio2Head: Audio-driven One-shot Talking-head
Generation with Natural Head Motion. In
<em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">International Joint Conference on Artificial
Intelligence (IJCAI)</em>. 1098–1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> [2021b]</span>
<span class="ltx_bibblock">
Ting-Chun Wang, Arun
Mallya, and Ming-Yu Liu.
2021b.

</span>
<span class="ltx_bibblock">One-shot free-view neural talking-head synthesis
for video conferencing. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10039–10049.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> [2023b]</span>
<span class="ltx_bibblock">
Wen Wang, Yan Jiang,
Kangyang Xie, Zide Liu,
Hao Chen, Yue Cao,
Xinlong Wang, and Chunhua Shen.
2023b.

</span>
<span class="ltx_bibblock">Zero-shot video editing using off-the-shelf image
diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17599</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Xiang Wang, Hangjie Yuan,
Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang,
Yujun Shen, Deli Zhao, and
Jingren Zhou. 2024.

</span>
<span class="ltx_bibblock">Videocomposer: Compositional video synthesis with
motion controllability.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems (NeurIPS)</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Huawei Wei, Zejun Yang,
and Zhisheng Wang. 2024.

</span>
<span class="ltx_bibblock">Aniportrait: Audio-driven synthesis of
photorealistic portrait animation.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.17694</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Sicheng Xu, Guojun Chen,
Yu-Xiao Guo, Jiaolong Yang,
Chong Li, Zhenyu Zang,
Yizhong Zhang, Xin Tong, and
Baining Guo. 2024.

</span>
<span class="ltx_bibblock">VASA-1: Lifelike Audio-Driven Talking Faces
Generated in Real Time.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.10667</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Ruihan Yang, Prakhar
Srivastava, and Stephan Mandt.
2023.

</span>
<span class="ltx_bibblock">Diffusion probabilistic modeling for video
generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Entropy</em> 25,
10 (2023), 1469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> [2022]</span>
<span class="ltx_bibblock">
Fei Yin, Yong Zhang,
Xiaodong Cun, Mingdeng Cao,
Yanbo Fan, Xuan Wang,
Qingyan Bai, Baoyuan Wu,
Jue Wang, and Yujiu Yang.
2022.

</span>
<span class="ltx_bibblock">Styleheat: One-shot high-resolution editable
talking face generation via pre-trained stylegan. In
<em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.
Springer, 85–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zakharov et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> [2020]</span>
<span class="ltx_bibblock">
Egor Zakharov, Aleksei
Ivakhnenko, Aliaksandra Shysheya, and
Victor Lempitsky. 2020.

</span>
<span class="ltx_bibblock">Fast bi-layer neural synthesis of one-shot
realistic head avatars. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XII 16</em>. Springer, 524–540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> [2023c]</span>
<span class="ltx_bibblock">
Bowen Zhang, Chenyang Qi,
Pan Zhang, Bo Zhang,
HsiangTao Wu, Dong Chen,
Qifeng Chen, Yong Wang, and
Fang Wen. 2023c.

</span>
<span class="ltx_bibblock">Metaportrait: Identity-preserving talking head
generation with fast personalized adaptation. In
<em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>. 22096–22105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> [2023a]</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Xiaodong
Cun, Xuan Wang, Yong Zhang,
Xi Shen, Yu Guo, Ying
Shan, and Fei Wang. 2023a.

</span>
<span class="ltx_bibblock">Sadtalker: Learning realistic 3d motion
coefficients for stylized audio-driven single image talking face animation.
In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.
8652–8661.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> [2023b]</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Xiaodong
Cun, Xuan Wang, Yong Zhang,
Xi Shen, Yu Guo, Ying
Shan, and Fei Wang. 2023b.

</span>
<span class="ltx_bibblock">Sadtalker: Learning realistic 3d motion
coefficients for stylized audio-driven single image talking face animation.
In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.
8652–8661.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> [2023]</span>
<span class="ltx_bibblock">
Daquan Zhou, Weimin Wang,
Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng.
2023.

</span>
<span class="ltx_bibblock">Magicvideo: Efficient video generation with latent
diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.11018</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> [2021]</span>
<span class="ltx_bibblock">
Hang Zhou, Yasheng Sun,
Wayne Wu, Chen Change Loy,
Xiaogang Wang, and Ziwei Liu.
2021.

</span>
<span class="ltx_bibblock">Pose-controllable talking face generation by
implicitly modularized audio-visual representation. In
<em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>. 4176–4186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> [2024]</span>
<span class="ltx_bibblock">
Shenhao Zhu, Junming Leo
Chen, Zuozhuo Dai, Yinghui Xu,
Xun Cao, Yao Yao, Hao
Zhu, , and Siyu Zhu.
2024.

</span>
<span class="ltx_bibblock">Champ: Controllable and consistent human image
animation with 3d parametric guidance.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.14781</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.08800" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.08801" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.08801">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.08801" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.08802" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 18:55:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
