<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.17927] The Evolution of Multimodal Model Architectures</title><meta property="og:description" content="This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape.
Systematically categorizing models by architecture type facilitates monit…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Evolution of Multimodal Model Architectures">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Evolution of Multimodal Model Architectures">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.17927">

<!--Generated on Wed Jun  5 19:18:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Evolution of Multimodal Model Architectures</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shakti N. Wadekar 
<br class="ltx_break">Purdue University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">swadekar@purdue.edu</span> 
<br class="ltx_break">Abhishek Chaurasia 
<br class="ltx_break">Chaos Industries Inc. 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">abhi@choasinc.com</span> 
<br class="ltx_break">Aman Chadha 
<br class="ltx_break">Stanford; Amazon 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">hi@aman.ai</span> 
<br class="ltx_break">Eugenio Culurciello 
<br class="ltx_break">Purdue University
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">euge@purdue.edu</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Work does not relate to position at Amazon.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape.
Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain.
Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types.
The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model.
The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage.
Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers.
On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model’s input stage.
The identified architecture types aid the monitoring of any-to-any multimodal model development.
Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models.
Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques.
To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2405.17927/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">Development timeline of Multimodal models grouped in four proposed architecture types.</span></figcaption>
</figure>
<figure id="S0.F2" class="ltx_figure"><img src="/html/2405.17927/assets/x2.png" id="S0.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="442" height="643" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S0.F2.3.2" class="ltx_text" style="font-size:90%;">Taxonomy of multimodal model architectures. Four distinct types of multimodal architectures and their sub-types are outlined. Various models are systematically catalogued to the types and sub-types. Deep Fusion: Type-A and Type-B fuses multimodal inputs within the internal layers of the model. Early Fusion: Type-C and Type-D facilitate fusion at the input stage. Type-A uses standard cross-attention, whereas Type-B utilizes custom-designed cross-attention or specialized layers. Type-C is a non-tokenizing multimodal model architecture, while Type-D, employs input-tokenization (discrete tokens). SCDF: Standard Cross-attention based Deep Fusion. CLDF: Custom Layer based Deep Fusion. NTEF: Non-Tokenized Early Fusion. TEF: Tokenized Early Fusion.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The multimodal domain of machine learning has seen significant advancements in recent years. The proliferation of models capable of processing images, audio, or video in conjunction with text (language) has notably expanded
(<cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Mizrahi et al. [<a href="#bib.bib3" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yang et al. [<a href="#bib.bib5" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Tang et al. [<a href="#bib.bib6" title="" class="ltx_ref">2023a</a>]</cite>).
Remarkable strides have been particularly evident in the integration of image and text modalities across diverse vision-language tasks, primarily because of the Transformer model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. [<a href="#bib.bib7" title="" class="ltx_ref">2017</a>]</cite>.
The Transformer model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. [<a href="#bib.bib7" title="" class="ltx_ref">2017</a>]</cite>, a pioneering deep neural network (NN) architecture, has spearheaded a unified framework for cross-domain learning. This singular model exhibits remarkable efficacy in comprehending and processing data from diverse domains.
The introduction of the Transformer model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. [<a href="#bib.bib7" title="" class="ltx_ref">2017</a>]</cite> for Natural Language Processing (NLP) in 2017 marked the inception of transformer-based model architectures.
Subsequently, the introduction of the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. [<a href="#bib.bib8" title="" class="ltx_ref">2021</a>]</cite> and CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. [<a href="#bib.bib9" title="" class="ltx_ref">2021</a>]</cite> for the vision domain showcased the versatility of transformers in handling image-related tasks.
This demonstration highlighted the transformer’s ability to learn from diverse domains, prompting a series of initiatives aimed at constructing models capable of jointly processing image and text data, leveraging the robust Transformer model architecture.
Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, a transformer-based multimodal model that incorporates both image and text data as input, exhibited outstanding performance on vision-language tasks. These results served as a catalyst for further advancements in the multimodal domain and encouraged the integration of additional modalities <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. [<a href="#bib.bib10" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Gong et al. [<a href="#bib.bib12" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Ma et al. [<a href="#bib.bib14" title="" class="ltx_ref">2023</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Diverse methodologies have been employed in numerous research efforts dedicated to handling mixed modalities,
including augmenting Large Language Models (LLMs) to create multimodal architectures <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Gong et al. [<a href="#bib.bib12" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib15" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Gao et al. [<a href="#bib.bib16" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib18" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Tian et al. [<a href="#bib.bib20" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib21" title="" class="ltx_ref">2024</a>]</cite>, training encoder-decoder style transformers with different input modalities <cite class="ltx_cite ltx_citemacro_cite">Mizrahi et al. [<a href="#bib.bib3" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib22" title="" class="ltx_ref">2022a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>, and exploring alternative approaches (Section <a href="#S2" title="2 Related Work ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
The plethora of research presents challenges in effectively monitoring the progression of model architectures and identifying emerging trends in next-generation multimodal model designs.
We examine contemporary landscape of state-of-the-art multimodal models, and identify distinct multimodal model architectures based on the fusion of inputs into the deep neural networks.
Primarily, we group existing multimodal architectures in four broad categories namely: Type - A, B, C, and D.
In Type-A and Type-B architectures, deep fusion of input modalities is realized through the integration of inputs within the internal layers of the model, whereas in Type-C and Type-D architectures, early fusion of modalities occurs at the input stage of the model.
Details of these four types are discussed in Section <a href="#S3" title="3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Summary of our contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To the best of our knowledge, this is the first work that explicitly identifies the four broad architecture types: Type - A, B, C, and D. Figure <a href="#S0.F2" title="Figure 2 ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the taxonomy of multimodal model architectures. We associate state-of-the-art models with these types, and outline their advantages and disadvantages, thereby facilitating a simplified comprehension, visualization and selection of multimodal model architectures.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Furthermore, our work also underscores the principal architectural types involved in constructing any-to-any modality multimodal models, which cannot be found in other survey works like <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Caffagni et al. [<a href="#bib.bib25" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib26" title="" class="ltx_ref">2023b</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Guo et al. [<a href="#bib.bib28" title="" class="ltx_ref">2023</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">To facilitate model selection, this study highlights the advantages and disadvantages of each architecture type, considering factors such as training data and compute requirements, architecture complexity, scalability, ease of integrating modalities, and any-to-any modality capability.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we list and discuss existing survey literature encompassing multimodal learning, multimodal data, and multimodal large language models (Table <a href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Our work illuminates a spectrum of multimodal architecture types through an examination of numerous multimodal works, which is absent in other survey literature.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">List of survey works related to recent multimodal developments.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Multimodal Survey Article</span></th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.2.1" class="ltx_tr">
<td id="S2.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">MM-LLMs: Recent Advances in MultiModal LLMs <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>
</td>
<td id="S2.T1.4.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">2024</td>
</tr>
<tr id="S2.T1.4.3.2" class="ltx_tr">
<td id="S2.T1.4.3.2.1" class="ltx_td ltx_align_left">A Survey on Multimodal Large Language Models <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>
</td>
<td id="S2.T1.4.3.2.2" class="ltx_td ltx_align_left">2024</td>
</tr>
<tr id="S2.T1.4.4.3" class="ltx_tr">
<td id="S2.T1.4.4.3.1" class="ltx_td ltx_align_left">The (R)Evolution of Multimodal LLMs: A Survey <cite class="ltx_cite ltx_citemacro_cite">Caffagni et al. [<a href="#bib.bib25" title="" class="ltx_ref">2024</a>]</cite>
</td>
<td id="S2.T1.4.4.3.2" class="ltx_td ltx_align_left">2024</td>
</tr>
<tr id="S2.T1.4.5.4" class="ltx_tr">
<td id="S2.T1.4.5.4.1" class="ltx_td ltx_align_left">Large-scale Multi-modal Pre-trained Models: A Survey <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib26" title="" class="ltx_ref">2023b</a>]</cite>
</td>
<td id="S2.T1.4.5.4.2" class="ltx_td ltx_align_left">2023</td>
</tr>
<tr id="S2.T1.4.6.5" class="ltx_tr">
<td id="S2.T1.4.6.5.1" class="ltx_td ltx_align_left">Multimodal Large Language Models: A Survey <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite>
</td>
<td id="S2.T1.4.6.5.2" class="ltx_td ltx_align_left">2023</td>
</tr>
<tr id="S2.T1.4.7.6" class="ltx_tr">
<td id="S2.T1.4.7.6.1" class="ltx_td ltx_align_left">Multimodal Learning With Transformers: A Survey <cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib29" title="" class="ltx_ref">2023</a>]</cite>
</td>
<td id="S2.T1.4.7.6.2" class="ltx_td ltx_align_left">2023</td>
</tr>
<tr id="S2.T1.4.8.7" class="ltx_tr">
<td id="S2.T1.4.8.7.1" class="ltx_td ltx_align_left ltx_border_bb">A Survey on Image-text Multimodal Models <cite class="ltx_cite ltx_citemacro_cite">Guo et al. [<a href="#bib.bib28" title="" class="ltx_ref">2023</a>]</cite>
</td>
<td id="S2.T1.4.8.7.2" class="ltx_td ltx_align_left ltx_border_bb">2023</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multimodal LLMs:</span>
<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>, examines recent advancements in Multimodal Large Language Models (MLLMs), and explores NNs created by enhancing LLMs with mixed modality.
It assesses the performance of mainstream MLLMs across 18 vision-language benchmarks.
While it offers a comprehensive overview of the general architecture of MLLMs, it notably overlooks the critical inclusion of Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> multimodal model architecture.
Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> is an emerging and popular multimodal model architecture type for developing any-to-any modality models.
<cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>, similar to <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>, offers insights into the intricate details of typical multimodal model architecture. However, it too lacks discussion on the Type-D multimodal architecture.
It thoroughly presents details of pretraining, finetuning, and alignment methods, as well as the data used for Multimodal Large Language Models (MLLMs).
<cite class="ltx_cite ltx_citemacro_cite">Caffagni et al. [<a href="#bib.bib25" title="" class="ltx_ref">2024</a>]</cite>, shows a general multimodal model architecture, provides a comprehensive inventory of the components present in multimodal architectures, encompassing a diverse range of LLM variants, vision encoders, and vision-to-language connectors/adapters.
It also compares these SOTA multimodal models on 14 multimodal benchmarks.
Multimodal models tailored to specific domains, such as document understanding, medical vision learning, autonomous driving, and embodied AI, are discussed, but it too noticeably lacks information about Type-D multimodal architecture.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Multimodal foundational models, training tasks, data and challenges:</span>
Foundational multimodal models are explored in <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite>.
It extensively describes multimodal tasks and its related datasets for training the foundational multimodal models.
<cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite> lacks details about the architectures of these models.
The survey work <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib26" title="" class="ltx_ref">2023b</a>]</cite>, contains details of the models till 2022 and reviews multimodal pretraining datasets, pretraining tasks, pretrained model architectures and downstream multimodal tasks.
Since the work only explores models till 2022, it lacks details of multimodal model architectures, which are currently prevalent.
<cite class="ltx_cite ltx_citemacro_cite">Baltrušaitis et al. [<a href="#bib.bib30" title="" class="ltx_ref">2018</a>]</cite>, highlights challenges in multimodal machine learning.
It enumerates five multimodal challenges: representation, translation, alignment, fusion, and co-learning.
Discussion about different types of model architectures is absent in this work.
<cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib29" title="" class="ltx_ref">2023</a>]</cite>, analyzes transformer model architecture from the perspective of multimodal learning.
It lists various self-attention variants for input multimodal data/embeddings fusion.
Based on the pretraining loss function, it categorizes the pretraining tasks used for multimodal learning with transformers.
Similar to <cite class="ltx_cite ltx_citemacro_cite">Baltrušaitis et al. [<a href="#bib.bib30" title="" class="ltx_ref">2018</a>]</cite>, it highlights challenges of multimodality fusion and alignment.
<cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib29" title="" class="ltx_ref">2023</a>]</cite> too fails to discuss about different types of multimodal model architectures.
<cite class="ltx_cite ltx_citemacro_cite">Guo et al. [<a href="#bib.bib28" title="" class="ltx_ref">2023</a>]</cite>, specifically talks about models for image and text modalities.
It investigates developments that have directly or indirectly influenced the evolution of multimodal large language models.
It lists various image-text multimodal tasks and matches the widely used state-of-the-art models (till October 2023) with each of these tasks.
The multimodal tasks include image captioning, visual reasoning, visual grounding and text-to-image generation.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multimodal Model Architectures: A Taxonomy</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The fusion of multimodal inputs into the deep neural network through various methods results in a range of architectural configurations.
This study analyzes model architectures with mixed modalities and categorizes them into four distinct types based on the fusion of modalities.
Two overarching categories are discernible: <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Deep Fusion</span>, wherein the fusion of modalities occurs within the internal layers of the model, and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">Early Fusion</span>, characterized by the fusion of modalities at the model’s input.
Within each category, we observe two primary clusters.
In the domain of Deep Fusion, the integration of modalities with internal layers manifests in: Type-A which employs standard cross-attention layers, and Type-B which utilizes custom-designed layers.
Conversely, in the domain of Early Fusion, multimodal inputs take two principal forms: non-tokenized<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>No discrete tokenization</span></span></span> multimodal inputs as Type C, and discretely tokenized multimodal inputs as Type-D.
These inputs are directly supplied to the input of the transformer model for early fusion, which can be either a decoder-only or an encoder-decoder style.
Therefore, we define four distinct types of multimodal model architectures within the current landscape of multimodal models: Type-A <a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, Type-B <a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, Type-C <a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> and Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Section <a href="#S3" title="3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> comprehensively outlines each architecture type, including information on their training data and computational requirements.
Figure <a href="#S0.F2" title="Figure 2 ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> maps together the multimodal model architecture types and the corresponding SOTA multimodal models.
Figure <a href="#S0.F1" title="Figure 1 ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the timeline of multimodal model development.
The 4 types, Type-A, Type-B, Type-C and Type-D are described in Section <a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, <a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, <a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> respectively.
Advantages and disadvantages of each multimodal model architecture type is listed in Section <a href="#S4" title="4 Advantages and Disadvantages ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Type-A: Standard Cross-Attention based Deep Fusion (SCDF)</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.17927/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Type-A multimodal model architecture. The input modalities are deeply fused into the internal layers of the LLM using standard cross-attention layer. The cross-attention can be added either before (sub-type A.1) or after (sub-type A.2) the self-attention layer. Modality-specific encoders process the different input modalities. A resampler is used to output a fixed number of modality (visual/audio/video) tokens, given a variable number of input tokens at the input.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Type-A architecture mostly comprise of early multimodal models.
Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a general Type-A model architecture.
It typically involves a pre-trained LLM and integrating standard cross-attention layers into its internal architecture to achieve deep fusion of input modalities.
The multimodal data (image/audio/video) is fed through modality specific encoders.
A resampler is used to generate a fixed number of tokens that aligns with the requirements of the decoder layer.
These resampler outputs are then directed to the internal layers of the LLM using cross-attention layers.
The Type-A models exhibit a dichotomy in this aspect – the cross-attention layer can be added before or after the self-attention layer in the model’s architecture.
This pre- and post-introduction of cross-attention w.r.t. self-attention in the LLM, results in the emergence of two distinct model architecture subtypes within Type-A.
Section <a href="#S3.SS1.SSS1" title="3.1.1 Subtype A.1 ‣ 3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a> and <a href="#S3.SS1.SSS2" title="3.1.2 Subtype A.2 ‣ 3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a> details more about each sub-type.
Models belonging to this architecture type include Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, OpenFlamingo (open-source replication of Flamingo) <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. [<a href="#bib.bib10" title="" class="ltx_ref">2023</a>]</cite>, Otter (trained on MIMIC-IT dataset on top of OpenFlamingo) <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>]</cite>, MultiModal-GPT (derived from OpenFlamingo) <cite class="ltx_cite ltx_citemacro_cite">Gong et al. [<a href="#bib.bib12" title="" class="ltx_ref">2023</a>]</cite>, PaLI-X <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib31" title="" class="ltx_ref">2023b</a>]</cite>, IDEFICS (open-access reproduction of Flamingo) <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite>, Dolphins (based on OpenFlamingo architecture) <cite class="ltx_cite ltx_citemacro_cite">Ma et al. [<a href="#bib.bib14" title="" class="ltx_ref">2023</a>]</cite>, VL-BART <cite class="ltx_cite ltx_citemacro_cite">Cho et al. [<a href="#bib.bib32" title="" class="ltx_ref">2021</a>]</cite> and VL-T5 <cite class="ltx_cite ltx_citemacro_cite">Cho et al. [<a href="#bib.bib32" title="" class="ltx_ref">2021</a>]</cite>.
At present, models with this architecture commonly engage in processing image and text modalities, subsequently producing textual outputs.
Pretraining necessitates a substantial volume of data samples and computational resources, as observed in the resource-intensive implementations of Flamingo, OpenFlamingo, PaLI-X, and IDEFICS.
Fine-tuning and/or instruction tuning these NNs can be achieved with minimal computational resources, a characteristic shared by the multimodal architectures discussed in this study.
This trend is evident in the Otter, Multimodal-GPT, and Dolphins models, where fine-tuning or instruction tuning is exclusively performed using specific data, leveraging limited computational resources (typically less than or equal to 8 A100 GPUs).
The comparative advantages and disadvantages of the Type-A multimodal model architecture, in relation to Types B, C, and D, are detailed in Section<a href="#S4.SS1" title="4.1 Type-A ‣ 4 Advantages and Disadvantages ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Subtype A.1</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the model architecture sub-type, featuring the cross-attention layers <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">before</span> each self-attention layer within the decoder (LLM).
Models belonging to this sub-group include Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, OpenFlamingo <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. [<a href="#bib.bib10" title="" class="ltx_ref">2023</a>]</cite>, Otter <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>]</cite>, MultiModal-GPT <cite class="ltx_cite ltx_citemacro_cite">Gong et al. [<a href="#bib.bib12" title="" class="ltx_ref">2023</a>]</cite>, PaLI-X <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib31" title="" class="ltx_ref">2023b</a>]</cite>, IDEFICS <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite> and Dolphins <cite class="ltx_cite ltx_citemacro_cite">Ma et al. [<a href="#bib.bib14" title="" class="ltx_ref">2023</a>]</cite>.
Flamingo and its derivative multimodal models generally belong to this architectural sub-type.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p"><span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
<span id="S3.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_bold">Flamingo</span> model is trained with next-text-token prediction objective, where the input consists of interleaved image and text pairs and the model outputs text.
For pretraining, it uses M3W <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>, ALIGN <cite class="ltx_cite ltx_citemacro_cite">Jia et al. [<a href="#bib.bib33" title="" class="ltx_ref">2021</a>]</cite>, LTIP (Long Text &amp; Image Pairs) <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite> and VTP (Video &amp; Text Pairs) <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>.
For finetuning, VQAV2 <cite class="ltx_cite ltx_citemacro_cite">Antol et al. [<a href="#bib.bib34" title="" class="ltx_ref">2015</a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib35" title="" class="ltx_ref">2015</a>]</cite>, VATEX <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib36" title="" class="ltx_ref">2019</a>]</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. [<a href="#bib.bib37" title="" class="ltx_ref">2018</a>]</cite>, MSRVTTQA <cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib38" title="" class="ltx_ref">2017</a>]</cite>, VisDial <cite class="ltx_cite ltx_citemacro_cite">Das et al. [<a href="#bib.bib39" title="" class="ltx_ref">2017</a>]</cite>, YouCook2 <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. [<a href="#bib.bib40" title="" class="ltx_ref">2018</a>]</cite>, and TextVQA <cite class="ltx_cite ltx_citemacro_cite">Singh et al. [<a href="#bib.bib41" title="" class="ltx_ref">2019</a>]</cite> datasets were used.
<span id="S3.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_bold">OpenFlamingo</span> models are also pretrained with the next-text-token prediction objective using 60M interleaved (MMC4 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib42" title="" class="ltx_ref">2024a</a>]</cite>) examples and 120M LAION-2B <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. [<a href="#bib.bib43" title="" class="ltx_ref">2022</a>]</cite> examples <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. [<a href="#bib.bib10" title="" class="ltx_ref">2023</a>]</cite>.
It uses similar finetuning datasets as Flamingo.
<span id="S3.SS1.SSS1.p2.1.4" class="ltx_text ltx_font_bold">Otter</span> created MIMIC-IT <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>]</cite> dataset, and trained OpenFlamingo model on it.
This work improves OpenFlamingo’s instruction-following and in-context learning ability.
<span id="S3.SS1.SSS1.p2.1.5" class="ltx_text ltx_font_bold">MultiModal-GPT</span> finetunes OpenFlamingo by adding LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. [<a href="#bib.bib44" title="" class="ltx_ref">2021</a>]</cite> weights to the LLM layers.
This model too is trained using next-text-token prediction objective.
Language training datasets include Dolly-15k and Alpaca-GPT4 <cite class="ltx_cite ltx_citemacro_cite">Peng et al. [<a href="#bib.bib45" title="" class="ltx_ref">2023</a>]</cite>.
Vision-language datasets encompass A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">Schwenk et al. [<a href="#bib.bib46" title="" class="ltx_ref">2022</a>]</cite>, COCO Caption <cite class="ltx_cite ltx_citemacro_cite">Karpathy and Fei-Fei [<a href="#bib.bib47" title="" class="ltx_ref">2015</a>]</cite>, OCR-VQA <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. [<a href="#bib.bib48" title="" class="ltx_ref">2019</a>]</cite> and data from LLaVA <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib49" title="" class="ltx_ref">2024a</a>]</cite> &amp; Mini-GPT4 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib50" title="" class="ltx_ref">2023a</a>]</cite>.
<span id="S3.SS1.SSS1.p2.1.6" class="ltx_text ltx_font_bold">PaLI-X</span> can process image, text and video inputs.
Image and video inputs are encoded with ViT encoder.
The text inputs and encoded image &amp; video outputs are then processed by an encoder-decoder style transformer model.
Most of the pretraining tasks (with the exception of the masked image token task) predict text-only output from the multimodal input <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib31" title="" class="ltx_ref">2023b</a>]</cite>.
Pretraining datasets include WebLI (image-text pairs) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib51" title="" class="ltx_ref">2022</a>]</cite>, CC3M <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. [<a href="#bib.bib52" title="" class="ltx_ref">2018</a>]</cite> and VTP (video data) datasets.
Finetuning datasets include COCO (Karpathy split) <cite class="ltx_cite ltx_citemacro_cite">Karpathy and Fei-Fei [<a href="#bib.bib47" title="" class="ltx_ref">2015</a>]</cite>, NoCaps <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. [<a href="#bib.bib53" title="" class="ltx_ref">2019</a>]</cite>, VQAv2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. [<a href="#bib.bib54" title="" class="ltx_ref">2017</a>]</cite>, OKVQA <cite class="ltx_cite ltx_citemacro_cite">Marino et al. [<a href="#bib.bib55" title="" class="ltx_ref">2019</a>]</cite>, TallyQA <cite class="ltx_cite ltx_citemacro_cite">Acharya et al. [<a href="#bib.bib56" title="" class="ltx_ref">2019</a>]</cite>, VizWizCap <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. [<a href="#bib.bib57" title="" class="ltx_ref">2020</a>]</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_cite">Singh et al. [<a href="#bib.bib41" title="" class="ltx_ref">2019</a>]</cite>, STVQA <cite class="ltx_cite ltx_citemacro_cite">Biten et al. [<a href="#bib.bib58" title="" class="ltx_ref">2019</a>]</cite>, OCRVQA <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. [<a href="#bib.bib48" title="" class="ltx_ref">2019</a>]</cite>, InfoVQA <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. [<a href="#bib.bib59" title="" class="ltx_ref">2022</a>]</cite>, DocVQA <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. [<a href="#bib.bib60" title="" class="ltx_ref">2021</a>]</cite>, ChartQA <cite class="ltx_cite ltx_citemacro_cite">Masry et al. [<a href="#bib.bib61" title="" class="ltx_ref">2022</a>]</cite>, MSR-VTT <cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib62" title="" class="ltx_ref">2016</a>]</cite>, Activity-Net <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. [<a href="#bib.bib63" title="" class="ltx_ref">2017a</a>]</cite>, VATEX <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib36" title="" class="ltx_ref">2019</a>]</cite>, SMIT <cite class="ltx_cite ltx_citemacro_cite">Monfort et al. [<a href="#bib.bib64" title="" class="ltx_ref">2021</a>]</cite>, NExT-QA <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. [<a href="#bib.bib65" title="" class="ltx_ref">2021</a>]</cite>.
<span id="S3.SS1.SSS1.p2.1.7" class="ltx_text ltx_font_bold">IDEFICS</span> was trained on Wikipedia (<cite class="ltx_cite ltx_citemacro_cite">Heafield [<a href="#bib.bib66" title="" class="ltx_ref">2011</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib67" title="" class="ltx_ref">2022</a>]</cite>), Public Multimodal Dataset <cite class="ltx_cite ltx_citemacro_cite">Singh et al. [<a href="#bib.bib68" title="" class="ltx_ref">2022</a>]</cite>, LAION <cite class="ltx_cite ltx_citemacro_cite">Webster et al. [<a href="#bib.bib69" title="" class="ltx_ref">2023</a>]</cite>, and on a new 115B token dataset called OBELICS <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite>.
It follows Flamingo architecture style.
LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. [<a href="#bib.bib70" title="" class="ltx_ref">2023</a>]</cite> is used as LLM and OpenClip as the vision encoder.
<span id="S3.SS1.SSS1.p2.1.8" class="ltx_text ltx_font_bold">Dolphins</span> is based on OpenFlamingo.
It is trained on driving data.
This work utilizes BDD-X <cite class="ltx_cite ltx_citemacro_cite">Kim et al. [<a href="#bib.bib71" title="" class="ltx_ref">2018</a>]</cite> to establish instruction dataset, focusing on four key AV (autonomous vehicle) tasks like behavior comprehension, control signal forecasting, behavior analysis, and in-depth conversation <cite class="ltx_cite ltx_citemacro_cite">Ma et al. [<a href="#bib.bib14" title="" class="ltx_ref">2023</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p"><span id="S3.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
For <span id="S3.SS1.SSS1.p3.1.2" class="ltx_text ltx_font_bold">Flamingo</span>, all training and evaluations were performed on TPUv4 instances. The
largest model containing 80 billion parameters was trained on 1536 chips for 15 days and sharded across 16 device <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. [<a href="#bib.bib1" title="" class="ltx_ref">2022</a>]</cite>.
<span id="S3.SS1.SSS1.p3.1.3" class="ltx_text ltx_font_bold">OpenFlamingo</span> was trained using 64 A100 GPUs.
While <span id="S3.SS1.SSS1.p3.1.4" class="ltx_text ltx_font_bold">Otter</span> only utilizes 1 A100 GPU or 4 RTX-3090 GPU, to instruction tune on MIMIC-IT <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>]</cite> dataset.
The Otter models undergo solely instruction tuning without any pretraining process.
<span id="S3.SS1.SSS1.p3.1.5" class="ltx_text ltx_font_bold">MultiModal-GPT</span> uses 8 A100 GPUs for finetuning training.
The MultiModal-GPT model does not undergo pretraining.
For <span id="S3.SS1.SSS1.p3.1.6" class="ltx_text ltx_font_bold">PaLI-X</span>, no resource detail are provided.
<span id="S3.SS1.SSS1.p3.1.7" class="ltx_text ltx_font_bold">IDEFICS</span>’s 9B-parameter models is trained on OBELICS-only <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite> and LAION-only <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. [<a href="#bib.bib43" title="" class="ltx_ref">2022</a>]</cite> datasets using 32 A100 (80GB) GPUs, and on OBELICS + LAION using 64 (80GB) A100s, for approximately 6 days.
IDEFICS’s large model is trained using 512 80GB A100 GPUs <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib13" title="" class="ltx_ref">2024a</a>]</cite>.
<span id="S3.SS1.SSS1.p3.1.8" class="ltx_text ltx_font_bold">Dolphins</span> is fully trained using 4 NVIDIA A100 GPUs.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Subtype A.2</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">This sub-type consists of a standard encoder-decoder transformer architecture, featuring a cross-attention layer placed <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">after</span> each self-attention layer within the decoder.
Generally, language models follow a decoder-only structure.
However, BART and T5 adopt an encoder-decoder style transformer architecture.
Unlike sub-type A.1, sub-type A.2 does not utilize resampler in its architecture.
Research work <cite class="ltx_cite ltx_citemacro_cite">Cho et al. [<a href="#bib.bib32" title="" class="ltx_ref">2021</a>]</cite>, extended these language models to build mixed modality models named VL-BART and VL-T5.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
VL-T5 and VL-BART are designed to handle input comprising of images and text, and produces textual outputs.
Visual embeddings from image are obtained using a CNN model.
The image embeddings and text tokens are given as input to the encoder of the BART/T5.
The model outputs text based on the task mentioned in the text input.
A standard next-text-token prediction objective is used for training.
Pretraining data includes, MS-COCO (<cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib72" title="" class="ltx_ref">2014</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib35" title="" class="ltx_ref">2015</a>]</cite>), Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. [<a href="#bib.bib73" title="" class="ltx_ref">2017b</a>]</cite>, VQA-v2.0 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. [<a href="#bib.bib54" title="" class="ltx_ref">2017</a>]</cite>, GQA balanced version <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning [<a href="#bib.bib74" title="" class="ltx_ref">2019</a>]</cite>, and Visual7W <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib75" title="" class="ltx_ref">2016</a>]</cite>.
Finetuning data encompasses VQA <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. [<a href="#bib.bib54" title="" class="ltx_ref">2017</a>]</cite>, GQA, NLVR<sup id="S3.SS1.SSS2.p2.1.2" class="ltx_sup">2</sup> <cite class="ltx_cite ltx_citemacro_cite">Suhr et al. [<a href="#bib.bib76" title="" class="ltx_ref">2018</a>]</cite>, RefCOCOg <cite class="ltx_cite ltx_citemacro_cite">Mao et al. [<a href="#bib.bib77" title="" class="ltx_ref">2016</a>]</cite>, COCO Caption Karpathy <cite class="ltx_cite ltx_citemacro_cite">Karpathy and Fei-Fei [<a href="#bib.bib47" title="" class="ltx_ref">2015</a>]</cite> and Multi30K <cite class="ltx_cite ltx_citemacro_cite">Elliott et al. [<a href="#bib.bib78" title="" class="ltx_ref">2016</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p"><span id="S3.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
VL-T5 and VL-BART are pretrained on 4 RTX 2080 Ti GPUs, for 4 days.
Total 30 training epochs and batch size of 320 and 600 were used for VL-T5 and VL-BART respectively.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Type-B: Custom Layer based Deep Fusion (CLDF)</h3>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2405.17927/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Type-B multimodal model architecture. The input modalities are deeply fused into the internal layers of the LLM using custom-designed layers. Custom cross-attention layers (sub-type A.1) or other custom layers (sub-type A.2) are used for modality fusion. A Linear Layer/MLP/Q-former is used to align different modalities with the decoder layer.</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Type-B architecture is built using a pretrained LLM, learnable linear layer/MLP/Q-former, custom-cross-attention-layers or custom-layers and modality encoders.
The difference between Type-A and Type-B architecture types is that,
<span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">in Type-A, a standard cross-attention layer is utilized,
while in Type-B, a custom-designed layer is or can be used</span>.
Similar to Type-A, in Type-B the input modalities are deeply fused into the internal layers of the model.
Example, LLaMA-Adapter-V2 model adds learnable embeddings to the output of the cross-attention layer before adding/concatenating the cross-attention output to the self-attention layer output.
Additionally, a learnable gating factor is included to control the contribution of the cross-attention layer on the output of self-attention layer, which is not typically done in Type-A architecture.
Models belonging to Type-B include LLaMA-Adapter <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib15" title="" class="ltx_ref">2023a</a>]</cite>, LLaMA-Adapter-V2 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. [<a href="#bib.bib16" title="" class="ltx_ref">2023</a>]</cite>, CogVLM <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>]</cite>, mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib18" title="" class="ltx_ref">2023a</a>]</cite>, CogAgent <cite class="ltx_cite ltx_citemacro_cite">Hong et al. [<a href="#bib.bib79" title="" class="ltx_ref">2023</a>]</cite>, InternVL <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>]</cite>, MM-Interleaved <cite class="ltx_cite ltx_citemacro_cite">Tian et al. [<a href="#bib.bib20" title="" class="ltx_ref">2024</a>]</cite>, CogCoM <cite class="ltx_cite ltx_citemacro_cite">Qi et al. [<a href="#bib.bib80" title="" class="ltx_ref">2024</a>]</cite>, InternLM-XComposer2 <cite class="ltx_cite ltx_citemacro_cite">Dong et al. [<a href="#bib.bib81" title="" class="ltx_ref">2024</a>]</cite>, MoE-LLaVA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib21" title="" class="ltx_ref">2024</a>]</cite>, and LION <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib82" title="" class="ltx_ref">2023c</a>]</cite>.
Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows general Type-B multimodal model architecture.
Two architecture sub-types exists for Type-B.
Sub-type B.1 <a href="#S3.SS2.SSS1" title="3.2.1 Sub-type B.1: Custom Cross-Attention Layer ‣ 3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> adds custom-cross-attention layer to the internal layers of the LLM, while the sub-type B.2 <a href="#S3.SS2.SSS2" title="3.2.2 Sub-type B.2: Custom Learnable Layer ‣ 3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> uses custom learnable layer other than cross-attention layer.
Sections <a href="#S3.SS2.SSS1" title="3.2.1 Sub-type B.1: Custom Cross-Attention Layer ‣ 3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> and <a href="#S3.SS2.SSS2" title="3.2.2 Sub-type B.2: Custom Learnable Layer ‣ 3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> provide more details about sub-type B.1 and B.2 respectively.
Models with multimodal input and text output dominate the Type-B architecture, similar to Type-A.
If a Mixture-of-Experts layer (<cite class="ltx_cite ltx_citemacro_cite">Jacobs et al. [<a href="#bib.bib83" title="" class="ltx_ref">1991</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Eigen et al. [<a href="#bib.bib84" title="" class="ltx_ref">2013</a>]</cite>) is used in the architecture, an auxilary loss is added to the standard auto-regressive loss (next-text-token prediction task) for load balancing.
The comparative advantages and disadvantages of the Type-B multimodal model architecture, in relation to Types A, C, and D, are detailed in Section<a href="#S4.SS2" title="4.2 Type-B ‣ 4 Advantages and Disadvantages ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Sub-type B.1: Custom Cross-Attention Layer</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Here, the multimodal models are built through the process of using pretrained LLM and adding a <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">custom cross-attention layer</span> to the internal layers of the decoder.
Models include LLaMA-Adapter <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib15" title="" class="ltx_ref">2023a</a>]</cite>, LLaMA-Adapter-v2 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. [<a href="#bib.bib16" title="" class="ltx_ref">2023</a>]</cite>, CogVLM <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>]</cite>, mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib18" title="" class="ltx_ref">2023a</a>]</cite>, CogAgent <cite class="ltx_cite ltx_citemacro_cite">Hong et al. [<a href="#bib.bib79" title="" class="ltx_ref">2023</a>]</cite>, InternVL <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>]</cite>, MM-Interleaved <cite class="ltx_cite ltx_citemacro_cite">Tian et al. [<a href="#bib.bib20" title="" class="ltx_ref">2024</a>]</cite>, and CogCoM <cite class="ltx_cite ltx_citemacro_cite">Qi et al. [<a href="#bib.bib80" title="" class="ltx_ref">2024</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p"><span id="S3.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">CogVLM</span> learns separate query (Q), key (K), and value (V) embeddings for text and images in each decoder layer.
These Q, K, V are initialized to same value as LLM at start of the training.
A visual expert module processes the encoder outputs, and then inputs it to the custom cross-attention layers in the decoder (LLM).
While <span id="S3.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_bold">mPLUG-Owl2</span>, introduces a custom-cross-attention layer in which a common Q is learnt for separate K and V for each modality.
This custom-cross-attention layer is called as ‘Modality adaptive module’ in this architecture.
<span id="S3.SS2.SSS1.p2.1.3" class="ltx_text ltx_font_bold">MM-Interleaved</span>, introduces a custom-cross-attention layer called as MMFS (Multi-scale Multi-image Feature Synchronizer).
This feature synchronizer is added after self-attention in each layer of the LLM.
In <span id="S3.SS2.SSS1.p2.1.4" class="ltx_text ltx_font_bold">LLaMA-Adapter</span> and <span id="S3.SS2.SSS1.p2.1.5" class="ltx_text ltx_font_bold">LLaMA-Adapter-V2</span>, a custom-cross-attention layer called as ‘adapter’ is used.
The Q in this cross-attention layer are learnable embeddings, and K, V are from the encoder outputs (after linear layer).
Additional learnable embeddings are added to the output of the cross-attention before sending it to the decoder layer.
This custom-cross-attention layer output is concatenated to the output of decoder layer.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p"><span id="S3.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
<span id="S3.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_bold">LLaMA-Adapter</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib15" title="" class="ltx_ref">2023a</a>]</cite> adds learnable prompts to layers of LLM for instruction following and multimdoal learning.
It uses standard next-text-token prediction objective for instruction tuning.
The input consists of both text and image modalities, whereas the output is limited to text only.
Dataset of size 52K samples is used to instruction tune LLaMA-Adapter.
<span id="S3.SS2.SSS1.p3.1.3" class="ltx_text ltx_font_bold">LLaMA-Adapter-V2</span> <cite class="ltx_cite ltx_citemacro_cite">Gao et al. [<a href="#bib.bib16" title="" class="ltx_ref">2023</a>]</cite> further extends LLaMA-Adapter to create a visual instruction model.
The input comprises text and images, while the output consists solely of text.
This model too is trained on standard next-text-token prediction task.
It uses 52K language instruction data <cite class="ltx_cite ltx_citemacro_cite">Peng et al. [<a href="#bib.bib45" title="" class="ltx_ref">2023</a>]</cite> &amp; 567K image-text captioning data from COCO caption dataset <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib35" title="" class="ltx_ref">2015</a>]</cite> and 80K conversation data collected by ShareGPT <cite class="ltx_cite ltx_citemacro_cite">ShareGPT [<a href="#bib.bib85" title="" class="ltx_ref">2023</a>]</cite> for finetuning/instruction tuning.
In <span id="S3.SS2.SSS1.p3.1.4" class="ltx_text ltx_font_bold">CogVLM</span>, an additional attention layer and FFN (Linear Layer/s) are added in parallel to the LLM’s self-attention and FFN layer, for learning image features. Model takes image and text as input, and outputs text.
Since output is text-only, this model too uses standard next-text-token prediction task for training.
Pretraining data include LAION-2B, COYO-700M <cite class="ltx_cite ltx_citemacro_cite">Byeon et al. [<a href="#bib.bib86" title="" class="ltx_ref">2022</a>]</cite>, LAION-115M <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib87" title="" class="ltx_ref">2023b</a>]</cite> and a subset
of LAION-400M is used.
Instruction tuning uses VQAv2, OKVQA, TextVQA, OCRVQA, ScienceQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib88" title="" class="ltx_ref">2022b</a>]</cite>, LLaVA-Instruct <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib89" title="" class="ltx_ref">2024a</a>]</cite>, LRV-Instruction <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib90" title="" class="ltx_ref">2023a</a>]</cite>, LLaVAR <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib91" title="" class="ltx_ref">2024b</a>]</cite>. Flickr30K Entities <cite class="ltx_cite ltx_citemacro_cite">Plummer et al. [<a href="#bib.bib92" title="" class="ltx_ref">2015</a>]</cite>, Ref-COCO <cite class="ltx_cite ltx_citemacro_cite">Kazemzadeh et al. [<a href="#bib.bib93" title="" class="ltx_ref">2014</a>]</cite>, Visual7W, VisualGenome and Grounded CoT-VQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib94" title="" class="ltx_ref">2023d</a>]</cite> datasets.
<span id="S3.SS2.SSS1.p3.1.5" class="ltx_text ltx_font_bold">mPLUG-Owl2</span> accepts both image and text inputs, producing text as output.
Training objective is the standard next-text-token prediction.
Pretraining datasets used are CC3M/CC12M <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. [<a href="#bib.bib95" title="" class="ltx_ref">2021</a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib72" title="" class="ltx_ref">2014</a>]</cite>, Laion-en <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. [<a href="#bib.bib43" title="" class="ltx_ref">2022</a>]</cite>, COYO <cite class="ltx_cite ltx_citemacro_cite">Byeon et al. [<a href="#bib.bib86" title="" class="ltx_ref">2022</a>]</cite>, DataComp <cite class="ltx_cite ltx_citemacro_cite">Gadre et al. [<a href="#bib.bib96" title="" class="ltx_ref">2024</a>]</cite>.
Visual Instruction tuning datasets include TextCaps <cite class="ltx_cite ltx_citemacro_cite">Sidorov et al. [<a href="#bib.bib97" title="" class="ltx_ref">2020</a>]</cite>, COCO, VQAv2, OKVQA, OCR-VQA, GQA, and A-OKVQA, Ref-COCO, VisualGenome, LLaVA-instruct-150K <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib89" title="" class="ltx_ref">2024a</a>]</cite>, ShareGPT-80K <cite class="ltx_cite ltx_citemacro_cite">ShareGPT [<a href="#bib.bib85" title="" class="ltx_ref">2023</a>]</cite>, SlimOrca <cite class="ltx_cite ltx_citemacro_cite">Lian et al. [<a href="#bib.bib98" title="" class="ltx_ref">2023</a>]</cite>.
<span id="S3.SS2.SSS1.p3.1.6" class="ltx_text ltx_font_bold">CogAgent</span> is trained using standard next-text-token prediction objective.
It processes image and text as input, and outputs text.
It is pretrained using datasets like LAION-2B, COYO, LAION-115M.
To train model for GUI grounding, CogAgent created CCS400K (Common Crawl Screenshot 400K) dataset <cite class="ltx_cite ltx_citemacro_cite">Hong et al. [<a href="#bib.bib79" title="" class="ltx_ref">2023</a>]</cite>.
Finetuning and alignment tuning uses Mind2Web <cite class="ltx_cite ltx_citemacro_cite">Deng et al. [<a href="#bib.bib99" title="" class="ltx_ref">2024</a>]</cite> and AITW <cite class="ltx_cite ltx_citemacro_cite">Rawles et al. [<a href="#bib.bib100" title="" class="ltx_ref">2024</a>]</cite>.
<span id="S3.SS2.SSS1.p3.1.7" class="ltx_text ltx_font_bold">InternVL</span>, similar to other models in this architecture type, takes image and text as input and outputs text.
Training progresses through three stages <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>]</cite>.
In stage 1, contrastive training is utilized to construct a 6 billion parameter vision model.
In stage 2, a standard generative training is used with InternViT and frozen QLLaMA for image captioning task.
In stage 3, supervised finetuning is done for visual QA and multimodal dialogue task.
Stage 1 and 2 uses LAION-en, LAION-multi <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. [<a href="#bib.bib43" title="" class="ltx_ref">2022</a>]</cite>,
COYO, Wukong <cite class="ltx_cite ltx_citemacro_cite">Gu et al. [<a href="#bib.bib101" title="" class="ltx_ref">2022</a>]</cite>, LAION-COCO <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. [<a href="#bib.bib43" title="" class="ltx_ref">2022</a>]</cite>, LAION-en, CC12M, CC3M, SBU <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. [<a href="#bib.bib102" title="" class="ltx_ref">2011</a>]</cite> datasets.
While stage 3 uses COCO Caption, TextCaps, VQAv2, OKVQA, A-OKVQA, IconQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib103" title="" class="ltx_ref">2021a</a>]</cite>, AI2D <cite class="ltx_cite ltx_citemacro_cite">Kembhavi et al. [<a href="#bib.bib104" title="" class="ltx_ref">2016</a>]</cite>, GQA, OCR-VQA, ChartQA, DocVQA, ST-VQA, EST-VQA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib105" title="" class="ltx_ref">2020a</a>]</cite>, InfoVQA, LLaVAR, Toloka <cite class="ltx_cite ltx_citemacro_cite">Ustalov et al. [<a href="#bib.bib106" title="" class="ltx_ref">2023</a>]</cite>, LLaVA-150K, SVIT <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib107" title="" class="ltx_ref">2023a</a>]</cite>, VisDial <cite class="ltx_cite ltx_citemacro_cite">Das et al. [<a href="#bib.bib39" title="" class="ltx_ref">2017</a>]</cite>, LRV-Instruction, LLaVA-Mix-665K <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib108" title="" class="ltx_ref">2023b</a>]</cite> datasets.
<span id="S3.SS2.SSS1.p3.1.8" class="ltx_text ltx_font_bold">MM-Interleaved</span>
model is trained end-to-end with next-text-token and next-image prediction task <cite class="ltx_cite ltx_citemacro_cite">Tian et al. [<a href="#bib.bib20" title="" class="ltx_ref">2024</a>]</cite>.
The architecture is made up of visual encoder, resampler, LLM, feature-synchronizer and a diffusion model.
Image and text are given as input, and model can output both image and text.
Model is pretrained on a mixture of image-text pairs and interleaved image-text sequences, including MMC4, LAION-2B, LAION-COCO, CC-12M and Objects365 <cite class="ltx_cite ltx_citemacro_cite">Shao et al. [<a href="#bib.bib109" title="" class="ltx_ref">2019</a>]</cite>.
Finetuning data include LLaVA-Mix-665K, COCO Caption, VQAv2, ChartQA, DocVQA, EST-VQA, InfoVQA, STVQA, TextCaps, LLaVAR, OCR-VQA, and DVQA, RefCOCO, RefCOCO+ <cite class="ltx_cite ltx_citemacro_cite">Mao et al. [<a href="#bib.bib77" title="" class="ltx_ref">2016</a>]</cite>, and RefCOCOg <cite class="ltx_cite ltx_citemacro_cite">Mao et al. [<a href="#bib.bib77" title="" class="ltx_ref">2016</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p"><span id="S3.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
LLaMA-Adapter uses 8 A100 GPUs. Only 1 hour of triaining required on 52K language instruction following dataset using LLaMA-7b.
LLaMA-Adapter-V2, since derived from LLaMA-Adapter, it too is trained using 8 A100 GPUs. Training time varies due to difference in training data sizes.
CogVLM, mPLUG-Owl2, CogAgent and MM-Interleaved did not explicitly include details about training resources in their work.
InternVL uses 640 A100 GPUs for stage 1 training, 160 A100 GPUs for stage 2 training and 8 to 32 A100 GPUs for stage 3 training.
</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Sub-type B.2: Custom Learnable Layer</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Instead of adding custom cross-attention layers to internal LLM layers, models using <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">custom learnable layers</span> belong to sub-type B.2 of Type-B multimodal model architecture.
Models incude InternLM-XComposer2 <cite class="ltx_cite ltx_citemacro_cite">Dong et al. [<a href="#bib.bib81" title="" class="ltx_ref">2024</a>]</cite>, MoE-LLaVA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib21" title="" class="ltx_ref">2024</a>]</cite> and LION <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib82" title="" class="ltx_ref">2023c</a>]</cite>.
<span id="S3.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">InternLM-XComposer2</span> adds LoRA weights to LLM layers for learning image modality.
The LoRA weights are added in parallel to each decoder layer and only processes image tokens.
No cross-attention layer is added in the decoder layers to build this model.
<span id="S3.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_bold">MoE-LLaVA</span> model is built on top of LLaVA multimodal model (LLaVA model belongs to Type-C).
In addition to the changes that exists in LLaVA model, in MoE-LLaVA each decoder layer of LLaVA is modified to create MoE-LLaVA model.
In each decoder layer, the FFN layer is modified to create Mixture-of-Expert (MoE) layer.
MoE layer consists of a router and multiple FFN layers in parallel.
<span id="S3.SS2.SSS2.p1.1.4" class="ltx_text ltx_font_bold">LION</span> modifies the FFN layer of each decoder layer.
The learnable module ’Mixture of adapters with routers’ is added in parallel to the FFN layers.
This module consists of LoRA and MoE layer for learning image modality features.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
<span id="S3.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_bold">InternLM-XComposer2</span> can process image and text as input, and outputs text.
The model is trained using a standard next-text token prediction task.
Its training process involves pretraining, finetuning and instruction tuning.
Pretraining datasets include ShareGPT4V-PT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib110" title="" class="ltx_ref">2023e</a>]</cite>, COCO, Nocaps, TextCaps, LAION-400M, SBU, CC-3M, Concept Data <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>]</cite>, WanJuan <cite class="ltx_cite ltx_citemacro_cite">He et al. [<a href="#bib.bib112" title="" class="ltx_ref">2023</a>]</cite>, Flicker, MMC-Instruction <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib113" title="" class="ltx_ref">2023c</a>]</cite>.
Finetuning data encompasses ShareGPT4V, COCO, Nocaps, VQAv2, GQA, OK-VQA, Science QA, AI2D, SQA, DVQA, ChartQA, MathQA <cite class="ltx_cite ltx_citemacro_cite">Amini et al. [<a href="#bib.bib114" title="" class="ltx_ref">2019</a>]</cite>, Geometry3K <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib115" title="" class="ltx_ref">2021b</a>]</cite>, A-OKVQA, KVQA <cite class="ltx_cite ltx_citemacro_cite">Shah et al. [<a href="#bib.bib116" title="" class="ltx_ref">2019</a>]</cite>, LLaVA-150k, LVIS-Instruct4V <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib117" title="" class="ltx_ref">2023c</a>]</cite>.
Instruction tuning data include LLaVA-150k, LVIS-Instruct4V,
ShareGPT-en&amp;zh <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. [<a href="#bib.bib118" title="" class="ltx_ref">2023</a>]</cite>, InternLM-Chat <cite class="ltx_cite ltx_citemacro_cite">Team [<a href="#bib.bib119" title="" class="ltx_ref">2023</a>]</cite>.
<span id="S3.SS2.SSS2.p2.1.3" class="ltx_text ltx_font_bold">MoE-LLaVA</span> too processes image and text as input, and outputs text.
An auxilary loss related to the mixture-of-experts load balancing is added to the standard auto-regressive loss.
Training consists of three stages: one pretraining and two finetuning stages.
Data used for pretraining is LLaVA 1.5-558k <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib108" title="" class="ltx_ref">2023b</a>]</cite> dataset.
First finetuning stage uses SViT-157k <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib107" title="" class="ltx_ref">2023a</a>]</cite>, LVIS-220k, LRV-331k <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib90" title="" class="ltx_ref">2023a</a>]</cite>, and MIMIC-IT-256k datasets.
Second finetuning stage uses LLaVA 1.5-mix-665k dataset.
<span id="S3.SS2.SSS2.p2.1.4" class="ltx_text ltx_font_bold">LION</span> receives input in the form of images and text, and produces text as its output.
It uses standard next-text-token prediction objective for training.
LoRA weights and MoE layer are added to LLM layers for learning image modality.
Training data includes LLaVA-Instruct-150K , OKVQA, A-OKVQA, VQAv2, OCR-VQA, COCO, TextCaps and Visual Genome datasets.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p"><span id="S3.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
All MoE-LLaVA training steps use 8 A800 (80G) GPUs.
InternLM-XComposer2 and LION does not explicitly provide resource details in their work.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Type-C: Non-Tokenized Early Fusion (NTEF)</h3>

<figure id="S3.F5" class="ltx_figure"><img src="/html/2405.17927/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Type-C multimodal model architecture. The (non-tokenized) input modalities are directly fed to the model at its input, rather than to its internal layers, resulting in early fusion. Different types of modules are used to connect modality encoder outputs to the LLM (model) like a Linear-Layer/MLP (sub-type C.1), Q-former and a Linear-Layer/MLP (sub-type C.2), Perceiver resampler (sub-type C.3), Custom learnable layers (sub-type C.4).</span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Type-C stands out as the most widely adopted multimodal model architecture.
Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> depicts a general Type-C multimodal model architecture.
The modular nature of this architecture type contributes to its simplicity in both construction and training.
Differing from Type-A and Type-B, in Type-C and Type-D architectures, the modality encoder output is solely directed and fused at the input of the model, without involvement in the internal layers of the model.
Hence, Type-C belongs to early fusion category (Figure <a href="#S0.F2" title="Figure 2 ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Pretrained LLM as decoder is used without any major architectural changes to its internal layers
(some models belonging to this type may have LoRA weights added to their decoder layers).
Pretrained image encoder or other modality encoder are used.
The encoder/s and the decoder are combined together with learnable module like single Linear-Layer, MLP, Q-former, attention-pooling layer, convolutional layer, perceiver resampler or variants of Q-former.
Q-former, first introduced in BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib87" title="" class="ltx_ref">2023b</a>]</cite>, is a lightweight transformer architecture consisting of two components.
One utilizes learnable query embeddings and cross-attention for images, while the other employs self-attention for text processing.
Both components share a self-attention layer, enhancing efficiency in multimodal tasks.
The Perceiver resampler, similar to Q-former, utilizes learnable queries and cross-attention mechanisms to process image data, generating a fixed number of visual tokens. However, unlike Q-former, it does not incorporate self-attention layers specifically tailored for text processing.
Unlike Type-A and B, in Type-C, no major internal model architectural changes are added to either encoder or decoder.
This allows the multimodal model belonging to Type-C, to incorporate off-the-shelf LLMs and encoders in their architecture.
A substantial number of models fall under Type-C. Thus, for clarity, our study categorizes them according to the type of connectors utilized to link the modality encoder and LLM.
Section <a href="#S3.SS3.SSS1" title="3.3.1 Sub-type C.1: Linear Layer/MLP ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>, <a href="#S3.SS3.SSS2" title="3.3.2 Sub-type C.2: Q-former and Linear Layer/MLP ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>, <a href="#S3.SS3.SSS3" title="3.3.3 Sub-type C.3: Perceiver Resampler ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.3</span></a> and <a href="#S3.SS3.SSS4" title="3.3.4 Sub-type C.4: Custom Learnable layer ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.4</span></a> lists and describes the the sub-types/categories.
The comparative advantages and disadvantages of the Type-C multimodal model architecture, in relation to Types A, B, and D, are detailed in Section<a href="#S4.SS3" title="4.3 Type-C ‣ 4 Advantages and Disadvantages ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Sub-type C.1: Linear Layer/MLP</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p"><span id="S3.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Models using only Linear Layer/MLP for connecting Encoder to the LLM (decoder)</span>:
DeepSeek-VL <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib120" title="" class="ltx_ref">2024</a>]</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib89" title="" class="ltx_ref">2024a</a>]</cite>, LLaVA-Med <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib49" title="" class="ltx_ref">2024a</a>]</cite>, LLaVAR <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib91" title="" class="ltx_ref">2024b</a>]</cite>, LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib108" title="" class="ltx_ref">2023b</a>]</cite>, LLaVA-Phi <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib121" title="" class="ltx_ref">2024b</a>]</cite>, LLaVA-NeXT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib122" title="" class="ltx_ref">2024b</a>]</cite>, PaLM-E <cite class="ltx_cite ltx_citemacro_cite">Driess et al. [<a href="#bib.bib123" title="" class="ltx_ref">2023</a>]</cite>, MiniGPT-v2 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib124" title="" class="ltx_ref">2023f</a>]</cite>, DetGPT <cite class="ltx_cite ltx_citemacro_cite">Pi et al. [<a href="#bib.bib125" title="" class="ltx_ref">2023</a>]</cite>, PandaGPT <cite class="ltx_cite ltx_citemacro_cite">Su et al. [<a href="#bib.bib126" title="" class="ltx_ref">2023</a>]</cite>, GILL <cite class="ltx_cite ltx_citemacro_cite">Koh et al. [<a href="#bib.bib127" title="" class="ltx_ref">2024</a>]</cite>, Shikra <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib94" title="" class="ltx_ref">2023d</a>]</cite>, GPT4RoI <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib128" title="" class="ltx_ref">2023c</a>]</cite>, ChatSpot <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib129" title="" class="ltx_ref">2023b</a>]</cite>, NExT-GPT <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>]</cite>, Fuyu <cite class="ltx_cite ltx_citemacro_cite">Bavishi et al. [<a href="#bib.bib130" title="" class="ltx_ref">2023</a>]</cite>, FROMAGe <cite class="ltx_cite ltx_citemacro_cite">Koh et al. [<a href="#bib.bib131" title="" class="ltx_ref">2023</a>]</cite>, ShareGPT4V-7B <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib110" title="" class="ltx_ref">2023e</a>]</cite>, GroundingGPT <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib132" title="" class="ltx_ref">2024b</a>]</cite>, ModaVerse <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib133" title="" class="ltx_ref">2024a</a>]</cite>, MLLM-Tool <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib134" title="" class="ltx_ref">2024b</a>]</cite>, ViGoR <cite class="ltx_cite ltx_citemacro_cite">Yan et al. [<a href="#bib.bib135" title="" class="ltx_ref">2024</a>]</cite>, CoDi <cite class="ltx_cite ltx_citemacro_cite">Tang et al. [<a href="#bib.bib136" title="" class="ltx_ref">2024</a>]</cite>, CoDi-2 <cite class="ltx_cite ltx_citemacro_cite">Tang et al. [<a href="#bib.bib6" title="" class="ltx_ref">2023a</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Sub-type C.2: Q-former and Linear Layer/MLP</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p"><span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Models using Q-former and Linear Layer/MLP for connecting Encoder to the LLM (decoder)</span>:
BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib87" title="" class="ltx_ref">2023b</a>]</cite>, MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib50" title="" class="ltx_ref">2023a</a>]</cite>, X-LLM <cite class="ltx_cite ltx_citemacro_cite">Chen et al. [<a href="#bib.bib137" title="" class="ltx_ref">2023g</a>]</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">Dai et al. [<a href="#bib.bib138" title="" class="ltx_ref">2024</a>]</cite>, EMU <cite class="ltx_cite ltx_citemacro_cite">Sun et al. [<a href="#bib.bib139" title="" class="ltx_ref">2023</a>]</cite>, Video-LLaMA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib140" title="" class="ltx_ref">2023d</a>]</cite>, BLIVA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. [<a href="#bib.bib141" title="" class="ltx_ref">2024</a>]</cite>, SALMONN <cite class="ltx_cite ltx_citemacro_cite">Tang et al. [<a href="#bib.bib142" title="" class="ltx_ref">2023b</a>]</cite>, X-InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">Panagopoulou et al. [<a href="#bib.bib143" title="" class="ltx_ref">2023</a>]</cite>, BuboGPT <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib144" title="" class="ltx_ref">2023c</a>]</cite>, VILA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib145" title="" class="ltx_ref">2023a</a>]</cite>, TinyGPT-V <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. [<a href="#bib.bib146" title="" class="ltx_ref">2023a</a>]</cite>, SPHINX <cite class="ltx_cite ltx_citemacro_cite">Lin et al. [<a href="#bib.bib147" title="" class="ltx_ref">2023b</a>]</cite>, SPHINX-X <cite class="ltx_cite ltx_citemacro_cite">Gao et al. [<a href="#bib.bib148" title="" class="ltx_ref">2024</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Sub-type C.3: Perceiver Resampler</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p"><span id="S3.SS3.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Models using Perceiver resampler for connecting Encoder to the LLM (decoder)</span>:
Idefics2 <cite class="ltx_cite ltx_citemacro_cite">Laurençon et al. [<a href="#bib.bib149" title="" class="ltx_ref">2024b</a>]</cite>, InternLM-XComposer <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>]</cite>, KOSMOS-2.5 <cite class="ltx_cite ltx_citemacro_cite">Lv et al. [<a href="#bib.bib150" title="" class="ltx_ref">2023</a>]</cite>, Monkey <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib151" title="" class="ltx_ref">2023c</a>]</cite>, V* <cite class="ltx_cite ltx_citemacro_cite">Wu and Xie [<a href="#bib.bib152" title="" class="ltx_ref">2023</a>]</cite>, Kosmos-G <cite class="ltx_cite ltx_citemacro_cite">Pan et al. [<a href="#bib.bib153" title="" class="ltx_ref">2024</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Sub-type C.4: Custom Learnable layer</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p"><span id="S3.SS3.SSS4.p1.1.1" class="ltx_text ltx_font_bold">Models using custom-module/layer for connecting Encoder to the LLM (decoder)</span>:
MM1 <cite class="ltx_cite ltx_citemacro_cite">McKinzie et al. [<a href="#bib.bib154" title="" class="ltx_ref">2024</a>]</cite>, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib155" title="" class="ltx_ref">2024</a>]</cite>, mPLUG-DocOwl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib156" title="" class="ltx_ref">2023b</a>]</cite>, EmbodiedGPT <cite class="ltx_cite ltx_citemacro_cite">Mu et al. [<a href="#bib.bib157" title="" class="ltx_ref">2024</a>]</cite>, Video-ChatGPT <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. [<a href="#bib.bib158" title="" class="ltx_ref">2023</a>]</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. [<a href="#bib.bib159" title="" class="ltx_ref">2023</a>]</cite>, AnyMAL <cite class="ltx_cite ltx_citemacro_cite">Moon et al. [<a href="#bib.bib160" title="" class="ltx_ref">2023</a>]</cite>, DocPedia <cite class="ltx_cite ltx_citemacro_cite">Feng et al. [<a href="#bib.bib161" title="" class="ltx_ref">2023</a>]</cite>, mPLUG-PaperOwl <cite class="ltx_cite ltx_citemacro_cite">Hu et al. [<a href="#bib.bib162" title="" class="ltx_ref">2023</a>]</cite>, Osprey <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. [<a href="#bib.bib163" title="" class="ltx_ref">2023b</a>]</cite>, EMU2 <cite class="ltx_cite ltx_citemacro_cite">Sun et al. [<a href="#bib.bib139" title="" class="ltx_ref">2023</a>]</cite>, KAM-CoT <cite class="ltx_cite ltx_citemacro_cite">Mondal et al. [<a href="#bib.bib164" title="" class="ltx_ref">2024</a>]</cite>, VisLingInstruct <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib165" title="" class="ltx_ref">2024c</a>]</cite>, MobileVLM <cite class="ltx_cite ltx_citemacro_cite">Chu et al. [<a href="#bib.bib166" title="" class="ltx_ref">2023</a>]</cite>, MobileVLM-V2 <cite class="ltx_cite ltx_citemacro_cite">Chu et al. [<a href="#bib.bib167" title="" class="ltx_ref">2024</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5 </span>Training Methods and Data</h4>

<div id="S3.SS3.SSS5.p1" class="ltx_para">
<p id="S3.SS3.SSS5.p1.1" class="ltx_p">Many recent multimodal survey works like <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Guo et al. [<a href="#bib.bib28" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Caffagni et al. [<a href="#bib.bib25" title="" class="ltx_ref">2024</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite> have explored Type-C multimodal model architecture in great detail.
This section will highlight some important details provided in these work here, that are related to model training strategies, data and resources.</p>
</div>
<div id="S3.SS3.SSS5.p2" class="ltx_para">
<p id="S3.SS3.SSS5.p2.1" class="ltx_p"><span id="S3.SS3.SSS5.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
Three stages of training used for Type-C multimodal model architecture are pre-training, instruction-tuning, and alignment tuning.
General training procedure (each model may have some variations):
Step 1 (Pretraining): Freeze LLM and Encoder.
Only train the projection layer/s for Vision-Language alignment.
Step 2 (Instruction and alignment tuning): Train projection layer and LLM for multimodal tasks.
The encoder is trained optionally if required.
But in general, only projection layer and LLM are trained.
Pretraining mainly aims to align different modalities and learn multimodal world knowledge typically through caption data (text) for images, audio and videos <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>.
Since the output modality is text, standard next-text-token prediction objective is used during pretraining.
Table <a href="#S3.T2" title="Table 2 ‣ 3.3.5 Training Methods and Data ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists pretraining data commonly used for Type-C architecture.
It contains captioning data for images, audio and video modalities.
LLMs and Multimodal-LLMs are being widely deployed in real-world applications, specially in chat-based applications (chat-bots).
These applications require the model to understand user query (instruction) and with a greater attention on details provided by the user.
In order to better align model for query (instruction) understanding, models are trained on instruction following datasets.
The training process is called as instruction tuning.
Instruction following datasets contain wide variety of tasks, hence making model more versatile across different tasks.
This training process not only increases the instruction following capability, but also leads to improved few-shot and zero-shot performances.
The survey works <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite> lists different ways used to instruction tune a multimodal model of Type-C.
Table <a href="#S3.T3" title="Table 3 ‣ 3.3.5 Training Methods and Data ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the datasets used for instruction tuning of Type-C models.
To further align model for human interactions (chat applications), the model is trained using RLHF (<cite class="ltx_cite ltx_citemacro_cite">Christiano et al. [<a href="#bib.bib168" title="" class="ltx_ref">2017</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Ziegler et al. [<a href="#bib.bib169" title="" class="ltx_ref">2019</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Stiennon et al. [<a href="#bib.bib170" title="" class="ltx_ref">2020</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Bai et al. [<a href="#bib.bib171" title="" class="ltx_ref">2022</a>]</cite>; <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. [<a href="#bib.bib172" title="" class="ltx_ref">2022</a>]</cite>) or DPO <cite class="ltx_cite ltx_citemacro_cite">Rafailov et al. [<a href="#bib.bib173" title="" class="ltx_ref">2024</a>]</cite> training strategies using human preference data.
Table <a href="#S3.T4" title="Table 4 ‣ 3.3.5 Training Methods and Data ‣ 3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> enumerates datasets used for model alignment.</p>
</div>
<div id="S3.SS3.SSS5.p3" class="ltx_para">
<p id="S3.SS3.SSS5.p3.1" class="ltx_p"><span id="S3.SS3.SSS5.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
Type-C multimodal model architecture is data and compute efficient. Hence, low training resources are required compared to all other types.</p>
</div>
<div id="S3.SS3.SSS5.p4" class="ltx_para">
<p id="S3.SS3.SSS5.p4.1" class="ltx_p"><span id="S3.SS3.SSS5.p4.1.1" class="ltx_text ltx_font_bold">LLaVA</span> use 8 A100s. Pretrained for 4 hours and finetuned within 10 hours.
<span id="S3.SS3.SSS5.p4.1.2" class="ltx_text ltx_font_bold">LLaVA-Med</span> takes 7 and 8 hours for stage 1 and 2 training on 8 (40G) A100 GPUs.
<span id="S3.SS3.SSS5.p4.1.3" class="ltx_text ltx_font_bold">LLaVAR</span> all experiments are run on NVIDIA A100 (80GB) GPUs.
<span id="S3.SS3.SSS5.p4.1.4" class="ltx_text ltx_font_bold">LLaVA-1.5</span> full training completed within 1 day on a single 8-A100 node.
<span id="S3.SS3.SSS5.p4.1.5" class="ltx_text ltx_font_bold">LLaVA-Phi</span> uses 8 A100 GPUs. Pretrained for 1.5 hours. 8 hours for visual instruction tuning.
<span id="S3.SS3.SSS5.p4.1.6" class="ltx_text ltx_font_bold">LLaVA-NeXT</span> full trains within approximately 1 day with 32 A100s GPUs.
<span id="S3.SS3.SSS5.p4.1.7" class="ltx_text ltx_font_bold">MiniGPT-v2</span> stage 1 training requires 8 A100 GPU for around 90 hours.
Second stage is trained on 4 A100 GPU for roughly 20 hours.
Last stage, training is executed on 4 A100 GPUs for around 7 hours.
<span id="S3.SS3.SSS5.p4.1.8" class="ltx_text ltx_font_bold">PandaGPT</span> uses 8 A100 (40G) GPUs for around 7 hours for training.
GILL training utilizes 2 A6000 GPUs for 2 days.
<span id="S3.SS3.SSS5.p4.1.9" class="ltx_text ltx_font_bold">Shikra</span> model’s all training runs on 8 NVIDIA A100 GPUs. It takes around 100 hours for stage one training and 20 hours for stage two.
All <span id="S3.SS3.SSS5.p4.1.10" class="ltx_text ltx_font_bold">FROMAGe</span> training steps are completed within 1 day (24 hours) using a single A6000 GPU.
<span id="S3.SS3.SSS5.p4.1.11" class="ltx_text ltx_font_bold">DeepSeek-VL</span> 7B model utilized a cluster of 64 nodes, each comprising 8 Nvidia A100 GPUs for 5 days, while DeepSeek-VL-1B consumed 7 days with 16 nodes for all training steps <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib120" title="" class="ltx_ref">2024</a>]</cite>.
<span id="S3.SS3.SSS5.p4.1.12" class="ltx_text ltx_font_bold">ModaVerse</span> model’s full training completed in 20 hours on 4 A100 GPUs.</p>
</div>
<div id="S3.SS3.SSS5.p5" class="ltx_para">
<p id="S3.SS3.SSS5.p5.1" class="ltx_p"><span id="S3.SS3.SSS5.p5.1.1" class="ltx_text ltx_font_bold">BLIP-2</span> uses 16 A100 (40G) GPUs.
In <span id="S3.SS3.SSS5.p5.1.2" class="ltx_text ltx_font_bold">MiniGPT-4</span>, stage 1 vision-language alignment training uses 4 A100 GPUs for 10 hours. And stage 2 finetuning takes 7 minutes on single A100 GPU.
All <span id="S3.SS3.SSS5.p5.1.3" class="ltx_text ltx_font_bold">X-LLM</span> experiments use up to 8 A100 (40G) GPUs.
<span id="S3.SS3.SSS5.p5.1.4" class="ltx_text ltx_font_bold">InstructBLIP</span> models are trained utilizing 16 Nvidia A100 (40G) GPUs for 1.5 days.
<span id="S3.SS3.SSS5.p5.1.5" class="ltx_text ltx_font_bold">EMU</span> pretraining uses 128 NVIDIA A100 (80G) GPUs. While, instruction tuning stage utilizes 16 A100 (80G) GPUs.
<span id="S3.SS3.SSS5.p5.1.6" class="ltx_text ltx_font_bold">X-InstructBLIP</span> requires 8 A100 (40GB) GPUs.
<span id="S3.SS3.SSS5.p5.1.7" class="ltx_text ltx_font_bold">TinyGPT-V</span> can be trained on a 24GB memory GPU. Phi-2 and CLIP are used as LLM and vision encoder.
<span id="S3.SS3.SSS5.p5.1.8" class="ltx_text ltx_font_bold">SPHINX</span> pre-trains for 125 hours on 32 A100 GPUs with a 7B language model and about takes twice amount of time for 13B language model.
Fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B language model.</p>
</div>
<div id="S3.SS3.SSS5.p6" class="ltx_para">
<p id="S3.SS3.SSS5.p6.1" class="ltx_p"><span id="S3.SS3.SSS5.p6.1.1" class="ltx_text ltx_font_bold">InternLM-XComposer</span> uses 128 Nvidia A100 GPUs.
<span id="S3.SS3.SSS5.p6.1.2" class="ltx_text ltx_font_bold">Monkey</span> model’s whole training process takes 40 A800 days for one epoch.
For <span id="S3.SS3.SSS5.p6.1.3" class="ltx_text ltx_font_bold">KOSMOS-G</span>, the training process took around 4 days with 256 NVIDIA V100 GPUs.
No resource details are provided in KOSMOS-2.5, V* and idefics2 works.</p>
</div>
<div id="S3.SS3.SSS5.p7" class="ltx_para">
<p id="S3.SS3.SSS5.p7.1" class="ltx_p"><span id="S3.SS3.SSS5.p7.1.1" class="ltx_text ltx_font_bold">Video-ChatGPT</span> 7B model training completed in 3 hours on 8 A100 (40GB) GPUs.
<span id="S3.SS3.SSS5.p7.1.2" class="ltx_text ltx_font_bold">AnyMAL</span> was able to train a 70B model on a single A100 (80GB) VRAM GPU through quantization.
Other models used a varying number of Nvidia A100 GPUs.
<span id="S3.SS3.SSS5.p7.1.3" class="ltx_text ltx_font_bold">DocPedia</span> used 8 A100 GPUs for training.
<span id="S3.SS3.SSS5.p7.1.4" class="ltx_text ltx_font_bold">mPLUG-PaperOwl</span> model is trained equivalent to costing 64 A100 days.
<span id="S3.SS3.SSS5.p7.1.5" class="ltx_text ltx_font_bold">Osprey</span>’s training is conducted on 4 NVIDIA A100 GPUs with 80GB memory.
In <span id="S3.SS3.SSS5.p7.1.6" class="ltx_text ltx_font_bold">MobileVLM</span>, first the language model pretraining is completed using 20 nodes equipped with 8 NVIDIA Tesla A100 GPUs each. Later, vision-language training is done in 5 hours with 8 NVIDIA Tesla A100 GPUs for MobileVLM 1.7B, and 8 hours for MobileVLM 3B.
<span id="S3.SS3.SSS5.p7.1.7" class="ltx_text ltx_font_bold">MobileVLM-v2</span>, trained on top of MobileVLM, is pretrained on 8 NVIDIA A100 GPUs for about 5 hour.
Finetuning is peformed with 8 NVIDIA A100 GPUs for around 9 hours.
Resource information are not clearly provided in MM1, mPLUG-Owl, mPLUG-DocOwl, EmbodiedGPT, Qwen-VL and EMU2.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.23.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.24.2" class="ltx_text" style="font-size:90%;">Pretraining data for Type-C. <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Wu et al. [<a href="#bib.bib27" title="" class="ltx_ref">2023b</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib174" title="" class="ltx_ref">2024c</a>]</cite> </span></figcaption>
<table id="S3.T2.21" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.21.22.1" class="ltx_tr">
<th id="S3.T2.21.22.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S3.T2.21.22.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Modality</th>
<th id="S3.T2.21.22.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Samples</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_t">ALLaVA</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mrow id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml"><mi id="S3.T2.1.1.1.m1.1.1.2.2" xref="S3.T2.1.1.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.1.1.1.m1.1.1.2.1" xref="S3.T2.1.1.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.1.1.1.m1.1.1.2.3" xref="S3.T2.1.1.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><ci id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1">→</ci><apply id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2"><plus id="S3.T2.1.1.1.m1.1.1.2.1.cmml" xref="S3.T2.1.1.1.m1.1.1.2.1"></plus><ci id="S3.T2.1.1.1.m1.1.1.2.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.1.1.1.m1.1.1.2.3.cmml" xref="S3.T2.1.1.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">709K</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_left">LVIS-Instruct4V</td>
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_center"><math id="S3.T2.2.2.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.2.2.1.m1.1a"><mrow id="S3.T2.2.2.1.m1.1.1" xref="S3.T2.2.2.1.m1.1.1.cmml"><mrow id="S3.T2.2.2.1.m1.1.1.2" xref="S3.T2.2.2.1.m1.1.1.2.cmml"><mi id="S3.T2.2.2.1.m1.1.1.2.2" xref="S3.T2.2.2.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.2.2.1.m1.1.1.2.1" xref="S3.T2.2.2.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.2.2.1.m1.1.1.2.3" xref="S3.T2.2.2.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.2.2.1.m1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.2.2.1.m1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.m1.1b"><apply id="S3.T2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1"><ci id="S3.T2.2.2.1.m1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1">→</ci><apply id="S3.T2.2.2.1.m1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1.2"><plus id="S3.T2.2.2.1.m1.1.1.2.1.cmml" xref="S3.T2.2.2.1.m1.1.1.2.1"></plus><ci id="S3.T2.2.2.1.m1.1.1.2.2.cmml" xref="S3.T2.2.2.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.2.2.1.m1.1.1.2.3.cmml" xref="S3.T2.2.2.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.2.2.1.m1.1.1.3.cmml" xref="S3.T2.2.2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_center">111K</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_left">ShareGPT4V-PT</td>
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_center"><math id="S3.T2.3.3.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.3.3.1.m1.1a"><mrow id="S3.T2.3.3.1.m1.1.1" xref="S3.T2.3.3.1.m1.1.1.cmml"><mrow id="S3.T2.3.3.1.m1.1.1.2" xref="S3.T2.3.3.1.m1.1.1.2.cmml"><mi id="S3.T2.3.3.1.m1.1.1.2.2" xref="S3.T2.3.3.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.3.3.1.m1.1.1.2.1" xref="S3.T2.3.3.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.3.3.1.m1.1.1.2.3" xref="S3.T2.3.3.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.3.3.1.m1.1.1.1" xref="S3.T2.3.3.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.3.3.1.m1.1.1.3" xref="S3.T2.3.3.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.1b"><apply id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1"><ci id="S3.T2.3.3.1.m1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1.1">→</ci><apply id="S3.T2.3.3.1.m1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.1.1.2"><plus id="S3.T2.3.3.1.m1.1.1.2.1.cmml" xref="S3.T2.3.3.1.m1.1.1.2.1"></plus><ci id="S3.T2.3.3.1.m1.1.1.2.2.cmml" xref="S3.T2.3.3.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.3.3.1.m1.1.1.2.3.cmml" xref="S3.T2.3.3.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.3.3.1.m1.1.1.3.cmml" xref="S3.T2.3.3.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_center">1.2M</td>
</tr>
<tr id="S3.T2.4.4" class="ltx_tr">
<td id="S3.T2.4.4.2" class="ltx_td ltx_align_left">COYO-700M</td>
<td id="S3.T2.4.4.1" class="ltx_td ltx_align_center"><math id="S3.T2.4.4.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.4.4.1.m1.1a"><mrow id="S3.T2.4.4.1.m1.1.1" xref="S3.T2.4.4.1.m1.1.1.cmml"><mrow id="S3.T2.4.4.1.m1.1.1.2" xref="S3.T2.4.4.1.m1.1.1.2.cmml"><mi id="S3.T2.4.4.1.m1.1.1.2.2" xref="S3.T2.4.4.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.4.4.1.m1.1.1.2.1" xref="S3.T2.4.4.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.4.4.1.m1.1.1.2.3" xref="S3.T2.4.4.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.4.4.1.m1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.4.4.1.m1.1.1.3" xref="S3.T2.4.4.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.1.m1.1b"><apply id="S3.T2.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1"><ci id="S3.T2.4.4.1.m1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1">→</ci><apply id="S3.T2.4.4.1.m1.1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.1.2"><plus id="S3.T2.4.4.1.m1.1.1.2.1.cmml" xref="S3.T2.4.4.1.m1.1.1.2.1"></plus><ci id="S3.T2.4.4.1.m1.1.1.2.2.cmml" xref="S3.T2.4.4.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.4.4.1.m1.1.1.2.3.cmml" xref="S3.T2.4.4.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.4.4.1.m1.1.1.3.cmml" xref="S3.T2.4.4.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.4.4.3" class="ltx_td ltx_align_center">747M</td>
</tr>
<tr id="S3.T2.5.5" class="ltx_tr">
<td id="S3.T2.5.5.2" class="ltx_td ltx_align_left">LAION-COCO</td>
<td id="S3.T2.5.5.1" class="ltx_td ltx_align_center"><math id="S3.T2.5.5.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.5.5.1.m1.1a"><mrow id="S3.T2.5.5.1.m1.1.1" xref="S3.T2.5.5.1.m1.1.1.cmml"><mrow id="S3.T2.5.5.1.m1.1.1.2" xref="S3.T2.5.5.1.m1.1.1.2.cmml"><mi id="S3.T2.5.5.1.m1.1.1.2.2" xref="S3.T2.5.5.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.5.5.1.m1.1.1.2.1" xref="S3.T2.5.5.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.5.5.1.m1.1.1.2.3" xref="S3.T2.5.5.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.5.5.1.m1.1.1.1" xref="S3.T2.5.5.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.5.5.1.m1.1.1.3" xref="S3.T2.5.5.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.1.m1.1b"><apply id="S3.T2.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.1.m1.1.1"><ci id="S3.T2.5.5.1.m1.1.1.1.cmml" xref="S3.T2.5.5.1.m1.1.1.1">→</ci><apply id="S3.T2.5.5.1.m1.1.1.2.cmml" xref="S3.T2.5.5.1.m1.1.1.2"><plus id="S3.T2.5.5.1.m1.1.1.2.1.cmml" xref="S3.T2.5.5.1.m1.1.1.2.1"></plus><ci id="S3.T2.5.5.1.m1.1.1.2.2.cmml" xref="S3.T2.5.5.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.5.5.1.m1.1.1.2.3.cmml" xref="S3.T2.5.5.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.5.5.1.m1.1.1.3.cmml" xref="S3.T2.5.5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.5.5.3" class="ltx_td ltx_align_center">600M</td>
</tr>
<tr id="S3.T2.6.6" class="ltx_tr">
<td id="S3.T2.6.6.2" class="ltx_td ltx_align_left">LAION-2B</td>
<td id="S3.T2.6.6.1" class="ltx_td ltx_align_center"><math id="S3.T2.6.6.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.6.6.1.m1.1a"><mrow id="S3.T2.6.6.1.m1.1.1" xref="S3.T2.6.6.1.m1.1.1.cmml"><mrow id="S3.T2.6.6.1.m1.1.1.2" xref="S3.T2.6.6.1.m1.1.1.2.cmml"><mi id="S3.T2.6.6.1.m1.1.1.2.2" xref="S3.T2.6.6.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.6.6.1.m1.1.1.2.1" xref="S3.T2.6.6.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.6.6.1.m1.1.1.2.3" xref="S3.T2.6.6.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.6.6.1.m1.1.1.1" xref="S3.T2.6.6.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.6.6.1.m1.1.1.3" xref="S3.T2.6.6.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.1.m1.1b"><apply id="S3.T2.6.6.1.m1.1.1.cmml" xref="S3.T2.6.6.1.m1.1.1"><ci id="S3.T2.6.6.1.m1.1.1.1.cmml" xref="S3.T2.6.6.1.m1.1.1.1">→</ci><apply id="S3.T2.6.6.1.m1.1.1.2.cmml" xref="S3.T2.6.6.1.m1.1.1.2"><plus id="S3.T2.6.6.1.m1.1.1.2.1.cmml" xref="S3.T2.6.6.1.m1.1.1.2.1"></plus><ci id="S3.T2.6.6.1.m1.1.1.2.2.cmml" xref="S3.T2.6.6.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.6.6.1.m1.1.1.2.3.cmml" xref="S3.T2.6.6.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.6.6.1.m1.1.1.3.cmml" xref="S3.T2.6.6.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.6.6.3" class="ltx_td ltx_align_center">2.3B</td>
</tr>
<tr id="S3.T2.7.7" class="ltx_tr">
<td id="S3.T2.7.7.2" class="ltx_td ltx_align_left">LAION-5B</td>
<td id="S3.T2.7.7.1" class="ltx_td ltx_align_center"><math id="S3.T2.7.7.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.7.7.1.m1.1a"><mrow id="S3.T2.7.7.1.m1.1.1" xref="S3.T2.7.7.1.m1.1.1.cmml"><mrow id="S3.T2.7.7.1.m1.1.1.2" xref="S3.T2.7.7.1.m1.1.1.2.cmml"><mi id="S3.T2.7.7.1.m1.1.1.2.2" xref="S3.T2.7.7.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.7.7.1.m1.1.1.2.1" xref="S3.T2.7.7.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.7.7.1.m1.1.1.2.3" xref="S3.T2.7.7.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.7.7.1.m1.1.1.1" xref="S3.T2.7.7.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.7.7.1.m1.1.1.3" xref="S3.T2.7.7.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.1.m1.1b"><apply id="S3.T2.7.7.1.m1.1.1.cmml" xref="S3.T2.7.7.1.m1.1.1"><ci id="S3.T2.7.7.1.m1.1.1.1.cmml" xref="S3.T2.7.7.1.m1.1.1.1">→</ci><apply id="S3.T2.7.7.1.m1.1.1.2.cmml" xref="S3.T2.7.7.1.m1.1.1.2"><plus id="S3.T2.7.7.1.m1.1.1.2.1.cmml" xref="S3.T2.7.7.1.m1.1.1.2.1"></plus><ci id="S3.T2.7.7.1.m1.1.1.2.2.cmml" xref="S3.T2.7.7.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.7.7.1.m1.1.1.2.3.cmml" xref="S3.T2.7.7.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.7.7.1.m1.1.1.3.cmml" xref="S3.T2.7.7.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.7.7.3" class="ltx_td ltx_align_center">5.9B</td>
</tr>
<tr id="S3.T2.8.8" class="ltx_tr">
<td id="S3.T2.8.8.2" class="ltx_td ltx_align_left">CC-12M</td>
<td id="S3.T2.8.8.1" class="ltx_td ltx_align_center"><math id="S3.T2.8.8.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.8.8.1.m1.1a"><mrow id="S3.T2.8.8.1.m1.1.1" xref="S3.T2.8.8.1.m1.1.1.cmml"><mrow id="S3.T2.8.8.1.m1.1.1.2" xref="S3.T2.8.8.1.m1.1.1.2.cmml"><mi id="S3.T2.8.8.1.m1.1.1.2.2" xref="S3.T2.8.8.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.8.8.1.m1.1.1.2.1" xref="S3.T2.8.8.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.8.8.1.m1.1.1.2.3" xref="S3.T2.8.8.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.8.8.1.m1.1.1.1" xref="S3.T2.8.8.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.8.8.1.m1.1.1.3" xref="S3.T2.8.8.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.1.m1.1b"><apply id="S3.T2.8.8.1.m1.1.1.cmml" xref="S3.T2.8.8.1.m1.1.1"><ci id="S3.T2.8.8.1.m1.1.1.1.cmml" xref="S3.T2.8.8.1.m1.1.1.1">→</ci><apply id="S3.T2.8.8.1.m1.1.1.2.cmml" xref="S3.T2.8.8.1.m1.1.1.2"><plus id="S3.T2.8.8.1.m1.1.1.2.1.cmml" xref="S3.T2.8.8.1.m1.1.1.2.1"></plus><ci id="S3.T2.8.8.1.m1.1.1.2.2.cmml" xref="S3.T2.8.8.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.8.8.1.m1.1.1.2.3.cmml" xref="S3.T2.8.8.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.8.8.1.m1.1.1.3.cmml" xref="S3.T2.8.8.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.8.8.3" class="ltx_td ltx_align_center">12.4M</td>
</tr>
<tr id="S3.T2.9.9" class="ltx_tr">
<td id="S3.T2.9.9.2" class="ltx_td ltx_align_left">CC-3M</td>
<td id="S3.T2.9.9.1" class="ltx_td ltx_align_center"><math id="S3.T2.9.9.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.9.9.1.m1.1a"><mrow id="S3.T2.9.9.1.m1.1.1" xref="S3.T2.9.9.1.m1.1.1.cmml"><mrow id="S3.T2.9.9.1.m1.1.1.2" xref="S3.T2.9.9.1.m1.1.1.2.cmml"><mi id="S3.T2.9.9.1.m1.1.1.2.2" xref="S3.T2.9.9.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.9.9.1.m1.1.1.2.1" xref="S3.T2.9.9.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.9.9.1.m1.1.1.2.3" xref="S3.T2.9.9.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.9.9.1.m1.1.1.1" xref="S3.T2.9.9.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.9.9.1.m1.1.1.3" xref="S3.T2.9.9.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.1.m1.1b"><apply id="S3.T2.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1"><ci id="S3.T2.9.9.1.m1.1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1.1">→</ci><apply id="S3.T2.9.9.1.m1.1.1.2.cmml" xref="S3.T2.9.9.1.m1.1.1.2"><plus id="S3.T2.9.9.1.m1.1.1.2.1.cmml" xref="S3.T2.9.9.1.m1.1.1.2.1"></plus><ci id="S3.T2.9.9.1.m1.1.1.2.2.cmml" xref="S3.T2.9.9.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.9.9.1.m1.1.1.2.3.cmml" xref="S3.T2.9.9.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.9.9.1.m1.1.1.3.cmml" xref="S3.T2.9.9.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.9.9.3" class="ltx_td ltx_align_center">3.3M</td>
</tr>
<tr id="S3.T2.10.10" class="ltx_tr">
<td id="S3.T2.10.10.2" class="ltx_td ltx_align_left">SBU Captions</td>
<td id="S3.T2.10.10.1" class="ltx_td ltx_align_center"><math id="S3.T2.10.10.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T2.10.10.1.m1.1a"><mrow id="S3.T2.10.10.1.m1.1.1" xref="S3.T2.10.10.1.m1.1.1.cmml"><mrow id="S3.T2.10.10.1.m1.1.1.2" xref="S3.T2.10.10.1.m1.1.1.2.cmml"><mi id="S3.T2.10.10.1.m1.1.1.2.2" xref="S3.T2.10.10.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T2.10.10.1.m1.1.1.2.1" xref="S3.T2.10.10.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.10.10.1.m1.1.1.2.3" xref="S3.T2.10.10.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.10.10.1.m1.1.1.1" xref="S3.T2.10.10.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.10.10.1.m1.1.1.3" xref="S3.T2.10.10.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.1.m1.1b"><apply id="S3.T2.10.10.1.m1.1.1.cmml" xref="S3.T2.10.10.1.m1.1.1"><ci id="S3.T2.10.10.1.m1.1.1.1.cmml" xref="S3.T2.10.10.1.m1.1.1.1">→</ci><apply id="S3.T2.10.10.1.m1.1.1.2.cmml" xref="S3.T2.10.10.1.m1.1.1.2"><plus id="S3.T2.10.10.1.m1.1.1.2.1.cmml" xref="S3.T2.10.10.1.m1.1.1.2.1"></plus><ci id="S3.T2.10.10.1.m1.1.1.2.2.cmml" xref="S3.T2.10.10.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T2.10.10.1.m1.1.1.2.3.cmml" xref="S3.T2.10.10.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.10.10.1.m1.1.1.3.cmml" xref="S3.T2.10.10.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.10.10.3" class="ltx_td ltx_align_center">1M</td>
</tr>
<tr id="S3.T2.11.11" class="ltx_tr">
<td id="S3.T2.11.11.2" class="ltx_td ltx_align_left ltx_border_t">VTP</td>
<td id="S3.T2.11.11.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.11.11.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T2.11.11.1.m1.1a"><mrow id="S3.T2.11.11.1.m1.1.1" xref="S3.T2.11.11.1.m1.1.1.cmml"><mrow id="S3.T2.11.11.1.m1.1.1.2" xref="S3.T2.11.11.1.m1.1.1.2.cmml"><mi id="S3.T2.11.11.1.m1.1.1.2.2" xref="S3.T2.11.11.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T2.11.11.1.m1.1.1.2.1" xref="S3.T2.11.11.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.11.11.1.m1.1.1.2.3" xref="S3.T2.11.11.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.11.11.1.m1.1.1.1" xref="S3.T2.11.11.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.11.11.1.m1.1.1.3" xref="S3.T2.11.11.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.1.m1.1b"><apply id="S3.T2.11.11.1.m1.1.1.cmml" xref="S3.T2.11.11.1.m1.1.1"><ci id="S3.T2.11.11.1.m1.1.1.1.cmml" xref="S3.T2.11.11.1.m1.1.1.1">→</ci><apply id="S3.T2.11.11.1.m1.1.1.2.cmml" xref="S3.T2.11.11.1.m1.1.1.2"><plus id="S3.T2.11.11.1.m1.1.1.2.1.cmml" xref="S3.T2.11.11.1.m1.1.1.2.1"></plus><ci id="S3.T2.11.11.1.m1.1.1.2.2.cmml" xref="S3.T2.11.11.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T2.11.11.1.m1.1.1.2.3.cmml" xref="S3.T2.11.11.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.11.11.1.m1.1.1.3.cmml" xref="S3.T2.11.11.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.11.11.3" class="ltx_td ltx_align_center ltx_border_t">27M</td>
</tr>
<tr id="S3.T2.12.12" class="ltx_tr">
<td id="S3.T2.12.12.2" class="ltx_td ltx_align_left">WebVid2M</td>
<td id="S3.T2.12.12.1" class="ltx_td ltx_align_center"><math id="S3.T2.12.12.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T2.12.12.1.m1.1a"><mrow id="S3.T2.12.12.1.m1.1.1" xref="S3.T2.12.12.1.m1.1.1.cmml"><mrow id="S3.T2.12.12.1.m1.1.1.2" xref="S3.T2.12.12.1.m1.1.1.2.cmml"><mi id="S3.T2.12.12.1.m1.1.1.2.2" xref="S3.T2.12.12.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T2.12.12.1.m1.1.1.2.1" xref="S3.T2.12.12.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.12.12.1.m1.1.1.2.3" xref="S3.T2.12.12.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.12.12.1.m1.1.1.1" xref="S3.T2.12.12.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.12.12.1.m1.1.1.3" xref="S3.T2.12.12.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.12.12.1.m1.1b"><apply id="S3.T2.12.12.1.m1.1.1.cmml" xref="S3.T2.12.12.1.m1.1.1"><ci id="S3.T2.12.12.1.m1.1.1.1.cmml" xref="S3.T2.12.12.1.m1.1.1.1">→</ci><apply id="S3.T2.12.12.1.m1.1.1.2.cmml" xref="S3.T2.12.12.1.m1.1.1.2"><plus id="S3.T2.12.12.1.m1.1.1.2.1.cmml" xref="S3.T2.12.12.1.m1.1.1.2.1"></plus><ci id="S3.T2.12.12.1.m1.1.1.2.2.cmml" xref="S3.T2.12.12.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T2.12.12.1.m1.1.1.2.3.cmml" xref="S3.T2.12.12.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.12.12.1.m1.1.1.3.cmml" xref="S3.T2.12.12.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.12.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.12.12.3" class="ltx_td ltx_align_center">2.5M</td>
</tr>
<tr id="S3.T2.13.13" class="ltx_tr">
<td id="S3.T2.13.13.2" class="ltx_td ltx_align_left">YouCook2</td>
<td id="S3.T2.13.13.1" class="ltx_td ltx_align_center"><math id="S3.T2.13.13.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T2.13.13.1.m1.1a"><mrow id="S3.T2.13.13.1.m1.1.1" xref="S3.T2.13.13.1.m1.1.1.cmml"><mrow id="S3.T2.13.13.1.m1.1.1.2" xref="S3.T2.13.13.1.m1.1.1.2.cmml"><mi id="S3.T2.13.13.1.m1.1.1.2.2" xref="S3.T2.13.13.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T2.13.13.1.m1.1.1.2.1" xref="S3.T2.13.13.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.13.13.1.m1.1.1.2.3" xref="S3.T2.13.13.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.13.13.1.m1.1.1.1" xref="S3.T2.13.13.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.13.13.1.m1.1.1.3" xref="S3.T2.13.13.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.13.13.1.m1.1b"><apply id="S3.T2.13.13.1.m1.1.1.cmml" xref="S3.T2.13.13.1.m1.1.1"><ci id="S3.T2.13.13.1.m1.1.1.1.cmml" xref="S3.T2.13.13.1.m1.1.1.1">→</ci><apply id="S3.T2.13.13.1.m1.1.1.2.cmml" xref="S3.T2.13.13.1.m1.1.1.2"><plus id="S3.T2.13.13.1.m1.1.1.2.1.cmml" xref="S3.T2.13.13.1.m1.1.1.2.1"></plus><ci id="S3.T2.13.13.1.m1.1.1.2.2.cmml" xref="S3.T2.13.13.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T2.13.13.1.m1.1.1.2.3.cmml" xref="S3.T2.13.13.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.13.13.1.m1.1.1.3.cmml" xref="S3.T2.13.13.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.13.13.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.13.13.3" class="ltx_td ltx_align_center">2.2K</td>
</tr>
<tr id="S3.T2.14.14" class="ltx_tr">
<td id="S3.T2.14.14.2" class="ltx_td ltx_align_left">MSR-VTT</td>
<td id="S3.T2.14.14.1" class="ltx_td ltx_align_center"><math id="S3.T2.14.14.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T2.14.14.1.m1.1a"><mrow id="S3.T2.14.14.1.m1.1.1" xref="S3.T2.14.14.1.m1.1.1.cmml"><mrow id="S3.T2.14.14.1.m1.1.1.2" xref="S3.T2.14.14.1.m1.1.1.2.cmml"><mi id="S3.T2.14.14.1.m1.1.1.2.2" xref="S3.T2.14.14.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T2.14.14.1.m1.1.1.2.1" xref="S3.T2.14.14.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.14.14.1.m1.1.1.2.3" xref="S3.T2.14.14.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.14.14.1.m1.1.1.1" xref="S3.T2.14.14.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.14.14.1.m1.1.1.3" xref="S3.T2.14.14.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.14.14.1.m1.1b"><apply id="S3.T2.14.14.1.m1.1.1.cmml" xref="S3.T2.14.14.1.m1.1.1"><ci id="S3.T2.14.14.1.m1.1.1.1.cmml" xref="S3.T2.14.14.1.m1.1.1.1">→</ci><apply id="S3.T2.14.14.1.m1.1.1.2.cmml" xref="S3.T2.14.14.1.m1.1.1.2"><plus id="S3.T2.14.14.1.m1.1.1.2.1.cmml" xref="S3.T2.14.14.1.m1.1.1.2.1"></plus><ci id="S3.T2.14.14.1.m1.1.1.2.2.cmml" xref="S3.T2.14.14.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T2.14.14.1.m1.1.1.2.3.cmml" xref="S3.T2.14.14.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.14.14.1.m1.1.1.3.cmml" xref="S3.T2.14.14.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.14.14.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.14.14.3" class="ltx_td ltx_align_center">200K</td>
</tr>
<tr id="S3.T2.15.15" class="ltx_tr">
<td id="S3.T2.15.15.2" class="ltx_td ltx_align_left ltx_border_t">AISHELL-1</td>
<td id="S3.T2.15.15.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.15.15.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T2.15.15.1.m1.1a"><mrow id="S3.T2.15.15.1.m1.1.1" xref="S3.T2.15.15.1.m1.1.1.cmml"><mrow id="S3.T2.15.15.1.m1.1.1.2" xref="S3.T2.15.15.1.m1.1.1.2.cmml"><mi id="S3.T2.15.15.1.m1.1.1.2.2" xref="S3.T2.15.15.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T2.15.15.1.m1.1.1.2.1" xref="S3.T2.15.15.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.15.15.1.m1.1.1.2.3" xref="S3.T2.15.15.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.15.15.1.m1.1.1.1" xref="S3.T2.15.15.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.15.15.1.m1.1.1.3" xref="S3.T2.15.15.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.15.15.1.m1.1b"><apply id="S3.T2.15.15.1.m1.1.1.cmml" xref="S3.T2.15.15.1.m1.1.1"><ci id="S3.T2.15.15.1.m1.1.1.1.cmml" xref="S3.T2.15.15.1.m1.1.1.1">→</ci><apply id="S3.T2.15.15.1.m1.1.1.2.cmml" xref="S3.T2.15.15.1.m1.1.1.2"><plus id="S3.T2.15.15.1.m1.1.1.2.1.cmml" xref="S3.T2.15.15.1.m1.1.1.2.1"></plus><ci id="S3.T2.15.15.1.m1.1.1.2.2.cmml" xref="S3.T2.15.15.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T2.15.15.1.m1.1.1.2.3.cmml" xref="S3.T2.15.15.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.15.15.1.m1.1.1.3.cmml" xref="S3.T2.15.15.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.15.15.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.15.15.3" class="ltx_td ltx_align_center ltx_border_t">128K</td>
</tr>
<tr id="S3.T2.16.16" class="ltx_tr">
<td id="S3.T2.16.16.2" class="ltx_td ltx_align_left">AISHELL-2</td>
<td id="S3.T2.16.16.1" class="ltx_td ltx_align_center"><math id="S3.T2.16.16.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T2.16.16.1.m1.1a"><mrow id="S3.T2.16.16.1.m1.1.1" xref="S3.T2.16.16.1.m1.1.1.cmml"><mrow id="S3.T2.16.16.1.m1.1.1.2" xref="S3.T2.16.16.1.m1.1.1.2.cmml"><mi id="S3.T2.16.16.1.m1.1.1.2.2" xref="S3.T2.16.16.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T2.16.16.1.m1.1.1.2.1" xref="S3.T2.16.16.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.16.16.1.m1.1.1.2.3" xref="S3.T2.16.16.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.16.16.1.m1.1.1.1" xref="S3.T2.16.16.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.16.16.1.m1.1.1.3" xref="S3.T2.16.16.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.16.16.1.m1.1b"><apply id="S3.T2.16.16.1.m1.1.1.cmml" xref="S3.T2.16.16.1.m1.1.1"><ci id="S3.T2.16.16.1.m1.1.1.1.cmml" xref="S3.T2.16.16.1.m1.1.1.1">→</ci><apply id="S3.T2.16.16.1.m1.1.1.2.cmml" xref="S3.T2.16.16.1.m1.1.1.2"><plus id="S3.T2.16.16.1.m1.1.1.2.1.cmml" xref="S3.T2.16.16.1.m1.1.1.2.1"></plus><ci id="S3.T2.16.16.1.m1.1.1.2.2.cmml" xref="S3.T2.16.16.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T2.16.16.1.m1.1.1.2.3.cmml" xref="S3.T2.16.16.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.16.16.1.m1.1.1.3.cmml" xref="S3.T2.16.16.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.16.16.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.16.16.3" class="ltx_td ltx_align_center">1M</td>
</tr>
<tr id="S3.T2.17.17" class="ltx_tr">
<td id="S3.T2.17.17.2" class="ltx_td ltx_align_left">WavCaps</td>
<td id="S3.T2.17.17.1" class="ltx_td ltx_align_center"><math id="S3.T2.17.17.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T2.17.17.1.m1.1a"><mrow id="S3.T2.17.17.1.m1.1.1" xref="S3.T2.17.17.1.m1.1.1.cmml"><mrow id="S3.T2.17.17.1.m1.1.1.2" xref="S3.T2.17.17.1.m1.1.1.2.cmml"><mi id="S3.T2.17.17.1.m1.1.1.2.2" xref="S3.T2.17.17.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T2.17.17.1.m1.1.1.2.1" xref="S3.T2.17.17.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.17.17.1.m1.1.1.2.3" xref="S3.T2.17.17.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.17.17.1.m1.1.1.1" xref="S3.T2.17.17.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.17.17.1.m1.1.1.3" xref="S3.T2.17.17.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.17.17.1.m1.1b"><apply id="S3.T2.17.17.1.m1.1.1.cmml" xref="S3.T2.17.17.1.m1.1.1"><ci id="S3.T2.17.17.1.m1.1.1.1.cmml" xref="S3.T2.17.17.1.m1.1.1.1">→</ci><apply id="S3.T2.17.17.1.m1.1.1.2.cmml" xref="S3.T2.17.17.1.m1.1.1.2"><plus id="S3.T2.17.17.1.m1.1.1.2.1.cmml" xref="S3.T2.17.17.1.m1.1.1.2.1"></plus><ci id="S3.T2.17.17.1.m1.1.1.2.2.cmml" xref="S3.T2.17.17.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T2.17.17.1.m1.1.1.2.3.cmml" xref="S3.T2.17.17.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.17.17.1.m1.1.1.3.cmml" xref="S3.T2.17.17.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.17.17.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.17.17.3" class="ltx_td ltx_align_center">24K</td>
</tr>
<tr id="S3.T2.18.18" class="ltx_tr">
<td id="S3.T2.18.18.2" class="ltx_td ltx_align_left">Common Voice</td>
<td id="S3.T2.18.18.1" class="ltx_td ltx_align_center"><math id="S3.T2.18.18.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T2.18.18.1.m1.1a"><mrow id="S3.T2.18.18.1.m1.1.1" xref="S3.T2.18.18.1.m1.1.1.cmml"><mrow id="S3.T2.18.18.1.m1.1.1.2" xref="S3.T2.18.18.1.m1.1.1.2.cmml"><mi id="S3.T2.18.18.1.m1.1.1.2.2" xref="S3.T2.18.18.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T2.18.18.1.m1.1.1.2.1" xref="S3.T2.18.18.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.18.18.1.m1.1.1.2.3" xref="S3.T2.18.18.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.18.18.1.m1.1.1.1" xref="S3.T2.18.18.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.18.18.1.m1.1.1.3" xref="S3.T2.18.18.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.18.18.1.m1.1b"><apply id="S3.T2.18.18.1.m1.1.1.cmml" xref="S3.T2.18.18.1.m1.1.1"><ci id="S3.T2.18.18.1.m1.1.1.1.cmml" xref="S3.T2.18.18.1.m1.1.1.1">→</ci><apply id="S3.T2.18.18.1.m1.1.1.2.cmml" xref="S3.T2.18.18.1.m1.1.1.2"><plus id="S3.T2.18.18.1.m1.1.1.2.1.cmml" xref="S3.T2.18.18.1.m1.1.1.2.1"></plus><ci id="S3.T2.18.18.1.m1.1.1.2.2.cmml" xref="S3.T2.18.18.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T2.18.18.1.m1.1.1.2.3.cmml" xref="S3.T2.18.18.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.18.18.1.m1.1.1.3.cmml" xref="S3.T2.18.18.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.18.18.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.18.18.3" class="ltx_td ltx_align_center">9.2K</td>
</tr>
<tr id="S3.T2.19.19" class="ltx_tr">
<td id="S3.T2.19.19.2" class="ltx_td ltx_align_left">LibriSpeech</td>
<td id="S3.T2.19.19.1" class="ltx_td ltx_align_center"><math id="S3.T2.19.19.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T2.19.19.1.m1.1a"><mrow id="S3.T2.19.19.1.m1.1.1" xref="S3.T2.19.19.1.m1.1.1.cmml"><mrow id="S3.T2.19.19.1.m1.1.1.2" xref="S3.T2.19.19.1.m1.1.1.2.cmml"><mi id="S3.T2.19.19.1.m1.1.1.2.2" xref="S3.T2.19.19.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T2.19.19.1.m1.1.1.2.1" xref="S3.T2.19.19.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T2.19.19.1.m1.1.1.2.3" xref="S3.T2.19.19.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T2.19.19.1.m1.1.1.1" xref="S3.T2.19.19.1.m1.1.1.1.cmml">→</mo><mi id="S3.T2.19.19.1.m1.1.1.3" xref="S3.T2.19.19.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.19.19.1.m1.1b"><apply id="S3.T2.19.19.1.m1.1.1.cmml" xref="S3.T2.19.19.1.m1.1.1"><ci id="S3.T2.19.19.1.m1.1.1.1.cmml" xref="S3.T2.19.19.1.m1.1.1.1">→</ci><apply id="S3.T2.19.19.1.m1.1.1.2.cmml" xref="S3.T2.19.19.1.m1.1.1.2"><plus id="S3.T2.19.19.1.m1.1.1.2.1.cmml" xref="S3.T2.19.19.1.m1.1.1.2.1"></plus><ci id="S3.T2.19.19.1.m1.1.1.2.2.cmml" xref="S3.T2.19.19.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T2.19.19.1.m1.1.1.2.3.cmml" xref="S3.T2.19.19.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T2.19.19.1.m1.1.1.3.cmml" xref="S3.T2.19.19.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.19.19.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T2.19.19.3" class="ltx_td ltx_align_center">1K</td>
</tr>
<tr id="S3.T2.20.20" class="ltx_tr">
<td id="S3.T2.20.20.2" class="ltx_td ltx_align_left ltx_border_t">MM5Product</td>
<td id="S3.T2.20.20.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.20.20.1.m1.1" class="ltx_Math" alttext="I+A+V+T" display="inline"><semantics id="S3.T2.20.20.1.m1.1a"><mrow id="S3.T2.20.20.1.m1.1.1" xref="S3.T2.20.20.1.m1.1.1.cmml"><mi id="S3.T2.20.20.1.m1.1.1.2" xref="S3.T2.20.20.1.m1.1.1.2.cmml">I</mi><mo id="S3.T2.20.20.1.m1.1.1.1" xref="S3.T2.20.20.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.20.20.1.m1.1.1.3" xref="S3.T2.20.20.1.m1.1.1.3.cmml">A</mi><mo id="S3.T2.20.20.1.m1.1.1.1a" xref="S3.T2.20.20.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.20.20.1.m1.1.1.4" xref="S3.T2.20.20.1.m1.1.1.4.cmml">V</mi><mo id="S3.T2.20.20.1.m1.1.1.1b" xref="S3.T2.20.20.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.20.20.1.m1.1.1.5" xref="S3.T2.20.20.1.m1.1.1.5.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.20.20.1.m1.1b"><apply id="S3.T2.20.20.1.m1.1.1.cmml" xref="S3.T2.20.20.1.m1.1.1"><plus id="S3.T2.20.20.1.m1.1.1.1.cmml" xref="S3.T2.20.20.1.m1.1.1.1"></plus><ci id="S3.T2.20.20.1.m1.1.1.2.cmml" xref="S3.T2.20.20.1.m1.1.1.2">𝐼</ci><ci id="S3.T2.20.20.1.m1.1.1.3.cmml" xref="S3.T2.20.20.1.m1.1.1.3">𝐴</ci><ci id="S3.T2.20.20.1.m1.1.1.4.cmml" xref="S3.T2.20.20.1.m1.1.1.4">𝑉</ci><ci id="S3.T2.20.20.1.m1.1.1.5.cmml" xref="S3.T2.20.20.1.m1.1.1.5">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.20.20.1.m1.1c">I+A+V+T</annotation></semantics></math></td>
<td id="S3.T2.20.20.3" class="ltx_td ltx_align_center ltx_border_t">6M</td>
</tr>
<tr id="S3.T2.21.21" class="ltx_tr">
<td id="S3.T2.21.21.2" class="ltx_td ltx_align_left ltx_border_bb">MSR-VTT</td>
<td id="S3.T2.21.21.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T2.21.21.1.m1.1" class="ltx_Math" alttext="I+A+V+T" display="inline"><semantics id="S3.T2.21.21.1.m1.1a"><mrow id="S3.T2.21.21.1.m1.1.1" xref="S3.T2.21.21.1.m1.1.1.cmml"><mi id="S3.T2.21.21.1.m1.1.1.2" xref="S3.T2.21.21.1.m1.1.1.2.cmml">I</mi><mo id="S3.T2.21.21.1.m1.1.1.1" xref="S3.T2.21.21.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.21.21.1.m1.1.1.3" xref="S3.T2.21.21.1.m1.1.1.3.cmml">A</mi><mo id="S3.T2.21.21.1.m1.1.1.1a" xref="S3.T2.21.21.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.21.21.1.m1.1.1.4" xref="S3.T2.21.21.1.m1.1.1.4.cmml">V</mi><mo id="S3.T2.21.21.1.m1.1.1.1b" xref="S3.T2.21.21.1.m1.1.1.1.cmml">+</mo><mi id="S3.T2.21.21.1.m1.1.1.5" xref="S3.T2.21.21.1.m1.1.1.5.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.21.21.1.m1.1b"><apply id="S3.T2.21.21.1.m1.1.1.cmml" xref="S3.T2.21.21.1.m1.1.1"><plus id="S3.T2.21.21.1.m1.1.1.1.cmml" xref="S3.T2.21.21.1.m1.1.1.1"></plus><ci id="S3.T2.21.21.1.m1.1.1.2.cmml" xref="S3.T2.21.21.1.m1.1.1.2">𝐼</ci><ci id="S3.T2.21.21.1.m1.1.1.3.cmml" xref="S3.T2.21.21.1.m1.1.1.3">𝐴</ci><ci id="S3.T2.21.21.1.m1.1.1.4.cmml" xref="S3.T2.21.21.1.m1.1.1.4">𝑉</ci><ci id="S3.T2.21.21.1.m1.1.1.5.cmml" xref="S3.T2.21.21.1.m1.1.1.5">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.21.21.1.m1.1c">I+A+V+T</annotation></semantics></math></td>
<td id="S3.T2.21.21.3" class="ltx_td ltx_align_center ltx_border_bb">10K</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.12.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.13.2" class="ltx_text" style="font-size:90%;">Instruction tuning data for Type-C. <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yin et al. [<a href="#bib.bib24" title="" class="ltx_ref">2024</a>]</cite></span></figcaption>
<table id="S3.T3.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.10.11.1" class="ltx_tr">
<th id="S3.T3.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S3.T3.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Modality</th>
<th id="S3.T3.10.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Samples</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_t">LLaVA-Instruct</td>
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mrow id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml"><mi id="S3.T3.1.1.1.m1.1.1.2.2" xref="S3.T3.1.1.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T3.1.1.1.m1.1.1.2.1" xref="S3.T3.1.1.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.1.1.1.m1.1.1.2.3" xref="S3.T3.1.1.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><ci id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1">→</ci><apply id="S3.T3.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2"><plus id="S3.T3.1.1.1.m1.1.1.2.1.cmml" xref="S3.T3.1.1.1.m1.1.1.2.1"></plus><ci id="S3.T3.1.1.1.m1.1.1.2.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T3.1.1.1.m1.1.1.2.3.cmml" xref="S3.T3.1.1.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_t">158K</td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_left">LVIS-Instruct</td>
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_center"><math id="S3.T3.2.2.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T3.2.2.1.m1.1a"><mrow id="S3.T3.2.2.1.m1.1.1" xref="S3.T3.2.2.1.m1.1.1.cmml"><mrow id="S3.T3.2.2.1.m1.1.1.2" xref="S3.T3.2.2.1.m1.1.1.2.cmml"><mi id="S3.T3.2.2.1.m1.1.1.2.2" xref="S3.T3.2.2.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T3.2.2.1.m1.1.1.2.1" xref="S3.T3.2.2.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.2.2.1.m1.1.1.2.3" xref="S3.T3.2.2.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.2.2.1.m1.1.1.1" xref="S3.T3.2.2.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.2.2.1.m1.1.1.3" xref="S3.T3.2.2.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><apply id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1"><ci id="S3.T3.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1.1">→</ci><apply id="S3.T3.2.2.1.m1.1.1.2.cmml" xref="S3.T3.2.2.1.m1.1.1.2"><plus id="S3.T3.2.2.1.m1.1.1.2.1.cmml" xref="S3.T3.2.2.1.m1.1.1.2.1"></plus><ci id="S3.T3.2.2.1.m1.1.1.2.2.cmml" xref="S3.T3.2.2.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T3.2.2.1.m1.1.1.2.3.cmml" xref="S3.T3.2.2.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.2.2.1.m1.1.1.3.cmml" xref="S3.T3.2.2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_center">220K</td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_left">ALLaVA</td>
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_center"><math id="S3.T3.3.3.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T3.3.3.1.m1.1a"><mrow id="S3.T3.3.3.1.m1.1.1" xref="S3.T3.3.3.1.m1.1.1.cmml"><mrow id="S3.T3.3.3.1.m1.1.1.2" xref="S3.T3.3.3.1.m1.1.1.2.cmml"><mi id="S3.T3.3.3.1.m1.1.1.2.2" xref="S3.T3.3.3.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T3.3.3.1.m1.1.1.2.1" xref="S3.T3.3.3.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.3.3.1.m1.1.1.2.3" xref="S3.T3.3.3.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.3.3.1.m1.1.1.1" xref="S3.T3.3.3.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.3.3.1.m1.1.1.3" xref="S3.T3.3.3.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><apply id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1"><ci id="S3.T3.3.3.1.m1.1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1.1">→</ci><apply id="S3.T3.3.3.1.m1.1.1.2.cmml" xref="S3.T3.3.3.1.m1.1.1.2"><plus id="S3.T3.3.3.1.m1.1.1.2.1.cmml" xref="S3.T3.3.3.1.m1.1.1.2.1"></plus><ci id="S3.T3.3.3.1.m1.1.1.2.2.cmml" xref="S3.T3.3.3.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T3.3.3.1.m1.1.1.2.3.cmml" xref="S3.T3.3.3.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.3.3.1.m1.1.1.3.cmml" xref="S3.T3.3.3.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_center">1.4M</td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_left ltx_border_t">MIMIC-IT</td>
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T3.4.4.1.m1.1" class="ltx_Math" alttext="I/V+T\rightarrow T" display="inline"><semantics id="S3.T3.4.4.1.m1.1a"><mrow id="S3.T3.4.4.1.m1.1.1" xref="S3.T3.4.4.1.m1.1.1.cmml"><mrow id="S3.T3.4.4.1.m1.1.1.2" xref="S3.T3.4.4.1.m1.1.1.2.cmml"><mrow id="S3.T3.4.4.1.m1.1.1.2.2" xref="S3.T3.4.4.1.m1.1.1.2.2.cmml"><mi id="S3.T3.4.4.1.m1.1.1.2.2.2" xref="S3.T3.4.4.1.m1.1.1.2.2.2.cmml">I</mi><mo id="S3.T3.4.4.1.m1.1.1.2.2.1" xref="S3.T3.4.4.1.m1.1.1.2.2.1.cmml">/</mo><mi id="S3.T3.4.4.1.m1.1.1.2.2.3" xref="S3.T3.4.4.1.m1.1.1.2.2.3.cmml">V</mi></mrow><mo id="S3.T3.4.4.1.m1.1.1.2.1" xref="S3.T3.4.4.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.4.4.1.m1.1.1.2.3" xref="S3.T3.4.4.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.4.4.1.m1.1.1.1" xref="S3.T3.4.4.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.4.4.1.m1.1.1.3" xref="S3.T3.4.4.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><apply id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1"><ci id="S3.T3.4.4.1.m1.1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1.1">→</ci><apply id="S3.T3.4.4.1.m1.1.1.2.cmml" xref="S3.T3.4.4.1.m1.1.1.2"><plus id="S3.T3.4.4.1.m1.1.1.2.1.cmml" xref="S3.T3.4.4.1.m1.1.1.2.1"></plus><apply id="S3.T3.4.4.1.m1.1.1.2.2.cmml" xref="S3.T3.4.4.1.m1.1.1.2.2"><divide id="S3.T3.4.4.1.m1.1.1.2.2.1.cmml" xref="S3.T3.4.4.1.m1.1.1.2.2.1"></divide><ci id="S3.T3.4.4.1.m1.1.1.2.2.2.cmml" xref="S3.T3.4.4.1.m1.1.1.2.2.2">𝐼</ci><ci id="S3.T3.4.4.1.m1.1.1.2.2.3.cmml" xref="S3.T3.4.4.1.m1.1.1.2.2.3">𝑉</ci></apply><ci id="S3.T3.4.4.1.m1.1.1.2.3.cmml" xref="S3.T3.4.4.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.4.4.1.m1.1.1.3.cmml" xref="S3.T3.4.4.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">I/V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_center ltx_border_t">2.8M</td>
</tr>
<tr id="S3.T3.5.5" class="ltx_tr">
<td id="S3.T3.5.5.2" class="ltx_td ltx_align_left">Video-ChatGPT</td>
<td id="S3.T3.5.5.1" class="ltx_td ltx_align_center"><math id="S3.T3.5.5.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T3.5.5.1.m1.1a"><mrow id="S3.T3.5.5.1.m1.1.1" xref="S3.T3.5.5.1.m1.1.1.cmml"><mrow id="S3.T3.5.5.1.m1.1.1.2" xref="S3.T3.5.5.1.m1.1.1.2.cmml"><mi id="S3.T3.5.5.1.m1.1.1.2.2" xref="S3.T3.5.5.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T3.5.5.1.m1.1.1.2.1" xref="S3.T3.5.5.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.5.5.1.m1.1.1.2.3" xref="S3.T3.5.5.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.5.5.1.m1.1.1.1" xref="S3.T3.5.5.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.5.5.1.m1.1.1.3" xref="S3.T3.5.5.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.m1.1b"><apply id="S3.T3.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1"><ci id="S3.T3.5.5.1.m1.1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1.1">→</ci><apply id="S3.T3.5.5.1.m1.1.1.2.cmml" xref="S3.T3.5.5.1.m1.1.1.2"><plus id="S3.T3.5.5.1.m1.1.1.2.1.cmml" xref="S3.T3.5.5.1.m1.1.1.2.1"></plus><ci id="S3.T3.5.5.1.m1.1.1.2.2.cmml" xref="S3.T3.5.5.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T3.5.5.1.m1.1.1.2.3.cmml" xref="S3.T3.5.5.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.5.5.1.m1.1.1.3.cmml" xref="S3.T3.5.5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.5.5.3" class="ltx_td ltx_align_center">100K</td>
</tr>
<tr id="S3.T3.6.6" class="ltx_tr">
<td id="S3.T3.6.6.2" class="ltx_td ltx_align_left">VideoChat</td>
<td id="S3.T3.6.6.1" class="ltx_td ltx_align_center"><math id="S3.T3.6.6.1.m1.1" class="ltx_Math" alttext="V+T\rightarrow T" display="inline"><semantics id="S3.T3.6.6.1.m1.1a"><mrow id="S3.T3.6.6.1.m1.1.1" xref="S3.T3.6.6.1.m1.1.1.cmml"><mrow id="S3.T3.6.6.1.m1.1.1.2" xref="S3.T3.6.6.1.m1.1.1.2.cmml"><mi id="S3.T3.6.6.1.m1.1.1.2.2" xref="S3.T3.6.6.1.m1.1.1.2.2.cmml">V</mi><mo id="S3.T3.6.6.1.m1.1.1.2.1" xref="S3.T3.6.6.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.6.6.1.m1.1.1.2.3" xref="S3.T3.6.6.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.6.6.1.m1.1.1.1" xref="S3.T3.6.6.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.6.6.1.m1.1.1.3" xref="S3.T3.6.6.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><apply id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1"><ci id="S3.T3.6.6.1.m1.1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1.1">→</ci><apply id="S3.T3.6.6.1.m1.1.1.2.cmml" xref="S3.T3.6.6.1.m1.1.1.2"><plus id="S3.T3.6.6.1.m1.1.1.2.1.cmml" xref="S3.T3.6.6.1.m1.1.1.2.1"></plus><ci id="S3.T3.6.6.1.m1.1.1.2.2.cmml" xref="S3.T3.6.6.1.m1.1.1.2.2">𝑉</ci><ci id="S3.T3.6.6.1.m1.1.1.2.3.cmml" xref="S3.T3.6.6.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.6.6.1.m1.1.1.3.cmml" xref="S3.T3.6.6.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">V+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.6.6.3" class="ltx_td ltx_align_center">11K</td>
</tr>
<tr id="S3.T3.7.7" class="ltx_tr">
<td id="S3.T3.7.7.2" class="ltx_td ltx_align_left ltx_border_t">Clotho-Detail</td>
<td id="S3.T3.7.7.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T3.7.7.1.m1.1" class="ltx_Math" alttext="A+T\rightarrow T" display="inline"><semantics id="S3.T3.7.7.1.m1.1a"><mrow id="S3.T3.7.7.1.m1.1.1" xref="S3.T3.7.7.1.m1.1.1.cmml"><mrow id="S3.T3.7.7.1.m1.1.1.2" xref="S3.T3.7.7.1.m1.1.1.2.cmml"><mi id="S3.T3.7.7.1.m1.1.1.2.2" xref="S3.T3.7.7.1.m1.1.1.2.2.cmml">A</mi><mo id="S3.T3.7.7.1.m1.1.1.2.1" xref="S3.T3.7.7.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.7.7.1.m1.1.1.2.3" xref="S3.T3.7.7.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.7.7.1.m1.1.1.1" xref="S3.T3.7.7.1.m1.1.1.1.cmml">→</mo><mi id="S3.T3.7.7.1.m1.1.1.3" xref="S3.T3.7.7.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.1.m1.1b"><apply id="S3.T3.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.1.m1.1.1"><ci id="S3.T3.7.7.1.m1.1.1.1.cmml" xref="S3.T3.7.7.1.m1.1.1.1">→</ci><apply id="S3.T3.7.7.1.m1.1.1.2.cmml" xref="S3.T3.7.7.1.m1.1.1.2"><plus id="S3.T3.7.7.1.m1.1.1.2.1.cmml" xref="S3.T3.7.7.1.m1.1.1.2.1"></plus><ci id="S3.T3.7.7.1.m1.1.1.2.2.cmml" xref="S3.T3.7.7.1.m1.1.1.2.2">𝐴</ci><ci id="S3.T3.7.7.1.m1.1.1.2.3.cmml" xref="S3.T3.7.7.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T3.7.7.1.m1.1.1.3.cmml" xref="S3.T3.7.7.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.1.m1.1c">A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">3.9K</td>
</tr>
<tr id="S3.T3.8.8" class="ltx_tr">
<td id="S3.T3.8.8.2" class="ltx_td ltx_align_left">BuboGPT’s IT</td>
<td id="S3.T3.8.8.1" class="ltx_td ltx_align_center"><math id="S3.T3.8.8.1.m1.1" class="ltx_Math" alttext="(I+A)/A+T\rightarrow T" display="inline"><semantics id="S3.T3.8.8.1.m1.1a"><mrow id="S3.T3.8.8.1.m1.1.1" xref="S3.T3.8.8.1.m1.1.1.cmml"><mrow id="S3.T3.8.8.1.m1.1.1.1" xref="S3.T3.8.8.1.m1.1.1.1.cmml"><mrow id="S3.T3.8.8.1.m1.1.1.1.1" xref="S3.T3.8.8.1.m1.1.1.1.1.cmml"><mrow id="S3.T3.8.8.1.m1.1.1.1.1.1.1" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T3.8.8.1.m1.1.1.1.1.1.1.2" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.2" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.2.cmml">I</mi><mo id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.1" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.3" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.3.cmml">A</mi></mrow><mo stretchy="false" id="S3.T3.8.8.1.m1.1.1.1.1.1.1.3" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.T3.8.8.1.m1.1.1.1.1.2" xref="S3.T3.8.8.1.m1.1.1.1.1.2.cmml">/</mo><mi id="S3.T3.8.8.1.m1.1.1.1.1.3" xref="S3.T3.8.8.1.m1.1.1.1.1.3.cmml">A</mi></mrow><mo id="S3.T3.8.8.1.m1.1.1.1.2" xref="S3.T3.8.8.1.m1.1.1.1.2.cmml">+</mo><mi id="S3.T3.8.8.1.m1.1.1.1.3" xref="S3.T3.8.8.1.m1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.8.8.1.m1.1.1.2" xref="S3.T3.8.8.1.m1.1.1.2.cmml">→</mo><mi id="S3.T3.8.8.1.m1.1.1.3" xref="S3.T3.8.8.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.1.m1.1b"><apply id="S3.T3.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1"><ci id="S3.T3.8.8.1.m1.1.1.2.cmml" xref="S3.T3.8.8.1.m1.1.1.2">→</ci><apply id="S3.T3.8.8.1.m1.1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1.1"><plus id="S3.T3.8.8.1.m1.1.1.1.2.cmml" xref="S3.T3.8.8.1.m1.1.1.1.2"></plus><apply id="S3.T3.8.8.1.m1.1.1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1"><divide id="S3.T3.8.8.1.m1.1.1.1.1.2.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.2"></divide><apply id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1"><plus id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.1"></plus><ci id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.1.1.1.3">𝐴</ci></apply><ci id="S3.T3.8.8.1.m1.1.1.1.1.3.cmml" xref="S3.T3.8.8.1.m1.1.1.1.1.3">𝐴</ci></apply><ci id="S3.T3.8.8.1.m1.1.1.1.3.cmml" xref="S3.T3.8.8.1.m1.1.1.1.3">𝑇</ci></apply><ci id="S3.T3.8.8.1.m1.1.1.3.cmml" xref="S3.T3.8.8.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.1.m1.1c">(I+A)/A+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T3.8.8.3" class="ltx_td ltx_align_center">9K</td>
</tr>
<tr id="S3.T3.9.9" class="ltx_tr">
<td id="S3.T3.9.9.2" class="ltx_td ltx_align_left ltx_border_t">T2M</td>
<td id="S3.T3.9.9.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T3.9.9.1.m1.1" class="ltx_Math" alttext="T\rightarrow I/V/A+T" display="inline"><semantics id="S3.T3.9.9.1.m1.1a"><mrow id="S3.T3.9.9.1.m1.1.1" xref="S3.T3.9.9.1.m1.1.1.cmml"><mi id="S3.T3.9.9.1.m1.1.1.2" xref="S3.T3.9.9.1.m1.1.1.2.cmml">T</mi><mo stretchy="false" id="S3.T3.9.9.1.m1.1.1.1" xref="S3.T3.9.9.1.m1.1.1.1.cmml">→</mo><mrow id="S3.T3.9.9.1.m1.1.1.3" xref="S3.T3.9.9.1.m1.1.1.3.cmml"><mrow id="S3.T3.9.9.1.m1.1.1.3.2" xref="S3.T3.9.9.1.m1.1.1.3.2.cmml"><mi id="S3.T3.9.9.1.m1.1.1.3.2.2" xref="S3.T3.9.9.1.m1.1.1.3.2.2.cmml">I</mi><mo id="S3.T3.9.9.1.m1.1.1.3.2.1" xref="S3.T3.9.9.1.m1.1.1.3.2.1.cmml">/</mo><mi id="S3.T3.9.9.1.m1.1.1.3.2.3" xref="S3.T3.9.9.1.m1.1.1.3.2.3.cmml">V</mi><mo id="S3.T3.9.9.1.m1.1.1.3.2.1a" xref="S3.T3.9.9.1.m1.1.1.3.2.1.cmml">/</mo><mi id="S3.T3.9.9.1.m1.1.1.3.2.4" xref="S3.T3.9.9.1.m1.1.1.3.2.4.cmml">A</mi></mrow><mo id="S3.T3.9.9.1.m1.1.1.3.1" xref="S3.T3.9.9.1.m1.1.1.3.1.cmml">+</mo><mi id="S3.T3.9.9.1.m1.1.1.3.3" xref="S3.T3.9.9.1.m1.1.1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.1.m1.1b"><apply id="S3.T3.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.1.m1.1.1"><ci id="S3.T3.9.9.1.m1.1.1.1.cmml" xref="S3.T3.9.9.1.m1.1.1.1">→</ci><ci id="S3.T3.9.9.1.m1.1.1.2.cmml" xref="S3.T3.9.9.1.m1.1.1.2">𝑇</ci><apply id="S3.T3.9.9.1.m1.1.1.3.cmml" xref="S3.T3.9.9.1.m1.1.1.3"><plus id="S3.T3.9.9.1.m1.1.1.3.1.cmml" xref="S3.T3.9.9.1.m1.1.1.3.1"></plus><apply id="S3.T3.9.9.1.m1.1.1.3.2.cmml" xref="S3.T3.9.9.1.m1.1.1.3.2"><divide id="S3.T3.9.9.1.m1.1.1.3.2.1.cmml" xref="S3.T3.9.9.1.m1.1.1.3.2.1"></divide><ci id="S3.T3.9.9.1.m1.1.1.3.2.2.cmml" xref="S3.T3.9.9.1.m1.1.1.3.2.2">𝐼</ci><ci id="S3.T3.9.9.1.m1.1.1.3.2.3.cmml" xref="S3.T3.9.9.1.m1.1.1.3.2.3">𝑉</ci><ci id="S3.T3.9.9.1.m1.1.1.3.2.4.cmml" xref="S3.T3.9.9.1.m1.1.1.3.2.4">𝐴</ci></apply><ci id="S3.T3.9.9.1.m1.1.1.3.3.cmml" xref="S3.T3.9.9.1.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.1.m1.1c">T\rightarrow I/V/A+T</annotation></semantics></math></td>
<td id="S3.T3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">14.7K</td>
</tr>
<tr id="S3.T3.10.10" class="ltx_tr">
<td id="S3.T3.10.10.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">MosIT</td>
<td id="S3.T3.10.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math id="S3.T3.10.10.1.m1.1" class="ltx_Math" alttext="I+V+A+T\rightarrow I+V+A+T" display="inline"><semantics id="S3.T3.10.10.1.m1.1a"><mrow id="S3.T3.10.10.1.m1.1.1" xref="S3.T3.10.10.1.m1.1.1.cmml"><mrow id="S3.T3.10.10.1.m1.1.1.2" xref="S3.T3.10.10.1.m1.1.1.2.cmml"><mi id="S3.T3.10.10.1.m1.1.1.2.2" xref="S3.T3.10.10.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T3.10.10.1.m1.1.1.2.1" xref="S3.T3.10.10.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.2.3" xref="S3.T3.10.10.1.m1.1.1.2.3.cmml">V</mi><mo id="S3.T3.10.10.1.m1.1.1.2.1a" xref="S3.T3.10.10.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.2.4" xref="S3.T3.10.10.1.m1.1.1.2.4.cmml">A</mi><mo id="S3.T3.10.10.1.m1.1.1.2.1b" xref="S3.T3.10.10.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.2.5" xref="S3.T3.10.10.1.m1.1.1.2.5.cmml">T</mi></mrow><mo stretchy="false" id="S3.T3.10.10.1.m1.1.1.1" xref="S3.T3.10.10.1.m1.1.1.1.cmml">→</mo><mrow id="S3.T3.10.10.1.m1.1.1.3" xref="S3.T3.10.10.1.m1.1.1.3.cmml"><mi id="S3.T3.10.10.1.m1.1.1.3.2" xref="S3.T3.10.10.1.m1.1.1.3.2.cmml">I</mi><mo id="S3.T3.10.10.1.m1.1.1.3.1" xref="S3.T3.10.10.1.m1.1.1.3.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.3.3" xref="S3.T3.10.10.1.m1.1.1.3.3.cmml">V</mi><mo id="S3.T3.10.10.1.m1.1.1.3.1a" xref="S3.T3.10.10.1.m1.1.1.3.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.3.4" xref="S3.T3.10.10.1.m1.1.1.3.4.cmml">A</mi><mo id="S3.T3.10.10.1.m1.1.1.3.1b" xref="S3.T3.10.10.1.m1.1.1.3.1.cmml">+</mo><mi id="S3.T3.10.10.1.m1.1.1.3.5" xref="S3.T3.10.10.1.m1.1.1.3.5.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.1.m1.1b"><apply id="S3.T3.10.10.1.m1.1.1.cmml" xref="S3.T3.10.10.1.m1.1.1"><ci id="S3.T3.10.10.1.m1.1.1.1.cmml" xref="S3.T3.10.10.1.m1.1.1.1">→</ci><apply id="S3.T3.10.10.1.m1.1.1.2.cmml" xref="S3.T3.10.10.1.m1.1.1.2"><plus id="S3.T3.10.10.1.m1.1.1.2.1.cmml" xref="S3.T3.10.10.1.m1.1.1.2.1"></plus><ci id="S3.T3.10.10.1.m1.1.1.2.2.cmml" xref="S3.T3.10.10.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T3.10.10.1.m1.1.1.2.3.cmml" xref="S3.T3.10.10.1.m1.1.1.2.3">𝑉</ci><ci id="S3.T3.10.10.1.m1.1.1.2.4.cmml" xref="S3.T3.10.10.1.m1.1.1.2.4">𝐴</ci><ci id="S3.T3.10.10.1.m1.1.1.2.5.cmml" xref="S3.T3.10.10.1.m1.1.1.2.5">𝑇</ci></apply><apply id="S3.T3.10.10.1.m1.1.1.3.cmml" xref="S3.T3.10.10.1.m1.1.1.3"><plus id="S3.T3.10.10.1.m1.1.1.3.1.cmml" xref="S3.T3.10.10.1.m1.1.1.3.1"></plus><ci id="S3.T3.10.10.1.m1.1.1.3.2.cmml" xref="S3.T3.10.10.1.m1.1.1.3.2">𝐼</ci><ci id="S3.T3.10.10.1.m1.1.1.3.3.cmml" xref="S3.T3.10.10.1.m1.1.1.3.3">𝑉</ci><ci id="S3.T3.10.10.1.m1.1.1.3.4.cmml" xref="S3.T3.10.10.1.m1.1.1.3.4">𝐴</ci><ci id="S3.T3.10.10.1.m1.1.1.3.5.cmml" xref="S3.T3.10.10.1.m1.1.1.3.5">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.1.m1.1c">I+V+A+T\rightarrow I+V+A+T</annotation></semantics></math></td>
<td id="S3.T3.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">5K</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.7.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.8.2" class="ltx_text" style="font-size:90%;">Alignment tuning data for Type-C. <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib23" title="" class="ltx_ref">2024a</a>]</cite></span></figcaption>
<table id="S3.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.5.6.1" class="ltx_tr">
<th id="S3.T4.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Dataset</th>
<th id="S3.T4.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Modality</th>
<th id="S3.T4.5.6.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Samples</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VLGuard’s IT</th>
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><mrow id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml"><mrow id="S3.T4.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.m1.1.1.2.cmml"><mi id="S3.T4.1.1.1.m1.1.1.2.2" xref="S3.T4.1.1.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T4.1.1.1.m1.1.1.2.1" xref="S3.T4.1.1.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T4.1.1.1.m1.1.1.2.3" xref="S3.T4.1.1.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T4.1.1.1.m1.1.1.1" xref="S3.T4.1.1.1.m1.1.1.1.cmml">→</mo><mi id="S3.T4.1.1.1.m1.1.1.3" xref="S3.T4.1.1.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1"><ci id="S3.T4.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1.1">→</ci><apply id="S3.T4.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.m1.1.1.2"><plus id="S3.T4.1.1.1.m1.1.1.2.1.cmml" xref="S3.T4.1.1.1.m1.1.1.2.1"></plus><ci id="S3.T4.1.1.1.m1.1.1.2.2.cmml" xref="S3.T4.1.1.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T4.1.1.1.m1.1.1.2.3.cmml" xref="S3.T4.1.1.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T4.1.1.1.m1.1.1.3.cmml" xref="S3.T4.1.1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_t">3K</td>
</tr>
<tr id="S3.T4.2.2" class="ltx_tr">
<th id="S3.T4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">RTVLM</th>
<td id="S3.T4.2.2.1" class="ltx_td ltx_align_center"><math id="S3.T4.2.2.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T4.2.2.1.m1.1a"><mrow id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml"><mrow id="S3.T4.2.2.1.m1.1.1.2" xref="S3.T4.2.2.1.m1.1.1.2.cmml"><mi id="S3.T4.2.2.1.m1.1.1.2.2" xref="S3.T4.2.2.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T4.2.2.1.m1.1.1.2.1" xref="S3.T4.2.2.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T4.2.2.1.m1.1.1.2.3" xref="S3.T4.2.2.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T4.2.2.1.m1.1.1.1" xref="S3.T4.2.2.1.m1.1.1.1.cmml">→</mo><mi id="S3.T4.2.2.1.m1.1.1.3" xref="S3.T4.2.2.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><apply id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1"><ci id="S3.T4.2.2.1.m1.1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1.1">→</ci><apply id="S3.T4.2.2.1.m1.1.1.2.cmml" xref="S3.T4.2.2.1.m1.1.1.2"><plus id="S3.T4.2.2.1.m1.1.1.2.1.cmml" xref="S3.T4.2.2.1.m1.1.1.2.1"></plus><ci id="S3.T4.2.2.1.m1.1.1.2.2.cmml" xref="S3.T4.2.2.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T4.2.2.1.m1.1.1.2.3.cmml" xref="S3.T4.2.2.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T4.2.2.1.m1.1.1.3.cmml" xref="S3.T4.2.2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_center">5K</td>
</tr>
<tr id="S3.T4.3.3" class="ltx_tr">
<th id="S3.T4.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">MMViG</th>
<td id="S3.T4.3.3.1" class="ltx_td ltx_align_center"><math id="S3.T4.3.3.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T4.3.3.1.m1.1a"><mrow id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml"><mrow id="S3.T4.3.3.1.m1.1.1.2" xref="S3.T4.3.3.1.m1.1.1.2.cmml"><mi id="S3.T4.3.3.1.m1.1.1.2.2" xref="S3.T4.3.3.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T4.3.3.1.m1.1.1.2.1" xref="S3.T4.3.3.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T4.3.3.1.m1.1.1.2.3" xref="S3.T4.3.3.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T4.3.3.1.m1.1.1.1" xref="S3.T4.3.3.1.m1.1.1.1.cmml">→</mo><mi id="S3.T4.3.3.1.m1.1.1.3" xref="S3.T4.3.3.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><apply id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1"><ci id="S3.T4.3.3.1.m1.1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1.1">→</ci><apply id="S3.T4.3.3.1.m1.1.1.2.cmml" xref="S3.T4.3.3.1.m1.1.1.2"><plus id="S3.T4.3.3.1.m1.1.1.2.1.cmml" xref="S3.T4.3.3.1.m1.1.1.2.1"></plus><ci id="S3.T4.3.3.1.m1.1.1.2.2.cmml" xref="S3.T4.3.3.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T4.3.3.1.m1.1.1.2.3.cmml" xref="S3.T4.3.3.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T4.3.3.1.m1.1.1.3.cmml" xref="S3.T4.3.3.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T4.3.3.3" class="ltx_td ltx_align_center">16K</td>
</tr>
<tr id="S3.T4.4.4" class="ltx_tr">
<th id="S3.T4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaVA-RLHF</th>
<td id="S3.T4.4.4.1" class="ltx_td ltx_align_center"><math id="S3.T4.4.4.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T4.4.4.1.m1.1a"><mrow id="S3.T4.4.4.1.m1.1.1" xref="S3.T4.4.4.1.m1.1.1.cmml"><mrow id="S3.T4.4.4.1.m1.1.1.2" xref="S3.T4.4.4.1.m1.1.1.2.cmml"><mi id="S3.T4.4.4.1.m1.1.1.2.2" xref="S3.T4.4.4.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T4.4.4.1.m1.1.1.2.1" xref="S3.T4.4.4.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T4.4.4.1.m1.1.1.2.3" xref="S3.T4.4.4.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T4.4.4.1.m1.1.1.1" xref="S3.T4.4.4.1.m1.1.1.1.cmml">→</mo><mi id="S3.T4.4.4.1.m1.1.1.3" xref="S3.T4.4.4.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.1.m1.1b"><apply id="S3.T4.4.4.1.m1.1.1.cmml" xref="S3.T4.4.4.1.m1.1.1"><ci id="S3.T4.4.4.1.m1.1.1.1.cmml" xref="S3.T4.4.4.1.m1.1.1.1">→</ci><apply id="S3.T4.4.4.1.m1.1.1.2.cmml" xref="S3.T4.4.4.1.m1.1.1.2"><plus id="S3.T4.4.4.1.m1.1.1.2.1.cmml" xref="S3.T4.4.4.1.m1.1.1.2.1"></plus><ci id="S3.T4.4.4.1.m1.1.1.2.2.cmml" xref="S3.T4.4.4.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T4.4.4.1.m1.1.1.2.3.cmml" xref="S3.T4.4.4.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T4.4.4.1.m1.1.1.3.cmml" xref="S3.T4.4.4.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T4.4.4.3" class="ltx_td ltx_align_center">10K</td>
</tr>
<tr id="S3.T4.5.5" class="ltx_tr">
<th id="S3.T4.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">RLHF-V’s IT</th>
<td id="S3.T4.5.5.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T4.5.5.1.m1.1" class="ltx_Math" alttext="I+T\rightarrow T" display="inline"><semantics id="S3.T4.5.5.1.m1.1a"><mrow id="S3.T4.5.5.1.m1.1.1" xref="S3.T4.5.5.1.m1.1.1.cmml"><mrow id="S3.T4.5.5.1.m1.1.1.2" xref="S3.T4.5.5.1.m1.1.1.2.cmml"><mi id="S3.T4.5.5.1.m1.1.1.2.2" xref="S3.T4.5.5.1.m1.1.1.2.2.cmml">I</mi><mo id="S3.T4.5.5.1.m1.1.1.2.1" xref="S3.T4.5.5.1.m1.1.1.2.1.cmml">+</mo><mi id="S3.T4.5.5.1.m1.1.1.2.3" xref="S3.T4.5.5.1.m1.1.1.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T4.5.5.1.m1.1.1.1" xref="S3.T4.5.5.1.m1.1.1.1.cmml">→</mo><mi id="S3.T4.5.5.1.m1.1.1.3" xref="S3.T4.5.5.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.1.m1.1b"><apply id="S3.T4.5.5.1.m1.1.1.cmml" xref="S3.T4.5.5.1.m1.1.1"><ci id="S3.T4.5.5.1.m1.1.1.1.cmml" xref="S3.T4.5.5.1.m1.1.1.1">→</ci><apply id="S3.T4.5.5.1.m1.1.1.2.cmml" xref="S3.T4.5.5.1.m1.1.1.2"><plus id="S3.T4.5.5.1.m1.1.1.2.1.cmml" xref="S3.T4.5.5.1.m1.1.1.2.1"></plus><ci id="S3.T4.5.5.1.m1.1.1.2.2.cmml" xref="S3.T4.5.5.1.m1.1.1.2.2">𝐼</ci><ci id="S3.T4.5.5.1.m1.1.1.2.3.cmml" xref="S3.T4.5.5.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S3.T4.5.5.1.m1.1.1.3.cmml" xref="S3.T4.5.5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.1.m1.1c">I+T\rightarrow T</annotation></semantics></math></td>
<td id="S3.T4.5.5.3" class="ltx_td ltx_align_center ltx_border_bb">1.4K</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Type-D: Tokenized Early Fusion (TEF)</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In Type-D, multimodal inputs are tokenized using a common tokenizer or modality specific tokenizers.
The tokenized inputs are then given to a pretrained LLM <a href="#S3.SS4.SSS1" title="3.4.1 Subtype D.1: Models using LLM ‣ 3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.1</span></a> or an encoder-decoder transformer model <a href="#S3.SS4.SSS2" title="3.4.2 Subtype D.2: Models using Encoder-Decoder style Transformer ‣ 3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>, which generates multimodal outputs.
Figure <a href="#S3.F6" title="Figure 6 ‣ 3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> represents a general Type-D multimodal model architecture.
Either pretrained modality specific tokenizers are used, or a tokenizer training stage is included in the training process.
The fundamental advantage of tokenizing the inputs is that, the model now can be trained auto-regressively to generate image, audio and different modality tokens along with text tokens.
Models belonging to Type-D include, LaVIT <cite class="ltx_cite ltx_citemacro_cite">Jin et al. [<a href="#bib.bib175" title="" class="ltx_ref">2024</a>]</cite>, TEAL <cite class="ltx_cite ltx_citemacro_cite">Yang et al. [<a href="#bib.bib5" title="" class="ltx_ref">2024</a>]</cite>, CM3Leon <cite class="ltx_cite ltx_citemacro_cite">Yu et al. [<a href="#bib.bib176" title="" class="ltx_ref">2023</a>]</cite>, VL-GPT <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib177" title="" class="ltx_ref">2023b</a>]</cite>, Unicode <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. [<a href="#bib.bib178" title="" class="ltx_ref">2024</a>]</cite>, SEED <cite class="ltx_cite ltx_citemacro_cite">Ge et al. [<a href="#bib.bib179" title="" class="ltx_ref">2023</a>]</cite>, 4M <cite class="ltx_cite ltx_citemacro_cite">Mizrahi et al. [<a href="#bib.bib3" title="" class="ltx_ref">2024</a>]</cite>, Unified-IO <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib22" title="" class="ltx_ref">2022a</a>]</cite>, Unified-IO-2 <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>.
The comparative advantages and disadvantages of the Type-D multimodal model architecture, in relation to Types A, B, and C, are detailed in Section<a href="#S4.SS4" title="4.4 Type-D ‣ 4 Advantages and Disadvantages ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2405.17927/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Type-D multimodal model architecture. The tokenized input modalities are directly fed into the model at its input. Either a decoder-only transformer (sub-type D.1) or an encoder-decoder style transformer (sub-type D.2) is used a the multimodal transformer in this architecture.</span></figcaption>
</figure>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Subtype D.1: Models using LLM</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Models that primarily use LLM are LaVIT, TEAL, CM3Leon, SEED, Unicode, VL-GPT.
<span id="S3.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_bold">LaVIT</span> aims at unified generative training for image and text modalities.
It is achieved using a visual tokenizer.
After image encoding using an image encoder, a visual tokenizer is used in LaVIT model architecture to tokenize the visual inputs.
<span id="S3.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_bold">TEAL</span>, tokenizes all modalities.
It has a tokenizer and a detokenizer module in its architecture.
A projection layer is used for connecting non-textual modalities to the LLM.
<span id="S3.SS4.SSS1.p1.1.3" class="ltx_text ltx_font_bold">CM3Leon</span> uses image tokenizer to tokenize images and then directly provide it to the LLM.
It uses OPT model as a LLM.
In <span id="S3.SS4.SSS1.p1.1.4" class="ltx_text ltx_font_bold">VL-GPT</span>, a tokenizer is first trained to convert images to image tokens.
Later, the trained tokenizer is used to feed the images to the LLM.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p"><span id="S3.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
<span id="S3.SS4.SSS1.p2.1.2" class="ltx_text ltx_font_bold">LaVIT</span> model is trained in two stages.
In stage I, a image tokenizer is trained.
In stage II, the main LaVIT model is trained.
LaVIT utilizes general auto-regressive objective function where likelihood of each multi-modal sequence is directly maximized <cite class="ltx_cite ltx_citemacro_cite">Jin et al. [<a href="#bib.bib175" title="" class="ltx_ref">2024</a>]</cite>.
Since both image and text are already represented as discrete tokens, the cross-entropy loss is used to supervise the token prediction at each location for both modalities (image and text) with a shared prediction head <cite class="ltx_cite ltx_citemacro_cite">Jin et al. [<a href="#bib.bib175" title="" class="ltx_ref">2024</a>]</cite>.
<span id="S3.SS4.SSS1.p2.1.3" class="ltx_text ltx_font_bold">TEAL</span> employs pretrained tokenizers.
During pretraining of TEAL, only the projection layers are trained.
The LLM and the encoders are frozen for textual and non-textual embedding alignment during pretraining.
It uses image-text and audio-text pairs for pretraining. Finetuning is done on downstream tasks.
Datasets like COCO-Caption, Science-QA, and CoVoST 2 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib180" title="" class="ltx_ref">2020b</a>]</cite> are used.
It uses general auto-regressive objective for training.
<span id="S3.SS4.SSS1.p2.1.4" class="ltx_text ltx_font_bold">CM3Leon</span> too uses standard next-token prediction loss (auto-regressive).
For pretraining, LAION and licensed images from shuttershock are used.
Finetuning uses COCO Captioning (2015), Flickr30k, Image Paragraph <cite class="ltx_cite ltx_citemacro_cite">Krause et al. [<a href="#bib.bib181" title="" class="ltx_ref">2017</a>]</cite>, Localized Narratives <cite class="ltx_cite ltx_citemacro_cite">Pont-Tuset et al. [<a href="#bib.bib182" title="" class="ltx_ref">2020</a>]</cite>, VQA2, VizWiz, OKVQA, ScienceQA, and InstructPix2Pix <cite class="ltx_cite ltx_citemacro_cite">Brooks et al. [<a href="#bib.bib183" title="" class="ltx_ref">2023</a>]</cite> datasets.
<span id="S3.SS4.SSS1.p2.1.5" class="ltx_text ltx_font_bold">VL-GPT</span>, also tokenizes the inputs, hence it too is able to use standard auto-regressive objective function for training.
Pretraining involves utilizing both image-text pairs and interleaved image-text sequences.
CC3M, LAION-Aestheics, LAION-COCO, Multimodal-C4 (MMC4) and OBELICS datasets are used for pretraining.
Later, the model is instruction tuned using LLAVA data <cite class="ltx_cite ltx_citemacro_cite">Liu et al. [<a href="#bib.bib89" title="" class="ltx_ref">2024a</a>]</cite>, SVIT, COCO Caption, InstructPix2Pix and Magicbrush <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib184" title="" class="ltx_ref">2024d</a>]</cite> datasets.</p>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para">
<p id="S3.SS4.SSS1.p3.1" class="ltx_p"><span id="S3.SS4.SSS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
For LaVIT, 64 A100 GPUs and 12 hour training required to train tokenizer. 256 A100 GPUs and 36 hour training required for pretraining full LaVIT model.
TEAL <cite class="ltx_cite ltx_citemacro_cite">Yang et al. [<a href="#bib.bib5" title="" class="ltx_ref">2024</a>]</cite> is fully trained with 8 A100 GPUs.
CM3Leon <cite class="ltx_cite ltx_citemacro_cite">Yu et al. [<a href="#bib.bib176" title="" class="ltx_ref">2023</a>]</cite> was pretrained on 2 trillion tokens with 256 or 512 A100 GPUs and finetuned on 30 billion tokens with 64 or 128 A100 GPUs.
In VL-GPT <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib177" title="" class="ltx_ref">2023b</a>]</cite>, tokenizer training used 8 NVIDIA A100 (40G) GPUs for 10,000 iterations with batch size of 1024.
Pretraining utilized 32 GPUs for 20,000 iterations with batch size of 4096 and instruction tuning was performed with 4 GPUs for 10,000 iterations using batch size of 512.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Subtype D.2: Models using Encoder-Decoder style Transformer</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Models using encoder-decoder style transformer instead of LLM are Unified-IO, Unified-IO 2 and 4M.
In <span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Unified-IO</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib22" title="" class="ltx_ref">2022a</a>]</cite> and <span id="S3.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_bold">Unified-IO-2</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>, an encoder-decoder style transformer is used.
The input modalities are tokenized using VQ-GAN style tokenizers.
To tokenize images and dense structures, VQ-GAN is employed.
For audio, ViT-VQGAN is utilized.
While these models may bear resemblance to Type-C, they diverge notably. The critical distinction lies in Type-D models’ utilization of discrete input and output modality-specific tokens.
The <span id="S3.SS4.SSS2.p1.1.3" class="ltx_text ltx_font_bold">4M</span> model is a multimodal model capable of processing text, RGB images, depth, normals, semantic segmentation maps, and CLIP feature maps.
In contrast to Unified-IO which uses VQ-GAN style tokenizers to tokenize all modalities, 4M utilizes modality specific tokenizers.
Here, text is tokenized using WordPiece, and VQ-VAE is used to tokenize image and image-like modalities.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p"><span id="S3.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Training and data:</span>
<span id="S3.SS4.SSS2.p2.1.2" class="ltx_text ltx_font_bold">Unified-IO-2</span> is trained from scratch on a large multimodal pre-training data corpus from diverse sources with a multimodal Mixture of Denoisers (MoD) objective <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>.
UNIFIED-IO-2 contains 7 billion parameters and is pre-trained from scratch on an extensive variety of multimodal data – 1 billion image-text pairs, 1 trillion text tokens, 180 million video clips, 130 million interleaved image &amp; text, 3 million 3D assets, and 1 million agent trajectories <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>.
The model is instruction-tuned with a massive multimodal data by combining more than 120 datasets covering 220 tasks across vision, language, audio, and action <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite>.
<span id="S3.SS4.SSS2.p2.1.3" class="ltx_text ltx_font_bold">4M</span> uses MultiMAE <cite class="ltx_cite ltx_citemacro_cite">Bachmann et al. [<a href="#bib.bib185" title="" class="ltx_ref">2022</a>]</cite> pretraining strategy, where it takes small set of tokens from all modalities at its input, and performs cross-modal prediction coding.
VQ-VAE is used for tokenizing image related modalities and WordPiece for text tokenization.
Conceptual Captions 12M (CC12M) is used for pretraining.
Finetuning datasets include ImageNet-21K, ImageNet-1K, COCO detection, ADE20K, and NYUv2.</p>
</div>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.1" class="ltx_p"><span id="S3.SS4.SSS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Compute resources:</span>
4M is pretrained on 500 billion tokens.
Both pretraining and finetuning utilizes 64 or 128 A100 GPUs.
For Unified-IO and Unified-IO-2, Google’s TPU are used. No other resource related details are provided in their work.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Advantages and Disadvantages</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Type-A</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Type-A (<a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) multimodal model architecture enables fine-grained control of how modality information flows in the model.
It is end-to-end trainable and omits design of custom layers by using standard learnable layers of transformers.
Example, standard cross-attention layer is used to fuse modalities in Type-A, while in Type-B (<a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), a specially designed layer is used to fuse modalities.
Type-A requires large number of training data samples and computational resources as compared to Type-B and Type-C multimodal model architectures.
Challenging to build model of this architecture type, due to the prerequisite understanding of the internal layers of the LLM.
Architecture is difficult to scale as compared to Type-C (<a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>),
especially if pretraining step is involved, because of the large number of training parameters and computational requirements.
Adding more modalities is challenging, because in Type-A, after adding image modality cross-attention layer to the LLM layer, adding other modalities to each LLM layer is difficult and has not been explored in current literature to best of our knowledge.
Type-B addresses this challenge with a gating mechanism, which allows direct addition of input modalities to the output LLM layers.
The gating mechanism involves a single learnable parameter, a multiplication and an addition operation.
The input modality is multiplied with the learnable parameter determining the contribution of the modality, later, its output is directly added to the LLM layer output.
Number of trainable parameters can be large, due to addition of learnable cross-attention layer in each LLM layer.
Unlike Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, which accommodate an autoregressive training objective across diverse modalities, Type-A encounter increased complexity when applying a standard autoregressive training objective to modalities beyond text.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Type-B</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Similar to Type-A (<a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), Type-B (<a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) also benefits from fine-grained control of how modality information flows in the model.
It is end-to-end trainable.
In contrast to Type-A, custom-designed layers are used in Type-B.
The custom design adds to more fine-grained control of modality fusion.
Compared to Type-A, the efficient custom design of the layers and the architecture of the model mitigate the need for extensive training data samples and computational resources.
Building these model require the knowledge of the internal layers of the LLM.
In contrast to Type-A, Type-B architecture is more scalable, due to customizable nature and computational efficiency of the custom learnable connector layers.
However, scaling may still present challenges in comparison to Type-C (<a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) architecture.
Adding more modalities is simplified compared to Type-A.
In Type-A, the addition of further modalities to each LLM layer becomes significantly more challenging after the inclusion of an image modality cross-attention layer, an area that remains underexplored in the literature.
The Type-B, uniquely provides an alternative by introducing a gating mechanism which can be utilized to add other modalities.
The gating mechanism enables direct addition of input modalities to the output LLM layers.
Number of trainable parameters can be controlled by design of efficient custom connector layers, hence Type-B can be efficient in terms of number of trainable parameters.
In contrast to Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, where an autoregressive training objective is readily applicable across various modalities, the implementation of a standard autoregressive training objective in Type-B for non-textual modalities presents greater complexity.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Type-C</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Type-C <a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> architecture is modular.
Parts of the model architecture can be swapped and the resulting new model can trained efficiently for multimodal tasks.
Unlike Type-A <a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and Type-B <a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, Type-C does not have fine-grained control of how modality information flows in the model.
Different modality inputs are fused only at the input of decoder (LLM).
It is end-to-end trainable.
Type-C requires less training data and computational resources as compared to Type-A, B and D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> multimodal model architectures.
It is easier to build compared to all other type of multimodal architectures, owing to its modular architecture.
Unlike Type-A and Type-B architectures, Type-C models do not necessitate detailed knowledge of the internal layers of the LLM. Instead, only the interface details of the LLM or encoder being newly integrated are required.
Type-C architecture is scalable due to its modular design, reduced training data requirements, and computational efficiency.
Adding more modalities is easier in Type-C, compared to Type-A, B and D.
A simple learnable Linear/MLP/Q-former/custom layer can be added between the modality encoder and the LLM and trained efficiently for augmenting different modalities.
Number of trainable parameters is least in Type-C compared to Type-A, B and D.
Hence, it is compute resource efficient from training perspective.
Unlike Type-D, where an auto-regressive objective can be used to train different modalities, here in Type-C, it is challenging to utilize standard auto-regressive objective for modalities other than text (language).
Type-C provides an alternate way to Type-D, for any-to-any multimodal model development due elimination of input modality tokenizers, its modular architecture, training efficiency and end-to-end trainable nature.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Type-D</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Type-D <a href="#S3.SS4" title="3.4 Type-D: Tokenized Early Fusion (TEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> has a simplified model architecture due to tokenization of input and output modalities, when compared to Type-A <a href="#S3.SS1" title="3.1 Type-A: Standard Cross-Attention based Deep Fusion (SCDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, B <a href="#S3.SS2" title="3.2 Type-B: Custom Layer based Deep Fusion (CLDF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and C <a href="#S3.SS3" title="3.3 Type-C: Non-Tokenized Early Fusion (NTEF) ‣ 3 Multimodal Model Architectures: A Taxonomy ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
It tokenizes all modalities.
This characteristic can be perceived as both advantageous and disadvantageous. Tokenization offers the advantage of enabling all modality training through a standard auto-regressive objective function. However, the challenge lies in training a universal tokenizer or modality-specific tokenizers.
In other architecture types, output embeddings from modality encoders are directly provided to the LLM without (discrete) tokenization.
Unlike Type-A and Type-B, Type-D does not have fine-grained control of how modality information flows in the model.
Different modality inputs are fused only at the input of the main transformer model.
This main transformer can be a decoder (LLM) or a encoder-decoder style transformer.
It is end-to-end trainable.
It requires large training data and computational resources as compared to Type-A, B and C multimodal model architectures.
Type-D model architecture are comparatively easier to construct than Type-A and Type-B models but is more complex to build compared to Type-C architecture type.
Type-D architecture is scalable, due to tokenization of modalities.
Incorporating additional modalities poses challenges, as it necessitates training a new tokenizer for each new modality or adapting an existing multimodal tokenizer, which can be a non-trivial undertaking.
All the modalities are learnt by the main transformer, and to add another modality or task, the model has be trained, and efficient training strategies for this type of model architecture have not been extensively investigated in the literature, to the best of our knowledge.
Adding additional modalities to the model architecture is simplest in the Type-C, when compared to Type-A, B and D.
Type-D has large number of trainable parameters compared to Type-A, B and C.
Since the LLM or transformer has to learn new modality tokens, the model has to be trained, and training in Type-D is computationally intensive.
In Type-D, an auto-regressive objective can be used to train different (all) modalities.
Like Type-C, Type-D provides a way for any-to-any multimodal model development, enabled by its tokenization of modalities and simplified training objective.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Next Generation Multimodal Architectures</h2>

<figure id="S5.F7" class="ltx_figure"><img src="/html/2405.17927/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Any-to-any Multimodal Model development timeline. The evolution from single modality models (left) to any-to-any modality models (right) is depicted. Any-to-any multimodal models belonging to Type-C and Type-D are noted in the figure. An alternate development timeline (green line at the bottom) for non-transformer based models like SSM (State-space models) is shown. Mamba is a language model. VL-mamba and Cobra are vision-language models.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section explores multimodal models with multimodal-input and multimodal-output.
Plethora of models exist for any-input-modality to output-text-modality.
In contrast, there are significantly fewer multimodal models capable of generating output modalities other than text.
Multimodal output generation is one of the primary challenge in the multimodal domain.
Type-C and Type-D multimodal architectures are at the forefront of development for any-to-any multimodal models.
The representative models are highlighted in Figure <a href="#S5.F7" title="Figure 7 ‣ 5 Next Generation Multimodal Architectures ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
These dominant multimodal model architectures address some, though not all, challenging aspects of multimodal generation.
Type-D simplifies the training process by utilizing input tokenization, enabling the use of a standard auto-regressive objective function for model training.
However, it still faces limitations in addressing the challenge of accommodating large data sizes and computational demands necessary for building multimodal generative models.
Type-C tackles the challenges associated with data and resources by leveraging pretrained components and integrating them with efficient connectors/adapters.
Yet, the training process remains challenging because of the diverse objective functions associated with different components in the model architecture.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">At present, there are three primary approaches for constructing any-to-any multimodal models: the first involves utilizing the end-to-end trainable Type-D model architecture, the second entails leveraging the end-to-end trainable Type-C architecture, and the third method employs a combination of Type-C with agents, which is non-end-to-end trainable.
<span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Type-D architecture</span>: Models generate multimodal outputs using tokenizers.
Unified-IO, Unified-IO 2 and 4M are the models from Type-D which enable any-to-any multimodal model development.
<span id="S5.p2.1.2" class="ltx_text ltx_font_bold">Type-C architecture</span>: Models generate multimodal outputs without using tokenizers.
NExt-GPT, CoDI and CoDI-2 models belong to Type-C assisting in any-to-any multimodal model development.
<span id="S5.p2.1.3" class="ltx_text ltx_font_bold">Type-C + agents:</span>
In this method, a Type-C multimodal model is trained to generate specific text outputs with a general format to aid the frozen pretrained modality decoder models (like text-to-image models, text-to-video models) for multimodal generation.
ModaVerse follows this process for creating an any-to-any multimodal model.
The absence of an end-to-end training process results in performance that is not superior to the other two methods.
Table <a href="#S5.T5" title="Table 5 ‣ 5 Next Generation Multimodal Architectures ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.T6" title="Table 6 ‣ 5 Next Generation Multimodal Architectures ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares performance of any-to-any multimodal models.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.3.2" class="ltx_text" style="font-size:90%;">Comparing two next generation any-to-any models. <cite class="ltx_cite ltx_citemacro_cite">Lu et al. [<a href="#bib.bib2" title="" class="ltx_ref">2023</a>]</cite></span></figcaption>
<table id="S5.T5.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.4.1.1" class="ltx_tr">
<td id="S5.T5.4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Model</td>
<td id="S5.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Image generation</td>
<td id="S5.T5.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">Audio generation &amp; captioning</td>
<td id="S5.T5.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Video captioning</td>
</tr>
<tr id="S5.T5.4.2.2" class="ltx_tr">
<td id="S5.T5.4.2.2.1" class="ltx_td"></td>
<td id="S5.T5.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">FID↓</td>
<td id="S5.T5.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">TIFA↑</td>
<td id="S5.T5.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t">FAD↓</td>
<td id="S5.T5.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t">IS↑</td>
<td id="S5.T5.4.2.2.6" class="ltx_td ltx_align_center ltx_border_t">KL↓</td>
<td id="S5.T5.4.2.2.7" class="ltx_td ltx_align_center ltx_border_tt">CIDEr↑</td>
<td id="S5.T5.4.2.2.8" class="ltx_td ltx_align_center ltx_border_t">CIDEr↑</td>
</tr>
<tr id="S5.T5.4.3.3" class="ltx_tr">
<td id="S5.T5.4.3.3.1" class="ltx_td ltx_align_center ltx_border_tt">UnifiedIO-2</td>
<td id="S5.T5.4.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">13.39</td>
<td id="S5.T5.4.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">81.3</td>
<td id="S5.T5.4.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">2.64</td>
<td id="S5.T5.4.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">5.89</td>
<td id="S5.T5.4.3.3.6" class="ltx_td ltx_align_center ltx_border_tt">1.80</td>
<td id="S5.T5.4.3.3.7" class="ltx_td ltx_align_center ltx_border_tt">48.9</td>
<td id="S5.T5.4.3.3.8" class="ltx_td ltx_align_center ltx_border_tt">48.8</td>
</tr>
<tr id="S5.T5.4.4.4" class="ltx_tr">
<td id="S5.T5.4.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">CoDI</td>
<td id="S5.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">11.26</td>
<td id="S5.T5.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">71.6</td>
<td id="S5.T5.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">1.80</td>
<td id="S5.T5.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">8.77</td>
<td id="S5.T5.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb">1.40</td>
<td id="S5.T5.4.4.4.7" class="ltx_td ltx_align_center ltx_border_bb">78.9</td>
<td id="S5.T5.4.4.4.8" class="ltx_td ltx_align_center ltx_border_bb">74.4</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S5.T6.3.2" class="ltx_text" style="font-size:90%;">Comparing next generation any-to-any models. Generation and captioning metrics for each modality are captured in the table. <cite class="ltx_cite ltx_citemacro_cite">Wang et al. [<a href="#bib.bib133" title="" class="ltx_ref">2024a</a>]</cite></span></figcaption>
<table id="S5.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.4.1.1" class="ltx_tr">
<th id="S5.T6.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S5.T6.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Image</th>
<th id="S5.T6.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Audio</th>
<th id="S5.T6.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Video</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.4.2.1" class="ltx_tr">
<td id="S5.T6.4.2.1.1" class="ltx_td"></td>
<th id="S5.T6.4.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID↓</th>
<th id="S5.T6.4.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CIDEr↑</th>
<th id="S5.T6.4.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">IS↑</th>
<th id="S5.T6.4.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CIDEr↑</th>
<th id="S5.T6.4.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIPSIM↑</th>
<th id="S5.T6.4.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">METEOR↑</th>
</tr>
<tr id="S5.T6.4.3.2" class="ltx_tr">
<td id="S5.T6.4.3.2.1" class="ltx_td ltx_align_center ltx_border_tt">CoDI</td>
<td id="S5.T6.4.3.2.2" class="ltx_td ltx_align_center ltx_border_tt">11.26</td>
<td id="S5.T6.4.3.2.3" class="ltx_td ltx_align_center ltx_border_tt">149.9</td>
<td id="S5.T6.4.3.2.4" class="ltx_td ltx_align_center ltx_border_tt">8.77</td>
<td id="S5.T6.4.3.2.5" class="ltx_td ltx_align_center ltx_border_tt">0.789</td>
<td id="S5.T6.4.3.2.6" class="ltx_td ltx_align_center ltx_border_tt">0.2890</td>
<td id="S5.T6.4.3.2.7" class="ltx_td ltx_align_center ltx_border_tt">32.5</td>
</tr>
<tr id="S5.T6.4.4.3" class="ltx_tr">
<td id="S5.T6.4.4.3.1" class="ltx_td ltx_align_center">NExT-GPT</td>
<td id="S5.T6.4.4.3.2" class="ltx_td ltx_align_center">11.28</td>
<td id="S5.T6.4.4.3.3" class="ltx_td ltx_align_center">156.7</td>
<td id="S5.T6.4.4.3.4" class="ltx_td ltx_align_center">8.35</td>
<td id="S5.T6.4.4.3.5" class="ltx_td ltx_align_center">0.802</td>
<td id="S5.T6.4.4.3.6" class="ltx_td ltx_align_center">0.3085</td>
<td id="S5.T6.4.4.3.7" class="ltx_td ltx_align_center">38.5</td>
</tr>
<tr id="S5.T6.4.5.4" class="ltx_tr">
<td id="S5.T6.4.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">ModaVerse</td>
<td id="S5.T6.4.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">11.24</td>
<td id="S5.T6.4.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">151.4</td>
<td id="S5.T6.4.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">8.22</td>
<td id="S5.T6.4.5.4.5" class="ltx_td ltx_align_center ltx_border_bb">0.792</td>
<td id="S5.T6.4.5.4.6" class="ltx_td ltx_align_center ltx_border_bb">0.3014</td>
<td id="S5.T6.4.5.4.7" class="ltx_td ltx_align_center ltx_border_bb">35.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">State space models (SSMs) <cite class="ltx_cite ltx_citemacro_cite">Gu et al. [<a href="#bib.bib186" title="" class="ltx_ref">2021</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Smith et al. [<a href="#bib.bib187" title="" class="ltx_ref">2023</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">Gu and Dao [<a href="#bib.bib188" title="" class="ltx_ref">2023</a>]</cite>, are emerging as a viable alternative to transformers based model architectures.
They tackle the inherent quadratic complexity of attention mechanisms in Transformers.
Examples such as VL-Mamba <cite class="ltx_cite ltx_citemacro_cite">Qiao et al. [<a href="#bib.bib189" title="" class="ltx_ref">2024</a>]</cite> and Cobra <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib190" title="" class="ltx_ref">2024</a>]</cite> serve as compelling illustrations of how SSMs can be extended to incorporate multimodal learning capabilities.
Their architecture closely resembles that of Type-C multimodal model architectures; therefore, it has been incorporated into the Type-C section in the Figure <a href="#S0.F1" title="Figure 1 ‣ The Evolution of Multimodal Model Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Moreover, works such as MambaTalk <cite class="ltx_cite ltx_citemacro_cite">Xu et al. [<a href="#bib.bib191" title="" class="ltx_ref">2024</a>]</cite> and SpikeMba <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib192" title="" class="ltx_ref">2024c</a>]</cite> are enhancing SSMs by incorporating modalities such as audio and video, respectively.
While any-to-any multimodal models based on SSMs have not yet been developed, the potential exists to construct such models.
Therefore, in the future, SSMs may emerge as a robust alternative to Transformer-based Type-C and Type-D multimodal model architectures for any-to-any multimodal tasks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work uniquely identifies and categorizes existing multimodal model architectures in four types.
Each architecture type is discussed in detail, including visualization of the general model architecture along with insights into their respective characteristics.
Through a thorough examination of existing architectural patterns used in the development of any-to-any multimodal models, this research effort sheds light on the two prevalent approaches (Type-C and Type-D) that are currently driving advancements in this field.
By comparing &amp; contrasting architecture types to each other by describing their advantage &amp; disadvantages, this work aids in model choices.
This study map a broad spectrum of existing multimodal models to the four identified types.
Though the model list is comprehensive, it is not exhaustive.
By establishing a taxonomy of multimodal architectures, we can effectively track and capture the evolving trends and advancements within the multimodal domain.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. [2022]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:23716–23736, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2023]</span>
<span class="ltx_bibblock">
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.17172</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mizrahi et al. [2024]</span>
<span class="ltx_bibblock">
David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir.

</span>
<span class="ltx_bibblock">4m: Massively multimodal masked modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023a]</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Next-gpt: Any-to-any multimodal llm.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05519</em>, 2023a.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2024]</span>
<span class="ltx_bibblock">
Zhen Yang, Yingxue Zhang, Fandong Meng, and Jie Zhou.

</span>
<span class="ltx_bibblock">Teal: Tokenize and embed all for multi-modal large language models, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023a]</span>
<span class="ltx_bibblock">
Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Codi-2: In-context, interleaved, and interactive any-to-any generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18775</em>, 2023a.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2021]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awadalla et al. [2023]</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.

</span>
<span class="ltx_bibblock">Openflamingo: An open-source framework for training large autoregressive vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.01390</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Otter: A multi-modal model with in-context instruction tuning, 2023a.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2023]</span>
<span class="ltx_bibblock">
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.

</span>
<span class="ltx_bibblock">Multimodal-gpt: A vision and language model for dialogue with humans.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.04790</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. [2024a]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al.

</span>
<span class="ltx_bibblock">Obelics: An open web-scale filtered dataset of interleaved image-text documents.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023]</span>
<span class="ltx_bibblock">
Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao.

</span>
<span class="ltx_bibblock">Dolphins: Multimodal language model for driving.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.00438</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao.

</span>
<span class="ltx_bibblock">Llama-adapter: Efficient fine-tuning of language models with zero-init attention.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16199</em>, 2023a.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023]</span>
<span class="ltx_bibblock">
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al.

</span>
<span class="ltx_bibblock">Llama-adapter v2: Parameter-efficient visual instruction model.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.15010</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023a]</span>
<span class="ltx_bibblock">
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.

</span>
<span class="ltx_bibblock">Cogvlm: Visual expert for pretrained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.03079</em>, 2023a.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023a]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.

</span>
<span class="ltx_bibblock">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.04257</em>, 2023a.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023a]</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14238</em>, 2023a.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. [2024]</span>
<span class="ltx_bibblock">
Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al.

</span>
<span class="ltx_bibblock">Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.10208</em>, 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2024]</span>
<span class="ltx_bibblock">
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan.

</span>
<span class="ltx_bibblock">Moe-llava: Mixture of experts for large vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.15947</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2022a]</span>
<span class="ltx_bibblock">
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.

</span>
<span class="ltx_bibblock">Unified-io: A unified model for vision, language, and multi-modal tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>, 2022a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024a]</span>
<span class="ltx_bibblock">
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.

</span>
<span class="ltx_bibblock">Mm-llms: Recent advances in multimodal large language models, 2024a.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2024]</span>
<span class="ltx_bibblock">
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.

</span>
<span class="ltx_bibblock">A survey on multimodal large language models, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caffagni et al. [2024]</span>
<span class="ltx_bibblock">
Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara.

</span>
<span class="ltx_bibblock">The (r) evolution of multimodal large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.12451</em>, 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023b]</span>
<span class="ltx_bibblock">
Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao.

</span>
<span class="ltx_bibblock">Large-scale multi-modal pre-trained models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Machine Intelligence Research</em>, 20(4):447–482, 2023b.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023b]</span>
<span class="ltx_bibblock">
Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip.

</span>
<span class="ltx_bibblock">Multimodal large language models: A survey.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Big Data (BigData)</em>, pages 2247–2256. IEEE, 2023b.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2023]</span>
<span class="ltx_bibblock">
Ruifeng Guo, Jingxuan Wei, Linzhuang Sun, Bihui Yu, Guiyong Chang, Dawei Liu, Sibo Zhang, Zhengbing Yao, Mingjun Xu, and Liping Bu.

</span>
<span class="ltx_bibblock">A survey on image-text multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15857</em>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
Peng Xu, Xiatian Zhu, and David A Clifton.

</span>
<span class="ltx_bibblock">Multimodal learning with transformers: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baltrušaitis et al. [2018]</span>
<span class="ltx_bibblock">
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency.

</span>
<span class="ltx_bibblock">Multimodal machine learning: A survey and taxonomy.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, 41(2):423–443, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023b]</span>
<span class="ltx_bibblock">
Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al.

</span>
<span class="ltx_bibblock">Pali-x: On scaling up a multilingual vision and language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18565</em>, 2023b.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. [2021]</span>
<span class="ltx_bibblock">
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Unifying vision-and-language tasks via text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 1931–1942. PMLR, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. [2021]</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with noisy text supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 4904–4916. PMLR, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang.

</span>
<span class="ltx_bibblock">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4581–4591, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. [2018]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2017]</span>
<span class="ltx_bibblock">
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.

</span>
<span class="ltx_bibblock">Video question answering via gradually refined attention over appearance and motion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM international conference on Multimedia</em>, pages 1645–1653, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. [2017]</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Visual dialog.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 326–335, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Luowei Zhou, Chenliang Xu, and Jason Corso.

</span>
<span class="ltx_bibblock">Towards automatic learning of procedures from web instructional videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 32, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2019]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 8317–8326, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2024a]</span>
<span class="ltx_bibblock">
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi.

</span>
<span class="ltx_bibblock">Multimodal c4: An open, billion-scale corpus of images interleaved with text.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. [2022]</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:25278–25294, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2023]</span>
<span class="ltx_bibblock">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Instruction tuning with gpt-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03277</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. [2022]</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">A-okvqa: A benchmark for visual question answering using world knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 146–162. Springer, 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and Fei-Fei [2015]</span>
<span class="ltx_bibblock">
Andrej Karpathy and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3128–3137, 2015.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. [2019]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">2019 international conference on document analysis and recognition (ICDAR)</em>, pages 947–952. IEEE, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024a]</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023a]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10592</em>, 2023a.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022]</span>
<span class="ltx_bibblock">
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.

</span>
<span class="ltx_bibblock">Pali: A jointly-scaled multilingual language-image model.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.06794</em>, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. [2018]</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. [2019]</span>
<span class="ltx_bibblock">
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.

</span>
<span class="ltx_bibblock">Nocaps: Novel object captioning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 8948–8957, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. [2019]</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</em>, pages 3195–3204, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya et al. [2019]</span>
<span class="ltx_bibblock">
Manoj Acharya, Kushal Kafle, and Christopher Kanan.

</span>
<span class="ltx_bibblock">Tallyqa: Answering complex counting questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, volume 33, pages 8076–8084, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. [2020]</span>
<span class="ltx_bibblock">
Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya.

</span>
<span class="ltx_bibblock">Captioning images taken by people who are blind.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16</em>, pages 417–434. Springer, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. [2019]</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 4291–4301, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. [2022]</span>
<span class="ltx_bibblock">
Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.

</span>
<span class="ltx_bibblock">Infographicvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pages 1697–1706, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. [2021]</span>
<span class="ltx_bibblock">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for vqa on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, pages 2200–2209, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al. [2022]</span>
<span class="ltx_bibblock">
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.

</span>
<span class="ltx_bibblock">Chartqa: A benchmark for question answering about charts with visual and logical reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.10244</em>, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2016]</span>
<span class="ltx_bibblock">
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.

</span>
<span class="ltx_bibblock">Msr-vtt: A large video description dataset for bridging video and language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 5288–5296, 2016.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. [2017a]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.

</span>
<span class="ltx_bibblock">Dense-captioning events in videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pages 706–715, 2017a.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monfort et al. [2021]</span>
<span class="ltx_bibblock">
Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva.

</span>
<span class="ltx_bibblock">Spoken moments: Learning joint audio-visual representations from video descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 14871–14881, 2021.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2021]</span>
<span class="ltx_bibblock">
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Next-qa: Next phase of question-answering to explaining temporal actions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 9777–9786, 2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heafield [2011]</span>
<span class="ltx_bibblock">
Kenneth Heafield.

</span>
<span class="ltx_bibblock">Kenlm: Faster and smaller language model queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the sixth workshop on statistical machine translation</em>, pages 187–197, 2011.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. [2022]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al.

</span>
<span class="ltx_bibblock">The bigscience roots corpus: A 1.6 tb composite multilingual dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:31809–31826, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2022]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Flava: A foundational language and vision alignment model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 15638–15650, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Webster et al. [2023]</span>
<span class="ltx_bibblock">
Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie.

</span>
<span class="ltx_bibblock">On the de-duplication of laion-2b.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12733</em>, 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models (2023).

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2018]</span>
<span class="ltx_bibblock">
Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata.

</span>
<span class="ltx_bibblock">Textual explanations for self-driving vehicles.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>, pages 563–578, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pages 740–755. Springer, 2014.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. [2017b]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123:32–73, 2017b.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning [2019]</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 6700–6709, 2019.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 4995–5004, 2016.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. [2018]</span>
<span class="ltx_bibblock">
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.

</span>
<span class="ltx_bibblock">A corpus for reasoning about natural language grounded in photographs.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.00491</em>, 2018.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. [2016]</span>
<span class="ltx_bibblock">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Generation and comprehension of unambiguous object descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 11–20, 2016.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. [2016]</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia.

</span>
<span class="ltx_bibblock">Multi30k: Multilingual english-german image descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1605.00459</em>, 2016.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. [2023]</span>
<span class="ltx_bibblock">
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al.

</span>
<span class="ltx_bibblock">Cogagent: A visual language model for gui agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.08914</em>, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2024]</span>
<span class="ltx_bibblock">
Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al.

</span>
<span class="ltx_bibblock">Cogcom: Train large vision-language models diving into details through chain of manipulations.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.04236</em>, 2024.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2024]</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al.

</span>
<span class="ltx_bibblock">Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16420</em>, 2024.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023c]</span>
<span class="ltx_bibblock">
Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie.

</span>
<span class="ltx_bibblock">Lion: Empowering multimodal large language model with dual-level visual knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11860</em>, 2023c.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. [1991]</span>
<span class="ltx_bibblock">
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Adaptive mixtures of local experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, 3(1):79–87, 1991.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eigen et al. [2013]</span>
<span class="ltx_bibblock">
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning factored representations in a deep mixture of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.4314</em>, 2013.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ShareGPT [2023]</span>
<span class="ltx_bibblock">
Teams ShareGPT.

</span>
<span class="ltx_bibblock">Sharegpt: Share your wildest chatgpt conversations with one click, 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sharegpt.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sharegpt.com/</a>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byeon et al. [2022]</span>
<span class="ltx_bibblock">
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.

</span>
<span class="ltx_bibblock">Coyo-700m: Image-text pair dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kakaobrain/coyo-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kakaobrain/coyo-dataset</a>, 2022.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 19730–19742. PMLR, 2023b.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2022b]</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:2507–2521, 2022b.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024a]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36, 2024a.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023a]</span>
<span class="ltx_bibblock">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Aligning large multi-modal model with robust instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.14565</em>, 2023a.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024b]</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding, 2024b.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. [2015]</span>
<span class="ltx_bibblock">
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pages 2641–2649, 2015.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemzadeh et al. [2014]</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pages 787–798, 2014.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023d]</span>
<span class="ltx_bibblock">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.

</span>
<span class="ltx_bibblock">Shikra: Unleashing multimodal llm’s referential dialogue magic.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15195</em>, 2023d.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. [2021]</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 3558–3568, 2021.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et al. [2024]</span>
<span class="ltx_bibblock">
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.

</span>
<span class="ltx_bibblock">Datacomp: In search of the next generation of multimodal datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et al. [2020]</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>, pages 742–758. Springer, 2020.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al. [2023]</span>
<span class="ltx_bibblock">
Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium".

</span>
<span class="ltx_bibblock">Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://https://huggingface.co/Open-Orca/SlimOrca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://https://huggingface.co/Open-Orca/SlimOrca</a>.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2024]</span>
<span class="ltx_bibblock">
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.

</span>
<span class="ltx_bibblock">Mind2web: Towards a generalist agent for the web.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawles et al. [2024]</span>
<span class="ltx_bibblock">
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap.

</span>
<span class="ltx_bibblock">Androidinthewild: A large-scale dataset for android device control.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2022]</span>
<span class="ltx_bibblock">
Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al.

</span>
<span class="ltx_bibblock">Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:26418–26431, 2022.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. [2011]</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.

</span>
<span class="ltx_bibblock">Im2text: Describing images using 1 million captioned photographs.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 24, 2011.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2021a]</span>
<span class="ltx_bibblock">
Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.

</span>
<span class="ltx_bibblock">Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.13214</em>, 2021a.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kembhavi et al. [2016]</span>
<span class="ltx_bibblock">
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">A diagram is worth a dozen images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14</em>, pages 235–251. Springer, 2016.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020a]</span>
<span class="ltx_bibblock">
Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang.

</span>
<span class="ltx_bibblock">On the general value of evidence, and bilingual scene-text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 10126–10135, 2020a.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ustalov et al. [2023]</span>
<span class="ltx_bibblock">
Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, and Alisa Smirnova.

</span>
<span class="ltx_bibblock">Toloka visual question answering benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16511</em>, 2023.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023a]</span>
<span class="ltx_bibblock">
Bo Zhao, Boya Wu, and Tiejun Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.04087</em>, 2023a.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03744</em>, 2023b.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2019]</span>
<span class="ltx_bibblock">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Objects365: A large-scale, high-quality dataset for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 8430–8439, 2019.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023e]</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.12793</em>, 2023e.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15112</em>, 2023b.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2023]</span>
<span class="ltx_bibblock">
Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin.

</span>
<span class="ltx_bibblock">Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.10755</em>, 2023.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023c]</span>
<span class="ltx_bibblock">
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu.

</span>
<span class="ltx_bibblock">Mmc: Advancing multimodal chart understanding with large-scale instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.10774</em>, 2023c.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et al. [2019]</span>
<span class="ltx_bibblock">
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Mathqa: Towards interpretable math word problem solving with operation-based formalisms.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.13319</em>, 2019.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2021b]</span>
<span class="ltx_bibblock">
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.

</span>
<span class="ltx_bibblock">Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.04165</em>, 2021b.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. [2019]</span>
<span class="ltx_bibblock">
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar.

</span>
<span class="ltx_bibblock">Kvqa: Knowledge-aware visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, volume 33, pages 8876–8884, 2019.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023c]</span>
<span class="ltx_bibblock">
Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang.

</span>
<span class="ltx_bibblock">To see is to believe: Prompting gpt-4v for better visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07574</em>, 2023c.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. [2023]</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">See https://vicuna. lmsys. org (accessed 14 April 2023)</em>, 2(3):6, 2023.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2023]</span>
<span class="ltx_bibblock">
InternLM Team.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced capabilities, 2023.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2024]</span>
<span class="ltx_bibblock">
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al.

</span>
<span class="ltx_bibblock">Deepseek-vl: towards real-world vision-language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.05525</em>, 2024.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2024b]</span>
<span class="ltx_bibblock">
Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.

</span>
<span class="ltx_bibblock">Llava-<math id="bib.bib121.1.m1.1" class="ltx_Math" alttext="phi" display="inline"><semantics id="bib.bib121.1.m1.1a"><mrow id="bib.bib121.1.m1.1.1" xref="bib.bib121.1.m1.1.1.cmml"><mi id="bib.bib121.1.m1.1.1.2" xref="bib.bib121.1.m1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="bib.bib121.1.m1.1.1.1" xref="bib.bib121.1.m1.1.1.1.cmml">​</mo><mi id="bib.bib121.1.m1.1.1.3" xref="bib.bib121.1.m1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="bib.bib121.1.m1.1.1.1a" xref="bib.bib121.1.m1.1.1.1.cmml">​</mo><mi id="bib.bib121.1.m1.1.1.4" xref="bib.bib121.1.m1.1.1.4.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="bib.bib121.1.m1.1b"><apply id="bib.bib121.1.m1.1.1.cmml" xref="bib.bib121.1.m1.1.1"><times id="bib.bib121.1.m1.1.1.1.cmml" xref="bib.bib121.1.m1.1.1.1"></times><ci id="bib.bib121.1.m1.1.1.2.cmml" xref="bib.bib121.1.m1.1.1.2">𝑝</ci><ci id="bib.bib121.1.m1.1.1.3.cmml" xref="bib.bib121.1.m1.1.1.3">ℎ</ci><ci id="bib.bib121.1.m1.1.1.4.cmml" xref="bib.bib121.1.m1.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib121.1.m1.1c">phi</annotation></semantics></math>: Efficient multi-modal assistant with small language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.02330</em>, 2024b.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://llava-vl.github.io/blog/2024-01-30-llava-next/</a>.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. [2023]</span>
<span class="ltx_bibblock">
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.03378</em>, 2023.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023f]</span>
<span class="ltx_bibblock">
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.09478</em>, 2023f.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pi et al. [2023]</span>
<span class="ltx_bibblock">
Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang.

</span>
<span class="ltx_bibblock">Detgpt: Detect what you need via reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14167</em>, 2023.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2023]</span>
<span class="ltx_bibblock">
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.

</span>
<span class="ltx_bibblock">Pandagpt: One model to instruction-follow them all.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16355</em>, 2023.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh et al. [2024]</span>
<span class="ltx_bibblock">
Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov.

</span>
<span class="ltx_bibblock">Generating images with multimodal language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023c]</span>
<span class="ltx_bibblock">
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.

</span>
<span class="ltx_bibblock">Gpt4roi: Instruction tuning large language model on region-of-interest.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.03601</em>, 2023c.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023b]</span>
<span class="ltx_bibblock">
Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong, Chunrui Han, et al.

</span>
<span class="ltx_bibblock">Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09474</em>, 2023b.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bavishi et al. [2023]</span>
<span class="ltx_bibblock">
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.

</span>
<span class="ltx_bibblock">Introducing our multimodal models, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.adept.ai/blog/fuyu-8b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.adept.ai/blog/fuyu-8b</a>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh et al. [2023]</span>
<span class="ltx_bibblock">
Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.

</span>
<span class="ltx_bibblock">Grounding language models to images for multimodal inputs and outputs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 17283–17300. PMLR, 2023.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024b]</span>
<span class="ltx_bibblock">
Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, and Tao Wang.

</span>
<span class="ltx_bibblock">Groundinggpt:language enhanced multi-modal grounding model, 2024b.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024a]</span>
<span class="ltx_bibblock">
Xinyu Wang, Bohan Zhuang, and Qi Wu.

</span>
<span class="ltx_bibblock">Modaverse: Efficiently transforming modalities with llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.06395</em>, 2024a.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024b]</span>
<span class="ltx_bibblock">
Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Xiaohua, Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao.

</span>
<span class="ltx_bibblock">Mllm-tool: A multimodal large language model for tool agent learning, 2024b.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2024]</span>
<span class="ltx_bibblock">
Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li.

</span>
<span class="ltx_bibblock">Vigor: Improving visual grounding of large vision language models with fine-grained reward modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06118</em>, 2024.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2024]</span>
<span class="ltx_bibblock">
Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Any-to-any generation via composable diffusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023g]</span>
<span class="ltx_bibblock">
Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu.

</span>
<span class="ltx_bibblock">X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.04160</em>, 2023g.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2024]</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023]</span>
<span class="ltx_bibblock">
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Generative pretraining in multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.05222</em>, 2023.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023d]</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02858</em>, 2023d.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2024]</span>
<span class="ltx_bibblock">
Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, and Zhuowen Tu.

</span>
<span class="ltx_bibblock">Bliva: A simple multimodal llm for better handling of text-rich visual questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 2256–2264, 2024.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023b]</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang.

</span>
<span class="ltx_bibblock">Salmonn: Towards generic hearing abilities for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.13289</em>, 2023b.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panagopoulou et al. [2023]</span>
<span class="ltx_bibblock">
Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongxu Li, Shafiq Joty, Ran Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles.

</span>
<span class="ltx_bibblock">X-instructblip: A framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18799</em>, 2023.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023c]</span>
<span class="ltx_bibblock">
Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang.

</span>
<span class="ltx_bibblock">Bubogpt: Enabling visual grounding in multi-modal llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.08581</em>, 2023c.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023a]</span>
<span class="ltx_bibblock">
Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.

</span>
<span class="ltx_bibblock">Vila: On pre-training for visual language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.07533</em>, 2023a.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2023a]</span>
<span class="ltx_bibblock">
Zhengqing Yuan, Zhaoxu Li, and Lichao Sun.

</span>
<span class="ltx_bibblock">Tinygpt-v: Efficient multimodal large language model via small backbones.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.16862</em>, 2023a.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023b]</span>
<span class="ltx_bibblock">
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al.

</span>
<span class="ltx_bibblock">Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07575</em>, 2023b.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2024]</span>
<span class="ltx_bibblock">
Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al.

</span>
<span class="ltx_bibblock">Sphinx-x: Scaling data and parameters for a family of multi-modal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05935</em>, 2024.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. [2024b]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh.

</span>
<span class="ltx_bibblock">What matters when building vision-language models?

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.02246</em>, 2024b.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. [2023]</span>
<span class="ltx_bibblock">
Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al.

</span>
<span class="ltx_bibblock">Kosmos-2.5: A multimodal literate model.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.11419</em>, 2023.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023c]</span>
<span class="ltx_bibblock">
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.

</span>
<span class="ltx_bibblock">Monkey: Image resolution and text label are important things for large multi-modal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.06607</em>, 2023c.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Xie [2023]</span>
<span class="ltx_bibblock">
Penghao Wu and Saining Xie.

</span>
<span class="ltx_bibblock">V*: Guided visual search as a core mechanism in multimodal llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14135</em>, 2023.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2024]</span>
<span class="ltx_bibblock">
Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei.

</span>
<span class="ltx_bibblock">Kosmos-g: Generating images in context with multimodal large language models, 2024.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McKinzie et al. [2024]</span>
<span class="ltx_bibblock">
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al.

</span>
<span class="ltx_bibblock">Mm1: Methods, analysis &amp; insights from multimodal llm pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.09611</em>, 2024.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2024]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality, 2024.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023b]</span>
<span class="ltx_bibblock">
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">mplug-docowl: Modularized multimodal large language model for document understanding, 2023b.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al. [2024]</span>
<span class="ltx_bibblock">
Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo.

</span>
<span class="ltx_bibblock">Embodiedgpt: Vision-language pre-training via embodied chain of thought.

</span>
<span class="ltx_bibblock"><em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al. [2023]</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.

</span>
<span class="ltx_bibblock">Video-chatgpt: Towards detailed video understanding via large vision and language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05424</em>, 2023.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-vl: A frontier large vision-language model with versatile abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.12966</em>, 2023.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. [2023]</span>
<span class="ltx_bibblock">
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al.

</span>
<span class="ltx_bibblock">Anymal: An efficient and scalable any-modality augmented language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16058</em>, 2023.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. [2023]</span>
<span class="ltx_bibblock">
Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang.

</span>
<span class="ltx_bibblock">Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11810</em>, 2023.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2023]</span>
<span class="ltx_bibblock">
Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">mplug-paperowl: Scientific diagram analysis with the multimodal large language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18248</em>, 2023.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2023b]</span>
<span class="ltx_bibblock">
Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.

</span>
<span class="ltx_bibblock">Osprey: Pixel understanding with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10032</em>, 2023b.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mondal et al. [2024]</span>
<span class="ltx_bibblock">
Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao.

</span>
<span class="ltx_bibblock">Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.12863</em>, 2024.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2024c]</span>
<span class="ltx_bibblock">
Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng Wang, and Dawei Yin.

</span>
<span class="ltx_bibblock">Vislinginstruct: Elevating zero-shot learning in multi-modal language models with autonomous instruction optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07398</em>, 2024c.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. [2023]</span>
<span class="ltx_bibblock">
Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al.

</span>
<span class="ltx_bibblock">Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.16886</em>, 2023.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. [2024]</span>
<span class="ltx_bibblock">
Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al.

</span>
<span class="ltx_bibblock">Mobilevlm v2: Faster and stronger baseline for vision language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.03766</em>, 2024.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. [2017]</span>
<span class="ltx_bibblock">
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al. [2019]</span>
<span class="ltx_bibblock">
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Fine-tuning language models from human preferences.

</span>
<span class="ltx_bibblock"><em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08593</em>, 2019.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al. [2020]</span>
<span class="ltx_bibblock">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano.

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:3008–3021, 2020.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2022]</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.05862</em>, 2022.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. [2024]</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024c]</span>
<span class="ltx_bibblock">
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.

</span>
<span class="ltx_bibblock">Mm-llms: Recent advances in multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.13601</em>, 2024c.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2024]</span>
<span class="ltx_bibblock">
Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, and Yadong Mu.

</span>
<span class="ltx_bibblock">Unified language-vision pretraining in llm with dynamic discrete visual tokenization, 2024.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2023]</span>
<span class="ltx_bibblock">
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al.

</span>
<span class="ltx_bibblock">Scaling autoregressive multi-modal models: Pretraining and instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.02591</em>, 2(3), 2023.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023b]</span>
<span class="ltx_bibblock">
Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan.

</span>
<span class="ltx_bibblock">Vl-gpt: A generative pre-trained transformer for vision and language understanding and generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.09251</em>, 2023b.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2024]</span>
<span class="ltx_bibblock">
Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu.

</span>
<span class="ltx_bibblock">Unicode: Learning a unified codebook for multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.09072</em>, 2024.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. [2023]</span>
<span class="ltx_bibblock">
Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan.

</span>
<span class="ltx_bibblock">Making llama see and draw with seed tokenizer.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01218</em>, 2023.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020b]</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, and Juan Pino.

</span>
<span class="ltx_bibblock">Covost 2 and massively multilingual speech-to-text translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.10310</em>, 2020b.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krause et al. [2017]</span>
<span class="ltx_bibblock">
Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">A hierarchical approach for generating descriptive image paragraphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 317–325, 2017.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pont-Tuset et al. [2020]</span>
<span class="ltx_bibblock">
Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">Connecting vision and language with localized narratives.

</span>
<span class="ltx_bibblock">In <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16</em>, pages 647–664. Springer, 2020.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al. [2023]</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 18392–18402, 2023.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024d]</span>
<span class="ltx_bibblock">
Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.

</span>
<span class="ltx_bibblock">Magicbrush: A manually annotated dataset for instruction-guided image editing.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024d.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bachmann et al. [2022]</span>
<span class="ltx_bibblock">
Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir.

</span>
<span class="ltx_bibblock">Multimae: Multi-modal multi-task masked autoencoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 348–367. Springer, 2022.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2021]</span>
<span class="ltx_bibblock">
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Combining recurrent, convolutional, and continuous-time models with linear state space layers.

</span>
<span class="ltx_bibblock"><em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 34:572–585, 2021.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. [2023]</span>
<span class="ltx_bibblock">
Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman.

</span>
<span class="ltx_bibblock">Simplified state space layers for sequence modeling, 2023.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu and Dao [2023]</span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.00752</em>, 2023.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al. [2024]</span>
<span class="ltx_bibblock">
Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu.

</span>
<span class="ltx_bibblock">Vl-mamba: Exploring state space models for multimodal learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.13600</em>, 2024.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2024]</span>
<span class="ltx_bibblock">
Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang.

</span>
<span class="ltx_bibblock">Cobra: Extending mamba to multi-modal large language model for efficient inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.14520</em>, 2024.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2024]</span>
<span class="ltx_bibblock">
Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, and Xiu Li.

</span>
<span class="ltx_bibblock">Mambatalk: Efficient holistic gesture synthesis with selective state space models.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.09471</em>, 2024.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024c]</span>
<span class="ltx_bibblock">
Wenrui Li, Xiaopeng Hong, and Xiaopeng Fan.

</span>
<span class="ltx_bibblock">Spikemba: Multi-modal spiking saliency mamba for temporal video grounding.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.01174</em>, 2024c.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.17926" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.17927" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.17927">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.17927" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.17928" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 19:18:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
