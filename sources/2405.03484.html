<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.03484] Whispy: Adapting STT Whisper Models to Real-Time Environments</title><meta property="og:description" content="Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Whispy: Adapting STT Whisper Models to Real-Time Environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Whispy: Adapting STT Whisper Models to Real-Time Environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.03484">

<!--Generated on Wed Jun  5 16:23:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="speech-to-text whisper transcription real-time">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Meetecho LTD, Napoli, Italy
<br class="ltx_break"><a target="_blank" href="https://www.meetecho.com/en" title="" class="ltx_ref">https://www.meetecho.com/en</a> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Napoli Federico II
<br class="ltx_break"><a target="_blank" href="https://www.unina.it" title="" class="ltx_ref">https://www.unina.it</a></span></span></span>
<h1 class="ltx_title ltx_title_document">Whispy: Adapting STT Whisper Models to Real-Time Environments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonio Bevilacqua
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paolo Saviano
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessandro Amirante
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon Pietro Romano
</span><span class="ltx_author_notes">1122</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection. However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications. In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models. As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost. We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output. Experimental results show how Whispy excels in robustness, promptness, and accuracy.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>speech-to-text whisper transcription real-time
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, Automatic Speech Recognition (ASR) systems have gained momentum across diverse sectors, propelled by advancements in machine learning, deep neural networks, and natural language processing techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These systems, equipped with sophisticated algorithms, have become indispensable tools for a wide range of applications, spanning from virtual assistants and smart home devices to customer service automation and healthcare diagnostics. The surge in ASR adoption is fueled by the proliferation of commercial solutions offered by leading tech companies and startups alikeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These solutions, often accessible as paid services through web-based Application Programming Interfaces (APIs), provide developers with convenient and scalable access to powerful speech recognition capabilities without the need for extensive expertise in machine learning or signal processing. As a result, businesses and developers can seamlessly integrate ASR functionality into their applications, enabling end-to-end speech-to-text transcription, voice commands, sentiment analysis, and moreÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Lately, the family of large-scale models called Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, released by the company OpenAI, secured state-of-the-art performance in the realm of ASR. Whisper models are multilingual, multitask systems capable of achieving human accuracy in transcribing and translating recorded audio, identifying spoken languages, and detecting voice activity (Voice Activity Detection, or VAD). One of the key strengths of Whisper models lies in their ability to achieve human-level accuracy in transcribing and translating recorded audio. This means that they can accurately convert spoken words into text and even translate them into different languages with a level of precision that rivals human capabilities. Additionally, Whisper models excel in identifying the languages being spoken and detecting voice activity within audio recordings. These capabilities have significant implications for various applications, including speech-to-text transcription services, multilingual communication platforms, and voice-controlled systems. By leveraging the power of large-scale deep learning models, Whisper has pushed the boundaries of what is possible in ASR, offering unprecedented accuracy and versatility in processing and understanding spoken language. Yet, Whisper models are limited to offline use only. Whisper requires the target speech data to be fully available at inference time, and generates transcriptions or translations at a speed that depends on many factors and is not easy to determine for long audio sources. This highly hinders their potential to be adopted in real-time applications, particularly in contexts where low-latency speech analysis and ASR are essential, such as in web conferencing.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present Whispy, a novel adaptation structure around Whisper models that allows for live transcription of audio streams. Whispy is designed as a self-contained ASR service, capable of consuming real-time speech data and producing precise and reliable transcriptions at a reduced computational cost and with an acceptable temporal delay. This is achieved by processing short audio chunks that accumulate within a shifting buffer. A straightforward and effective agreement algorithm, based on the Levenshtein distanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> between strings, extracts the most accurate transcript suggestions when overlapping portions of the buffer are transcribed. Whispy is highly configurable and flexible, and performed consistently well across all of our experimental campaigns.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In real-time communication and collaboration contexts, ASR techniques play a pivotal role by enabling real-time speech transcription, thereby enhancing these environments with functionalities such as speech summarisation and diarisation. These capabilities offer users the ability to work with spoken content in real-time, facilitating tasks like extracting key insights from conversations and identifying speakers. The Whispy system, specifically designed with this purpose in mind, is intended to seamlessly integrate with various real-time communication systems. While Whispyâ€™s functionality remains agnostic to the underlying real-time communication platform, the current implementation discussed in this paper assumes integration with a WebRTC-enabled architecture, leveraging the open-source Janus media server developed by Meetecho at its coreÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of this paper is organised as follows. SectionÂ <a href="#S2" title="2 Related works â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> briefly explores the space of currently available solutions for live ASR. In SectionÂ <a href="#S3" title="3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we present the Whispy architecture and working principles. Experimental setups and results are discussed in SectionÂ <a href="#S4" title="4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, a compendium of our contribution, with some closing remarks and suggestions for future extensions, is provided in SectionÂ <a href="#S5" title="5 Conclusions and future works â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A substantial body of literature has explored capabilities and applications of ASR systems, mainly in offline setups. Spanning from the medical and healthcare context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, to education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, surveillance and security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and smart homes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, speech-to-text models have been adopted for a variety of tasks. However, research in this sense has mainly been focusing on improving transcription performance and context-awareness of ASR systems, rather than extending their capabilities to real-time scenarios. In this regard, to the best of our knowledge, most of the currently available solutions come from private companies and startups that offer such services to a price and therefore do not disclose useful insight on their processes.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the open source domain, interesting projects exist that investigate potential adaptations of Whisper as a real-time Speech-To-Text (STT) system. Within solutions like <span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter">VoiceStreamAI</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or <span id="S2.p2.1.2" class="ltx_text ltx_font_typewriter">Whisper-live</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the streaming audio data is split into smaller chunks, and then each chunk is processed using a Whisper model instance. This method has the drawback of not providing Whisper with enough context information, so the transcriptions for short chunks may be of extremely poor quality, in particular when sentences or words are cut at the beginning or the end of a chunk. Similarly, <span id="S2.p2.1.3" class="ltx_text ltx_font_typewriter">Whisper-streaming</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> offers a client-server solution based on chunking input audio streams and then running a local agreement algorithm to align current transcriptions with the previously transcribed text. Whilst quite similar to our approach in terms of audio management, <span id="S2.p2.1.4" class="ltx_text ltx_font_typewriter">Whisper-streaming</span> relies on word-level timestamps to extract text updates from transcription sequences of already processed text.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this paper, we address the main issues that arise from the discussed solutions, and we test our system against commonly used ASR benchmark datasets in order to evaluate both the quality of the produced transcriptions, and the introduced temporal delay.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data and methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The purpose of this paper is to evaluate how Whispy, our wrapper designed around the Whisper pretrained models, can be successfully used to transcribe audio data fetched from live sources.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We designed Whispy to be a production-ready system that can be used as a self-contained transcription service. Our objective was twofold: to provide a robust real-time speech-to-text container, and to engineer a flexible multimedia-oriented blueprint for a software structure capable of processing live audio and video streams and feeding them into extensible AI components.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">As shown in FigureÂ <a href="#S3.F1" title="Figure 1 â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the main actors in the Whispy transcriber are:</p>
</div>
<div id="S3.p4" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">an input pipeline, responsible for managing inbound real-time audio streams and adapting them to formats suitable for any following components;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">a shifting data register, in charge of storing the received audio data and making them available to the transcriber or any other entity that may need them;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">a transcriber, holding and invoking the actual Whisper model.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Furthermore, Whispy exposes a number of web APIs that can be used as signalling entry points to negotiate the connection parameters of input streams and output destinations, as well as start and stop the transcription process.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.03484/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>General Whispy service architecture. The overall system lives within an HTTP server, that we use as interface to set up the incoming stream, define the transcription destination, and update options such as model size or Voice Activity Detection (VAD) parameters.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Input pipeline</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Whispy receives and preprocesses incoming data streams through a two-layer pipeline. First, an <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">ffmpeg</span> asynchronous process (created with the use of the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">ffmpeg-python</span> libraryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>) is configured with all the relevant multimedia information received upon a remote <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">/warmup</span> POST call, such as input codec, number of channels, and audio sampling rate. The <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">ffmpeg</span> process then produces a local Real-time Transfer Protocol (RTP) stream, that is consumed by an RTP client where buffering and further data manipulations may take place.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This input structure presents many advantages. It allows us to fully leverage <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">ffmpeg</span> capabilities for the heavy-lifting of data manipulation, such as encoding and decoding, and it also makes the whole application easily extensible to multiple input/output scenarios. Moreover, it completely decouples the data source from the Whispy service, making it agnostic to the application context.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data register</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Once the RTP client buffers a set amount of data, it sends it to a data structure that acts as a First In, First Out (FIFO) shifting register. This register can hold an audio waveform up to a fixed length, determined by the formula <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="C_{n}*C_{d}*f_{s}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">C</mi><mi id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">n</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">âˆ—</mo><msub id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">C</mi><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">d</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1a" xref="S3.SS2.p1.1.m1.1.1.1.cmml">âˆ—</mo><msub id="S3.SS2.p1.1.m1.1.1.4" xref="S3.SS2.p1.1.m1.1.1.4.cmml"><mi id="S3.SS2.p1.1.m1.1.1.4.2" xref="S3.SS2.p1.1.m1.1.1.4.2.cmml">f</mi><mi id="S3.SS2.p1.1.m1.1.1.4.3" xref="S3.SS2.p1.1.m1.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">ğ¶</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">ğ‘›</ci></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ğ‘‘</ci></apply><apply id="S3.SS2.p1.1.m1.1.1.4.cmml" xref="S3.SS2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.4.1.cmml" xref="S3.SS2.p1.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.4.2.cmml" xref="S3.SS2.p1.1.m1.1.1.4.2">ğ‘“</ci><ci id="S3.SS2.p1.1.m1.1.1.4.3.cmml" xref="S3.SS2.p1.1.m1.1.1.4.3">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">C_{n}*C_{d}*f_{s}</annotation></semantics></math>, where:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><math id="S3.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="C_{n}" display="inline"><semantics id="S3.I2.i1.p1.1.m1.1a"><msub id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.1.1.2" xref="S3.I2.i1.p1.1.m1.1.1.2.cmml">C</mi><mi id="S3.I2.i1.p1.1.m1.1.1.3" xref="S3.I2.i1.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><apply id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.1.m1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.1.1.2">ğ¶</ci><ci id="S3.I2.i1.p1.1.m1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">C_{n}</annotation></semantics></math> is the number of audio chunks that can be stored in the register;</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><math id="S3.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="C_{d}" display="inline"><semantics id="S3.I2.i2.p1.1.m1.1a"><msub id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mi id="S3.I2.i2.p1.1.m1.1.1.2" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">C</mi><mi id="S3.I2.i2.p1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.p1.1.m1.1.1.2">ğ¶</ci><ci id="S3.I2.i2.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">C_{d}</annotation></semantics></math> is the temporal duration in seconds of each audio chunk;</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><math id="S3.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S3.I2.i3.p1.1.m1.1a"><msub id="S3.I2.i3.p1.1.m1.1.1" xref="S3.I2.i3.p1.1.m1.1.1.cmml"><mi id="S3.I2.i3.p1.1.m1.1.1.2" xref="S3.I2.i3.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.I2.i3.p1.1.m1.1.1.3" xref="S3.I2.i3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.m1.1b"><apply id="S3.I2.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.1.m1.1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i3.p1.1.m1.1.1.2.cmml" xref="S3.I2.i3.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S3.I2.i3.p1.1.m1.1.1.3.cmml" xref="S3.I2.i3.p1.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.m1.1c">f_{s}</annotation></semantics></math> is the incoming audio sampling rate (as produced by the RTP client).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.5" class="ltx_p">As an example, a register that contains <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn type="integer" id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">20</annotation></semantics></math> total seconds of audio sampled at <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn type="integer" id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">16</annotation></semantics></math> kHz, split over <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><cn type="integer" id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">5</annotation></semantics></math> chunks of <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mn id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">4</annotation></semantics></math> seconds each, will have a length of <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="5*4*16000=320k" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mrow id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml"><mn id="S3.SS2.p3.5.m5.1.1.2.2" xref="S3.SS2.p3.5.m5.1.1.2.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.5.m5.1.1.2.1" xref="S3.SS2.p3.5.m5.1.1.2.1.cmml">âˆ—</mo><mn id="S3.SS2.p3.5.m5.1.1.2.3" xref="S3.SS2.p3.5.m5.1.1.2.3.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.5.m5.1.1.2.1a" xref="S3.SS2.p3.5.m5.1.1.2.1.cmml">âˆ—</mo><mn id="S3.SS2.p3.5.m5.1.1.2.4" xref="S3.SS2.p3.5.m5.1.1.2.4.cmml">16000</mn></mrow><mo id="S3.SS2.p3.5.m5.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml"><mn id="S3.SS2.p3.5.m5.1.1.3.2" xref="S3.SS2.p3.5.m5.1.1.3.2.cmml">320</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.3.1" xref="S3.SS2.p3.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p3.5.m5.1.1.3.3" xref="S3.SS2.p3.5.m5.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><eq id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1"></eq><apply id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><times id="S3.SS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1.2.1"></times><cn type="integer" id="S3.SS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2.2">5</cn><cn type="integer" id="S3.SS2.p3.5.m5.1.1.2.3.cmml" xref="S3.SS2.p3.5.m5.1.1.2.3">4</cn><cn type="integer" id="S3.SS2.p3.5.m5.1.1.2.4.cmml" xref="S3.SS2.p3.5.m5.1.1.2.4">16000</cn></apply><apply id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3"><times id="S3.SS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.1"></times><cn type="integer" id="S3.SS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2">320</cn><ci id="S3.SS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">5*4*16000=320k</annotation></semantics></math> data points. In addition, the data register keeps track of the absolute number of data blocks that were appended to it, as well as the temporal starting and ending coordinates of its content at any given time.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Transcriber</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The transcriber is the component that holds and invokes the actual Whisper model. In our implementation, we use <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">faster-whisper</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which provides inference models quantized and optimized with <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">CTranslate2</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in order to improve the modelsâ€™ inference performance and considerably reduce their memory requirements.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">When a transcription is in progress, the transcriber periodically polls the data register for new audio chunks, using the absolute sequence number as reference. Once a new audio chunk is available, the transcriber fetches a copy of the entire register content and:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">checks the waveform corresponding to the latest audio chunk only for voice activity (pre-VAD). If no voice is detected, the transcriber does not perform any transcription, marks the latest chunk as silent, and flushes the content of the data register;</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">if the waveform corresponding to the latest audio chunk contains voice activity, the entire waveform is transcribed;</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">the newly produced transcription is first filtered to detect potential <em id="S3.I3.i3.p1.1.1" class="ltx_emph ltx_font_italic">hallucinations</em>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, then processed against the previous transcription to generate a suggestion that fixes the overlap between subsequent transcription strings.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">A more detailed explanation of the transcription process is provided in AlgorithmÂ <a href="#algorithm1" title="In 3.3 Transcriber â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It is important to highlight that VAD operations heavily rely on the pretrained Silero VAD modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which are integrated into the <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">faster-whisper</span> library. The inclusion of a pre-VAD step serves as a strategic optimization, aimed at preventing unnecessary re-transcriptions triggered by periods of silence or non-voice audio segments added to the buffer. Despite the introduction of this pre-VAD step, VAD remains a crucial component of the <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_typewriter">faster-whisper</span> transcription pipeline. This is because conducting VAD during transcription significantly enhances the quality of the text output and effectively reduces the likelihood of encountering hallucinations or other undesirable artifacts in the transcribed content. By incorporating the pretrained Silero VAD models within the <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_typewriter">faster-whisper</span> framework, the transcription process benefits from enhanced accuracy and efficiency. This integrated approach ensures a smoother and more reliable transcription experience for users, ultimately improving the overall performance and usability of the system.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Another crucial aspect of our transcription policy is the re-transcription of portions of audio that were already previously processed, as shown on line <a href="#algorithm1" title="In 3.3 Transcriber â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in Algorithm <a href="#algorithm1" title="In 3.3 Transcriber â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This behaviour results in transcript outputs that are context aware, thus eliminating any issues caused by chunks cutting in mid-sentence or mid-word. Moreover, unexpected delays in the pipeline that may result in more than one chunk to be appended to data register without the transcriber fetching them via polling are ultimately resolved without data loss.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.6" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.6.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>


</div>
<div id="algorithm1.6.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>








</div>
<div id="algorithm1.6.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>







</div>
<div id="algorithm1.6.10" class="ltx_listingline">
<span id="algorithm1.6.10.1" class="ltx_text ltx_font_bold">Input :</span>Â Shifting register <span id="algorithm1.6.10.2" class="ltx_text ltx_font_sansserif">register</span> with <span id="algorithm1.6.10.3" class="ltx_text ltx_font_sansserif">data</span> content
</div>
<div id="algorithm1.6.11" class="ltx_listingline">
<span id="algorithm1.6.11.1" class="ltx_text ltx_font_bold">Input :</span>Â Transcription model <span id="algorithm1.6.11.2" class="ltx_text ltx_font_sansserif">model</span> 
</div>
<div id="algorithm1.6.12" class="ltx_listingline">
<span id="algorithm1.6.12.1" class="ltx_text ltx_font_bold">Input :</span>Â Previous transcription result <span id="algorithm1.6.12.2" class="ltx_text ltx_font_sansserif">last_trx</span> 
</div>
<div id="algorithm1.6.13" class="ltx_listingline">
<span id="algorithm1.6.13.1" class="ltx_text ltx_font_bold">Output :</span>Â Transcription object <span id="algorithm1.6.13.2" class="ltx_text ltx_font_sansserif">trx</span> 
</div>
<div id="algorithm1.6.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>

</div>
<div id="algorithm1.1.1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span><span id="algorithm1.1.1.1" class="ltx_text ltx_font_sansserif">last_chunk</span> <math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mo stretchy="false" id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><ci id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.1.1.2" class="ltx_text ltx_font_sansserif">register</span>.<span id="algorithm1.1.1.3" class="ltx_text ltx_font_sansserif">data</span>.<span id="algorithm1.1.1.4" class="ltx_text ltx_font_typewriter">get_last_chunk(<em id="algorithm1.1.1.4.1" class="ltx_emph ltx_font_serif ltx_font_italic"></em>)</span>
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>
<span id="algorithm1.2.2.1" class="ltx_text ltx_font_sansserif">has_voice_activity</span> <math id="algorithm1.2.2.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.2.2.m1.1a"><mo stretchy="false" id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><ci id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.2.2.2" class="ltx_text ltx_font_typewriter">run_vad(<em id="algorithm1.2.2.2.1" class="ltx_emph ltx_font_sansserif">last_chunk</em>)</span>
</div>
<div id="algorithm1.6.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>
<span id="algorithm1.6.15.1" class="ltx_text ltx_font_bold">if</span>Â <em id="algorithm1.6.15.2" class="ltx_emph ltx_font_italic"><span id="algorithm1.6.15.2.1" class="ltx_text ltx_font_sansserif ltx_font_upright">has_voice_activity</span> is False</em>Â <span id="algorithm1.6.15.3" class="ltx_text ltx_font_bold">then</span>
</div>
<div id="algorithm1.6.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm1.6.16.1" class="ltx_text ltx_font_sansserif">register</span>.<span id="algorithm1.6.16.2" class="ltx_text ltx_font_typewriter">flush(<em id="algorithm1.6.16.2.1" class="ltx_emph ltx_font_serif ltx_font_italic"></em>)</span>
</div>
<div id="algorithm1.6.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm1.6.17.1" class="ltx_text ltx_font_bold">return</span> <em id="algorithm1.6.17.2" class="ltx_emph ltx_font_italic">â€™silenceâ€™</em>

</div>
<div id="algorithm1.6.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span><span id="algorithm1.3.3.1" class="ltx_text ltx_font_sansserif">trx</span> <math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.3.3.m1.1a"><mo stretchy="false" id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><ci id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.3.3.2" class="ltx_text ltx_font_sansserif">model</span>.<span id="algorithm1.3.3.3" class="ltx_text ltx_font_typewriter">transcribe(<em id="algorithm1.3.3.3.1" class="ltx_emph ltx_font_sansserif">data</em>)</span>
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>
<span id="algorithm1.4.4.1" class="ltx_text ltx_font_sansserif">hallucination</span> <math id="algorithm1.4.4.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.4.4.m1.1a"><mo stretchy="false" id="algorithm1.4.4.m1.1.1" xref="algorithm1.4.4.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m1.1b"><ci id="algorithm1.4.4.m1.1.1.cmml" xref="algorithm1.4.4.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.4.4.2" class="ltx_text ltx_font_typewriter">is_hallucination(<em id="algorithm1.4.4.2.1" class="ltx_emph ltx_font_sansserif">trx</em>)</span>
</div>
<div id="algorithm1.6.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>

</div>
<div id="algorithm1.6.20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span><span id="algorithm1.6.20.1" class="ltx_text ltx_font_bold">if</span>Â <em id="algorithm1.6.20.2" class="ltx_emph ltx_font_italic"><span id="algorithm1.6.20.2.1" class="ltx_text ltx_font_sansserif ltx_font_upright">hallucination</span> is True</em>Â <span id="algorithm1.6.20.3" class="ltx_text ltx_font_bold">then</span>
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span id="algorithm1.5.5.1" class="ltx_text ltx_font_sansserif">trx</span> <math id="algorithm1.5.5.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.5.5.m1.1a"><mo stretchy="false" id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><ci id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.5.5.2" class="ltx_text ltx_font_typewriter">filter_hallucination(<em id="algorithm1.5.5.2.1" class="ltx_emph ltx_font_sansserif">trx</em>)</span>

</div>
<div id="algorithm1.6.21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span>
</div>
<div id="algorithm1.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">17</span><span id="algorithm1.6.6.1" class="ltx_text ltx_font_sansserif">trx</span> <math id="algorithm1.6.6.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.6.6.m1.1a"><mo stretchy="false" id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><ci id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">\leftarrow</annotation></semantics></math> <span id="algorithm1.6.6.2" class="ltx_text ltx_font_typewriter">generate_suggestion(<em id="algorithm1.6.6.2.1" class="ltx_emph ltx_font_serif ltx_font_italic"><span id="algorithm1.6.6.2.1.1" class="ltx_text ltx_font_sansserif ltx_font_upright">trx</span>, <span id="algorithm1.6.6.2.1.2" class="ltx_text ltx_font_sansserif ltx_font_upright">last_trx</span></em>)</span>
</div>
<div id="algorithm1.6.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">18</span>
<span id="algorithm1.6.22.1" class="ltx_text ltx_font_bold">return</span> <em id="algorithm1.6.22.2" class="ltx_emph ltx_font_sansserif">trx</em>

</div>
<div id="algorithm1.6.23" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.8.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 1</span> </span>Transcription algorithm</figcaption>
</figure>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">Two other important components of the transcription process are the hallucination filter and the suggestion generator. Whilst the first is required because of the intrinsic tendency of large text-generation models to produce, from time to time, unreliable or unpredictable outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we need the latter as Whispy transcribes portions of overlapping audio between pairs of consecutive audio chunks.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Generating suggestions</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.2" class="ltx_p">When any given audio chunk <math id="S3.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mi id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">n</annotation></semantics></math> is added to the data register and processed for transcription, the produced text will include a transcription of a substantial portion of audio that was already transcribed up to chunk <math id="S3.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="n-1" display="inline"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mrow id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">n</mi><mo id="S3.SS3.SSS1.p1.2.m2.1.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><minus id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1"></minus><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">ğ‘›</ci><cn type="integer" id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">n-1</annotation></semantics></math>. In order to determine which part of the newly generated text should be used as transcription for the current chunk, we designed a search technique based on the Levenshtein distanceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The Levenshtein distance, often referred to as edit distance, is a simple metric that quantifies the distance between a pair of sequences in terms of how many insertions, deletions and substitutions are required to change one sequence into the other. As shown in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3.3.1 Generating suggestions â€£ 3.3 Transcriber â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, Whispy generates a sequence of tokens from the transcription of an audio chunk, and then, going backwards from the last token, computes the edit distance between all the subsequences and the transcription text of the previous chunk. The subsequence with the minimum edit distance to the previous transcription text is used to extract the suggested transcription for the current chunk.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.03484/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Practical example of the Whispy suggestion mechanism.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Filtering hallucinations</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Hallucinations occasionally produced by the Whisper models have the potential to extensively disrupt the Whispy transcription outcome. Other than content-oriented hallucinationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, Whisper can produce hallucinatory artifacts resulting in the repetition of a single token or sequence of tokensÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. In our implementation, we designed a naive hallucination filter capable of reliably detecting repeating tokens and skim them before suggestions are produced. As we will discuss in Chapter <a href="#S4" title="4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, however, during our test campaigns we did not experience a sufficiently large number of hallucinatory events to determine the goodness of the hallucination filter.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In our experimental campaigns, we tested Whispy with emphasis on transcription quality and live transcription delay. We also compared the results obtained with Whispy against the outcomes of the Whisper offline transcriptions, in order to determine whether the buffering mechanism we designed results in less reliable transcripts, and whether any differences are statistically significant or hold any practical relevance. In doing so, we selected a number of heterogeneous datasets commonly used to test ASR systems, namely:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">ESIC</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a corpus of recorded speeches from European Parliament sessions, for which human transcriptions are available. ESIC includes <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter">dev</span>, <span id="S4.I1.i1.p1.1.3" class="ltx_text ltx_font_typewriter">dev2</span> and <span id="S4.I1.i1.p1.1.4" class="ltx_text ltx_font_typewriter">test</span> partitions. We incporporated the latter in our experiments, but we had to exclude a number of items from the partition as they either were incorrectly labeled as containing English speech or carried non-English transcripts.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">librispeech</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a dataset of readings from audiobooks for which the text is available. Librispeech is split into a <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">clean</span> partition and a <span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_typewriter">other</span> partition, depending on the audio quality. Out of the box, these datasets come in the form of short audio clips where each clip contains a sentence extracted from a particular book. We merged together clips from the same readings in order to obtain longer audio data.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">TEDlium</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, a corpus of recorded TED talks with their transcripts. We tested the legacy <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_typewriter">test</span> partition, commonly used as benchmark in literature, composed of <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mn id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><cn type="integer" id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">11</annotation></semantics></math> talks.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Rev16</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, a set of 16 recordings from the Rev.ai podcast.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.4" class="ltx_p">We preprocessed all the audio data at our disposal by appending a short period of silence at the end of every clip. This silence ensures that Whispy correctly processes the content of the buffer at the end of the stream whenever the last chunk in it is shorter than the set chunk length. According to the definitions outlined in SectionÂ <a href="#S3.SS2" title="3.2 Data register â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we set a chunk length of <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">4</annotation></semantics></math> seconds and a buffer length of <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="integer" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">5</annotation></semantics></math> chunks, for a total of <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">20</annotation></semantics></math> seconds of audio. Hence we added <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.p3.4.m4.1a"><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><cn type="integer" id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">4</annotation></semantics></math> seconds of silence at the end of every tested audio clip. When testing different hyperparameter configurations, and specifically different chunk lengths, we modified the trailing silence accordingly. We designed a flexible testing pipeline that dynamically instantiates all the required services for both Whispy and Whisper systems, then streams all the audio clips within a dataset in order to obtain the Whispy transcripts, invokes the Whisper offline model, and eventually computes all the required metrics based on the ground truth transcription. Both Whispy and Whisper were tested with the model instances <span id="S4.p3.4.1" class="ltx_text ltx_font_typewriter">base</span>, <span id="S4.p3.4.2" class="ltx_text ltx_font_typewriter">small</span>, <span id="S4.p3.4.3" class="ltx_text ltx_font_typewriter">medium</span>, and <span id="S4.p3.4.4" class="ltx_text ltx_font_typewriter">large-v3</span>, and always configured with the same set of parameters. Our testing pipeline runs on a cloud infrastructure powered by four Tesla T4 cards that are used to run multiple tests in parallel, so each Whispy instance is allocated a single T4 card at any given time.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Transcription quality</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate the transcription quality by computing the Word Error Rate (WER) between any transcription generated by either Whisper or Whispy, and the corresponding ground truth text. WER is a metric based on the edit distance between sequences, that is commonly regarded as a standard when assessing the performance of ASR systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. In order to avoid unfair penalisation of hypothesis texts, we first normalise all generated text data by removing punctuation and multiple whitespaces, converting all tokens to lower case, and expanding English contractions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Transcription quality â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the overall WER distributions for all the tested systems across all the tested datasets. The results show how Whispy falls short of Whisper by only 1% to 2% in terms of error rate, with the exception of the rev16 dataset, for which the difference spans from 5% to 7%.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2405.03484/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Across all the tested datasets, excluding rev16, each instance of our Whispy implementation performs within a 1-2% negative difference from its corresponding offline Whisper version. Each box in the graph represents the distribution of the Word Error Rate scored by the labeled model.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To get a better understanding of the performance difference between the tested offline and real-time methods, we additionally present the Critical Difference (CD) diagram of our tests in Figure <a href="#S4.F4" title="Figure 4 â€£ 4.1 Transcription quality â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. CD diagrams are compact tools used to aggregate the results of multiple statistical significance tests. Each model is ranked based on its score across the datasets, and models for which the pairwise score difference is not statistically significant, are merged through a bold horizontal line. CD diagrams rely on Wilcoxon signed-ranks test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. According to our results, Whispy is not statistically different from Whisper, regardless of the rankings of the different models under test. It is worth mentioning that we used Word Accuracy instead of WER when inspecting critical differences, as CD diagrams require a rankable metric where higher is better.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.03484/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Diagram of the critical differences among the tested models. The continuous bold line suggests there are no statistically significant differences in the obtained results, despite offline Whisper models performing, on average, better than real-time Whispy models.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Lastly, we inspect the pairwise WER scores for the datasets under test as reported in Figure <a href="#S4.F5" title="Figure 5 â€£ 4.1 Transcription quality â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where each scatter plot shows how Whispy performs against Whisper on a given dataset. In most cases, audio clips cluster around the quadrant bisectors, on which WER scores are identical for both Whisper and Whispy. Some datasets still present a slight upward spray, but the overall tendency is that Whispy is generally aligned with Whisper in terms of WER-based transcription quality.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.03484/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="452" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Pairwisewise comparison of offline Whisper transcription WER (x axis) against real-time Whispy transcription WER (y axis). Data points above the quadrant bisector represent audio clips for which Whispy scored lower than Whisper (higher WER), while the region below the quadrant bisector contains all data points for which Whispy scored higher than Whisper (lower WER).</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Timing performance</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">When evaluating the latency introduced by Whispy, we take into account the temporal delays associated with each one of the steps illustrated in Algorithm <a href="#algorithm1" title="In 3.3 Transcriber â€£ 3 Data and methodology â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. These include: VAD processing on the newest chunks (pre-VAD), VAD processing on the entire buffer, actual transcription, and suggestion generation. Hence, we did not focus on the analysis of the overall traversing time for audio chunks received by Whispy, as the decoding delay of the incoming <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_typewriter">ffmpeg</span> stream is negligible and the study of reception and transmission delays over RTP is out of the scope of this paper. Instead, we collected a set of timers for every transcription chunk produced by Whispy, and then aggregated them into averages for a single audio clip and weighted averages for a full dataset, so that the length of individual clips are considered. Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Timing performance â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a full list of the recorded delays introduced by Whispy, split by dataset and model size. We did not include pre-VAD and VAD times in this list because, due to the adoption of the same Silero pre-trained VAD model regardless of target dataset and Whisper size, they were consistent at 0.06 <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\pm</annotation></semantics></math> 0.2 seconds and 0.21 <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\pm</annotation></semantics></math> 0.02 seconds respectively across all datasets and model sizes. Similarly, the suggestion generation step required a time always recorded to be around 1 millisecond, so it was excluded from the visualisation. The <span id="S4.SS2.p1.2.2" class="ltx_text ltx_font_italic">total</span> column, however, accounts for all the available times registered.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Whispy processing times</figcaption>
<table id="S4.T1.40" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.40.41.1" class="ltx_tr">
<th id="S4.T1.40.41.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T1.40.41.1.1.1" class="ltx_text ltx_font_bold">dataset</span></th>
<th id="S4.T1.40.41.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.40.41.1.2.1" class="ltx_text ltx_font_bold">model</span></th>
<th id="S4.T1.40.41.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T1.40.41.1.3.1" class="ltx_text ltx_font_bold">trx</span></th>
<th id="S4.T1.40.41.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.40.41.1.4.1" class="ltx_text ltx_font_bold">total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">large-v3</td>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">0.63 <math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.07</th>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">0.88 <math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\pm</annotation></semantics></math> 0.08</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.4.4.3.1" class="ltx_text">ESIC</span></th>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_left ltx_border_r">medium</td>
<th id="S4.T1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.63 <math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mo id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\pm</annotation></semantics></math> 0.07</th>
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_left">0.90 <math id="S4.T1.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.4.4.2.m1.1a"><mo id="S4.T1.4.4.2.m1.1.1" xref="S4.T1.4.4.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T1.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">\pm</annotation></semantics></math> 0.09</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">small</td>
<th id="S4.T1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.28 <math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mo id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\pm</annotation></semantics></math> 0.03</th>
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_left">0.55 <math id="S4.T1.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.6.6.2.m1.1a"><mo id="S4.T1.6.6.2.m1.1.1" xref="S4.T1.6.6.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T1.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.2.m1.1c">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<th id="S4.T1.8.8.3" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.8.8.4" class="ltx_td ltx_align_left ltx_border_r">base</td>
<th id="S4.T1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.16 <math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mo id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">\pm</annotation></semantics></math> 0.02</th>
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_left">0.44 <math id="S4.T1.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.8.8.2.m1.1a"><mo id="S4.T1.8.8.2.m1.1.1" xref="S4.T1.8.8.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T1.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<th id="S4.T1.10.10.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">large-v3</td>
<th id="S4.T1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1.28 <math id="S4.T1.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><mo id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">\pm</annotation></semantics></math> 0.14</th>
<td id="S4.T1.10.10.2" class="ltx_td ltx_align_left ltx_border_t">1.56 <math id="S4.T1.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.10.10.2.m1.1a"><mo id="S4.T1.10.10.2.m1.1.1" xref="S4.T1.10.10.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T1.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.2.m1.1c">\pm</annotation></semantics></math> 0.16</td>
</tr>
<tr id="S4.T1.12.12" class="ltx_tr">
<th id="S4.T1.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.12.12.3.1" class="ltx_text">libri clean</span></th>
<td id="S4.T1.12.12.4" class="ltx_td ltx_align_left ltx_border_r">medium</td>
<th id="S4.T1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.88 <math id="S4.T1.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.11.11.1.m1.1a"><mo id="S4.T1.11.11.1.m1.1.1" xref="S4.T1.11.11.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T1.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">\pm</annotation></semantics></math> 0.08</th>
<td id="S4.T1.12.12.2" class="ltx_td ltx_align_left">1.03 <math id="S4.T1.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.12.12.2.m1.1a"><mo id="S4.T1.12.12.2.m1.1.1" xref="S4.T1.12.12.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T1.12.12.2.m1.1.1.cmml" xref="S4.T1.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.2.m1.1c">\pm</annotation></semantics></math> 0.09</td>
</tr>
<tr id="S4.T1.14.14" class="ltx_tr">
<td id="S4.T1.14.14.3" class="ltx_td ltx_align_left ltx_border_r">small</td>
<th id="S4.T1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.33 <math id="S4.T1.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.13.13.1.m1.1a"><mo id="S4.T1.13.13.1.m1.1.1" xref="S4.T1.13.13.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">\pm</annotation></semantics></math> 0.03</th>
<td id="S4.T1.14.14.2" class="ltx_td ltx_align_left">0.61 <math id="S4.T1.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.14.14.2.m1.1a"><mo id="S4.T1.14.14.2.m1.1.1" xref="S4.T1.14.14.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T1.14.14.2.m1.1.1.cmml" xref="S4.T1.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.2.m1.1c">\pm</annotation></semantics></math> 0.07</td>
</tr>
<tr id="S4.T1.16.16" class="ltx_tr">
<th id="S4.T1.16.16.3" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.16.16.4" class="ltx_td ltx_align_left ltx_border_r">base</td>
<th id="S4.T1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.19 <math id="S4.T1.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.15.15.1.m1.1a"><mo id="S4.T1.15.15.1.m1.1.1" xref="S4.T1.15.15.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T1.15.15.1.m1.1.1.cmml" xref="S4.T1.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.1.m1.1c">\pm</annotation></semantics></math> 0.02</th>
<td id="S4.T1.16.16.2" class="ltx_td ltx_align_left">0.49 <math id="S4.T1.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.16.16.2.m1.1a"><mo id="S4.T1.16.16.2.m1.1.1" xref="S4.T1.16.16.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T1.16.16.2.m1.1.1.cmml" xref="S4.T1.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.2.m1.1c">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr id="S4.T1.18.18" class="ltx_tr">
<th id="S4.T1.18.18.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.18.18.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">large-v3</td>
<th id="S4.T1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1.16 <math id="S4.T1.17.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.17.17.1.m1.1a"><mo id="S4.T1.17.17.1.m1.1.1" xref="S4.T1.17.17.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T1.17.17.1.m1.1.1.cmml" xref="S4.T1.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.1.m1.1c">\pm</annotation></semantics></math> 0.11</th>
<td id="S4.T1.18.18.2" class="ltx_td ltx_align_left ltx_border_t">1.44 <math id="S4.T1.18.18.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.18.18.2.m1.1a"><mo id="S4.T1.18.18.2.m1.1.1" xref="S4.T1.18.18.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.2.m1.1b"><csymbol cd="latexml" id="S4.T1.18.18.2.m1.1.1.cmml" xref="S4.T1.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.2.m1.1c">\pm</annotation></semantics></math> 0.16</td>
</tr>
<tr id="S4.T1.20.20" class="ltx_tr">
<th id="S4.T1.20.20.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.20.20.3.1" class="ltx_text">libri other</span></th>
<td id="S4.T1.20.20.4" class="ltx_td ltx_align_left ltx_border_r">medium</td>
<th id="S4.T1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.74 <math id="S4.T1.19.19.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.19.19.1.m1.1a"><mo id="S4.T1.19.19.1.m1.1.1" xref="S4.T1.19.19.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.1.m1.1b"><csymbol cd="latexml" id="S4.T1.19.19.1.m1.1.1.cmml" xref="S4.T1.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.1.m1.1c">\pm</annotation></semantics></math> 0.08</th>
<td id="S4.T1.20.20.2" class="ltx_td ltx_align_left">1.02 <math id="S4.T1.20.20.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.20.20.2.m1.1a"><mo id="S4.T1.20.20.2.m1.1.1" xref="S4.T1.20.20.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.2.m1.1b"><csymbol cd="latexml" id="S4.T1.20.20.2.m1.1.1.cmml" xref="S4.T1.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.2.m1.1c">\pm</annotation></semantics></math> 0.11</td>
</tr>
<tr id="S4.T1.22.22" class="ltx_tr">
<td id="S4.T1.22.22.3" class="ltx_td ltx_align_left ltx_border_r">small</td>
<th id="S4.T1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.33 <math id="S4.T1.21.21.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.21.21.1.m1.1a"><mo id="S4.T1.21.21.1.m1.1.1" xref="S4.T1.21.21.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.21.1.m1.1b"><csymbol cd="latexml" id="S4.T1.21.21.1.m1.1.1.cmml" xref="S4.T1.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.21.1.m1.1c">\pm</annotation></semantics></math> 0.03</th>
<td id="S4.T1.22.22.2" class="ltx_td ltx_align_left">0.61 <math id="S4.T1.22.22.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.22.22.2.m1.1a"><mo id="S4.T1.22.22.2.m1.1.1" xref="S4.T1.22.22.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.2.m1.1b"><csymbol cd="latexml" id="S4.T1.22.22.2.m1.1.1.cmml" xref="S4.T1.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.2.m1.1c">\pm</annotation></semantics></math> 0.07</td>
</tr>
<tr id="S4.T1.24.24" class="ltx_tr">
<th id="S4.T1.24.24.3" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.24.24.4" class="ltx_td ltx_align_left ltx_border_r">base</td>
<th id="S4.T1.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.18 <math id="S4.T1.23.23.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.23.23.1.m1.1a"><mo id="S4.T1.23.23.1.m1.1.1" xref="S4.T1.23.23.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.23.1.m1.1b"><csymbol cd="latexml" id="S4.T1.23.23.1.m1.1.1.cmml" xref="S4.T1.23.23.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.23.1.m1.1c">\pm</annotation></semantics></math> 0.02</th>
<td id="S4.T1.24.24.2" class="ltx_td ltx_align_left">0.46 <math id="S4.T1.24.24.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.24.24.2.m1.1a"><mo id="S4.T1.24.24.2.m1.1.1" xref="S4.T1.24.24.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.24.2.m1.1b"><csymbol cd="latexml" id="S4.T1.24.24.2.m1.1.1.cmml" xref="S4.T1.24.24.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.24.2.m1.1c">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr id="S4.T1.26.26" class="ltx_tr">
<th id="S4.T1.26.26.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.26.26.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">large-v3</td>
<th id="S4.T1.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1.24 <math id="S4.T1.25.25.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.25.25.1.m1.1a"><mo id="S4.T1.25.25.1.m1.1.1" xref="S4.T1.25.25.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.25.1.m1.1b"><csymbol cd="latexml" id="S4.T1.25.25.1.m1.1.1.cmml" xref="S4.T1.25.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.25.1.m1.1c">\pm</annotation></semantics></math> 0.12</th>
<td id="S4.T1.26.26.2" class="ltx_td ltx_align_left ltx_border_t">1.52 <math id="S4.T1.26.26.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.26.26.2.m1.1a"><mo id="S4.T1.26.26.2.m1.1.1" xref="S4.T1.26.26.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.26.26.2.m1.1b"><csymbol cd="latexml" id="S4.T1.26.26.2.m1.1.1.cmml" xref="S4.T1.26.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.26.26.2.m1.1c">\pm</annotation></semantics></math> 0.14</td>
</tr>
<tr id="S4.T1.28.28" class="ltx_tr">
<th id="S4.T1.28.28.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.28.28.3.1" class="ltx_text">tedlium</span></th>
<td id="S4.T1.28.28.4" class="ltx_td ltx_align_left ltx_border_r">medium</td>
<th id="S4.T1.27.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.73 <math id="S4.T1.27.27.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.27.27.1.m1.1a"><mo id="S4.T1.27.27.1.m1.1.1" xref="S4.T1.27.27.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.27.1.m1.1b"><csymbol cd="latexml" id="S4.T1.27.27.1.m1.1.1.cmml" xref="S4.T1.27.27.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.27.1.m1.1c">\pm</annotation></semantics></math> 0.07</th>
<td id="S4.T1.28.28.2" class="ltx_td ltx_align_left">1.01 <math id="S4.T1.28.28.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.28.28.2.m1.1a"><mo id="S4.T1.28.28.2.m1.1.1" xref="S4.T1.28.28.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.28.28.2.m1.1b"><csymbol cd="latexml" id="S4.T1.28.28.2.m1.1.1.cmml" xref="S4.T1.28.28.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.28.28.2.m1.1c">\pm</annotation></semantics></math> 0.09</td>
</tr>
<tr id="S4.T1.30.30" class="ltx_tr">
<td id="S4.T1.30.30.3" class="ltx_td ltx_align_left ltx_border_r">small</td>
<th id="S4.T1.29.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.32 <math id="S4.T1.29.29.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.29.29.1.m1.1a"><mo id="S4.T1.29.29.1.m1.1.1" xref="S4.T1.29.29.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.29.1.m1.1b"><csymbol cd="latexml" id="S4.T1.29.29.1.m1.1.1.cmml" xref="S4.T1.29.29.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.29.1.m1.1c">\pm</annotation></semantics></math> 0.03</th>
<td id="S4.T1.30.30.2" class="ltx_td ltx_align_left">0.59 <math id="S4.T1.30.30.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.30.30.2.m1.1a"><mo id="S4.T1.30.30.2.m1.1.1" xref="S4.T1.30.30.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.30.30.2.m1.1b"><csymbol cd="latexml" id="S4.T1.30.30.2.m1.1.1.cmml" xref="S4.T1.30.30.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.30.2.m1.1c">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr id="S4.T1.32.32" class="ltx_tr">
<th id="S4.T1.32.32.3" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.32.32.4" class="ltx_td ltx_align_left ltx_border_r">base</td>
<th id="S4.T1.31.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.18 <math id="S4.T1.31.31.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.31.31.1.m1.1a"><mo id="S4.T1.31.31.1.m1.1.1" xref="S4.T1.31.31.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.31.31.1.m1.1b"><csymbol cd="latexml" id="S4.T1.31.31.1.m1.1.1.cmml" xref="S4.T1.31.31.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.31.31.1.m1.1c">\pm</annotation></semantics></math> 0.02</th>
<td id="S4.T1.32.32.2" class="ltx_td ltx_align_left">0.45 <math id="S4.T1.32.32.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.32.32.2.m1.1a"><mo id="S4.T1.32.32.2.m1.1.1" xref="S4.T1.32.32.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.32.32.2.m1.1b"><csymbol cd="latexml" id="S4.T1.32.32.2.m1.1.1.cmml" xref="S4.T1.32.32.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.32.32.2.m1.1c">\pm</annotation></semantics></math> 0.04</td>
</tr>
<tr id="S4.T1.34.34" class="ltx_tr">
<th id="S4.T1.34.34.3" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.34.34.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">large-v3</td>
<th id="S4.T1.33.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1.39 <math id="S4.T1.33.33.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.33.33.1.m1.1a"><mo id="S4.T1.33.33.1.m1.1.1" xref="S4.T1.33.33.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.33.33.1.m1.1b"><csymbol cd="latexml" id="S4.T1.33.33.1.m1.1.1.cmml" xref="S4.T1.33.33.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.33.33.1.m1.1c">\pm</annotation></semantics></math> 0.11</th>
<td id="S4.T1.34.34.2" class="ltx_td ltx_align_left ltx_border_t">1.66 <math id="S4.T1.34.34.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.34.34.2.m1.1a"><mo id="S4.T1.34.34.2.m1.1.1" xref="S4.T1.34.34.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.34.34.2.m1.1b"><csymbol cd="latexml" id="S4.T1.34.34.2.m1.1.1.cmml" xref="S4.T1.34.34.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.34.34.2.m1.1c">\pm</annotation></semantics></math> 0.14</td>
</tr>
<tr id="S4.T1.36.36" class="ltx_tr">
<th id="S4.T1.36.36.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T1.36.36.3.1" class="ltx_text">rev16</span></th>
<td id="S4.T1.36.36.4" class="ltx_td ltx_align_left ltx_border_r">medium</td>
<th id="S4.T1.35.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.80 <math id="S4.T1.35.35.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.35.35.1.m1.1a"><mo id="S4.T1.35.35.1.m1.1.1" xref="S4.T1.35.35.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.35.35.1.m1.1b"><csymbol cd="latexml" id="S4.T1.35.35.1.m1.1.1.cmml" xref="S4.T1.35.35.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.35.35.1.m1.1c">\pm</annotation></semantics></math> 0.06</th>
<td id="S4.T1.36.36.2" class="ltx_td ltx_align_left">1.06 <math id="S4.T1.36.36.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.36.36.2.m1.1a"><mo id="S4.T1.36.36.2.m1.1.1" xref="S4.T1.36.36.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.36.36.2.m1.1b"><csymbol cd="latexml" id="S4.T1.36.36.2.m1.1.1.cmml" xref="S4.T1.36.36.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.36.36.2.m1.1c">\pm</annotation></semantics></math> 0.07</td>
</tr>
<tr id="S4.T1.38.38" class="ltx_tr">
<td id="S4.T1.38.38.3" class="ltx_td ltx_align_left ltx_border_r">small</td>
<th id="S4.T1.37.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">0.35 <math id="S4.T1.37.37.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.37.37.1.m1.1a"><mo id="S4.T1.37.37.1.m1.1.1" xref="S4.T1.37.37.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.37.37.1.m1.1b"><csymbol cd="latexml" id="S4.T1.37.37.1.m1.1.1.cmml" xref="S4.T1.37.37.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.37.37.1.m1.1c">\pm</annotation></semantics></math> 0.03</th>
<td id="S4.T1.38.38.2" class="ltx_td ltx_align_left">0.63 <math id="S4.T1.38.38.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.38.38.2.m1.1a"><mo id="S4.T1.38.38.2.m1.1.1" xref="S4.T1.38.38.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.38.38.2.m1.1b"><csymbol cd="latexml" id="S4.T1.38.38.2.m1.1.1.cmml" xref="S4.T1.38.38.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.38.38.2.m1.1c">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr id="S4.T1.40.40" class="ltx_tr">
<th id="S4.T1.40.40.3" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<td id="S4.T1.40.40.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">base</td>
<th id="S4.T1.39.39.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">0.20 <math id="S4.T1.39.39.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.39.39.1.m1.1a"><mo id="S4.T1.39.39.1.m1.1.1" xref="S4.T1.39.39.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.39.39.1.m1.1b"><csymbol cd="latexml" id="S4.T1.39.39.1.m1.1.1.cmml" xref="S4.T1.39.39.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.39.39.1.m1.1c">\pm</annotation></semantics></math> 0.02</th>
<td id="S4.T1.40.40.2" class="ltx_td ltx_align_left ltx_border_b">0.47 <math id="S4.T1.40.40.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.40.40.2.m1.1a"><mo id="S4.T1.40.40.2.m1.1.1" xref="S4.T1.40.40.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.40.40.2.m1.1b"><csymbol cd="latexml" id="S4.T1.40.40.2.m1.1.1.cmml" xref="S4.T1.40.40.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.40.40.2.m1.1c">\pm</annotation></semantics></math> 0.04</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Whispy carries a transcription delay spanning from 0.88 seconds to 1.66 seconds when the <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">large-v3</span> Whisper model is used. This delay decreases to a minimum of 0.44 seconds for the <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">base</span> model instances. The low transcription times recorded for the ESIC dataset are caused by the short duration of its audio clips. In fact, when a live transcription starts there is a ramp-up phase when the buffer fills with audio chunks, during which Whispy does not process the entire buffer content but rather a portion of it. In our setup of 5 chunks of 4 seconds each, Whispy first transcribes 4 seconds of audio, then 8 seconds, up until the buffer is full and there is a transcription of 20 seconds of audio every 4 seconds. For audio clips of 1 to 2 minutes, the low processing times at the beginning of the stream favourably skew the average transcription time.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Total delay times should be added to the selected chunk length to obtain the operational delay experienced when transcribing a live audio stream with Whispy. Therefore, in our experimental campaigns transcripts started being produced after roughly 5 seconds of audio activity, and were subsequently delivered at a similar cadence. It is worth mentioning that the Whisper processing times are bound to the choice of hardware and the performance of the selected model, while Whispy introduces a physiological delay caused by its chunking mechanism. However, the length of the chunks acts as an hyperparameter, and can therefore be tuned so that overall latency is reduced.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Chunk length and buffer size</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We briefly investigated the effects of configuring Whispy with different values of chunk length and buffer size. The evaluation of these 2 hyperparameters was targeted at the TEDlium dataset only, as it shows stable transcription results in the base experiment line, and it also includes long-form audio clips of a mean duration of around 15 minutes. Tested values for the chunk length are 2, 4, and 6 seconds, while tested values for the buffer size are 3, 5, and 15 chunks.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 â€£ 4.3 Chunk length and buffer size â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the breakdown of WER (subfigure <a href="#S4.F6.sf1" title="In Figure 6 â€£ 4.3 Chunk length and buffer size â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6a</span></a>) and transcription latency (subfigure <a href="#S4.F6.sf2" title="In Figure 6 â€£ 4.3 Chunk length and buffer size â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6b</span></a>) for all the hyperparameter configurations. Error rate in transcriptions generally decreases when longer audio chunks are used, and the opposite behaviour can be seen with respect to the buffer size. While it is certainly immediate to highlight the relationship between the total size of the audio submitted to the transcriber in each iteration and the processing traversal time, as similar delays occur for similar combinations of chunk lengths and buffer sizes, it is less intuitive to pinpoint the reason why different hyperparameters impact the outcome of the suggestions and, therefore, of the transcription quality.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.03484/assets/x6.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_square" width="185" height="176" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>hyperwer</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.03484/assets/x7.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_square" width="175" height="176" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>hyperlatency</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Longer chunks lead to a lower WER, on average, for all the tested models. However, increasing the number of chunks in the buffer worsens the transcription performance (<a href="#S4.F6.sf1" title="In Figure 6 â€£ 4.3 Chunk length and buffer size â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6a</span></a>). The reverse trend appears when measuring the transcription latency, for which combinations of longer chunks and buffer sizes result in longer processing times (<a href="#S4.F6.sf2" title="In Figure 6 â€£ 4.3 Chunk length and buffer size â€£ 4 Experimental results â€£ Whispy: Adapting STT Whisper Models to Real-Time Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6b</span></a>).</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Both chunk size and buffer length impact the Levenshtein distance calculation when a suggestion is generated. Few tokens transcribed differently between two subsequent transcriptions in an overlapping window can vary the distance the same way a small set of new words does. On the other hand, longer chunks are more stable but can introduce larger errors. Small deviations among multiple re-transcriptions will affect the overall transcribed text less than the most recently introduced audio. However, when an error occurs, it can affect a higher number of tokens, thus increasing the WER.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and future works</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduced Whispy, an adaptable ASR system built on pre-trained Whisper models, designed for real-time transcription of audio data. Whispy offers remarkable flexibility in deployment and integration with diverse real-world architectures. At its core, Whispy employs a versatile pipeline mechanism, potentially capable of handling multimodal data sources, including video and tabular data. Through rigorous testing on commonly used ASR benchmark datasets, we observed that Whispy maintains transcription performance comparable to its offline counterparts, while exhibiting minimal latency and configurable system settings. Additionally, Whispy features resistance to unexpected source or processing delays.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Moreover, Whispy is slated for production use at the upcoming Internet Engineering Task Force (IETF) meetings. The IETF recordings, along with their corresponding transcriptions generated by Whispy, will provide an invaluable, rich dataset that can be made publicly available to the research community. This dataset holds immense potential for re-training the underlying models and further advancing the state-of-the-art in ASR technology.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Several aspects of Whispy still warrant further exploration and refinement, as ongoing tests may uncover limitations or areas for improvement which will inform future iterations of the system. Firstly, the required steps of VAD can be leveraged to exclude silent regions within the audio buffer, thus reducing the required amount of data processing and lowering the overall system latency. Furthermore, later developments for Whispy may include expanding testing to additional publicly available datasets, increasing the hyperparameter tuning grid for optimization, and enhancing the capabilities of the hallucination filter. Additionally, there is potential to extend Whispyâ€™s functionality to encompass tasks such as diarization and summarization, as well as to evolve into a multimodal system capable of processing audio, video, and tabular inputs.</p>
</div>
<div id="S5.p4" class="ltx_para">
<span id="S5.p4.1" class="ltx_ERROR undefined">{credits}</span>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_ERROR undefined">\discintname</span>
</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span><span id="S5.SS1.SSS1.1.1" class="ltx_ERROR undefined">\discintname</span>
</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">The authors are all employed with Meetecho s.r.l. which funded and supported this paper. Meetecho is also the main entity behind the Janus WebRTC server. The authors have otherwise no competing interests to declare that are relevant to the content of this article.</p>
</div>
</section>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Speech To Text Technology Blog | Rev Blog â€” rev.com. <a target="_blank" href="https://www.rev.com/blog/speech-to-text-technology" title="" class="ltx_ref">https://www.rev.com/blog/speech-to-text-technology</a>, [Accessed 12-03-2024]

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Abdullah, H., Warren, K., Bindschaedler, V., Papernot, N., Traynor, P.: Sok: The faults in our asrs: An overview of attacks against automatic speech recognition and speaker identification systems. In: 2021 IEEE Symposium on Security and Privacy (SP). pp. 730â€“747 (2021). https://doi.org/10.1109/SP40001.2021.00014

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Amirante, A., Castaldi, T., Miniero, L., Romano, S.P.: Janus: A general purpose webrtc gateway. In: Proceedings of the Conference on Principles, Systems and Applications of IP Telecommunications. pp. 7:1â€“7:8. IPTComm â€™14, ACM, New York, NY, USA (2014). https://doi.org/10.1145/2670386.2670389, <a target="_blank" href="http://doi.acm.org/10.1145/2670386.2670389" title="" class="ltx_ref">http://doi.acm.org/10.1145/2670386.2670389</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Butler, J., Trager, B., Behm, B.: Exploration of automatic speech recognition for deaf and hard of hearing students in higher education classes. In: Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility. p. 32â€“42. ASSETS â€™19, Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3308561.3353772, <a target="_blank" href="https://doi.org/10.1145/3308561.3353772" title="" class="ltx_ref">https://doi.org/10.1145/3308561.3353772</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
DemÅ¡ar, J.: Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">7</span>(1), 1â€“30 (2006), <a target="_blank" href="http://jmlr.org/papers/v7/demsar06a.html" title="" class="ltx_ref">http://jmlr.org/papers/v7/demsar06a.html</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Frieske, R., Shi, B.E.: Hallucinations in neural automatic speech recognition: Identifying errors and hallucinatory models (2024)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Hernandez, F., Nguyen, V., Ghannay, S., Tomashenko, N., EstÃ¨ve, Y.: TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation, p. 198â€“208. Springer International Publishing (2018). https://doi.org/10.1007/978-3-319-99579-3_21

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Huh, J., Park, S., Lee, J., Ye, J.C.: Improving medical speech-to-text accuracy with vision-language pre-training model. IEEE journal of biomedical and health informatics <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">PP</span> (2023). https://doi.org/10.48550/arXiv.2303.00091

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Irugalbandara, C., Naseem, A.S., Perera, S., Kiruthikan, S., Logeeshan, V.: A secure and smart home automation system with speech recognition and power measurement capabilities. Sensors <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">23</span>(13) (2023). https://doi.org/10.3390/s23135784, <a target="_blank" href="https://www.mdpi.com/1424-8220/23/13/5784" title="" class="ltx_ref">https://www.mdpi.com/1424-8220/23/13/5784</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of hallucination in natural language generation. ACM Computing Surveys <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">55</span>(12), 1â€“38 (Mar 2023). https://doi.org/10.1145/3571730, <a target="_blank" href="http://dx.doi.org/10.1145/3571730" title="" class="ltx_ref">http://dx.doi.org/10.1145/3571730</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Klein, G., Minh-Thuc, Nguyen, V., Takahashi, K., Kanavos, P., Feil, M., Mas, J., Berkes, V., Han, B., scotfang, jhnwnd, jgcb00, Merx, R., Segal, N., Mukhutdinov, D., Chouteau, C., Caglayan, O., Bodza, S., S, S., Peretokin, V., Filipov, V., Zhang, X., Wang, Y., amrrs, Dockes, C., chengduo, chiiyeh, dependabot[bot], homink, nickchomey: Opennmt/ctranslate2: Efficient inference with transformer models. <a target="_blank" href="https://github.com/OpenNMT/CTranslate2" title="" class="ltx_ref">https://github.com/OpenNMT/CTranslate2</a>, [Accessed 19-02-2024]

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Klein, G., Purfview, Kim, K.H., Mas, J., FlippFuzz, Anthony, +, trungkienbkhn, palladium123, metame, makaveli, Caglayan, O., Oscaarjs, MinorJinx, Ashraf, M., FleiÃŸ, J., Barman, H., Enzinger, E., MachÃ¡Äek, D., Axelrod, D., Chuan, N.D., Yochum, C., Bakar, B., Moreno, A.Z., Sood, A., Wata, A.: GitHub - SYSTRAN/faster-whisper: Faster Whisper transcription with CTranslate2 â€” github.com. <a target="_blank" href="https://github.com/SYSTRAN/faster-whisper" title="" class="ltx_ref">https://github.com/SYSTRAN/faster-whisper</a>, [Accessed 15-02-2024]

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Koenecke, A., Choi, A.S.G., Mei, K., Schellmann, H., Sloane, M.: Careless whisper: Speech-to-text hallucination harms (2024)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kroening, K., Depau, D., Clauss, C., Stier, N., Jacotsu, McDonald, K., Trybus, M., Filho, T., magnusvmt, Raul, lcjh, laurentalacoque, apatsekin, Chen, S., Nitaym, Singaravelan, K., DeLaHunt, J., iulianOnofrei (U-lee aan), Angelo, deÂ Laat, A., Kolpakov, A., 372046933: kkroening/ffmpeg-python. <a target="_blank" href="https://github.com/kkroening/ffmpeg-python" title="" class="ltx_ref">https://github.com/kkroening/ffmpeg-python</a>, [Accessed 15-02-2024]

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kuhn, K., Kersken, V., Reuter, B., Egger, N., Zimmermann, G.: Measuring the Accuracy of Automatic Speech Recognition Solutions. ACM Trans. Access. Comput. <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">16</span>(4) (jan 2024). https://doi.org/10.1145/3636513, <a target="_blank" href="https://doi.org/10.1145/3636513" title="" class="ltx_ref">https://doi.org/10.1145/3636513</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
MachÃ¡Äek, D., Dabre, R., Bojar, O.: Turning whisper into real-time transcription system. In: System Demonstrations. pp. 17â€“24. Asian Federation of Natural Language Processing, Bali, Indonesia (November 2023)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
MachÃ¡Äek, D., Å½ilinec, M., Bojar, O.: Lost in Interpreting: Speech Translation from Source or Interpreter? In: Proc. Interspeech 2021. pp. 2376â€“2380 (2021). https://doi.org/10.21437/Interspeech.2021-2232

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Matsane, L., Jadhav, A., Ajoodha, R.: The use of automatic speech recognition in education for identifying attitudes of the speakers. In: 2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). pp.Â 1â€“7 (2020). https://doi.org/10.1109/CSDE50874.2020.9411528

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Meister, A., Novikov, M., Karpov, N., Bakhturina, E., Lavrukhin, V., Ginsburg, B.: Librispeech-pc: Benchmark for evaluation of punctuation and capitalization capabilities of end-to-end asr models (2023)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Miller, F.P., Vandome, A.F., McBrewster, J.: Levenshtein Distance: Information theory, Computer science, String (computer science), String metric, Damerau?Levenshtein distance, Spell checker, Hamming distance. Alpha Press (2009)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
von Neumann, T., Boeddeker, C., Kinoshita, K., Delcroix, M., Haeb-Umbach, R.: On word error rate definitions and their efficient computation for multi-speaker speech recognition systems. In: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE (Jun 2023). https://doi.org/10.1109/icassp49357.2023.10094784, <a target="_blank" href="http://dx.doi.org/10.1109/ICASSP49357.2023.10094784" title="" class="ltx_ref">http://dx.doi.org/10.1109/ICASSP49357.2023.10094784</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Prabhavalkar, R., Hori, T., Sainath, T.N., SchlÃ¼ter, R., Watanabe, S.: End-to-End Speech Recognition: A Survey. IEEE/ACM Transactions on Audio, Speech, and Language Processing <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">32</span>, 325â€“351 (2024). https://doi.org/10.1109/TASLP.2023.3328283

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever, I.: Robust speech recognition via large-scale weak supervision (2022)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Saccoia, A.: alesaccoia/voicestreamai. <a target="_blank" href="https://github.com/alesaccoia/VoiceStreamAI" title="" class="ltx_ref">https://github.com/alesaccoia/VoiceStreamAI</a>, [Accessed 29-02-2024]

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Sheikholmolouki, A.: Alireza29675/whisper-live. <a target="_blank" href="https://github.com/Alireza29675/whisper-live" title="" class="ltx_ref">https://github.com/Alireza29675/whisper-live</a>, [Accessed 29-02-2024]

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Team, S.: Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier. <a target="_blank" href="https://github.com/snakers4/silero-vad" title="" class="ltx_ref">https://github.com/snakers4/silero-vad</a>, [Accessed 15-02-2024]

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Vargas, C., Gaiera, A., BrandÃ¡n, A., Renato, A., BenÃ­tez, S., Luna, D.: Automatic speech recognition system to record progress notes in a mobile ehr: A pilot study. Studies in health technology and informatics <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">310</span>, 124â€“128 (01 2024). https://doi.org/10.3233/SHTI230940

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.03483" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.03484" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.03484">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.03484" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.03485" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 16:23:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
