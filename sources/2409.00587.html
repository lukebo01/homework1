<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.00587] FLUX that Plays Music</title><meta property="og:description" content="This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic.
Generally, along with design in advanced Flux111https://github.com/black-forest-l…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FLUX that Plays Music">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FLUX that Plays Music">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.00587">

<!--Generated on Sat Oct  5 22:59:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">FLUX that Plays Music</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhengcong Fei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingyuan Fan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changqian Yu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junshi Huang
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic.
Generally, along with design in advanced Flux<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/black-forest-labs/flux</span></span></span> model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs.
Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations.
Our experimental data, code, and model weights are made publicly available at: <a target="_blank" href="https://github.com/feizc/FluxMusic" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/feizc/FluxMusic</a>.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">Kunlun Inc.

</p>
</div>
<div id="p3" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Music, as a form of artistic expression, holds profound cultural importance and resonates deeply with human experiences <cite class="ltx_cite ltx_citemacro_citep">(Briot et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. The task of text-to-music generation, which involves converting textual descriptions of emotions, styles, instruments, and other musical elements into audio, offers innovative tools and new avenues for multimedia creation <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite>. Recent advancements in generative models have led to significant progress in this area <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib91" title="" class="ltx_ref">2017</a>; Dong et al., <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Mittal et al., <a href="#bib.bib67" title="" class="ltx_ref">2021</a>)</cite>. Traditionally, approaches to text-to-music generation have relied on either language models or diffusion models to represent quantized waveforms or spectral features <cite class="ltx_cite ltx_citemacro_citep">(Agostinelli et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Lam et al., <a href="#bib.bib48" title="" class="ltx_ref">2024</a>; Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>; Evans et al., <a href="#bib.bib22" title="" class="ltx_ref">2024</a>; Schneider et al., <a href="#bib.bib77" title="" class="ltx_ref">2024</a>; Fei et al., <a href="#bib.bib28" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib27" title="" class="ltx_ref">2023c</a>; Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2024b</a>)</cite>. Among these, diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite>, trained to reverse the process of data transformation from structured states to random noise <cite class="ltx_cite ltx_citemacro_citep">(Sohl-Dickstein et al., <a href="#bib.bib78" title="" class="ltx_ref">2015</a>; Song &amp; Ermon, <a href="#bib.bib80" title="" class="ltx_ref">2020</a>)</cite>, have shown exceptional effectiveness in modeling high-dimensional perceptual data, including music <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a href="#bib.bib38" title="" class="ltx_ref">2020</a>; Huang et al., <a href="#bib.bib42" title="" class="ltx_ref">2023c</a>; Ho et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Kong et al., <a href="#bib.bib47" title="" class="ltx_ref">2020b</a>; Rombach et al., <a href="#bib.bib73" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the iterative nature of diffusion process, coupled with the significant computational costs and extended sampling times during inference, there has been a growing body of research focused on developing more efficient training strategy and accelerating sampling schedule  <cite class="ltx_cite ltx_citemacro_citep">(Karras et al., <a href="#bib.bib43" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib58" title="" class="ltx_ref">2022a</a>; Lu et al., <a href="#bib.bib61" title="" class="ltx_ref">2022a</a>; Fei, <a href="#bib.bib33" title="" class="ltx_ref">2019</a>; Lu et al., <a href="#bib.bib62" title="" class="ltx_ref">2022b</a>; Kingma &amp; Gao, <a href="#bib.bib45" title="" class="ltx_ref">2024</a>)</cite>, such as distillation <cite class="ltx_cite ltx_citemacro_citep">(Sauer et al., <a href="#bib.bib76" title="" class="ltx_ref">2024</a>; Song et al., <a href="#bib.bib82" title="" class="ltx_ref">2023</a>; Song &amp; Dhariwal, <a href="#bib.bib79" title="" class="ltx_ref">2023</a>)</cite>. A particularly effective approach involves defining a forward path from data to noise, which facilitates more efficient training <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib63" title="" class="ltx_ref">2024a</a>)</cite> as well as better generative performance. One effective method among them is the Rectified Flow <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib58" title="" class="ltx_ref">2022a</a>; Albergo &amp; Vanden-Eijnden, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>; Lipman et al., <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>, where data and noise are connected along a linear trajectory. It offers improved theoretical properties and has shown promising results in image generation<cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib63" title="" class="ltx_ref">2024a</a>; Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>, however, its application in music creation remains largely unexplored.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the design of model architectures, traditional diffusion models frequently employ U-Net <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a href="#bib.bib74" title="" class="ltx_ref">2015</a>)</cite> as the foundational structure. However, the inherent inductive biases of convolutional neural networks inadequately captures the spatial correlations within signals <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> and are insensitive to scaling laws <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib51" title="" class="ltx_ref">2024b</a>)</cite>. Transformer-based diffusion models have effectively addressed these limitations <cite class="ltx_cite ltx_citemacro_citep">(Peebles &amp; Xie, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>; Bao et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Fei et al., <a href="#bib.bib29" title="" class="ltx_ref">2024b</a>, <a href="#bib.bib30" title="" class="ltx_ref">c</a>)</cite> by treating images as sequences of concatenated patches and utilizing stacked transformer blocks for noise prediction. The incorporation of cross-attention for integrating textual information has established this approach as the standard for generating high-resolution images and videos from natural language descriptions, demonstrating impressive generalization capabilities <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>, <a href="#bib.bib9" title="" class="ltx_ref">2024a</a>; Fei et al., <a href="#bib.bib32" title="" class="ltx_ref">2024e</a>; Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>; Fei et al., <a href="#bib.bib26" title="" class="ltx_ref">2023b</a>; Ma et al., <a href="#bib.bib64" title="" class="ltx_ref">2024b</a>, <a href="#bib.bib64" title="" class="ltx_ref">b</a>; Yang et al., <a href="#bib.bib92" title="" class="ltx_ref">2024</a>)</cite>. Notably, the recently open-sourced FLUX model, with its well-designed structure, exhibits strong semantic understanding and produces high-quality images, positioning it as a promising framework for conditional generation tasks.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.00587/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="496" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.8.1" class="ltx_text ltx_font_bold">Model architecture of FluxMusic. </span> We use frozen CLAP-L and T5-XXL as text encoders for conditioned caption feature extraction. The coarse text information concatenated with timestep embedding <math id="S1.F1.4.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S1.F1.4.m1.1b"><mi id="S1.F1.4.m1.1.1" xref="S1.F1.4.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S1.F1.4.m1.1c"><ci id="S1.F1.4.m1.1.1.cmml" xref="S1.F1.4.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m1.1d">y</annotation></semantics></math> are used to modulation mechanism. The fine-grained text <math id="S1.F1.5.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S1.F1.5.m2.1b"><mi id="S1.F1.5.m2.1.1" xref="S1.F1.5.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S1.F1.5.m2.1c"><ci id="S1.F1.5.m2.1.1.cmml" xref="S1.F1.5.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m2.1d">c</annotation></semantics></math> concatenated with music sequence <math id="S1.F1.6.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S1.F1.6.m3.1b"><mi id="S1.F1.6.m3.1.1" xref="S1.F1.6.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S1.F1.6.m3.1c"><ci id="S1.F1.6.m3.1.1.cmml" xref="S1.F1.6.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m3.1d">x</annotation></semantics></math> are input to a stacked of double stream block and single steam blocks to predict nose in a latent VAE space.
</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we explore the application of rectified flow Transformers within noise-predictive diffusion for text-to-music generation, introducing FluxMusic as a unified and scalable generative framework in the latent VAE space of the mel-spectrogram, as illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Building upon the text-to-image FLUX model, we present a transformer-based architecture that initially integrates learnable double streams attention for the concatenated music-text sequence, facilitating a bidirectional flow of information between the modalities. Subsequently, the text stream is dropped, leaving a stacked single music stream for noised patch prediction. We leverage multiple pre-trained text encoders for extracting conditioned caption features and inference flexibility. Coarse textual information from CLAP-L <cite class="ltx_cite ltx_citemacro_citep">(Elizalde et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, combined with time step embeddings, is employed in the modulation mechanism, while fine-grained textual details from T5-XXL <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite> are concatenated with music patch sequence as input. We train the model with rectified flow formulation and investigate its scalability. Through a in-depth study, we compare our new formulation to existing diffusion formulations and demonstrate its benefits for training efficiency and performance enhancement.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The primary contributions of this work are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a Flux-like Transformer architecture for text-to-music generation, equipped with rectified flow training. To the best of our knowledge, this is the first study to apply rectified flow transformers to text-to-music generation;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We perform a comprehensive system analysis, encompassing network design, rectified flow sampling, and parameter scaling, demonstrating the advantages of FluxMusic architecture in text-to-music generation;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experimental results demonstrate that FluxMusic achieves generative performance on par with other recent models with adequate training on both automatic metrics and human preference ratings. Finally, we make the results, code, and model weights publicly available to support further research.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-to-music Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Text-to-music generation seeks to produce music clips that correspond to descriptive or summarized text inputs. Prior approaches have primarily employed language models (LMs) or diffusion models (DMs) to generate quantized waveform representations or spectral features. For generating discrete representation of waveform, models such as MusicLM <cite class="ltx_cite ltx_citemacro_citep">(Agostinelli et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, MusicGen <cite class="ltx_cite ltx_citemacro_citep">(Copet et al., <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>, MeLoDy <cite class="ltx_cite ltx_citemacro_citep">(Lam et al., <a href="#bib.bib48" title="" class="ltx_ref">2024</a>)</cite>, and JEN-1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib52" title="" class="ltx_ref">2024c</a>)</cite> utilize LMs and DMs on residual codebooks derived from quantization-based audio codecs <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al., <a href="#bib.bib94" title="" class="ltx_ref">2021</a>; Défossez et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>. Conversely, models like Moûsai <cite class="ltx_cite ltx_citemacro_citep">(Schneider et al., <a href="#bib.bib77" title="" class="ltx_ref">2024</a>)</cite>, Noise2Music <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite>, Riffusion <cite class="ltx_cite ltx_citemacro_citep">(Forsgren &amp; Martiros, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>, AudioLDM 2 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, MusicLDM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2024b</a>)</cite>, and StableAudio <cite class="ltx_cite ltx_citemacro_citep">(Evans et al., <a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite> employ U-Net-based diffusion techniques to model mel-spectrograms or latent representations obtained through pretrained VAEs, subsequently converting them into audio waveforms using pretrained vocoders <cite class="ltx_cite ltx_citemacro_citep">(Kong et al., <a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>. Additionally, models such as Mustango <cite class="ltx_cite ltx_citemacro_citep">(Melechovsky et al., <a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite> and Music Controlnet <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib89" title="" class="ltx_ref">2024</a>)</cite> incorporate control signals or personalization <cite class="ltx_cite ltx_citemacro_citep">(Plitsis et al., <a href="#bib.bib69" title="" class="ltx_ref">2024</a>; Fei et al., <a href="#bib.bib25" title="" class="ltx_ref">2023a</a>)</cite>, including chords and beats, in a manner similar to ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib95" title="" class="ltx_ref">2023</a>)</cite>. Our method along with this approach by modeling the mel-spectrogram within a latent VAE space.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Diffusion Transformers</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib86" title="" class="ltx_ref">2017</a>)</cite> has achieved remarkable success in language models <cite class="ltx_cite ltx_citemacro_citep">(Radford, <a href="#bib.bib70" title="" class="ltx_ref">2018</a>; Radford et al., <a href="#bib.bib71" title="" class="ltx_ref">2019</a>; Raffel et al., <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite> and has also demonstrated significant potential across various computer vision tasks, including image classification <cite class="ltx_cite ltx_citemacro_citep">(DOSOVITSKIY, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; He et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Touvron et al., <a href="#bib.bib85" title="" class="ltx_ref">2021</a>; Zhou et al., <a href="#bib.bib98" title="" class="ltx_ref">2021</a>; Yuan et al., <a href="#bib.bib93" title="" class="ltx_ref">2021</a>; Han et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, object detection <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib59" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib87" title="" class="ltx_ref">2021</a>, <a href="#bib.bib88" title="" class="ltx_ref">2022</a>; Carion et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, and semantic segmentation <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a href="#bib.bib97" title="" class="ltx_ref">2021</a>; Xie et al., <a href="#bib.bib90" title="" class="ltx_ref">2021</a>; Strudel et al., <a href="#bib.bib83" title="" class="ltx_ref">2021</a>)</cite>, among others <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib84" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib54" title="" class="ltx_ref">2022b</a>; Zhao et al., <a href="#bib.bib96" title="" class="ltx_ref">2021</a>; Liu et al., <a href="#bib.bib60" title="" class="ltx_ref">2022b</a>; He et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Li et al., <a href="#bib.bib53" title="" class="ltx_ref">2022a</a>)</cite>. Building on this success, the diffusion Transformer <cite class="ltx_cite ltx_citemacro_citep">(Peebles &amp; Xie, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>; Fei et al., <a href="#bib.bib31" title="" class="ltx_ref">2024d</a>)</cite> and its variants <cite class="ltx_cite ltx_citemacro_citep">(Bao et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Fei et al., <a href="#bib.bib29" title="" class="ltx_ref">2024b</a>)</cite> have replaced the convolutional-based U-Net backbone <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a href="#bib.bib74" title="" class="ltx_ref">2015</a>)</cite> with Transformers, resulting in greater scalability and more straightforward parameter expansion compared to U-Net diffusion models. This scalability advantage has been particularly evident in domains such as video generation <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib64" title="" class="ltx_ref">2024b</a>)</cite>, image generation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, and speech generation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>. Notably, recent works such as Make-an-audio 2 <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib42" title="" class="ltx_ref">2023c</a>, <a href="#bib.bib40" title="" class="ltx_ref">a</a>)</cite> and StableAudio 2 <cite class="ltx_cite ltx_citemacro_citep">(Evans et al., <a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite> also explored the DiT architecture for audio and sound generation. In contrast, our work investigates the effectiveness of new multi-modal diffusion Transformer structure similar to Flux and optimized it with rectified flow.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">FluxMusic is a conceptually simple extension of FLUX, designed to facilitate text-to-music generation within a latent space. An overview of the model structure is illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In the following, we begin with a review of rectified flow as applied to diffusion models, followed by a detailed examination of the architecture for each component. We also discuss considerations regarding model scaling and data quality.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Rectified Flow Trajectories</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.8" class="ltx_p">In this work, we explore generative models that estabilish a mapping between samples <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="x_{1}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">x</mi><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x_{1}</annotation></semantics></math> from a noise distribution <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="p_{1}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">p</mi><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑝</ci><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">p_{1}</annotation></semantics></math> to samples <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">x</mi><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑥</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">x_{0}</annotation></semantics></math> from a data distribution <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="p_{0}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">p</mi><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑝</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">p_{0}</annotation></semantics></math> through a framework of ordinary differential equation (ODE).
The connection can be expressed as <math id="S3.SS1.p1.5.m5.2" class="ltx_Math" alttext="dy_{t}=v_{\Theta}(y_{t},t)\,dt" display="inline"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.2" xref="S3.SS1.p1.5.m5.2.2.cmml"><mrow id="S3.SS1.p1.5.m5.2.2.3" xref="S3.SS1.p1.5.m5.2.2.3.cmml"><mi id="S3.SS1.p1.5.m5.2.2.3.2" xref="S3.SS1.p1.5.m5.2.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.2.2.3.1" xref="S3.SS1.p1.5.m5.2.2.3.1.cmml">​</mo><msub id="S3.SS1.p1.5.m5.2.2.3.3" xref="S3.SS1.p1.5.m5.2.2.3.3.cmml"><mi id="S3.SS1.p1.5.m5.2.2.3.3.2" xref="S3.SS1.p1.5.m5.2.2.3.3.2.cmml">y</mi><mi id="S3.SS1.p1.5.m5.2.2.3.3.3" xref="S3.SS1.p1.5.m5.2.2.3.3.3.cmml">t</mi></msub></mrow><mo id="S3.SS1.p1.5.m5.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.cmml">=</mo><mrow id="S3.SS1.p1.5.m5.2.2.1" xref="S3.SS1.p1.5.m5.2.2.1.cmml"><msub id="S3.SS1.p1.5.m5.2.2.1.3" xref="S3.SS1.p1.5.m5.2.2.1.3.cmml"><mi id="S3.SS1.p1.5.m5.2.2.1.3.2" xref="S3.SS1.p1.5.m5.2.2.1.3.2.cmml">v</mi><mi mathvariant="normal" id="S3.SS1.p1.5.m5.2.2.1.3.3" xref="S3.SS1.p1.5.m5.2.2.1.3.3.cmml">Θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.2.2.1.2" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">​</mo><mrow id="S3.SS1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.1.2.cmml">(</mo><msub id="S3.SS1.p1.5.m5.2.2.1.1.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1.2.cmml">y</mi><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.1.2.cmml">,</mo><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">t</mi><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.1.1.4" xref="S3.SS1.p1.5.m5.2.2.1.1.2.cmml">)</mo></mrow><mo lspace="0.170em" rspace="0em" id="S3.SS1.p1.5.m5.2.2.1.2a" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">​</mo><mi id="S3.SS1.p1.5.m5.2.2.1.4" xref="S3.SS1.p1.5.m5.2.2.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.2.2.1.2b" xref="S3.SS1.p1.5.m5.2.2.1.2.cmml">​</mo><mi id="S3.SS1.p1.5.m5.2.2.1.5" xref="S3.SS1.p1.5.m5.2.2.1.5.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><apply id="S3.SS1.p1.5.m5.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2"><eq id="S3.SS1.p1.5.m5.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2"></eq><apply id="S3.SS1.p1.5.m5.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.3"><times id="S3.SS1.p1.5.m5.2.2.3.1.cmml" xref="S3.SS1.p1.5.m5.2.2.3.1"></times><ci id="S3.SS1.p1.5.m5.2.2.3.2.cmml" xref="S3.SS1.p1.5.m5.2.2.3.2">𝑑</ci><apply id="S3.SS1.p1.5.m5.2.2.3.3.cmml" xref="S3.SS1.p1.5.m5.2.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.3.3.1.cmml" xref="S3.SS1.p1.5.m5.2.2.3.3">subscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.3.3.2.cmml" xref="S3.SS1.p1.5.m5.2.2.3.3.2">𝑦</ci><ci id="S3.SS1.p1.5.m5.2.2.3.3.3.cmml" xref="S3.SS1.p1.5.m5.2.2.3.3.3">𝑡</ci></apply></apply><apply id="S3.SS1.p1.5.m5.2.2.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1"><times id="S3.SS1.p1.5.m5.2.2.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.2"></times><apply id="S3.SS1.p1.5.m5.2.2.1.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.1.3.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.3">subscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.1.3.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.3.2">𝑣</ci><ci id="S3.SS1.p1.5.m5.2.2.1.3.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.3.3">Θ</ci></apply><interval closure="open" id="S3.SS1.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1"><apply id="S3.SS1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1.2">𝑦</ci><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.1.3">𝑡</ci></apply><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑡</ci></interval><ci id="S3.SS1.p1.5.m5.2.2.1.4.cmml" xref="S3.SS1.p1.5.m5.2.2.1.4">𝑑</ci><ci id="S3.SS1.p1.5.m5.2.2.1.5.cmml" xref="S3.SS1.p1.5.m5.2.2.1.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">dy_{t}=v_{\Theta}(y_{t},t)\,dt</annotation></semantics></math> where the velocity <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">v</annotation></semantics></math> is parameterized by the weights <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi mathvariant="normal" id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\Theta</annotation></semantics></math> of a neural network.
<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> proposed directly solving it using differentiable ODE solvers. However, it proves to be computationally intensive, particularly when applied to large neural network architectures that parameterize <math id="S3.SS1.p1.8.m8.2" class="ltx_Math" alttext="v_{\Theta}(y_{t},t)" display="inline"><semantics id="S3.SS1.p1.8.m8.2a"><mrow id="S3.SS1.p1.8.m8.2.2" xref="S3.SS1.p1.8.m8.2.2.cmml"><msub id="S3.SS1.p1.8.m8.2.2.3" xref="S3.SS1.p1.8.m8.2.2.3.cmml"><mi id="S3.SS1.p1.8.m8.2.2.3.2" xref="S3.SS1.p1.8.m8.2.2.3.2.cmml">v</mi><mi mathvariant="normal" id="S3.SS1.p1.8.m8.2.2.3.3" xref="S3.SS1.p1.8.m8.2.2.3.3.cmml">Θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.2.2.2" xref="S3.SS1.p1.8.m8.2.2.2.cmml">​</mo><mrow id="S3.SS1.p1.8.m8.2.2.1.1" xref="S3.SS1.p1.8.m8.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.8.m8.2.2.1.1.2" xref="S3.SS1.p1.8.m8.2.2.1.2.cmml">(</mo><msub id="S3.SS1.p1.8.m8.2.2.1.1.1" xref="S3.SS1.p1.8.m8.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.8.m8.2.2.1.1.1.2" xref="S3.SS1.p1.8.m8.2.2.1.1.1.2.cmml">y</mi><mi id="S3.SS1.p1.8.m8.2.2.1.1.1.3" xref="S3.SS1.p1.8.m8.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p1.8.m8.2.2.1.1.3" xref="S3.SS1.p1.8.m8.2.2.1.2.cmml">,</mo><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">t</mi><mo stretchy="false" id="S3.SS1.p1.8.m8.2.2.1.1.4" xref="S3.SS1.p1.8.m8.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.2b"><apply id="S3.SS1.p1.8.m8.2.2.cmml" xref="S3.SS1.p1.8.m8.2.2"><times id="S3.SS1.p1.8.m8.2.2.2.cmml" xref="S3.SS1.p1.8.m8.2.2.2"></times><apply id="S3.SS1.p1.8.m8.2.2.3.cmml" xref="S3.SS1.p1.8.m8.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.2.2.3.1.cmml" xref="S3.SS1.p1.8.m8.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.8.m8.2.2.3.2.cmml" xref="S3.SS1.p1.8.m8.2.2.3.2">𝑣</ci><ci id="S3.SS1.p1.8.m8.2.2.3.3.cmml" xref="S3.SS1.p1.8.m8.2.2.3.3">Θ</ci></apply><interval closure="open" id="S3.SS1.p1.8.m8.2.2.1.2.cmml" xref="S3.SS1.p1.8.m8.2.2.1.1"><apply id="S3.SS1.p1.8.m8.2.2.1.1.1.cmml" xref="S3.SS1.p1.8.m8.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.8.m8.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.8.m8.2.2.1.1.1.2">𝑦</ci><ci id="S3.SS1.p1.8.m8.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.8.m8.2.2.1.1.1.3">𝑡</ci></apply><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝑡</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.2c">v_{\Theta}(y_{t},t)</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.11" class="ltx_p">A more effective strategy involves directly regressing a vector field <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="u_{t}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">u</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑢</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">u_{t}</annotation></semantics></math> that defines a probability trajectory between <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="p_{0}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">p</mi><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑝</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">p_{0}</annotation></semantics></math> and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="p_{1}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">p</mi><mn id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑝</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">p_{1}</annotation></semantics></math>.
To construct such a vector field <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="u_{t}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">u</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑢</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">u_{t}</annotation></semantics></math>, we consider a forward process that corresponds to a probability path <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="p_{t}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">p</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑝</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">p_{t}</annotation></semantics></math> transitioning from <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="p_{0}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">p</mi><mn id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝑝</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">p_{0}</annotation></semantics></math> to <math id="S3.SS1.p2.7.m7.2" class="ltx_Math" alttext="p_{1}=\mathcal{N}(0,1)" display="inline"><semantics id="S3.SS1.p2.7.m7.2a"><mrow id="S3.SS1.p2.7.m7.2.3" xref="S3.SS1.p2.7.m7.2.3.cmml"><msub id="S3.SS1.p2.7.m7.2.3.2" xref="S3.SS1.p2.7.m7.2.3.2.cmml"><mi id="S3.SS1.p2.7.m7.2.3.2.2" xref="S3.SS1.p2.7.m7.2.3.2.2.cmml">p</mi><mn id="S3.SS1.p2.7.m7.2.3.2.3" xref="S3.SS1.p2.7.m7.2.3.2.3.cmml">1</mn></msub><mo id="S3.SS1.p2.7.m7.2.3.1" xref="S3.SS1.p2.7.m7.2.3.1.cmml">=</mo><mrow id="S3.SS1.p2.7.m7.2.3.3" xref="S3.SS1.p2.7.m7.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.2.3.3.2" xref="S3.SS1.p2.7.m7.2.3.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.2.3.3.1" xref="S3.SS1.p2.7.m7.2.3.3.1.cmml">​</mo><mrow id="S3.SS1.p2.7.m7.2.3.3.3.2" xref="S3.SS1.p2.7.m7.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.7.m7.2.3.3.3.2.1" xref="S3.SS1.p2.7.m7.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">0</mn><mo id="S3.SS1.p2.7.m7.2.3.3.3.2.2" xref="S3.SS1.p2.7.m7.2.3.3.3.1.cmml">,</mo><mn id="S3.SS1.p2.7.m7.2.2" xref="S3.SS1.p2.7.m7.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS1.p2.7.m7.2.3.3.3.2.3" xref="S3.SS1.p2.7.m7.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.2b"><apply id="S3.SS1.p2.7.m7.2.3.cmml" xref="S3.SS1.p2.7.m7.2.3"><eq id="S3.SS1.p2.7.m7.2.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.1"></eq><apply id="S3.SS1.p2.7.m7.2.3.2.cmml" xref="S3.SS1.p2.7.m7.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.2.3.2.1.cmml" xref="S3.SS1.p2.7.m7.2.3.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.2.3.2.2.cmml" xref="S3.SS1.p2.7.m7.2.3.2.2">𝑝</ci><cn type="integer" id="S3.SS1.p2.7.m7.2.3.2.3.cmml" xref="S3.SS1.p2.7.m7.2.3.2.3">1</cn></apply><apply id="S3.SS1.p2.7.m7.2.3.3.cmml" xref="S3.SS1.p2.7.m7.2.3.3"><times id="S3.SS1.p2.7.m7.2.3.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.3.1"></times><ci id="S3.SS1.p2.7.m7.2.3.3.2.cmml" xref="S3.SS1.p2.7.m7.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p2.7.m7.2.3.3.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.3.3.2"><cn type="integer" id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">0</cn><cn type="integer" id="S3.SS1.p2.7.m7.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.2c">p_{1}=\mathcal{N}(0,1)</annotation></semantics></math>. This can be represented as <math id="S3.SS1.p2.8.m8.4" class="ltx_Math" alttext="z_{t}=a_{t}x_{0}+b_{t}\epsilon\quad\text{, where}\;\epsilon\sim\mathcal{N}(0,I)" display="inline"><semantics id="S3.SS1.p2.8.m8.4a"><mrow id="S3.SS1.p2.8.m8.4.4.2" xref="S3.SS1.p2.8.m8.4.4.3.cmml"><mrow id="S3.SS1.p2.8.m8.3.3.1.1" xref="S3.SS1.p2.8.m8.3.3.1.1.cmml"><msub id="S3.SS1.p2.8.m8.3.3.1.1.2" xref="S3.SS1.p2.8.m8.3.3.1.1.2.cmml"><mi id="S3.SS1.p2.8.m8.3.3.1.1.2.2" xref="S3.SS1.p2.8.m8.3.3.1.1.2.2.cmml">z</mi><mi id="S3.SS1.p2.8.m8.3.3.1.1.2.3" xref="S3.SS1.p2.8.m8.3.3.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS1.p2.8.m8.3.3.1.1.1" xref="S3.SS1.p2.8.m8.3.3.1.1.1.cmml">=</mo><mrow id="S3.SS1.p2.8.m8.3.3.1.1.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.cmml"><mrow id="S3.SS1.p2.8.m8.3.3.1.1.3.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.cmml"><msub id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.cmml"><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.2.cmml">a</mi><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.3.3.1.1.3.2.1" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.1.cmml">​</mo><msub id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.cmml"><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.2.cmml">x</mi><mn id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.SS1.p2.8.m8.3.3.1.1.3.1" xref="S3.SS1.p2.8.m8.3.3.1.1.3.1.cmml">+</mo><mrow id="S3.SS1.p2.8.m8.3.3.1.1.3.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.cmml"><msub id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.2" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.2.cmml">b</mi><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.3.3.1.1.3.3.1" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.3.3.1.1.3.3.3" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.3.cmml">ϵ</mi></mrow></mrow></mrow><mspace width="1em" id="S3.SS1.p2.8.m8.4.4.2.3" xref="S3.SS1.p2.8.m8.4.4.3a.cmml"></mspace><mrow id="S3.SS1.p2.8.m8.4.4.2.2" xref="S3.SS1.p2.8.m8.4.4.2.2.cmml"><mrow id="S3.SS1.p2.8.m8.4.4.2.2.2" xref="S3.SS1.p2.8.m8.4.4.2.2.2.cmml"><mtext id="S3.SS1.p2.8.m8.4.4.2.2.2.2" xref="S3.SS1.p2.8.m8.4.4.2.2.2.2a.cmml">, where</mtext><mo lspace="0.280em" rspace="0em" id="S3.SS1.p2.8.m8.4.4.2.2.2.1" xref="S3.SS1.p2.8.m8.4.4.2.2.2.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.4.4.2.2.2.3" xref="S3.SS1.p2.8.m8.4.4.2.2.2.3.cmml">ϵ</mi></mrow><mo id="S3.SS1.p2.8.m8.4.4.2.2.1" xref="S3.SS1.p2.8.m8.4.4.2.2.1.cmml">∼</mo><mrow id="S3.SS1.p2.8.m8.4.4.2.2.3" xref="S3.SS1.p2.8.m8.4.4.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.4.4.2.2.3.2" xref="S3.SS1.p2.8.m8.4.4.2.2.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.4.4.2.2.3.1" xref="S3.SS1.p2.8.m8.4.4.2.2.3.1.cmml">​</mo><mrow id="S3.SS1.p2.8.m8.4.4.2.2.3.3.2" xref="S3.SS1.p2.8.m8.4.4.2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.8.m8.4.4.2.2.3.3.2.1" xref="S3.SS1.p2.8.m8.4.4.2.2.3.3.1.cmml">(</mo><mn id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">0</mn><mo id="S3.SS1.p2.8.m8.4.4.2.2.3.3.2.2" xref="S3.SS1.p2.8.m8.4.4.2.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.8.m8.2.2" xref="S3.SS1.p2.8.m8.2.2.cmml">I</mi><mo stretchy="false" id="S3.SS1.p2.8.m8.4.4.2.2.3.3.2.3" xref="S3.SS1.p2.8.m8.4.4.2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.4b"><apply id="S3.SS1.p2.8.m8.4.4.3.cmml" xref="S3.SS1.p2.8.m8.4.4.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.4.4.3a.cmml" xref="S3.SS1.p2.8.m8.4.4.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.8.m8.3.3.1.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1"><eq id="S3.SS1.p2.8.m8.3.3.1.1.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.1"></eq><apply id="S3.SS1.p2.8.m8.3.3.1.1.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.3.3.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.3.3.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.2.2">𝑧</ci><ci id="S3.SS1.p2.8.m8.3.3.1.1.2.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3"><plus id="S3.SS1.p2.8.m8.3.3.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.1"></plus><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2"><times id="S3.SS1.p2.8.m8.3.3.1.1.3.2.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.1"></times><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.2">𝑎</ci><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.2.3">𝑡</ci></apply><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3">subscript</csymbol><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.2">𝑥</ci><cn type="integer" id="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.2.3.3">0</cn></apply></apply><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3"><times id="S3.SS1.p2.8.m8.3.3.1.1.3.3.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.1"></times><apply id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.2">𝑏</ci><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.2.3">𝑡</ci></apply><ci id="S3.SS1.p2.8.m8.3.3.1.1.3.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3.1.1.3.3.3">italic-ϵ</ci></apply></apply></apply><apply id="S3.SS1.p2.8.m8.4.4.2.2.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2"><csymbol cd="latexml" id="S3.SS1.p2.8.m8.4.4.2.2.1.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.1">similar-to</csymbol><apply id="S3.SS1.p2.8.m8.4.4.2.2.2.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.2"><times id="S3.SS1.p2.8.m8.4.4.2.2.2.1.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.2.1"></times><ci id="S3.SS1.p2.8.m8.4.4.2.2.2.2a.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.2.2"><mtext id="S3.SS1.p2.8.m8.4.4.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.2.2">, where</mtext></ci><ci id="S3.SS1.p2.8.m8.4.4.2.2.2.3.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.2.3">italic-ϵ</ci></apply><apply id="S3.SS1.p2.8.m8.4.4.2.2.3.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.3"><times id="S3.SS1.p2.8.m8.4.4.2.2.3.1.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.3.1"></times><ci id="S3.SS1.p2.8.m8.4.4.2.2.3.2.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p2.8.m8.4.4.2.2.3.3.1.cmml" xref="S3.SS1.p2.8.m8.4.4.2.2.3.3.2"><cn type="integer" id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">0</cn><ci id="S3.SS1.p2.8.m8.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2">𝐼</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.4c">z_{t}=a_{t}x_{0}+b_{t}\epsilon\quad\text{, where}\;\epsilon\sim\mathcal{N}(0,I)</annotation></semantics></math>.
With the conditions <math id="S3.SS1.p2.9.m9.2" class="ltx_Math" alttext="a_{0}=1,b_{0}=0,a_{1}=0" display="inline"><semantics id="S3.SS1.p2.9.m9.2a"><mrow id="S3.SS1.p2.9.m9.2.2.2" xref="S3.SS1.p2.9.m9.2.2.3.cmml"><mrow id="S3.SS1.p2.9.m9.1.1.1.1" xref="S3.SS1.p2.9.m9.1.1.1.1.cmml"><msub id="S3.SS1.p2.9.m9.1.1.1.1.2" xref="S3.SS1.p2.9.m9.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.9.m9.1.1.1.1.2.2" xref="S3.SS1.p2.9.m9.1.1.1.1.2.2.cmml">a</mi><mn id="S3.SS1.p2.9.m9.1.1.1.1.2.3" xref="S3.SS1.p2.9.m9.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS1.p2.9.m9.1.1.1.1.1" xref="S3.SS1.p2.9.m9.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.9.m9.1.1.1.1.3" xref="S3.SS1.p2.9.m9.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.9.m9.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p2.9.m9.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.3.cmml"><mrow id="S3.SS1.p2.9.m9.2.2.2.2.1.1" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.cmml"><msub id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.cmml"><mi id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.2.cmml">b</mi><mn id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS1.p2.9.m9.2.2.2.2.1.1.1" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.9.m9.2.2.2.2.1.1.3" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.3.cmml">0</mn></mrow><mo id="S3.SS1.p2.9.m9.2.2.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p2.9.m9.2.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.cmml"><msub id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2.cmml">a</mi><mn id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p2.9.m9.2.2.2.2.2.2.1" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.1.cmml">=</mo><mn id="S3.SS1.p2.9.m9.2.2.2.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml">0</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.2b"><apply id="S3.SS1.p2.9.m9.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.3a.cmml" xref="S3.SS1.p2.9.m9.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.9.m9.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1"><eq id="S3.SS1.p2.9.m9.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.1"></eq><apply id="S3.SS1.p2.9.m9.1.1.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.2.2">𝑎</ci><cn type="integer" id="S3.SS1.p2.9.m9.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.2.3">0</cn></apply><cn type="integer" id="S3.SS1.p2.9.m9.1.1.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.p2.9.m9.2.2.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.2.2.3a.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.9.m9.2.2.2.2.1.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1"><eq id="S3.SS1.p2.9.m9.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.1"></eq><apply id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.2">𝑏</ci><cn type="integer" id="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.2.3">0</cn></apply><cn type="integer" id="S3.SS1.p2.9.m9.2.2.2.2.1.1.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.1.1.3">0</cn></apply><apply id="S3.SS1.p2.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2"><eq id="S3.SS1.p2.9.m9.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.1"></eq><apply id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2">𝑎</ci><cn type="integer" id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3">1</cn></apply><cn type="integer" id="S3.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.2c">a_{0}=1,b_{0}=0,a_{1}=0</annotation></semantics></math> and <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="b_{1}=1" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mrow id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><msub id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2.2" xref="S3.SS1.p2.10.m10.1.1.2.2.cmml">b</mi><mn id="S3.SS1.p2.10.m10.1.1.2.3" xref="S3.SS1.p2.10.m10.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS1.p2.10.m10.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><eq id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1"></eq><apply id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.2.1.cmml" xref="S3.SS1.p2.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.2.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2.2">𝑏</ci><cn type="integer" id="S3.SS1.p2.10.m10.1.1.2.3.cmml" xref="S3.SS1.p2.10.m10.1.1.2.3">1</cn></apply><cn type="integer" id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">b_{1}=1</annotation></semantics></math>, the marginals <math id="S3.SS1.p2.11.m11.4" class="ltx_Math" alttext="p_{t}(z_{t})=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}p_{t}(z_{t}|\epsilon)\;" display="inline"><semantics id="S3.SS1.p2.11.m11.4a"><mrow id="S3.SS1.p2.11.m11.4.4" xref="S3.SS1.p2.11.m11.4.4.cmml"><mrow id="S3.SS1.p2.11.m11.3.3.1" xref="S3.SS1.p2.11.m11.3.3.1.cmml"><msub id="S3.SS1.p2.11.m11.3.3.1.3" xref="S3.SS1.p2.11.m11.3.3.1.3.cmml"><mi id="S3.SS1.p2.11.m11.3.3.1.3.2" xref="S3.SS1.p2.11.m11.3.3.1.3.2.cmml">p</mi><mi id="S3.SS1.p2.11.m11.3.3.1.3.3" xref="S3.SS1.p2.11.m11.3.3.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.11.m11.3.3.1.2" xref="S3.SS1.p2.11.m11.3.3.1.2.cmml">​</mo><mrow id="S3.SS1.p2.11.m11.3.3.1.1.1" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.11.m11.3.3.1.1.1.2" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.11.m11.3.3.1.1.1.1" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.cmml"><mi id="S3.SS1.p2.11.m11.3.3.1.1.1.1.2" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.2.cmml">z</mi><mi id="S3.SS1.p2.11.m11.3.3.1.1.1.1.3" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS1.p2.11.m11.3.3.1.1.1.3" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.11.m11.4.4.3" xref="S3.SS1.p2.11.m11.4.4.3.cmml">=</mo><mrow id="S3.SS1.p2.11.m11.4.4.2" xref="S3.SS1.p2.11.m11.4.4.2.cmml"><msub id="S3.SS1.p2.11.m11.4.4.2.3" xref="S3.SS1.p2.11.m11.4.4.2.3.cmml"><mi id="S3.SS1.p2.11.m11.4.4.2.3.2" xref="S3.SS1.p2.11.m11.4.4.2.3.2.cmml">𝔼</mi><mrow id="S3.SS1.p2.11.m11.2.2.2" xref="S3.SS1.p2.11.m11.2.2.2.cmml"><mi id="S3.SS1.p2.11.m11.2.2.2.4" xref="S3.SS1.p2.11.m11.2.2.2.4.cmml">ϵ</mi><mo id="S3.SS1.p2.11.m11.2.2.2.3" xref="S3.SS1.p2.11.m11.2.2.2.3.cmml">∼</mo><mrow id="S3.SS1.p2.11.m11.2.2.2.5" xref="S3.SS1.p2.11.m11.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.11.m11.2.2.2.5.2" xref="S3.SS1.p2.11.m11.2.2.2.5.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.11.m11.2.2.2.5.1" xref="S3.SS1.p2.11.m11.2.2.2.5.1.cmml">​</mo><mrow id="S3.SS1.p2.11.m11.2.2.2.5.3.2" xref="S3.SS1.p2.11.m11.2.2.2.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.11.m11.2.2.2.5.3.2.1" xref="S3.SS1.p2.11.m11.2.2.2.5.3.1.cmml">(</mo><mn id="S3.SS1.p2.11.m11.1.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.11.m11.2.2.2.5.3.2.2" xref="S3.SS1.p2.11.m11.2.2.2.5.3.1.cmml">,</mo><mi id="S3.SS1.p2.11.m11.2.2.2.2" xref="S3.SS1.p2.11.m11.2.2.2.2.cmml">I</mi><mo stretchy="false" id="S3.SS1.p2.11.m11.2.2.2.5.3.2.3" xref="S3.SS1.p2.11.m11.2.2.2.5.3.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.11.m11.4.4.2.2" xref="S3.SS1.p2.11.m11.4.4.2.2.cmml">​</mo><msub id="S3.SS1.p2.11.m11.4.4.2.4" xref="S3.SS1.p2.11.m11.4.4.2.4.cmml"><mi id="S3.SS1.p2.11.m11.4.4.2.4.2" xref="S3.SS1.p2.11.m11.4.4.2.4.2.cmml">p</mi><mi id="S3.SS1.p2.11.m11.4.4.2.4.3" xref="S3.SS1.p2.11.m11.4.4.2.4.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.11.m11.4.4.2.2a" xref="S3.SS1.p2.11.m11.4.4.2.2.cmml">​</mo><mrow id="S3.SS1.p2.11.m11.4.4.2.1.1" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.11.m11.4.4.2.1.1.2" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.11.m11.4.4.2.1.1.1" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.cmml"><msub id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.cmml"><mi id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.2" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.2.cmml">z</mi><mi id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.3" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p2.11.m11.4.4.2.1.1.1.1" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p2.11.m11.4.4.2.1.1.1.3" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.3.cmml">ϵ</mi></mrow><mo stretchy="false" id="S3.SS1.p2.11.m11.4.4.2.1.1.3" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.4b"><apply id="S3.SS1.p2.11.m11.4.4.cmml" xref="S3.SS1.p2.11.m11.4.4"><eq id="S3.SS1.p2.11.m11.4.4.3.cmml" xref="S3.SS1.p2.11.m11.4.4.3"></eq><apply id="S3.SS1.p2.11.m11.3.3.1.cmml" xref="S3.SS1.p2.11.m11.3.3.1"><times id="S3.SS1.p2.11.m11.3.3.1.2.cmml" xref="S3.SS1.p2.11.m11.3.3.1.2"></times><apply id="S3.SS1.p2.11.m11.3.3.1.3.cmml" xref="S3.SS1.p2.11.m11.3.3.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.3.3.1.3.1.cmml" xref="S3.SS1.p2.11.m11.3.3.1.3">subscript</csymbol><ci id="S3.SS1.p2.11.m11.3.3.1.3.2.cmml" xref="S3.SS1.p2.11.m11.3.3.1.3.2">𝑝</ci><ci id="S3.SS1.p2.11.m11.3.3.1.3.3.cmml" xref="S3.SS1.p2.11.m11.3.3.1.3.3">𝑡</ci></apply><apply id="S3.SS1.p2.11.m11.3.3.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.3.3.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.11.m11.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.p2.11.m11.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p2.11.m11.3.3.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S3.SS1.p2.11.m11.4.4.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2"><times id="S3.SS1.p2.11.m11.4.4.2.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2.2"></times><apply id="S3.SS1.p2.11.m11.4.4.2.3.cmml" xref="S3.SS1.p2.11.m11.4.4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.4.4.2.3.1.cmml" xref="S3.SS1.p2.11.m11.4.4.2.3">subscript</csymbol><ci id="S3.SS1.p2.11.m11.4.4.2.3.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2.3.2">𝔼</ci><apply id="S3.SS1.p2.11.m11.2.2.2.cmml" xref="S3.SS1.p2.11.m11.2.2.2"><csymbol cd="latexml" id="S3.SS1.p2.11.m11.2.2.2.3.cmml" xref="S3.SS1.p2.11.m11.2.2.2.3">similar-to</csymbol><ci id="S3.SS1.p2.11.m11.2.2.2.4.cmml" xref="S3.SS1.p2.11.m11.2.2.2.4">italic-ϵ</ci><apply id="S3.SS1.p2.11.m11.2.2.2.5.cmml" xref="S3.SS1.p2.11.m11.2.2.2.5"><times id="S3.SS1.p2.11.m11.2.2.2.5.1.cmml" xref="S3.SS1.p2.11.m11.2.2.2.5.1"></times><ci id="S3.SS1.p2.11.m11.2.2.2.5.2.cmml" xref="S3.SS1.p2.11.m11.2.2.2.5.2">𝒩</ci><interval closure="open" id="S3.SS1.p2.11.m11.2.2.2.5.3.1.cmml" xref="S3.SS1.p2.11.m11.2.2.2.5.3.2"><cn type="integer" id="S3.SS1.p2.11.m11.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1.1">0</cn><ci id="S3.SS1.p2.11.m11.2.2.2.2.cmml" xref="S3.SS1.p2.11.m11.2.2.2.2">𝐼</ci></interval></apply></apply></apply><apply id="S3.SS1.p2.11.m11.4.4.2.4.cmml" xref="S3.SS1.p2.11.m11.4.4.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.4.4.2.4.1.cmml" xref="S3.SS1.p2.11.m11.4.4.2.4">subscript</csymbol><ci id="S3.SS1.p2.11.m11.4.4.2.4.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2.4.2">𝑝</ci><ci id="S3.SS1.p2.11.m11.4.4.2.4.3.cmml" xref="S3.SS1.p2.11.m11.4.4.2.4.3">𝑡</ci></apply><apply id="S3.SS1.p2.11.m11.4.4.2.1.1.1.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1"><csymbol cd="latexml" id="S3.SS1.p2.11.m11.4.4.2.1.1.1.1.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.1.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.2.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.2">𝑧</ci><ci id="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.3.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.2.3">𝑡</ci></apply><ci id="S3.SS1.p2.11.m11.4.4.2.1.1.1.3.cmml" xref="S3.SS1.p2.11.m11.4.4.2.1.1.1.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.4c">p_{t}(z_{t})=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}p_{t}(z_{t}|\epsilon)\;</annotation></semantics></math> align with both the data and noise distribution.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">Referring to <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a href="#bib.bib55" title="" class="ltx_ref">2023</a>; Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>, we can construct a marginal vector field <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="u_{t}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">u</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑢</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">u_{t}</annotation></semantics></math> that generates the marginal probability paths <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="p_{t}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">p</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑝</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">p_{t}</annotation></semantics></math>, using the conditional vector fields as follows:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\displaystyle u_{t}(z)=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}[u_{t}(z|\epsilon)\frac{p_{t}(z|\epsilon)}{p_{t}(z)}]," display="inline"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml"><msub id="S3.E1.m1.6.6.1.1.3.2" xref="S3.E1.m1.6.6.1.1.3.2.cmml"><mi id="S3.E1.m1.6.6.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.3.2.2.cmml">u</mi><mi id="S3.E1.m1.6.6.1.1.3.2.3" xref="S3.E1.m1.6.6.1.1.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.3.1" xref="S3.E1.m1.6.6.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.3.3.2" xref="S3.E1.m1.6.6.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.3.3.2.1" xref="S3.E1.m1.6.6.1.1.3.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">z</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.3.3.2.2" xref="S3.E1.m1.6.6.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.2" xref="S3.E1.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">ϵ</mi><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">∼</mo><mrow id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.5.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.5.1" xref="S3.E1.m1.2.2.2.5.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.5.3.2" xref="S3.E1.m1.2.2.2.5.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.5.3.2.1" xref="S3.E1.m1.2.2.2.5.3.1.cmml">(</mo><mn id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">0</mn><mo id="S3.E1.m1.2.2.2.5.3.2.2" xref="S3.E1.m1.2.2.2.5.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">I</mi><mo stretchy="false" id="S3.E1.m1.2.2.2.5.3.2.3" xref="S3.E1.m1.2.2.2.5.3.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.cmml">u</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">ϵ</mi></mrow><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.1.1.1.1.2a" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.cmml">​</mo><mstyle displaystyle="true" id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mfrac id="S3.E1.m1.4.4a" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><msub id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml"><mi id="S3.E1.m1.3.3.1.3.2" xref="S3.E1.m1.3.3.1.3.2.cmml">p</mi><mi id="S3.E1.m1.3.3.1.3.3" xref="S3.E1.m1.3.3.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">ϵ</mi></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><msub id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml"><mi id="S3.E1.m1.4.4.2.3.2" xref="S3.E1.m1.4.4.2.3.2.cmml">p</mi><mi id="S3.E1.m1.4.4.2.3.3" xref="S3.E1.m1.4.4.2.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.2.4.2" xref="S3.E1.m1.4.4.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.2.4.2.1" xref="S3.E1.m1.4.4.2.cmml">(</mo><mi id="S3.E1.m1.4.4.2.1" xref="S3.E1.m1.4.4.2.1.cmml">z</mi><mo stretchy="false" id="S3.E1.m1.4.4.2.4.2.2" xref="S3.E1.m1.4.4.2.cmml">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo stretchy="false" id="S3.E1.m1.6.6.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"></eq><apply id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3"><times id="S3.E1.m1.6.6.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.1"></times><apply id="S3.E1.m1.6.6.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2">𝑢</ci><ci id="S3.E1.m1.6.6.1.1.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3">𝑡</ci></apply><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑧</ci></apply><apply id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.2"></times><apply id="S3.E1.m1.6.6.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2">𝔼</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3">similar-to</csymbol><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">italic-ϵ</ci><apply id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5"><times id="S3.E1.m1.2.2.2.5.1.cmml" xref="S3.E1.m1.2.2.2.5.1"></times><ci id="S3.E1.m1.2.2.2.5.2.cmml" xref="S3.E1.m1.2.2.2.5.2">𝒩</ci><interval closure="open" id="S3.E1.m1.2.2.2.5.3.1.cmml" xref="S3.E1.m1.2.2.2.5.3.2"><cn type="integer" id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">0</cn><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝐼</ci></interval></apply></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2">𝑢</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3">italic-ϵ</ci></apply><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><divide id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4"></divide><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><apply id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.3.1.cmml" xref="S3.E1.m1.3.3.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.3.2.cmml" xref="S3.E1.m1.3.3.1.3.2">𝑝</ci><ci id="S3.E1.m1.3.3.1.3.3.cmml" xref="S3.E1.m1.3.3.1.3.3">𝑡</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">𝑧</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">italic-ϵ</ci></apply></apply><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2"></times><apply id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.3.1.cmml" xref="S3.E1.m1.4.4.2.3">subscript</csymbol><ci id="S3.E1.m1.4.4.2.3.2.cmml" xref="S3.E1.m1.4.4.2.3.2">𝑝</ci><ci id="S3.E1.m1.4.4.2.3.3.cmml" xref="S3.E1.m1.4.4.2.3.3">𝑡</ci></apply><ci id="S3.E1.m1.4.4.2.1.cmml" xref="S3.E1.m1.4.4.2.1">𝑧</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\displaystyle u_{t}(z)=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}[u_{t}(z|\epsilon)\frac{p_{t}(z|\epsilon)}{p_{t}(z)}],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.7" class="ltx_p">The conditional flow matching objective can be then formulated as:</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.6" class="ltx_math_unparsed" alttext="\displaystyle{L}=\mathbb{E}_{t,p_{t}(z|\epsilon),p(\epsilon)}||v_{\Theta}(z,t)-u_{t}(z|\epsilon)||_{2}^{2}\;," display="inline"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6b"><mi id="S3.E2.m1.6.7">L</mi><mo id="S3.E2.m1.6.8">=</mo><msub id="S3.E2.m1.6.9"><mi id="S3.E2.m1.6.9.2">𝔼</mi><mrow id="S3.E2.m1.4.4.4.4"><mi id="S3.E2.m1.2.2.2.2">t</mi><mo id="S3.E2.m1.4.4.4.4.3">,</mo><mrow id="S3.E2.m1.3.3.3.3.1"><msub id="S3.E2.m1.3.3.3.3.1.3"><mi id="S3.E2.m1.3.3.3.3.1.3.2">p</mi><mi id="S3.E2.m1.3.3.3.3.1.3.3">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.3.1.2">​</mo><mrow id="S3.E2.m1.3.3.3.3.1.1.1"><mo stretchy="false" id="S3.E2.m1.3.3.3.3.1.1.1.2">(</mo><mrow id="S3.E2.m1.3.3.3.3.1.1.1.1"><mi id="S3.E2.m1.3.3.3.3.1.1.1.1.2">z</mi><mo fence="false" id="S3.E2.m1.3.3.3.3.1.1.1.1.1">|</mo><mi id="S3.E2.m1.3.3.3.3.1.1.1.1.3">ϵ</mi></mrow><mo stretchy="false" id="S3.E2.m1.3.3.3.3.1.1.1.3">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4.4.4">,</mo><mrow id="S3.E2.m1.4.4.4.4.2"><mi id="S3.E2.m1.4.4.4.4.2.2">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.4.2.1">​</mo><mrow id="S3.E2.m1.4.4.4.4.2.3.2"><mo stretchy="false" id="S3.E2.m1.4.4.4.4.2.3.2.1">(</mo><mi id="S3.E2.m1.1.1.1.1">ϵ</mi><mo stretchy="false" id="S3.E2.m1.4.4.4.4.2.3.2.2">)</mo></mrow></mrow></mrow></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2.m1.6.10">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2.m1.6.11">|</mo><msub id="S3.E2.m1.6.12"><mi id="S3.E2.m1.6.12.2">v</mi><mi mathvariant="normal" id="S3.E2.m1.6.12.3">Θ</mi></msub><mrow id="S3.E2.m1.6.13"><mo stretchy="false" id="S3.E2.m1.6.13.1">(</mo><mi id="S3.E2.m1.5.5">z</mi><mo id="S3.E2.m1.6.13.2">,</mo><mi id="S3.E2.m1.6.6">t</mi><mo stretchy="false" id="S3.E2.m1.6.13.3">)</mo></mrow><mo id="S3.E2.m1.6.14">−</mo><msub id="S3.E2.m1.6.15"><mi id="S3.E2.m1.6.15.2">u</mi><mi id="S3.E2.m1.6.15.3">t</mi></msub><mrow id="S3.E2.m1.6.16"><mo stretchy="false" id="S3.E2.m1.6.16.1">(</mo><mi id="S3.E2.m1.6.16.2">z</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2.m1.6.16.3">|</mo><mi id="S3.E2.m1.6.16.4">ϵ</mi><mo stretchy="false" id="S3.E2.m1.6.16.5">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2.m1.6.17">|</mo><msubsup id="S3.E2.m1.6.18"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2.m1.6.18.2.2">|</mo><mn id="S3.E2.m1.6.18.2.3">2</mn><mn id="S3.E2.m1.6.18.3">2</mn></msubsup><mo id="S3.E2.m1.6.19">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\displaystyle{L}=\mathbb{E}_{t,p_{t}(z|\epsilon),p(\epsilon)}||v_{\Theta}(z,t)-u_{t}(z|\epsilon)||_{2}^{2}\;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.3" class="ltx_p">where the conditional vector fields <math id="S3.SS1.p3.3.m1.1" class="ltx_Math" alttext="u_{t}(z|\epsilon)" display="inline"><semantics id="S3.SS1.p3.3.m1.1a"><mrow id="S3.SS1.p3.3.m1.1.1" xref="S3.SS1.p3.3.m1.1.1.cmml"><msub id="S3.SS1.p3.3.m1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.3.cmml"><mi id="S3.SS1.p3.3.m1.1.1.3.2" xref="S3.SS1.p3.3.m1.1.1.3.2.cmml">u</mi><mi id="S3.SS1.p3.3.m1.1.1.3.3" xref="S3.SS1.p3.3.m1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.3.m1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.2.cmml">​</mo><mrow id="S3.SS1.p3.3.m1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.3.m1.1.1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.3.m1.1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.SS1.p3.3.m1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p3.3.m1.1.1.1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.1.1.1.3.cmml">ϵ</mi></mrow><mo stretchy="false" id="S3.SS1.p3.3.m1.1.1.1.1.3" xref="S3.SS1.p3.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m1.1b"><apply id="S3.SS1.p3.3.m1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1"><times id="S3.SS1.p3.3.m1.1.1.2.cmml" xref="S3.SS1.p3.3.m1.1.1.2"></times><apply id="S3.SS1.p3.3.m1.1.1.3.cmml" xref="S3.SS1.p3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m1.1.1.3.1.cmml" xref="S3.SS1.p3.3.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.3.m1.1.1.3.2.cmml" xref="S3.SS1.p3.3.m1.1.1.3.2">𝑢</ci><ci id="S3.SS1.p3.3.m1.1.1.3.3.cmml" xref="S3.SS1.p3.3.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.p3.3.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.3.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p3.3.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.p3.3.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m1.1.1.1.1.1.3">italic-ϵ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m1.1c">u_{t}(z|\epsilon)</annotation></semantics></math> provides a tractable and equivalent objective.
Although there exists different variants of the above formalism, we focus on Rectified Flows (RF) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib58" title="" class="ltx_ref">2022a</a>; Albergo &amp; Vanden-Eijnden, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>; Lipman et al., <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>, which define the forward process as straight paths between the data distribution and a standard normal distribution:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="z_{t}=(1-t)x_{0}+t\epsilon," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml">z</mi><mi id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">x</mi><mn id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.3.3.cmml">ϵ</mi></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2">𝑧</ci><ci id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑥</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">0</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.3.2">𝑡</ci><ci id="S3.E3.m1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">z_{t}=(1-t)x_{0}+t\epsilon,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.6" class="ltx_p">with the loss function <math id="S3.SS1.p3.4.m1.1" class="ltx_Math" alttext="{L}" display="inline"><semantics id="S3.SS1.p3.4.m1.1a"><mi id="S3.SS1.p3.4.m1.1.1" xref="S3.SS1.p3.4.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m1.1b"><ci id="S3.SS1.p3.4.m1.1.1.cmml" xref="S3.SS1.p3.4.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m1.1c">{L}</annotation></semantics></math> corresponds to <math id="S3.SS1.p3.5.m2.1" class="ltx_Math" alttext="w_{t}^{\text{RF}}=\frac{t}{1-t}" display="inline"><semantics id="S3.SS1.p3.5.m2.1a"><mrow id="S3.SS1.p3.5.m2.1.1" xref="S3.SS1.p3.5.m2.1.1.cmml"><msubsup id="S3.SS1.p3.5.m2.1.1.2" xref="S3.SS1.p3.5.m2.1.1.2.cmml"><mi id="S3.SS1.p3.5.m2.1.1.2.2.2" xref="S3.SS1.p3.5.m2.1.1.2.2.2.cmml">w</mi><mi id="S3.SS1.p3.5.m2.1.1.2.2.3" xref="S3.SS1.p3.5.m2.1.1.2.2.3.cmml">t</mi><mtext id="S3.SS1.p3.5.m2.1.1.2.3" xref="S3.SS1.p3.5.m2.1.1.2.3a.cmml">RF</mtext></msubsup><mo id="S3.SS1.p3.5.m2.1.1.1" xref="S3.SS1.p3.5.m2.1.1.1.cmml">=</mo><mfrac id="S3.SS1.p3.5.m2.1.1.3" xref="S3.SS1.p3.5.m2.1.1.3.cmml"><mi id="S3.SS1.p3.5.m2.1.1.3.2" xref="S3.SS1.p3.5.m2.1.1.3.2.cmml">t</mi><mrow id="S3.SS1.p3.5.m2.1.1.3.3" xref="S3.SS1.p3.5.m2.1.1.3.3.cmml"><mn id="S3.SS1.p3.5.m2.1.1.3.3.2" xref="S3.SS1.p3.5.m2.1.1.3.3.2.cmml">1</mn><mo id="S3.SS1.p3.5.m2.1.1.3.3.1" xref="S3.SS1.p3.5.m2.1.1.3.3.1.cmml">−</mo><mi id="S3.SS1.p3.5.m2.1.1.3.3.3" xref="S3.SS1.p3.5.m2.1.1.3.3.3.cmml">t</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m2.1b"><apply id="S3.SS1.p3.5.m2.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1"><eq id="S3.SS1.p3.5.m2.1.1.1.cmml" xref="S3.SS1.p3.5.m2.1.1.1"></eq><apply id="S3.SS1.p3.5.m2.1.1.2.cmml" xref="S3.SS1.p3.5.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.1.1.2.1.cmml" xref="S3.SS1.p3.5.m2.1.1.2">superscript</csymbol><apply id="S3.SS1.p3.5.m2.1.1.2.2.cmml" xref="S3.SS1.p3.5.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m2.1.1.2.2.1.cmml" xref="S3.SS1.p3.5.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.5.m2.1.1.2.2.2.cmml" xref="S3.SS1.p3.5.m2.1.1.2.2.2">𝑤</ci><ci id="S3.SS1.p3.5.m2.1.1.2.2.3.cmml" xref="S3.SS1.p3.5.m2.1.1.2.2.3">𝑡</ci></apply><ci id="S3.SS1.p3.5.m2.1.1.2.3a.cmml" xref="S3.SS1.p3.5.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p3.5.m2.1.1.2.3.cmml" xref="S3.SS1.p3.5.m2.1.1.2.3">RF</mtext></ci></apply><apply id="S3.SS1.p3.5.m2.1.1.3.cmml" xref="S3.SS1.p3.5.m2.1.1.3"><divide id="S3.SS1.p3.5.m2.1.1.3.1.cmml" xref="S3.SS1.p3.5.m2.1.1.3"></divide><ci id="S3.SS1.p3.5.m2.1.1.3.2.cmml" xref="S3.SS1.p3.5.m2.1.1.3.2">𝑡</ci><apply id="S3.SS1.p3.5.m2.1.1.3.3.cmml" xref="S3.SS1.p3.5.m2.1.1.3.3"><minus id="S3.SS1.p3.5.m2.1.1.3.3.1.cmml" xref="S3.SS1.p3.5.m2.1.1.3.3.1"></minus><cn type="integer" id="S3.SS1.p3.5.m2.1.1.3.3.2.cmml" xref="S3.SS1.p3.5.m2.1.1.3.3.2">1</cn><ci id="S3.SS1.p3.5.m2.1.1.3.3.3.cmml" xref="S3.SS1.p3.5.m2.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m2.1c">w_{t}^{\text{RF}}=\frac{t}{1-t}</annotation></semantics></math>. The network output directly parameterizes the velocity <math id="S3.SS1.p3.6.m3.1" class="ltx_Math" alttext="v_{\Theta}" display="inline"><semantics id="S3.SS1.p3.6.m3.1a"><msub id="S3.SS1.p3.6.m3.1.1" xref="S3.SS1.p3.6.m3.1.1.cmml"><mi id="S3.SS1.p3.6.m3.1.1.2" xref="S3.SS1.p3.6.m3.1.1.2.cmml">v</mi><mi mathvariant="normal" id="S3.SS1.p3.6.m3.1.1.3" xref="S3.SS1.p3.6.m3.1.1.3.cmml">Θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m3.1b"><apply id="S3.SS1.p3.6.m3.1.1.cmml" xref="S3.SS1.p3.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m3.1.1.1.cmml" xref="S3.SS1.p3.6.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m3.1.1.2.cmml" xref="S3.SS1.p3.6.m3.1.1.2">𝑣</ci><ci id="S3.SS1.p3.6.m3.1.1.3.cmml" xref="S3.SS1.p3.6.m3.1.1.3">Θ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m3.1c">v_{\Theta}</annotation></semantics></math>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.5" class="ltx_td ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<th id="S3.T1.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">#Params</th>
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">#DoubleStream <math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mi id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">m</annotation></semantics></math>
</th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">#SingleStream <math id="S3.T1.2.2.2.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T1.2.2.2.m1.1a"><mi id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">n</annotation></semantics></math>
</th>
<th id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">Hidden dim. <math id="S3.T1.3.3.3.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.T1.3.3.3.m1.1a"><mi id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">d</annotation></semantics></math>
</th>
<th id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">Head number <math id="S3.T1.4.4.4.m1.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.T1.4.4.4.m1.1a"><mi id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">h</annotation></semantics></math>
</th>
<th id="S3.T1.4.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;">Gflops</th>
</tr>
<tr id="S3.T1.4.5.1" class="ltx_tr">
<td id="S3.T1.4.5.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">Small</td>
<td id="S3.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">142.3M</td>
<td id="S3.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">8</td>
<td id="S3.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">512</td>
<td id="S3.T1.4.5.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.5.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">194.5G</td>
</tr>
<tr id="S3.T1.4.6.2" class="ltx_tr">
<td id="S3.T1.4.6.2.1" class="ltx_td ltx_align_left" style="padding-left:5.7pt;padding-right:5.7pt;">Base</td>
<td id="S3.T1.4.6.2.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">473.9M</td>
<td id="S3.T1.4.6.2.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">12</td>
<td id="S3.T1.4.6.2.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">24</td>
<td id="S3.T1.4.6.2.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">768</td>
<td id="S3.T1.4.6.2.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.6.2.7" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">654.4G</td>
</tr>
<tr id="S3.T1.4.7.3" class="ltx_tr">
<td id="S3.T1.4.7.3.1" class="ltx_td ltx_align_left" style="padding-left:5.7pt;padding-right:5.7pt;">Large</td>
<td id="S3.T1.4.7.3.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">840.6M</td>
<td id="S3.T1.4.7.3.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">12</td>
<td id="S3.T1.4.7.3.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">24</td>
<td id="S3.T1.4.7.3.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">1024</td>
<td id="S3.T1.4.7.3.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.7.3.7" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;">1162.6G</td>
</tr>
<tr id="S3.T1.4.8.4" class="ltx_tr">
<td id="S3.T1.4.8.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">Giant</td>
<td id="S3.T1.4.8.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">2109.9M</td>
<td id="S3.T1.4.8.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.8.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">32</td>
<td id="S3.T1.4.8.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">1408</td>
<td id="S3.T1.4.8.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">16</td>
<td id="S3.T1.4.8.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;">2928.0G</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.6.1" class="ltx_text ltx_font_bold">Scaling law of FluxMusic model size.</span> The model sizes and detailed hyperparameters settings for scaling experiments.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To enable text-conditioned music generation, our FluxMusic model integrate both textual and musical modalities. We leverage pre-trained models to derive appropriate representations and then describe the architecture of our Flux-based model in detail.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Music compression.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.3" class="ltx_p">To better represent music, following <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, each 10.24-second audio clip, sampled at 16kHz, is first converted into a <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="64\times 1024" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">64\times 1024</annotation></semantics></math> mel-spectrogram, with 64 mel-bins, a hop length of 160, and a window length of 1024. This spectrogram is then compressed into a <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="16\times 128" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><times id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">16</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">16\times 128</annotation></semantics></math> latent representation, denoted as <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="X_{spec}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">X</mi><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1a" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.4" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1b" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.5" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.5.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝑋</ci><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3"><times id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.2">𝑠</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.3">𝑝</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.4">𝑒</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.5.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.5">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">X_{spec}</annotation></semantics></math>, using a Variational Autoencoder (VAE) pretrained on AudioLDM 2<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/cvssp/audioldm2-music/tree/main</span></span></span>. This latent space representation serves as the basis for noise addition and model training. Finally, a pretrained Hifi-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kong et al., <a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite> is employed to reconstruct the waveform from the generated mel-spectrogram.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Rectified flow transformers for music generation.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.4" class="ltx_p">Our architecture builds upon the MMDiT <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> and Flux architecture. Specifically, we first construct an input sequence consisting of embedding of the text and noised music. The noised latent music representation <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="X_{spec}\in\mathbb{R}^{h\times w\times c}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">X</mi><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1a" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1b" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.5" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.5.cmml">c</mi></mrow></msub><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml">w</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1a" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><in id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1"></in><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2">𝑋</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3"><times id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.2">𝑠</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.3">𝑝</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.4.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.4">𝑒</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.5.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.5">𝑐</ci></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3"><times id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2">ℎ</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.3">𝑤</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">X_{spec}\in\mathbb{R}^{h\times w\times c}</annotation></semantics></math> is flatten 2<math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><times id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">\times</annotation></semantics></math>2 patches to a sequence of length <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\frac{1}{2}h\cdot\frac{1}{2}w" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.cmml"><mfrac id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.cmml"><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.2.cmml">1</mn><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.3.cmml">h</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.1.cmml">⋅</mo><mfrac id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.2.cmml">1</mn><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.3.cmml">2</mn></mfrac></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1"></times><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2"><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.1">⋅</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.1"></times><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2"><divide id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2"></divide><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.2">1</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.2.3">2</cn></apply><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.2.3">ℎ</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3"><divide id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3"></divide><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.2">1</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.3.3">2</cn></apply></apply><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\frac{1}{2}h\cdot\frac{1}{2}w</annotation></semantics></math>. After aligning the dimensionality of the patch encoding and the fine-grained text encoding <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">c</annotation></semantics></math>, we concatenate the two sequences.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">We then forward with two type layers including double stream block and single stream blocks. In the double stream block, we employ two distinct sets of weights for the text and music modalities, effectively treating them as independent transformers that merge during the attention operation. This allows each modality to maintain its own space while still considering the other.
In the single stream block, the text component is dropped, focusing solely on music sequence modeling with modulation.
It is also found in AuraFlow<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://blog.fal.ai/auraflow/</span></span></span> that removing some of MMDiT layers to just be single DiT block were much more scalable and compute efficient way to train these models.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.2" class="ltx_p">We incorporate embeddings of the timestep <math id="S3.SS2.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.1.m1.1c">t</annotation></semantics></math> and coarse text <math id="S3.SS2.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.2.m2.1a"><mi id="S3.SS2.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.2.m2.1c">y</annotation></semantics></math> into the modulation mechanism.
Drawing from <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>, we employ multiple text encoders to capture various levels of textual information, thereby enhancing overall model performance and increasing flexibility during inference. By applying individual dropout rates during training, our model allows the use of any subset of text encoders during inference. This flexibility extends to the ability to pre-store blank textual representations, bypassing the need for network computation during inference.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discussion</h3>

<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model at Scale.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.6" class="ltx_p">In summary, the hyper-parameters of proposed FluxMusic architecture include the following key elements:
the number of double stream blocks <math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">m</annotation></semantics></math>, number of single stream blocks <math id="S3.SS3.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">n</annotation></semantics></math>, hidden state dimension <math id="S3.SS3.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">d</annotation></semantics></math>, and attention head number <math id="S3.SS3.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.4.m4.1c">h</annotation></semantics></math>.
Various configurations of FluxMusic are listed in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Rectified Flow Trajectories ‣ 3 Methodology ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, span a broad range of model sizes and computational requirements, from 142M to 2.1B parameters and from 194.5G to 2928.0G Flops. This range provides a thorough examination of the model’s scalability.
Additionally, the Gflop metric, evaluated for a 16<math id="S3.SS3.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.5.m5.1a"><mo id="S3.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.5.m5.1b"><times id="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.5.m5.1c">\times</annotation></semantics></math>128 text-to-music generation with a patch size of <math id="S3.SS3.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="p=2" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.6.m6.1a"><mrow id="S3.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml">p</mi><mo id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1"><eq id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1"></eq><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2">𝑝</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.6.m6.1c">p=2</annotation></semantics></math>, i.e., 10s music clips according to blank text, is calculated using the <span id="S3.SS3.SSS0.Px1.p1.6.1" class="ltx_text ltx_font_typewriter">thop</span> Python package.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data incorporation.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">It is widely recognized that synthetically generated captions can greatly improve performance of generative model at scale, i.e., text-to-image generation <cite class="ltx_cite ltx_citemacro_citep">(Fei et al., <a href="#bib.bib32" title="" class="ltx_ref">2024e</a>; Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>; Betker et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Fei, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Fei et al., <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>.
We follow their design and incorporate enriched music captions produced by a fine-tuned large language model.
Specifically, we use the LP-MusicCaps model <cite class="ltx_cite ltx_citemacro_citep">(Doh et al., <a href="#bib.bib16" title="" class="ltx_ref">2024</a>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>, available on Huggingface<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/seungheondoh/ttmr-pp</span></span></span>.
To mitigate the potential risk of the text-to-music model forgetting certain concepts not covered in the music captioner’s knowledge base, we maintain a balanced input by using a mixture of 20% original captions and 80% synthetic captions.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental settings</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We employ several datasets, including AudioSet Music Subset (ASM) <cite class="ltx_cite ltx_citemacro_citep">(Gemmeke et al., <a href="#bib.bib35" title="" class="ltx_ref">2017</a>)</cite>, MagnaTagTune, Million Song Dataset (MSD) <cite class="ltx_cite ltx_citemacro_citep">(Bertin-Mahieux et al., <a href="#bib.bib4" title="" class="ltx_ref">2011</a>)</cite>, MagnaTagTune (MTT) <cite class="ltx_cite ltx_citemacro_citep">(Law et al., <a href="#bib.bib49" title="" class="ltx_ref">2009</a>)</cite>, Free Music Archive (FMA) <cite class="ltx_cite ltx_citemacro_citep">(Defferrard et al., <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite>, Music4All <cite class="ltx_cite ltx_citemacro_citep">(Santana et al., <a href="#bib.bib75" title="" class="ltx_ref">2020</a>)</cite>, and an additional private dataset.
Each audio track was segmented into 10-second clips and uniformly sampled at 16 kHz to ensure consistency across the datasets. Detailed captions corresponding to these clips were sourced from Hugging Face datasets<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/collections/seungheondoh/enriching-music-descriptions-661e9342edcea210d61e981d</span></span></span>. Additionally, we automatically labeled the remaining music data using LP-MusicCaps models. This preprocessing resulted in a comprehensive training dataset encompassing a total of <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">\sim</annotation></semantics></math>22K hours of diverse music content.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p2.1" class="ltx_p">To benchmark our MusicFlux model against prior work, we conducted evaluations using the widely recognized MusicCaps dataset <cite class="ltx_cite ltx_citemacro_citep">(Agostinelli et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> and the Song-Describer-Dataset <cite class="ltx_cite ltx_citemacro_citep">(Manco et al., <a href="#bib.bib65" title="" class="ltx_ref">2023</a>)</cite>. The prior dataset comprises 5.5K clips of 10.24 seconds each, accompanied by high-quality music descriptions provided by ten professional musicians. The latter dataset contains 706 licensed high-quality music recordings.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementail details.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We utilize the last hidden state of FLAN-T5-XXL as fine-grained textual information and the pooler output of CLAP-L as coarse textual features.
Referring to <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, our training process involves 10-second music clips, randomly sampled from full tracks. The training configuration includes a batch size of 128, a gradient clipping threshold of 1.0, and a learning rate of 1e-4.
During inference, we apply a rectified flow with 50 steps and use a guidance scale of 3.5. To ensure model stability and performance, we maintain a secondary copy of the model weights, updated every 100 training batches through an exponential moving average (EMA) with a decay rate of 0.99, following the approach outlined by <cite class="ltx_cite ltx_citemacro_citet">Peebles &amp; Xie (<a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>. For unconditional diffusion guidance, we independently set the outputs of each of the two text encoders to null with a probability of 10%.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metrics.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The generated results are assessed using several objective metrics, including the Fréchet Audio Distance (FAD) <cite class="ltx_cite ltx_citemacro_citep">(Kilgour et al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, Kullback-Leibler Divergence (KL), Inception Score (IS).
To ensure a standardized and consistent evaluation process, all metrics are calculated utilizing the <span id="S4.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">audioldm_eval</span> library <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.00587/assets/structure.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="433" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S4.F2.2.1" class="ltx_text ltx_font_bold">The loss curve of different model structure with similar parameters.</span> We can see that combine double and single stream block is much more scalable and compute efficient way for music generation model.
</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.00587/assets/scale.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="436" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.2.1" class="ltx_text ltx_font_bold">The loss curve of different model parameters with same structure. </span> We can see that increase model parameters consistently improve the generative performance.
</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To assess the effectiveness of the proposed approaches and compare different strategies, we isolate and test only the specific components under consideration while keeping all other parts frozen. Ablation studies are performed on a subset of the training set, specifically utilizing the ASM and FMA datasets. For evaluation purposes, we employ an out-of-domain set comprising 1K samples randomly selected from the MTT dataset.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.4" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;"></th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">FAD<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">IS<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">CLAP<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<th id="S4.T2.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">DDIM</th>
<td id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">7.42</td>
<td id="S4.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">1.67</td>
<td id="S4.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">0.201</td>
</tr>
<tr id="S4.T2.3.3.5.2" class="ltx_tr">
<th id="S4.T2.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;">RF</th>
<td id="S4.T2.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;">5.89</td>
<td id="S4.T2.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;">2.43</td>
<td id="S4.T2.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;">0.312</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.5.1" class="ltx_text ltx_font_bold">Effect of rectified flow training in text-to-music generation.</span> We train small version of FluxMusic with different sampling schedule and results show the superiority of RF training with comparable computation burden. </figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.8" class="ltx_inline-block ltx_transformed_outer" style="width:499.0pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T3.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.8.8.9.1" class="ltx_tr">
<th id="S4.T3.8.8.9.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.8.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.8.8.9.1.2.1" class="ltx_text">Details</span></th>
<th id="S4.T3.8.8.9.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T3.8.8.9.1.3.1" class="ltx_text">MusicCaps</span></th>
<th id="S4.T3.8.8.9.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T3.8.8.9.1.4.1" class="ltx_text">Song Describer Dataset</span></th>
</tr>
<tr id="S4.T3.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.9" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T3.8.8.8.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.8.8.8.10.1" class="ltx_text">Params</span></th>
<th id="S4.T3.8.8.8.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.8.8.8.11.1" class="ltx_text">Hours</span></th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.1.1.1.1.1" class="ltx_text">FAD</span> <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.2.2.2.2.1" class="ltx_text">KL</span> <math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.3.3.3.3.1" class="ltx_text">IS</span> <math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.4.4.4.4.1" class="ltx_text">CLAP</span> <math id="S4.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T3.4.4.4.4.m1.1.1" xref="S4.T3.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.5.5.5.5.1" class="ltx_text">FAD</span> <math id="S4.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.5.m1.1.1" xref="S4.T3.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.6.6.6.6.1" class="ltx_text">KL</span> <math id="S4.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T3.6.6.6.6.m1.1.1" xref="S4.T3.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.m1.1b"><ci id="S4.T3.6.6.6.6.m1.1.1.cmml" xref="S4.T3.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.7.7.7.7.1" class="ltx_text">IS</span> <math id="S4.T3.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.7.7.7.7.m1.1a"><mo stretchy="false" id="S4.T3.7.7.7.7.m1.1.1" xref="S4.T3.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.7.m1.1b"><ci id="S4.T3.7.7.7.7.m1.1.1.cmml" xref="S4.T3.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T3.8.8.8.8.1" class="ltx_text">CLAP</span> <math id="S4.T3.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.8.8.8.8.m1.1a"><mo stretchy="false" id="S4.T3.8.8.8.8.m1.1.1" xref="S4.T3.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.8.m1.1b"><ci id="S4.T3.8.8.8.8.m1.1.1.cmml" xref="S4.T3.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.8.8.10.1" class="ltx_tr">
<th id="S4.T3.8.8.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MusicLM</th>
<td id="S4.T3.8.8.10.1.2" class="ltx_td ltx_align_center ltx_border_t">1290M</td>
<td id="S4.T3.8.8.10.1.3" class="ltx_td ltx_align_center ltx_border_t">280k</td>
<td id="S4.T3.8.8.10.1.4" class="ltx_td ltx_align_center ltx_border_t">4.00</td>
<td id="S4.T3.8.8.10.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.8.8.10.1.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.8.8.11.2" class="ltx_tr">
<th id="S4.T3.8.8.11.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MusicGen</th>
<td id="S4.T3.8.8.11.2.2" class="ltx_td ltx_align_center">1.5B</td>
<td id="S4.T3.8.8.11.2.3" class="ltx_td ltx_align_center">20k</td>
<td id="S4.T3.8.8.11.2.4" class="ltx_td ltx_align_center">3.80</td>
<td id="S4.T3.8.8.11.2.5" class="ltx_td ltx_align_center">1.22</td>
<td id="S4.T3.8.8.11.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.11.2.7" class="ltx_td ltx_align_center">0.31</td>
<td id="S4.T3.8.8.11.2.8" class="ltx_td ltx_align_center">5.38</td>
<td id="S4.T3.8.8.11.2.9" class="ltx_td ltx_align_center">1.01</td>
<td id="S4.T3.8.8.11.2.10" class="ltx_td ltx_align_center">1.92</td>
<td id="S4.T3.8.8.11.2.11" class="ltx_td ltx_align_center">0.18</td>
</tr>
<tr id="S4.T3.8.8.12.3" class="ltx_tr">
<th id="S4.T3.8.8.12.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mousai</th>
<td id="S4.T3.8.8.12.3.2" class="ltx_td ltx_align_center">1042M</td>
<td id="S4.T3.8.8.12.3.3" class="ltx_td ltx_align_center">2.5k</td>
<td id="S4.T3.8.8.12.3.4" class="ltx_td ltx_align_center">7.50</td>
<td id="S4.T3.8.8.12.3.5" class="ltx_td ltx_align_center">1.59</td>
<td id="S4.T3.8.8.12.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.12.3.7" class="ltx_td ltx_align_center">0.23</td>
<td id="S4.T3.8.8.12.3.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.12.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.12.3.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.12.3.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.8.8.13.4" class="ltx_tr">
<th id="S4.T3.8.8.13.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Jen-1</th>
<td id="S4.T3.8.8.13.4.2" class="ltx_td ltx_align_center">746M</td>
<td id="S4.T3.8.8.13.4.3" class="ltx_td ltx_align_center">5.0k</td>
<td id="S4.T3.8.8.13.4.4" class="ltx_td ltx_align_center">2.0</td>
<td id="S4.T3.8.8.13.4.5" class="ltx_td ltx_align_center">1.29</td>
<td id="S4.T3.8.8.13.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.13.4.7" class="ltx_td ltx_align_center">0.33</td>
<td id="S4.T3.8.8.13.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.13.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.13.4.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.13.4.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.8.8.14.5" class="ltx_tr">
<th id="S4.T3.8.8.14.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AudioLDM 2 (Full)</th>
<td id="S4.T3.8.8.14.5.2" class="ltx_td ltx_align_center">712M</td>
<td id="S4.T3.8.8.14.5.3" class="ltx_td ltx_align_center">17.9k</td>
<td id="S4.T3.8.8.14.5.4" class="ltx_td ltx_align_center">3.13</td>
<td id="S4.T3.8.8.14.5.5" class="ltx_td ltx_align_center">1.20</td>
<td id="S4.T3.8.8.14.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.14.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.14.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.14.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.14.5.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.8.8.14.5.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.8.8.15.6" class="ltx_tr">
<th id="S4.T3.8.8.15.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AudioLDM 2 (Music)</th>
<td id="S4.T3.8.8.15.6.2" class="ltx_td ltx_align_center">712M</td>
<td id="S4.T3.8.8.15.6.3" class="ltx_td ltx_align_center">10.8k</td>
<td id="S4.T3.8.8.15.6.4" class="ltx_td ltx_align_center">4.04</td>
<td id="S4.T3.8.8.15.6.5" class="ltx_td ltx_align_center">1.46</td>
<td id="S4.T3.8.8.15.6.6" class="ltx_td ltx_align_center">2.67</td>
<td id="S4.T3.8.8.15.6.7" class="ltx_td ltx_align_center">0.34</td>
<td id="S4.T3.8.8.15.6.8" class="ltx_td ltx_align_center">2.77</td>
<td id="S4.T3.8.8.15.6.9" class="ltx_td ltx_align_center">0.84</td>
<td id="S4.T3.8.8.15.6.10" class="ltx_td ltx_align_center">1.91</td>
<td id="S4.T3.8.8.15.6.11" class="ltx_td ltx_align_center">0.28</td>
</tr>
<tr id="S4.T3.8.8.16.7" class="ltx_tr">
<th id="S4.T3.8.8.16.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">QA-MDT (U-Net)</th>
<td id="S4.T3.8.8.16.7.2" class="ltx_td ltx_align_center">1.0B</td>
<td id="S4.T3.8.8.16.7.3" class="ltx_td ltx_align_center">12.5k</td>
<td id="S4.T3.8.8.16.7.4" class="ltx_td ltx_align_center">2.03</td>
<td id="S4.T3.8.8.16.7.5" class="ltx_td ltx_align_center">1.51</td>
<td id="S4.T3.8.8.16.7.6" class="ltx_td ltx_align_center">2.41</td>
<td id="S4.T3.8.8.16.7.7" class="ltx_td ltx_align_center">0.33</td>
<td id="S4.T3.8.8.16.7.8" class="ltx_td ltx_align_center">1.01</td>
<td id="S4.T3.8.8.16.7.9" class="ltx_td ltx_align_center">0.83</td>
<td id="S4.T3.8.8.16.7.10" class="ltx_td ltx_align_center">1.92</td>
<td id="S4.T3.8.8.16.7.11" class="ltx_td ltx_align_center">0.30</td>
</tr>
<tr id="S4.T3.8.8.17.8" class="ltx_tr">
<th id="S4.T3.8.8.17.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">QA-MDT (DiT)</th>
<td id="S4.T3.8.8.17.8.2" class="ltx_td ltx_align_center">675M</td>
<td id="S4.T3.8.8.17.8.3" class="ltx_td ltx_align_center">12.5k</td>
<td id="S4.T3.8.8.17.8.4" class="ltx_td ltx_align_center">1.65</td>
<td id="S4.T3.8.8.17.8.5" class="ltx_td ltx_align_center">1.31</td>
<td id="S4.T3.8.8.17.8.6" class="ltx_td ltx_align_center">2.80</td>
<td id="S4.T3.8.8.17.8.7" class="ltx_td ltx_align_center">0.35</td>
<td id="S4.T3.8.8.17.8.8" class="ltx_td ltx_align_center">1.04</td>
<td id="S4.T3.8.8.17.8.9" class="ltx_td ltx_align_center">0.83</td>
<td id="S4.T3.8.8.17.8.10" class="ltx_td ltx_align_center">1.94</td>
<td id="S4.T3.8.8.17.8.11" class="ltx_td ltx_align_center">0.32</td>
</tr>
<tr id="S4.T3.8.8.18.9" class="ltx_tr">
<th id="S4.T3.8.8.18.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">FluxMusic</th>
<td id="S4.T3.8.8.18.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.1B</td>
<td id="S4.T3.8.8.18.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">22K</td>
<td id="S4.T3.8.8.18.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1.43</td>
<td id="S4.T3.8.8.18.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1.25</td>
<td id="S4.T3.8.8.18.9.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.98</td>
<td id="S4.T3.8.8.18.9.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.36</td>
<td id="S4.T3.8.8.18.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1.01</td>
<td id="S4.T3.8.8.18.9.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.83</td>
<td id="S4.T3.8.8.18.9.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.03</td>
<td id="S4.T3.8.8.18.9.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.35</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.10.1" class="ltx_text ltx_font_bold">Evaluation results for text-to-music generation with diffusion-based models and language-based models.</span> We can see that with compettive parameters and training data, FluxMusic achieve best results in most metrics, demonstrating the promising of structure. </figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"></th>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2"><span id="S4.T4.1.1.1.2.1" class="ltx_text">Experts</span></td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" colspan="2"><span id="S4.T4.1.1.1.3.1" class="ltx_text">Beginners</span></td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.1.2.2.1.1" class="ltx_text">Model</span></th>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.1.2.2.2.1" class="ltx_text">OVL</span></td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.1.2.2.3.1" class="ltx_text">REL</span></td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.1.2.2.4.1" class="ltx_text">OVL</span></td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S4.T4.1.2.2.5.1" class="ltx_text">REL</span></td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<th id="S4.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Ground Truth</th>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">4.20</td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">4.15</td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">4.00</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">3.85</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<th id="S4.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">AudioLDM 2</th>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">2.55</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">2.45</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">3.12</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">3.74</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<th id="S4.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">MusicGen</th>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">3.13</td>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">3.34</td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">3.06</td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">3.70</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<th id="S4.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">FluxMusic</th>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">3.35</td>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">3.54</td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">3.25</td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">3.80</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.5.1" class="ltx_text ltx_font_bold">Evaluation results of text-to-music performances in human evaluation.</span> We denoted for text relevance (<span id="S4.T4.6.2" class="ltx_text">REL</span>) and overall quality (<span id="S4.T4.7.3" class="ltx_text">OVL</span>), with higher scores indicating better performance. </figcaption>
</figure>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Advantage of model architecture.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We examine the architectural design choices for the diffusion network within FluxMusic, focusing on two specific variants: (1) utilizing double stream blocks exclusively throughout the entire network, and (2) employing a combination of both double and single stream blocks. In particular, we use a small version of the model, with one variant comprising 15 double stream blocks, referred to as 15D_0S, and the other combining 8 double stream blocks with 16 single stream blocks, referred to as 8D_16S. These configurations result in parameter counts of approximately 145.5M and 142.3M, respectively.
As depicted in Figure <a href="#S4.F2" title="Figure 2 ‣ Evaluation metrics. ‣ 4.1 Experimental settings ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the combined double and single modality stream block architecture not only accelerates the training process but also enhances generative performance, despite maintaining a comparable parameter scale. Consequently, we designate the mixed structure as the default configuration.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of rectified flow.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Model Analysis ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents a comparative analysis of various training strategies employed in FluxMusic, including DDIM and rectified flow, using the small model version. Both strategy training with 128 batch size and 200K training steps to maintain an identical computation cost. As anticipated, and in line with prior research <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>, rectified flow training demonstrates a positive impact on generative performance within the music domain.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of model parameter scale.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We examine the scaling properties of the FluxMusic framework by analyzing the impact of model depth, defined by the number of double and single stream layers, and model width, characterized by the hidden size dimension. Specifically, we train four variants of FluxMusic using 10-second clips, with model configurations ranging from small to giant, as detailed in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Rectified Flow Trajectories ‣ 3 Methodology ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
As the loss cureve depicted in Figure <a href="#S4.F3" title="Figure 3 ‣ Evaluation metrics. ‣ 4.1 Experimental settings ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, performance improves as the depth of double:single modality block increases from 8:16 to 16:32, and similarly, expanding the width from 512 to 1408 results in further performance gains. It is important to note that the model’s performance has not yet fully converged, as training was conducted for only 200K steps. Nonetheless, across all configurations, substantial improvements are observed at all training stages as the depth and width of the FluxMusic architecture are increased.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Compared with Previous Methods</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We conducted a comparative analysis of our proposed MusicFlux method against several prominent prior text-to-music approaches, including AudioLDM 2 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, Mousai <cite class="ltx_cite ltx_citemacro_citep">(Schneider et al., <a href="#bib.bib77" title="" class="ltx_ref">2024</a>)</cite>, Jen-1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib52" title="" class="ltx_ref">2024c</a>)</cite>, and QA-MDT <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib50" title="" class="ltx_ref">2024a</a>)</cite>, which model music using spectral latent spaces, as well as MusicLM <cite class="ltx_cite ltx_citemacro_citep">(Agostinelli et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> and MusicGen <cite class="ltx_cite ltx_citemacro_citep">(Copet et al., <a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>, which employ discrete representations. All the results of these comparisons are summarized in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Model Analysis ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The experimental outcomes highlight the significant advantages of our FluxMusic models, which achieve state-of-the-art performance across multiple objective metrics. These findings underscore the scalability potential of the FluxMusic framework, particularly as model and dataset sizes consistently increase.
Although FluxMusic exhibited a slight advantage in FAD and KL metrics on the Song-Describer-Dataset, this may be attributed to instabilities stemming from the dataset’s limited size. Further, our superiority in text-to-music generation was corroborated through additional subjective evaluations.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.00587/assets/x2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="442" height="390" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S4.F4.2.1" class="ltx_text ltx_font_bold">Generated mel-spectrum cases of different training steps. </span> We plot small version of MusicFlux at every 50K training steps and we can find that the image becomes orderly and fine-grained instead of random and disorderly with the training continues.
</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.00587/assets/x3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="442" height="390" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.2.1" class="ltx_text ltx_font_bold">Generated mel-spectrum cases of different model parameters. </span> With model size increase, the resulting mel-spectrum becomes more content rich and rhythmically distinct.
</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.00587/assets/x4.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.2.1" class="ltx_text ltx_font_bold">Generated mel-spectrum cases of different classifier-free guidance.</span> We plot four clips with diverse textual prompts from small version of FluxMusic model and concatenate them in one figure. It can be seen that increasing the CFG number results in a more pronounced contrast in the generated mel-spectrum. To consistent with previous works, we set CFG=3.5 by default.
</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Human Evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We laso conducted a human evaluation, following settings outlined in <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib50" title="" class="ltx_ref">2024a</a>; Liu et al., <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, to assess the performance of text-to-music generation. This evaluation focused on two key aspects of the generated audio samples: (i) overall quality (OVL) and (ii) relevance to the textual input (REL).
For the overall quality assessment, human raters were asked to evaluate the perceptual quality of the samples on a scale from 1 to 5. Similarly, the text relevance test required raters to score the alignment between the audio and the corresponding text input, also on a 1 to 5 scale.
Our evaluation team comprised individuals from diverse backgrounds, including professional music producers and novices with little to no prior knowledge in this domain. These groups are categorized as experts and beginners. Each randomly selected audio sample was evaluated by at least ten raters to ensure robust results.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">As reflected in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Model Analysis ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our proposed FluxMusic method significantly enhances both the overall quality of the music and its alignment with the text input. These improvements can be attributed to the RF training strategy and the advanced architecture of our model. Notably, the feedback from experts indicates substantial gains, highlighting the model’s potential utility for professionals in the audio industry.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Visualization and Music Examples</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">For a more convenient understanding, we visualize some generated music clips towards different prompt, from different perspective. These visualizations encompass: (1) different training step, (2) model parameter at scale, (2) setting of classifier-free guidance (CFG) number, the results are presented in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Compared with Previous Methods ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <a href="#S4.F5" title="Figure 5 ‣ 4.3 Compared with Previous Methods ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and <a href="#S4.F6" title="Figure 6 ‣ 4.3 Compared with Previous Methods ‣ 4 Experiments ‣ FLUX that Plays Music" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, respectively. For more cases and listen intuitively, we recommand to visit the project webpage.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we explore an extension of the FLUX framework for text-to-music generation. Our model, FluxMusic, utilizes rectified flow transformers to predict mel-spectra iteratively within a latent VAE space. Experiments demonstrate the advanced performance comparable to existing benchmarks. Moreover, our study yields several notable findings: first, a simple rectified flow transformer performs effectively for audio spectrograms. Then, we identify the optimal strategy and learning approach through an ablation study. Future research will investigate scalability using a mixture-of-experts architecture and distillation techniques to enhance inference efficiency.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agostinelli et al. (2023)</span>
<span class="ltx_bibblock">
Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., et al.

</span>
<span class="ltx_bibblock">Musiclm: Generating music from text.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.11325</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albergo &amp; Vanden-Eijnden (2022)</span>
<span class="ltx_bibblock">
Albergo, M. S. and Vanden-Eijnden, E.

</span>
<span class="ltx_bibblock">Building normalizing flows with stochastic interpolants, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. (2023)</span>
<span class="ltx_bibblock">
Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J.

</span>
<span class="ltx_bibblock">All are worth words: A vit backbone for diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  22669–22679, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertin-Mahieux et al. (2011)</span>
<span class="ltx_bibblock">
Bertin-Mahieux, T., Ellis, D. P., Whitman, B., and Lamere, P.

</span>
<span class="ltx_bibblock">The million song dataset.

</span>
<span class="ltx_bibblock">2011.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Betker et al. (2023)</span>
<span class="ltx_bibblock">
Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al.

</span>
<span class="ltx_bibblock">Improving image generation with better captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf</em>, 2(3):8, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briot et al. (2017)</span>
<span class="ltx_bibblock">
Briot, J.-P., Hadjeres, G., and Pachet, F.-D.

</span>
<span class="ltx_bibblock">Deep learning techniques for music generation–a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1709.01620</em>, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z.

</span>
<span class="ltx_bibblock">Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z.

</span>
<span class="ltx_bibblock">Pixart-<math id="bib.bib9.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib9.1.m1.1a"><mo id="bib.bib9.1.m1.1.1" xref="bib.bib9.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.1.m1.1b"><ci id="bib.bib9.1.m1.1.1.cmml" xref="bib.bib9.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">\backslash</annotation></semantics></math>sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04692</em>, 2024a.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Chen, K., Wu, Y., Liu, H., Nezhurina, M., Berg-Kirkpatrick, T., and Dubnov, S.

</span>
<span class="ltx_bibblock">Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1206–1210. IEEE, 2024b.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2018)</span>
<span class="ltx_bibblock">
Chen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K.

</span>
<span class="ltx_bibblock">Neural ordinary differential equations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:49310446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:49310446</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Copet et al. (2024)</span>
<span class="ltx_bibblock">
Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Défossez, A.

</span>
<span class="ltx_bibblock">Simple and controllable music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Defferrard et al. (2016)</span>
<span class="ltx_bibblock">
Defferrard, M., Benzi, K., Vandergheynst, P., and Bresson, X.

</span>
<span class="ltx_bibblock">Fma: A dataset for music analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.01840</em>, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Défossez et al. (2022)</span>
<span class="ltx_bibblock">
Défossez, A., Copet, J., Synnaeve, G., and Adi, Y.

</span>
<span class="ltx_bibblock">High fidelity neural audio compression.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13438</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doh et al. (2023)</span>
<span class="ltx_bibblock">
Doh, S., Choi, K., Lee, J., and Nam, J.

</span>
<span class="ltx_bibblock">Lp-musiccaps: Llm-based pseudo music captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.16372</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doh et al. (2024)</span>
<span class="ltx_bibblock">
Doh, S., Lee, M., Jeong, D., and Nam, J.

</span>
<span class="ltx_bibblock">Enriching music descriptions with a finetuned-llm and metadata for text-to-music retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  826–830. IEEE, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2018)</span>
<span class="ltx_bibblock">
Dong, H.-W., Hsiao, W.-Y., Yang, L.-C., and Yang, Y.-H.

</span>
<span class="ltx_bibblock">Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 32, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DOSOVITSKIY (2020)</span>
<span class="ltx_bibblock">
DOSOVITSKIY, A.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elizalde et al. (2023)</span>
<span class="ltx_bibblock">
Elizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H.

</span>
<span class="ltx_bibblock">Clap learning audio concepts from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5. IEEE, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2021)</span>
<span class="ltx_bibblock">
Esser, P., Rombach, R., and Ommer, B.

</span>
<span class="ltx_bibblock">Taming transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  12873–12883, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2024)</span>
<span class="ltx_bibblock">
Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evans et al. (2024)</span>
<span class="ltx_bibblock">
Evans, Z., Parker, J. D., Carr, C., Zukowski, Z., Taylor, J., and Pons, J.

</span>
<span class="ltx_bibblock">Stable audio open.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.14358</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei (2021)</span>
<span class="ltx_bibblock">
Fei, Z.

</span>
<span class="ltx_bibblock">Partially non-autoregressive image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 35, pp.  1309–1316, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2022)</span>
<span class="ltx_bibblock">
Fei, Z., Yan, X., Wang, S., and Tian, Q.

</span>
<span class="ltx_bibblock">Deecap: Dynamic early exiting for efficient image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  12216–12226, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2023a)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., and Huang, J.

</span>
<span class="ltx_bibblock">Gradient-free textual inversion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>, pp.  1364–1373, 2023a.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2023b)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., and Huang, J.

</span>
<span class="ltx_bibblock">A-jepa: Joint-embedding predictive architecture can listen.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.15830</em>, 2023b.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2023c)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., Zhu, L., Huang, J., Wei, X., and Wei, X.

</span>
<span class="ltx_bibblock">Masked auto-encoders meet generative adversarial networks and beyond.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  24449–24459, 2023c.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2024a)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., and Huang, J.

</span>
<span class="ltx_bibblock">Music consistency models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.13358</em>, 2024a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2024b)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., Yu, C., and Huang, J.

</span>
<span class="ltx_bibblock">Scalable diffusion models with state space backbone.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05608</em>, 2024b.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2024c)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., Yu, C., Li, D., and Huang, J.

</span>
<span class="ltx_bibblock">Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.04478</em>, 2024c.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2024d)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., Yu, C., Li, D., and Huang, J.

</span>
<span class="ltx_bibblock">Scaling diffusion transformers to 16 billion parameters.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.11633</em>, 2024d.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al. (2024e)</span>
<span class="ltx_bibblock">
Fei, Z., Fan, M., Yu, C., Li, D., Zhang, Y., and Huang, J.

</span>
<span class="ltx_bibblock">Dimba: Transformer-mamba diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.01159</em>, 2024e.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei (2019)</span>
<span class="ltx_bibblock">
Fei, Z.-c.

</span>
<span class="ltx_bibblock">Fast image caption generation with position alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06365</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forsgren &amp; Martiros (2022)</span>
<span class="ltx_bibblock">
Forsgren, S. and Martiros, H.

</span>
<span class="ltx_bibblock">Riffusion-stable diffusion for real-time music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">URL https://riffusion. com</em>, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemmeke et al. (2017)</span>
<span class="ltx_bibblock">
Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M.

</span>
<span class="ltx_bibblock">Audio set: An ontology and human-labeled dataset for audio events.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, pp.  776–780. IEEE, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2021)</span>
<span class="ltx_bibblock">
Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y.

</span>
<span class="ltx_bibblock">Transformer in transformer.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2022)</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  16000–16009, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Ho, J., Jain, A., and Abbeel, P.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2022)</span>
<span class="ltx_bibblock">
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J.

</span>
<span class="ltx_bibblock">Video diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:8633–8646, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023a)</span>
<span class="ltx_bibblock">
Huang, J., Ren, Y., Huang, R., Yang, D., Ye, Z., Zhang, C., Liu, J., Yin, X., Ma, Z., and Zhao, Z.

</span>
<span class="ltx_bibblock">Make-an-audio 2: Temporal-enhanced text-to-audio generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18474</em>, 2023a.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023b)</span>
<span class="ltx_bibblock">
Huang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A., Chen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., et al.

</span>
<span class="ltx_bibblock">Noise2music: Text-conditioned music generation with diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.03917</em>, 2023b.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023c)</span>
<span class="ltx_bibblock">
Huang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M., Ye, Z., Liu, J., Yin, X., and Zhao, Z.

</span>
<span class="ltx_bibblock">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.  13916–13932. PMLR, 2023c.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2023)</span>
<span class="ltx_bibblock">
Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S.

</span>
<span class="ltx_bibblock">Analyzing and improving the training dynamics of diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.02696</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kilgour et al. (2018)</span>
<span class="ltx_bibblock">
Kilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M.

</span>
<span class="ltx_bibblock">Fr<math id="bib.bib44.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib44.1.m1.1a"><mo id="bib.bib44.1.m1.1.1" xref="bib.bib44.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib44.1.m1.1b"><ci id="bib.bib44.1.m1.1.1.cmml" xref="bib.bib44.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib44.1.m1.1c">\backslash</annotation></semantics></math>’echet audio distance: A metric for evaluating music enhancement algorithms.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.08466</em>, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma &amp; Gao (2024)</span>
<span class="ltx_bibblock">
Kingma, D. and Gao, R.

</span>
<span class="ltx_bibblock">Understanding diffusion objectives as the elbo with simple data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. (2020a)</span>
<span class="ltx_bibblock">
Kong, J., Kim, J., and Bae, J.

</span>
<span class="ltx_bibblock">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:17022–17033, 2020a.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. (2020b)</span>
<span class="ltx_bibblock">
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Diffwave: A versatile diffusion model for audio synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.09761</em>, 2020b.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al. (2024)</span>
<span class="ltx_bibblock">
Lam, M. W., Tian, Q., Li, T., Yin, Z., Feng, S., Tu, M., Ji, Y., Xia, R., Ma, M., Song, X., et al.

</span>
<span class="ltx_bibblock">Efficient neural music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Law et al. (2009)</span>
<span class="ltx_bibblock">
Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S.

</span>
<span class="ltx_bibblock">Evaluation of algorithms using games: The case of music tagging.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">ISMIR</em>, pp.  387–392. Citeseer, 2009.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Li, C., Wang, R., Liu, L., Du, J., Sun, Y., Guo, Z., Zhang, Z., and Jiang, Y.

</span>
<span class="ltx_bibblock">Quality-aware masked diffusion transformer for enhanced music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.15863</em>, 2024a.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Li, H., Zou, Y., Wang, Y., Majumder, O., Xie, Y., Manmatha, R., Swaminathan, A., Tu, Z., Ermon, S., and Soatto, S.

</span>
<span class="ltx_bibblock">On the scalability of diffusion-based text-to-image generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9400–9409, 2024b.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024c)</span>
<span class="ltx_bibblock">
Li, P. P., Chen, B., Yao, Y., Wang, Y., Wang, A., and Wang, A.

</span>
<span class="ltx_bibblock">Jen-1: Text-guided universal music generation with omnidirectional diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">2024 IEEE Conference on Artificial Intelligence (CAI)</em>, pp.  762–769. IEEE, 2024c.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., and Dai, J.

</span>
<span class="ltx_bibblock">Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers.

</span>
<span class="ltx_bibblock">2022a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
Li, Z., Wang, W., Xie, E., Yu, Z., Anandkumar, A., Alvarez, J. M., Luo, P., and Lu, T.

</span>
<span class="ltx_bibblock">Panoptic segformer: Delving deeper into panoptic segmentation with transformers.

</span>
<span class="ltx_bibblock">2022b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipman et al. (2023)</span>
<span class="ltx_bibblock">
Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=PqvMRDCJT9t" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=PqvMRDCJT9t</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Liu, H., Huang, R., Lin, X., Xu, W., Zheng, M., Chen, H., He, J., and Zhao, Z.

</span>
<span class="ltx_bibblock">Vit-tts: visual text-to-speech with scalable diffusion transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12708</em>, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Liu, H., Yuan, Y., Liu, X., Mei, X., Kong, Q., Tian, Q., Wang, Y., Wang, W., Wang, Y., and Plumbley, M. D.

</span>
<span class="ltx_bibblock">Audioldm 2: Learning holistic audio generation with self-supervised pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2024.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Liu, X., Gong, C., and Liu, Q.

</span>
<span class="ltx_bibblock">Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022a.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted windows.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., and Hu, H.

</span>
<span class="ltx_bibblock">Video swin transformer.

</span>
<span class="ltx_bibblock">2022b.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022a)</span>
<span class="ltx_bibblock">
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.

</span>
<span class="ltx_bibblock">Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:5775–5787, 2022a.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022b)</span>
<span class="ltx_bibblock">
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.

</span>
<span class="ltx_bibblock">Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.01095</em>, 2022b.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024a)</span>
<span class="ltx_bibblock">
Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., and Xie, S.

</span>
<span class="ltx_bibblock">Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.08740</em>, 2024a.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024b)</span>
<span class="ltx_bibblock">
Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C., and Qiao, Y.

</span>
<span class="ltx_bibblock">Latte: Latent diffusion transformer for video generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.03048</em>, 2024b.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manco et al. (2023)</span>
<span class="ltx_bibblock">
Manco, I., Weck, B., Doh, S., Won, M., Zhang, Y., Bodganov, D., Wu, Y., Chen, K., Tovstogan, P., Benetos, E., et al.

</span>
<span class="ltx_bibblock">The song describer dataset: a corpus of audio captions for music-and-language evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.10057</em>, 2023.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melechovsky et al. (2023)</span>
<span class="ltx_bibblock">
Melechovsky, J., Guo, Z., Ghosal, D., Majumder, N., Herremans, D., and Poria, S.

</span>
<span class="ltx_bibblock">Mustango: Toward controllable text-to-music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08355</em>, 2023.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et al. (2021)</span>
<span class="ltx_bibblock">
Mittal, G., Engel, J., Hawthorne, C., and Simon, I.

</span>
<span class="ltx_bibblock">Symbolic music generation with diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.16091</em>, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles &amp; Xie (2023)</span>
<span class="ltx_bibblock">
Peebles, W. and Xie, S.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. IEEE, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/iccv51070.2023.00387</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://dx.doi.org/10.1109/ICCV51070.2023.00387" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/ICCV51070.2023.00387</a>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plitsis et al. (2024)</span>
<span class="ltx_bibblock">
Plitsis, M., Kouzelis, T., Paraskevopoulos, G., Katsouros, V., and Panagakis, Y.

</span>
<span class="ltx_bibblock">Investigating personalization methods in text to music generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1081–1085. IEEE, 2024.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford (2018)</span>
<span class="ltx_bibblock">
Radford, A.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, 21(140):1–67, 2020.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. (2015)</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., and Brox, T.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>, pp.  234–241. Springer, 2015.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santana et al. (2020)</span>
<span class="ltx_bibblock">
Santana, I. A. P., Pinhelli, F., Donini, J., Catharin, L., Mangolin, R. B., Feltrim, V. D., Domingues, M. A., et al.

</span>
<span class="ltx_bibblock">Music4all: A new music database and its applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">2020 International Conference on Systems, Signals and Image Processing (IWSSIP)</em>, pp.  399–404. IEEE, 2020.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sauer et al. (2024)</span>
<span class="ltx_bibblock">
Sauer, A., Boesel, F., Dockhorn, T., Blattmann, A., Esser, P., and Rombach, R.

</span>
<span class="ltx_bibblock">Fast high-resolution image synthesis with latent adversarial diffusion distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.12015</em>, 2024.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneider et al. (2024)</span>
<span class="ltx_bibblock">
Schneider, F., Kamal, O., Jin, Z., and Schölkopf, B.

</span>
<span class="ltx_bibblock">Moûsai: Efficient text-to-music diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  8050–8068, 2024.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohl-Dickstein et al. (2015)</span>
<span class="ltx_bibblock">
Sohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan, N., and Ganguli, S.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium thermodynamics.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1503.03585, 2015.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:14888175" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:14888175</a>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song &amp; Dhariwal (2023)</span>
<span class="ltx_bibblock">
Song, Y. and Dhariwal, P.

</span>
<span class="ltx_bibblock">Improved techniques for training consistency models.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14189</em>, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song &amp; Ermon (2020)</span>
<span class="ltx_bibblock">
Song, Y. and Ermon, S.

</span>
<span class="ltx_bibblock">Generative modeling by estimating gradients of the data distribution, 2020.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2020)</span>
<span class="ltx_bibblock">
Song, Y., Sohl-Dickstein, J. N., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B.

</span>
<span class="ltx_bibblock">Score-based generative modeling through stochastic differential equations.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2011.13456, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:227209335" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:227209335</a>.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023)</span>
<span class="ltx_bibblock">
Song, Y., Dhariwal, P., Chen, M., and Sutskever, I.

</span>
<span class="ltx_bibblock">Consistency models.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.01469</em>, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strudel et al. (2021)</span>
<span class="ltx_bibblock">
Strudel, R., Garcia, R., Laptev, I., and Schmid, C.

</span>
<span class="ltx_bibblock">Segmenter: Transformer for semantic segmentation.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2020)</span>
<span class="ltx_bibblock">
Sun, P., Cao, J., Jiang, Y., Zhang, R., Xie, E., Yuan, Z., Wang, C., and Luo, P.

</span>
<span class="ltx_bibblock">Transtrack: Multiple object tracking with transformer.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2021)</span>
<span class="ltx_bibblock">
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H.

</span>
<span class="ltx_bibblock">Training data-efficient image transformers &amp; distillation through attention.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.

</span>
<span class="ltx_bibblock">Attention is all you need, 2017.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L.

</span>
<span class="ltx_bibblock">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L.

</span>
<span class="ltx_bibblock">Pvt v2: Improved baselines with pyramid vision transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Computational Visual Media</em>, 2022.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Wu, S.-L., Donahue, C., Watanabe, S., and Bryan, N. J.

</span>
<span class="ltx_bibblock">Music controlnet: Multiple time-varying controls for music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 32:2692–2703, 2024.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2021)</span>
<span class="ltx_bibblock">
Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P.

</span>
<span class="ltx_bibblock">Segformer: Simple and efficient design for semantic segmentation with transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:12077–12090, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2017)</span>
<span class="ltx_bibblock">
Yang, L.-C., Chou, S.-Y., and Yang, Y.-H.

</span>
<span class="ltx_bibblock">Midinet: A convolutional generative adversarial network for symbolic-domain music generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1703.10847</em>, 2017.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al.

</span>
<span class="ltx_bibblock">Cogvideox: Text-to-video diffusion models with an expert transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.06072</em>, 2024.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021)</span>
<span class="ltx_bibblock">
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F. E., Feng, J., and Yan, S.

</span>
<span class="ltx_bibblock">Tokens-to-token vit: Training vision transformers from scratch on imagenet.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al. (2021)</span>
<span class="ltx_bibblock">
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M.

</span>
<span class="ltx_bibblock">Soundstream: An end-to-end neural audio codec.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 30:495–507, 2021.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhang, L., Rao, A., and Agrawala, M.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  3836–3847, 2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2021)</span>
<span class="ltx_bibblock">
Zhao, H., Jiang, L., Jia, J., Torr, P. H., and Koltun, V.

</span>
<span class="ltx_bibblock">Point transformer.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2021)</span>
<span class="ltx_bibblock">
Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H., et al.

</span>
<span class="ltx_bibblock">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2021)</span>
<span class="ltx_bibblock">
Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., and Feng, J.

</span>
<span class="ltx_bibblock">Deepvit: Towards deeper vision transformer.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.00586" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.00587" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.00587">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.00587" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.00588" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:59:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
