<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.07914] Can Large Language Models Understand Spatial Audio?</title><meta property="og:description" content="This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs.
By leveraging LLMs' advanced cognitive and inferential abâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Can Large Language Models Understand Spatial Audio?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Can Large Language Models Understand Spatial Audio?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.07914">

<!--Generated on Fri Jul  5 23:54:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1,*]ChangliTang
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1,*]WenyiYu
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]GuangzhiSun
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=3]XianzhaoChen
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=3]TianTan
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=3]
<br class="ltx_break">WeiLi
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=3]JunZhang
<span id="p1.3.7" class="ltx_ERROR undefined">\name</span>[affiliation=3]LuLu
<span id="p1.3.8" class="ltx_ERROR undefined">\name</span>[affiliation=3]ZejunMa
<span id="p1.3.9" class="ltx_ERROR undefined">\name</span>[affiliation=3]YuxuanWang
<span id="p1.3.10" class="ltx_ERROR undefined">\name</span>[affiliation=1,â™£]ChaoZhang</p>
</div>
<h1 class="ltx_title ltx_title_document">Can Large Language Models Understand Spatial Audio?</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.2" class="ltx_p">This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs.
By leveraging LLMs' advanced cognitive and inferential abilities, the aim is to enhance understanding of 3D environments via audio.
We study 3 spatial audio tasks: sound source localization (SSL), far-field speech recognition (FSR), and localisation-informed speech extraction (LSE), achieving notable progress in each task.
For SSL, our approach achieves an MAE of <math id="id4.1.m1.1" class="ltx_Math" alttext="2.70^{\circ}" display="inline"><semantics id="id4.1.m1.1a"><msup id="id4.1.m1.1.1" xref="id4.1.m1.1.1.cmml"><mn id="id4.1.m1.1.1.2" xref="id4.1.m1.1.1.2.cmml">2.70</mn><mo id="id4.1.m1.1.1.3" xref="id4.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="id4.1.m1.1b"><apply id="id4.1.m1.1.1.cmml" xref="id4.1.m1.1.1"><csymbol cd="ambiguous" id="id4.1.m1.1.1.1.cmml" xref="id4.1.m1.1.1">superscript</csymbol><cn type="float" id="id4.1.m1.1.1.2.cmml" xref="id4.1.m1.1.1.2">2.70</cn><compose id="id4.1.m1.1.1.3.cmml" xref="id4.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.1.m1.1c">2.70^{\circ}</annotation></semantics></math> on the Spatial LibriSpeech dataset, substantially surpassing the prior benchmark of about <math id="id5.2.m2.1" class="ltx_Math" alttext="6.60^{\circ}" display="inline"><semantics id="id5.2.m2.1a"><msup id="id5.2.m2.1.1" xref="id5.2.m2.1.1.cmml"><mn id="id5.2.m2.1.1.2" xref="id5.2.m2.1.1.2.cmml">6.60</mn><mo id="id5.2.m2.1.1.3" xref="id5.2.m2.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="id5.2.m2.1b"><apply id="id5.2.m2.1.1.cmml" xref="id5.2.m2.1.1"><csymbol cd="ambiguous" id="id5.2.m2.1.1.1.cmml" xref="id5.2.m2.1.1">superscript</csymbol><cn type="float" id="id5.2.m2.1.1.2.cmml" xref="id5.2.m2.1.1.2">6.60</cn><compose id="id5.2.m2.1.1.3.cmml" xref="id5.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.2.m2.1c">6.60^{\circ}</annotation></semantics></math>.
Moreover, our model can employ spatial cues to improve FSR accuracy and execute LSE by selectively attending to sounds originating from a specified direction via text prompts, even amidst overlapping speech.
These findings highlight the potential of adapting LLMs to grasp physical audio concepts, paving the way for LLM-based agents in 3D environments.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Spatial audio, auditory large language models, sound source localisation, far-field speech recognition
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Empowering large language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> with multimodal perception abilities has emerged as a popular yet challenging research area nowadays. This burgeoning field of research emphasises the integration of LLMs with encoders capable of processing multimodal inputs, including image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>,
silent video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
The development of connection modules and LLM adaptors plays a pivotal role in aligning the encoder output spaces with the LLM input spaces. Such integrated multimodal LLMs are typically trained through cross-modal pre-training and instruction tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote1.1" class="ltx_sup">âˆ—</sup>Equal contribution</span></span></span><span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><sup id="footnote1a.1" class="ltx_sup">â™£</sup>Corresponding author</span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the success of multimodal LLMs in managing diverse non-spatial audio and visual tasks, they currently lack the ability to process spatial audio with precise localisation of the sound source in 3D space. In contrast, humans, endowed with binaural hearing, possess the ability to identify sounds, gauge their distance and direction, and selectively focus on sounds originating from a specific direction. This ability allows us to locate a specific person talking in the room and distinguish his/her location from any other speech or sound source. These abilities involve spatial auditory perception and reasoning, which is under-explored in auditory LLMs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To fill the gap in auditory abilities between humans and LLMs, this paper proposes a spatial audio perception approach in auditory LLMs for 3D spatial speech localisation and recognition. Four-channel spatial audio recorded in the first-order ambisonics (FOA) format is used in our experiments.
Instead of using speech and audio encoders dedicated to spatial audio processing, we simply use the Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> speech encoder to encode the semantic content in the omnidirectional signal while providing the intensity vectors (IV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> derived from the B-format audio as spatial information to the LLM. Moreover, a window-level Q-Former <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is used as a modality aligner to connect the Whisper encoder and the LLM.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.2" class="ltx_p">Three spatial speech tasks including 3D sound source localisation (SSL), far-field speech recognition (FSR) and localisation-informed speech extraction (LSE) are studied in this paper. The first two tasks are performed using the Spatial LibriSpeech dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which is a spatially augmented synthetic version of LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> with only one speech source in each sample. The performance of concatenating IV with audio features before and after Q-Former and expanding the vocabulary of the LLM by treating angles as special tokens were compared on the 3D SSL task. A mean angular error (MAE) of <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="2.70^{\circ}" display="inline"><semantics id="S1.p4.1.m1.1a"><msup id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">2.70</mn><mo id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1">superscript</csymbol><cn type="float" id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">2.70</cn><compose id="S1.p4.1.m1.1.1.3.cmml" xref="S1.p4.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">2.70^{\circ}</annotation></semantics></math> can be achieved by concatenating IV with the Whisper encoder output vectors before the Q-Former, whereas the MAE is about <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="90^{\circ}" display="inline"><semantics id="S1.p4.2.m2.1a"><msup id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml"><mn id="S1.p4.2.m2.1.1.2" xref="S1.p4.2.m2.1.1.2.cmml">90</mn><mo id="S1.p4.2.m2.1.1.3" xref="S1.p4.2.m2.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S1.p4.2.m2.1.1.2.cmml" xref="S1.p4.2.m2.1.1.2">90</cn><compose id="S1.p4.2.m2.1.1.3.cmml" xref="S1.p4.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">90^{\circ}</annotation></semantics></math> if no spatial information is fed into the LLM. Regarding FSR, our model demonstrated the ability to reduce word error rates (WERs) with additional spatial information. Regarding LSE, Soundspaces 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is used to simulate audio samples with two (possibly overlapped) sound sources from different locations based on LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and the dataset is called Dual-Source Spatial (DSS) LibriSpeech. Results show that our model can extract the target speech from the specified direction successfully across a wide range of overlapping ratios.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>On Auditory LLMs</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Several studies have attempted to extend LLMs to support direct speech inputs with a connection module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. When LLM-based speech synthesis is also considered, the LLM output space can be augmented with speech tokens as well, such as AudioPaLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Unlike speech, audio event inputs are often treated as fixed-sized spectrogram images that can be processed using visual-language LLM methods without explicitly modelling temporal correlations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Moreover, audio-LLMs have been extended to process speech and audio together in an end-to-end fashion for generic hearing abilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
A contemporary paper to this paper proposes using auditory LLM for question-answering-based SSL for audio events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
However, none of these models can perform SSL based on spatial audio or hear selectively based on the location of the sound source, highlighting the novelty and necessity of this paper.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>On 3D SSL</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For non-speech audio events, the precise localisation and interpretation of sound sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> are also important. Researchers have innovated with acoustic simulation techniques and developed algorithms that exploit spatial audio's potential <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Specifically, the sound event localisation and detection (SELD) task was introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which was adopted by the Detection and Classification of Acoustic Scenes and Events (DCASE) community where a range of approaches was designed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>On Spatial FSR</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Spatial information is crucial for many far-field speech processing tasks, such as source separation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, speaker diarisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and overlapped speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Meanwhile, efforts have been put into augmenting spatial speech with location annotations via simulation. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposed a simulation of overlapped spatial speech datasets for continuous speech separation under a simplified setting with fixed speaker locations. On the other hand, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposed a simulation of varying locations of single speakers with noise interference.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Structure</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our model architecture is shown in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3.1 Model Structure â€£ 3 Methods â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which contains a Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> encoder, a modality aligner and an LLM with the LoRA module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. The model accepts spatial audio as input and gives the corresponding textual responses based on textual prompts. Audio recorded in the first-order ambisonics (FOA) format is considered here and the mono-channel signal from the omnidirectional microphone is used as input to the Whisper encoder. To compensate for the lack of spatial information in the omnidirectional signal, intensity vectors (IVs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> calculated from the B-format audio are provided for the LLM. Specifically, the IVs can be introduced after either the Whisper encoder or the modality aligner.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">Let <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ğ—</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathbf{X}</annotation></semantics></math> be the input mono-channel omnidirectional signal and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{I}</annotation></semantics></math> be the intensity vectors. With the Whisper encoder <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\text{Enc}(\cdot)" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.2" xref="S3.SS1.p2.3.m3.1.2.cmml"><mtext id="S3.SS1.p2.3.m3.1.2.2" xref="S3.SS1.p2.3.m3.1.2.2a.cmml">Enc</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.2.1" xref="S3.SS1.p2.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p2.3.m3.1.2.3.2" xref="S3.SS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.1.2.3.2.1" xref="S3.SS1.p2.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS1.p2.3.m3.1.2.3.2.2" xref="S3.SS1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.2.cmml" xref="S3.SS1.p2.3.m3.1.2"><times id="S3.SS1.p2.3.m3.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.2.1"></times><ci id="S3.SS1.p2.3.m3.1.2.2a.cmml" xref="S3.SS1.p2.3.m3.1.2.2"><mtext id="S3.SS1.p2.3.m3.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.2.2">Enc</mtext></ci><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\text{Enc}(\cdot)</annotation></semantics></math>, the hidden auditory feature <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{Z}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">ğ™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathbf{Z}</annotation></semantics></math> can be obtained as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathbf{Z}=\text{Enc}(\mathbf{X})." display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">ğ™</mi><mo id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mtext id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2a.cmml">Enc</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.1" xref="S3.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğ—</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.2" xref="S3.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"></eq><ci id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2">ğ™</ci><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><times id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1"></times><ci id="S3.E1.m1.2.2.1.1.3.2a.cmml" xref="S3.E1.m1.2.2.1.1.3.2"><mtext id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">Enc</mtext></ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathbf{Z}=\text{Enc}(\mathbf{X}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.9" class="ltx_p">The ``Before'' option introduces the IVs <math id="S3.SS1.p2.5.m1.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S3.SS1.p2.5.m1.1a"><mi id="S3.SS1.p2.5.m1.1.1" xref="S3.SS1.p2.5.m1.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m1.1b"><ci id="S3.SS1.p2.5.m1.1.1.cmml" xref="S3.SS1.p2.5.m1.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m1.1c">\mathbf{I}</annotation></semantics></math> before the modality aligner. By controlling the frame rates of <math id="S3.SS1.p2.6.m2.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S3.SS1.p2.6.m2.1a"><mi id="S3.SS1.p2.6.m2.1.1" xref="S3.SS1.p2.6.m2.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m2.1b"><ci id="S3.SS1.p2.6.m2.1.1.cmml" xref="S3.SS1.p2.6.m2.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m2.1c">\mathbf{I}</annotation></semantics></math> and <math id="S3.SS1.p2.7.m3.1" class="ltx_Math" alttext="\mathbf{Z}" display="inline"><semantics id="S3.SS1.p2.7.m3.1a"><mi id="S3.SS1.p2.7.m3.1.1" xref="S3.SS1.p2.7.m3.1.1.cmml">ğ™</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m3.1b"><ci id="S3.SS1.p2.7.m3.1.1.cmml" xref="S3.SS1.p2.7.m3.1.1">ğ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m3.1c">\mathbf{Z}</annotation></semantics></math> to be the same, the two features can be concatenated frame by frame. The obtained feature <math id="S3.SS1.p2.8.m4.1" class="ltx_Math" alttext="\mathbf{Z^{\prime}}" display="inline"><semantics id="S3.SS1.p2.8.m4.1a"><msup id="S3.SS1.p2.8.m4.1.1" xref="S3.SS1.p2.8.m4.1.1.cmml"><mi id="S3.SS1.p2.8.m4.1.1.2" xref="S3.SS1.p2.8.m4.1.1.2.cmml">ğ™</mi><mo id="S3.SS1.p2.8.m4.1.1.3" xref="S3.SS1.p2.8.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m4.1b"><apply id="S3.SS1.p2.8.m4.1.1.cmml" xref="S3.SS1.p2.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m4.1.1.1.cmml" xref="S3.SS1.p2.8.m4.1.1">superscript</csymbol><ci id="S3.SS1.p2.8.m4.1.1.2.cmml" xref="S3.SS1.p2.8.m4.1.1.2">ğ™</ci><ci id="S3.SS1.p2.8.m4.1.1.3.cmml" xref="S3.SS1.p2.8.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m4.1c">\mathbf{Z^{\prime}}</annotation></semantics></math> will then be fed into the modality aligner and get hidden feature <math id="S3.SS1.p2.9.m5.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS1.p2.9.m5.1a"><mi id="S3.SS1.p2.9.m5.1.1" xref="S3.SS1.p2.9.m5.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m5.1b"><ci id="S3.SS1.p2.9.m5.1.1.cmml" xref="S3.SS1.p2.9.m5.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m5.1c">\mathbf{H}</annotation></semantics></math>, as shown in Eqn.Â (<a href="#S3.E2" title="In 3.1 Model Structure â€£ 3 Methods â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and (<a href="#S3.E3" title="In 3.1 Model Structure â€£ 3 Methods â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>):</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="\begin{cases}\mathbf{Z^{\prime}}=\text{Concat}(\mathbf{Z},\mathbf{I}),&amp;\text{``Before''}\\
\mathbf{Z^{\prime}}=\mathbf{Z},&amp;\text{otherwise}\end{cases}," display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.6.2"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.5.5.1.cmml"><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.5.5.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E2.m1.4.4.4" xref="S3.E2.m1.5.5.1.cmml"><mtr id="S3.E2.m1.4.4.4a" xref="S3.E2.m1.5.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4b" xref="S3.E2.m1.5.5.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml"><msup id="S3.E2.m1.1.1.1.1.1.1.3.1.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2.2.cmml">ğ™</mi><mo id="S3.E2.m1.1.1.1.1.1.1.3.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2.3.cmml">â€²</mo></msup><mo id="S3.E2.m1.1.1.1.1.1.1.3.1.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.3.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.3.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.2a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.3.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.1.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.2.1" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">ğ™</mi><mo id="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4c" xref="S3.E2.m1.5.5.1.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.1a.cmml">``Before''</mtext></mtd></mtr><mtr id="S3.E2.m1.4.4.4d" xref="S3.E2.m1.5.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4e" xref="S3.E2.m1.5.5.1.cmml"><mrow id="S3.E2.m1.3.3.3.3.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.3.3.1.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.cmml"><msup id="S3.E2.m1.3.3.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.3.3.1.1.1.1.2.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.2.cmml">ğ™</mi><mo id="S3.E2.m1.3.3.3.3.1.1.1.1.2.3" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.E2.m1.3.3.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1.cmml">=</mo><mi id="S3.E2.m1.3.3.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.3.3.1.1.1.1.3.cmml">ğ™</mi></mrow><mo id="S3.E2.m1.3.3.3.3.1.1.1.2" xref="S3.E2.m1.3.3.3.3.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4f" xref="S3.E2.m1.5.5.1.cmml"><mtext id="S3.E2.m1.4.4.4.4.2.1" xref="S3.E2.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow><mo id="S3.E2.m1.5.6.2.1">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.4.4"><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.4.4.5">cases</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><eq id="S3.E2.m1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2.2">ğ™</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.2.3">â€²</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.3.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3.1.3.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.2"><mtext id="S3.E2.m1.1.1.1.1.1.1.3.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.2">Concat</mtext></ci><interval closure="open" id="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.1.3.3.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">ğ™</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">ğˆ</ci></interval></apply></apply><ci id="S3.E2.m1.2.2.2.2.2.1a.cmml" xref="S3.E2.m1.2.2.2.2.2.1"><mtext id="S3.E2.m1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1">``Before''</mtext></ci><apply id="S3.E2.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1"><eq id="S3.E2.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.1"></eq><apply id="S3.E2.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2">superscript</csymbol><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.2">ğ™</ci><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.2.3">â€²</ci></apply><ci id="S3.E2.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.1.3">ğ™</ci></apply><ci id="S3.E2.m1.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.4.4.4.4.2.1"><mtext id="S3.E2.m1.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\begin{cases}\mathbf{Z^{\prime}}=\text{Concat}(\mathbf{Z},\mathbf{I}),&amp;\text{``Before''}\\
\mathbf{Z^{\prime}}=\mathbf{Z},&amp;\text{otherwise}\end{cases},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\mathbf{H}=\text{Aligner}(\mathbf{Z^{\prime}})." display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">ğ‡</mi><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3a.cmml">Aligner</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">ğ™</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">ğ‡</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">Aligner</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">ğ™</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathbf{H}=\text{Aligner}(\mathbf{Z^{\prime}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.17" class="ltx_p">The ``After'' option concatenates the IVs to <math id="S3.SS1.p2.10.m1.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS1.p2.10.m1.1a"><mi id="S3.SS1.p2.10.m1.1.1" xref="S3.SS1.p2.10.m1.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m1.1b"><ci id="S3.SS1.p2.10.m1.1.1.cmml" xref="S3.SS1.p2.10.m1.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m1.1c">\mathbf{H}</annotation></semantics></math> after the modality aligner.
Since the frame rate of <math id="S3.SS1.p2.11.m2.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S3.SS1.p2.11.m2.1a"><mi id="S3.SS1.p2.11.m2.1.1" xref="S3.SS1.p2.11.m2.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m2.1b"><ci id="S3.SS1.p2.11.m2.1.1.cmml" xref="S3.SS1.p2.11.m2.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m2.1c">\mathbf{I}</annotation></semantics></math> does not match that of <math id="S3.SS1.p2.12.m3.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS1.p2.12.m3.1a"><mi id="S3.SS1.p2.12.m3.1.1" xref="S3.SS1.p2.12.m3.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m3.1b"><ci id="S3.SS1.p2.12.m3.1.1.cmml" xref="S3.SS1.p2.12.m3.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m3.1c">\mathbf{H}</annotation></semantics></math>, the interpolation method is employed here to keep their frame rate the same. Let <math id="S3.SS1.p2.13.m4.1" class="ltx_Math" alttext="\mathbf{I^{\prime}}" display="inline"><semantics id="S3.SS1.p2.13.m4.1a"><msup id="S3.SS1.p2.13.m4.1.1" xref="S3.SS1.p2.13.m4.1.1.cmml"><mi id="S3.SS1.p2.13.m4.1.1.2" xref="S3.SS1.p2.13.m4.1.1.2.cmml">ğˆ</mi><mo id="S3.SS1.p2.13.m4.1.1.3" xref="S3.SS1.p2.13.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m4.1b"><apply id="S3.SS1.p2.13.m4.1.1.cmml" xref="S3.SS1.p2.13.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m4.1.1.1.cmml" xref="S3.SS1.p2.13.m4.1.1">superscript</csymbol><ci id="S3.SS1.p2.13.m4.1.1.2.cmml" xref="S3.SS1.p2.13.m4.1.1.2">ğˆ</ci><ci id="S3.SS1.p2.13.m4.1.1.3.cmml" xref="S3.SS1.p2.13.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m4.1c">\mathbf{I^{\prime}}</annotation></semantics></math> be the vectors interpolated from <math id="S3.SS1.p2.14.m5.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S3.SS1.p2.14.m5.1a"><mi id="S3.SS1.p2.14.m5.1.1" xref="S3.SS1.p2.14.m5.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m5.1b"><ci id="S3.SS1.p2.14.m5.1.1.cmml" xref="S3.SS1.p2.14.m5.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m5.1c">\mathbf{I}</annotation></semantics></math>.
After concatenating <math id="S3.SS1.p2.15.m6.1" class="ltx_Math" alttext="\mathbf{I^{\prime}}" display="inline"><semantics id="S3.SS1.p2.15.m6.1a"><msup id="S3.SS1.p2.15.m6.1.1" xref="S3.SS1.p2.15.m6.1.1.cmml"><mi id="S3.SS1.p2.15.m6.1.1.2" xref="S3.SS1.p2.15.m6.1.1.2.cmml">ğˆ</mi><mo id="S3.SS1.p2.15.m6.1.1.3" xref="S3.SS1.p2.15.m6.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m6.1b"><apply id="S3.SS1.p2.15.m6.1.1.cmml" xref="S3.SS1.p2.15.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m6.1.1.1.cmml" xref="S3.SS1.p2.15.m6.1.1">superscript</csymbol><ci id="S3.SS1.p2.15.m6.1.1.2.cmml" xref="S3.SS1.p2.15.m6.1.1.2">ğˆ</ci><ci id="S3.SS1.p2.15.m6.1.1.3.cmml" xref="S3.SS1.p2.15.m6.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m6.1c">\mathbf{I^{\prime}}</annotation></semantics></math> to <math id="S3.SS1.p2.16.m7.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS1.p2.16.m7.1a"><mi id="S3.SS1.p2.16.m7.1.1" xref="S3.SS1.p2.16.m7.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m7.1b"><ci id="S3.SS1.p2.16.m7.1.1.cmml" xref="S3.SS1.p2.16.m7.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m7.1c">\mathbf{H}</annotation></semantics></math> there is a linear mapping to obtain text-like tokens <math id="S3.SS1.p2.17.m8.1" class="ltx_Math" alttext="\mathbf{H^{\prime}}" display="inline"><semantics id="S3.SS1.p2.17.m8.1a"><msup id="S3.SS1.p2.17.m8.1.1" xref="S3.SS1.p2.17.m8.1.1.cmml"><mi id="S3.SS1.p2.17.m8.1.1.2" xref="S3.SS1.p2.17.m8.1.1.2.cmml">ğ‡</mi><mo id="S3.SS1.p2.17.m8.1.1.3" xref="S3.SS1.p2.17.m8.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m8.1b"><apply id="S3.SS1.p2.17.m8.1.1.cmml" xref="S3.SS1.p2.17.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m8.1.1.1.cmml" xref="S3.SS1.p2.17.m8.1.1">superscript</csymbol><ci id="S3.SS1.p2.17.m8.1.1.2.cmml" xref="S3.SS1.p2.17.m8.1.1.2">ğ‡</ci><ci id="S3.SS1.p2.17.m8.1.1.3.cmml" xref="S3.SS1.p2.17.m8.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m8.1c">\mathbf{H^{\prime}}</annotation></semantics></math>, which are then fed into the LLM, as Eqn.Â (<a href="#S3.E4" title="In 3.1 Model Structure â€£ 3 Methods â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) shows.</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.5" class="ltx_Math" alttext="\begin{cases}\mathbf{H^{\prime}}=\text{Linear}(\text{Concat}(\mathbf{H},\mathbf{I^{\prime}})),&amp;\text{``After''}\\
\mathbf{H^{\prime}}=\text{Linear}(\mathbf{H})&amp;\text{otherwise}\end{cases}." display="block"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.6.2"><mrow id="S3.E4.m1.4.4" xref="S3.E4.m1.5.5.1.cmml"><mo id="S3.E4.m1.4.4.5" xref="S3.E4.m1.5.5.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E4.m1.4.4.4" xref="S3.E4.m1.5.5.1.cmml"><mtr id="S3.E4.m1.4.4.4a" xref="S3.E4.m1.5.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4b" xref="S3.E4.m1.5.5.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml"><msup id="S3.E4.m1.1.1.1.1.1.1.2.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3.2.cmml">ğ‡</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3.3.cmml">â€²</mo></msup><mo id="S3.E4.m1.1.1.1.1.1.1.2.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.cmml"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.3a.cmml">Linear</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">ğ‡</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml">ğˆ</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4c" xref="S3.E4.m1.5.5.1.cmml"><mtext id="S3.E4.m1.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.1a.cmml">``After''</mtext></mtd></mtr><mtr id="S3.E4.m1.4.4.4d" xref="S3.E4.m1.5.5.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4e" xref="S3.E4.m1.5.5.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1" xref="S3.E4.m1.3.3.3.3.1.1.cmml"><msup id="S3.E4.m1.3.3.3.3.1.1.3" xref="S3.E4.m1.3.3.3.3.1.1.3.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.3.2" xref="S3.E4.m1.3.3.3.3.1.1.3.2.cmml">ğ‡</mi><mo id="S3.E4.m1.3.3.3.3.1.1.3.3" xref="S3.E4.m1.3.3.3.3.1.1.3.3.cmml">â€²</mo></msup><mo id="S3.E4.m1.3.3.3.3.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.3.3.1.1.4" xref="S3.E4.m1.3.3.3.3.1.1.4.cmml"><mtext id="S3.E4.m1.3.3.3.3.1.1.4.2" xref="S3.E4.m1.3.3.3.3.1.1.4.2a.cmml">Linear</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.3.1.1.4.1" xref="S3.E4.m1.3.3.3.3.1.1.4.1.cmml">â€‹</mo><mrow id="S3.E4.m1.3.3.3.3.1.1.4.3.2" xref="S3.E4.m1.3.3.3.3.1.1.4.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.3.3.1.1.4.3.2.1" xref="S3.E4.m1.3.3.3.3.1.1.4.cmml">(</mo><mi id="S3.E4.m1.3.3.3.3.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.cmml">ğ‡</mi><mo stretchy="false" id="S3.E4.m1.3.3.3.3.1.1.4.3.2.2" xref="S3.E4.m1.3.3.3.3.1.1.4.cmml">)</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.4.4.4f" xref="S3.E4.m1.5.5.1.cmml"><mtext id="S3.E4.m1.4.4.4.4.2.1" xref="S3.E4.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow><mo lspace="0em" id="S3.E4.m1.5.6.2.1">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.1.cmml" xref="S3.E4.m1.4.4"><csymbol cd="latexml" id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.4.4.5">cases</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"><eq id="S3.E4.m1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3.2">ğ‡</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.3.3">â€²</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.1.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.3"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.3">Linear</mtext></ci><apply id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3"><mtext id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.3">Concat</mtext></ci><interval closure="open" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1"><ci id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">ğ‡</ci><apply id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2">ğˆ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3">â€²</ci></apply></interval></apply></apply></apply><ci id="S3.E4.m1.2.2.2.2.2.1a.cmml" xref="S3.E4.m1.2.2.2.2.2.1"><mtext id="S3.E4.m1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1">``After''</mtext></ci><apply id="S3.E4.m1.3.3.3.3.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1"><eq id="S3.E4.m1.3.3.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.2"></eq><apply id="S3.E4.m1.3.3.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.3">superscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.3.2">ğ‡</ci><ci id="S3.E4.m1.3.3.3.3.1.1.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.3.3">â€²</ci></apply><apply id="S3.E4.m1.3.3.3.3.1.1.4.cmml" xref="S3.E4.m1.3.3.3.3.1.1.4"><times id="S3.E4.m1.3.3.3.3.1.1.4.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.4.1"></times><ci id="S3.E4.m1.3.3.3.3.1.1.4.2a.cmml" xref="S3.E4.m1.3.3.3.3.1.1.4.2"><mtext id="S3.E4.m1.3.3.3.3.1.1.4.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.4.2">Linear</mtext></ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1">ğ‡</ci></apply></apply><ci id="S3.E4.m1.4.4.4.4.2.1a.cmml" xref="S3.E4.m1.4.4.4.4.2.1"><mtext id="S3.E4.m1.4.4.4.4.2.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">\begin{cases}\mathbf{H^{\prime}}=\text{Linear}(\text{Concat}(\mathbf{H},\mathbf{I^{\prime}})),&amp;\text{``After''}\\
\mathbf{H^{\prime}}=\text{Linear}(\mathbf{H})&amp;\text{otherwise}\end{cases}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">Given a text prompt <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathbf{P}</annotation></semantics></math>, the final output textual response <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{\hat{Y}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mover accent="true" id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">ğ˜</mi><mo id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><ci id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1">^</ci><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathbf{\hat{Y}}</annotation></semantics></math> is generated by the LLM based on <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{H^{\prime}}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msup id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">ğ‡</mi><mo id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">ğ‡</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\mathbf{H^{\prime}}</annotation></semantics></math>:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\mathbf{\hat{Y}}=\arg\max\nolimits_{\mathbf{Y}}P(\mathbf{Y}|\mathbf{P},\mathbf{H^{\prime}})." display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.3.2" xref="S3.E5.m1.2.2.1.1.3.2.cmml">ğ˜</mi><mo id="S3.E5.m1.2.2.1.1.3.1" xref="S3.E5.m1.2.2.1.1.3.1.cmml">^</mo></mover><mo id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E5.m1.2.2.1.1.1.3a" xref="S3.E5.m1.2.2.1.1.1.3.cmml">â¡</mo><mrow id="S3.E5.m1.2.2.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.3.2.cmml"><msub id="S3.E5.m1.2.2.1.1.1.3.2.1" xref="S3.E5.m1.2.2.1.1.1.3.2.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.3.2.1.2" xref="S3.E5.m1.2.2.1.1.1.3.2.1.2.cmml">max</mi><mi id="S3.E5.m1.2.2.1.1.1.3.2.1.3" xref="S3.E5.m1.2.2.1.1.1.3.2.1.3.cmml">ğ˜</mi></msub><mo lspace="0.167em" id="S3.E5.m1.2.2.1.1.1.3.2a" xref="S3.E5.m1.2.2.1.1.1.3.2.cmml">â¡</mo><mi id="S3.E5.m1.2.2.1.1.1.3.2.2" xref="S3.E5.m1.2.2.1.1.1.3.2.2.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml">ğ˜</mi><mo fence="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">ğ</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">ğ‡</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">â€²</mo></msup></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><apply id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"><ci id="S3.E5.m1.2.2.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3.1">^</ci><ci id="S3.E5.m1.2.2.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2">ğ˜</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"></times><apply id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3"><arg id="S3.E5.m1.2.2.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1"></arg><apply id="S3.E5.m1.2.2.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2"><apply id="S3.E5.m1.2.2.1.1.1.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.3.2.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2.1">subscript</csymbol><max id="S3.E5.m1.2.2.1.1.1.3.2.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2.1.2"></max><ci id="S3.E5.m1.2.2.1.1.1.3.2.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2.1.3">ğ˜</ci></apply><ci id="S3.E5.m1.2.2.1.1.1.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2.2">ğ‘ƒ</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2">conditional</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3">ğ˜</ci><list id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2">ğ‡</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3">â€²</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\mathbf{\hat{Y}}=\arg\max\nolimits_{\mathbf{Y}}P(\mathbf{Y}|\mathbf{P},\mathbf{H^{\prime}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.5" class="ltx_p">The cross-entropy loss is computed based on <math id="S3.SS1.p3.5.m1.2" class="ltx_Math" alttext="P(\mathbf{Y}|\mathbf{P},\mathbf{H^{\prime}})" display="inline"><semantics id="S3.SS1.p3.5.m1.2a"><mrow id="S3.SS1.p3.5.m1.2.2" xref="S3.SS1.p3.5.m1.2.2.cmml"><mi id="S3.SS1.p3.5.m1.2.2.3" xref="S3.SS1.p3.5.m1.2.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m1.2.2.2" xref="S3.SS1.p3.5.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.p3.5.m1.2.2.1.1" xref="S3.SS1.p3.5.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.5.m1.2.2.1.1.2" xref="S3.SS1.p3.5.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.5.m1.2.2.1.1.1" xref="S3.SS1.p3.5.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.5.m1.2.2.1.1.1.3" xref="S3.SS1.p3.5.m1.2.2.1.1.1.3.cmml">ğ˜</mi><mo fence="false" id="S3.SS1.p3.5.m1.2.2.1.1.1.2" xref="S3.SS1.p3.5.m1.2.2.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.2.cmml"><mi id="S3.SS1.p3.5.m1.1.1" xref="S3.SS1.p3.5.m1.1.1.cmml">ğ</mi><mo id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.2" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.2.cmml">,</mo><msup id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.2.cmml">ğ‡</mi><mo id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.3.cmml">â€²</mo></msup></mrow></mrow><mo stretchy="false" id="S3.SS1.p3.5.m1.2.2.1.1.3" xref="S3.SS1.p3.5.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m1.2b"><apply id="S3.SS1.p3.5.m1.2.2.cmml" xref="S3.SS1.p3.5.m1.2.2"><times id="S3.SS1.p3.5.m1.2.2.2.cmml" xref="S3.SS1.p3.5.m1.2.2.2"></times><ci id="S3.SS1.p3.5.m1.2.2.3.cmml" xref="S3.SS1.p3.5.m1.2.2.3">ğ‘ƒ</ci><apply id="S3.SS1.p3.5.m1.2.2.1.1.1.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p3.5.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.2">conditional</csymbol><ci id="S3.SS1.p3.5.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.3">ğ˜</ci><list id="S3.SS1.p3.5.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1"><ci id="S3.SS1.p3.5.m1.1.1.cmml" xref="S3.SS1.p3.5.m1.1.1">ğ</ci><apply id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.2">ğ‡</ci><ci id="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.5.m1.2.2.1.1.1.1.1.1.3">â€²</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m1.2c">P(\mathbf{Y}|\mathbf{P},\mathbf{H^{\prime}})</annotation></semantics></math> during training. Both the Whisper encoder and the LLM are frozen. Only the modality aligner and the LoRA structure are updated.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.11" class="ltx_p">Since the angle is always a number, except tokenising the angles with the tokeniser of the LLM, an optional idea is to expand the vocabulary of the LLM and treat the numbers as special text tokens. Suppose <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">n</annotation></semantics></math> and <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">d</annotation></semantics></math> are the original size and text embedding dimension of the vocabulary and <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">m</annotation></semantics></math> is the number of special tokens to be added.
Let the original vocabulary embedding of the LLM be <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{V}\in\mathcal{R}^{n\times d}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mrow id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">ğ•</mi><mo id="S3.SS1.p4.4.m4.1.1.1" xref="S3.SS1.p4.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.4.m4.1.1.3.2" xref="S3.SS1.p4.4.m4.1.1.3.2.cmml">â„›</mi><mrow id="S3.SS1.p4.4.m4.1.1.3.3" xref="S3.SS1.p4.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.p4.4.m4.1.1.3.3.2" xref="S3.SS1.p4.4.m4.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.4.m4.1.1.3.3.1" xref="S3.SS1.p4.4.m4.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p4.4.m4.1.1.3.3.3" xref="S3.SS1.p4.4.m4.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><in id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1"></in><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">ğ•</ci><apply id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.3.1.cmml" xref="S3.SS1.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.3.2.cmml" xref="S3.SS1.p4.4.m4.1.1.3.2">â„›</ci><apply id="S3.SS1.p4.4.m4.1.1.3.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3.3"><times id="S3.SS1.p4.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p4.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.p4.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p4.4.m4.1.1.3.3.2">ğ‘›</ci><ci id="S3.SS1.p4.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\mathbf{V}\in\mathcal{R}^{n\times d}</annotation></semantics></math> and the expanded vocabulary embedding be <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{V}_{\text{new}}\in\mathcal{R}^{m\times d}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mrow id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><msub id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2.2" xref="S3.SS1.p4.5.m5.1.1.2.2.cmml">ğ•</mi><mtext id="S3.SS1.p4.5.m5.1.1.2.3" xref="S3.SS1.p4.5.m5.1.1.2.3a.cmml">new</mtext></msub><mo id="S3.SS1.p4.5.m5.1.1.1" xref="S3.SS1.p4.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.5.m5.1.1.3.2" xref="S3.SS1.p4.5.m5.1.1.3.2.cmml">â„›</mi><mrow id="S3.SS1.p4.5.m5.1.1.3.3" xref="S3.SS1.p4.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p4.5.m5.1.1.3.3.2" xref="S3.SS1.p4.5.m5.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.5.m5.1.1.3.3.1" xref="S3.SS1.p4.5.m5.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p4.5.m5.1.1.3.3.3" xref="S3.SS1.p4.5.m5.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><in id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1.1"></in><apply id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.2.1.cmml" xref="S3.SS1.p4.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2.2">ğ•</ci><ci id="S3.SS1.p4.5.m5.1.1.2.3a.cmml" xref="S3.SS1.p4.5.m5.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p4.5.m5.1.1.2.3.cmml" xref="S3.SS1.p4.5.m5.1.1.2.3">new</mtext></ci></apply><apply id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.p4.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.p4.5.m5.1.1.3.2">â„›</ci><apply id="S3.SS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3"><times id="S3.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p4.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3.2">ğ‘š</ci><ci id="S3.SS1.p4.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\mathbf{V}_{\text{new}}\in\mathcal{R}^{m\times d}</annotation></semantics></math>. We freeze <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mi id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">ğ•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><ci id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">ğ•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">\mathbf{V}</annotation></semantics></math> and only update <math id="S3.SS1.p4.7.m7.1" class="ltx_Math" alttext="\mathbf{V}_{\text{new}}" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><msub id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml"><mi id="S3.SS1.p4.7.m7.1.1.2" xref="S3.SS1.p4.7.m7.1.1.2.cmml">ğ•</mi><mtext id="S3.SS1.p4.7.m7.1.1.3" xref="S3.SS1.p4.7.m7.1.1.3a.cmml">new</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><apply id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.2.cmml" xref="S3.SS1.p4.7.m7.1.1.2">ğ•</ci><ci id="S3.SS1.p4.7.m7.1.1.3a.cmml" xref="S3.SS1.p4.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS1.p4.7.m7.1.1.3.cmml" xref="S3.SS1.p4.7.m7.1.1.3">new</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">\mathbf{V}_{\text{new}}</annotation></semantics></math>. Due to the change in the vocabulary size, the final linear layer of the LLM has to be expanded. That is, we add a small new linear layer <math id="S3.SS1.p4.8.m8.1" class="ltx_Math" alttext="\mathbf{W}_{\text{new}}\in\mathcal{R}^{h\times m}" display="inline"><semantics id="S3.SS1.p4.8.m8.1a"><mrow id="S3.SS1.p4.8.m8.1.1" xref="S3.SS1.p4.8.m8.1.1.cmml"><msub id="S3.SS1.p4.8.m8.1.1.2" xref="S3.SS1.p4.8.m8.1.1.2.cmml"><mi id="S3.SS1.p4.8.m8.1.1.2.2" xref="S3.SS1.p4.8.m8.1.1.2.2.cmml">ğ–</mi><mtext id="S3.SS1.p4.8.m8.1.1.2.3" xref="S3.SS1.p4.8.m8.1.1.2.3a.cmml">new</mtext></msub><mo id="S3.SS1.p4.8.m8.1.1.1" xref="S3.SS1.p4.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p4.8.m8.1.1.3" xref="S3.SS1.p4.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.8.m8.1.1.3.2" xref="S3.SS1.p4.8.m8.1.1.3.2.cmml">â„›</mi><mrow id="S3.SS1.p4.8.m8.1.1.3.3" xref="S3.SS1.p4.8.m8.1.1.3.3.cmml"><mi id="S3.SS1.p4.8.m8.1.1.3.3.2" xref="S3.SS1.p4.8.m8.1.1.3.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.8.m8.1.1.3.3.1" xref="S3.SS1.p4.8.m8.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p4.8.m8.1.1.3.3.3" xref="S3.SS1.p4.8.m8.1.1.3.3.3.cmml">m</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.1b"><apply id="S3.SS1.p4.8.m8.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1"><in id="S3.SS1.p4.8.m8.1.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1.1"></in><apply id="S3.SS1.p4.8.m8.1.1.2.cmml" xref="S3.SS1.p4.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.1.1.2.1.cmml" xref="S3.SS1.p4.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.8.m8.1.1.2.2.cmml" xref="S3.SS1.p4.8.m8.1.1.2.2">ğ–</ci><ci id="S3.SS1.p4.8.m8.1.1.2.3a.cmml" xref="S3.SS1.p4.8.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p4.8.m8.1.1.2.3.cmml" xref="S3.SS1.p4.8.m8.1.1.2.3">new</mtext></ci></apply><apply id="S3.SS1.p4.8.m8.1.1.3.cmml" xref="S3.SS1.p4.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.1.1.3.1.cmml" xref="S3.SS1.p4.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.8.m8.1.1.3.2.cmml" xref="S3.SS1.p4.8.m8.1.1.3.2">â„›</ci><apply id="S3.SS1.p4.8.m8.1.1.3.3.cmml" xref="S3.SS1.p4.8.m8.1.1.3.3"><times id="S3.SS1.p4.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p4.8.m8.1.1.3.3.1"></times><ci id="S3.SS1.p4.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p4.8.m8.1.1.3.3.2">â„</ci><ci id="S3.SS1.p4.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p4.8.m8.1.1.3.3.3">ğ‘š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">\mathbf{W}_{\text{new}}\in\mathcal{R}^{h\times m}</annotation></semantics></math> next to the original linear layer <math id="S3.SS1.p4.9.m9.1" class="ltx_Math" alttext="\mathbf{W}\in\mathcal{R}^{h\times n}" display="inline"><semantics id="S3.SS1.p4.9.m9.1a"><mrow id="S3.SS1.p4.9.m9.1.1" xref="S3.SS1.p4.9.m9.1.1.cmml"><mi id="S3.SS1.p4.9.m9.1.1.2" xref="S3.SS1.p4.9.m9.1.1.2.cmml">ğ–</mi><mo id="S3.SS1.p4.9.m9.1.1.1" xref="S3.SS1.p4.9.m9.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p4.9.m9.1.1.3" xref="S3.SS1.p4.9.m9.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.9.m9.1.1.3.2" xref="S3.SS1.p4.9.m9.1.1.3.2.cmml">â„›</mi><mrow id="S3.SS1.p4.9.m9.1.1.3.3" xref="S3.SS1.p4.9.m9.1.1.3.3.cmml"><mi id="S3.SS1.p4.9.m9.1.1.3.3.2" xref="S3.SS1.p4.9.m9.1.1.3.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.9.m9.1.1.3.3.1" xref="S3.SS1.p4.9.m9.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p4.9.m9.1.1.3.3.3" xref="S3.SS1.p4.9.m9.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m9.1b"><apply id="S3.SS1.p4.9.m9.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1"><in id="S3.SS1.p4.9.m9.1.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1.1"></in><ci id="S3.SS1.p4.9.m9.1.1.2.cmml" xref="S3.SS1.p4.9.m9.1.1.2">ğ–</ci><apply id="S3.SS1.p4.9.m9.1.1.3.cmml" xref="S3.SS1.p4.9.m9.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m9.1.1.3.1.cmml" xref="S3.SS1.p4.9.m9.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.9.m9.1.1.3.2.cmml" xref="S3.SS1.p4.9.m9.1.1.3.2">â„›</ci><apply id="S3.SS1.p4.9.m9.1.1.3.3.cmml" xref="S3.SS1.p4.9.m9.1.1.3.3"><times id="S3.SS1.p4.9.m9.1.1.3.3.1.cmml" xref="S3.SS1.p4.9.m9.1.1.3.3.1"></times><ci id="S3.SS1.p4.9.m9.1.1.3.3.2.cmml" xref="S3.SS1.p4.9.m9.1.1.3.3.2">â„</ci><ci id="S3.SS1.p4.9.m9.1.1.3.3.3.cmml" xref="S3.SS1.p4.9.m9.1.1.3.3.3">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m9.1c">\mathbf{W}\in\mathcal{R}^{h\times n}</annotation></semantics></math>. Similarly, <math id="S3.SS1.p4.10.m10.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS1.p4.10.m10.1a"><mi id="S3.SS1.p4.10.m10.1.1" xref="S3.SS1.p4.10.m10.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m10.1b"><ci id="S3.SS1.p4.10.m10.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m10.1c">\mathbf{W}</annotation></semantics></math> is frozen and <math id="S3.SS1.p4.11.m11.1" class="ltx_Math" alttext="\mathbf{W}_{\text{new}}" display="inline"><semantics id="S3.SS1.p4.11.m11.1a"><msub id="S3.SS1.p4.11.m11.1.1" xref="S3.SS1.p4.11.m11.1.1.cmml"><mi id="S3.SS1.p4.11.m11.1.1.2" xref="S3.SS1.p4.11.m11.1.1.2.cmml">ğ–</mi><mtext id="S3.SS1.p4.11.m11.1.1.3" xref="S3.SS1.p4.11.m11.1.1.3a.cmml">new</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.11.m11.1b"><apply id="S3.SS1.p4.11.m11.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.11.m11.1.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p4.11.m11.1.1.2.cmml" xref="S3.SS1.p4.11.m11.1.1.2">ğ–</ci><ci id="S3.SS1.p4.11.m11.1.1.3a.cmml" xref="S3.SS1.p4.11.m11.1.1.3"><mtext mathsize="70%" id="S3.SS1.p4.11.m11.1.1.3.cmml" xref="S3.SS1.p4.11.m11.1.1.3">new</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.11.m11.1c">\mathbf{W}_{\text{new}}</annotation></semantics></math> is learnable.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.07914/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The model structure is shown above. There are two options for introducing spatial information, adding intensity vectors before or after the Q-Former, respectively. Numbers can be also added to the LLM vocabulary as special tokens (ST.), optionally.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Strategy</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Question-answering (QA) format data was used for all the tasks. To predict the direction-of-arrivals (DoAs) given by azimuth angle and elevation angle, the text prompt can be <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">``What is the azimuth angle of the `Male speech' sound?"</span>, and the model is required to directly output a number. For each audio, both questions about the azimuth angle and the elevation angle were asked to determine the DoA. Similarly, prompts for speech recognition and localisation informed extraction can be <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">``Please transcribe the speech (on your right) into a written format. "</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>The DSS LibriSpeech Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To compensate for the fact that Spatial LibriSpeech only consists of non-overlapped speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and introduce more flexible and diverse sound source locations than LibriCSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we simulated a DSS LibriSpeech dataset. Specifically, the state-of-the-art audio simulator, Soundspaces 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> was used to provide highly realistic room impulse responses (RIRs). As for the environment meshes, we utilised detailed enough mesh renderings of 90 buildings from Matterport3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Each building consists of 24.5 rooms across 2.61 floors on average.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.4" class="ltx_p">For each room, we tried to place the receiver at a navigable coordinate, while for the two sound sources, we placed them within the same room with the receiver. As will be mentioned in Sec. <a href="#S5" title="5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, sound source locations for two difficulty levels were considered. A simple setting was that the two sound sources were placed on the left or right side of the receiver, noted as <span id="S3.SS3.p2.4.1" class="ltx_text ltx_font_typewriter">left/right</span> dataset. The left side means the DoAs from the receiver is azimuth angle <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="\phi\in[60^{\circ},120^{\circ}]" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.4" xref="S3.SS3.p2.1.m1.2.2.4.cmml">Ï•</mi><mo id="S3.SS3.p2.1.m1.2.2.3" xref="S3.SS3.p2.1.m1.2.2.3.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">[</mo><msup id="S3.SS3.p2.1.m1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml">60</mn><mo id="S3.SS3.p2.1.m1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml">âˆ˜</mo></msup><mo id="S3.SS3.p2.1.m1.2.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">,</mo><msup id="S3.SS3.p2.1.m1.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.cmml"><mn id="S3.SS3.p2.1.m1.2.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml">120</mn><mo id="S3.SS3.p2.1.m1.2.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml">âˆ˜</mo></msup><mo stretchy="false" id="S3.SS3.p2.1.m1.2.2.2.2.5" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2"><in id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.3"></in><ci id="S3.SS3.p2.1.m1.2.2.4.cmml" xref="S3.SS3.p2.1.m1.2.2.4">italic-Ï•</ci><interval closure="closed" id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1">superscript</csymbol><cn type="integer" id="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2">60</cn><compose id="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3"></compose></apply><apply id="S3.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2">120</cn><compose id="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3"></compose></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">\phi\in[60^{\circ},120^{\circ}]</annotation></semantics></math> and elevation angle <math id="S3.SS3.p2.2.m2.2" class="ltx_Math" alttext="\theta\in[-30^{\circ},30^{\circ}]" display="inline"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml"><mi id="S3.SS3.p2.2.m2.2.2.4" xref="S3.SS3.p2.2.m2.2.2.4.cmml">Î¸</mi><mo id="S3.SS3.p2.2.m2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.3.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.2.m2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.2.m2.1.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS3.p2.2.m2.1.1.1.1.1a" xref="S3.SS3.p2.2.m2.1.1.1.1.1.cmml">âˆ’</mo><msup id="S3.SS3.p2.2.m2.1.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2.cmml"><mn id="S3.SS3.p2.2.m2.1.1.1.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2.2.cmml">30</mn><mo id="S3.SS3.p2.2.m2.1.1.1.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2.3.cmml">âˆ˜</mo></msup></mrow><mo id="S3.SS3.p2.2.m2.2.2.2.2.4" xref="S3.SS3.p2.2.m2.2.2.2.3.cmml">,</mo><msup id="S3.SS3.p2.2.m2.2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.2.cmml"><mn id="S3.SS3.p2.2.m2.2.2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.2.2.cmml">30</mn><mo id="S3.SS3.p2.2.m2.2.2.2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.2.2.2.3.cmml">âˆ˜</mo></msup><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.2.2.5" xref="S3.SS3.p2.2.m2.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><apply id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2"><in id="S3.SS3.p2.2.m2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.3"></in><ci id="S3.SS3.p2.2.m2.2.2.4.cmml" xref="S3.SS3.p2.2.m2.2.2.4">ğœƒ</ci><interval closure="closed" id="S3.SS3.p2.2.m2.2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2"><apply id="S3.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1"><minus id="S3.SS3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1"></minus><apply id="S3.SS3.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2.2">30</cn><compose id="S3.SS3.p2.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS3.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2.2">30</cn><compose id="S3.SS3.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2.3"></compose></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">\theta\in[-30^{\circ},30^{\circ}]</annotation></semantics></math>. The right side means the DoAs from the receiver is azimuth angle <math id="S3.SS3.p2.3.m3.2" class="ltx_Math" alttext="\phi\in[-120^{\circ},-60^{\circ}]" display="inline"><semantics id="S3.SS3.p2.3.m3.2a"><mrow id="S3.SS3.p2.3.m3.2.2" xref="S3.SS3.p2.3.m3.2.2.cmml"><mi id="S3.SS3.p2.3.m3.2.2.4" xref="S3.SS3.p2.3.m3.2.2.4.cmml">Ï•</mi><mo id="S3.SS3.p2.3.m3.2.2.3" xref="S3.SS3.p2.3.m3.2.2.3.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.3.m3.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m3.2.2.2.2.3" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.3.m3.1.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS3.p2.3.m3.1.1.1.1.1a" xref="S3.SS3.p2.3.m3.1.1.1.1.1.cmml">âˆ’</mo><msup id="S3.SS3.p2.3.m3.1.1.1.1.1.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.cmml"><mn id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml">120</mn><mo id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml">âˆ˜</mo></msup></mrow><mo id="S3.SS3.p2.3.m3.2.2.2.2.4" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p2.3.m3.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.cmml"><mo id="S3.SS3.p2.3.m3.2.2.2.2.2a" xref="S3.SS3.p2.3.m3.2.2.2.2.2.cmml">âˆ’</mo><msup id="S3.SS3.p2.3.m3.2.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.cmml"><mn id="S3.SS3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml">60</mn><mo id="S3.SS3.p2.3.m3.2.2.2.2.2.2.3" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml">âˆ˜</mo></msup></mrow><mo stretchy="false" id="S3.SS3.p2.3.m3.2.2.2.2.5" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.2b"><apply id="S3.SS3.p2.3.m3.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2"><in id="S3.SS3.p2.3.m3.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.3"></in><ci id="S3.SS3.p2.3.m3.2.2.4.cmml" xref="S3.SS3.p2.3.m3.2.2.4">italic-Ï•</ci><interval closure="closed" id="S3.SS3.p2.3.m3.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2"><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"><minus id="S3.SS3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"></minus><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2">120</cn><compose id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS3.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2"><minus id="S3.SS3.p2.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2"></minus><apply id="S3.SS3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.2">60</cn><compose id="S3.SS3.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.3"></compose></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.2c">\phi\in[-120^{\circ},-60^{\circ}]</annotation></semantics></math> and elevation angle <math id="S3.SS3.p2.4.m4.2" class="ltx_Math" alttext="\theta\in[-30^{\circ},30^{\circ}]" display="inline"><semantics id="S3.SS3.p2.4.m4.2a"><mrow id="S3.SS3.p2.4.m4.2.2" xref="S3.SS3.p2.4.m4.2.2.cmml"><mi id="S3.SS3.p2.4.m4.2.2.4" xref="S3.SS3.p2.4.m4.2.2.4.cmml">Î¸</mi><mo id="S3.SS3.p2.4.m4.2.2.3" xref="S3.SS3.p2.4.m4.2.2.3.cmml">âˆˆ</mo><mrow id="S3.SS3.p2.4.m4.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m4.2.2.2.2.3" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.4.m4.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS3.p2.4.m4.1.1.1.1.1a" xref="S3.SS3.p2.4.m4.1.1.1.1.1.cmml">âˆ’</mo><msup id="S3.SS3.p2.4.m4.1.1.1.1.1.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml"><mn id="S3.SS3.p2.4.m4.1.1.1.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.2.cmml">30</mn><mo id="S3.SS3.p2.4.m4.1.1.1.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.3.cmml">âˆ˜</mo></msup></mrow><mo id="S3.SS3.p2.4.m4.2.2.2.2.4" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">,</mo><msup id="S3.SS3.p2.4.m4.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.2.2.cmml"><mn id="S3.SS3.p2.4.m4.2.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml">30</mn><mo id="S3.SS3.p2.4.m4.2.2.2.2.2.3" xref="S3.SS3.p2.4.m4.2.2.2.2.2.3.cmml">âˆ˜</mo></msup><mo stretchy="false" id="S3.SS3.p2.4.m4.2.2.2.2.5" xref="S3.SS3.p2.4.m4.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.2b"><apply id="S3.SS3.p2.4.m4.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2"><in id="S3.SS3.p2.4.m4.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.3"></in><ci id="S3.SS3.p2.4.m4.2.2.4.cmml" xref="S3.SS3.p2.4.m4.2.2.4">ğœƒ</ci><interval closure="closed" id="S3.SS3.p2.4.m4.2.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2"><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1"><minus id="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1"></minus><apply id="S3.SS3.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.2">30</cn><compose id="S3.SS3.p2.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS3.p2.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2">30</cn><compose id="S3.SS3.p2.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2.3"></compose></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.2c">\theta\in[-30^{\circ},30^{\circ}]</annotation></semantics></math>. Another more complicated setting was that the two sound sources were placed randomly, noted as <span id="S3.SS3.p2.4.2" class="ltx_text ltx_font_typewriter">random</span> dataset, which puts higher requirements on the model's spatial angle resolution capability. As for the distances between the sound sources and the receiver, we just randomly placed the sound sources as long as they were in the same room.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For the training sets, we tried to place the receiver at 10 different navigable locations in each room and generated 26704 pairs of RIRs (<span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">left/right</span> dataset and <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">random</span> dataset each has 13352 pairs), which were then convolved with randomly sampled utterances from LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">train-clean-100h</span> subset to finally get a non-overlapped version <span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_typewriter">random</span> dataset by activating the two sources sequentially and another ``fully" overlapped version <span id="S3.SS3.p3.1.5" class="ltx_text ltx_font_typewriter">left/right</span> dataset by activating the two sources simultaneously.<span id="footnote1b" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The generated non-overlapped dataset cannot be treated equally as a concatenated version of Spatial LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, because the two utterances are activated under the same reverberation environment.
Here, â€œfullyâ€ does not mean that the overlapping ratio of the two utterances is 100% because the durations of them may be different.</span></span></span> Each training set consists of about 50k samples.
For the test sets, we generated several datasets each containing 2k samples with a varying degree of overlapping ratio by sampling utterances from LibriSpeech <span id="S3.SS3.p3.1.6" class="ltx_text ltx_font_typewriter">test-clean</span> subset using the method above.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Model Specifications</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We adopt the Whisper-large-v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> encoder as the audio encoder, the window-level Q-Former <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as the modality aligner, and the Vicuna-7b-v1.5 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> as the LLM.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">For intensity vectors, the hyper-parameters for Short-Time Fourier Transform (STFT) were set to window length=<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="800" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">800</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">800</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">800</annotation></semantics></math> and hop length=<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="320" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">320</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">320</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">320</annotation></semantics></math>, which generated IVs at the same frame rate 50Hz as the output of the Whisper encoder. Following the best setting in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we utilised Q-Former with a window slide of 0.33s. For LoRA, the rank and the scaling factor were set to 8 and 4.0, respectively. As for expanding the LLM's vocabulary, we added integers from <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="-180" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mo id="S4.SS1.p2.3.m3.1.1a" xref="S4.SS1.p2.3.m3.1.1.cmml">âˆ’</mo><mn id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">180</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><minus id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"></minus><cn type="integer" id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">180</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">-180</annotation></semantics></math> to <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="180" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">180</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn type="integer" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">180</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">180</annotation></semantics></math> as special text tokens.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Specifications</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To compare different model structures without consuming too much resource, experiments in Sec.Â <a href="#S5.SS1" title="5.1 3D Sound Source Localisation (SSL) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> and Â <a href="#S5.SS2" title="5.2 Far-field Speech Recognition (FSR) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> were performed with randomly selected 51k samples (approximately 30% of the total data) from the training set of Spatial LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, with 1k samples designated for the validation set, and tested on the official Spatial LibriSpeech test set. In Sec.Â <a href="#S5.SS2" title="5.2 Far-field Speech Recognition (FSR) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we also collected samples from
the original LibriSpeech dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for a close-talk version of the same data used in Spatial LibriSpeech to train a close-talk version system. The proposed DSS LibriSpeech was used in Sec.Â <a href="#S5.SS3" title="5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Task Specifications</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To explore whether LLM can understand spatial speech well, three tasks are focused on in our experiments. Sec.<a href="#S5.SS1" title="5.1 3D Sound Source Localisation (SSL) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> investigates the use of LLM for 3D SSL while finding the best way to fuse spatial information. Sec.<a href="#S5.SS2" title="5.2 Far-field Speech Recognition (FSR) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> analysed FSR and LSE was investigated further in Sec.<a href="#S5.SS3" title="5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For all the experiments, we set the batch size to 16 and trained the models with 8 A100 GPUs. Models in Sec.<a href="#S5.SS1" title="5.1 3D Sound Source Localisation (SSL) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> and Sec.<a href="#S5.SS2" title="5.2 Far-field Speech Recognition (FSR) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> were trained for no more than 30,000 steps and the best checkpoint on the validation set was used for testing. Moreover, models in TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> were trained for 15,000 steps on the non-overlapped <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">random</span> dataset and by continuing training this model on the ``fully" overlapped <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">left/right</span> dataset for another 15,000 steps, we got the models in Fig.Â <a href="#S5.F2" title="Figure 2 â€£ 5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>3D Sound Source Localisation (SSL)</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We first tried introducing spatial information to allow the model to determine the direction of the sound source. Different methods are compared here, including training without any spatial information (w/o IV), using intensity vectors with either the ``Before'' or ``After'' option, and using ``Before'' and tokenising numbers as special tokens.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.5" class="ltx_p">To evaluate the performance of SSL, <span id="S5.SS1.p2.5.1" class="ltx_text ltx_font_italic">i.e.</span> decoding the exact orientation of the sound event, the average differences of the azimuth angle, the elevation angle and the angular distance are calculated to get <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\Delta_{a}" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><msub id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">Î”</mi><mi id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">Î”</ci><ci id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\Delta_{a}</annotation></semantics></math>, <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\Delta_{e}" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><msub id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">Î”</mi><mi id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">Î”</ci><ci id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\Delta_{e}</annotation></semantics></math> and <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="\Delta_{d}" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">Î”</mi><mi id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">Î”</ci><ci id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\Delta_{d}</annotation></semantics></math> respectively. Note that <math id="S5.SS1.p2.4.m4.1" class="ltx_Math" alttext="\Delta_{d}" display="inline"><semantics id="S5.SS1.p2.4.m4.1a"><msub id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p2.4.m4.1.1.2" xref="S5.SS1.p2.4.m4.1.1.2.cmml">Î”</mi><mi id="S5.SS1.p2.4.m4.1.1.3" xref="S5.SS1.p2.4.m4.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><apply id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.1.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p2.4.m4.1.1.2.cmml" xref="S5.SS1.p2.4.m4.1.1.2">Î”</ci><ci id="S5.SS1.p2.4.m4.1.1.3.cmml" xref="S5.SS1.p2.4.m4.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">\Delta_{d}</annotation></semantics></math> is the MAE metric. Since it is the angular distance that most truly reflects the accuracy of the prediction, <math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="\Delta_{d}" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><msub id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p2.5.m5.1.1.2" xref="S5.SS1.p2.5.m5.1.1.2.cmml">Î”</mi><mi id="S5.SS1.p2.5.m5.1.1.3" xref="S5.SS1.p2.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><apply id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.2.cmml" xref="S5.SS1.p2.5.m5.1.1.2">Î”</ci><ci id="S5.SS1.p2.5.m5.1.1.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">\Delta_{d}</annotation></semantics></math> serves as our primary reference metric. The results are shown in TableÂ <a href="#S5.T1" title="Table 1 â€£ 5.1 3D Sound Source Localisation (SSL) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of different modelling methods for 3D SSL. ``w/IV'' and ``w/o IV'' refer to with or without using IV, and ``ST." refers to using special tokens to tokenise numbers.</figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta_{a}^{\circ}\downarrow" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml"><msubsup id="S5.T1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S5.T1.1.1.1.m1.1.1.2.2.2" xref="S5.T1.1.1.1.m1.1.1.2.2.2.cmml">Î”</mi><mi id="S5.T1.1.1.1.m1.1.1.2.2.3" xref="S5.T1.1.1.1.m1.1.1.2.2.3.cmml">a</mi><mo id="S5.T1.1.1.1.m1.1.1.2.3" xref="S5.T1.1.1.1.m1.1.1.2.3.cmml">âˆ˜</mo></msubsup><mo stretchy="false" id="S5.T1.1.1.1.m1.1.1.1" xref="S5.T1.1.1.1.m1.1.1.1.cmml">â†“</mo><mi id="S5.T1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"><ci id="S5.T1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1.1">â†“</ci><apply id="S5.T1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.1.1.1.m1.1.1.2.1.cmml" xref="S5.T1.1.1.1.m1.1.1.2">superscript</csymbol><apply id="S5.T1.1.1.1.m1.1.1.2.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.1.1.1.m1.1.1.2.2.1.cmml" xref="S5.T1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S5.T1.1.1.1.m1.1.1.2.2.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2.2.2">Î”</ci><ci id="S5.T1.1.1.1.m1.1.1.2.2.3.cmml" xref="S5.T1.1.1.1.m1.1.1.2.2.3">ğ‘</ci></apply><compose id="S5.T1.1.1.1.m1.1.1.2.3.cmml" xref="S5.T1.1.1.1.m1.1.1.2.3"></compose></apply><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\Delta_{a}^{\circ}\downarrow</annotation></semantics></math></th>
<th id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.2.2.2.m1.1" class="ltx_Math" alttext="\Delta_{e}^{\circ}\downarrow" display="inline"><semantics id="S5.T1.2.2.2.m1.1a"><mrow id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml"><msubsup id="S5.T1.2.2.2.m1.1.1.2" xref="S5.T1.2.2.2.m1.1.1.2.cmml"><mi mathvariant="normal" id="S5.T1.2.2.2.m1.1.1.2.2.2" xref="S5.T1.2.2.2.m1.1.1.2.2.2.cmml">Î”</mi><mi id="S5.T1.2.2.2.m1.1.1.2.2.3" xref="S5.T1.2.2.2.m1.1.1.2.2.3.cmml">e</mi><mo id="S5.T1.2.2.2.m1.1.1.2.3" xref="S5.T1.2.2.2.m1.1.1.2.3.cmml">âˆ˜</mo></msubsup><mo stretchy="false" id="S5.T1.2.2.2.m1.1.1.1" xref="S5.T1.2.2.2.m1.1.1.1.cmml">â†“</mo><mi id="S5.T1.2.2.2.m1.1.1.3" xref="S5.T1.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><apply id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1"><ci id="S5.T1.2.2.2.m1.1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1.1">â†“</ci><apply id="S5.T1.2.2.2.m1.1.1.2.cmml" xref="S5.T1.2.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.2.2.2.m1.1.1.2.1.cmml" xref="S5.T1.2.2.2.m1.1.1.2">superscript</csymbol><apply id="S5.T1.2.2.2.m1.1.1.2.2.cmml" xref="S5.T1.2.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.2.2.2.m1.1.1.2.2.1.cmml" xref="S5.T1.2.2.2.m1.1.1.2">subscript</csymbol><ci id="S5.T1.2.2.2.m1.1.1.2.2.2.cmml" xref="S5.T1.2.2.2.m1.1.1.2.2.2">Î”</ci><ci id="S5.T1.2.2.2.m1.1.1.2.2.3.cmml" xref="S5.T1.2.2.2.m1.1.1.2.2.3">ğ‘’</ci></apply><compose id="S5.T1.2.2.2.m1.1.1.2.3.cmml" xref="S5.T1.2.2.2.m1.1.1.2.3"></compose></apply><csymbol cd="latexml" id="S5.T1.2.2.2.m1.1.1.3.cmml" xref="S5.T1.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\Delta_{e}^{\circ}\downarrow</annotation></semantics></math></th>
<th id="S5.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.3.3.3.m1.1" class="ltx_Math" alttext="\Delta_{d}^{\circ}\text{(MAE)}\downarrow" display="inline"><semantics id="S5.T1.3.3.3.m1.1a"><mrow id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml"><mrow id="S5.T1.3.3.3.m1.1.1.2" xref="S5.T1.3.3.3.m1.1.1.2.cmml"><msubsup id="S5.T1.3.3.3.m1.1.1.2.2" xref="S5.T1.3.3.3.m1.1.1.2.2.cmml"><mi mathvariant="normal" id="S5.T1.3.3.3.m1.1.1.2.2.2.2" xref="S5.T1.3.3.3.m1.1.1.2.2.2.2.cmml">Î”</mi><mi id="S5.T1.3.3.3.m1.1.1.2.2.2.3" xref="S5.T1.3.3.3.m1.1.1.2.2.2.3.cmml">d</mi><mo id="S5.T1.3.3.3.m1.1.1.2.2.3" xref="S5.T1.3.3.3.m1.1.1.2.2.3.cmml">âˆ˜</mo></msubsup><mo lspace="0em" rspace="0em" id="S5.T1.3.3.3.m1.1.1.2.1" xref="S5.T1.3.3.3.m1.1.1.2.1.cmml">â€‹</mo><mtext id="S5.T1.3.3.3.m1.1.1.2.3" xref="S5.T1.3.3.3.m1.1.1.2.3a.cmml">(MAE)</mtext></mrow><mo stretchy="false" id="S5.T1.3.3.3.m1.1.1.1" xref="S5.T1.3.3.3.m1.1.1.1.cmml">â†“</mo><mi id="S5.T1.3.3.3.m1.1.1.3" xref="S5.T1.3.3.3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><apply id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1"><ci id="S5.T1.3.3.3.m1.1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1.1">â†“</ci><apply id="S5.T1.3.3.3.m1.1.1.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2"><times id="S5.T1.3.3.3.m1.1.1.2.1.cmml" xref="S5.T1.3.3.3.m1.1.1.2.1"></times><apply id="S5.T1.3.3.3.m1.1.1.2.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2"><csymbol cd="ambiguous" id="S5.T1.3.3.3.m1.1.1.2.2.1.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2">superscript</csymbol><apply id="S5.T1.3.3.3.m1.1.1.2.2.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2"><csymbol cd="ambiguous" id="S5.T1.3.3.3.m1.1.1.2.2.2.1.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2">subscript</csymbol><ci id="S5.T1.3.3.3.m1.1.1.2.2.2.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2.2.2">Î”</ci><ci id="S5.T1.3.3.3.m1.1.1.2.2.2.3.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2.2.3">ğ‘‘</ci></apply><compose id="S5.T1.3.3.3.m1.1.1.2.2.3.cmml" xref="S5.T1.3.3.3.m1.1.1.2.2.3"></compose></apply><ci id="S5.T1.3.3.3.m1.1.1.2.3a.cmml" xref="S5.T1.3.3.3.m1.1.1.2.3"><mtext id="S5.T1.3.3.3.m1.1.1.2.3.cmml" xref="S5.T1.3.3.3.m1.1.1.2.3">(MAE)</mtext></ci></apply><csymbol cd="latexml" id="S5.T1.3.3.3.m1.1.1.3.cmml" xref="S5.T1.3.3.3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\Delta_{d}^{\circ}\text{(MAE)}\downarrow</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.4.1" class="ltx_tr">
<th id="S5.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">w/o IV</th>
<td id="S5.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">90.19</td>
<td id="S5.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">8.26</td>
<td id="S5.T1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">90.16</td>
</tr>
<tr id="S5.T1.3.5.2" class="ltx_tr">
<th id="S5.T1.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/ IV (``Before'')</th>
<td id="S5.T1.3.5.2.2" class="ltx_td ltx_align_center"><span id="S5.T1.3.5.2.2.1" class="ltx_text ltx_font_bold">1.60</span></td>
<td id="S5.T1.3.5.2.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.5.2.3.1" class="ltx_text ltx_font_bold">1.76</span></td>
<td id="S5.T1.3.5.2.4" class="ltx_td ltx_align_center"><span id="S5.T1.3.5.2.4.1" class="ltx_text ltx_font_bold">2.70</span></td>
</tr>
<tr id="S5.T1.3.6.3" class="ltx_tr">
<th id="S5.T1.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/ IV (``After'')</th>
<td id="S5.T1.3.6.3.2" class="ltx_td ltx_align_center">3.35</td>
<td id="S5.T1.3.6.3.3" class="ltx_td ltx_align_center">2.56</td>
<td id="S5.T1.3.6.3.4" class="ltx_td ltx_align_center">4.69</td>
</tr>
<tr id="S5.T1.3.7.4" class="ltx_tr">
<th id="S5.T1.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/ IV (``Before'') &amp; ST.</th>
<td id="S5.T1.3.7.4.2" class="ltx_td ltx_align_center">1.88</td>
<td id="S5.T1.3.7.4.3" class="ltx_td ltx_align_center">1.85</td>
<td id="S5.T1.3.7.4.4" class="ltx_td ltx_align_center">2.97</td>
</tr>
<tr id="S5.T1.3.8.5" class="ltx_tr">
<th id="S5.T1.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Spatial LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<td id="S5.T1.3.8.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
<td id="S5.T1.3.8.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
<td id="S5.T1.3.8.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Â Â Â 6.60 <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, median angular error is used instead of MAE.</span></span></span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">From TableÂ <a href="#S5.T1" title="Table 1 â€£ 5.1 3D Sound Source Localisation (SSL) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, without any spatial information, the model fails to determine the azimuth angle well, leading to significant deviations in the angular distance. On the other hand, once intensity vectors are introduced, the predictive accuracy of the angles increases significantly, where using ``Before'' leads to about 60% reduction in MAE compared to the official baseline.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">In terms of effectiveness between the different methods, the prediction of direction is more accurate using ``Before'' compared to ``After''. This suggests that it is difficult to align the spatial information, such as IVs, with the input textual space of the LLM, using only a single linear transformation like in ``After''. It is possible that a more powerful transformation can align the IVs well with the LLM input space.
In addition, adding special tokens for numbers to LLM's input embedding slightly impairs the performance. This implies that LLM itself has a good understanding of numbers and doesn't need a separate expanded vocabulary to learn numbers.
Moreover, LLM may not understand the relative relationships between the newly added number tokens, which is different from the original tokenisation was trained on a large amount of textual corpus, which might explain the drop in performance.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Based on the results, it is proved to be a decent setup using intensity vectors with ``Before'' without adding special number tokens. Therefore, we use this configuration throughout the rest of the experiments, denoted by ``w/ IV".</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Far-field Speech Recognition (FSR)</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Only being able to recognise the direction of the sound is not sufficient to achieve full spatial audio understanding of LLMs. It is more necessary to understand the audio content in conjunction with the spatial information. We investigated speech recognition for spatial audio here, and consider three settings: using non-Spatial LibriSpeech data and using Spatial LibriSpeech with and without adding intensity vectors. The results are shown in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 Far-field Speech Recognition (FSR) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of speech recognition under different settings. ``w/IV'' and ``w/o IV'' refer to with or without using IV.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data</th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">WER%<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">w/o IV</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">LibriSpeech</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">8.6</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o IV</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center">Spatial LibriSpeech</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center">9.0</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">w/ IV</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">Spatial LibriSpeech</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.4.3.3.1" class="ltx_text ltx_font_bold">7.6</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Training on spatial audio with intensity vectors turns out to be the best according to the results, indicating that LLM has learnt to use spatial information to assist FSR.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Localisation-Informed Speech Extraction (LSE)</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Three metrics are leveraged to analyse the performance of the localisation-informed speech extraction task. First, we found that audio-LLM cannot always transcribe the speech from the required direction but recognizes the speech from the other direction or outputs a concatenated/mixed version of the two utterances. Thus we propose a GPT-assisted approach <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We inform GPT-3.5-turbo the output of the model, the transcription of the target speech and the transcription of the distracting speech and design a prompt to request it to judge whether the model attempts to transcribe the target speech. LLM-assisted evaluation methods are widely used in NLP and ML. The correctness is based on randomly sampled human checks.</span></span></span> to judge whether the audio-LLM extracts the target speech successfully and reports a success rate (SR). Then we calculate WERs for both the successful samples only and all the samples, noted as sWER and WER respectively, to evaluate the accuracy of the model in recognizing target speech.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">We first analysed whether spatial information is crucial for LSE using a non-overlapped version dataset. As shown in TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, if any spatial information is not provided for the LLM, the model achieves SR% around 50 on both <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">left/right</span> and <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">random</span> datasets, which means the model cannot extract the speech from the target direction as required. On the contrary, models with IV as input can extract the target speech successfully at a SR% over 80, which demonstrates the importance of spatial information in this task.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Moreover, TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the model's performance on <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">random</span> dataset is worse than that on <span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">left/right</span> dataset, which reveals that the closer the angular distance between the two sound sources is, the harder it is for the model to distinguish the two utterances from each other.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on the non-overlapped version of DSS LibriSpeech. ``w/ IV'' and ``w/o IV'' refer to with or without using IV. Performance on <span id="S5.T3.6.1" class="ltx_text ltx_font_typewriter">left/right</span> dataset and <span id="S5.T3.7.2" class="ltx_text ltx_font_typewriter">random</span> dataset are compared.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S5.T3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data</th>
<th id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SR%<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">sWER%<math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">WER%<math id="S5.T3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.3.4.1" class="ltx_tr">
<th id="S5.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T3.3.4.1.1.1" class="ltx_text">w/o IV</span></th>
<td id="S5.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">left/right</td>
<td id="S5.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">50.1</td>
<td id="S5.T3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">4.4</td>
<td id="S5.T3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t">67.2</td>
</tr>
<tr id="S5.T3.3.5.2" class="ltx_tr">
<td id="S5.T3.3.5.2.1" class="ltx_td ltx_align_center">random</td>
<td id="S5.T3.3.5.2.2" class="ltx_td ltx_align_center">50.0</td>
<td id="S5.T3.3.5.2.3" class="ltx_td ltx_align_center">4.0</td>
<td id="S5.T3.3.5.2.4" class="ltx_td ltx_align_center">67.5</td>
</tr>
<tr id="S5.T3.3.6.3" class="ltx_tr">
<th id="S5.T3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T3.3.6.3.1.1" class="ltx_text">w/ IV</span></th>
<td id="S5.T3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_t">left/right</td>
<td id="S5.T3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t">96.3</td>
<td id="S5.T3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_t">5.1</td>
<td id="S5.T3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_t">9.0</td>
</tr>
<tr id="S5.T3.3.7.4" class="ltx_tr">
<td id="S5.T3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_bb">random</td>
<td id="S5.T3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_bb">80.2</td>
<td id="S5.T3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_bb">5.4</td>
<td id="S5.T3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_bb">29.8</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Next, we used the <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">left/right</span> dataset to investigate the impact of the overlapping ratio of the two utterances. As shown in Fig.Â <a href="#S5.F2" title="Figure 2 â€£ 5.3 Localisation-Informed Speech Extraction (LSE) â€£ 5 Experimental Results â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, with the increasing of the overlapping ratio, the model's performance gradually degrades. It shows that the model still struggles to extract the specified speech under circumstances where utterances are highly overlapped with each other. This performance degradation is attributed to both the reduction in success rate and the deterioration in WERs.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F2.1.1" class="ltx_p ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:208.1pt;"><span id="S5.F2.1.1.1" class="ltx_text"><img src="/html/2406.07914/assets/x2.png" id="S5.F2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="121" height="121" alt="Refer to caption"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F2.2.1" class="ltx_p ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:208.1pt;"><span id="S5.F2.2.1.1" class="ltx_text"><img src="/html/2406.07914/assets/x3.png" id="S5.F2.2.1.1.g1" class="ltx_graphics ltx_img_square" width="121" height="121" alt="Refer to caption"></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Results on the <span id="S5.F2.4.1" class="ltx_text ltx_font_typewriter">left/right</span> dataset with overlapping ratio from 0% to 70%. The dotted lines are the performance on the ``fully" overlapped test set generated by activating the two sources simultaneously. (as described in FootnoteÂ <a href="#footnote1b" title="footnote 1 â€£ 3.3 The DSS LibriSpeech Dataset â€£ 3 Methods â€£ Can Large Language Models Understand Spatial Audio?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper delves into the vital exploration of enabling LLMs to understand spatial audio.
Experiments show that introducing spatial information, such as intensity vectors, before the modality aligner is suitable for the auditory LLMs to perceive spatial audio well, and expanding the LLMs' vocabulary with direction-related special tokens can impair the performance.
Advanced results are obtained in SSL with an MAE of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="2.70^{\circ}" display="inline"><semantics id="S6.p1.1.m1.1a"><msup id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">2.70</mn><mo id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1">superscript</csymbol><cn type="float" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">2.70</cn><compose id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">2.70^{\circ}</annotation></semantics></math>, which is a nearly 60% improvement over the official Spatial LibriSpeech baseline using only 30% of all training samples in the dataset.
Meanwhile, the pivotal role of spatial information in enhancing LLMs' comprehension of spatial audio in FSR is also underscored.
In addition, an overlapped spatial speech dataset is simulated, and our model can selectively attend to audio in different directions based on different text prompts, showing the potential of future artificial intelligence to perceive and understand real-world sounds like humans.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
OpenAI, ``GPT-4 technical report,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.-A. Lachaux <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``LLaMA: Open and efficient foundation language models,'' <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W.-L. Chiang, Z.Â Li, Z.Â Lin <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality,'' March 2023. [Online]. Available: <a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-03-30-vicuna/</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.-B. Alayrac, J.Â Donahue, P.Â Luc, A.Â Miech, I.Â Barr <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Flamingo: a visual language model for few-shot learning,'' in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, New Orleans, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.Â Li, D.Â Li, S.Â Savarese, and S.Â Hoi, ``BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, Hawaii, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.Â Dai, J.Â Li, D.Â Li, A.Â Tiong, J.Â Zhao, W.Â Wang, B.Â Li, P.Â Fung, and S.Â Hoi, ``InstructBLIP: Towards general-purpose vision-language models with instruction tuning,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, New Orleans, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.Â Zhao, I.Â Misra, P.Â KrÃ¤henbÃ¼hl, and R.Â Girdhar, ``Learning video representations from large language models,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, New Orleans, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.Â Radhakrishnan, C.-H.Â H. Yang, S.Â Khan, R.Â Kumar, N.Â Kiani, D.Â Gomez-Cabrero, and J.Â TegnÃ©r, ``Whispering LLaMA: A cross-modal generative error correction framework for speech recognition,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. EMNLP</em>, Singapore, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.Â Wu, Y.Â Gaur, Z.Â Chen, L.Â Zhou, Y.Â Zhu, T.Â Wang, J.Â Li, S.Â Liu, B.Â Ren, L.Â Liu, and Y.Â Wu, ``On decoder-only architecture for speech-to-text and large language model integration,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. ASRU</em>, Taipei, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M.Â Wang, W.Â Han, I.Â Shafran, Z.Â Wu, C.-C. Chiu, Y.Â Cao, Y.Â Wang, N.Â Chen, Y.Â Zhang, H.Â Soltau, P.Â Rubenstein, L.Â Zilka, D.Â Yu, Z.Â Meng, G.Â Pundak, N.Â Siddhartha, J.Â Schalkwyk, and Y.Â Wu, ``On decoder-only architecture for speech-to-text and large language model integration,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. ASRU</em>, Taipei, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y.Â Gong, S.Â Khurana, L.Â Karlinsky, and J.Â Glass, ``Whisper-AT: Noise-robust automatic speech recognizers are also strong general audio event taggers,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Dublin, Ireland, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.Â Fathullah, C.Â Wu, E.Â Lakomkin, J.Â Jia, Y.Â Shangguan, K.Â Li, J.Â Guo, W.Â Xiong, J.Â Mahadeokar, O.Â Kalinli <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Prompting large language models with speech recognition abilities,'' in <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">in Proc. ICASSP</em>, Seoul, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Z.Â Chen, H.Â Huang, A.Â Andrusenko, O.Â Hrinchuk, K.Â C. Puvvada, J.Â Li, S.Â Ghosh, J.Â Balam, and B.Â Ginsburg, ``SALM: Speech-augmented language model with in-context learning for speech recognition and translation,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">in Proc. ICASSP</em>, Seoul, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
W.Â Yu, C.Â Tang, G.Â Sun, X.Â Chen, T.Â Tan, W.Â Li, L.Â Lu, Z.Â Ma, and C.Â Zhang, ``Connecting speech encoder and large language model for asr,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Seoul, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C.Â Tang, W.Â Yu, G.Â Sun, X.Â Chen, T.Â Tan, W.Â Li, L.Â Lu, Z.Â Ma, and C.Â Zhang, ``Extending large language models for speech and audio captioning,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Seoul, 2024.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.Â Gong, H.Â Luo, A.Â H. Liu, L.Â Karlinsky, and J.Â R. Glass, ``Listen, think, and understand,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, Vienna, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C.Â Tang, W.Â Yu, G.Â Sun, X.Â Chen, T.Â Tan, W.Â Li, L.Â Lu, Z.Â Ma, and C.Â Zhang, ``SALMONN: Towards generic hearing abilities for large language models,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, Vienna, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J.Â Wei, M.Â Bosma, V.Â Y. Zhao, K.Â Guu, A.Â W. Yu, B.Â Lester, N.Â Du, A.Â M. Dai, and Q.Â V. Le, ``Finetuned language models are zero-shot learners,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H.Â W. Chung, L.Â Hou, S.Â Longpre, B.Â Zoph, Y.Â Tay <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Scaling instruction-finetuned language models,'' <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">arXiv:2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L.Â Ouyang, J.Â Wu, X.Â Jiang, D.Â Almeida, C.Â Wainwright, P.Â Mishkin, C.Â Zhang, S.Â Agarwal, K.Â Slama, A.Â Ray <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Training language models to follow instructions with human feedback,'' in <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, New Orleans, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â Ahonen, V.Â Pulkki, and T.Â Lokki, ``Teleconference application and B-format microphone array for directional audio coding,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. ICAES</em>, SaariselkÃ¤, 2007.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M.Â Sarabia, E.Â Menyaylenko, A.Â Toso, S.Â Seto, Z.Â Aldeneh, S.Â Pirhosseinloo, L.Â Zappella, B.-J. Theobald, N.Â Apostoloff, and J.Â Sheaffer, ``Spatial LibriSpeech: An augmented dataset for spatial audio learning,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Dublin, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V.Â Panayotov, G.Â Chen, D.Â Povey, and S.Â Khudanpur, ``Librispeech: An ASR corpus based on public domain audio books,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, South Brisbane, 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C.Â Chen, C.Â Schissler, S.Â Garg, P.Â Kobernik, A.Â Clegg, P.Â Calamia, D.Â Batra, P.Â W. Robinson, and K.Â Grauman, ``SoundSpaces 2.0: A simulation platform for visual-acoustic learning,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS Datasets and Benchmarks Track</em>, New Orleans, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P.Â K. Rubenstein, C.Â Asawaroengchai, D.Â D. Nguyen, A.Â Bapna, Z.Â Borsos, F.Â d.Â C. Quitry, P.Â Chen, D.Â E. Badawy, W.Â Han, E.Â Kharitonov <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``AudioPaLM: A large language model that can speak and listen,'' <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">arXiv:2306.12925</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z.Â Zheng, P.Â Peng, Z.Â Ma, X.Â Chen, E.Â Choi, and D.Â Harwath, ``BAT: Learning to reason about spatial sounds with large language models,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, Vienna, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
E.Â Guizzo, C.Â Marinoni, M.Â Pennese, X.Â Ren, X.Â Zheng, C.Â Zhang, B.Â Masiero, A.Â Uncini, and D.Â Comminiello, ``L3DAS22 challenge: Learning 3D audio sources in a real office environment,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Singapore, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R.Â Scheibler, E.Â Bezzam, and I.Â Dokmanic, ``Pyroomacoustics: A Python package for audio room simulation and array processing algorithms,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Calgary, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C.Â Chen, C.Â Schissler, S.Â Garg, P.Â Kobernik, A.Â Clegg, P.Â Calamia, D.Â Batra, P.Â W. Robinson, and K.Â Grauman, ``Soundspaces 2.0: A simulation platform for visual-acoustic learning,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, New Orleans, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S.Â Adavanne, A.Â Politis, J.Â Nikunen, and T.Â Virtanen, ``Sound event localization and detection of overlapping sources using convolutional recurrent neural networks,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Journal of Selected Topics in Signal Processing</em>, vol.Â 13, p. 34â€“48, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Q.Â Wang, H.Â Wu, Z.Â Jing, F.Â Ma, Y.Â Fang, Y.Â Wang, T.Â Chen, J.-Y. Pan, J.Â Du, and C.-H. Lee, ``The USTCiFlytek system for sound event localization and detection of DCASE2020 challenge,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. DCASE</em>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y.Â Zhang, S.Â Wang, Z.Â Li, K.Â Guo <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Data augmentation and class-based ensembled CNNConformer networks for sound event localization and detection,'' in <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proc. DCASE</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.Â J. Rennie, P.Â Aarabi, and B.Â J. Frey, ``Variational probabilistic speech separation using microphone arrays,'' <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol.Â 15, no.Â 1, pp. 135â€“149, 2007.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D.Â Vijayasenan, F.Â Valente, and H.Â Bourlard, ``An information theoretic combination of MFCC and TDOA features for speaker diarization,'' <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol.Â 19, no.Â 2, pp. 431â€“438, 2011.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
X.Â Anguera, C.Â Wooters, and J.Â Hernando, ``Acoustic beamforming for speaker diarization of meetings,'' <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol.Â 15, no.Â 7, pp. 2011â€“2022, 2007.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
W.Â He, L.Â Lu, B.Â Zhang, J.Â Mahadeokar <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Spatial attention for far-field speech recognition with deep beamforming neural networks,'' in <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Barcelona, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z.Â Chen, T.Â Y.Â L. Lu, T.Â Zhou, Z.Â Meng, Y.Â Luo, J.Â Wu, X.Â Xiao, and J.Â Li, ``Continuous speech separation: Dataset and analysis,'' in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Barcelona, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
E.Â J. Hu, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, W.Â Chen <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``LoRA: Low-Rank Adaptation of large language models,'' in <em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A.Â Chang, A.Â Dai, T.Â Funkhouser, M.Â Halber, M.Â NieÃŸner, M.Â Savva, S.Â Song, A.Â Zeng, and Y.Â Zhang, ``Matterport3D: Learning from RGB-D data in indoor environments,'' in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proc. 3DV</em>, Qingdao, 2017.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.07913" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.07914" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.07914">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.07914" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.07915" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:54:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
