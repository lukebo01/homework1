<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.16677] Crossmodal ASR Error Correction with Discrete Speech Units</title><meta property="og:description" content="ASR remains unsatisfactory in scenarios where the speaking style diverges from that used to train ASR systems, resulting in erroneous transcripts. To address this, ASR Error Correction (AEC), a post-ASR processing appr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Crossmodal ASR Error Correction with Discrete Speech Units">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Crossmodal ASR Error Correction with Discrete Speech Units">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.16677">

<!--Generated on Wed Jun  5 17:29:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Crossmodal ASR Error Correction with Discrete Speech Units</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">ASR remains unsatisfactory in scenarios where the speaking style diverges from that used to train ASR systems, resulting in erroneous transcripts. To address this, ASR Error Correction (AEC), a post-ASR processing approach, is required. In this work, we tackle an understudied issue: the <span id="id1.id1.1" class="ltx_text ltx_font_smallcaps">Low-Resource Out-of-Domain</span> (LROOD) problem, by investigating crossmodal AEC on very limited downstream data with 1-best hypothesis transcription. We explore pre-training and fine-tuning strategies and uncover an ASR domain discrepancy phenomenon, shedding light on appropriate training schemes for LROOD data. Moreover, we propose the incorporation of discrete speech units to align with and enhance the word embeddings for improving AEC quality. Results from multiple corpora and several evaluation metrics demonstrate the feasibility and efficacy of our proposed AEC approach on LROOD data as well as its generalizability and superiority on large-scale data. Finally, a study on speech emotion recognition confirms that our model produces ASR error-robust transcripts suitable for downstream applications.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
ASR Error Correction, Discrete Speech Units, Low-Resource Speech, Out-of-Domain Data</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past decade, the field of Automatic Speech Recognition (ASR) has made significant progress, driven by improvements in computing resources and training schemes, as well as the availability of huge-amount data. In particular, speech foundation models have demonstrated excellent performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, pushing the boundaries of ASR performance to new heights. In addition, their learned representations have proven valuable not only for ASR but also for various downstream applications and scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, in some situations, the speech data does not have much in common with the data used to train the ASR systems, resulting in an out-of-domain problem. For example, the acoustic characteristics of emotional, pathological, or children’s speech could contain irregular, disfluent, and ambiguous patterns, which are different from and unexpected in the speech found in audiobooks or general speaking scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. As a result, when ASR is applied to these downstream tasks (e.g., emotion or depression detection), the transcription is of poor quality and difficult to use <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To further improve the performance of ASR, ASR Error Correction (AEC) approaches have been proposed, allowing for post-processing without modifying the acoustic model. Traditionally, researchers trained an external language model to be incorporated into the ASR system for re-scoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. More recently, there has been a trend toward using generative error correction via Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, replacing traditional language models. Additionally, end-to-end AEC, which maps erroneous transcripts to ground-truth text using a Sequence-to-Sequence (S2S) approach, has become prevalent in scenarios where ASR is treated as a black box <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Furthermore, some work has used both acoustic information and ASR hypotheses as input instead of text-only data, achieving crossmodal AEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite these advances, AEC is still a challenging task, especially for <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Low-Resource Out-of-Domain (LROOD) data</span>. Therefore, we explore this relatively unexplored aspect, aiming to provide a comprehensive analysis toward a better understanding of AEC, as well as to improve the crossmodal AEC using Discrete Speech Units (DSUs). The exploration steps with respective research problems and hypotheses are as follows.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> While S2S models have been established for AEC, research on LROOD scenarios is limited. <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">Many previous studies were performed on the same large corpus without considering the LROOD problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, leaving challenges remain such as determining effective Pre-Training (PT) and Fine-Tuning (FT) strategies with LROOD data</span>. Therefore, we compare AEC performance with and without PT or FT on LROOD data using an S2S model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> None of the prior works has considered the characteristics of the ASR models that are the source of transcript generation. Moreover, although some studies have used data augmentation to produce more erroneous sentences for LROOD downstream corpora for AEC training, we argue that such arbitrary augmentation is unreliable because the error patterns of the augmented data differ from the original ASR errors. <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">We hypothesize that different ASR models may produce distinct patterns of ASR errors (e.g., some may have more insertions, substitutions, or deletions, and some may remove or retain disfluencies), which requires that the AEC model be trained for corresponding ASR domain errors</span>. Thus, through a comparative analysis using transcripts obtained from different ASR models with nearly the same WER, we investigate this issue and refer to it as <span id="S1.p6.1.3" class="ltx_text ltx_font_smallcaps">asr domain discrepancy</span>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">3)</span> Acoustic information has proven useful for crossmodal AEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, but <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">it is not always possible to acquire audio sources for the PT stage (e.g., due to privacy or other ethical issues)</span>. Therefore, determining how to better incorporate audio features and which acoustic features are useful remains an open question, considering high-WER speech usually contains low-quality audio that can introduce distortions into the crossmodal training. To address this, we improve crossmodal AEC by incorporating DSUs only in the FT stage, representing a resource-efficient and effort-saving approach.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">4)</span> <span id="S1.p8.1.2" class="ltx_text ltx_font_italic">Very few studies have applied corrected ASR transcripts to downstream tasks to evaluate AEC extrinsically</span>. Hence, we conduct Speech Emotion Recognition (SER) using the transcripts corrected by our proposed AEC approach, validating its potential for downstream applications.</p>
</div>
<div id="S1.p9" class="ltx_para ltx_noindent">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text ltx_font_bold">Related Work</span>: To our knowledge, we are the first to address the LROOD problem in AEC. The closest works are <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which fused audio into the language model as crossmodal AEC. They either fine-tuned models on large downstream data or used the same corpus for all phases of model training. This work, however, proposes DSU fusion in FT, provides insights for tasks that require high-quality transcripts yet are constrained by limited resources.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Corpora, ASR Models, and Metrics</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Two corpora are primarily used in this work: Common Voice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Common Voice is one of the largest publicly available multilingual and diverse speech datasets. We adopt its 13.0 English version, using 150k utterances from the training set and the entire test set, bringing the total number to 166k. IEMOCAP is an acted corpus consisting of dyadic sessions in which actors perform improvisations or scripted scenarios specifically selected to elicit emotional expression. We follow prior research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, using four basic emotions with 5,500 utterances whose transcripts are not empty.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">CMU-MOSI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and MSP-Podcast <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> are also used for validation, with results presented, but detailed analysis is omitted due to limited space. CMU-MOSI contains only 2,199 samples, which is approximately half the size of IEMOCAP. We randomly select 1,800 samples to fine-tune our AEC model and the remaining samples for testing. For MSP-Podcast, we adopt its Odyssey 2024 emotion challenge version, which contains a training set of 68,119 samples and a test set of 19,815 samples, making it approximately 16 times larger than IEMOCAP. These two corpora are used for: <span id="S2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1<span id="S2.p2.1.1.1" class="ltx_text ltx_font_upright">)</span></span> confirming the generalizability of our approach and further proving its effectiveness; and <span id="S2.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> investigating the impact of data size (i.e., how our performance varies with different amounts of FT data).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">For the ASR models, we use <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">Wav2Vec 2.0</span> (<span id="S2.p3.1.2" class="ltx_text ltx_font_italic">W2V2</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in its <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">base-960h</span> version, a <span id="S2.p3.1.4" class="ltx_text ltx_font_italic">Conformer</span> model (<span id="S2.p3.1.5" class="ltx_text ltx_font_italic">CONF</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> from <span id="S2.p3.1.6" class="ltx_text ltx_font_italic">ESPnet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and the <span id="S2.p3.1.7" class="ltx_text ltx_font_italic">Whisper</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in its <span id="S2.p3.1.8" class="ltx_text ltx_font_italic">tiny.en</span> version. We combine the transcripts of <span id="S2.p3.1.9" class="ltx_text ltx_font_italic">W2V2</span>, <span id="S2.p3.1.10" class="ltx_text ltx_font_italic">CONF</span>, and the ground truth, resulting in a mixture of different system error types that mimic the transcript of a random ASR system (<span id="S2.p3.1.11" class="ltx_text ltx_font_italic">Random</span>). This mixture is then compared with the <span id="S2.p3.1.12" class="ltx_text ltx_font_italic">Whisper</span>-based transcript with a comparable WER to investigate the ASR domain discrepancy problem in AEC (see Sec. <a href="#S3.SS1.SSS2" title="3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_italic">Whisper</span> is run on all four corpora, yielding satisfactory WERs (see Table <a href="#S2.T1" title="Table 1 ‣ 2 Corpora, ASR Models, and Metrics ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) for the following reasons: <span id="S2.p4.1.2" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> the WERs indicate that the corpora fall outside the domain used for training <span id="S2.p4.1.3" class="ltx_text ltx_font_italic">Whisper</span>, aligning with our goal to study the OOD problem; <span id="S2.p4.1.4" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> the WER of Common Voice (for PT) is close to the others (for FT), ensuring the error ratios are consistent in PT and FT. Otherwise in PT, too many errors can result in a serious over-correction problem in subsequent FT and inference phases (which generally occurs in OOD scenarios, as we have observed in our experiments), while too few errors may lead to insufficient learning of error-gold pairs. This setting was usually ignored in the literature, where many studies trained the AEC model on Librispeech with less than 10% WER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, thereby hindering their generalizability. Subsequently, to discover the ASR domain discrepancy problem, <span id="S2.p4.1.5" class="ltx_text ltx_font_italic">which is a new concept presented by this work</span>, we create the transcript of IEMOCAP from the <span id="S2.p4.1.6" class="ltx_text ltx_font_italic">Random</span> model for comparison, which has almost the same WER as that from <span id="S2.p4.1.7" class="ltx_text ltx_font_italic">Whisper</span>. As said, we omit experimental details on CMU-MOSI and MSP-Podcast.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>WERs (%) of the ASR transcripts.</figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.3.1" class="ltx_tr">
<td id="S2.T1.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.3.1.1.1" class="ltx_text ltx_font_bold">ASR Model</span></td>
<td id="S2.T1.3.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.3.1.2.1" class="ltx_text ltx_font_bold">Corpus</span></td>
<td id="S2.T1.3.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.3.1.3.1" class="ltx_text ltx_font_bold">WER</span></td>
</tr>
<tr id="S2.T1.3.2" class="ltx_tr">
<td id="S2.T1.3.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S2.T1.3.2.1.1" class="ltx_text ltx_font_italic">Whisper</span></td>
<td id="S2.T1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">Common Voice</td>
<td id="S2.T1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">19.11</td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_left">IEMOCAP</td>
<td id="S2.T1.3.3.2" class="ltx_td ltx_align_left">17.18</td>
</tr>
<tr id="S2.T1.3.4" class="ltx_tr">
<td id="S2.T1.3.4.1" class="ltx_td"></td>
<td id="S2.T1.3.4.2" class="ltx_td ltx_align_left">CMU-MOSI</td>
<td id="S2.T1.3.4.3" class="ltx_td ltx_align_left">17.84</td>
</tr>
<tr id="S2.T1.3.5" class="ltx_tr">
<td id="S2.T1.3.5.1" class="ltx_td"></td>
<td id="S2.T1.3.5.2" class="ltx_td ltx_align_left">MSP-Podcast</td>
<td id="S2.T1.3.5.3" class="ltx_td ltx_align_left">17.65</td>
</tr>
<tr id="S2.T1.3.6" class="ltx_tr">
<td id="S2.T1.3.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S2.T1.3.6.1.1" class="ltx_text ltx_font_italic">Random</span></td>
<td id="S2.T1.3.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">IEMOCAP</td>
<td id="S2.T1.3.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">17.12</td>
</tr>
</table>
</figure>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">In the following experiments, we employ three metrics to evaluate AEC performance: WER, BLEU, and GLEU. Unlike WER, which examines individual words, BLEU considers n-grams as units for further validating the quality of corrected transcripts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Additionally, GLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is designed with a specific focus on per-sentence reward objectives, addressing certain limitations associated with applying BLEU to individual sentences. We believe that BLEU and GLEU analyze a broader word span containing context for downstream tasks to infer syntactic and semantic information. Utilizing all three metrics, we aim to comprehensively assess AEC quality from different perspectives.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach, Experiments, and Results</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.16677/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="495" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.4.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Architecture of our crossmodal AEC with discrete speech unit (<span id="S3.F1.5.2" class="ltx_text" style="color:#FFBFBF;">Pink</span>: trainable; <span id="S3.F1.6.3" class="ltx_text" style="color:#00FFFF;">Blue</span>: frozen).</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The architecture of our proposed AEC approach is shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. On one path, the audio input is transcribed by the ASR model into a textual transcript, which is then tokenized for the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">RoBERTa-base</span> encoder to generate word embeddings. On the other path, the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">HuBERT</span> encoder produces Self-Supervised Representations (SSR) from the audio input, followed by mean pooling to generate SSR-based Acoustic Word Embeddings (AWEs) as DSUs. Note that the Montreal Forced Aligner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> was used on the ASR transcripts beforehand to determine the word boundaries for mean pooling. Next, cross-attention aligns the AWEs and word embeddings to obtain the acoustic-enhanced word embeddings for the Transformer decoder to produce corrected word tokens.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Crossmodal AEC on LROOD Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We first investigate PT and FT without incorporating audio information, revealing the ASR domain discrepancy problem mentioned earlier. Subsequently, we incorporate different acoustic features and propose the use of DSUs for better audio-text alignment to generate corrected word tokens.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Pre-Training &amp; Fine-Tuning</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">To pre-train the AEC model, 166k samples from Common Voice were recognized by <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Whisper</span>, with 1,000 random samples held out as the test set, and the rest for training and validation with an 80%-20% split. The training aims to recover the gold transcripts from the ASR output. With a batch size of 256, an initial learning rate of 1e-5, and the Adam optimizer, we train the model for 30 epochs using cross-entropy loss and select the best checkpoint based on WER as the evaluation metric. Decoding is performed using beam search with a size of 5. The S2S backbone model is adopted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Performance on the test set is shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1.1 Pre-Training &amp; Fine-Tuning ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>AEC Performance on the test set of Common Voice.</figcaption>
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:223.9pt;height:50.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.4pt,1.9pt) scale(0.93,0.93) ;">
<table id="S3.T2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.1" class="ltx_tr">
<td id="S3.T2.3.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">WER</span></td>
<td id="S3.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S3.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">GLEU</span></td>
</tr>
<tr id="S3.T2.3.1.2" class="ltx_tr">
<td id="S3.T2.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.3.1.2.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T2.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">19.30</td>
<td id="S3.T2.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">70.56</td>
<td id="S3.T2.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">71.24</td>
</tr>
<tr id="S3.T2.3.1.3" class="ltx_tr">
<td id="S3.T2.3.1.3.1" class="ltx_td ltx_align_left ltx_border_b">Best checkpoint</td>
<td id="S3.T2.3.1.3.2" class="ltx_td ltx_align_center ltx_border_b">18.19</td>
<td id="S3.T2.3.1.3.3" class="ltx_td ltx_align_center ltx_border_b">72.14</td>
<td id="S3.T2.3.1.3.4" class="ltx_td ltx_align_center ltx_border_b">72.45</td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Furthermore, to prevent the over-correction problem, we continue training this saved checkpoint on TED transcriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to learn to copy the gold transcripts (i.e., ground truth <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mo stretchy="false" id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">\rightarrow</annotation></semantics></math> ground truth) and potentially enhance its domain robustness. This continue-training lasts for two epochs, ensuring it does not overfit while maintaining correction stability. We save the checkpoint<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Publicly available: https://github.com/yc-li20/Crossmodal_AEC</span></span></span> as the base model for subsequent experiments.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Next, we fine-tune this model on the training set of IEMOCAP for 40 epochs with a batch size of 64, an initial learning rate of 2e-5 (excluding the parameters of “bias” and “LayerNorm.weight”), an epsilon of 1e-8, and the Adam optimizer. Following the standard five-fold split of IEMOCAP, the FT is performed five times (each time using four folds for FT and one for testing), and the final performance is reported based on the transcript composed of the corrected results obtained from five instances. The details of FT on the other two corpora are omitted yet the final results will be presented in Sec. <a href="#S3.SS1.SSS4" title="3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.4</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.5.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Comparison results on IEMOCAP of w/ and w/o pre-training or fine-tuning.</figcaption>
<div id="S3.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:178.3pt;height:83.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,3.1pt) scale(0.93,0.93) ;">
<table id="S3.T3.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.3.3.3.4.1" class="ltx_text ltx_font_bold">PT</span></td>
<td id="S3.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.3.3.3.5.1" class="ltx_text ltx_font_bold">FT</span></td>
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.T3.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T3.2.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.1.m1.1b"><ci id="S3.T3.2.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.T3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T3.3.3.3.3.1.m1.1.1" xref="S3.T3.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.3.1.m1.1b"><ci id="S3.T3.3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.T3.3.3.4" class="ltx_tr">
<td id="S3.T3.3.3.4.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S3.T3.3.3.4.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T3.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">17.18</td>
<td id="S3.T3.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">76.56</td>
<td id="S3.T3.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">75.29</td>
</tr>
<tr id="S3.T3.3.3.5" class="ltx_tr">
<td id="S3.T3.3.3.5.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.3.5.1.1" class="ltx_ERROR undefined">\hdashline</span>✓</td>
<td id="S3.T3.3.3.5.2" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T3.3.3.5.3" class="ltx_td ltx_align_center">17.14</td>
<td id="S3.T3.3.3.5.4" class="ltx_td ltx_align_center">76.61</td>
<td id="S3.T3.3.3.5.5" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="S3.T3.3.3.6" class="ltx_tr">
<td id="S3.T3.3.3.6.1" class="ltx_td ltx_align_center">✗</td>
<td id="S3.T3.3.3.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T3.3.3.6.3" class="ltx_td ltx_align_center">17.08</td>
<td id="S3.T3.3.3.6.4" class="ltx_td ltx_align_center">77.01</td>
<td id="S3.T3.3.3.6.5" class="ltx_td ltx_align_center">75.52</td>
</tr>
<tr id="S3.T3.3.3.7" class="ltx_tr">
<td id="S3.T3.3.3.7.1" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S3.T3.3.3.7.2" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S3.T3.3.3.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.3.3.7.3.1" class="ltx_text ltx_font_bold">16.40</span></td>
<td id="S3.T3.3.3.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.3.3.7.4.1" class="ltx_text ltx_font_bold">78.00</span></td>
<td id="S3.T3.3.3.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.3.3.7.5.1" class="ltx_text ltx_font_bold">76.58</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ 3.1.1 Pre-Training &amp; Fine-Tuning ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that without FT, a pre-trained model cannot perform well. The improvement is hardly noticeable on IEMOCAP, whereas the improvement is significant on the test set of Common Voice (Table <a href="#S3.T2" title="Table 2 ‣ 3.1.1 Pre-Training &amp; Fine-Tuning ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), despite their original ASR transcripts being of similar quality (Table <a href="#S2.T1" title="Table 1 ‣ 2 Corpora, ASR Models, and Metrics ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This is likely due to the domain discrepancy between Common Voice and IEMOCAP, which results in the model pre-trained on the former being unable to recognize some erroneous OOD words in the latter. However, even without PT, the model can still improve transcript quality after FT<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Technically, since there is no PT on Common Voice, it is not appropriate to use the term “FT” as the model is directly trained on IEMOCAP. However, we keep “FT” here for consistency.</span></span></span> on LROOD data.</p>
</div>
<div id="S3.SS1.SSS1.p5" class="ltx_para">
<p id="S3.SS1.SSS1.p5.1" class="ltx_p">Our best result comes from using both PT and FT, which indicates that the capacity learned during PT is activated and enhanced by FT. This combination well alleviates the LROOD problem.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>ASR Domain Discrepancy Problem</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">To study the impact of ASR domain discrepancy, a concept we define as novel, we conduct experiments by FT the AEC model on the output from another ASR system. Specifically, we use the transcript generated by <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Random</span> and compare it to the transcript generated by <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">Whisper</span>. The results are presented in Table <a href="#S3.SS1.SSS2" title="3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">We can observe that without PT on the transcript of Common Voice (which is generated by <span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Whisper</span>), the difference in the metric values remains small after FT<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>, compared to their original ASR transcripts. However, this pattern disappears with PT, as the transcript quality from <span id="S3.SS1.SSS2.p2.1.2" class="ltx_text ltx_font_italic">Whisper</span> becomes better than that from <span id="S3.SS1.SSS2.p2.1.3" class="ltx_text ltx_font_italic">Random</span>, highlighting the detrimental impact of ASR domain discrepancy. This phenomenon suggests that to correct transcripts from an ASR model, it is crucial to use the same ASR model as that used in PT (i.e., to continue using the same ASR model in both PT and FT). Nevertheless, the transcript quality from <span id="S3.SS1.SSS2.p2.1.4" class="ltx_text ltx_font_italic">Random</span> still improves, indicating that PT on a large corpus, even if its transcript is from a different ASR model, is still indispensable in LROOD scenarios.</p>
</div>
<figure id="S3.SS1.SSS2.3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.SS1.SSS2.3.4.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Comparison results on IEMOCAP of fine-tuning on transcript generated by different ASR models.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.SS1.SSS2.3.3" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:183.9pt;height:167.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.9pt,6.3pt) scale(0.93,0.93) ;">
<table id="S3.SS1.SSS2.3.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.SS1.SSS2.3.3.3.3" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.SS1.SSS2.3.3.3.3.4.1" class="ltx_text ltx_font_bold">ASR Model</span></td>
<td id="S3.SS1.SSS2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.SS1.SSS2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS2.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1b"><ci id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.SS1.SSS2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.SS1.SSS2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1.1" xref="S3.SS1.SSS2.2.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1b"><ci id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.2.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.SS1.SSS2.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.SS1.SSS2.3.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1.1" xref="S3.SS1.SSS2.3.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1b"><ci id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1.1.cmml" xref="S3.SS1.SSS2.3.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.3.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.4" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4">Original ASR transcript</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.5" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.5.1" class="ltx_td ltx_align_left"><span id="S3.SS1.SSS2.3.3.3.5.1.1" class="ltx_text ltx_font_italic">Whisper</span></td>
<td id="S3.SS1.SSS2.3.3.3.5.2" class="ltx_td ltx_align_center">17.18</td>
<td id="S3.SS1.SSS2.3.3.3.5.3" class="ltx_td ltx_align_center">76.56</td>
<td id="S3.SS1.SSS2.3.3.3.5.4" class="ltx_td ltx_align_center">75.29</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.6" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.6.1" class="ltx_td ltx_align_left"><span id="S3.SS1.SSS2.3.3.3.6.1.1" class="ltx_text ltx_font_italic">Random</span></td>
<td id="S3.SS1.SSS2.3.3.3.6.2" class="ltx_td ltx_align_center">17.12</td>
<td id="S3.SS1.SSS2.3.3.3.6.3" class="ltx_td ltx_align_center">76.64</td>
<td id="S3.SS1.SSS2.3.3.3.6.4" class="ltx_td ltx_align_center">75.38</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.7" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.7.1" class="ltx_td ltx_align_left" colspan="4">
<span id="S3.SS1.SSS2.3.3.3.7.1.1" class="ltx_ERROR undefined">\hdashline</span>     ✗pre-training</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.8" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.8.1" class="ltx_td ltx_align_left"><span id="S3.SS1.SSS2.3.3.3.8.1.1" class="ltx_text ltx_font_italic">Whisper</span></td>
<td id="S3.SS1.SSS2.3.3.3.8.2" class="ltx_td ltx_align_center">17.08</td>
<td id="S3.SS1.SSS2.3.3.3.8.3" class="ltx_td ltx_align_center">77.01</td>
<td id="S3.SS1.SSS2.3.3.3.8.4" class="ltx_td ltx_align_center">75.52</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.9" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.9.1" class="ltx_td ltx_align_left"><span id="S3.SS1.SSS2.3.3.3.9.1.1" class="ltx_text ltx_font_italic">Random</span></td>
<td id="S3.SS1.SSS2.3.3.3.9.2" class="ltx_td ltx_align_center">17.03</td>
<td id="S3.SS1.SSS2.3.3.3.9.3" class="ltx_td ltx_align_center">77.08</td>
<td id="S3.SS1.SSS2.3.3.3.9.4" class="ltx_td ltx_align_center">75.61</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.10" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.10.1" class="ltx_td ltx_align_left" colspan="4">
<span id="S3.SS1.SSS2.3.3.3.10.1.1" class="ltx_ERROR undefined">\hdashline</span>     ✓pre-training</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.11" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.11.1" class="ltx_td ltx_align_left"><span id="S3.SS1.SSS2.3.3.3.11.1.1" class="ltx_text ltx_font_italic">Whisper</span></td>
<td id="S3.SS1.SSS2.3.3.3.11.2" class="ltx_td ltx_align_center">16.40</td>
<td id="S3.SS1.SSS2.3.3.3.11.3" class="ltx_td ltx_align_center">78.00</td>
<td id="S3.SS1.SSS2.3.3.3.11.4" class="ltx_td ltx_align_center">76.58</td>
</tr>
<tr id="S3.SS1.SSS2.3.3.3.12" class="ltx_tr">
<td id="S3.SS1.SSS2.3.3.3.12.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.SS1.SSS2.3.3.3.12.1.1" class="ltx_text ltx_font_italic">Random</span></td>
<td id="S3.SS1.SSS2.3.3.3.12.2" class="ltx_td ltx_align_center ltx_border_b">16.54</td>
<td id="S3.SS1.SSS2.3.3.3.12.3" class="ltx_td ltx_align_center ltx_border_b">77.57</td>
<td id="S3.SS1.SSS2.3.3.3.12.4" class="ltx_td ltx_align_center ltx_border_b">76.42</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<section id="S3.SS1.SSS3" class="ltx_subsubsection ltx_figure_panel">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Incorporation of Discrete Speech Units</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">So far, we have investigated how PT and FT contribute to text-only S2S AEC. To further improve the quality of error correction, we study the incorporation of acoustic information. Previous studies usually incorporated acoustic information in all stages—PT, FT, and testing—and only utilized continuous features such as Mel-spectrogram or raw SSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, we argue that these practices do not apply to LROOD scenarios for the following reasons:</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p"><span id="S3.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> The audio source of large-scale PT data is not always accessible due to privacy or other ethical concerns. <span id="S3.SS1.SSS3.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> The high-WER OOD speech usually contains low-quality audio that can introduce acoustic distortions (e.g., prosody variation or noise) into crossmodal training. <span id="S3.SS1.SSS3.p2.1.3" class="ltx_text ltx_font_bold ltx_font_italic">3)</span> It is challenging to align discrete word embeddings with continuous audio features. To this end, we propose to discretize the audio features to create DSUs and avoid incorporating such acoustic information in PT, making it a resource-efficient and effort-saving approach.</p>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p">We utilize AWEs, which are fixed-dimensional vectors representing variable-length spoken word segments as DSUs. These vectors map acoustic features extracted from audio signals to vectors, where similar words or linguistic units have similar embeddings in the vector space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. AWEs can capture information about phonetics and other acoustic aspects of speech, offering promising potential for word discrimination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS3.p4" class="ltx_para">
<p id="S3.SS1.SSS3.p4.1" class="ltx_p">Following recent studies on the analysis of AWEs from self-supervised speech models, we use SSR from <span id="S3.SS1.SSS3.p4.1.1" class="ltx_text ltx_font_italic">HuBERT</span> with mean pooling followed by forced alignment to find the word boundary, as this practice has been shown to be competitive with the state-of-the-art on English AWEs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. On the other hand, we also use Mel-spectrogram and continuous raw SSR for comparison. After a layer-wise analysis (omitted due to space), we use AWEs from <span id="S3.SS1.SSS3.p4.1.2" class="ltx_text ltx_font_italic">HuBERT</span> layer 7 and raw SSR from <span id="S3.SS1.SSS3.p4.1.3" class="ltx_text ltx_font_italic">HuBERT</span> layer 8 as they performed the best among all layers, respectively. This aligns with a previous finding that <span id="S3.SS1.SSS3.p4.1.4" class="ltx_text ltx_font_italic">HuBERT</span> encodes the most word information between the middle layer and the last layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS3.p5" class="ltx_para">
<p id="S3.SS1.SSS3.p5.10" class="ltx_p">To incorporate the DSUs, we set the maximum sequence length as that of corresponding word embeddings and 0-pad the short sequence. To incorporate continuous features for comparison, we first downsample them to the same sequence length as the word embeddings using a fast Fourier transform. Unlike <span id="S3.SS1.SSS3.p5.10.1" class="ltx_text ltx_font_italic">HuBERT</span>, which has the same feature dimension of 768 as <span id="S3.SS1.SSS3.p5.10.2" class="ltx_text ltx_font_italic">RoBERTa</span>, we use a feed-forward layer for Mel-spectrogram to expand its dimension to this size. After such pre-processing, we implement cross-attention to align acoustic features with word embeddings:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\displaystyle A^{\prime}=Attn(Q_{w},K_{a},V_{a})=softmax(\frac{Q_{w}K_{a}^{T}}{\sqrt{d_{k}}})V_{a}" display="inline"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><msup id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml"><mi id="S3.E1.m1.4.4.5.2" xref="S3.E1.m1.4.4.5.2.cmml">A</mi><mo id="S3.E1.m1.4.4.5.3" xref="S3.E1.m1.4.4.5.3.cmml">′</mo></msup><mo id="S3.E1.m1.4.4.6" xref="S3.E1.m1.4.4.6.cmml">=</mo><mrow id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.5" xref="S3.E1.m1.4.4.3.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.3.4" xref="S3.E1.m1.4.4.3.4.cmml">​</mo><mi id="S3.E1.m1.4.4.3.6" xref="S3.E1.m1.4.4.3.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.3.4a" xref="S3.E1.m1.4.4.3.4.cmml">​</mo><mi id="S3.E1.m1.4.4.3.7" xref="S3.E1.m1.4.4.3.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.3.4b" xref="S3.E1.m1.4.4.3.4.cmml">​</mo><mi id="S3.E1.m1.4.4.3.8" xref="S3.E1.m1.4.4.3.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.3.4c" xref="S3.E1.m1.4.4.3.4.cmml">​</mo><mrow id="S3.E1.m1.4.4.3.3.3" xref="S3.E1.m1.4.4.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.3.3.3.4" xref="S3.E1.m1.4.4.3.3.4.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">Q</mi><mi id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">w</mi></msub><mo id="S3.E1.m1.4.4.3.3.3.5" xref="S3.E1.m1.4.4.3.3.4.cmml">,</mo><msub id="S3.E1.m1.3.3.2.2.2.2" xref="S3.E1.m1.3.3.2.2.2.2.cmml"><mi id="S3.E1.m1.3.3.2.2.2.2.2" xref="S3.E1.m1.3.3.2.2.2.2.2.cmml">K</mi><mi id="S3.E1.m1.3.3.2.2.2.2.3" xref="S3.E1.m1.3.3.2.2.2.2.3.cmml">a</mi></msub><mo id="S3.E1.m1.4.4.3.3.3.6" xref="S3.E1.m1.4.4.3.3.4.cmml">,</mo><msub id="S3.E1.m1.4.4.3.3.3.3" xref="S3.E1.m1.4.4.3.3.3.3.cmml"><mi id="S3.E1.m1.4.4.3.3.3.3.2" xref="S3.E1.m1.4.4.3.3.3.3.2.cmml">V</mi><mi id="S3.E1.m1.4.4.3.3.3.3.3" xref="S3.E1.m1.4.4.3.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.E1.m1.4.4.3.3.3.7" xref="S3.E1.m1.4.4.3.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.7" xref="S3.E1.m1.4.4.7.cmml">=</mo><mrow id="S3.E1.m1.4.4.8" xref="S3.E1.m1.4.4.8.cmml"><mi id="S3.E1.m1.4.4.8.2" xref="S3.E1.m1.4.4.8.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.3" xref="S3.E1.m1.4.4.8.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1a" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.4" xref="S3.E1.m1.4.4.8.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1b" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.5" xref="S3.E1.m1.4.4.8.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1c" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.6" xref="S3.E1.m1.4.4.8.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1d" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.7" xref="S3.E1.m1.4.4.8.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1e" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mi id="S3.E1.m1.4.4.8.8" xref="S3.E1.m1.4.4.8.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1f" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.8.9.2" xref="S3.E1.m1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.8.9.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mstyle displaystyle="true" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mfrac id="S3.E1.m1.1.1a" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.2.2.2" xref="S3.E1.m1.1.1.2.2.2.cmml">Q</mi><mi id="S3.E1.m1.1.1.2.2.3" xref="S3.E1.m1.1.1.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">​</mo><msubsup id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2.2" xref="S3.E1.m1.1.1.2.3.2.2.cmml">K</mi><mi id="S3.E1.m1.1.1.2.3.2.3" xref="S3.E1.m1.1.1.2.3.2.3.cmml">a</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">d</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac></mstyle><mo stretchy="false" id="S3.E1.m1.4.4.8.9.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.8.1g" xref="S3.E1.m1.4.4.8.1.cmml">​</mo><msub id="S3.E1.m1.4.4.8.10" xref="S3.E1.m1.4.4.8.10.cmml"><mi id="S3.E1.m1.4.4.8.10.2" xref="S3.E1.m1.4.4.8.10.2.cmml">V</mi><mi id="S3.E1.m1.4.4.8.10.3" xref="S3.E1.m1.4.4.8.10.3.cmml">a</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><and id="S3.E1.m1.4.4a.cmml" xref="S3.E1.m1.4.4"></and><apply id="S3.E1.m1.4.4b.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.6.cmml" xref="S3.E1.m1.4.4.6"></eq><apply id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.5.1.cmml" xref="S3.E1.m1.4.4.5">superscript</csymbol><ci id="S3.E1.m1.4.4.5.2.cmml" xref="S3.E1.m1.4.4.5.2">𝐴</ci><ci id="S3.E1.m1.4.4.5.3.cmml" xref="S3.E1.m1.4.4.5.3">′</ci></apply><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><times id="S3.E1.m1.4.4.3.4.cmml" xref="S3.E1.m1.4.4.3.4"></times><ci id="S3.E1.m1.4.4.3.5.cmml" xref="S3.E1.m1.4.4.3.5">𝐴</ci><ci id="S3.E1.m1.4.4.3.6.cmml" xref="S3.E1.m1.4.4.3.6">𝑡</ci><ci id="S3.E1.m1.4.4.3.7.cmml" xref="S3.E1.m1.4.4.3.7">𝑡</ci><ci id="S3.E1.m1.4.4.3.8.cmml" xref="S3.E1.m1.4.4.3.8">𝑛</ci><vector id="S3.E1.m1.4.4.3.3.4.cmml" xref="S3.E1.m1.4.4.3.3.3"><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">𝑄</ci><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">𝑤</ci></apply><apply id="S3.E1.m1.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2.2.2">𝐾</ci><ci id="S3.E1.m1.3.3.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.2.2.3">𝑎</ci></apply><apply id="S3.E1.m1.4.4.3.3.3.3.cmml" xref="S3.E1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.3.3.3.1.cmml" xref="S3.E1.m1.4.4.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.4.4.3.3.3.3.2.cmml" xref="S3.E1.m1.4.4.3.3.3.3.2">𝑉</ci><ci id="S3.E1.m1.4.4.3.3.3.3.3.cmml" xref="S3.E1.m1.4.4.3.3.3.3.3">𝑎</ci></apply></vector></apply></apply><apply id="S3.E1.m1.4.4c.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.7.cmml" xref="S3.E1.m1.4.4.7"></eq><share href="#S3.E1.m1.4.4.3.cmml" id="S3.E1.m1.4.4d.cmml" xref="S3.E1.m1.4.4"></share><apply id="S3.E1.m1.4.4.8.cmml" xref="S3.E1.m1.4.4.8"><times id="S3.E1.m1.4.4.8.1.cmml" xref="S3.E1.m1.4.4.8.1"></times><ci id="S3.E1.m1.4.4.8.2.cmml" xref="S3.E1.m1.4.4.8.2">𝑠</ci><ci id="S3.E1.m1.4.4.8.3.cmml" xref="S3.E1.m1.4.4.8.3">𝑜</ci><ci id="S3.E1.m1.4.4.8.4.cmml" xref="S3.E1.m1.4.4.8.4">𝑓</ci><ci id="S3.E1.m1.4.4.8.5.cmml" xref="S3.E1.m1.4.4.8.5">𝑡</ci><ci id="S3.E1.m1.4.4.8.6.cmml" xref="S3.E1.m1.4.4.8.6">𝑚</ci><ci id="S3.E1.m1.4.4.8.7.cmml" xref="S3.E1.m1.4.4.8.7">𝑎</ci><ci id="S3.E1.m1.4.4.8.8.cmml" xref="S3.E1.m1.4.4.8.8">𝑥</ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.4.4.8.9.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.4.4.8.9.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><apply id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.2.2.2">𝑄</ci><ci id="S3.E1.m1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.2.2.3">𝑤</ci></apply><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3">superscript</csymbol><apply id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.2.3.2.2">𝐾</ci><ci id="S3.E1.m1.1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.1.2.3.2.3">𝑎</ci></apply><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><root id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"></root><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑑</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply><apply id="S3.E1.m1.4.4.8.10.cmml" xref="S3.E1.m1.4.4.8.10"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.8.10.1.cmml" xref="S3.E1.m1.4.4.8.10">subscript</csymbol><ci id="S3.E1.m1.4.4.8.10.2.cmml" xref="S3.E1.m1.4.4.8.10.2">𝑉</ci><ci id="S3.E1.m1.4.4.8.10.3.cmml" xref="S3.E1.m1.4.4.8.10.3">𝑎</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\displaystyle A^{\prime}=Attn(Q_{w},K_{a},V_{a})=softmax(\frac{Q_{w}K_{a}^{T}}{\sqrt{d_{k}}})V_{a}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p5.9" class="ltx_p">where <math id="S3.SS1.SSS3.p5.1.m1.1" class="ltx_Math" alttext="Q_{t}" display="inline"><semantics id="S3.SS1.SSS3.p5.1.m1.1a"><msub id="S3.SS1.SSS3.p5.1.m1.1.1" xref="S3.SS1.SSS3.p5.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p5.1.m1.1.1.2" xref="S3.SS1.SSS3.p5.1.m1.1.1.2.cmml">Q</mi><mi id="S3.SS1.SSS3.p5.1.m1.1.1.3" xref="S3.SS1.SSS3.p5.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.1.m1.1b"><apply id="S3.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p5.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1.2">𝑄</ci><ci id="S3.SS1.SSS3.p5.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.1.m1.1c">Q_{t}</annotation></semantics></math>, <math id="S3.SS1.SSS3.p5.2.m2.1" class="ltx_Math" alttext="K_{a}" display="inline"><semantics id="S3.SS1.SSS3.p5.2.m2.1a"><msub id="S3.SS1.SSS3.p5.2.m2.1.1" xref="S3.SS1.SSS3.p5.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p5.2.m2.1.1.2" xref="S3.SS1.SSS3.p5.2.m2.1.1.2.cmml">K</mi><mi id="S3.SS1.SSS3.p5.2.m2.1.1.3" xref="S3.SS1.SSS3.p5.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.2.m2.1b"><apply id="S3.SS1.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p5.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1.2">𝐾</ci><ci id="S3.SS1.SSS3.p5.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.2.m2.1c">K_{a}</annotation></semantics></math>, and <math id="S3.SS1.SSS3.p5.3.m3.1" class="ltx_Math" alttext="V_{a}" display="inline"><semantics id="S3.SS1.SSS3.p5.3.m3.1a"><msub id="S3.SS1.SSS3.p5.3.m3.1.1" xref="S3.SS1.SSS3.p5.3.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p5.3.m3.1.1.2" xref="S3.SS1.SSS3.p5.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.SSS3.p5.3.m3.1.1.3" xref="S3.SS1.SSS3.p5.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.3.m3.1b"><apply id="S3.SS1.SSS3.p5.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.3.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p5.3.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1.2">𝑉</ci><ci id="S3.SS1.SSS3.p5.3.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.3.m3.1c">V_{a}</annotation></semantics></math> represent the respective matrix for query (word embeddings), key (acoustic features), and value (acoustic features), <math id="S3.SS1.SSS3.p5.4.m4.1" class="ltx_Math" alttext="d_{k}" display="inline"><semantics id="S3.SS1.SSS3.p5.4.m4.1a"><msub id="S3.SS1.SSS3.p5.4.m4.1.1" xref="S3.SS1.SSS3.p5.4.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p5.4.m4.1.1.2" xref="S3.SS1.SSS3.p5.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS3.p5.4.m4.1.1.3" xref="S3.SS1.SSS3.p5.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.4.m4.1b"><apply id="S3.SS1.SSS3.p5.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.4.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p5.4.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p5.4.m4.1.1.2">𝑑</ci><ci id="S3.SS1.SSS3.p5.4.m4.1.1.3.cmml" xref="S3.SS1.SSS3.p5.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.4.m4.1c">d_{k}</annotation></semantics></math> is the size of a key vector, and <math id="S3.SS1.SSS3.p5.5.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.SS1.SSS3.p5.5.m5.1a"><msup id="S3.SS1.SSS3.p5.5.m5.1.1" xref="S3.SS1.SSS3.p5.5.m5.1.1.cmml"><mi id="S3.SS1.SSS3.p5.5.m5.1.1.2" xref="S3.SS1.SSS3.p5.5.m5.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS3.p5.5.m5.1.1.3" xref="S3.SS1.SSS3.p5.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.5.m5.1b"><apply id="S3.SS1.SSS3.p5.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.5.m5.1.1.1.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p5.5.m5.1.1.2.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1.2">𝐴</ci><ci id="S3.SS1.SSS3.p5.5.m5.1.1.3.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.5.m5.1c">A^{\prime}</annotation></semantics></math> is the word-aligned acoustic features. Next, we add <math id="S3.SS1.SSS3.p5.6.m6.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.SS1.SSS3.p5.6.m6.1a"><msup id="S3.SS1.SSS3.p5.6.m6.1.1" xref="S3.SS1.SSS3.p5.6.m6.1.1.cmml"><mi id="S3.SS1.SSS3.p5.6.m6.1.1.2" xref="S3.SS1.SSS3.p5.6.m6.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS3.p5.6.m6.1.1.3" xref="S3.SS1.SSS3.p5.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.6.m6.1b"><apply id="S3.SS1.SSS3.p5.6.m6.1.1.cmml" xref="S3.SS1.SSS3.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.6.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p5.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p5.6.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p5.6.m6.1.1.2">𝐴</ci><ci id="S3.SS1.SSS3.p5.6.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p5.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.6.m6.1c">A^{\prime}</annotation></semantics></math> and <math id="S3.SS1.SSS3.p5.7.m7.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.SSS3.p5.7.m7.1a"><mi id="S3.SS1.SSS3.p5.7.m7.1.1" xref="S3.SS1.SSS3.p5.7.m7.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.7.m7.1b"><ci id="S3.SS1.SSS3.p5.7.m7.1.1.cmml" xref="S3.SS1.SSS3.p5.7.m7.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.7.m7.1c">W</annotation></semantics></math> for the Transformer decoder with optimizable parameters <math id="S3.SS1.SSS3.p5.8.m8.1" class="ltx_Math" alttext="\theta_{T}" display="inline"><semantics id="S3.SS1.SSS3.p5.8.m8.1a"><msub id="S3.SS1.SSS3.p5.8.m8.1.1" xref="S3.SS1.SSS3.p5.8.m8.1.1.cmml"><mi id="S3.SS1.SSS3.p5.8.m8.1.1.2" xref="S3.SS1.SSS3.p5.8.m8.1.1.2.cmml">θ</mi><mi id="S3.SS1.SSS3.p5.8.m8.1.1.3" xref="S3.SS1.SSS3.p5.8.m8.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.8.m8.1b"><apply id="S3.SS1.SSS3.p5.8.m8.1.1.cmml" xref="S3.SS1.SSS3.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.8.m8.1.1.1.cmml" xref="S3.SS1.SSS3.p5.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p5.8.m8.1.1.2.cmml" xref="S3.SS1.SSS3.p5.8.m8.1.1.2">𝜃</ci><ci id="S3.SS1.SSS3.p5.8.m8.1.1.3.cmml" xref="S3.SS1.SSS3.p5.8.m8.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.8.m8.1c">\theta_{T}</annotation></semantics></math> to generate a corrected version <math id="S3.SS1.SSS3.p5.9.m9.1" class="ltx_Math" alttext="W^{\prime}" display="inline"><semantics id="S3.SS1.SSS3.p5.9.m9.1a"><msup id="S3.SS1.SSS3.p5.9.m9.1.1" xref="S3.SS1.SSS3.p5.9.m9.1.1.cmml"><mi id="S3.SS1.SSS3.p5.9.m9.1.1.2" xref="S3.SS1.SSS3.p5.9.m9.1.1.2.cmml">W</mi><mo id="S3.SS1.SSS3.p5.9.m9.1.1.3" xref="S3.SS1.SSS3.p5.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.9.m9.1b"><apply id="S3.SS1.SSS3.p5.9.m9.1.1.cmml" xref="S3.SS1.SSS3.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p5.9.m9.1.1.1.cmml" xref="S3.SS1.SSS3.p5.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p5.9.m9.1.1.2.cmml" xref="S3.SS1.SSS3.p5.9.m9.1.1.2">𝑊</ci><ci id="S3.SS1.SSS3.p5.9.m9.1.1.3.cmml" xref="S3.SS1.SSS3.p5.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.9.m9.1c">W^{\prime}</annotation></semantics></math>:</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\displaystyle W^{\prime}=\arg\!\max_{W}P(W|addition(A^{\prime},W);\theta_{T})" display="inline"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msup id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml">W</mi><mo id="S3.E2.m1.2.2.3.3" xref="S3.E2.m1.2.2.3.3.cmml">′</mo></msup><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mrow id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml"><mi id="S3.E2.m1.2.2.1.3.1" xref="S3.E2.m1.2.2.1.3.1.cmml">arg</mi><mo id="S3.E2.m1.2.2.1.3a" xref="S3.E2.m1.2.2.1.3.cmml">⁡</mo><mrow id="S3.E2.m1.2.2.1.3.2" xref="S3.E2.m1.2.2.1.3.2.cmml"><munder id="S3.E2.m1.2.2.1.3.2.1" xref="S3.E2.m1.2.2.1.3.2.1.cmml"><mi id="S3.E2.m1.2.2.1.3.2.1.2" xref="S3.E2.m1.2.2.1.3.2.1.2.cmml">max</mi><mi id="S3.E2.m1.2.2.1.3.2.1.3" xref="S3.E2.m1.2.2.1.3.2.1.3.cmml">W</mi></munder><mo lspace="0.167em" id="S3.E2.m1.2.2.1.3.2a" xref="S3.E2.m1.2.2.1.3.2.cmml">⁡</mo><mi id="S3.E2.m1.2.2.1.3.2.2" xref="S3.E2.m1.2.2.1.3.2.2.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.4" xref="S3.E2.m1.2.2.1.1.1.1.4.cmml">W</mi><mo fence="false" id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.2.3.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.4" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.5" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2b" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.6" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2c" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.7" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2d" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.8" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2e" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.9" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2f" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.10" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2g" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><msup id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">A</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">W</mi><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E2.m1.2.2.1.1.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2.2.cmml">θ</mi><mi id="S3.E2.m1.2.2.1.1.1.1.2.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2.3.cmml">T</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3">superscript</csymbol><ci id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2">𝑊</ci><ci id="S3.E2.m1.2.2.3.3.cmml" xref="S3.E2.m1.2.2.3.3">′</ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><apply id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3"><arg id="S3.E2.m1.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.1.3.1"></arg><apply id="S3.E2.m1.2.2.1.3.2.cmml" xref="S3.E2.m1.2.2.1.3.2"><apply id="S3.E2.m1.2.2.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.3.2.1.1.cmml" xref="S3.E2.m1.2.2.1.3.2.1">subscript</csymbol><max id="S3.E2.m1.2.2.1.3.2.1.2.cmml" xref="S3.E2.m1.2.2.1.3.2.1.2"></max><ci id="S3.E2.m1.2.2.1.3.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3.2.1.3">𝑊</ci></apply><ci id="S3.E2.m1.2.2.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.3.2.2">𝑃</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">conditional</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.1.1.4">𝑊</ci><list id="S3.E2.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2.2"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">𝑎</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.4">𝑑</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.5">𝑑</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.6">𝑖</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.7">𝑡</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.8.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.8">𝑖</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.9.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.9">𝑜</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.10.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.10">𝑛</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">𝐴</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">′</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑊</ci></interval></apply><apply id="S3.E2.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2.2">𝜃</ci><ci id="S3.E2.m1.2.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2.2.2.3">𝑇</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle W^{\prime}=\arg\!\max_{W}P(W|addition(A^{\prime},W);\theta_{T})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p5.11" class="ltx_p">The results of fusing acoustic features are shown in Table <a href="#S3.T5" title="Table 5 ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with previous experimental results included for comparison.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.5.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>Result summary on IEMOCAP.</figcaption>
<div id="S3.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.4pt;height:133.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.3pt,5.0pt) scale(0.93,0.93) ;">
<table id="S3.T5.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.3.3" class="ltx_tr">
<td id="S3.T5.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T5.3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T5.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T5.1.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.1.m1.1b"><ci id="S3.T5.1.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.T5.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T5.2.2.2.2.1.m1.1.1" xref="S3.T5.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.2.1.m1.1b"><ci id="S3.T5.2.2.2.2.1.m1.1.1.cmml" xref="S3.T5.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.T5.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T5.3.3.3.3.1.m1.1.1" xref="S3.T5.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.3.3.3.3.1.m1.1b"><ci id="S3.T5.3.3.3.3.1.m1.1.1.cmml" xref="S3.T5.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.T5.3.3.4" class="ltx_tr">
<td id="S3.T5.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T5.3.3.4.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T5.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">17.18</td>
<td id="S3.T5.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">76.56</td>
<td id="S3.T5.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">75.29</td>
</tr>
<tr id="S3.T5.3.3.5" class="ltx_tr">
<td id="S3.T5.3.3.5.1" class="ltx_td ltx_align_left">
<span id="S3.T5.3.3.5.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T5.3.3.5.1.2" class="ltx_text ltx_font_italic">PT</span>
</td>
<td id="S3.T5.3.3.5.2" class="ltx_td ltx_align_center">17.14</td>
<td id="S3.T5.3.3.5.3" class="ltx_td ltx_align_center">76.61</td>
<td id="S3.T5.3.3.5.4" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="S3.T5.3.3.6" class="ltx_tr">
<td id="S3.T5.3.3.6.1" class="ltx_td ltx_align_left"><span id="S3.T5.3.3.6.1.1" class="ltx_text ltx_font_italic">FT</span></td>
<td id="S3.T5.3.3.6.2" class="ltx_td ltx_align_center">17.08</td>
<td id="S3.T5.3.3.6.3" class="ltx_td ltx_align_center">77.01</td>
<td id="S3.T5.3.3.6.4" class="ltx_td ltx_align_center">75.52</td>
</tr>
<tr id="S3.T5.3.3.7" class="ltx_tr">
<td id="S3.T5.3.3.7.1" class="ltx_td ltx_align_left"><span id="S3.T5.3.3.7.1.1" class="ltx_text ltx_font_italic">PT+FT</span></td>
<td id="S3.T5.3.3.7.2" class="ltx_td ltx_align_center">16.40</td>
<td id="S3.T5.3.3.7.3" class="ltx_td ltx_align_center">78.00</td>
<td id="S3.T5.3.3.7.4" class="ltx_td ltx_align_center">76.58</td>
</tr>
<tr id="S3.T5.3.3.8" class="ltx_tr">
<td id="S3.T5.3.3.8.1" class="ltx_td ltx_align_left"><span id="S3.T5.3.3.8.1.1" class="ltx_text ltx_font_italic">PT+FT+Mel-spec</span></td>
<td id="S3.T5.3.3.8.2" class="ltx_td ltx_align_center">17.36</td>
<td id="S3.T5.3.3.8.3" class="ltx_td ltx_align_center">76.82</td>
<td id="S3.T5.3.3.8.4" class="ltx_td ltx_align_center">75.48</td>
</tr>
<tr id="S3.T5.3.3.9" class="ltx_tr">
<td id="S3.T5.3.3.9.1" class="ltx_td ltx_align_left"><span id="S3.T5.3.3.9.1.1" class="ltx_text ltx_font_italic">PT+FT+HuBERT SSR</span></td>
<td id="S3.T5.3.3.9.2" class="ltx_td ltx_align_center">16.20</td>
<td id="S3.T5.3.3.9.3" class="ltx_td ltx_align_center">78.01</td>
<td id="S3.T5.3.3.9.4" class="ltx_td ltx_align_center">76.71</td>
</tr>
<tr id="S3.T5.3.3.10" class="ltx_tr">
<td id="S3.T5.3.3.10.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T5.3.3.10.1.1" class="ltx_text ltx_font_italic">PT+FT+HuBERT AWEs</span></td>
<td id="S3.T5.3.3.10.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T5.3.3.10.2.1" class="ltx_text ltx_font_bold">16.07</span></td>
<td id="S3.T5.3.3.10.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T5.3.3.10.3.1" class="ltx_text ltx_font_bold">78.22</span></td>
<td id="S3.T5.3.3.10.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T5.3.3.10.4.1" class="ltx_text ltx_font_bold">76.96</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS1.SSS3.p6" class="ltx_para">
<p id="S3.SS1.SSS3.p6.1" class="ltx_p">We note that <span id="S3.SS1.SSS3.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> compared with other acoustic features, <span id="S3.SS1.SSS3.p6.1.2" class="ltx_text ltx_font_italic">HuBERT</span> AWEs provide the best results across all metrics. This verifies our hypothesis that <span id="S3.SS1.SSS3.p6.1.3" class="ltx_text ltx_font_italic">DSUs align more easily with word embeddings than continuous acoustic features</span>. <span id="S3.SS1.SSS3.p6.1.4" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> The inclusion of Mel-spec worsens WER rather than improves it, which contrasts with findings in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This phenomenon is reasonable and consistent with discussions in Sec. <a href="#S3.SS1.SSS3" title="3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>: <span id="S3.SS1.SSS3.p6.1.5" class="ltx_text ltx_font_italic">i)</span> IEMOCAP being emotional speech, contains intense prosody variation, making it challenging to encode phonetic information from Mel-spec; <span id="S3.SS1.SSS3.p6.1.6" class="ltx_text ltx_font_italic">ii)</span> the small-size data for FT (4.4k samples with an average duration of 5 seconds) hinders the model from sufficiently learning linguistic information from Mel-spec, representing a low-resource scenario; <span id="S3.SS1.SSS3.p6.1.7" class="ltx_text ltx_font_italic">iii)</span> our incorporation of audio features only happens during FT and testing, causing Mel-spec to struggle to provide sufficient information to word embeddings. In contrast, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> conducted all training phases using the same large corpus, making their findings inapplicable to LROOD scenarios. <span id="S3.SS1.SSS3.p6.1.8" class="ltx_text ltx_font_bold ltx_font_italic">3)</span> Interestingly, despite that Mel-spec worsens WER compared to the original ASR transcript and PT, BLEU and GLEU record improvement. This is likely because the corrected texts are more fluent and structurally correct with respect to the reference (favourable for BLEU and GLEU), while still containing word-level mistakes captured by WER. This demonstrates the contribution of audio to high-level linguistic information, which corroborates our later finding in SER (Sec. <a href="#S4" title="4 AEC for Downstream Use – SER ‣ 3.1.5 Performance Comparison with Literature ‣ 3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Evaluation on Additional Corpora</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">As mentioned before, we test the performance of our proposed approach on two more corpora: CMU-MOSI and MSP-Podcast, to verify its generalizability. The results are shown in Table <a href="#S3.T6" title="Table 6 ‣ 3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S3.T7" title="Table 7 ‣ 3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. All experimental settings remain the same, while several non-optimal models are omitted for brevity.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">It can be noted that <span id="S3.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> PT fails to provide better results than the original ASR transcript on CMU-MOSI, whereas the performance improvement is significant on MSP-Podcast. This phenomenon is plausible due to the OOD problem: CMU-MOSI consists of monologue speech with opinions on specific topics (mainly about movies), containing a high proportion of OOD words, making the PT model trained on Common Voice less effective. In contrast, MSP-Podcast consists of natural, real-life speech recorded in podcast settings, sharing more linguistic similarities with Common Voice. <span id="S3.SS1.SSS4.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> Both FT and the incorporation of DSUs bring performance improvements on CMU-MOSI, despite PT not being effective and the FT data being extremely limited at only 1,800 samples. Since the data size and domain similarity of IEMOCAP are between those of CMU-MOSI and MSP-Podcast, its performance improvement also falls in between (Table <a href="#S3.T5" title="Table 5 ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Furthermore, the performance improvement is even more significant on MSP-Podcast, indicating that the more data available for FT, the better the performance. These findings demonstrate the efficacy of our approach in LROOD scenarios and also highlight its generalizability and potential across various scenarios.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T6.5.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>Result summary on CMU-MOSI.</figcaption>
<div id="S3.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.4pt;height:83.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.3pt,3.1pt) scale(0.93,0.93) ;">
<table id="S3.T6.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.3.3.3" class="ltx_tr">
<td id="S3.T6.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T6.3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.T6.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T6.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T6.1.1.1.1.1.m1.1.1" xref="S3.T6.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.1.1.m1.1b"><ci id="S3.T6.1.1.1.1.1.m1.1.1.cmml" xref="S3.T6.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T6.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.T6.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T6.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T6.2.2.2.2.1.m1.1.1" xref="S3.T6.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.2.2.2.2.1.m1.1b"><ci id="S3.T6.2.2.2.2.1.m1.1.1.cmml" xref="S3.T6.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T6.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.T6.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T6.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T6.3.3.3.3.1.m1.1.1" xref="S3.T6.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.3.3.3.3.1.m1.1b"><ci id="S3.T6.3.3.3.3.1.m1.1.1.cmml" xref="S3.T6.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.T6.3.3.4" class="ltx_tr">
<td id="S3.T6.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T6.3.3.4.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T6.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">17.84</td>
<td id="S3.T6.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">72.82</td>
<td id="S3.T6.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">72.17</td>
</tr>
<tr id="S3.T6.3.3.5" class="ltx_tr">
<td id="S3.T6.3.3.5.1" class="ltx_td ltx_align_left">
<span id="S3.T6.3.3.5.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T6.3.3.5.1.2" class="ltx_text ltx_font_italic">PT</span>
</td>
<td id="S3.T6.3.3.5.2" class="ltx_td ltx_align_center">17.88</td>
<td id="S3.T6.3.3.5.3" class="ltx_td ltx_align_center">72.80</td>
<td id="S3.T6.3.3.5.4" class="ltx_td ltx_align_center">72.16</td>
</tr>
<tr id="S3.T6.3.3.6" class="ltx_tr">
<td id="S3.T6.3.3.6.1" class="ltx_td ltx_align_left"><span id="S3.T6.3.3.6.1.1" class="ltx_text ltx_font_italic">PT+FT</span></td>
<td id="S3.T6.3.3.6.2" class="ltx_td ltx_align_center">17.65</td>
<td id="S3.T6.3.3.6.3" class="ltx_td ltx_align_center">73.31</td>
<td id="S3.T6.3.3.6.4" class="ltx_td ltx_align_center">72.63</td>
</tr>
<tr id="S3.T6.3.3.7" class="ltx_tr">
<td id="S3.T6.3.3.7.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T6.3.3.7.1.1" class="ltx_text ltx_font_italic">PT+FT+HuBERT AWEs</span></td>
<td id="S3.T6.3.3.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T6.3.3.7.2.1" class="ltx_text ltx_font_bold">17.22</span></td>
<td id="S3.T6.3.3.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T6.3.3.7.3.1" class="ltx_text ltx_font_bold">73.98</span></td>
<td id="S3.T6.3.3.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T6.3.3.7.4.1" class="ltx_text ltx_font_bold">73.01</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T7.5.1.1" class="ltx_text ltx_font_bold">Table 7</span>: </span>Result summary on MSP-Podcast.</figcaption>
<div id="S3.T7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.4pt;height:83.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.3pt,3.1pt) scale(0.93,0.93) ;">
<table id="S3.T7.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T7.3.3.3" class="ltx_tr">
<td id="S3.T7.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T7.3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.T7.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T7.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T7.1.1.1.1.1.m1.1.1" xref="S3.T7.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.1.1.m1.1b"><ci id="S3.T7.1.1.1.1.1.m1.1.1.cmml" xref="S3.T7.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T7.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.T7.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T7.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T7.2.2.2.2.1.m1.1.1" xref="S3.T7.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.2.1.m1.1b"><ci id="S3.T7.2.2.2.2.1.m1.1.1.cmml" xref="S3.T7.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T7.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.T7.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T7.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T7.3.3.3.3.1.m1.1.1" xref="S3.T7.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T7.3.3.3.3.1.m1.1b"><ci id="S3.T7.3.3.3.3.1.m1.1.1.cmml" xref="S3.T7.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.T7.3.3.4" class="ltx_tr">
<td id="S3.T7.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T7.3.3.4.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T7.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">17.65</td>
<td id="S3.T7.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">81.32</td>
<td id="S3.T7.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">78.02</td>
</tr>
<tr id="S3.T7.3.3.5" class="ltx_tr">
<td id="S3.T7.3.3.5.1" class="ltx_td ltx_align_left">
<span id="S3.T7.3.3.5.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T7.3.3.5.1.2" class="ltx_text ltx_font_italic">PT</span>
</td>
<td id="S3.T7.3.3.5.2" class="ltx_td ltx_align_center">16.23</td>
<td id="S3.T7.3.3.5.3" class="ltx_td ltx_align_center">82.59</td>
<td id="S3.T7.3.3.5.4" class="ltx_td ltx_align_center">79.14</td>
</tr>
<tr id="S3.T7.3.3.6" class="ltx_tr">
<td id="S3.T7.3.3.6.1" class="ltx_td ltx_align_left"><span id="S3.T7.3.3.6.1.1" class="ltx_text ltx_font_italic">PT+FT</span></td>
<td id="S3.T7.3.3.6.2" class="ltx_td ltx_align_center">14.73</td>
<td id="S3.T7.3.3.6.3" class="ltx_td ltx_align_center">83.16</td>
<td id="S3.T7.3.3.6.4" class="ltx_td ltx_align_center">80.84</td>
</tr>
<tr id="S3.T7.3.3.7" class="ltx_tr">
<td id="S3.T7.3.3.7.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T7.3.3.7.1.1" class="ltx_text ltx_font_italic">PT+FT+HuBERT AWEs</span></td>
<td id="S3.T7.3.3.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T7.3.3.7.2.1" class="ltx_text ltx_font_bold">13.89</span></td>
<td id="S3.T7.3.3.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T7.3.3.7.3.1" class="ltx_text ltx_font_bold">83.64</span></td>
<td id="S3.T7.3.3.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T7.3.3.7.4.1" class="ltx_text ltx_font_bold">81.80</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S3.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>Performance Comparison with Literature</h4>

<div id="S3.SS1.SSS5.p1" class="ltx_para">
<p id="S3.SS1.SSS5.p1.1" class="ltx_p">To confirm the effectiveness of our approach, we compare it with the literature by adopting the following baselines:</p>
</div>
<div id="S3.SS1.SSS5.p2" class="ltx_para">
<p id="S3.SS1.SSS5.p2.1" class="ltx_p"><span id="S3.SS1.SSS5.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> Crossmodal AEC using continuous acoustic information: Mel-spectrogram <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></p>
</div>
<div id="S3.SS1.SSS5.p3" class="ltx_para">
<p id="S3.SS1.SSS5.p3.1" class="ltx_p"><span id="S3.SS1.SSS5.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> Crossmodal AEC using continuous acoustic information: raw self-supervised representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS5.p4" class="ltx_para">
<p id="S3.SS1.SSS5.p4.1" class="ltx_p"><span id="S3.SS1.SSS5.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">3)</span> Generative AEC using an LLM with 1-best ASR hypothesis and Alpaca prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS5.p5" class="ltx_para">
<p id="S3.SS1.SSS5.p5.1" class="ltx_p"><span id="S3.SS1.SSS5.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">4)</span> Generative AEC using an LLM with N-best ASR hypothesis and Alpaca prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS5.p6" class="ltx_para">
<p id="S3.SS1.SSS5.p6.1" class="ltx_p"><span id="S3.SS1.SSS5.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">5)</span> Generative AEC using an LLM with 1-best ASR hypothesis and Task-Activating prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS5.p7" class="ltx_para">
<p id="S3.SS1.SSS5.p7.1" class="ltx_p"><span id="S3.SS1.SSS5.p7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">6)</span> Generative AEC using an LLM with N-best ASR hypothesis and Task-Activating prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS5.p8" class="ltx_para">
<p id="S3.SS1.SSS5.p8.1" class="ltx_p">Since the comparisons with <span id="S3.SS1.SSS5.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1)</span> and <span id="S3.SS1.SSS5.p8.1.2" class="ltx_text ltx_font_bold ltx_font_italic">2)</span> have already been presented in Table <a href="#S3.T5" title="Table 5 ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and discussed, we omit them here. For the remaining comparisons, we attempt the Alpaca prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Task-Activating (TA) prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> using <span id="S3.SS1.SSS5.p8.1.3" class="ltx_text ltx_font_italic">InstructGPT</span> on both 1-best and 5-best hypotheses. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1.5 Performance Comparison with Literature ‣ 3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how the Alpaca prompt and TA prompt are used. The results are presented in Table <a href="#S3.T8" title="Table 8 ‣ 3.1.5 Performance Comparison with Literature ‣ 3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S3.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T8.5.1.1" class="ltx_text ltx_font_bold">Table 8</span>: </span>Performance comparison with generative AEC approaches.</figcaption>
<div id="S3.T8.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:243.8pt;height:117.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.2pt,4.4pt) scale(0.93,0.93) ;">
<table id="S3.T8.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T8.3.3.3" class="ltx_tr">
<td id="S3.T8.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T8.3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S3.T8.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T8.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T8.1.1.1.1.1.m1.1.1" xref="S3.T8.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T8.1.1.1.1.1.m1.1b"><ci id="S3.T8.1.1.1.1.1.m1.1.1.cmml" xref="S3.T8.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T8.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S3.T8.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T8.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S3.T8.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T8.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T8.2.2.2.2.1.m1.1.1" xref="S3.T8.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T8.2.2.2.2.1.m1.1b"><ci id="S3.T8.2.2.2.2.1.m1.1.1.cmml" xref="S3.T8.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T8.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S3.T8.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T8.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S3.T8.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T8.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T8.3.3.3.3.1.m1.1.1" xref="S3.T8.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T8.3.3.3.3.1.m1.1b"><ci id="S3.T8.3.3.3.3.1.m1.1.1.cmml" xref="S3.T8.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T8.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S3.T8.3.3.4" class="ltx_tr">
<td id="S3.T8.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T8.3.3.4.1.1" class="ltx_text ltx_font_italic">Original ASR transcript</span></td>
<td id="S3.T8.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">17.18</td>
<td id="S3.T8.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">76.56</td>
<td id="S3.T8.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">75.29</td>
</tr>
<tr id="S3.T8.3.3.5" class="ltx_tr">
<td id="S3.T8.3.3.5.1" class="ltx_td ltx_align_left"><span id="S3.T8.3.3.5.1.1" class="ltx_text ltx_font_italic">Our full model</span></td>
<td id="S3.T8.3.3.5.2" class="ltx_td ltx_align_center"><span id="S3.T8.3.3.5.2.1" class="ltx_text ltx_font_bold">16.07</span></td>
<td id="S3.T8.3.3.5.3" class="ltx_td ltx_align_center"><span id="S3.T8.3.3.5.3.1" class="ltx_text ltx_font_bold">78.22</span></td>
<td id="S3.T8.3.3.5.4" class="ltx_td ltx_align_center"><span id="S3.T8.3.3.5.4.1" class="ltx_text ltx_font_bold">76.96</span></td>
</tr>
<tr id="S3.T8.3.3.6" class="ltx_tr">
<td id="S3.T8.3.3.6.1" class="ltx_td ltx_align_left">
<span id="S3.T8.3.3.6.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T8.3.3.6.1.2" class="ltx_text ltx_font_bold ltx_font_italic">3)<span id="S3.T8.3.3.6.1.2.1" class="ltx_text ltx_font_medium"> Alpaca prompt<sub id="S3.T8.3.3.6.1.2.1.1" class="ltx_sub">1-best</sub></span></span>
</td>
<td id="S3.T8.3.3.6.2" class="ltx_td ltx_align_center">17.18</td>
<td id="S3.T8.3.3.6.3" class="ltx_td ltx_align_center">76.56</td>
<td id="S3.T8.3.3.6.4" class="ltx_td ltx_align_center">75.29</td>
</tr>
<tr id="S3.T8.3.3.7" class="ltx_tr">
<td id="S3.T8.3.3.7.1" class="ltx_td ltx_align_left"><span id="S3.T8.3.3.7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">4)<span id="S3.T8.3.3.7.1.1.1" class="ltx_text ltx_font_medium"> Alpaca prompt<sub id="S3.T8.3.3.7.1.1.1.1" class="ltx_sub">5-best</sub></span></span></td>
<td id="S3.T8.3.3.7.2" class="ltx_td ltx_align_center">17.01</td>
<td id="S3.T8.3.3.7.3" class="ltx_td ltx_align_center">76.97</td>
<td id="S3.T8.3.3.7.4" class="ltx_td ltx_align_center">75.44</td>
</tr>
<tr id="S3.T8.3.3.8" class="ltx_tr">
<td id="S3.T8.3.3.8.1" class="ltx_td ltx_align_left"><span id="S3.T8.3.3.8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">5)<span id="S3.T8.3.3.8.1.1.1" class="ltx_text ltx_font_medium"> TA prompt<sub id="S3.T8.3.3.8.1.1.1.1" class="ltx_sub">1-best</sub></span></span></td>
<td id="S3.T8.3.3.8.2" class="ltx_td ltx_align_center">17.18</td>
<td id="S3.T8.3.3.8.3" class="ltx_td ltx_align_center">76.57</td>
<td id="S3.T8.3.3.8.4" class="ltx_td ltx_align_center">75.30</td>
</tr>
<tr id="S3.T8.3.3.9" class="ltx_tr">
<td id="S3.T8.3.3.9.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T8.3.3.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic">6)<span id="S3.T8.3.3.9.1.1.1" class="ltx_text ltx_font_medium"> TA prompt<sub id="S3.T8.3.3.9.1.1.1.1" class="ltx_sub">5-best</sub></span></span></td>
<td id="S3.T8.3.3.9.2" class="ltx_td ltx_align_center ltx_border_b">16.62</td>
<td id="S3.T8.3.3.9.3" class="ltx_td ltx_align_center ltx_border_b">77.99</td>
<td id="S3.T8.3.3.9.4" class="ltx_td ltx_align_center ltx_border_b">75.98</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.16677/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="277" height="156" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.16677/assets/x3.png" id="S3.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="280" height="157" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>An illustration of the Alpaca prompt (upper) and Task-Activating prompt (below) used in this work.</figcaption>
</figure>
<div id="S3.SS1.SSS5.p9" class="ltx_para">
<p id="S3.SS1.SSS5.p9.1" class="ltx_p">From the comparison results, it can be observed that: the generative AEC approaches underperform our S2S crossmodal AEC approach, particularly as the 1-best hypothesis shows hardly any difference compared to the original ASR transcript, confirming our effectiveness for scenarios where only the 1-best hypothesis is available.</p>
</div>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>AEC for Downstream Use – SER</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To verify the quality and usability of our AEC approaches in downstream applications, we compare SER performances using the corrected transcript and the original ASR transcript.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Following the same training scheme as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, we train the SER model on the ground-truth transcript of the IEMOCAP training set and evaluate its performance on the ASR transcript of the test set, employing five-fold cross-validation. Textual features are extracted using <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">BERT</span>. The SER model consists of two bidirectional LSTM layers (hidden state: 32), a self-attention layer (hidden state: 64, heads: 16), a dense layer (hidden state: 64) with ReLU activation, and an output layer with Softmax activation. We use the AdamW optimizer with a learning rate of 1e-4 and weight decay of 1e-5 and a batch size of 64. Training is performed for 150 epochs, and the reported results are the best Unweighted Accuracy (UA) achieved. The only difference from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is that they used the pooler output from <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">BERT</span>, while we use hidden states.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T9.6.1.1" class="ltx_text ltx_font_bold">Table 9</span>: </span>Comparison results of SER performance.</figcaption>
<div id="S4.T9.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:214.2pt;height:50.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,1.9pt) scale(0.93,0.93) ;">
<table id="S4.T9.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T9.4.4.4" class="ltx_tr">
<td id="S4.T9.4.4.4.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T9.4.4.4.5.1" class="ltx_text ltx_font_bold">Transcript</span></td>
<td id="S4.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.1.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S4.T9.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.1.1.1.1.1.m1.1.1" xref="S4.T9.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.1.1.m1.1b"><ci id="S4.T9.1.1.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S4.T9.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU<math id="S4.T9.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T9.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T9.2.2.2.2.1.m1.1.1" xref="S4.T9.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T9.2.2.2.2.1.m1.1b"><ci id="S4.T9.2.2.2.2.1.m1.1.1.cmml" xref="S4.T9.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T9.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.3.3.3.3.1" class="ltx_text ltx_font_bold">GLEU<math id="S4.T9.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T9.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T9.3.3.3.3.1.m1.1.1" xref="S4.T9.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T9.3.3.3.3.1.m1.1b"><ci id="S4.T9.3.3.3.3.1.m1.1.1.cmml" xref="S4.T9.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T9.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.4.4.4.4.1" class="ltx_text ltx_font_bold">UA<math id="S4.T9.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T9.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T9.4.4.4.4.1.m1.1.1" xref="S4.T9.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T9.4.4.4.4.1.m1.1b"><ci id="S4.T9.4.4.4.4.1.m1.1.1.cmml" xref="S4.T9.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T9.4.4.5" class="ltx_tr">
<td id="S4.T9.4.4.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T9.4.4.5.1.1" class="ltx_text ltx_font_italic">Original</span></td>
<td id="S4.T9.4.4.5.2" class="ltx_td ltx_align_center ltx_border_t">17.18</td>
<td id="S4.T9.4.4.5.3" class="ltx_td ltx_align_center ltx_border_t">76.56</td>
<td id="S4.T9.4.4.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.29</td>
<td id="S4.T9.4.4.5.5" class="ltx_td ltx_align_center ltx_border_t">60.92</td>
</tr>
<tr id="S4.T9.4.4.6" class="ltx_tr">
<td id="S4.T9.4.4.6.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T9.4.4.6.1.1" class="ltx_text ltx_font_italic">Corrected</span></td>
<td id="S4.T9.4.4.6.2" class="ltx_td ltx_align_center ltx_border_b">16.07</td>
<td id="S4.T9.4.4.6.3" class="ltx_td ltx_align_center ltx_border_b">78.22</td>
<td id="S4.T9.4.4.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">76.96</td>
<td id="S4.T9.4.4.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T9.4.4.6.5.1" class="ltx_text ltx_font_bold">61.82</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">As expected, SER performance can be improved by using the corrected transcript. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, UA increased from 55.4 to 57.1 (<span id="S4.p3.1.1" class="ltx_text ltx_font_italic">+1.70</span>) with WER decreasing from 20 to 15 (<span id="S4.p3.1.2" class="ltx_text ltx_font_italic">-5.00</span>). In our case, UA increased from 60.92 to 61.82 (<span id="S4.p3.1.3" class="ltx_text ltx_font_italic">+0.90</span>) with WER decreasing from 17.18 to 16.07 (<span id="S4.p3.1.4" class="ltx_text ltx_font_italic">-1.11</span>), which represents a more significant improvement. This observation can be attributed to the fact that AEC with DSUs not only reduces WER but potentially does so via preserving syntax and semantics better, leading to higher usability in downstream tasks (resonates with the last finding in Sec. <a href="#S3.SS1.SSS3" title="3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>). However, further analysis is needed to understand the nature of ASR errors: where they occur and how they are corrected.</p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we pre-train an S2S AEC model on large corpora and fine-tune it on an LROOD corpus with the assistance of DSUs. The results indicate that for AEC on LROOD data, PT, FT, and DSUs are all important. Moreover, the ASR domain discrepancy problem requires attention and should be alleviated by using the same ASR model to generate transcripts in all phases of AEC applications. We compare different acoustic features and demonstrate the superiority of DSUs over continuous features in aligning with word embeddings. A downstream task of SER further demonstrates the improved quality of the corrected transcript, highlighting the applicability of our approach. Additionally, as demonstrated by the experiment in Sec. <a href="#S3.SS1.SSS4" title="3.1.4 Evaluation on Additional Corpora ‣ 3.1.3 Incorporation of Discrete Speech Units ‣ 3.1.2 ASR Domain Discrepancy Problem ‣ 3.1 Crossmodal AEC on LROOD Data ‣ 3 Approach, Experiments, and Results ‣ Crossmodal ASR Error Correction with Discrete Speech Units" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.4</span></a>, our approach is expected to perform better on larger downstream data.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli,

</span>
<span class="ltx_bibblock">“wav2vec 2.0: A framework for self-supervised learning of speech representations,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 12449–12460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed,

</span>
<span class="ltx_bibblock">“Hubert: Self-supervised speech representation learning by masked prediction of hidden units,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever,

</span>
<span class="ltx_bibblock">“Robust speech recognition via large-scale weak supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>. PMLR, 2023, pp. 28492–28518.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai,

</span>
<span class="ltx_bibblock">“Exploration of a self-supervised speech model: A study on emotional corpora,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</span>. IEEE, 2023, pp. 868–875.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, and Yuzong Liu,

</span>
<span class="ltx_bibblock">“On-device constrained self-supervised speech representation learning for keyword spotting via knowledge distillation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass,

</span>
<span class="ltx_bibblock">“Whisper-at: Noise-robust automatic speech recognizers are also strong general audio event taggers,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yuanchao Li, Zeyu Zhao, Ondrej Klejch, Peter Bell, and Catherine Lai,

</span>
<span class="ltx_bibblock">“ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Interspeech 2023</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Wen Wu, Chao Zhang, and Philip C Woodland,

</span>
<span class="ltx_bibblock">“Self-supervised representations in speech-based depression detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Satwik Dutta, Sarah A. Tao, Jacob C. Reyna, Rebecca E. Hacker, Dwight W. Irvin, Jay Buzhardt, and John H. L. Hansen,

</span>
<span class="ltx_bibblock">“Challenges remain in building asr for spontaneous preschool children speech in naturalistic educational environments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yuanchao Li, Peter Bell, and Catherine Lai,

</span>
<span class="ltx_bibblock">“Fusing ASR outputs in joint training for speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2022, pp. 7362–7366.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Tomohiro Tanaka, Ryo Masumura, Hirokazu Masataki, and Yushi Aono,

</span>
<span class="ltx_bibblock">“Neural error corrective language models for automatic speech recognition.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">INTERSPEECH</span>, 2018, pp. 401–405.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, and Andreas Stolcke,

</span>
<span class="ltx_bibblock">“Generative speech recognition error correction with large language models,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.15649</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Anirudh Mani, Shruti Palaskar, Nimshi Venkat Meripo, Sandeep Konam, and Florian Metze,

</span>
<span class="ltx_bibblock">“ASR error correction and domain adaptation using machine translation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2020, pp. 6344–6348.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Junwei Liao, Sefik Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun Shou, Hong Qu, and Michael Zeng,

</span>
<span class="ltx_bibblock">“Improving readability for automatic speech recognition transcription,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Asian and Low-Resource Language Information Processing</span>, vol. 22, no. 5, pp. 1–23, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Binghuai Lin and Liyuan Wang,

</span>
<span class="ltx_bibblock">“Multi-modal ASR error correction with joint ASR error detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jing Du, Shiliang Pu, Qinbo Dong, Chao Jin, Xin Qi, Dian Gu, Ru Wu, and Hongwei Zhou,

</span>
<span class="ltx_bibblock">“Cross-modal ASR post-processing system for error correction and utterance rejection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.03313</span>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A Kiani, David Gomez-Cabrero, and Jesper Tegnér,

</span>
<span class="ltx_bibblock">“Whispering llama: A cross-modal generative error correction framework for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">The 2023 Conference on Empirical Methods in Natural Language Processing</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, and Chao-Han Huck Yang,

</span>
<span class="ltx_bibblock">“It’s never too late: Fusing acoustic information into large language models for automatic speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.05457</span>, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Ye Bai, Jianhua Tao, and Xuefei Liu,

</span>
<span class="ltx_bibblock">“End-to-end spelling correction conditioned on acoustic feature for code-switching speech recognition.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko Takashima, Takafumi Moriya, Takanori Ashihara, Shota Orihashi, and Naoki Makishima,

</span>
<span class="ltx_bibblock">“Cross-modal transformer-based neural correction models for automatic speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber,

</span>
<span class="ltx_bibblock">“Common voice: A massively-multilingual speech corpus,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC)</span>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan,

</span>
<span class="ltx_bibblock">“IEMOCAP: Interactive emotional dyadic motion capture database,”

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Language resources and evaluation</span>, vol. 42, pp. 335–359, 2008.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency,

</span>
<span class="ltx_bibblock">“Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, vol. 31, no. 6, pp. 82–88, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Reza Lotfian and Carlos Busso,

</span>
<span class="ltx_bibblock">“Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing</span>, vol. 10, no. 4, pp. 471–483, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al.,

</span>
<span class="ltx_bibblock">“Conformer: Convolution-augmented transformer for speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al.,

</span>
<span class="ltx_bibblock">“Espnet: End-to-end speech processing toolkit,”

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jinxi Guo, Tara N Sainath, and Ron J Weiss,

</span>
<span class="ltx_bibblock">“A spelling correction model for end-to-end speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2019, pp. 5651–5655.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.,

</span>
<span class="ltx_bibblock">“Google’s neural machine translation system: Bridging the gap between human and machine translation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1609.08144</span>, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger,

</span>
<span class="ltx_bibblock">“Montreal forced aligner: Trainable text-speech alignment using kaldi.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2017, vol. 2017, pp. 498–502.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Pinzhen Chen and Gerasimos Lampouras,

</span>
<span class="ltx_bibblock">“Exploring data augmentation for code generation tasks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics: EACL 2023</span>, 2023, pp. 1497–1505.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Mauro Cettolo, Christian Girardi, and Marcello Federico,

</span>
<span class="ltx_bibblock">“WIT3: Web inventory of transcribed and translated talks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of the 16th Annual Conference of the European Association for Machine Translation</span>, Trento, Italy, 2012, pp. 261–268, European Association for Machine Translation.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Andrew L Maas, Stephen D Miller, Tyler M O’neil, Andrew Y Ng, and Patrick Nguyen,

</span>
<span class="ltx_bibblock">“Word-level acoustic modeling with convolutional vector regression,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proc. ICML Workshop Representation Learn</span>, 2012.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Keith Levin, Katharine Henry, Aren Jansen, and Karen Livescu,

</span>
<span class="ltx_bibblock">“Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2013 IEEE workshop on automatic speech recognition and understanding</span>. IEEE, 2013, pp. 410–415.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Yevgen Matusevych, Herman Kamper, and Sharon Goldwater,

</span>
<span class="ltx_bibblock">“Analyzing autoencoder-based acoustic word embeddings,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">ICLR Workshop on Bridging AI and Cognitive Science</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Ramon Sanabria, Hao Tang, and Sharon Goldwater,

</span>
<span class="ltx_bibblock">“Analyzing acoustic word embeddings from pre-trained self-supervised speech models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Alexandra Saliba, Yuanchao Li, Ramon Sanabria, and Catherine Lai,

</span>
<span class="ltx_bibblock">“Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)</span>. IEEE, 2024.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Ankita Pasad, Bowen Shi, and Karen Livescu,

</span>
<span class="ltx_bibblock">“Comparative layer-wise analysis of self-supervised speech models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto,

</span>
<span class="ltx_bibblock">“Stanford alpaca: An instruction-following llama model,” 2023.

</span>
</li>
</ul>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.16675" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.16677" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.16677">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.16677" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.16678" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 17:29:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
