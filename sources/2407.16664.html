<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.16664] Towards scalable efficient on-device ASR with transfer learning</title><meta property="og:description" content="Multilingual pretraining for transfer learning significantly boosts the robustness of low-resource monolingual ASR models. This study systematically investigates three main aspects: (a) the impact of transfer learning â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards scalable efficient on-device ASR with transfer learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards scalable efficient on-device ASR with transfer learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.16664">

<!--Generated on Mon Aug  5 16:24:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">LaxmiPandey
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>KeLi
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>JinxiGuo
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>DebjyotiPaul
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>ArthurGuo
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>JayMahadeokar
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>XuedongZhang




</p>
</div>
<h1 class="ltx_title ltx_title_document">Towards scalable efficient on-device ASR with transfer learning</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Multilingual pretraining for transfer learning significantly boosts the robustness of low-resource monolingual ASR models. This study systematically investigates three main aspects: (a) the impact of transfer learning on model performance during initial training or fine-tuning, (b) the influence of transfer learning across dataset domains and languages, and (c) the effect on rare-word recognition compared to non-rare words. Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across languages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets. Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining. Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech recognition, transfer learning, multilingual pretraining
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Speech Recognition (ASR) systems based on end-to-end neural networks face challenges when languages lack ample training data, resulting in suboptimal performance. Previous studies have explored various techniques for training ASR in low-resource languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. One such strategy involves leveraging transfer learning.
Transfer learning (TL) leverages the knowledge gained from other high-resource languages to enhance the performance of ASR models in low-resource settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Multilingual pretraining extends this principle of TL by using data from a wide range of languages to create a shared, multilingual representation capturing phonetic and linguistic characteristics. This shared representation is then fine-tuned for the target low-resource language, allowing the ASR system to leverage the phonetic and linguistic characteristics learned from the diverse set of languages during pretraining. As a result, the ASR system becomes more robust, adaptable, and accurate when it comes to transcribing speech in previously underrepresented languages.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.16664/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="159" height="61" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Visual representation of our multilingual pretraining strategy, showcasing both in-domain (MLS:seed | MLS:target) and out-of-domain (In-house:seed | MLS:target) approaches, alongside the ASR model training architecture with Alignment Restricted RNNT (AR-RNNT) and MinWER loss function.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Our research investigates the substantial enhancement in accuracy and efficiency achieved by employing multilingual transfer learning through pretraining in monolingual Automatic Speech Recognition (ASR) models for languages with limited training data. Our main objective is to explore the most effective approach for implementing pretraining-based transfer learning across languages, aiming to quantify its influence on both the quality of performance and computational efficiency. The innovation in our work does not solely rest on the transfer learning technique but rather on our comprehensive analysis of its effective utilization.</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We first study the efficacy of transfer learning (TL) at two important stages of model training: initial training with the RNNT loss, and fine-tuning with the MinWER loss, with the goal to scrutinize whether TL exhibits greater effectiveness in one stage as opposed to the other.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We then explore the impact of domain-specific pretraining. We aim to investigate the comparative effectiveness of out-of-domain pretraining, utilizing multilingual data, against in-domain pretraining with multilingual data for the purpose of transfer learning.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our third inquiry looks at whether transfer learning confers a more substantial benefit to rare words compared to common (non-rare) words. This specific investigation focuses on the impact of both in-domain and out-of-domain pretraining on the performance of rare and non-rare words.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several previous studies have endeavored to enhance low-resource languages by employing multilingual pretraining techniques, a technique that harnesses the knowledge gained from resource-rich languages to enhance ASR performance for those with limited training data.
The literature on multilingual pretraining for transfer learning in ASR underscores the promise of this approach in achieving cross-linguistic robustness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Various strategies have been proposed to enhance the performance of low-resource Automatic Speech Recognition (ASR) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, including transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, multi-task training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Transfer learning involves leveraging a well-trained Acoustic Model (AM) from a high-resource language to bootstrap the low-resource AM. Multi-task training and ensemble learning aim to utilize multilingual data and share model parameters, demonstrating successful strategies in low-resource scenarios. Recent advancements in the End-to-End (E2E) framework have introduced multilingual approaches, such as the multilingual RNNT model with language-specific adapters and datasampling to address data imbalance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Another work introduces an approach featuring an audio-to-byte End-to-End (E2E) system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, leveraging bytes as target units for enhanced scalability across multiple languages. Additionally, a transformer-based multilingual End-to-End (E2E) model has been introduced, integrating language information into the model at both the decoder and encoder levels. This involves the use of language identity tokens, and providing language information to acoustic vectors through one-hot vectors or learned language embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. While multilingual methods are appealing for low-resource languages, transfer learning stands out for its simplicity and effectiveness, requiring only pre-trained models without the need for the original high-resource language data. In our exploration of strategies to improve low-resource RNNT models, we focus on the simplicity and effectiveness of transfer learning.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.8.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.8.8.9.1" class="ltx_tr">
<th id="S2.T1.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.8.8.9.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S2.T1.8.8.9.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S2.T1.8.8.9.1.2.1" class="ltx_text ltx_font_bold">MLS</span></td>
<td id="S2.T1.8.8.9.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S2.T1.8.8.9.1.3.1" class="ltx_text ltx_font_bold">In-house</span></td>
</tr>
<tr id="S2.T1.8.8.10.2" class="ltx_tr">
<td id="S2.T1.8.8.10.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.1.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S2.T1.8.8.10.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.2.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S2.T1.8.8.10.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.3.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S2.T1.8.8.10.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.4.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S2.T1.8.8.10.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.5.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S2.T1.8.8.10.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.8.8.10.2.6.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S2.T1.8.8.11.3" class="ltx_tr">
<th id="S2.T1.8.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">A. Monolingual ASR <span id="S2.T1.8.8.11.3.1.1" class="ltx_text" style="font-size:70%;">(baseline)</span>
</th>
<td id="S2.T1.8.8.11.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.99</td>
<td id="S2.T1.8.8.11.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.78</td>
<td id="S2.T1.8.8.11.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.38</td>
<td id="S2.T1.8.8.11.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.3</td>
<td id="S2.T1.8.8.11.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.32</td>
<td id="S2.T1.8.8.11.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.80</td>
</tr>
<tr id="S2.T1.2.2.2" class="ltx_tr">
<th id="S2.T1.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">B. Multilingual ASR <span id="S2.T1.2.2.2.3.1" class="ltx_text" style="font-size:70%;">(seed)</span>
</th>
<td id="S2.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.93</td>
<td id="S2.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.14</td>
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.53Â <span id="S2.T1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(33.4%<math id="S2.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</td>
<td id="S2.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.35</td>
<td id="S2.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.59</td>
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.97Â <span id="S2.T1.2.2.2.2.1" class="ltx_text" style="font-size:70%;">(52.2%<math id="S2.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S2.T1.4.4.4" class="ltx_tr">
<th id="S2.T1.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">C. B seeded monolingual RNNT ASR</th>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.01</td>
<td id="S2.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.73</td>
<td id="S2.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.37Â <span id="S2.T1.3.3.3.1.1" class="ltx_text" style="font-size:70%;">(28.1%<math id="S2.T1.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.3.3.1.1.m1.1a"><mo stretchy="false" id="S2.T1.3.3.3.1.1.m1.1.1" xref="S2.T1.3.3.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.1.m1.1b"><ci id="S2.T1.3.3.3.1.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
<td id="S2.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.4</td>
<td id="S2.T1.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.31</td>
<td id="S2.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.85Â <span id="S2.T1.4.4.4.2.1" class="ltx_text" style="font-size:70%;">(33.4%<math id="S2.T1.4.4.4.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.4.4.4.2.1.m1.1a"><mo stretchy="false" id="S2.T1.4.4.4.2.1.m1.1.1" xref="S2.T1.4.4.4.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.2.1.m1.1b"><ci id="S2.T1.4.4.4.2.1.m1.1.1.cmml" xref="S2.T1.4.4.4.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.2.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S2.T1.6.6.6" class="ltx_tr">
<th id="S2.T1.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">D. B seeded monolingual MinWER ASR</th>
<td id="S2.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.81</td>
<td id="S2.T1.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.93</td>
<td id="S2.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.87Â <span id="S2.T1.5.5.5.1.1" class="ltx_text" style="font-size:70%;">(6.9%<math id="S2.T1.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.5.5.5.1.1.m1.1a"><mo stretchy="false" id="S2.T1.5.5.5.1.1.m1.1.1" xref="S2.T1.5.5.5.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.1.1.m1.1b"><ci id="S2.T1.5.5.5.1.1.m1.1.1.cmml" xref="S2.T1.5.5.5.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</td>
<td id="S2.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.4</td>
<td id="S2.T1.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.33</td>
<td id="S2.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.85Â <span id="S2.T1.6.6.6.2.1" class="ltx_text" style="font-size:70%;">(13.3%<math id="S2.T1.6.6.6.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.6.6.6.2.1.m1.1a"><mo stretchy="false" id="S2.T1.6.6.6.2.1.m1.1.1" xref="S2.T1.6.6.6.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.2.1.m1.1b"><ci id="S2.T1.6.6.6.2.1.m1.1.1.cmml" xref="S2.T1.6.6.6.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S2.T1.8.8.8" class="ltx_tr">
<th id="S2.T1.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.3.1" class="ltx_text ltx_font_bold">E. C seeded monolingual MinWER ASR</span></th>
<td id="S2.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.4.1" class="ltx_text ltx_font_bold">9.71</span></td>
<td id="S2.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.5.1" class="ltx_text ltx_font_bold">17.57</span></td>
<td id="S2.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.7.1.1" class="ltx_text ltx_font_bold">13.64Â <span id="S2.T1.7.7.7.1.1.1" class="ltx_text" style="font-size:70%;">(36.2%<math id="S2.T1.7.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.7.7.7.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.7.7.7.1.1.1.m1.1.1" xref="S2.T1.7.7.7.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.1.1.1.m1.1b"><ci id="S2.T1.7.7.7.1.1.1.m1.1.1.cmml" xref="S2.T1.7.7.7.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span></span></td>
<td id="S2.T1.8.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.6.1" class="ltx_text ltx_font_bold">9.61</span></td>
<td id="S2.T1.8.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.7.1" class="ltx_text ltx_font_bold">3.87</span></td>
<td id="S2.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.8.8.8.2.1" class="ltx_text ltx_font_bold">6.74Â <span id="S2.T1.8.8.8.2.1.1" class="ltx_text" style="font-size:70%;">(42.8%<math id="S2.T1.8.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.8.8.8.2.1.1.m1.1a"><mo stretchy="false" id="S2.T1.8.8.8.2.1.1.m1.1.1" xref="S2.T1.8.8.8.2.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.2.1.1.m1.1b"><ci id="S2.T1.8.8.8.2.1.1.m1.1.1.cmml" xref="S2.T1.8.8.8.2.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.2.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.10.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.11.2" class="ltx_text" style="font-size:90%;">Effectiveness of Multilingual Encoder-Only Pretraining during RNNT Training and MinWER Finetuning Stages Across Languages: A Comparative Analysis Against Multilingual and Monolingual Baselines.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>AR-RNNT Loss Training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.10" class="ltx_p">An RNNT model comprises three main components: an encoder, a prediction network, and a joiner network. The encoder processes acoustic feature vectors <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x_{t}</annotation></semantics></math> into a sequence of hidden states <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="h^{enc}t" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><msup id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">h</mi><mrow id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.3.2" xref="S3.SS1.p1.2.m2.1.1.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.2.3.1" xref="S3.SS1.p1.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.2.m2.1.1.2.3.3" xref="S3.SS1.p1.2.m2.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.2.3.1a" xref="S3.SS1.p1.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.2.m2.1.1.2.3.4" xref="S3.SS1.p1.2.m2.1.1.2.3.4.cmml">c</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">â„</ci><apply id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3"><times id="S3.SS1.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3.2">ğ‘’</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3.3">ğ‘›</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.4.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3.4">ğ‘</ci></apply></apply><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">h^{enc}t</annotation></semantics></math> over time <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">t</annotation></semantics></math>. The prediction network takes the previous sub-word label prediction <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="y{u-1}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mrow id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.2.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.2.1" xref="S3.SS1.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S3.SS1.p1.4.m4.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.2.3.cmml">u</mi></mrow><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><minus id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></minus><apply id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><times id="S3.SS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.2.1"></times><ci id="S3.SS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2.2">ğ‘¦</ci><ci id="S3.SS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3">ğ‘¢</ci></apply><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">y{u-1}</annotation></semantics></math> and produces a hidden representation <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="h^{pre}_{u}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msubsup id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">h</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">u</mi><mrow id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.3.2" xref="S3.SS1.p1.5.m5.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.1.1.2.3.1" xref="S3.SS1.p1.5.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.5.m5.1.1.2.3.3" xref="S3.SS1.p1.5.m5.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.1.1.2.3.1a" xref="S3.SS1.p1.5.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.5.m5.1.1.2.3.4" xref="S3.SS1.p1.5.m5.1.1.2.3.4.cmml">e</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">â„</ci><apply id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3"><times id="S3.SS1.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3.1"></times><ci id="S3.SS1.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3.2">ğ‘</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3.3">ğ‘Ÿ</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.4.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3.4">ğ‘’</ci></apply></apply><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">h^{pre}_{u}</annotation></semantics></math> for label index <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">u</annotation></semantics></math>. The joiner network combines encoder output <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="h^{enc}t" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><msup id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2.2" xref="S3.SS1.p1.7.m7.1.1.2.2.cmml">h</mi><mrow id="S3.SS1.p1.7.m7.1.1.2.3" xref="S3.SS1.p1.7.m7.1.1.2.3.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2.3.2" xref="S3.SS1.p1.7.m7.1.1.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m7.1.1.2.3.1" xref="S3.SS1.p1.7.m7.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.7.m7.1.1.2.3.3" xref="S3.SS1.p1.7.m7.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m7.1.1.2.3.1a" xref="S3.SS1.p1.7.m7.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.7.m7.1.1.2.3.4" xref="S3.SS1.p1.7.m7.1.1.2.3.4.cmml">c</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m7.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><times id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1"></times><apply id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2.2">â„</ci><apply id="S3.SS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3"><times id="S3.SS1.p1.7.m7.1.1.2.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3.1"></times><ci id="S3.SS1.p1.7.m7.1.1.2.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3.2">ğ‘’</ci><ci id="S3.SS1.p1.7.m7.1.1.2.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3.3">ğ‘›</ci><ci id="S3.SS1.p1.7.m7.1.1.2.3.4.cmml" xref="S3.SS1.p1.7.m7.1.1.2.3.4">ğ‘</ci></apply></apply><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">h^{enc}t</annotation></semantics></math> and prediction network output <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="h^{pre}u" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><msup id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2.2" xref="S3.SS1.p1.8.m8.1.1.2.2.cmml">h</mi><mrow id="S3.SS1.p1.8.m8.1.1.2.3" xref="S3.SS1.p1.8.m8.1.1.2.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2.3.2" xref="S3.SS1.p1.8.m8.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.2.3.1" xref="S3.SS1.p1.8.m8.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.2.3.3" xref="S3.SS1.p1.8.m8.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.2.3.1a" xref="S3.SS1.p1.8.m8.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.2.3.4" xref="S3.SS1.p1.8.m8.1.1.2.3.4.cmml">e</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><times id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1"></times><apply id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.2.1.cmml" xref="S3.SS1.p1.8.m8.1.1.2">superscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2.2">â„</ci><apply id="S3.SS1.p1.8.m8.1.1.2.3.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3"><times id="S3.SS1.p1.8.m8.1.1.2.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3.1"></times><ci id="S3.SS1.p1.8.m8.1.1.2.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3.2">ğ‘</ci><ci id="S3.SS1.p1.8.m8.1.1.2.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3.3">ğ‘Ÿ</ci><ci id="S3.SS1.p1.8.m8.1.1.2.3.4.cmml" xref="S3.SS1.p1.8.m8.1.1.2.3.4">ğ‘’</ci></apply></apply><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">h^{pre}u</annotation></semantics></math> to compute output logits <math id="S3.SS1.p1.9.m9.2" class="ltx_Math" alttext="h{t,u}" display="inline"><semantics id="S3.SS1.p1.9.m9.2a"><mrow id="S3.SS1.p1.9.m9.2.2.1" xref="S3.SS1.p1.9.m9.2.2.2.cmml"><mrow id="S3.SS1.p1.9.m9.2.2.1.1" xref="S3.SS1.p1.9.m9.2.2.1.1.cmml"><mi id="S3.SS1.p1.9.m9.2.2.1.1.2" xref="S3.SS1.p1.9.m9.2.2.1.1.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.9.m9.2.2.1.1.1" xref="S3.SS1.p1.9.m9.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.9.m9.2.2.1.1.3" xref="S3.SS1.p1.9.m9.2.2.1.1.3.cmml">t</mi></mrow><mo id="S3.SS1.p1.9.m9.2.2.1.2" xref="S3.SS1.p1.9.m9.2.2.2.cmml">,</mo><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.2b"><list id="S3.SS1.p1.9.m9.2.2.2.cmml" xref="S3.SS1.p1.9.m9.2.2.1"><apply id="S3.SS1.p1.9.m9.2.2.1.1.cmml" xref="S3.SS1.p1.9.m9.2.2.1.1"><times id="S3.SS1.p1.9.m9.2.2.1.1.1.cmml" xref="S3.SS1.p1.9.m9.2.2.1.1.1"></times><ci id="S3.SS1.p1.9.m9.2.2.1.1.2.cmml" xref="S3.SS1.p1.9.m9.2.2.1.1.2">â„</ci><ci id="S3.SS1.p1.9.m9.2.2.1.1.3.cmml" xref="S3.SS1.p1.9.m9.2.2.1.1.3">ğ‘¡</ci></apply><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">ğ‘¢</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.2c">h{t,u}</annotation></semantics></math>, followed by a softmax operation to obtain final posterior probabilities <math id="S3.SS1.p1.10.m10.2" class="ltx_Math" alttext="P{t,u}" display="inline"><semantics id="S3.SS1.p1.10.m10.2a"><mrow id="S3.SS1.p1.10.m10.2.2.1" xref="S3.SS1.p1.10.m10.2.2.2.cmml"><mrow id="S3.SS1.p1.10.m10.2.2.1.1" xref="S3.SS1.p1.10.m10.2.2.1.1.cmml"><mi id="S3.SS1.p1.10.m10.2.2.1.1.2" xref="S3.SS1.p1.10.m10.2.2.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.2.2.1.1.1" xref="S3.SS1.p1.10.m10.2.2.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p1.10.m10.2.2.1.1.3" xref="S3.SS1.p1.10.m10.2.2.1.1.3.cmml">t</mi></mrow><mo id="S3.SS1.p1.10.m10.2.2.1.2" xref="S3.SS1.p1.10.m10.2.2.2.cmml">,</mo><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.2b"><list id="S3.SS1.p1.10.m10.2.2.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1"><apply id="S3.SS1.p1.10.m10.2.2.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1"><times id="S3.SS1.p1.10.m10.2.2.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.1"></times><ci id="S3.SS1.p1.10.m10.2.2.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.2">ğ‘ƒ</ci><ci id="S3.SS1.p1.10.m10.2.2.1.1.3.cmml" xref="S3.SS1.p1.10.m10.2.2.1.1.3">ğ‘¡</ci></apply><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">ğ‘¢</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.2c">P{t,u}</annotation></semantics></math> for output tokens.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">In the context of the AR-RNNT loss function, alignment constraints are imposed to ensure the alignment between input and output sequences follows specific restrictions, beneficial for tasks like speech recognition. The loss function of AR-RNNT is derived from the negative log posterior of the output label sequence <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">y</annotation></semantics></math> given the input acoustic feature <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">x</annotation></semantics></math>, incorporating alignment constraints to enhance model performance in sequence-to-sequence tasks.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="L_{RNNT}=-logP(y|x)" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">T</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1a" xref="S3.E1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2c" xref="S3.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ğ¿</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">ğ‘…</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ğ‘</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">ğ‘</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">ğ‘‡</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><minus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ğ‘™</ci><ci id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4">ğ‘œ</ci><ci id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5">ğ‘”</ci><ci id="S3.E1.m1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.6">ğ‘ƒ</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L_{RNNT}=-logP(y|x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">where <math id="S3.SS1.p3.1.m1.2" class="ltx_Math" alttext="P(y|x)=\sum_{\hat{y}}P(\hat{y}|x),\hat{y}\in A" display="inline"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.2.2" xref="S3.SS1.p3.1.m1.2.2.3.cmml"><mrow id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml"><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.cmml"><msub id="S3.SS1.p3.1.m1.1.1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.cmml"><mo id="S3.SS1.p3.1.m1.1.1.1.1.2.2.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mover accent="true" id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.2.cmml">y</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.1.cmml">^</mo></mover></msub><mrow id="S3.SS1.p3.1.m1.1.1.1.1.2.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.2.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.cmml"><mover accent="true" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.2.cmml">y</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.1.cmml">^</mo></mover><mo fence="false" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.SS1.p3.1.m1.2.2.2.3" xref="S3.SS1.p3.1.m1.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p3.1.m1.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.cmml"><mover accent="true" id="S3.SS1.p3.1.m1.2.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.p3.1.m1.2.2.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.2.2.cmml">y</mi><mo id="S3.SS1.p3.1.m1.2.2.2.2.2.1" xref="S3.SS1.p3.1.m1.2.2.2.2.2.1.cmml">^</mo></mover><mo id="S3.SS1.p3.1.m1.2.2.2.2.1" xref="S3.SS1.p3.1.m1.2.2.2.2.1.cmml">âˆˆ</mo><mi id="S3.SS1.p3.1.m1.2.2.2.2.3" xref="S3.SS1.p3.1.m1.2.2.2.2.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><apply id="S3.SS1.p3.1.m1.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.2.2.3a.cmml" xref="S3.SS1.p3.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><eq id="S3.SS1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.3"></eq><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2"></times><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3">ğ‘ƒ</ci><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2"><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2">subscript</csymbol><sum id="S3.SS1.p3.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.2"></sum><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3"><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.1">^</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.2.3.2">ğ‘¦</ci></apply></apply><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1"><times id="S3.SS1.p3.1.m1.1.1.1.1.2.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.2"></times><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.3">ğ‘ƒ</ci><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2"><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.1">^</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2.1.1.1.1.3">ğ‘¥</ci></apply></apply></apply></apply><apply id="S3.SS1.p3.1.m1.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2"><in id="S3.SS1.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.1"></in><apply id="S3.SS1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.2"><ci id="S3.SS1.p3.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.2.1">^</ci><ci id="S3.SS1.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.2.2">ğ‘¦</ci></apply><ci id="S3.SS1.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.3">ğ´</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">P(y|x)=\sum_{\hat{y}}P(\hat{y}|x),\hat{y}\in A</annotation></semantics></math>. A is the set of all
the possible alignments (containing both blank and non-blank
labels) between input x and output y.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MinWER Loss Finetuning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">The MinWER-loss fine-tuning adopts the alignment-restricted MinWER loss and set-up proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which is an extension of the original RNNT MinWER training method introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This method is specifically engineered to prioritize training with the WER objective and enable discrinimative training. It does so by employing a loss function based on the weighted average of word errors within N-best hypotheses.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="L_{MinWER}=\sum_{y_{i}}\hat{P}(y_{i}|x)R(y_{i},y^{r})" display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.5" xref="S3.E2.m1.3.3.5.cmml"><mi id="S3.E2.m1.3.3.5.2" xref="S3.E2.m1.3.3.5.2.cmml">L</mi><mrow id="S3.E2.m1.3.3.5.3" xref="S3.E2.m1.3.3.5.3.cmml"><mi id="S3.E2.m1.3.3.5.3.2" xref="S3.E2.m1.3.3.5.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.5.3.1" xref="S3.E2.m1.3.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.5.3.3" xref="S3.E2.m1.3.3.5.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.5.3.1a" xref="S3.E2.m1.3.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.5.3.4" xref="S3.E2.m1.3.3.5.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.5.3.1b" xref="S3.E2.m1.3.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.5.3.5" xref="S3.E2.m1.3.3.5.3.5.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.5.3.1c" xref="S3.E2.m1.3.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.5.3.6" xref="S3.E2.m1.3.3.5.3.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.5.3.1d" xref="S3.E2.m1.3.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.5.3.7" xref="S3.E2.m1.3.3.5.3.7.cmml">R</mi></mrow></msub><mo rspace="0.111em" id="S3.E2.m1.3.3.4" xref="S3.E2.m1.3.3.4.cmml">=</mo><mrow id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><munder id="S3.E2.m1.3.3.3.4" xref="S3.E2.m1.3.3.3.4.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.3.4.2" xref="S3.E2.m1.3.3.3.4.2.cmml">âˆ‘</mo><msub id="S3.E2.m1.3.3.3.4.3" xref="S3.E2.m1.3.3.3.4.3.cmml"><mi id="S3.E2.m1.3.3.3.4.3.2" xref="S3.E2.m1.3.3.3.4.3.2.cmml">y</mi><mi id="S3.E2.m1.3.3.3.4.3.3" xref="S3.E2.m1.3.3.3.4.3.3.cmml">i</mi></msub></munder><mrow id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml"><mover accent="true" id="S3.E2.m1.3.3.3.3.5" xref="S3.E2.m1.3.3.3.3.5.cmml"><mi id="S3.E2.m1.3.3.3.3.5.2" xref="S3.E2.m1.3.3.3.3.5.2.cmml">P</mi><mo id="S3.E2.m1.3.3.3.3.5.1" xref="S3.E2.m1.3.3.3.3.5.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.3.4" xref="S3.E2.m1.3.3.3.3.4.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.3.4a" xref="S3.E2.m1.3.3.3.3.4.cmml">â€‹</mo><mi id="S3.E2.m1.3.3.3.3.6" xref="S3.E2.m1.3.3.3.3.6.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.3.4b" xref="S3.E2.m1.3.3.3.3.4.cmml">â€‹</mo><mrow id="S3.E2.m1.3.3.3.3.3.2" xref="S3.E2.m1.3.3.3.3.3.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.3.3.3.2.3" xref="S3.E2.m1.3.3.3.3.3.3.cmml">(</mo><msub id="S3.E2.m1.2.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.2.1.1.2.cmml">y</mi><mi id="S3.E2.m1.2.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.3.3.3.3.3.2.4" xref="S3.E2.m1.3.3.3.3.3.3.cmml">,</mo><msup id="S3.E2.m1.3.3.3.3.3.2.2" xref="S3.E2.m1.3.3.3.3.3.2.2.cmml"><mi id="S3.E2.m1.3.3.3.3.3.2.2.2" xref="S3.E2.m1.3.3.3.3.3.2.2.2.cmml">y</mi><mi id="S3.E2.m1.3.3.3.3.3.2.2.3" xref="S3.E2.m1.3.3.3.3.3.2.2.3.cmml">r</mi></msup><mo stretchy="false" id="S3.E2.m1.3.3.3.3.3.2.5" xref="S3.E2.m1.3.3.3.3.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.4.cmml" xref="S3.E2.m1.3.3.4"></eq><apply id="S3.E2.m1.3.3.5.cmml" xref="S3.E2.m1.3.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.5.1.cmml" xref="S3.E2.m1.3.3.5">subscript</csymbol><ci id="S3.E2.m1.3.3.5.2.cmml" xref="S3.E2.m1.3.3.5.2">ğ¿</ci><apply id="S3.E2.m1.3.3.5.3.cmml" xref="S3.E2.m1.3.3.5.3"><times id="S3.E2.m1.3.3.5.3.1.cmml" xref="S3.E2.m1.3.3.5.3.1"></times><ci id="S3.E2.m1.3.3.5.3.2.cmml" xref="S3.E2.m1.3.3.5.3.2">ğ‘€</ci><ci id="S3.E2.m1.3.3.5.3.3.cmml" xref="S3.E2.m1.3.3.5.3.3">ğ‘–</ci><ci id="S3.E2.m1.3.3.5.3.4.cmml" xref="S3.E2.m1.3.3.5.3.4">ğ‘›</ci><ci id="S3.E2.m1.3.3.5.3.5.cmml" xref="S3.E2.m1.3.3.5.3.5">ğ‘Š</ci><ci id="S3.E2.m1.3.3.5.3.6.cmml" xref="S3.E2.m1.3.3.5.3.6">ğ¸</ci><ci id="S3.E2.m1.3.3.5.3.7.cmml" xref="S3.E2.m1.3.3.5.3.7">ğ‘…</ci></apply></apply><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><apply id="S3.E2.m1.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.4.1.cmml" xref="S3.E2.m1.3.3.3.4">subscript</csymbol><sum id="S3.E2.m1.3.3.3.4.2.cmml" xref="S3.E2.m1.3.3.3.4.2"></sum><apply id="S3.E2.m1.3.3.3.4.3.cmml" xref="S3.E2.m1.3.3.3.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.4.3.1.cmml" xref="S3.E2.m1.3.3.3.4.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.4.3.2.cmml" xref="S3.E2.m1.3.3.3.4.3.2">ğ‘¦</ci><ci id="S3.E2.m1.3.3.3.4.3.3.cmml" xref="S3.E2.m1.3.3.3.4.3.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3"><times id="S3.E2.m1.3.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.3.4"></times><apply id="S3.E2.m1.3.3.3.3.5.cmml" xref="S3.E2.m1.3.3.3.3.5"><ci id="S3.E2.m1.3.3.3.3.5.1.cmml" xref="S3.E2.m1.3.3.3.3.5.1">^</ci><ci id="S3.E2.m1.3.3.3.3.5.2.cmml" xref="S3.E2.m1.3.3.3.3.5.2">ğ‘ƒ</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2">ğ‘¦</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">ğ‘¥</ci></apply><ci id="S3.E2.m1.3.3.3.3.6.cmml" xref="S3.E2.m1.3.3.3.3.6">ğ‘…</ci><interval closure="open" id="S3.E2.m1.3.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3.3.2"><apply id="S3.E2.m1.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1.2">ğ‘¦</ci><ci id="S3.E2.m1.2.2.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.3.3.3.3.3.2.2.cmml" xref="S3.E2.m1.3.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.3.2.2.1.cmml" xref="S3.E2.m1.3.3.3.3.3.2.2">superscript</csymbol><ci id="S3.E2.m1.3.3.3.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.3.3.3.2.2.2">ğ‘¦</ci><ci id="S3.E2.m1.3.3.3.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.3.3.3.2.2.3">ğ‘Ÿ</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">L_{MinWER}=\sum_{y_{i}}\hat{P}(y_{i}|x)R(y_{i},y^{r})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.4" class="ltx_p">Here, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\hat{P}(y_{i}|x)" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mover accent="true" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">P</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.1.m1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.SS2.p1.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.SS2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS2.p1.1.m1.1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS2.p1.1.m1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"></times><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><ci id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1">^</ci><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S3.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.2">ğ‘¦</ci><ci id="S3.SS2.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\hat{P}(y_{i}|x)</annotation></semantics></math> represents the posterior probability of a hypothesis <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘¦</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">y_{i}</annotation></semantics></math>, while <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="R(\textperiodcentered)" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.1" xref="S3.SS2.p1.3.m3.1.2.cmml">(</mo><mi mathvariant="normal" id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">Â·</mi><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.2" xref="S3.SS2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><times id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></times><ci id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2">ğ‘…</ci><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">Â·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">R(\textperiodcentered)</annotation></semantics></math> quantifies the risk function, gauging the edit-distance at the word level between the hypothesis yi and the reference transcription <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="y^{r}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">y</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ğ‘¦</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">y^{r}</annotation></semantics></math> in MinWER training.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For the baseline RNNT model, the encoder is a 20-layer streamable low-latency Emformer with 40 ms lookahead, and process audio segments of 160 ms. It has input dimension of 512, hidden dimension 2048, 8 self-attention heads, and 1024-dimensional FC projection. The predictor consists of three LSTM layers with 512
hidden dimension.
The output vocabulary comprises 5001 unigram SentencePieces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, derived from the training transcripts, including the blank symbol We train all models using Alignment Restricted RNNT
loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, where the alignment is provided by a hybrid acoustic model.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Additionally, regarding the pretraining methodology, we explored different scenarios where we could apply pretraining to these components individually or collectively. Interestingly, our findings revealed that applying pretraining exclusively to the encoder component yielded better convergence compared to the alternative scenarios. This suggests that focusing pretraining efforts solely on the encoder enhances the model's ability to learn and adapt effectively to the given task. As a result, we have decided to adopt encoder-only pretraining consistently for all models in this study.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We utilize two multilingual datasets for our experiments:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Multilingual Librispeech (MLS) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>:</span> The MLS dataset contains multilingual speech derived from read audiobooks. We select four languages to be studied, English (EN), French (FR), Italian (IT), and Dutch (NL). The sizes of the training audio for the 4 languages are 44.7k hrs, 1.1k hrs, 0.2k hrs, 1.6k hrs, respectively. We categorized Italian (IT) and French (FR) as low-resource languages due to the relatively limited amount of training data and development hours allocated to these languages when compared to others. For evaluation purposes, we used the MLS test set.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">In-house datasets:</span> This dataset is a collection of data from various sources. These datasets were collected from third-party vendors via crowd-sourced volunteers responding to artificial prompts with mobile devices. The content varies from voice assistant commands to simulations of conversations between people, as well as Facebook products such as public Facebook videos and voice commands to Portal. Videos used in this dataset are from 120 different countries. Additionally, data was generated from an in-house TTS model to increase the diversity of sentence patterns in our training data. All in-house datasets are de-identified. Depending on the source, the data was further augmented with various distortion methods, including speed perturbation, simulated reverberation, and randomly sampled additive background noise extracted from public Facebook videos. We selected five languages for in-domain datasets, including English (EN), French (FR), Italian (IT), German (DE), and Spanish (ES). The sizes of the training audio for the 4 languages are 1.6k hrs, 0.6k hrs, 0.5k hrs, 0.3k hrs, 0.9k hrs, respectively. In alignment with MLS's classification of low-resource languages, we have similarly categorized IT and FR as low-resource languages for this dataset. For evaluation purposes, we use handtranscribed data from the Ray Ban / Meta Smartglasses, Portal, Video, and Conversational data sources, ranging from 3K-15K utterances with no overlap with the training data.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Impact of Transfer Learning: RNNT Training vs. MinWER Finetuning</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">This study focuses on examining the influence of multilingual pretraining on monolingual ASR models. It investigates whether applying pretraining proves beneficial at the RNNT training stage or during Minwer finetuning.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We initially trained two multilingual models on the MLS dataset and the in-house dataset, respectively. These multilingual models served as the foundation for training monolingual models. We established a baseline by training monolingual ASR models on individual language data from MLS and in-house datasets. Subsequently, we conducted a performance analysis, comparing four distinct configurations.</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">In the first configuration, we trained a multilingual model with combined data from various languages [section <a href="#S4.SS2" title="4.2 Dataset â€£ 4 Experimental Setup â€£ Towards scalable efficient on-device ASR with transfer learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>], using it as a seed model.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">The second configuration involved applying pretraining to the RNNT training stage.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">In the third configuration, pretraining took place during the MinWER finetuning stage.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">Lastly, our fourth configuration utilized pretraining during the RNNT training stage, followed by fine-tuning the converged model using MinWER optimization.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Our findings suggest that the third configuration works best, which involves initial pretraining during the RNNT stage followed by fine-tuning with MinWER. This configuration demonstrates a substantial improvement, achieving an average WERR of 36.2% and 42.8% compared to the baseline for the MLS and in-house datasets, respectively. We employ this best-performed configuration for all subsequent experiments in this study.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Influence of Domain-Specific Pretraining: In-Domain vs Out-of-Domain</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This study explores how the utilization of a model trained in a specific domain influences its performance when applied to a different domain. To conduct our experiments, we devised two primary approaches: in-domain pretraining and out-of-domain pretraining. In the case of in-domain pretraining, we initially trained a multilingual model on the MLS dataset. Subsequently, using this model as a seed, we further trained monolingual models on language-specific data exclusively from the MLS dataset. On the other hand, out-of-domain pretraining involved training a multilingual model on the in-house dataset and then using it as a seed to train monolingual models on language-specific data from the MLS dataset.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Our comprehensive experimentation aims to uncover the subtle dynamics of domain-specific pretraining, specifically delving into the distinctions between in-domain and out-of-domain pretraining. Our results reveal that in-domain pretraining led to a 36.2% improvement compared to monolingual baselines. Conversely, out-of-domain pretraining demonstrated a more substantial impact with a 46.4% Word Error Rate Reduction (WERR) against the baseline.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:241.2pt;height:98.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.0pt,13.9pt) scale(0.78,0.78) ;">
<table id="S5.T2.2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.2.3.1" class="ltx_tr">
<td id="S5.T2.2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S5.T2.2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">In-domain pretraining | Seed: MLS Multilingual, Target: MLS</span></td>
</tr>
<tr id="S5.T2.2.2.2.4.2" class="ltx_tr">
<td id="S5.T2.2.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.4.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T2.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.4.2.2.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S5.T2.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.4.2.3.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S5.T2.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.4.2.4.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S5.T2.2.2.2.5.3" class="ltx_tr">
<td id="S5.T2.2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">A. Monolingual ASR <span id="S5.T2.2.2.2.5.3.1.1" class="ltx_text" style="font-size:70%;">(baseline)</span>
</td>
<td id="S5.T2.2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.99</td>
<td id="S5.T2.2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.78</td>
<td id="S5.T2.2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.38</td>
</tr>
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Seeded monolingual MinWER ASR</td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.71</td>
<td id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.57</td>
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.64Â <span id="S5.T2.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(36.2%<math id="S5.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S5.T2.2.2.2.6.4" class="ltx_tr">
<td id="S5.T2.2.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S5.T2.2.2.2.6.4.1.1" class="ltx_text ltx_font_bold">Out-of-domain pretraining | Seed: In-house Multilingual, Target: MLS</span></td>
</tr>
<tr id="S5.T2.2.2.2.7.5" class="ltx_tr">
<td id="S5.T2.2.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">A. Monolingual ASR <span id="S5.T2.2.2.2.7.5.1.1" class="ltx_text" style="font-size:70%;">(baseline)</span>
</td>
<td id="S5.T2.2.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.91</td>
<td id="S5.T2.2.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.92</td>
<td id="S5.T2.2.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.93</td>
</tr>
<tr id="S5.T2.2.2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Seeded monolingual MinWER ASR</td>
<td id="S5.T2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">9.61</td>
<td id="S5.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">16.01</td>
<td id="S5.T2.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">12.81Â <span id="S5.T2.2.2.2.2.1.1" class="ltx_text" style="font-size:70%;">(46.4%<math id="S5.T2.2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.2.2.2.2.1.1.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.2.1.1.m1.1.1" xref="S5.T2.2.2.2.2.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.5.2" class="ltx_text" style="font-size:90%;">Impact of In-Domain vs Out-of-Domain Pretraining on Low-Resource Languages Compared to Multilingual and Monolingual Baselines.</span></figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">These findings reveals the effectiveness of employing out-of-domain pretraining, leading to an approximately 28.1% higher WERR compared to in-domain pretraining. Leveraging a pretrained model trained on more diverse data of the same languages proved beneficial, enhancing model generalization and leading to significant performance improvements.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Effect of Transfer Learning on Rare and Non-Rare Words</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">This study aims to delve into the differntial effectiveness of transfer learning in enhancing both rare and non-rare words. In our experiment, words are categorized as rare or non-rare based on their frequency of occurrence. Specifically, rare words are identified as those with infrequent occurrences, typically falling below a predetermined threshold, while non-rare words are considered to be more commonly used.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Our investigation focused on the impact of in-domain and out-of-domain pretraining on the performance of rare and non-rare words. Interestingly, in the context of in-domain pretraining, non-rare words demonstrated a higher benefit compared to rare words, with rare words exhibiting a 32.1% improvement, while non-rare words showed a 43.2% WERR against the baseline. On the other hand, out-of-domain pretraining proved to be more effective for rare words than non-rare words, showcasing a 46.2% WERR for rare words and a 42.1% WERR for non-rare words. Considering out-of-domain pretraining as a more diverse approach contributes significantly to overall improvement. However, in the comparison between rare and non-rare words, it becomes evident that rare words experienced a greater benefit in this particular scenario.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.8pt;height:81.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-97.0pt,31.7pt) scale(0.56,0.56) ;">
<table id="S5.T3.4.4.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.4.4.4.5.1" class="ltx_tr">
<td id="S5.T3.4.4.4.5.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7"><span id="S5.T3.4.4.4.5.1.1.1" class="ltx_text ltx_font_bold">In-domain pretraining | Seed: MLS Multilingual, Target: MLS</span></td>
</tr>
<tr id="S5.T3.4.4.4.6.2" class="ltx_tr">
<td id="S5.T3.4.4.4.6.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T3.4.4.4.6.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T3.4.4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T3.4.4.4.6.2.2.1" class="ltx_text ltx_font_bold">Rare</span></td>
<td id="S5.T3.4.4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T3.4.4.4.6.2.3.1" class="ltx_text ltx_font_bold">Non-rare</span></td>
</tr>
<tr id="S5.T3.4.4.4.7.3" class="ltx_tr">
<td id="S5.T3.4.4.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.1.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S5.T3.4.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.2.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S5.T3.4.4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.3.1" class="ltx_text ltx_font_bold">Avg</span></td>
<td id="S5.T3.4.4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.4.1" class="ltx_text ltx_font_bold">FR</span></td>
<td id="S5.T3.4.4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.5.1" class="ltx_text ltx_font_bold">IT</span></td>
<td id="S5.T3.4.4.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.4.4.4.7.3.6.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S5.T3.4.4.4.8.4" class="ltx_tr">
<td id="S5.T3.4.4.4.8.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">A. Monolingual ASR</td>
<td id="S5.T3.4.4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.33</td>
<td id="S5.T3.4.4.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.22</td>
<td id="S5.T3.4.4.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.26</td>
<td id="S5.T3.4.4.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.58</td>
<td id="S5.T3.4.4.4.8.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.11</td>
<td id="S5.T3.4.4.4.8.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.29</td>
</tr>
<tr id="S5.T3.2.2.2.2" class="ltx_tr">
<td id="S5.T3.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Seeded monolingual MinWER ASR</td>
<td id="S5.T3.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.34</td>
<td id="S5.T3.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.67</td>
<td id="S5.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.01Â <span id="S5.T3.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(32.1%<math id="S5.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
<td id="S5.T3.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.73</td>
<td id="S5.T3.2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.29</td>
<td id="S5.T3.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.51Â <span id="S5.T3.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">(43.2%<math id="S5.T3.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.2.2.1.m1.1.1" xref="S5.T3.2.2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
</tr>
<tr id="S5.T3.4.4.4.9.5" class="ltx_tr">
<td id="S5.T3.4.4.4.9.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7"><span id="S5.T3.4.4.4.9.5.1.1" class="ltx_text ltx_font_bold">Out-of-domain pretraining | Seed: In-house Multilingual, Target: MLS</span></td>
</tr>
<tr id="S5.T3.4.4.4.10.6" class="ltx_tr">
<td id="S5.T3.4.4.4.10.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">A. Monolingual ASR</td>
<td id="S5.T3.4.4.4.10.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.26</td>
<td id="S5.T3.4.4.4.10.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.71</td>
<td id="S5.T3.4.4.4.10.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.98</td>
<td id="S5.T3.4.4.4.10.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.76</td>
<td id="S5.T3.4.4.4.10.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.77</td>
<td id="S5.T3.4.4.4.10.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.23</td>
</tr>
<tr id="S5.T3.4.4.4.4" class="ltx_tr">
<td id="S5.T3.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Seeded monolingual MinWER ASR</td>
<td id="S5.T3.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">20.16</td>
<td id="S5.T3.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15.32</td>
<td id="S5.T3.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17.74Â <span id="S5.T3.3.3.3.3.1.1" class="ltx_text" style="font-size:70%;">(46.2%<math id="S5.T3.3.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.3.3.3.3.1.1.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.3.1.1.m1.1.1" xref="S5.T3.3.3.3.3.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.1.1.m1.1b"><ci id="S5.T3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T3.3.3.3.3.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
<td id="S5.T3.4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7.51</td>
<td id="S5.T3.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15.91</td>
<td id="S5.T3.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">11.71Â <span id="S5.T3.4.4.4.4.2.1" class="ltx_text" style="font-size:70%;">(42.1%<math id="S5.T3.4.4.4.4.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.4.4.4.4.2.1.m1.1a"><mo stretchy="false" id="S5.T3.4.4.4.4.2.1.m1.1.1" xref="S5.T3.4.4.4.4.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.2.1.m1.1b"><ci id="S5.T3.4.4.4.4.2.1.m1.1.1.cmml" xref="S5.T3.4.4.4.4.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.2.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.6.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.7.2" class="ltx_text" style="font-size:90%;">Influence of In-Domain vs Out-of-Domain Pretraining on Rare and Non-Rare Words.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We further conducted multiple ablation studies to comprehend the impact of hold and warm-up steps in model training, evaluate the zero-shot language performance with our pretraining approach, and analyze how transfer learning influences the model convergence. These aspects will be discussed in the following subsections.

<br class="ltx_break"></p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Influence of Warm-Up and Hold Steps</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In non-transfer learning ASR model training, we opted for a fixed 3-step learning rate (LR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> during warm-up and hold steps, aiming to balance rapid early convergence with stable later-stage training for optimized performance. However, during the transfer learning stage, we observed that setting Warm-Up and Hold Steps to <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mn id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><cn type="integer" id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> expedites model convergence and reduces training time by eliminating initial steps that may hinder the adaptation process. Warm-Up Steps typically involve a gradual adjustment of learning rates, allowing the model to acclimate to the new task. However, in scenarios where the pretrained model is already well-established, starting with a warm-up may be unnecessary. Similarly, Hold Steps, which involve maintaining a constant learning rate, may not be essential when transferring knowledge from a pretrained model with well-tuned parameters. By bypassing these initial adjustments, the model can promptly leverage its existing knowledge and adapt swiftly to the new task, resulting in faster convergence and more efficient training.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Effect of pretraining on zero-shot language performance</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In the follow-up study, we investigated the impact of transfer learning on the performance of zero-shot languages, referring to languages not included in the pretraining model. Our approach involved training a multilingual model using datasets in English, Italian, and French languages. Subsequently, we utilized this model as a seed to initiate the training of monolingual models specifically for German and Spanish. Notably, our findings highlight a significant enhancement in the performance of the Spanish model, achieving a remarkable 26.6% WERR. Conversely, the German model exhibited a WER regression of 44.8%. It is important to note that both models were evaluated against baselines without any pretraining.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:240.0pt;height:43.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.0pt,5.4pt) scale(0.8,0.8) ;">
<table id="S6.T4.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.2.2.2.3.1" class="ltx_tr">
<th id="S6.T4.2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T4.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T4.2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">ES</span></th>
<th id="S6.T4.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T4.2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">DE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.2.2.2.4.1" class="ltx_tr">
<td id="S6.T4.2.2.2.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Monolingual ASR</td>
<td id="S6.T4.2.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.96</td>
<td id="S6.T4.2.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.23</td>
</tr>
<tr id="S6.T4.2.2.2.2" class="ltx_tr">
<td id="S6.T4.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Seeded monolingual MinWER ASR</td>
<td id="S6.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.58Â <span id="S6.T4.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">(26.6%<math id="S6.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T4.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T4.1.1.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</td>
<td id="S6.T4.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17.71Â <span id="S6.T4.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">(44.8%<math id="S6.T4.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S6.T4.2.2.2.2.2.1.m1.1.1" xref="S6.T4.2.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.2.1.m1.1b"><ci id="S6.T4.2.2.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.5.2" class="ltx_text" style="font-size:90%;">Illustrating the impact of pretraining on zero-shot languages: exploring scenarios where the language is not part of the pretraining stage.</span></figcaption>
</figure>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">We speculate that the reason Spanish works well with English, Italian, and French is because they share similar language roots. On the other hand, German might not fit as well due to its potential linguistic differences and distinct acoustic patterns. This suggests that when creating a pretraining model, combining languages with similar linguistic backgrounds could significantly enhance its performance with languages that were not part of the initial training set.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Impact of Transfer Learning on Model Convergence</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In our investigation, initializing the model with a fully converged pretrained seed model yielded noteworthy benefits in terms of faster convergence compared to training the model from scratch. On average, our findings indicate a substantial reduction in training time, with the transfer learning model demonstrating a remarkable decrease from approximately 7 days (168 hours) to around 2 days (48 hours). Additionally, this accelerated convergence is accompanied by a significant reduction in power consumption, dropping from 1196 kWH to 299 kWH, marking a 75% decrease. This study sheds light on the efficiency gains achieved through the incorporation of transfer learning in the model initialization process.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">The faster convergence of the transfer learning model is due to leveraging knowledge from the fully converged pretrained seed model. By utilizing learned features, representations, and patterns, the transfer learning model benefits from a more informed initialization, facilitating rapid adaptation to the target task specifics. This results in fewer iterations needed for parameter optimization and achieving convergence. The knowledge transfer from the pretrained model expedites the learning process.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In summary, the study highlights the consistent effectiveness of incorporating multilingual pretraining at the baseline stage for enhancing monolingual ASR, particularly in languages like Italian and French. Notably, out-of-domain pretraining proves even more advantageous compared to in-domain pretraining across both baseline scenarios. Additionally, the research underscores the positive impact of multilingual pretraining on both rare and non-rare words, with non-rare words showing a slightly greater influence in in-domain pretraining, while rare words benefit more from out-of-domain pretraining. These findings collectively underscore the strategic importance of pretraining approaches in optimizing ASR models for enhanced language-specific and cross-linguistic performance.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In future work, we intend to explore targeted layer-wise transfer learning, focusing on key layers for effective knowledge transfer rather than transferring the entire encoder-based pretraining. We also aim to use language-based identification to selectively transfer knowledge within the same language family, addressing cases where language relationships may impact performance differently, such as observed effects on German and Spanish. These steps will enhance the precision of multilingual transfer learning.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S.Â Thomas, S.Â Ganapathy, and H.Â Hermansky, ``Multilingual mlp features for low-resource lvcsr systems,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2012, pp. 4269â€“4272.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J.Â Kunze, L.Â Kirsch, I.Â Kurenkov, A.Â Krug, J.Â Johannsmeier, and S.Â Stober, ``Transfer learning for speech recognition on a budget,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.00290</em>, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.Â Khare, A.Â R. Mittal, A.Â Diwan, S.Â Sarawagi, P.Â Jyothi, and S.Â Bharadwaj, ``Low resource asr: The surprising effectiveness of high resource transliteration.'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2021, pp. 1529â€“1533.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A.Â Ghoshal, P.Â Swietojanski, and S.Â Renals, ``Multilingual training of deep neural networks,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2013 IEEE international conference on acoustics, speech and signal processing</em>.Â Â Â IEEE, 2013, pp. 7319â€“7323.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.-T. Huang, J.Â Li, D.Â Yu, L.Â Deng, and Y.Â Gong, ``Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2013 IEEE international conference on acoustics, speech and signal processing</em>.Â Â Â IEEE, 2013, pp. 7304â€“7308.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.Â Conneau, A.Â Baevski, R.Â Collobert, A.Â Mohamed, and M.Â Auli, ``Unsupervised cross-lingual representation learning for speech recognition,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.13979</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.Â Huang, D.Â Yu, C.Â Liu, and Y.Â Gong, ``Multi-accent deep neural network acoustic model with accent-specific top layer using the kld-regularized model adaptation,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Fifteenth Annual Conference of the International Speech Communication Association</em>, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â Babu, C.Â Wang, A.Â Tjandra, K.Â Lakhotia, Q.Â Xu, N.Â Goyal, K.Â Singh, P.Â von Platen, Y.Â Saraf, J.Â Pino <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Xls-r: Self-supervised cross-lingual speech representation learning at scale,'' <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P.Â Swietojanski, A.Â Ghoshal, and S.Â Renals, ``Unsupervised cross-lingual knowledge transfer in dnn-based lvcsr,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2012 IEEE Spoken Language Technology Workshop (SLT)</em>.Â Â Â IEEE, 2012, pp. 246â€“251.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
G.Â Heigold, V.Â Vanhoucke, A.Â Senior, P.Â Nguyen, M.Â Ranzato, M.Â Devin, and J.Â Dean, ``Multilingual acoustic models using distributed deep neural networks,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2013 IEEE international conference on acoustics, speech and signal processing</em>.Â Â Â IEEE, 2013, pp. 8619â€“8623.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H.Â Arsikere, A.Â Sapru, and S.Â Garimella, ``Multi-dialect acoustic modeling using phone mapping and online i-vectors,'' 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D.Â Yu and L.Â Deng, ``Efficient and effective algorithms for training single-hidden-layer neural networks,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Pattern recognition letters</em>, vol.Â 33, no.Â 5, pp. 554â€“558, 2012.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L.Â Deng and J.Â Platt, ``Ensemble deep learning for speech recognition,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. interspeech</em>, 2014.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.Â Elfeky, M.Â Bastani, X.Â Velez, P.Â Moreno, and A.Â Waters, ``Towards acoustic model unification across dialects,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Spoken Language Technology Workshop (SLT)</em>.Â Â Â IEEE, 2016, pp. 624â€“628.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Kannan, A.Â Datta, T.Â N. Sainath, E.Â Weinstein, B.Â Ramabhadran, Y.Â Wu, A.Â Bapna, Z.Â Chen, and S.Â Lee, ``Large-scale multilingual speech recognition with a streaming end-to-end model,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05330</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
B.Â Thomas, S.Â Kessler, and S.Â Karout, ``Efficient adapter transfer of self-supervised speech models for automatic speech recognition,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 7102â€“7106.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B.Â Li, Y.Â Zhang, T.Â Sainath, Y.Â Wu, and W.Â Chan, ``Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2019, pp. 5621â€“5625.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V.Â M. Shetty and M.Â S.Â M. NJ, ``Improving the performance of transformer based low resource speech recognition for indian languages,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 8279â€“8283.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.Â Guo, N.Â Moritz, Y.Â Ma, F.Â Seide, C.Â Wu, M.Â Jay, O.Â Kalinli, C.Â Fuegen, and M.Â Seltzer, ``Effective internal language model training and fusion for factorized transducer model,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.Â Guo, G.Â Tiwari, J.Â Droppo, M.Â VanÂ Segbroeck, C.-W. Huang, A.Â Stolcke, and R.Â Maas, ``Efficient minimum word error rate training of rnn-transducer for end-to-end speech recognition,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T.Â Kudo, ``Subword regularization: Improving neural network translation models with multiple subword candidates,'' 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â Mahadeokar, Y.Â Shangguan, D.Â Le, G.Â Keren, H.Â Su, T.Â Le, C.-F. Yeh, C.Â Fuegen, and M.Â L. Seltzer, ``Alignment restricted streaming recurrent neural network transducer,'' 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V.Â Pratap, Q.Â Xu, A.Â Sriram, G.Â Synnaeve, and R.Â Collobert, ``Mls: A large-scale multilingual dataset for speech research,'' <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.03411</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D.Â S. Park, W.Â Chan, Y.Â Zhang, C.-C. Chiu, B.Â Zoph, E.Â D. Cubuk, and Q.Â V. Le, ``Specaugment: A simple data augmentation method for automatic speech recognition,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.08779</em>, 2019.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.16663" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.16664" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.16664">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.16664" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.16665" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 16:24:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
