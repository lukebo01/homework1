<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.02052] The USTC-NERCSLIP Systems for the ICMC-ASR Challenge</title><meta property="og:description" content="This report describes the submitted system to the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) challenge, which considers the ASR task with multi-speaker overlapping and Mandarin accent dynamics in the ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The USTC-NERCSLIP Systems for the ICMC-ASR Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The USTC-NERCSLIP Systems for the ICMC-ASR Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.02052">

<!--Generated on Mon Aug  5 18:50:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The USTC-NERCSLIP Systems for the ICMC-ASR Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This report describes the submitted system to the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) challenge, which considers the ASR task with multi-speaker overlapping and Mandarin accent dynamics in the ICMC case. We implement the front-end speaker diarization using the self-supervised learning representation based multi-speaker embedding and beamforming using the speaker position, respectively. For ASR, we employ an iterative pseudo-label generation method based on fusion model to obtain text labels of unsupervised data. To mitigate the impact of accent, an Accent-ASR framework is proposed, which captures pronunciation-related accent features at a fine-grained level and linguistic information at a coarse-grained level. On the ICMC-ASR eval set, the proposed system achieves a CER of 13.16% on track 1 and a cpCER of 21.48% on track 2, which significantly outperforms the official baseline system and obtains the first rank on both tracks.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">‚Äî‚Äâ</span></span>ICMC-ASR challenge, speaker diarization, multi-channel beamforming, pseudo-label.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>System description</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_bold">Front-end processing:</span>
For the front-end processing, we employ the guided source separation (GSS) framework¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, but with several modifications, including channel selection and beamforming. We propose a multi-source sound localization module using energy and phase differences to select the channel for the target speaker. Compared to the traditional maximum signal-noise ratio (SNR) selection criterion, it can alleviate the impact of non-target speakers with high speaking volume. The recursive smoothing technique is exploited for the estimation of power spectral density matrices in the design of MVDR beamformer, which is shown to be more effective to suppress interference sources and noises and provide higher-quality single-channel audio for the downstream ASR.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.4.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Training Data Description.</figcaption>
<div id="S1.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:286.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(48.2pt,-35.5pt) scale(1.328472095035,1.328472095035) ;">
<table id="S1.T1.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.2.2.3" class="ltx_tr">
<td id="S1.T1.2.2.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S1.T1.2.2.3.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S1.T1.2.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S1.T1.2.2.3.2.1" class="ltx_text ltx_font_bold">Data type</span></td>
<td id="S1.T1.2.2.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S1.T1.2.2.3.3.1" class="ltx_text ltx_font_bold">Label type</span></td>
<td id="S1.T1.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S1.T1.2.2.3.4.1" class="ltx_text ltx_font_bold">Hours</span></td>
</tr>
<tr id="S1.T1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">ICMC</td>
<td id="S1.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Far+Near+speed<math id="S1.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.m1.1b"><times id="S1.T1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.m1.1c">\times</annotation></semantics></math>3</td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S1.T1.1.1.1.3.1" class="ltx_text">supervised</span></td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">386</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">ICMC+addNoise</td>
<td id="S1.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Far+speed<math id="S1.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.2.2.2.1.m1.1a"><mo id="S1.T1.2.2.2.1.m1.1.1" xref="S1.T1.2.2.2.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.1.m1.1b"><times id="S1.T1.2.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.1.m1.1c">\times</annotation></semantics></math>3</td>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">1544</td>
</tr>
<tr id="S1.T1.2.2.4" class="ltx_tr">
<td id="S1.T1.2.2.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">3D-SPEAKER</td>
<td id="S1.T1.2.2.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Near</td>
<td id="S1.T1.2.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="4"><span id="S1.T1.2.2.4.3.1" class="ltx_text">unsupervised</span></td>
<td id="S1.T1.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">1124</td>
</tr>
<tr id="S1.T1.2.2.5" class="ltx_tr">
<td id="S1.T1.2.2.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AliMeeting</td>
<td id="S1.T1.2.2.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Far+Near</td>
<td id="S1.T1.2.2.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">236</td>
</tr>
<tr id="S1.T1.2.2.6" class="ltx_tr">
<td id="S1.T1.2.2.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">AISHELL-4</td>
<td id="S1.T1.2.2.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Far+Near</td>
<td id="S1.T1.2.2.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">240</td>
</tr>
<tr id="S1.T1.2.2.7" class="ltx_tr">
<td id="S1.T1.2.2.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Aidatatang</td>
<td id="S1.T1.2.2.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Near</td>
<td id="S1.T1.2.2.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">200</td>
</tr>
<tr id="S1.T1.2.2.8" class="ltx_tr">
<td id="S1.T1.2.2.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">MagicData</td>
<td id="S1.T1.2.2.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Near</td>
<td id="S1.T1.2.2.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">(text is not</td>
<td id="S1.T1.2.2.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">180</td>
</tr>
<tr id="S1.T1.2.2.9" class="ltx_tr">
<td id="S1.T1.2.2.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">KeSpeech</td>
<td id="S1.T1.2.2.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Near</td>
<td id="S1.T1.2.2.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">allowed to use)</td>
<td id="S1.T1.2.2.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">1542</td>
</tr>
<tr id="S1.T1.2.2.10" class="ltx_tr">
<td id="S1.T1.2.2.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="3"><span id="S1.T1.2.2.10.1.1" class="ltx_text">WenetSpeech</span></td>
<td id="S1.T1.2.2.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Near</td>
<td id="S1.T1.2.2.10.3" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S1.T1.2.2.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_l ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="3"><span id="S1.T1.2.2.10.4.1" class="ltx_text">5500</span></td>
</tr>
<tr id="S1.T1.2.2.11" class="ltx_tr">
<td id="S1.T1.2.2.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">(drama, talk,</td>
<td id="S1.T1.2.2.11.2" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr id="S1.T1.2.2.12" class="ltx_tr">
<td id="S1.T1.2.2.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;">interview)</td>
<td id="S1.T1.2.2.12.2" class="ltx_td ltx_border_bb ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
</table>
</span></div>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.7" class="ltx_p"><span id="S1.p2.7.1" class="ltx_text ltx_font_bold">Data resource:</span>
All data resources used for the proposed system are summarized in Table 1, which include two categories. One originates from the official ICMC-ASR labelled data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and the other from the external openSLR unlabelled data following the official rules¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For the far-field data, the single-channel audio is extracted by oracle diarization and GSS. For unsupervised data, we propose an iterative pseudo-label generation (PLG) method based on the fusion ASR model. To enhance the generalization of the fusion model, we consider different input features and encoder structures: 1) conformer encoder-decoder (ED) model based on self-supervised learning representation (SSLR), and 2) ebranchformer ED model using Fbank. The former is employed to adapt long-duration audio, while the latter is used for short-duration counterparts. The SSLR is extracted by the adaptive wavLM model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which is trained using the WenetSpeech full 2.5wh dataset. For pseudo-label generation, a small amount of supervised data <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="B_{0}" display="inline"><semantics id="S1.p2.1.m1.1a"><msub id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">B</mi><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">subscript</csymbol><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">ùêµ</ci><cn type="integer" id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">B_{0}</annotation></semantics></math> is initially used to train and fuse into <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="PLG_{0}" display="inline"><semantics id="S1.p2.2.m2.1a"><mrow id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml"><mi id="S1.p2.2.m2.1.1.2" xref="S1.p2.2.m2.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S1.p2.2.m2.1.1.1" xref="S1.p2.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S1.p2.2.m2.1.1.3" xref="S1.p2.2.m2.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S1.p2.2.m2.1.1.1a" xref="S1.p2.2.m2.1.1.1.cmml">‚Äã</mo><msub id="S1.p2.2.m2.1.1.4" xref="S1.p2.2.m2.1.1.4.cmml"><mi id="S1.p2.2.m2.1.1.4.2" xref="S1.p2.2.m2.1.1.4.2.cmml">G</mi><mn id="S1.p2.2.m2.1.1.4.3" xref="S1.p2.2.m2.1.1.4.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1"><times id="S1.p2.2.m2.1.1.1.cmml" xref="S1.p2.2.m2.1.1.1"></times><ci id="S1.p2.2.m2.1.1.2.cmml" xref="S1.p2.2.m2.1.1.2">ùëÉ</ci><ci id="S1.p2.2.m2.1.1.3.cmml" xref="S1.p2.2.m2.1.1.3">ùêø</ci><apply id="S1.p2.2.m2.1.1.4.cmml" xref="S1.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S1.p2.2.m2.1.1.4.1.cmml" xref="S1.p2.2.m2.1.1.4">subscript</csymbol><ci id="S1.p2.2.m2.1.1.4.2.cmml" xref="S1.p2.2.m2.1.1.4.2">ùê∫</ci><cn type="integer" id="S1.p2.2.m2.1.1.4.3.cmml" xref="S1.p2.2.m2.1.1.4.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">PLG_{0}</annotation></semantics></math>. The unsupervised data is then split into different batches, say <math id="S1.p2.3.m3.3" class="ltx_Math" alttext="B_{1},...,B_{N}" display="inline"><semantics id="S1.p2.3.m3.3a"><mrow id="S1.p2.3.m3.3.3.2" xref="S1.p2.3.m3.3.3.3.cmml"><msub id="S1.p2.3.m3.2.2.1.1" xref="S1.p2.3.m3.2.2.1.1.cmml"><mi id="S1.p2.3.m3.2.2.1.1.2" xref="S1.p2.3.m3.2.2.1.1.2.cmml">B</mi><mn id="S1.p2.3.m3.2.2.1.1.3" xref="S1.p2.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S1.p2.3.m3.3.3.2.3" xref="S1.p2.3.m3.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">‚Ä¶</mi><mo id="S1.p2.3.m3.3.3.2.4" xref="S1.p2.3.m3.3.3.3.cmml">,</mo><msub id="S1.p2.3.m3.3.3.2.2" xref="S1.p2.3.m3.3.3.2.2.cmml"><mi id="S1.p2.3.m3.3.3.2.2.2" xref="S1.p2.3.m3.3.3.2.2.2.cmml">B</mi><mi id="S1.p2.3.m3.3.3.2.2.3" xref="S1.p2.3.m3.3.3.2.2.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.3b"><list id="S1.p2.3.m3.3.3.3.cmml" xref="S1.p2.3.m3.3.3.2"><apply id="S1.p2.3.m3.2.2.1.1.cmml" xref="S1.p2.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S1.p2.3.m3.2.2.1.1.1.cmml" xref="S1.p2.3.m3.2.2.1.1">subscript</csymbol><ci id="S1.p2.3.m3.2.2.1.1.2.cmml" xref="S1.p2.3.m3.2.2.1.1.2">ùêµ</ci><cn type="integer" id="S1.p2.3.m3.2.2.1.1.3.cmml" xref="S1.p2.3.m3.2.2.1.1.3">1</cn></apply><ci id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">‚Ä¶</ci><apply id="S1.p2.3.m3.3.3.2.2.cmml" xref="S1.p2.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S1.p2.3.m3.3.3.2.2.1.cmml" xref="S1.p2.3.m3.3.3.2.2">subscript</csymbol><ci id="S1.p2.3.m3.3.3.2.2.2.cmml" xref="S1.p2.3.m3.3.3.2.2.2">ùêµ</ci><ci id="S1.p2.3.m3.3.3.2.2.3.cmml" xref="S1.p2.3.m3.3.3.2.2.3">ùëÅ</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.3c">B_{1},...,B_{N}</annotation></semantics></math>. Subsequently, the pseudo-labels for the unsupervised data in <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="B_{N}" display="inline"><semantics id="S1.p2.4.m4.1a"><msub id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml"><mi id="S1.p2.4.m4.1.1.2" xref="S1.p2.4.m4.1.1.2.cmml">B</mi><mi id="S1.p2.4.m4.1.1.3" xref="S1.p2.4.m4.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><apply id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S1.p2.4.m4.1.1.1.cmml" xref="S1.p2.4.m4.1.1">subscript</csymbol><ci id="S1.p2.4.m4.1.1.2.cmml" xref="S1.p2.4.m4.1.1.2">ùêµ</ci><ci id="S1.p2.4.m4.1.1.3.cmml" xref="S1.p2.4.m4.1.1.3">ùëÅ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">B_{N}</annotation></semantics></math> are generated by <math id="S1.p2.5.m5.1" class="ltx_Math" alttext="PLG_{N-1}" display="inline"><semantics id="S1.p2.5.m5.1a"><mrow id="S1.p2.5.m5.1.1" xref="S1.p2.5.m5.1.1.cmml"><mi id="S1.p2.5.m5.1.1.2" xref="S1.p2.5.m5.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S1.p2.5.m5.1.1.1" xref="S1.p2.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S1.p2.5.m5.1.1.3" xref="S1.p2.5.m5.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S1.p2.5.m5.1.1.1a" xref="S1.p2.5.m5.1.1.1.cmml">‚Äã</mo><msub id="S1.p2.5.m5.1.1.4" xref="S1.p2.5.m5.1.1.4.cmml"><mi id="S1.p2.5.m5.1.1.4.2" xref="S1.p2.5.m5.1.1.4.2.cmml">G</mi><mrow id="S1.p2.5.m5.1.1.4.3" xref="S1.p2.5.m5.1.1.4.3.cmml"><mi id="S1.p2.5.m5.1.1.4.3.2" xref="S1.p2.5.m5.1.1.4.3.2.cmml">N</mi><mo id="S1.p2.5.m5.1.1.4.3.1" xref="S1.p2.5.m5.1.1.4.3.1.cmml">‚àí</mo><mn id="S1.p2.5.m5.1.1.4.3.3" xref="S1.p2.5.m5.1.1.4.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.5.m5.1b"><apply id="S1.p2.5.m5.1.1.cmml" xref="S1.p2.5.m5.1.1"><times id="S1.p2.5.m5.1.1.1.cmml" xref="S1.p2.5.m5.1.1.1"></times><ci id="S1.p2.5.m5.1.1.2.cmml" xref="S1.p2.5.m5.1.1.2">ùëÉ</ci><ci id="S1.p2.5.m5.1.1.3.cmml" xref="S1.p2.5.m5.1.1.3">ùêø</ci><apply id="S1.p2.5.m5.1.1.4.cmml" xref="S1.p2.5.m5.1.1.4"><csymbol cd="ambiguous" id="S1.p2.5.m5.1.1.4.1.cmml" xref="S1.p2.5.m5.1.1.4">subscript</csymbol><ci id="S1.p2.5.m5.1.1.4.2.cmml" xref="S1.p2.5.m5.1.1.4.2">ùê∫</ci><apply id="S1.p2.5.m5.1.1.4.3.cmml" xref="S1.p2.5.m5.1.1.4.3"><minus id="S1.p2.5.m5.1.1.4.3.1.cmml" xref="S1.p2.5.m5.1.1.4.3.1"></minus><ci id="S1.p2.5.m5.1.1.4.3.2.cmml" xref="S1.p2.5.m5.1.1.4.3.2">ùëÅ</ci><cn type="integer" id="S1.p2.5.m5.1.1.4.3.3.cmml" xref="S1.p2.5.m5.1.1.4.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.5.m5.1c">PLG_{N-1}</annotation></semantics></math> in a cyclic iterative fashion, and <math id="S1.p2.6.m6.1" class="ltx_Math" alttext="PLG_{N}" display="inline"><semantics id="S1.p2.6.m6.1a"><mrow id="S1.p2.6.m6.1.1" xref="S1.p2.6.m6.1.1.cmml"><mi id="S1.p2.6.m6.1.1.2" xref="S1.p2.6.m6.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S1.p2.6.m6.1.1.1" xref="S1.p2.6.m6.1.1.1.cmml">‚Äã</mo><mi id="S1.p2.6.m6.1.1.3" xref="S1.p2.6.m6.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S1.p2.6.m6.1.1.1a" xref="S1.p2.6.m6.1.1.1.cmml">‚Äã</mo><msub id="S1.p2.6.m6.1.1.4" xref="S1.p2.6.m6.1.1.4.cmml"><mi id="S1.p2.6.m6.1.1.4.2" xref="S1.p2.6.m6.1.1.4.2.cmml">G</mi><mi id="S1.p2.6.m6.1.1.4.3" xref="S1.p2.6.m6.1.1.4.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.6.m6.1b"><apply id="S1.p2.6.m6.1.1.cmml" xref="S1.p2.6.m6.1.1"><times id="S1.p2.6.m6.1.1.1.cmml" xref="S1.p2.6.m6.1.1.1"></times><ci id="S1.p2.6.m6.1.1.2.cmml" xref="S1.p2.6.m6.1.1.2">ùëÉ</ci><ci id="S1.p2.6.m6.1.1.3.cmml" xref="S1.p2.6.m6.1.1.3">ùêø</ci><apply id="S1.p2.6.m6.1.1.4.cmml" xref="S1.p2.6.m6.1.1.4"><csymbol cd="ambiguous" id="S1.p2.6.m6.1.1.4.1.cmml" xref="S1.p2.6.m6.1.1.4">subscript</csymbol><ci id="S1.p2.6.m6.1.1.4.2.cmml" xref="S1.p2.6.m6.1.1.4.2">ùê∫</ci><ci id="S1.p2.6.m6.1.1.4.3.cmml" xref="S1.p2.6.m6.1.1.4.3">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.6.m6.1c">PLG_{N}</annotation></semantics></math> is trained and fused by <math id="S1.p2.7.m7.1" class="ltx_Math" alttext="B_{N}" display="inline"><semantics id="S1.p2.7.m7.1a"><msub id="S1.p2.7.m7.1.1" xref="S1.p2.7.m7.1.1.cmml"><mi id="S1.p2.7.m7.1.1.2" xref="S1.p2.7.m7.1.1.2.cmml">B</mi><mi id="S1.p2.7.m7.1.1.3" xref="S1.p2.7.m7.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.7.m7.1b"><apply id="S1.p2.7.m7.1.1.cmml" xref="S1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S1.p2.7.m7.1.1.1.cmml" xref="S1.p2.7.m7.1.1">subscript</csymbol><ci id="S1.p2.7.m7.1.1.2.cmml" xref="S1.p2.7.m7.1.1.2">ùêµ</ci><ci id="S1.p2.7.m7.1.1.3.cmml" xref="S1.p2.7.m7.1.1.3">ùëÅ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.7.m7.1c">B_{N}</annotation></semantics></math>. As such, we can obtain 10,952 hours of data for ASR training in total.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Speaker Diarization:</span>
The speaker diarization system trained on 1544 hours data in Table 1 consists of three components: multi-channel voice activity detection (VAD), clustering-based speaker diarization (CSD) and multi-channel target-speaker VAD (MC-TS-VAD)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Due to the additional non-target speaker (recorder) in the official data, traditional i-vector can not effectively distinguish between target and non-target speakers. To improve the generalization of speaker embeddings, we consider the fusion of the SSLR-based x-vector for the i-vector in the MC-TS-VAD module. This embedding fusion enables the diarization system to fully exploit speaker information and is thus helpful for speaker separation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Speech Recognition:</span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.02052/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="263" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†1</span>: </span>The proposed Accent-ASR framework.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Due to the impact of accents on Mandarin ASR, we propose an Accent-ASR framework, which enables fine-grained units to capture pronunciation-related accent characteristics and coarse-grained units to learn linguistic information¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. As shown in Fig. 1, the proposed Accent-ASR is based on multi-task learning for aligning Pinyin sequences, and combines aligned Pinyin features with acoustic features of encoder through twin cross-attention fusion and contrastive learning, such that fine-grained units can better capture pronunciation details. Note that the aligned Pinyin features are derived from embedding and transformer modules. In the fusion layer, in addition to the aforementioned two types of features, frame-segment level speaker information is further introduced to help distinguish coarse-grained units that are generated by speakers of different accents. Besides, in order to increase the diversity of the Accent-ASR fusion model, we modify and combine the front and back sub-modules of encoder, where the front-end module includes Conv2D, VGG and gateCNN, and the backbone module consists of conformer and ebranchformer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. But the decoder always adopts the transformer structure.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Performance Evaluation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Results on the Dev and Eval Sets of Track 1:</span>
The CER performance of the pseudo-label iterative generation based on the PLG fusion model is shown in Table 2. As the number of iterations increases, the performance of PLG significantly improves. Additionally, we observe that the wavLM-based ED model has competitive results given a relatively small amount of data. However, as the data amount increases to 5000 hours, wavLM-based ED model becomes inferior to that of Fbank-based ED model.
Table 3 shows the results of ASR single and fusion models on Track 1 dev and eval sets. M0 represents the official baseline. M1-M5 are single Accent-ASR model composed of different encoder front-end and back-end sub-modules. M6 denotes the post fusion based on weight adaptation, which achieves a relative improvement of 52.8% and 49.8% compared to M0 on dev and eval sets, respectively.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Results on the Dev and Eval Sets of Track 2:</span>
Table 4 presents the automatic speaker diarization and recognition (ASDR) results on Track 2 dev and eval sets. Our speaker diarization system achieves a diarization error rate (DER) of 10.21%. In comparison to the official baseline, our system achieves a cpCER of 16.31% on the dev set, which relatively improves by 75.3%. Note that there is a difference of smaller than 5% compared to Track 1 with oracle diarization. On the eval set, our system outperforms the official baseline with a relative improvement of 70.5%.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Pseudo-Label Iterative Generation Results (CER%).</figcaption>
<div id="S2.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:162.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(70.9pt,-27.0pt) scale(1.50049806034483,1.50049806034483) ;">
<table id="S2.T2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.3.1.1" class="ltx_tr">
<td id="S2.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S2.T2.3.1.1.1.1" class="ltx_text ltx_font_bold">Iteration</span></td>
<td id="S2.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S2.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">Data(hrs)</span></td>
<td id="S2.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S2.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">wavLM+</span></td>
<td id="S2.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S2.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">conv2d+</span></td>
<td id="S2.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S2.T2.3.1.1.5.1" class="ltx_text ltx_font_bold">Fusion</span></td>
</tr>
<tr id="S2.T2.3.1.2" class="ltx_tr">
<td id="S2.T2.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S2.T2.3.1.2.1.1" class="ltx_text ltx_font_bold">conformer ED</span></td>
<td id="S2.T2.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S2.T2.3.1.2.2.1" class="ltx_text ltx_font_bold">ebranchformer ED</span></td>
</tr>
<tr id="S2.T2.3.1.3" class="ltx_tr">
<td id="S2.T2.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">B0</td>
<td id="S2.T2.3.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">396</td>
<td id="S2.T2.3.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">29.77</td>
<td id="S2.T2.3.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">30.64</td>
<td id="S2.T2.3.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">27.8</td>
</tr>
<tr id="S2.T2.3.1.4" class="ltx_tr">
<td id="S2.T2.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">B1</td>
<td id="S2.T2.3.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">3910</td>
<td id="S2.T2.3.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">25.32</td>
<td id="S2.T2.3.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">26.4</td>
<td id="S2.T2.3.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">23.77</td>
</tr>
<tr id="S2.T2.3.1.5" class="ltx_tr">
<td id="S2.T2.3.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">B2</td>
<td id="S2.T2.3.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">5452</td>
<td id="S2.T2.3.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">22.94</td>
<td id="S2.T2.3.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">21.9</td>
<td id="S2.T2.3.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">22.87</td>
</tr>
<tr id="S2.T2.3.1.6" class="ltx_tr">
<td id="S2.T2.3.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">B3</td>
<td id="S2.T2.3.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">10952</td>
<td id="S2.T2.3.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">22.94 (5452h)</td>
<td id="S2.T2.3.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">21.14</td>
<td id="S2.T2.3.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">20.83</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Results of single/fusion models on Track 1 (CER%).</figcaption>
<div id="S2.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:195.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(55.9pt,-25.7pt) scale(1.35702683672599,1.35702683672599) ;">
<table id="S2.T3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.3.1.1" class="ltx_tr">
<td id="S2.T3.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T3.3.1.1.1.1" class="ltx_text ltx_font_bold">ID</span></td>
<td id="S2.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T3.3.1.1.2.1" class="ltx_text ltx_font_bold">Model based on Accent-ASR</span></td>
<td id="S2.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T3.3.1.1.3.1" class="ltx_text ltx_font_bold">Dev</span></td>
<td id="S2.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T3.3.1.1.4.1" class="ltx_text ltx_font_bold">Eval</span></td>
</tr>
<tr id="S2.T3.3.1.2" class="ltx_tr">
<td id="S2.T3.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M0</td>
<td id="S2.T3.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">Official Baseline</td>
<td id="S2.T3.3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">32.92</td>
<td id="S2.T3.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">26.24</td>
</tr>
<tr id="S2.T3.3.1.3" class="ltx_tr">
<td id="S2.T3.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M1</td>
<td id="S2.T3.3.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">conv2d+conformer</td>
<td id="S2.T3.3.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">20.87</td>
<td id="S2.T3.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">-</td>
</tr>
<tr id="S2.T3.3.1.4" class="ltx_tr">
<td id="S2.T3.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M2</td>
<td id="S2.T3.3.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">conv2d+ebranchformer</td>
<td id="S2.T3.3.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">20.68</td>
<td id="S2.T3.3.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">-</td>
</tr>
<tr id="S2.T3.3.1.5" class="ltx_tr">
<td id="S2.T3.3.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M3</td>
<td id="S2.T3.3.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">VGG+ebranchformer</td>
<td id="S2.T3.3.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">20.31</td>
<td id="S2.T3.3.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">-</td>
</tr>
<tr id="S2.T3.3.1.6" class="ltx_tr">
<td id="S2.T3.3.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M4</td>
<td id="S2.T3.3.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">gateCNN+conformer</td>
<td id="S2.T3.3.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">19.83</td>
<td id="S2.T3.3.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">-</td>
</tr>
<tr id="S2.T3.3.1.7" class="ltx_tr">
<td id="S2.T3.3.1.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M5</td>
<td id="S2.T3.3.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">gateCNN+ebranchformer</td>
<td id="S2.T3.3.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">18.53</td>
<td id="S2.T3.3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">14.72</td>
</tr>
<tr id="S2.T3.3.1.8" class="ltx_tr">
<td id="S2.T3.3.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M6</td>
<td id="S2.T3.3.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">fusion models based on M1-M5</td>
<td id="S2.T3.3.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">15.54</td>
<td id="S2.T3.3.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">13.16</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S2.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>The ASDR results on Track 2 (%).</figcaption>
<div id="S2.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:102pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.6pt,-15.0pt) scale(1.41724593395862,1.41724593395862) ;">
<table id="S2.T4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T4.3.1.1" class="ltx_tr">
<td id="S2.T4.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T4.3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S2.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T4.3.1.1.2.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S2.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T4.3.1.1.3.1" class="ltx_text ltx_font_bold">Dev</span></td>
<td id="S2.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;"><span id="S2.T4.3.1.1.4.1" class="ltx_text ltx_font_bold">Eval</span></td>
</tr>
<tr id="S2.T4.3.1.2" class="ltx_tr">
<td id="S2.T4.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">Speaker Diarization</td>
<td id="S2.T4.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">DER (%)</td>
<td id="S2.T4.3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">10.21</td>
<td id="S2.T4.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">-</td>
</tr>
<tr id="S2.T4.3.1.3" class="ltx_tr">
<td id="S2.T4.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M0 Offical Baseline</td>
<td id="S2.T4.3.1.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;" rowspan="2"><span id="S2.T4.3.1.3.2.1" class="ltx_text">cpCER (%)</span></td>
<td id="S2.T4.3.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">65.90</td>
<td id="S2.T4.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">72.88</td>
</tr>
<tr id="S2.T4.3.1.4" class="ltx_tr">
<td id="S2.T4.3.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">M6 Fusion models</td>
<td id="S2.T4.3.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">16.31</td>
<td id="S2.T4.3.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t" style="padding-left:14.2pt;padding-right:14.2pt;">21.48</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ruoyu Wang, Maokui He, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúThe ustc-nercslip systems for chime-7 challenge,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proc. CHiME 2023</span>, 2023, pp. 13‚Äì18.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
He¬†Wang, Pengcheng Guo, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúIcmc-asr: The icassp 2024 in-car multi-channel automatic speech recognition challenge,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.03473</span>, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Kazuya Kawakami, Luyu Wang, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúLearning robust and multilingual speech representations,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.11128</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Qijie Shao, Pengcheng Guo, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúDecoupling and interacting multi-task learning network for joint speech and accent recognition,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE/ACM TASLP</span>, vol. 32, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Weitai Zhang, Zhongyi Ye, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúThe ustc-nelslip offline speech translation systems for iwslt 2022,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. IWSLT</span>, 2022, pp. 198‚Äì207.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.02051" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.02052" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.02052">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.02052" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.02053" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:50:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
