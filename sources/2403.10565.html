<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.10565] 1 Introduction</title><meta property="og:description" content="In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error râ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.10565">

<!--Generated on Fri Apr  5 17:01:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\titre</span>
<p id="p1.2" class="ltx_p">PTSD-MDNNÂ : Fusion tardive de rÃ©seaux de neurones profonds multimodaux pour la dÃ©tection du trouble de stress post-traumatique</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\auteurs</span><span id="p2.2" class="ltx_ERROR undefined">\auteur</span>
<p id="p2.3" class="ltx_p">LongNguyen-Phuoclong.nguyen-phuoc@emse.fr1,2
<span id="p2.3.1" class="ltx_ERROR undefined">\auteur</span>RenaldGaboriaurenald.gaboriau@mjinnov.com2
<span id="p2.3.2" class="ltx_ERROR undefined">\auteur</span>DimitriDelacroixdimitri.delacroix@mjinnov.com2
<span id="p2.3.3" class="ltx_ERROR undefined">\auteur</span>LaurentNavarronavarro@emse.fr1</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\affils</span>
<p id="p3.2" class="ltx_p">Mines Saint-Ã‰tienne, University of Lyon, University Jean Monnet, Inserm, U 1059 Sainbiose, Centre CIS, 42023 Saint-Ã‰tienne, France

MJ Lab, MJ INNOV, 42000 Saint-Etienne, France</p>
</div>
<div id="p4" class="ltx_para">
<span id="p4.1" class="ltx_ERROR undefined">\resume</span>
<p id="p4.2" class="ltx_p">Afin de proposer un moyen plus objectif et plus rapide de diagnostiquer le trouble de stress post-traumatique (TSPT), nous prÃ©sentons <code id="p4.2.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> qui fusionne deux rÃ©seaux de neurones convolutifs unimodaux et qui donne un faible taux dâ€™erreurs de dÃ©tection. En ne prenant que des vidÃ©os et des audios comme entrÃ©es, le modÃ¨le pourrait Ãªtre utilisÃ© dans la configuration de sÃ©ances de tÃ©lÃ©consultation, dans lâ€™optimisation des parcours patients ou encore pour lâ€™interaction humain-robot.</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present <code id="id1.id1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contexte GÃ©nÃ©ral</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Tout individu, au cours de sa vie, peut rencontrer des situations potentiellement traumatogÃ¨nes. Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> dÃ©finit comme traumatogÃ¨ne toute situation qui implique Â« une mort effective, une menace de mort, une blessure grave ou des violences sexuelles Â». En France, le TSPT toucherait entre 1 et 2% de la population. Les symptÃ´mes du TSPT causent des problÃ¨mes importants dans les situations sociales ou professionnelles.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Traditionnellement, le TSPT a Ã©tÃ© diagnostiquÃ© par des professionnels de la santÃ© impliquant des questionnaires. La collecte dâ€™informations par le biais dâ€™un questionnaire auto-dÃ©claratif a des limites car souvent biaisÃ©sÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>Â : (1) les distorsions de la mÃ©moire et de la perception de soi des patients rendent Ã©galement le diagnostic difficile, et (2) les patients sont souvent gÃªnÃ©s dâ€™Ãªtre diagnostiquÃ©s et ne veulent pas visiter les cliniques pour le diagnostic. Finalement, peu de mesures objectives ou qualitatives sont disponibles pour aider les cliniciens Ã  diagnostiquer ce trouble. Un exemple dâ€™un tel entretien est le Clinician-Administered PTSD Scale (CAPS) ou encore le PCL-5Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">La rÃ©cente pandÃ©mie de SRAS-CoV-2 peut Ãªtre considÃ©rÃ©e comme un Ã©vÃ©nement traumatique mondial qui a fait Ã©merger: (1) un fort impact sur la santÃ© mental, (2) la rÃ©alisation de nombreux soins mÃ©dicaux transformÃ©e en distanciel. Par consÃ©quent, considÃ©rant le biais des modes de collecte de donnÃ©es auto-dÃ©claratifs et le changement radical induit par les consultations en distanciel, il est nÃ©cessaire de trouver un moyen plus objectif et rapide pour diagnostiquer le TSPT.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Contexte ThÃ©orique</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Nous distinguons deux catÃ©gories de modÃ¨les dâ€™intelligence artificielle : les modÃ¨les de pronostic et les modÃ¨les de diagnostic du TSPT. Notre papier se concentre sur le diagnostic, câ€™est-Ã -dire la dÃ©tection de lâ€™Ã©tat actuel des patients. Une grande majoritÃ© des Ã©tudes sur le diagnostic utilisent des techniques dâ€™apprentissage automatique supervisÃ© sur des donnÃ©es structurÃ©es. Par exemple,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> ont tous appliquÃ© des algorithmes de Machine Ã  Vecteurs de Support (SVM) sur des questionnaires. Toujours sur ces donnÃ©es auto-dÃ©claratives et tabulaires,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> ont utilisÃ© lâ€™optimisation minimale sÃ©quentielle, le perceptron multicouches et la classification naÃ¯ve bayÃ©sienne. Concernant les donnÃ©es biomÃ©triques, il semble que les modifications de la variabilitÃ© de la frÃ©quence cardiaque sont significativement associÃ©es au TSPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Enfin, la conductance cutanÃ©e peut Ãªtre utilisÃ©e comme outil de diagnosticÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> car elle semble en particulier corrÃ©lÃ©e Ã  son intensitÃ©.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Ã€ lâ€™image des donnÃ©es non structurÃ©es qui alimentent la prochaine gÃ©nÃ©ration des modÃ¨les dâ€™IA, de nombreuses Ã©tudes de dÃ©tection du TSPT ont profitÃ© de ces avancÃ©es pour puiser lâ€™information dans diffÃ©rentes sources disponibles dans le cadre clinique ou quotidien des patients. Parmi les moyens dâ€™acquisition et de restitution dâ€™imagerie mÃ©dicale, les Ã©tudes dâ€™imagerie par rÃ©sonance magnÃ©tique (IRM) structurelleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> et fonctionnelleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> ont permis des progrÃ¨s considÃ©rables dans la comprÃ©hension des mÃ©canismes neuronaux sous-jacents au TSPT. Ainsi couplÃ©es avec lâ€™agorithme SVM, elles permettent de dÃ©tecter le TSPT avec de bon rÃ©sultats.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.10565/assets/PTSD-MDNN.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1196" height="393" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Lâ€™architecture de PTSD-MDNN</figcaption>
</figure>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Dâ€™autres Ã©tudes ont cherchÃ© des alternatives aux donnÃ©es mÃ©dicales traditionnelles. Le diagnostic des patients atteints de TSPT par lâ€™analyse des signaux vocaux a Ã©tÃ© Ã©tudiÃ© depuis ces derniÃ¨res annÃ©esÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Des approches de text mining ou de traitement du langage naturel ont Ã©tÃ© Ã©galement utilisÃ©esÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. MÃªme si la tÃ©lÃ©mÃ©decine via vidÃ©o confÃ©rence peut rÃ©duire le dÃ©lai de la prise en charge et Ãªtre aussi efficace que le traitement en personneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, seulÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> se concentre sur ce type de donnÃ©es audiovisuelles en utilisant diffÃ©rentes architectures de rÃ©seaux de neurones par types de modalitÃ©s de donnÃ©es. Actuellement, la question de la fusion de ces diffÃ©rentes modalitÃ©s audio-vidÃ©o pour la dÃ©tection du TSPT reste entiÃ¨re.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Motivation</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">MalgrÃ© la variÃ©tÃ© des Ã©tudes citÃ©es ci-dessus, le diagnostic du TSPT nÃ©cessite des capteurs ou dispositifs mÃ©dicaux Ã  caractÃ¨re invasif dont la disponibilitÃ© nâ€™est pas garantie, notamment en temps de crise. Nous proposons, dans ce papier, un rÃ©seau de neurones profond multimodal pour la dÃ©tection automatique de TSPT (<code id="S1.SS3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN:</code> <code id="S1.SS3.p1.1.2" class="ltx_verbatim ltx_font_typewriter">Post Traumatic Stress Disorder</code> <code id="S1.SS3.p1.1.3" class="ltx_verbatim ltx_font_typewriter"> - Multimodal Deep Neural Network</code>) en utilisant comme entrÃ©es de simples vidÃ©os et audios des patients en situation rÃ©elle. Nous montrons quâ€™en plus dâ€™Ãªtre adaptÃ© Ã  ces donnÃ©es audiovisuelles facilement collectÃ©es, notre modÃ¨le obtient de meilleurs rÃ©sultats en fusionnant ces deux modalitÃ©s.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>MÃ©thode ProposÃ©e</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.4" class="ltx_p">Nous prÃ©sentons un aperÃ§u de <code id="S2.p1.4.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> dans la FigÂ <a href="#S1.F1" title="Figure 1 â€£ 1.2 Contexte ThÃ©orique â€£ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Le modÃ¨le gÃ©nÃ©ral se compose de deux sous-modÃ¨les qui prennent chacun en entrÃ©e une modalitÃ© diffÃ©rente. Le vecteur sortant de la derniÃ¨re couche du classement vidÃ©o <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="y_{v}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">y</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ğ‘¦</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">y_{v}</annotation></semantics></math> est concatenÃ© avec le vecteur sortant de la derniÃ¨re couche du classement audio <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="y_{a}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">y</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ğ‘¦</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">y_{a}</annotation></semantics></math> pour former une matrice <math id="S2.p1.3.m3.2" class="ltx_Math" alttext="M_{v,a}" display="inline"><semantics id="S2.p1.3.m3.2a"><msub id="S2.p1.3.m3.2.3" xref="S2.p1.3.m3.2.3.cmml"><mi id="S2.p1.3.m3.2.3.2" xref="S2.p1.3.m3.2.3.2.cmml">M</mi><mrow id="S2.p1.3.m3.2.2.2.4" xref="S2.p1.3.m3.2.2.2.3.cmml"><mi id="S2.p1.3.m3.1.1.1.1" xref="S2.p1.3.m3.1.1.1.1.cmml">v</mi><mo id="S2.p1.3.m3.2.2.2.4.1" xref="S2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S2.p1.3.m3.2.2.2.2" xref="S2.p1.3.m3.2.2.2.2.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.2b"><apply id="S2.p1.3.m3.2.3.cmml" xref="S2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.2.3.1.cmml" xref="S2.p1.3.m3.2.3">subscript</csymbol><ci id="S2.p1.3.m3.2.3.2.cmml" xref="S2.p1.3.m3.2.3.2">ğ‘€</ci><list id="S2.p1.3.m3.2.2.2.3.cmml" xref="S2.p1.3.m3.2.2.2.4"><ci id="S2.p1.3.m3.1.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1.1">ğ‘£</ci><ci id="S2.p1.3.m3.2.2.2.2.cmml" xref="S2.p1.3.m3.2.2.2.2">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.2c">M_{v,a}</annotation></semantics></math> . AprÃ¨s cette fusion tardive de modalitÃ©s, la matrice <math id="S2.p1.4.m4.2" class="ltx_Math" alttext="M_{v,a}" display="inline"><semantics id="S2.p1.4.m4.2a"><msub id="S2.p1.4.m4.2.3" xref="S2.p1.4.m4.2.3.cmml"><mi id="S2.p1.4.m4.2.3.2" xref="S2.p1.4.m4.2.3.2.cmml">M</mi><mrow id="S2.p1.4.m4.2.2.2.4" xref="S2.p1.4.m4.2.2.2.3.cmml"><mi id="S2.p1.4.m4.1.1.1.1" xref="S2.p1.4.m4.1.1.1.1.cmml">v</mi><mo id="S2.p1.4.m4.2.2.2.4.1" xref="S2.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S2.p1.4.m4.2.2.2.2" xref="S2.p1.4.m4.2.2.2.2.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.2b"><apply id="S2.p1.4.m4.2.3.cmml" xref="S2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.2.3.1.cmml" xref="S2.p1.4.m4.2.3">subscript</csymbol><ci id="S2.p1.4.m4.2.3.2.cmml" xref="S2.p1.4.m4.2.3.2">ğ‘€</ci><list id="S2.p1.4.m4.2.2.2.3.cmml" xref="S2.p1.4.m4.2.2.2.4"><ci id="S2.p1.4.m4.1.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1.1">ğ‘£</ci><ci id="S2.p1.4.m4.2.2.2.2.cmml" xref="S2.p1.4.m4.2.2.2.2">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.2c">M_{v,a}</annotation></semantics></math> est injectÃ©e dans un dernier rÃ©seau de neurones Ã  deux couches afin de dÃ©tecter le TSPT.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Classement VidÃ©o</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">Le sous-modÃ¨le de classification vidÃ©o utilise un rÃ©seau de neurones convolutif (2+1)DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> avec des connexions rÃ©siduelles Ã  18 couches de profondeur (ResNet18). La convolution (2+1)D permet la dÃ©composition des dimensions spatiale et temporelle, crÃ©ant ainsi deux Ã©tapes distinctes. Un avantage de cette approche est que la factorisation des convolutions en dimensions spatiales et temporelles permet de rÃ©duire le nombre de paramÃ¨tres par rapport Ã  la convolution 3D complÃ¨te. La convolution spatiale prend les donnÃ©es sous la forme <math id="S2.SS1.p1.1.m1.3" class="ltx_Math" alttext="(1,largeur,hauteur)" display="inline"><semantics id="S2.SS1.p1.1.m1.3a"><mrow id="S2.SS1.p1.1.m1.3.3.2" xref="S2.SS1.p1.1.m1.3.3.3.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.3.3.2.3" xref="S2.SS1.p1.1.m1.3.3.3.cmml">(</mo><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">1</mn><mo id="S2.SS1.p1.1.m1.3.3.2.4" xref="S2.SS1.p1.1.m1.3.3.3.cmml">,</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1a" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.4" xref="S2.SS1.p1.1.m1.2.2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1b" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.5" xref="S2.SS1.p1.1.m1.2.2.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1c" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.6" xref="S2.SS1.p1.1.m1.2.2.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1d" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.7" xref="S2.SS1.p1.1.m1.2.2.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1e" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.8" xref="S2.SS1.p1.1.m1.2.2.1.1.8.cmml">r</mi></mrow><mo id="S2.SS1.p1.1.m1.3.3.2.5" xref="S2.SS1.p1.1.m1.3.3.3.cmml">,</mo><mrow id="S2.SS1.p1.1.m1.3.3.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.cmml"><mi id="S2.SS1.p1.1.m1.3.3.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.3" xref="S2.SS1.p1.1.m1.3.3.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1a" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.4" xref="S2.SS1.p1.1.m1.3.3.2.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1b" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.5" xref="S2.SS1.p1.1.m1.3.3.2.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1c" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.6" xref="S2.SS1.p1.1.m1.3.3.2.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1d" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.7" xref="S2.SS1.p1.1.m1.3.3.2.2.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1e" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">â€‹</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.8" xref="S2.SS1.p1.1.m1.3.3.2.2.8.cmml">r</mi></mrow><mo stretchy="false" id="S2.SS1.p1.1.m1.3.3.2.6" xref="S2.SS1.p1.1.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.3b"><vector id="S2.SS1.p1.1.m1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">1</cn><apply id="S2.SS1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1"><times id="S2.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1"></times><ci id="S2.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2">ğ‘™</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3">ğ‘</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.4.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.4">ğ‘Ÿ</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.5.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5">ğ‘”</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.6.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.6">ğ‘’</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.7.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.7">ğ‘¢</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.8.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.8">ğ‘Ÿ</ci></apply><apply id="S2.SS1.p1.1.m1.3.3.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2"><times id="S2.SS1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.1"></times><ci id="S2.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2">â„</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.3">ğ‘</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.4.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.4">ğ‘¢</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.5.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.5">ğ‘¡</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.6.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.6">ğ‘’</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.7.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.7">ğ‘¢</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.8.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.8">ğ‘Ÿ</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.3c">(1,largeur,hauteur)</annotation></semantics></math>, tandis que la convolution temporelle prend les donnÃ©es sous la forme <math id="S2.SS1.p1.2.m2.3" class="ltx_Math" alttext="(temps,1,1)" display="inline"><semantics id="S2.SS1.p1.2.m2.3a"><mrow id="S2.SS1.p1.2.m2.3.3.1" xref="S2.SS1.p1.2.m2.3.3.2.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.3.3.1.2" xref="S2.SS1.p1.2.m2.3.3.2.cmml">(</mo><mrow id="S2.SS1.p1.2.m2.3.3.1.1" xref="S2.SS1.p1.2.m2.3.3.1.1.cmml"><mi id="S2.SS1.p1.2.m2.3.3.1.1.2" xref="S2.SS1.p1.2.m2.3.3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.3" xref="S2.SS1.p1.2.m2.3.3.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1a" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.4" xref="S2.SS1.p1.2.m2.3.3.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1b" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.5" xref="S2.SS1.p1.2.m2.3.3.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1c" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.6" xref="S2.SS1.p1.2.m2.3.3.1.1.6.cmml">s</mi></mrow><mo id="S2.SS1.p1.2.m2.3.3.1.3" xref="S2.SS1.p1.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">1</mn><mo id="S2.SS1.p1.2.m2.3.3.1.4" xref="S2.SS1.p1.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.p1.2.m2.3.3.1.5" xref="S2.SS1.p1.2.m2.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.3b"><vector id="S2.SS1.p1.2.m2.3.3.2.cmml" xref="S2.SS1.p1.2.m2.3.3.1"><apply id="S2.SS1.p1.2.m2.3.3.1.1.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1"><times id="S2.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.1"></times><ci id="S2.SS1.p1.2.m2.3.3.1.1.2.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.2">ğ‘¡</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.3.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.3">ğ‘’</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.4.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.4">ğ‘š</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.5.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.5">ğ‘</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.6.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.6">ğ‘ </ci></apply><cn type="integer" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">1</cn><cn type="integer" id="S2.SS1.p1.2.m2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2">1</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.3c">(temps,1,1)</annotation></semantics></math> comme illustrÃ© dans la FigÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1 Classement VidÃ©o â€£ 2 MÃ©thode ProposÃ©e" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Le redimensionnement de la vidÃ©o est nÃ©cessaire pour: (1) effectuer un sous-Ã©chantillonnage des donnÃ©es, (2) examiner des parties spÃ©cifiques des images, (3) rÃ©duire la dimensionnalitÃ© pour un traitement plus rapide.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.10565/assets/2plus1CNN.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Les convolutions spatiales et temporelles factorisÃ©es dâ€™une convolution (2+1)D avec une taille de noyau <math id="S2.F2.5.m1.1" class="ltx_Math" alttext="(3\times 3\times 3)" display="inline"><semantics id="S2.F2.5.m1.1b"><mrow id="S2.F2.5.m1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.5.m1.1.1.1.2" xref="S2.F2.5.m1.1.1.1.1.cmml">(</mo><mrow id="S2.F2.5.m1.1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.cmml"><mn id="S2.F2.5.m1.1.1.1.1.2" xref="S2.F2.5.m1.1.1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.5.m1.1.1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.1.cmml">Ã—</mo><mn id="S2.F2.5.m1.1.1.1.1.3" xref="S2.F2.5.m1.1.1.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.5.m1.1.1.1.1.1b" xref="S2.F2.5.m1.1.1.1.1.1.cmml">Ã—</mo><mn id="S2.F2.5.m1.1.1.1.1.4" xref="S2.F2.5.m1.1.1.1.1.4.cmml">3</mn></mrow><mo stretchy="false" id="S2.F2.5.m1.1.1.1.3" xref="S2.F2.5.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.5.m1.1c"><apply id="S2.F2.5.m1.1.1.1.1.cmml" xref="S2.F2.5.m1.1.1.1"><times id="S2.F2.5.m1.1.1.1.1.1.cmml" xref="S2.F2.5.m1.1.1.1.1.1"></times><cn type="integer" id="S2.F2.5.m1.1.1.1.1.2.cmml" xref="S2.F2.5.m1.1.1.1.1.2">3</cn><cn type="integer" id="S2.F2.5.m1.1.1.1.1.3.cmml" xref="S2.F2.5.m1.1.1.1.1.3">3</cn><cn type="integer" id="S2.F2.5.m1.1.1.1.1.4.cmml" xref="S2.F2.5.m1.1.1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m1.1d">(3\times 3\times 3)</annotation></semantics></math> nÃ©cessitent des matrices de poids de taille <math id="S2.F2.6.m2.2" class="ltx_Math" alttext="(9\times canaux^{2})+(3\times canaux^{2})" display="inline"><semantics id="S2.F2.6.m2.2b"><mrow id="S2.F2.6.m2.2.2" xref="S2.F2.6.m2.2.2.cmml"><mrow id="S2.F2.6.m2.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.6.m2.1.1.1.1.2" xref="S2.F2.6.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.F2.6.m2.1.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.cmml"><mrow id="S2.F2.6.m2.1.1.1.1.1.2" xref="S2.F2.6.m2.1.1.1.1.1.2.cmml"><mn id="S2.F2.6.m2.1.1.1.1.1.2.2" xref="S2.F2.6.m2.1.1.1.1.1.2.2.cmml">9</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.6.m2.1.1.1.1.1.2.1" xref="S2.F2.6.m2.1.1.1.1.1.2.1.cmml">Ã—</mo><mi id="S2.F2.6.m2.1.1.1.1.1.2.3" xref="S2.F2.6.m2.1.1.1.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.1.1.1.1.1.3" xref="S2.F2.6.m2.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1b" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.1.1.1.1.1.4" xref="S2.F2.6.m2.1.1.1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1c" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.1.1.1.1.1.5" xref="S2.F2.6.m2.1.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1d" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.1.1.1.1.1.6" xref="S2.F2.6.m2.1.1.1.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1e" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">â€‹</mo><msup id="S2.F2.6.m2.1.1.1.1.1.7" xref="S2.F2.6.m2.1.1.1.1.1.7.cmml"><mi id="S2.F2.6.m2.1.1.1.1.1.7.2" xref="S2.F2.6.m2.1.1.1.1.1.7.2.cmml">x</mi><mn id="S2.F2.6.m2.1.1.1.1.1.7.3" xref="S2.F2.6.m2.1.1.1.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.6.m2.1.1.1.1.3" xref="S2.F2.6.m2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.F2.6.m2.2.2.3" xref="S2.F2.6.m2.2.2.3.cmml">+</mo><mrow id="S2.F2.6.m2.2.2.2.1" xref="S2.F2.6.m2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.F2.6.m2.2.2.2.1.2" xref="S2.F2.6.m2.2.2.2.1.1.cmml">(</mo><mrow id="S2.F2.6.m2.2.2.2.1.1" xref="S2.F2.6.m2.2.2.2.1.1.cmml"><mrow id="S2.F2.6.m2.2.2.2.1.1.2" xref="S2.F2.6.m2.2.2.2.1.1.2.cmml"><mn id="S2.F2.6.m2.2.2.2.1.1.2.2" xref="S2.F2.6.m2.2.2.2.1.1.2.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.6.m2.2.2.2.1.1.2.1" xref="S2.F2.6.m2.2.2.2.1.1.2.1.cmml">Ã—</mo><mi id="S2.F2.6.m2.2.2.2.1.1.2.3" xref="S2.F2.6.m2.2.2.2.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.2.2.2.1.1.3" xref="S2.F2.6.m2.2.2.2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1b" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.2.2.2.1.1.4" xref="S2.F2.6.m2.2.2.2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1c" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.2.2.2.1.1.5" xref="S2.F2.6.m2.2.2.2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1d" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">â€‹</mo><mi id="S2.F2.6.m2.2.2.2.1.1.6" xref="S2.F2.6.m2.2.2.2.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1e" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">â€‹</mo><msup id="S2.F2.6.m2.2.2.2.1.1.7" xref="S2.F2.6.m2.2.2.2.1.1.7.cmml"><mi id="S2.F2.6.m2.2.2.2.1.1.7.2" xref="S2.F2.6.m2.2.2.2.1.1.7.2.cmml">x</mi><mn id="S2.F2.6.m2.2.2.2.1.1.7.3" xref="S2.F2.6.m2.2.2.2.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.6.m2.2.2.2.1.3" xref="S2.F2.6.m2.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.6.m2.2c"><apply id="S2.F2.6.m2.2.2.cmml" xref="S2.F2.6.m2.2.2"><plus id="S2.F2.6.m2.2.2.3.cmml" xref="S2.F2.6.m2.2.2.3"></plus><apply id="S2.F2.6.m2.1.1.1.1.1.cmml" xref="S2.F2.6.m2.1.1.1.1"><times id="S2.F2.6.m2.1.1.1.1.1.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.1"></times><apply id="S2.F2.6.m2.1.1.1.1.1.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2"><times id="S2.F2.6.m2.1.1.1.1.1.2.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.1"></times><cn type="integer" id="S2.F2.6.m2.1.1.1.1.1.2.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.2">9</cn><ci id="S2.F2.6.m2.1.1.1.1.1.2.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.3">ğ‘</ci></apply><ci id="S2.F2.6.m2.1.1.1.1.1.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.3">ğ‘</ci><ci id="S2.F2.6.m2.1.1.1.1.1.4.cmml" xref="S2.F2.6.m2.1.1.1.1.1.4">ğ‘›</ci><ci id="S2.F2.6.m2.1.1.1.1.1.5.cmml" xref="S2.F2.6.m2.1.1.1.1.1.5">ğ‘</ci><ci id="S2.F2.6.m2.1.1.1.1.1.6.cmml" xref="S2.F2.6.m2.1.1.1.1.1.6">ğ‘¢</ci><apply id="S2.F2.6.m2.1.1.1.1.1.7.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S2.F2.6.m2.1.1.1.1.1.7.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7">superscript</csymbol><ci id="S2.F2.6.m2.1.1.1.1.1.7.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7.2">ğ‘¥</ci><cn type="integer" id="S2.F2.6.m2.1.1.1.1.1.7.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7.3">2</cn></apply></apply><apply id="S2.F2.6.m2.2.2.2.1.1.cmml" xref="S2.F2.6.m2.2.2.2.1"><times id="S2.F2.6.m2.2.2.2.1.1.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.1"></times><apply id="S2.F2.6.m2.2.2.2.1.1.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2"><times id="S2.F2.6.m2.2.2.2.1.1.2.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.1"></times><cn type="integer" id="S2.F2.6.m2.2.2.2.1.1.2.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.2">3</cn><ci id="S2.F2.6.m2.2.2.2.1.1.2.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.3">ğ‘</ci></apply><ci id="S2.F2.6.m2.2.2.2.1.1.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.3">ğ‘</ci><ci id="S2.F2.6.m2.2.2.2.1.1.4.cmml" xref="S2.F2.6.m2.2.2.2.1.1.4">ğ‘›</ci><ci id="S2.F2.6.m2.2.2.2.1.1.5.cmml" xref="S2.F2.6.m2.2.2.2.1.1.5">ğ‘</ci><ci id="S2.F2.6.m2.2.2.2.1.1.6.cmml" xref="S2.F2.6.m2.2.2.2.1.1.6">ğ‘¢</ci><apply id="S2.F2.6.m2.2.2.2.1.1.7.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7"><csymbol cd="ambiguous" id="S2.F2.6.m2.2.2.2.1.1.7.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7">superscript</csymbol><ci id="S2.F2.6.m2.2.2.2.1.1.7.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7.2">ğ‘¥</ci><cn type="integer" id="S2.F2.6.m2.2.2.2.1.1.7.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m2.2d">(9\times canaux^{2})+(3\times canaux^{2})</annotation></semantics></math>. Ceci est moins de la moitiÃ© de celles nÃ©cessaires pour la convolution 3D complÃ¨te <math id="S2.F2.7.m3.1" class="ltx_Math" alttext="(27\times canaux^{2})" display="inline"><semantics id="S2.F2.7.m3.1b"><mrow id="S2.F2.7.m3.1.1.1" xref="S2.F2.7.m3.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.7.m3.1.1.1.2" xref="S2.F2.7.m3.1.1.1.1.cmml">(</mo><mrow id="S2.F2.7.m3.1.1.1.1" xref="S2.F2.7.m3.1.1.1.1.cmml"><mrow id="S2.F2.7.m3.1.1.1.1.2" xref="S2.F2.7.m3.1.1.1.1.2.cmml"><mn id="S2.F2.7.m3.1.1.1.1.2.2" xref="S2.F2.7.m3.1.1.1.1.2.2.cmml">27</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.7.m3.1.1.1.1.2.1" xref="S2.F2.7.m3.1.1.1.1.2.1.cmml">Ã—</mo><mi id="S2.F2.7.m3.1.1.1.1.2.3" xref="S2.F2.7.m3.1.1.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1" xref="S2.F2.7.m3.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.7.m3.1.1.1.1.3" xref="S2.F2.7.m3.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1b" xref="S2.F2.7.m3.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.7.m3.1.1.1.1.4" xref="S2.F2.7.m3.1.1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1c" xref="S2.F2.7.m3.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.7.m3.1.1.1.1.5" xref="S2.F2.7.m3.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1d" xref="S2.F2.7.m3.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.F2.7.m3.1.1.1.1.6" xref="S2.F2.7.m3.1.1.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1e" xref="S2.F2.7.m3.1.1.1.1.1.cmml">â€‹</mo><msup id="S2.F2.7.m3.1.1.1.1.7" xref="S2.F2.7.m3.1.1.1.1.7.cmml"><mi id="S2.F2.7.m3.1.1.1.1.7.2" xref="S2.F2.7.m3.1.1.1.1.7.2.cmml">x</mi><mn id="S2.F2.7.m3.1.1.1.1.7.3" xref="S2.F2.7.m3.1.1.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.7.m3.1.1.1.3" xref="S2.F2.7.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.7.m3.1c"><apply id="S2.F2.7.m3.1.1.1.1.cmml" xref="S2.F2.7.m3.1.1.1"><times id="S2.F2.7.m3.1.1.1.1.1.cmml" xref="S2.F2.7.m3.1.1.1.1.1"></times><apply id="S2.F2.7.m3.1.1.1.1.2.cmml" xref="S2.F2.7.m3.1.1.1.1.2"><times id="S2.F2.7.m3.1.1.1.1.2.1.cmml" xref="S2.F2.7.m3.1.1.1.1.2.1"></times><cn type="integer" id="S2.F2.7.m3.1.1.1.1.2.2.cmml" xref="S2.F2.7.m3.1.1.1.1.2.2">27</cn><ci id="S2.F2.7.m3.1.1.1.1.2.3.cmml" xref="S2.F2.7.m3.1.1.1.1.2.3">ğ‘</ci></apply><ci id="S2.F2.7.m3.1.1.1.1.3.cmml" xref="S2.F2.7.m3.1.1.1.1.3">ğ‘</ci><ci id="S2.F2.7.m3.1.1.1.1.4.cmml" xref="S2.F2.7.m3.1.1.1.1.4">ğ‘›</ci><ci id="S2.F2.7.m3.1.1.1.1.5.cmml" xref="S2.F2.7.m3.1.1.1.1.5">ğ‘</ci><ci id="S2.F2.7.m3.1.1.1.1.6.cmml" xref="S2.F2.7.m3.1.1.1.1.6">ğ‘¢</ci><apply id="S2.F2.7.m3.1.1.1.1.7.cmml" xref="S2.F2.7.m3.1.1.1.1.7"><csymbol cd="ambiguous" id="S2.F2.7.m3.1.1.1.1.7.1.cmml" xref="S2.F2.7.m3.1.1.1.1.7">superscript</csymbol><ci id="S2.F2.7.m3.1.1.1.1.7.2.cmml" xref="S2.F2.7.m3.1.1.1.1.7.2">ğ‘¥</ci><cn type="integer" id="S2.F2.7.m3.1.1.1.1.7.3.cmml" xref="S2.F2.7.m3.1.1.1.1.7.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.7.m3.1d">(27\times canaux^{2})</annotation></semantics></math> </figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Classement Audio</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Nous avons adaptÃ© le modÃ¨leÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> basÃ© sur une transformation de Fourier Ã  court terme (TFCT) avec des fenÃªtres plus longues (64 ms) chevauchÃ©es Ã  75%, ce qui donne une meilleure rÃ©solution en frÃ©quence pour la voix humaine. Nous avons ensuite converti les frÃ©quences du spectrogramme en Ã©chelle logarithmique de Mel qui se rapproche de la perception humaine du son. Finalement, la transformÃ©e en cosinus discrÃ¨te de type II (DCT) donne les coefficients Mel-Frequency Cepstral (MFCC) par 80 filtres triangulaires crÃ©Ã©s pour couvrir la plage de frÃ©quences Mel. Nous sÃ©lectionnons uniquement 13 premiers qui sont utiles Ã  la reconnaissance de la paroleÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Le classement audio est entraÃ®nÃ© Ã  partir de ces MFCC en utilisant un rÃ©seau de neurones convolutif illustrÃ© dans la FigÂ <a href="#S1.F1" title="Figure 1 â€£ 1.2 Contexte ThÃ©orique â€£ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Le modÃ¨le commence par deux couches de convolution 2D avec 16 filtres chacune, une taille de noyau de (3,3) et une fonction dâ€™activation ReLu. Ces couches convolutives 2D sont utilisÃ©es pour extraire des caractÃ©ristiques importantes des MFCC dâ€™entrÃ©e, qui sont des matrices de taille (778, 13, 1). Ensuite, les couches dâ€™Aplatissement (Flatten) et dâ€™Extinction (Dropout) sâ€™enchainent pour respectivement convertir la sortie de la derniÃ¨re couche convolutive en un vecteur Ã  une dimension et pour dÃ©sactiver alÃ©atoirement certains neurones afin de contourner le surapprentissage. Deux couches entiÃ¨rement connectÃ©es sont ensuite combinÃ©es via une fonction SigmoÃ¯d, ce qui permet dâ€™obtenir les probabilitÃ©s des classes Ã  prÃ©dire.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion Tardive Des ModalitÃ©s</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.2" class="ltx_p">Nous avons choisi pour notre modÃ¨le une fusion tardive, dite &lt;&lt; fusion orientÃ©e dÃ©cisions &gt;&gt;. En effet,Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> a montrÃ© queÂ : (1) un meilleur rÃ©seau unimodal peut surpasser un rÃ©seau multimodal contre le problÃ¨me de surapprentissage; (2) diffÃ©rentes modalitÃ©s se surajustent et se gÃ©nÃ©ralisent Ã  des rythmes diffÃ©rents, donc les entraÃ®ner conjointement avec une seule stratÃ©gie dâ€™optimisation nâ€™est pas optimal. Nous avons imaginÃ© un mÃ©canisme de &lt;&lt; correction dâ€™erreur &gt;&gt; qui fusionne des prÃ©dictions provenant de deux rÃ©seaux unimodaux qui sont entrainÃ©s sÃ©parÃ©ment. La fonction de perte que nous avons utilisÃ© pour ce rÃ©seau de fusion tardive est la mÃªme que pour les sous-modÃ¨les: Perte dâ€™entropie croisÃ©e binaire dÃ©finie comme la formuleÂ <a href="#S2.E1" title="In 2.3 Fusion Tardive Des ModalitÃ©s â€£ 2 MÃ©thode ProposÃ©e" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Cette formule suppose que les <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><msub id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ğ‘</ci><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">p_{i}</annotation></semantics></math> sont des probabilitÃ©s et les <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><msub id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">y</mi><mi id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">ğ‘¦</ci><ci id="S2.SS3.p1.2.m2.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">y_{i}</annotation></semantics></math> sont des labels (0;1).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="L=-\frac{1}{N}\sum_{i=1}^{2}y_{i}log(p_{i})=-\frac{1}{N}[y_{1}log(p_{1})+y_{2}log(p_{2})]" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.4" xref="S2.E1.m1.2.2.4.cmml">L</mi><mo id="S2.E1.m1.2.2.5" xref="S2.E1.m1.2.2.5.cmml">=</mo><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1a" xref="S2.E1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml">1</mn><mi id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><munderover id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.1.1.1.2.2.3.3" xref="S2.E1.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S2.E1.m1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.2.3.cmml">2</mn></munderover><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2a" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2b" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2c" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.6" xref="S2.E1.m1.2.2.6.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2a" xref="S2.E1.m1.2.2.2.cmml">âˆ’</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mfrac id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mn id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">1</mn><mi id="S2.E1.m1.2.2.2.1.3.3" xref="S2.E1.m1.2.2.2.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.2.cmml">y</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.4" xref="S2.E1.m1.2.2.2.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2a" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.5" xref="S2.E1.m1.2.2.2.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2b" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.6" xref="S2.E1.m1.2.2.2.1.1.1.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2c" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml">p</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">+</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml">y</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.2.3.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.4" xref="S2.E1.m1.2.2.2.1.1.1.1.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2a" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.5" xref="S2.E1.m1.2.2.2.1.1.1.1.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2b" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">â€‹</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.6" xref="S2.E1.m1.2.2.2.1.1.1.1.2.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2c" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2.cmml">p</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><and id="S2.E1.m1.2.2a.cmml" xref="S2.E1.m1.2.2"></and><apply id="S2.E1.m1.2.2b.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.5.cmml" xref="S2.E1.m1.2.2.5"></eq><ci id="S2.E1.m1.2.2.4.cmml" xref="S2.E1.m1.2.2.4">ğ¿</ci><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><minus id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1"></minus><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3"></divide><cn type="integer" id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2">1</cn><ci id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3">ğ‘</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><apply id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E1.m1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.2"></sum><apply id="S2.E1.m1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3"><eq id="S2.E1.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S2.E1.m1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.3">2</cn></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.4">ğ‘™</ci><ci id="S2.E1.m1.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.1.1.5">ğ‘œ</ci><ci id="S2.E1.m1.1.1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.1.1.6">ğ‘”</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply><apply id="S2.E1.m1.2.2c.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.6.cmml" xref="S2.E1.m1.2.2.6"></eq><share href="#S2.E1.m1.1.1.1.cmml" id="S2.E1.m1.2.2d.cmml" xref="S2.E1.m1.2.2"></share><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><divide id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3"></divide><cn type="integer" id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">1</cn><ci id="S2.E1.m1.2.2.2.1.3.3.cmml" xref="S2.E1.m1.2.2.2.1.3.3">ğ‘</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1"><plus id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3"></plus><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.2">ğ‘¦</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.3">1</cn></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.4">ğ‘™</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.5.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.5">ğ‘œ</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.6.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.6">ğ‘”</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><times id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"></times><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.2">ğ‘¦</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.3">2</cn></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.4.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.4">ğ‘™</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.5.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.5">ğ‘œ</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.6.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.6">ğ‘”</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2">ğ‘</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">L=-\frac{1}{N}\sum_{i=1}^{2}y_{i}log(p_{i})=-\frac{1}{N}[y_{1}log(p_{1})+y_{2}log(p_{2})]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Ã‰valuation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Base De DonnÃ©es</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">En gÃ©nÃ©ral, il est difficile de collecter des donnÃ©es de haute qualitÃ© auprÃ¨s de personnes qui prÃ©sentent des symptÃ´mes de TSPT. Il peut y avoir aussi des considÃ©rations Ã©thiques qui limitent la collecte et lâ€™utilisation de donnÃ©es en milieu naturel. Cela peut Ãªtre particuliÃ¨rement difficile dans le contexte dâ€™un trouble sensible comme le TSPT, oÃ¹ les participants peuvent Ãªtre rÃ©ticents Ã  divulguer des informations personnelles.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Par consÃ©quent, seules quatre bases de donnÃ©es non structurÃ©es pour la dÃ©tection du TSPT existent: eDAIC-WOZÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, FEMHÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, AuroraÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> et PTSD in-the-wildÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Parmi elles, seules les bases eDAIC-WOZ et PTSD in-the-wild disposent Ã  la fois de modalitÃ©s audio et vidÃ©o. Nous avons choisi dâ€™appliquer notre modÃ¨le <code id="S3.SS1.p2.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> sur la base PTSD in-the-wild pour son caractÃ¨re rÃ©el en milieu naturel. La base de donnÃ©es PTSD in-the-wild (EULA) contient 634 vidÃ©os Ã©quilibrÃ©esÂ : 317 vidÃ©os de sujets avec TSPT et 317 vidÃ©os de sujets tÃ©moins sains avec aucun symptÃ´me de TSPT.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>RÃ©sultats</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Nous nous intÃ©ressons Ã  lâ€™Ã©valuation dâ€™une classification binaire avec deux classesÂ : TSPT (positif) et Non-TSPT (nÃ©gatif) avec diffÃ©rentes mÃ©triques de classification populairesÂ : lâ€™accuracy, la prÃ©cision, le rappel. Nous avons suivi le processus train/validation/test proposÃ© par <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (80%/10%/10%) pour entraÃ®ner (<code id="S3.SS2.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code>) sur une carte GPU NVIDIA A100 SXM 40Go avec taille de batch de 8, un taux dâ€™apprentissage de 0.001, et un optimiseur Adam pour 50 Ã©poques. Ces paramÃ¨tres sont optimaux pour Ã©viter le surapprentissage liÃ© Ã  la taille des donnÃ©es. De plus, nous avons utilisÃ© diffÃ©rentes mÃ©thodes de rÃ©gularisation pour le classement audioÂ (TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2 RÃ©sultats â€£ 3 Ã‰valuation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>RÃ©sultats du classement sur les donnÃ©es test</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">ModalitÃ©</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">RÃ©gularisation</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Accuracy</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">PrÃ©cision</span></th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Rappel</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">VidÃ©o</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">0,89</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">0,84</span></td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">0,84</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">0,72</span></td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">0,68</span></td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">0,81</span></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">0,73</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.4.1" class="ltx_text" style="font-size:90%;">0,67</span></td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.5.1" class="ltx_text" style="font-size:90%;">0,90</span></td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">L2</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">0,75</span></td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.4.1" class="ltx_text" style="font-size:90%;">0,72</span></td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">0,81</span></td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">VidÃ©o + Audio</span></th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">0,89</span></td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">0,90</span></td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">0,87</span></td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">VidÃ©o + Audio</span></th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">L2</span></td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">0,92</span></td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">0,88</span></td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">0,97</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discussion</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Le meilleur des modÃ¨les unimodaux est le classement vidÃ©o avec une accuracy de 0,89. En revanche, le classement audio de base ne donne pas de trÃ¨s bons rÃ©sultats mÃªme si les rÃ©gularisations L1 et L2 lâ€™amÃ©liorent respectivement Ã  0,73 et 0,75. Notre approche de fusion tardive des modalitÃ©s apporte de rÃ©elles amÃ©liorations par rapport aux classements unimodaux car <code id="S3.SS3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> donne la meilleure accuracy (0,92), avec une rÃ©gularisation L2, et le meilleur rappel (0,97). Le principal avantage dâ€™une fusion tardive est la prise en charge des diffÃ©rentes modalitÃ©s non alignÃ©es ainsi nous ne dÃ©pendons pas de lâ€™interopÃ©rabilitÃ© des capteurs. De plus, lâ€™entrainement indÃ©pendant des deux sous-modÃ¨les permet de gagner du temps en effectuant des tÃ¢ches en parallÃ¨le. Enfin, cette fusion permet dâ€™apporter la flexibilitÃ© pour le choix des sous-modÃ¨les adaptÃ©s Ã  chaque modalitÃ©. Il est Ã  noter que la diffÃ©rence de taille des fichiers de la base PTSD in-the-wild (le plus courtÂ : 0Â minÂ 35Â s et le plus longÂ : 44Â minÂ 40Â s) peut crÃ©er des difficultÃ©s pour lâ€™extraction des variables en entrÃ©e du modÃ¨le de convolution.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Nous proposons <code id="S4.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code>, un modÃ¨le qui fusionne tardivement des modalitÃ©s audio et vidÃ©o pour dÃ©tecter le TSPT. GrÃ¢ce Ã  un mÃ©canisme de correction dâ€™erreur, notre modÃ¨le surpasse les modÃ¨les unimodaux. En plus dâ€™Ãªtre non invasif, <code id="S4.p1.1.2" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> traite les informations sensibles sur les patients Ã  trÃ¨s bas niveau (pixel, MFCC), ce qui permet de garder une certaine confidentialitÃ© pour les patients par rapport aux approches type NLP oÃ¹ les paroles sont transcrites.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Ce travail ouvre une multitude de travaux futurs. PremiÃ¨rement, nous avons lâ€™intention dâ€™extraire des variables de haut niveau Ã  partir dâ€™aspects comportementaux subtils, tels que les mouvements du corps, les expressions faciales pour la vision ainsi que la prosodie et la parole pour lâ€™audio. DeuxiÃ¨ment, dâ€™autres directions concernent la fusion des modalitÃ©s Ã  travers le mÃ©canisme de lâ€™attention inter-modalitÃ©.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">American Psychiatric Association</span>,
Marc-Antoine <span id="bib.bib1.2.2" class="ltx_text ltx_font_smallcaps">Crocq</span>, Julien-Daniel
<span id="bib.bib1.3.3" class="ltx_text ltx_font_smallcaps">Guelfi</span>, Patrice
<span id="bib.bib1.4.4" class="ltx_text ltx_font_smallcaps">Boyer</span>, Marie-Claire
<span id="bib.bib1.5.5" class="ltx_text ltx_font_smallcaps">Pull</span> et Charles-Bernard
<span id="bib.bib1.6.6" class="ltx_text ltx_font_smallcaps">Pull</span> :

</span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text ltx_font_italic">DSM-5 - Manuel diagnostique et statistique des troubles
mentaux</span>.

</span>
<span class="ltx_bibblock">Elsevier Masson, juin 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Debrup <span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">Banerjee</span>, Kazi
<span id="bib.bib2.2.2" class="ltx_text ltx_font_smallcaps">Islam</span>, Keyi
<span id="bib.bib2.3.3" class="ltx_text ltx_font_smallcaps">Xue</span>, Gang
<span id="bib.bib2.4.4" class="ltx_text ltx_font_smallcaps">Mei</span>, Lemin
<span id="bib.bib2.5.5" class="ltx_text ltx_font_smallcaps">Xiao</span>, Guangfan
<span id="bib.bib2.6.6" class="ltx_text ltx_font_smallcaps">Zhang</span>, Roger
<span id="bib.bib2.7.7" class="ltx_text ltx_font_smallcaps">Xu</span>, Cai
<span id="bib.bib2.8.8" class="ltx_text ltx_font_smallcaps">Lei</span>, Shuiwang
<span id="bib.bib2.9.9" class="ltx_text ltx_font_smallcaps">Ji</span> et Jiang
<span id="bib.bib2.10.10" class="ltx_text ltx_font_smallcaps">Li</span> :

</span>
<span class="ltx_bibblock">A deep transfer learning approach for improved post-traumatic stress
disorder diagnosis.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.11.1" class="ltx_text ltx_font_italic">Knowl Inf Syst</span>, 60(3):1693â€“1724,
septembre 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jeroen <span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">Breebaart</span> et MartinÂ F.
<span id="bib.bib3.2.2" class="ltx_text ltx_font_smallcaps">McKinney</span> :

</span>
<span class="ltx_bibblock">Features for Audio Classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">In</em> Wim F.Â J. <span id="bib.bib3.4.2" class="ltx_text ltx_font_smallcaps">Verhaegh</span>, Emile <span id="bib.bib3.5.3" class="ltx_text ltx_font_smallcaps">Aarts</span> et Jan <span id="bib.bib3.6.4" class="ltx_text ltx_font_smallcaps">Korst</span>, Ã©diteurs : <span id="bib.bib3.7.5" class="ltx_text ltx_font_italic">Algorithms in Ambient
Intelligence</span>, Philips Research, pages 113â€“129. Springer Netherlands,
Dordrecht, 2004.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
MichaelÂ S. <span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">Breen</span>, KevinÂ G.F.
<span id="bib.bib4.2.2" class="ltx_text ltx_font_smallcaps">Thomas</span>, DavidÂ S.
<span id="bib.bib4.3.3" class="ltx_text ltx_font_smallcaps">Baldwin</span> et Gosia
<span id="bib.bib4.4.4" class="ltx_text ltx_font_smallcaps">Lipinska</span> :

</span>
<span class="ltx_bibblock">Modelling PTSD diagnosis using sleep, memory, and adrenergic
metabolites: An exploratory machine-learning study.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text ltx_font_italic">Human Psychopharmacology: Clinical and Experimental</span>,
34(2):e2691, 2019.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hup.2691.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jonathan <span id="bib.bib5.1.1" class="ltx_text ltx_font_smallcaps">Gratch</span>, Ron
<span id="bib.bib5.2.2" class="ltx_text ltx_font_smallcaps">Artstein</span>, Gale
<span id="bib.bib5.3.3" class="ltx_text ltx_font_smallcaps">Lucas</span>, Giota
<span id="bib.bib5.4.4" class="ltx_text ltx_font_smallcaps">Stratou</span>, Stefan
<span id="bib.bib5.5.5" class="ltx_text ltx_font_smallcaps">Scherer</span>, Angela
<span id="bib.bib5.6.6" class="ltx_text ltx_font_smallcaps">Nazarian</span>, Rachel
<span id="bib.bib5.7.7" class="ltx_text ltx_font_smallcaps">Wood</span>, Jill
<span id="bib.bib5.8.8" class="ltx_text ltx_font_smallcaps">Boberg</span>, David
<span id="bib.bib5.9.9" class="ltx_text ltx_font_smallcaps">DeVault</span>, Stacy
<span id="bib.bib5.10.10" class="ltx_text ltx_font_smallcaps">Marsella</span>, David
<span id="bib.bib5.11.11" class="ltx_text ltx_font_smallcaps">Traum</span>, Skip
<span id="bib.bib5.12.12" class="ltx_text ltx_font_smallcaps">Rizzo</span> et Louis-Philippe
<span id="bib.bib5.13.13" class="ltx_text ltx_font_smallcaps">Morency</span> :

</span>
<span class="ltx_bibblock">The Distress Analysis Interview Corpus of human and computer
interviews.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.14.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib5.15.2" class="ltx_text ltx_font_italic">Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LRECâ€™14)</span>,
Reykjavik, Iceland, mai 2014. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Marit <span id="bib.bib6.1.1" class="ltx_text ltx_font_smallcaps">Hauschildt</span>, Maarten J.Â V.
<span id="bib.bib6.2.2" class="ltx_text ltx_font_smallcaps">Peters</span>, Steffen
<span id="bib.bib6.3.3" class="ltx_text ltx_font_smallcaps">Moritz</span> et Lena
<span id="bib.bib6.4.4" class="ltx_text ltx_font_smallcaps">Jelinek</span> :

</span>
<span class="ltx_bibblock">Heart rate variability in response to affective scenes in
posttraumatic stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text ltx_font_italic">Biological Psychology</span>, 88(2):215â€“222,
dÃ©cembre 2011.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Rebecca <span id="bib.bib7.1.1" class="ltx_text ltx_font_smallcaps">Hinrichs</span>, Vasiliki
<span id="bib.bib7.2.2" class="ltx_text ltx_font_smallcaps">Michopoulos</span>, Sterling
<span id="bib.bib7.3.3" class="ltx_text ltx_font_smallcaps">Winters</span>, AlexÂ O.
<span id="bib.bib7.4.4" class="ltx_text ltx_font_smallcaps">Rothbaum</span>, BarbaraÂ O.
<span id="bib.bib7.5.5" class="ltx_text ltx_font_smallcaps">Rothbaum</span>, KerryÂ J.
<span id="bib.bib7.6.6" class="ltx_text ltx_font_smallcaps">Ressler</span> et Tanja
<span id="bib.bib7.7.7" class="ltx_text ltx_font_smallcaps">Jovanovic</span> :

</span>
<span class="ltx_bibblock">Mobile assessment of heightened skin conductance in posttraumatic
stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text ltx_font_italic">Depression and Anxiety</span>, 34(6):502â€“507, 2017.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/da.22610.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
KaziÂ Aminul <span id="bib.bib8.1.1" class="ltx_text ltx_font_smallcaps">Islam</span>, Daniel
<span id="bib.bib8.2.2" class="ltx_text ltx_font_smallcaps">Perez</span> et Jiang
<span id="bib.bib8.3.3" class="ltx_text ltx_font_smallcaps">Li</span> :

</span>
<span class="ltx_bibblock">A Transfer Learning Approach for the 2018 FEMH Voice Data
Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.4.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib8.5.2" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Big
Data (Big Data)</span>, pages 5252â€“5257, dÃ©cembre 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Stefan <span id="bib.bib9.1.1" class="ltx_text ltx_font_smallcaps">Kahl</span>, Amanda
<span id="bib.bib9.2.2" class="ltx_text ltx_font_smallcaps">Navine</span>, Tom
<span id="bib.bib9.3.3" class="ltx_text ltx_font_smallcaps">Denton</span>, Holger
<span id="bib.bib9.4.4" class="ltx_text ltx_font_smallcaps">Klinck</span>, Patrick
<span id="bib.bib9.5.5" class="ltx_text ltx_font_smallcaps">Hart</span>, HervÃ©
<span id="bib.bib9.6.6" class="ltx_text ltx_font_smallcaps">Glotin</span>, HervÃ©
<span id="bib.bib9.7.7" class="ltx_text ltx_font_smallcaps">GoÃ«au</span>, Willem-Pier
<span id="bib.bib9.8.8" class="ltx_text ltx_font_smallcaps">Vellinga</span>, Robert
<span id="bib.bib9.9.9" class="ltx_text ltx_font_smallcaps">PlanquÃ©</span> et Alexis
<span id="bib.bib9.10.10" class="ltx_text ltx_font_smallcaps">Joly</span> :

</span>
<span class="ltx_bibblock">Overview of BirdCLEF 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.11.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib9.12.2" class="ltx_text ltx_font_italic">Proceedings of the Working Notes of CLEF 2022 -
Conference and Labs of the Evaluation Forum</span>, Bologna, Italy,
septembre 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
JanÂ A. <span id="bib.bib10.1.1" class="ltx_text ltx_font_smallcaps">Lindsay</span>, MichaelÂ R.
<span id="bib.bib10.2.2" class="ltx_text ltx_font_smallcaps">Kauth</span>, Sonora
<span id="bib.bib10.3.3" class="ltx_text ltx_font_smallcaps">Hudson</span>, LindseyÂ A.
<span id="bib.bib10.4.4" class="ltx_text ltx_font_smallcaps">Martin</span>, DavidÂ J.
<span id="bib.bib10.5.5" class="ltx_text ltx_font_smallcaps">Ramsey</span>, Lawrence
<span id="bib.bib10.6.6" class="ltx_text ltx_font_smallcaps">Daily</span> et John
<span id="bib.bib10.7.7" class="ltx_text ltx_font_smallcaps">Rader</span> :

</span>
<span class="ltx_bibblock">Implementation of Video Telehealth to Improve Access to
Evidence-Based Psychotherapy for Posttraumatic Stress Disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text ltx_font_italic">Telemedicine and e-Health</span>, 21(6):467â€“472, juin 2015.

</span>
<span class="ltx_bibblock">Publisher: Mary Ann Liebert, Inc., publishers.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
SevinÃ§Â Ä°lhan <span id="bib.bib11.1.1" class="ltx_text ltx_font_smallcaps">Omurca</span> et Ekin
<span id="bib.bib11.2.2" class="ltx_text ltx_font_smallcaps">Ekinci</span> :

</span>
<span class="ltx_bibblock">An alternative evaluation of post traumatic stress disorder with
machine learning methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib11.4.2" class="ltx_text ltx_font_italic">2015 International Symposium on Innovations in
Intelligent SysTems and Applications (INISTA)</span>, pages 1â€“7, septembre
2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Dhanesh <span id="bib.bib12.1.1" class="ltx_text ltx_font_smallcaps">Ramachandram</span> et GrahamÂ W.
<span id="bib.bib12.2.2" class="ltx_text ltx_font_smallcaps">Taylor</span> :

</span>
<span class="ltx_bibblock">Deep Multimodal Learning: A Survey on Recent Advances and
Trends.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 34(6):96â€“108, novembre 2017.

</span>
<span class="ltx_bibblock">Conference Name: IEEE Signal Processing Magazine.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D.Â <span id="bib.bib13.1.1" class="ltx_text ltx_font_smallcaps">Rangaprakash</span>, Gopikrishna
<span id="bib.bib13.2.2" class="ltx_text ltx_font_smallcaps">Deshpande</span>, ThomasÂ A.
<span id="bib.bib13.3.3" class="ltx_text ltx_font_smallcaps">Daniel</span>, AdamÂ M.
<span id="bib.bib13.4.4" class="ltx_text ltx_font_smallcaps">Goodman</span>, JenniferÂ L.
<span id="bib.bib13.5.5" class="ltx_text ltx_font_smallcaps">Robinson</span>, Nouha
<span id="bib.bib13.6.6" class="ltx_text ltx_font_smallcaps">Salibi</span>, JeffreyÂ S.
<span id="bib.bib13.7.7" class="ltx_text ltx_font_smallcaps">Katz</span>, ThomasÂ S.
<span id="bib.bib13.8.8" class="ltx_text ltx_font_smallcaps">DenneyÂ Jr.</span> et MichaelÂ N.
<span id="bib.bib13.9.9" class="ltx_text ltx_font_smallcaps">Dretsch</span> :

</span>
<span class="ltx_bibblock">Compromised hippocampus-striatum pathway as a potential imaging
biomarker of mild-traumatic brain injury and posttraumatic stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.10.1" class="ltx_text ltx_font_italic">Human Brain Mapping</span>, 38(6):2843â€“2864,
2017.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.23551.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Niels <span id="bib.bib14.1.1" class="ltx_text ltx_font_smallcaps">Rathlev</span> :

</span>
<span class="ltx_bibblock">Correction: The AURORA Study: a longitudinal, multimodal
library of brain biology and function after traumatic stress exposure.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text ltx_font_italic">All Scholarly Works</span>, septembre 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
LaurenÂ E. <span id="bib.bib15.1.1" class="ltx_text ltx_font_smallcaps">Salminen</span>, RajendraÂ A.
<span id="bib.bib15.2.2" class="ltx_text ltx_font_smallcaps">Morey</span>, BrandalynÂ C.
<span id="bib.bib15.3.3" class="ltx_text ltx_font_smallcaps">Riedel</span>, Neda
<span id="bib.bib15.4.4" class="ltx_text ltx_font_smallcaps">Jahanshad</span>, EmilyÂ L.
<span id="bib.bib15.5.5" class="ltx_text ltx_font_smallcaps">Dennis</span> et PaulÂ M.
<span id="bib.bib15.6.6" class="ltx_text ltx_font_smallcaps">Thompson</span> :

</span>
<span class="ltx_bibblock">Adaptive Identification of Cortical and Subcortical Imaging
Markers of Early Life Stress and Posttraumatic Stress Disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text ltx_font_italic">Journal of Neuroimaging</span>, 29(3):335â€“343, 2019.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jon.12600.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Moctar AbdoulÂ Latif <span id="bib.bib16.1.1" class="ltx_text ltx_font_smallcaps">Sawadogo</span>,
Furkan <span id="bib.bib16.2.2" class="ltx_text ltx_font_smallcaps">Pala</span>, Gurkirat
<span id="bib.bib16.3.3" class="ltx_text ltx_font_smallcaps">Singh</span>, Imen
<span id="bib.bib16.4.4" class="ltx_text ltx_font_smallcaps">Selmi</span>, Pauline
<span id="bib.bib16.5.5" class="ltx_text ltx_font_smallcaps">Puteaux</span> et Alice
<span id="bib.bib16.6.6" class="ltx_text ltx_font_smallcaps">Othmani</span> :

</span>
<span class="ltx_bibblock">PTSD in the Wild: A Video Database for Studying
Post-Traumatic Stress Disorder Recognition in Unconstrained
Environments, septembre 2022.

</span>
<span class="ltx_bibblock">arXiv:2209.14085 [cs].

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jeff <span id="bib.bib17.1.1" class="ltx_text ltx_font_smallcaps">Sawalha</span>, Muhammad
<span id="bib.bib17.2.2" class="ltx_text ltx_font_smallcaps">Yousefnezhad</span>, Zehra
<span id="bib.bib17.3.3" class="ltx_text ltx_font_smallcaps">Shah</span>, Matthew R.Â G.
<span id="bib.bib17.4.4" class="ltx_text ltx_font_smallcaps">Brown</span>, AndrewÂ J.
<span id="bib.bib17.5.5" class="ltx_text ltx_font_smallcaps">Greenshaw</span> et Russell
<span id="bib.bib17.6.6" class="ltx_text ltx_font_smallcaps">Greiner</span> :

</span>
<span class="ltx_bibblock">Detecting Presence of PTSD Using Sentiment Analysis From
Text Data.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text ltx_font_italic">Frontiers in Psychiatry</span>, 12, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Chappidi <span id="bib.bib18.1.1" class="ltx_text ltx_font_smallcaps">Suneetha</span> et Raju
<span id="bib.bib18.2.2" class="ltx_text ltx_font_smallcaps">Anitha</span> :

</span>
<span class="ltx_bibblock">A Survey Of Machine Learning Techniques OnSpeech Based
Emotion Recognition And Post Traumatic Stress
DisorderDetection.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">nq</span>, 20(14):1â€“11, dÃ©cembre 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
DuÂ <span id="bib.bib19.1.1" class="ltx_text ltx_font_smallcaps">Tran</span>, Heng
<span id="bib.bib19.2.2" class="ltx_text ltx_font_smallcaps">Wang</span>, Lorenzo
<span id="bib.bib19.3.3" class="ltx_text ltx_font_smallcaps">Torresani</span>, Jamie
<span id="bib.bib19.4.4" class="ltx_text ltx_font_smallcaps">Ray</span>, Yann
<span id="bib.bib19.5.5" class="ltx_text ltx_font_smallcaps">LeCun</span> et Manohar
<span id="bib.bib19.6.6" class="ltx_text ltx_font_smallcaps">Paluri</span> :

</span>
<span class="ltx_bibblock">A Closer Look at Spatiotemporal Convolutions for Action
Recognition, avril 2018.

</span>
<span class="ltx_bibblock">arXiv:1711.11248 [cs].

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Weiyao <span id="bib.bib20.1.1" class="ltx_text ltx_font_smallcaps">Wang</span>,
DuÂ <span id="bib.bib20.2.2" class="ltx_text ltx_font_smallcaps">Tran</span> et Matt
<span id="bib.bib20.3.3" class="ltx_text ltx_font_smallcaps">Feiszli</span> :

</span>
<span class="ltx_bibblock">What Makes Training Multi-Modal Classification Networks
Hard?

</span>
<span class="ltx_bibblock"><em id="bib.bib20.4.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib20.5.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</span>, pages 12695â€“12705, 2020.

</span>
</li>
</ul>
</section>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.10564" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.10565" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.10565">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.10565" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.10566" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:01:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
