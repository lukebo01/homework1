<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.10565] 1 Introduction</title><meta property="og:description" content="In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error r…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.10565">

<!--Generated on Fri Apr  5 17:01:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\titre</span>
<p id="p1.2" class="ltx_p">PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\auteurs</span><span id="p2.2" class="ltx_ERROR undefined">\auteur</span>
<p id="p2.3" class="ltx_p">LongNguyen-Phuoclong.nguyen-phuoc@emse.fr1,2
<span id="p2.3.1" class="ltx_ERROR undefined">\auteur</span>RenaldGaboriaurenald.gaboriau@mjinnov.com2
<span id="p2.3.2" class="ltx_ERROR undefined">\auteur</span>DimitriDelacroixdimitri.delacroix@mjinnov.com2
<span id="p2.3.3" class="ltx_ERROR undefined">\auteur</span>LaurentNavarronavarro@emse.fr1</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\affils</span>
<p id="p3.2" class="ltx_p">Mines Saint-Étienne, University of Lyon, University Jean Monnet, Inserm, U 1059 Sainbiose, Centre CIS, 42023 Saint-Étienne, France

MJ Lab, MJ INNOV, 42000 Saint-Etienne, France</p>
</div>
<div id="p4" class="ltx_para">
<span id="p4.1" class="ltx_ERROR undefined">\resume</span>
<p id="p4.2" class="ltx_p">Afin de proposer un moyen plus objectif et plus rapide de diagnostiquer le trouble de stress post-traumatique (TSPT), nous présentons <code id="p4.2.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> qui fusionne deux réseaux de neurones convolutifs unimodaux et qui donne un faible taux d’erreurs de détection. En ne prenant que des vidéos et des audios comme entrées, le modèle pourrait être utilisé dans la configuration de séances de téléconsultation, dans l’optimisation des parcours patients ou encore pour l’interaction humain-robot.</p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present <code id="id1.id1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contexte Général</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Tout individu, au cours de sa vie, peut rencontrer des situations potentiellement traumatogènes.  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> définit comme traumatogène toute situation qui implique « une mort effective, une menace de mort, une blessure grave ou des violences sexuelles ». En France, le TSPT toucherait entre 1 et 2% de la population. Les symptômes du TSPT causent des problèmes importants dans les situations sociales ou professionnelles.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Traditionnellement, le TSPT a été diagnostiqué par des professionnels de la santé impliquant des questionnaires. La collecte d’informations par le biais d’un questionnaire auto-déclaratif a des limites car souvent biaisés <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> : (1) les distorsions de la mémoire et de la perception de soi des patients rendent également le diagnostic difficile, et (2) les patients sont souvent gênés d’être diagnostiqués et ne veulent pas visiter les cliniques pour le diagnostic. Finalement, peu de mesures objectives ou qualitatives sont disponibles pour aider les cliniciens à diagnostiquer ce trouble. Un exemple d’un tel entretien est le Clinician-Administered PTSD Scale (CAPS) ou encore le PCL-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">La récente pandémie de SRAS-CoV-2 peut être considérée comme un événement traumatique mondial qui a fait émerger: (1) un fort impact sur la santé mental, (2) la réalisation de nombreux soins médicaux transformée en distanciel. Par conséquent, considérant le biais des modes de collecte de données auto-déclaratifs et le changement radical induit par les consultations en distanciel, il est nécessaire de trouver un moyen plus objectif et rapide pour diagnostiquer le TSPT.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Contexte Théorique</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Nous distinguons deux catégories de modèles d’intelligence artificielle : les modèles de pronostic et les modèles de diagnostic du TSPT. Notre papier se concentre sur le diagnostic, c’est-à-dire la détection de l’état actuel des patients. Une grande majorité des études sur le diagnostic utilisent des techniques d’apprentissage automatique supervisé sur des données structurées. Par exemple, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> ont tous appliqué des algorithmes de Machine à Vecteurs de Support (SVM) sur des questionnaires. Toujours sur ces données auto-déclaratives et tabulaires, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> ont utilisé l’optimisation minimale séquentielle, le perceptron multicouches et la classification naïve bayésienne. Concernant les données biométriques, il semble que les modifications de la variabilité de la fréquence cardiaque sont significativement associées au TSPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Enfin, la conductance cutanée peut être utilisée comme outil de diagnostic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> car elle semble en particulier corrélée à son intensité.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">À l’image des données non structurées qui alimentent la prochaine génération des modèles d’IA, de nombreuses études de détection du TSPT ont profité de ces avancées pour puiser l’information dans différentes sources disponibles dans le cadre clinique ou quotidien des patients. Parmi les moyens d’acquisition et de restitution d’imagerie médicale, les études d’imagerie par résonance magnétique (IRM) structurelle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> et fonctionnelle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> ont permis des progrès considérables dans la compréhension des mécanismes neuronaux sous-jacents au TSPT. Ainsi couplées avec l’agorithme SVM, elles permettent de détecter le TSPT avec de bon résultats.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.10565/assets/PTSD-MDNN.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1196" height="393" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>L’architecture de PTSD-MDNN</figcaption>
</figure>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">D’autres études ont cherché des alternatives aux données médicales traditionnelles. Le diagnostic des patients atteints de TSPT par l’analyse des signaux vocaux a été étudié depuis ces dernières années <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Des approches de text mining ou de traitement du langage naturel ont été également utilisées <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Même si la télémédecine via vidéo conférence peut réduire le délai de la prise en charge et être aussi efficace que le traitement en personne <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, seul <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> se concentre sur ce type de données audiovisuelles en utilisant différentes architectures de réseaux de neurones par types de modalités de données. Actuellement, la question de la fusion de ces différentes modalités audio-vidéo pour la détection du TSPT reste entière.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Motivation</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Malgré la variété des études citées ci-dessus, le diagnostic du TSPT nécessite des capteurs ou dispositifs médicaux à caractère invasif dont la disponibilité n’est pas garantie, notamment en temps de crise. Nous proposons, dans ce papier, un réseau de neurones profond multimodal pour la détection automatique de TSPT (<code id="S1.SS3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN:</code> <code id="S1.SS3.p1.1.2" class="ltx_verbatim ltx_font_typewriter">Post Traumatic Stress Disorder</code> <code id="S1.SS3.p1.1.3" class="ltx_verbatim ltx_font_typewriter"> - Multimodal Deep Neural Network</code>) en utilisant comme entrées de simples vidéos et audios des patients en situation réelle. Nous montrons qu’en plus d’être adapté à ces données audiovisuelles facilement collectées, notre modèle obtient de meilleurs résultats en fusionnant ces deux modalités.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Méthode Proposée</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.4" class="ltx_p">Nous présentons un aperçu de <code id="S2.p1.4.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> dans la Fig <a href="#S1.F1" title="Figure 1 ‣ 1.2 Contexte Théorique ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Le modèle général se compose de deux sous-modèles qui prennent chacun en entrée une modalité différente. Le vecteur sortant de la dernière couche du classement vidéo <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="y_{v}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">y</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">𝑦</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">y_{v}</annotation></semantics></math> est concatené avec le vecteur sortant de la dernière couche du classement audio <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="y_{a}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">y</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝑦</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">y_{a}</annotation></semantics></math> pour former une matrice <math id="S2.p1.3.m3.2" class="ltx_Math" alttext="M_{v,a}" display="inline"><semantics id="S2.p1.3.m3.2a"><msub id="S2.p1.3.m3.2.3" xref="S2.p1.3.m3.2.3.cmml"><mi id="S2.p1.3.m3.2.3.2" xref="S2.p1.3.m3.2.3.2.cmml">M</mi><mrow id="S2.p1.3.m3.2.2.2.4" xref="S2.p1.3.m3.2.2.2.3.cmml"><mi id="S2.p1.3.m3.1.1.1.1" xref="S2.p1.3.m3.1.1.1.1.cmml">v</mi><mo id="S2.p1.3.m3.2.2.2.4.1" xref="S2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S2.p1.3.m3.2.2.2.2" xref="S2.p1.3.m3.2.2.2.2.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.2b"><apply id="S2.p1.3.m3.2.3.cmml" xref="S2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.2.3.1.cmml" xref="S2.p1.3.m3.2.3">subscript</csymbol><ci id="S2.p1.3.m3.2.3.2.cmml" xref="S2.p1.3.m3.2.3.2">𝑀</ci><list id="S2.p1.3.m3.2.2.2.3.cmml" xref="S2.p1.3.m3.2.2.2.4"><ci id="S2.p1.3.m3.1.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1.1">𝑣</ci><ci id="S2.p1.3.m3.2.2.2.2.cmml" xref="S2.p1.3.m3.2.2.2.2">𝑎</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.2c">M_{v,a}</annotation></semantics></math> . Après cette fusion tardive de modalités, la matrice <math id="S2.p1.4.m4.2" class="ltx_Math" alttext="M_{v,a}" display="inline"><semantics id="S2.p1.4.m4.2a"><msub id="S2.p1.4.m4.2.3" xref="S2.p1.4.m4.2.3.cmml"><mi id="S2.p1.4.m4.2.3.2" xref="S2.p1.4.m4.2.3.2.cmml">M</mi><mrow id="S2.p1.4.m4.2.2.2.4" xref="S2.p1.4.m4.2.2.2.3.cmml"><mi id="S2.p1.4.m4.1.1.1.1" xref="S2.p1.4.m4.1.1.1.1.cmml">v</mi><mo id="S2.p1.4.m4.2.2.2.4.1" xref="S2.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S2.p1.4.m4.2.2.2.2" xref="S2.p1.4.m4.2.2.2.2.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.2b"><apply id="S2.p1.4.m4.2.3.cmml" xref="S2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.2.3.1.cmml" xref="S2.p1.4.m4.2.3">subscript</csymbol><ci id="S2.p1.4.m4.2.3.2.cmml" xref="S2.p1.4.m4.2.3.2">𝑀</ci><list id="S2.p1.4.m4.2.2.2.3.cmml" xref="S2.p1.4.m4.2.2.2.4"><ci id="S2.p1.4.m4.1.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1.1">𝑣</ci><ci id="S2.p1.4.m4.2.2.2.2.cmml" xref="S2.p1.4.m4.2.2.2.2">𝑎</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.2c">M_{v,a}</annotation></semantics></math> est injectée dans un dernier réseau de neurones à deux couches afin de détecter le TSPT.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Classement Vidéo</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">Le sous-modèle de classification vidéo utilise un réseau de neurones convolutif (2+1)D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> avec des connexions résiduelles à 18 couches de profondeur (ResNet18). La convolution (2+1)D permet la décomposition des dimensions spatiale et temporelle, créant ainsi deux étapes distinctes. Un avantage de cette approche est que la factorisation des convolutions en dimensions spatiales et temporelles permet de réduire le nombre de paramètres par rapport à la convolution 3D complète. La convolution spatiale prend les données sous la forme <math id="S2.SS1.p1.1.m1.3" class="ltx_Math" alttext="(1,largeur,hauteur)" display="inline"><semantics id="S2.SS1.p1.1.m1.3a"><mrow id="S2.SS1.p1.1.m1.3.3.2" xref="S2.SS1.p1.1.m1.3.3.3.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.3.3.2.3" xref="S2.SS1.p1.1.m1.3.3.3.cmml">(</mo><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">1</mn><mo id="S2.SS1.p1.1.m1.3.3.2.4" xref="S2.SS1.p1.1.m1.3.3.3.cmml">,</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1a" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.4" xref="S2.SS1.p1.1.m1.2.2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1b" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.5" xref="S2.SS1.p1.1.m1.2.2.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1c" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.6" xref="S2.SS1.p1.1.m1.2.2.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1d" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.7" xref="S2.SS1.p1.1.m1.2.2.1.1.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.2.2.1.1.1e" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.8" xref="S2.SS1.p1.1.m1.2.2.1.1.8.cmml">r</mi></mrow><mo id="S2.SS1.p1.1.m1.3.3.2.5" xref="S2.SS1.p1.1.m1.3.3.3.cmml">,</mo><mrow id="S2.SS1.p1.1.m1.3.3.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.cmml"><mi id="S2.SS1.p1.1.m1.3.3.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.3" xref="S2.SS1.p1.1.m1.3.3.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1a" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.4" xref="S2.SS1.p1.1.m1.3.3.2.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1b" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.5" xref="S2.SS1.p1.1.m1.3.3.2.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1c" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.6" xref="S2.SS1.p1.1.m1.3.3.2.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1d" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.7" xref="S2.SS1.p1.1.m1.3.3.2.2.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.3.3.2.2.1e" xref="S2.SS1.p1.1.m1.3.3.2.2.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.3.3.2.2.8" xref="S2.SS1.p1.1.m1.3.3.2.2.8.cmml">r</mi></mrow><mo stretchy="false" id="S2.SS1.p1.1.m1.3.3.2.6" xref="S2.SS1.p1.1.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.3b"><vector id="S2.SS1.p1.1.m1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">1</cn><apply id="S2.SS1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1"><times id="S2.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1"></times><ci id="S2.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2">𝑙</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3">𝑎</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.4.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.4">𝑟</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.5.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5">𝑔</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.6.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.6">𝑒</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.7.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.7">𝑢</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.8.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.8">𝑟</ci></apply><apply id="S2.SS1.p1.1.m1.3.3.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2"><times id="S2.SS1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.1"></times><ci id="S2.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2">ℎ</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.3">𝑎</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.4.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.4">𝑢</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.5.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.5">𝑡</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.6.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.6">𝑒</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.7.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.7">𝑢</ci><ci id="S2.SS1.p1.1.m1.3.3.2.2.8.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.8">𝑟</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.3c">(1,largeur,hauteur)</annotation></semantics></math>, tandis que la convolution temporelle prend les données sous la forme <math id="S2.SS1.p1.2.m2.3" class="ltx_Math" alttext="(temps,1,1)" display="inline"><semantics id="S2.SS1.p1.2.m2.3a"><mrow id="S2.SS1.p1.2.m2.3.3.1" xref="S2.SS1.p1.2.m2.3.3.2.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.3.3.1.2" xref="S2.SS1.p1.2.m2.3.3.2.cmml">(</mo><mrow id="S2.SS1.p1.2.m2.3.3.1.1" xref="S2.SS1.p1.2.m2.3.3.1.1.cmml"><mi id="S2.SS1.p1.2.m2.3.3.1.1.2" xref="S2.SS1.p1.2.m2.3.3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.3" xref="S2.SS1.p1.2.m2.3.3.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1a" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.4" xref="S2.SS1.p1.2.m2.3.3.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1b" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.5" xref="S2.SS1.p1.2.m2.3.3.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.3.3.1.1.1c" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.3.3.1.1.6" xref="S2.SS1.p1.2.m2.3.3.1.1.6.cmml">s</mi></mrow><mo id="S2.SS1.p1.2.m2.3.3.1.3" xref="S2.SS1.p1.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">1</mn><mo id="S2.SS1.p1.2.m2.3.3.1.4" xref="S2.SS1.p1.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.p1.2.m2.3.3.1.5" xref="S2.SS1.p1.2.m2.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.3b"><vector id="S2.SS1.p1.2.m2.3.3.2.cmml" xref="S2.SS1.p1.2.m2.3.3.1"><apply id="S2.SS1.p1.2.m2.3.3.1.1.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1"><times id="S2.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.1"></times><ci id="S2.SS1.p1.2.m2.3.3.1.1.2.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.2">𝑡</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.3.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.3">𝑒</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.4.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.4">𝑚</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.5.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.5">𝑝</ci><ci id="S2.SS1.p1.2.m2.3.3.1.1.6.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.6">𝑠</ci></apply><cn type="integer" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">1</cn><cn type="integer" id="S2.SS1.p1.2.m2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2">1</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.3c">(temps,1,1)</annotation></semantics></math> comme illustré dans la Fig <a href="#S2.F2" title="Figure 2 ‣ 2.1 Classement Vidéo ‣ 2 Méthode Proposée" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Le redimensionnement de la vidéo est nécessaire pour: (1) effectuer un sous-échantillonnage des données, (2) examiner des parties spécifiques des images, (3) réduire la dimensionnalité pour un traitement plus rapide.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.10565/assets/2plus1CNN.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Les convolutions spatiales et temporelles factorisées d’une convolution (2+1)D avec une taille de noyau <math id="S2.F2.5.m1.1" class="ltx_Math" alttext="(3\times 3\times 3)" display="inline"><semantics id="S2.F2.5.m1.1b"><mrow id="S2.F2.5.m1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.5.m1.1.1.1.2" xref="S2.F2.5.m1.1.1.1.1.cmml">(</mo><mrow id="S2.F2.5.m1.1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.cmml"><mn id="S2.F2.5.m1.1.1.1.1.2" xref="S2.F2.5.m1.1.1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.5.m1.1.1.1.1.1" xref="S2.F2.5.m1.1.1.1.1.1.cmml">×</mo><mn id="S2.F2.5.m1.1.1.1.1.3" xref="S2.F2.5.m1.1.1.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.5.m1.1.1.1.1.1b" xref="S2.F2.5.m1.1.1.1.1.1.cmml">×</mo><mn id="S2.F2.5.m1.1.1.1.1.4" xref="S2.F2.5.m1.1.1.1.1.4.cmml">3</mn></mrow><mo stretchy="false" id="S2.F2.5.m1.1.1.1.3" xref="S2.F2.5.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.5.m1.1c"><apply id="S2.F2.5.m1.1.1.1.1.cmml" xref="S2.F2.5.m1.1.1.1"><times id="S2.F2.5.m1.1.1.1.1.1.cmml" xref="S2.F2.5.m1.1.1.1.1.1"></times><cn type="integer" id="S2.F2.5.m1.1.1.1.1.2.cmml" xref="S2.F2.5.m1.1.1.1.1.2">3</cn><cn type="integer" id="S2.F2.5.m1.1.1.1.1.3.cmml" xref="S2.F2.5.m1.1.1.1.1.3">3</cn><cn type="integer" id="S2.F2.5.m1.1.1.1.1.4.cmml" xref="S2.F2.5.m1.1.1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m1.1d">(3\times 3\times 3)</annotation></semantics></math> nécessitent des matrices de poids de taille <math id="S2.F2.6.m2.2" class="ltx_Math" alttext="(9\times canaux^{2})+(3\times canaux^{2})" display="inline"><semantics id="S2.F2.6.m2.2b"><mrow id="S2.F2.6.m2.2.2" xref="S2.F2.6.m2.2.2.cmml"><mrow id="S2.F2.6.m2.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.6.m2.1.1.1.1.2" xref="S2.F2.6.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.F2.6.m2.1.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.cmml"><mrow id="S2.F2.6.m2.1.1.1.1.1.2" xref="S2.F2.6.m2.1.1.1.1.1.2.cmml"><mn id="S2.F2.6.m2.1.1.1.1.1.2.2" xref="S2.F2.6.m2.1.1.1.1.1.2.2.cmml">9</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.6.m2.1.1.1.1.1.2.1" xref="S2.F2.6.m2.1.1.1.1.1.2.1.cmml">×</mo><mi id="S2.F2.6.m2.1.1.1.1.1.2.3" xref="S2.F2.6.m2.1.1.1.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.1.1.1.1.1.3" xref="S2.F2.6.m2.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1b" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.1.1.1.1.1.4" xref="S2.F2.6.m2.1.1.1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1c" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.1.1.1.1.1.5" xref="S2.F2.6.m2.1.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1d" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.1.1.1.1.1.6" xref="S2.F2.6.m2.1.1.1.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.1.1.1.1.1.1e" xref="S2.F2.6.m2.1.1.1.1.1.1.cmml">​</mo><msup id="S2.F2.6.m2.1.1.1.1.1.7" xref="S2.F2.6.m2.1.1.1.1.1.7.cmml"><mi id="S2.F2.6.m2.1.1.1.1.1.7.2" xref="S2.F2.6.m2.1.1.1.1.1.7.2.cmml">x</mi><mn id="S2.F2.6.m2.1.1.1.1.1.7.3" xref="S2.F2.6.m2.1.1.1.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.6.m2.1.1.1.1.3" xref="S2.F2.6.m2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.F2.6.m2.2.2.3" xref="S2.F2.6.m2.2.2.3.cmml">+</mo><mrow id="S2.F2.6.m2.2.2.2.1" xref="S2.F2.6.m2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.F2.6.m2.2.2.2.1.2" xref="S2.F2.6.m2.2.2.2.1.1.cmml">(</mo><mrow id="S2.F2.6.m2.2.2.2.1.1" xref="S2.F2.6.m2.2.2.2.1.1.cmml"><mrow id="S2.F2.6.m2.2.2.2.1.1.2" xref="S2.F2.6.m2.2.2.2.1.1.2.cmml"><mn id="S2.F2.6.m2.2.2.2.1.1.2.2" xref="S2.F2.6.m2.2.2.2.1.1.2.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.6.m2.2.2.2.1.1.2.1" xref="S2.F2.6.m2.2.2.2.1.1.2.1.cmml">×</mo><mi id="S2.F2.6.m2.2.2.2.1.1.2.3" xref="S2.F2.6.m2.2.2.2.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.2.2.2.1.1.3" xref="S2.F2.6.m2.2.2.2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1b" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.2.2.2.1.1.4" xref="S2.F2.6.m2.2.2.2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1c" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.2.2.2.1.1.5" xref="S2.F2.6.m2.2.2.2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1d" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">​</mo><mi id="S2.F2.6.m2.2.2.2.1.1.6" xref="S2.F2.6.m2.2.2.2.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.6.m2.2.2.2.1.1.1e" xref="S2.F2.6.m2.2.2.2.1.1.1.cmml">​</mo><msup id="S2.F2.6.m2.2.2.2.1.1.7" xref="S2.F2.6.m2.2.2.2.1.1.7.cmml"><mi id="S2.F2.6.m2.2.2.2.1.1.7.2" xref="S2.F2.6.m2.2.2.2.1.1.7.2.cmml">x</mi><mn id="S2.F2.6.m2.2.2.2.1.1.7.3" xref="S2.F2.6.m2.2.2.2.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.6.m2.2.2.2.1.3" xref="S2.F2.6.m2.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.6.m2.2c"><apply id="S2.F2.6.m2.2.2.cmml" xref="S2.F2.6.m2.2.2"><plus id="S2.F2.6.m2.2.2.3.cmml" xref="S2.F2.6.m2.2.2.3"></plus><apply id="S2.F2.6.m2.1.1.1.1.1.cmml" xref="S2.F2.6.m2.1.1.1.1"><times id="S2.F2.6.m2.1.1.1.1.1.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.1"></times><apply id="S2.F2.6.m2.1.1.1.1.1.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2"><times id="S2.F2.6.m2.1.1.1.1.1.2.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.1"></times><cn type="integer" id="S2.F2.6.m2.1.1.1.1.1.2.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.2">9</cn><ci id="S2.F2.6.m2.1.1.1.1.1.2.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.2.3">𝑐</ci></apply><ci id="S2.F2.6.m2.1.1.1.1.1.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.3">𝑎</ci><ci id="S2.F2.6.m2.1.1.1.1.1.4.cmml" xref="S2.F2.6.m2.1.1.1.1.1.4">𝑛</ci><ci id="S2.F2.6.m2.1.1.1.1.1.5.cmml" xref="S2.F2.6.m2.1.1.1.1.1.5">𝑎</ci><ci id="S2.F2.6.m2.1.1.1.1.1.6.cmml" xref="S2.F2.6.m2.1.1.1.1.1.6">𝑢</ci><apply id="S2.F2.6.m2.1.1.1.1.1.7.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7"><csymbol cd="ambiguous" id="S2.F2.6.m2.1.1.1.1.1.7.1.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7">superscript</csymbol><ci id="S2.F2.6.m2.1.1.1.1.1.7.2.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7.2">𝑥</ci><cn type="integer" id="S2.F2.6.m2.1.1.1.1.1.7.3.cmml" xref="S2.F2.6.m2.1.1.1.1.1.7.3">2</cn></apply></apply><apply id="S2.F2.6.m2.2.2.2.1.1.cmml" xref="S2.F2.6.m2.2.2.2.1"><times id="S2.F2.6.m2.2.2.2.1.1.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.1"></times><apply id="S2.F2.6.m2.2.2.2.1.1.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2"><times id="S2.F2.6.m2.2.2.2.1.1.2.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.1"></times><cn type="integer" id="S2.F2.6.m2.2.2.2.1.1.2.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.2">3</cn><ci id="S2.F2.6.m2.2.2.2.1.1.2.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.2.3">𝑐</ci></apply><ci id="S2.F2.6.m2.2.2.2.1.1.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.3">𝑎</ci><ci id="S2.F2.6.m2.2.2.2.1.1.4.cmml" xref="S2.F2.6.m2.2.2.2.1.1.4">𝑛</ci><ci id="S2.F2.6.m2.2.2.2.1.1.5.cmml" xref="S2.F2.6.m2.2.2.2.1.1.5">𝑎</ci><ci id="S2.F2.6.m2.2.2.2.1.1.6.cmml" xref="S2.F2.6.m2.2.2.2.1.1.6">𝑢</ci><apply id="S2.F2.6.m2.2.2.2.1.1.7.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7"><csymbol cd="ambiguous" id="S2.F2.6.m2.2.2.2.1.1.7.1.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7">superscript</csymbol><ci id="S2.F2.6.m2.2.2.2.1.1.7.2.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7.2">𝑥</ci><cn type="integer" id="S2.F2.6.m2.2.2.2.1.1.7.3.cmml" xref="S2.F2.6.m2.2.2.2.1.1.7.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m2.2d">(9\times canaux^{2})+(3\times canaux^{2})</annotation></semantics></math>. Ceci est moins de la moitié de celles nécessaires pour la convolution 3D complète <math id="S2.F2.7.m3.1" class="ltx_Math" alttext="(27\times canaux^{2})" display="inline"><semantics id="S2.F2.7.m3.1b"><mrow id="S2.F2.7.m3.1.1.1" xref="S2.F2.7.m3.1.1.1.1.cmml"><mo stretchy="false" id="S2.F2.7.m3.1.1.1.2" xref="S2.F2.7.m3.1.1.1.1.cmml">(</mo><mrow id="S2.F2.7.m3.1.1.1.1" xref="S2.F2.7.m3.1.1.1.1.cmml"><mrow id="S2.F2.7.m3.1.1.1.1.2" xref="S2.F2.7.m3.1.1.1.1.2.cmml"><mn id="S2.F2.7.m3.1.1.1.1.2.2" xref="S2.F2.7.m3.1.1.1.1.2.2.cmml">27</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F2.7.m3.1.1.1.1.2.1" xref="S2.F2.7.m3.1.1.1.1.2.1.cmml">×</mo><mi id="S2.F2.7.m3.1.1.1.1.2.3" xref="S2.F2.7.m3.1.1.1.1.2.3.cmml">c</mi></mrow><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1" xref="S2.F2.7.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.7.m3.1.1.1.1.3" xref="S2.F2.7.m3.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1b" xref="S2.F2.7.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.7.m3.1.1.1.1.4" xref="S2.F2.7.m3.1.1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1c" xref="S2.F2.7.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.7.m3.1.1.1.1.5" xref="S2.F2.7.m3.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1d" xref="S2.F2.7.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.F2.7.m3.1.1.1.1.6" xref="S2.F2.7.m3.1.1.1.1.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F2.7.m3.1.1.1.1.1e" xref="S2.F2.7.m3.1.1.1.1.1.cmml">​</mo><msup id="S2.F2.7.m3.1.1.1.1.7" xref="S2.F2.7.m3.1.1.1.1.7.cmml"><mi id="S2.F2.7.m3.1.1.1.1.7.2" xref="S2.F2.7.m3.1.1.1.1.7.2.cmml">x</mi><mn id="S2.F2.7.m3.1.1.1.1.7.3" xref="S2.F2.7.m3.1.1.1.1.7.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S2.F2.7.m3.1.1.1.3" xref="S2.F2.7.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.7.m3.1c"><apply id="S2.F2.7.m3.1.1.1.1.cmml" xref="S2.F2.7.m3.1.1.1"><times id="S2.F2.7.m3.1.1.1.1.1.cmml" xref="S2.F2.7.m3.1.1.1.1.1"></times><apply id="S2.F2.7.m3.1.1.1.1.2.cmml" xref="S2.F2.7.m3.1.1.1.1.2"><times id="S2.F2.7.m3.1.1.1.1.2.1.cmml" xref="S2.F2.7.m3.1.1.1.1.2.1"></times><cn type="integer" id="S2.F2.7.m3.1.1.1.1.2.2.cmml" xref="S2.F2.7.m3.1.1.1.1.2.2">27</cn><ci id="S2.F2.7.m3.1.1.1.1.2.3.cmml" xref="S2.F2.7.m3.1.1.1.1.2.3">𝑐</ci></apply><ci id="S2.F2.7.m3.1.1.1.1.3.cmml" xref="S2.F2.7.m3.1.1.1.1.3">𝑎</ci><ci id="S2.F2.7.m3.1.1.1.1.4.cmml" xref="S2.F2.7.m3.1.1.1.1.4">𝑛</ci><ci id="S2.F2.7.m3.1.1.1.1.5.cmml" xref="S2.F2.7.m3.1.1.1.1.5">𝑎</ci><ci id="S2.F2.7.m3.1.1.1.1.6.cmml" xref="S2.F2.7.m3.1.1.1.1.6">𝑢</ci><apply id="S2.F2.7.m3.1.1.1.1.7.cmml" xref="S2.F2.7.m3.1.1.1.1.7"><csymbol cd="ambiguous" id="S2.F2.7.m3.1.1.1.1.7.1.cmml" xref="S2.F2.7.m3.1.1.1.1.7">superscript</csymbol><ci id="S2.F2.7.m3.1.1.1.1.7.2.cmml" xref="S2.F2.7.m3.1.1.1.1.7.2">𝑥</ci><cn type="integer" id="S2.F2.7.m3.1.1.1.1.7.3.cmml" xref="S2.F2.7.m3.1.1.1.1.7.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.7.m3.1d">(27\times canaux^{2})</annotation></semantics></math> </figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Classement Audio</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Nous avons adapté le modèle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> basé sur une transformation de Fourier à court terme (TFCT) avec des fenêtres plus longues (64 ms) chevauchées à 75%, ce qui donne une meilleure résolution en fréquence pour la voix humaine. Nous avons ensuite converti les fréquences du spectrogramme en échelle logarithmique de Mel qui se rapproche de la perception humaine du son. Finalement, la transformée en cosinus discrète de type II (DCT) donne les coefficients Mel-Frequency Cepstral (MFCC) par 80 filtres triangulaires créés pour couvrir la plage de fréquences Mel. Nous sélectionnons uniquement 13 premiers qui sont utiles à la reconnaissance de la parole <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Le classement audio est entraîné à partir de ces MFCC en utilisant un réseau de neurones convolutif illustré dans la Fig <a href="#S1.F1" title="Figure 1 ‣ 1.2 Contexte Théorique ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Le modèle commence par deux couches de convolution 2D avec 16 filtres chacune, une taille de noyau de (3,3) et une fonction d’activation ReLu. Ces couches convolutives 2D sont utilisées pour extraire des caractéristiques importantes des MFCC d’entrée, qui sont des matrices de taille (778, 13, 1). Ensuite, les couches d’Aplatissement (Flatten) et d’Extinction (Dropout) s’enchainent pour respectivement convertir la sortie de la dernière couche convolutive en un vecteur à une dimension et pour désactiver aléatoirement certains neurones afin de contourner le surapprentissage. Deux couches entièrement connectées sont ensuite combinées via une fonction Sigmoïd, ce qui permet d’obtenir les probabilités des classes à prédire.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion Tardive Des Modalités</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.2" class="ltx_p">Nous avons choisi pour notre modèle une fusion tardive, dite &lt;&lt; fusion orientée décisions &gt;&gt;. En effet, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> a montré que : (1) un meilleur réseau unimodal peut surpasser un réseau multimodal contre le problème de surapprentissage; (2) différentes modalités se surajustent et se généralisent à des rythmes différents, donc les entraîner conjointement avec une seule stratégie d’optimisation n’est pas optimal. Nous avons imaginé un mécanisme de &lt;&lt; correction d’erreur &gt;&gt; qui fusionne des prédictions provenant de deux réseaux unimodaux qui sont entrainés séparément. La fonction de perte que nous avons utilisé pour ce réseau de fusion tardive est la même que pour les sous-modèles: Perte d’entropie croisée binaire définie comme la formule <a href="#S2.E1" title="In 2.3 Fusion Tardive Des Modalités ‣ 2 Méthode Proposée" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Cette formule suppose que les <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><msub id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝑝</ci><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">p_{i}</annotation></semantics></math> sont des probabilités et les <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><msub id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">y</mi><mi id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">𝑦</ci><ci id="S2.SS3.p1.2.m2.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">y_{i}</annotation></semantics></math> sont des labels (0;1).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="L=-\frac{1}{N}\sum_{i=1}^{2}y_{i}log(p_{i})=-\frac{1}{N}[y_{1}log(p_{1})+y_{2}log(p_{2})]" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.4" xref="S2.E1.m1.2.2.4.cmml">L</mi><mo id="S2.E1.m1.2.2.5" xref="S2.E1.m1.2.2.5.cmml">=</mo><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1a" xref="S2.E1.m1.1.1.1.cmml">−</mo><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml">1</mn><mi id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><munderover id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E1.m1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2.2.3.2" xref="S2.E1.m1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E1.m1.1.1.1.1.1.2.2.3.1" xref="S2.E1.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.1.1.1.2.2.3.3" xref="S2.E1.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S2.E1.m1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.2.3.cmml">2</mn></munderover><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2a" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.1.1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2b" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.1.1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2c" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.6" xref="S2.E1.m1.2.2.6.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2a" xref="S2.E1.m1.2.2.2.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mfrac id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mn id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">1</mn><mi id="S2.E1.m1.2.2.2.1.3.3" xref="S2.E1.m1.2.2.2.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.3.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.2.cmml">y</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.1.3.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.4" xref="S2.E1.m1.2.2.2.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2a" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.5" xref="S2.E1.m1.2.2.2.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2b" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.6" xref="S2.E1.m1.2.2.2.1.1.1.1.1.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.1.2c" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml">p</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">+</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml">y</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.2.3.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.4" xref="S2.E1.m1.2.2.2.1.1.1.1.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2a" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.5" xref="S2.E1.m1.2.2.2.1.1.1.1.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2b" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">​</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.6" xref="S2.E1.m1.2.2.2.1.1.1.1.2.6.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.1.1.1.2.2c" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2.cmml">p</mi><mn id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><and id="S2.E1.m1.2.2a.cmml" xref="S2.E1.m1.2.2"></and><apply id="S2.E1.m1.2.2b.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.5.cmml" xref="S2.E1.m1.2.2.5"></eq><ci id="S2.E1.m1.2.2.4.cmml" xref="S2.E1.m1.2.2.4">𝐿</ci><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><minus id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1"></minus><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3"></divide><cn type="integer" id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2">1</cn><ci id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><apply id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E1.m1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.2"></sum><apply id="S2.E1.m1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3"><eq id="S2.E1.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S2.E1.m1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.2.3">2</cn></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.4">𝑙</ci><ci id="S2.E1.m1.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.1.1.5">𝑜</ci><ci id="S2.E1.m1.1.1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.1.1.6">𝑔</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply><apply id="S2.E1.m1.2.2c.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.6.cmml" xref="S2.E1.m1.2.2.6"></eq><share href="#S2.E1.m1.1.1.1.cmml" id="S2.E1.m1.2.2d.cmml" xref="S2.E1.m1.2.2"></share><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><divide id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3"></divide><cn type="integer" id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">1</cn><ci id="S2.E1.m1.2.2.2.1.3.3.cmml" xref="S2.E1.m1.2.2.2.1.3.3">𝑁</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1"><plus id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3"></plus><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"><times id="S2.E1.m1.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.2">𝑦</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.3.3">1</cn></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.4">𝑙</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.5.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.5">𝑜</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.6.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.6">𝑔</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.2">𝑝</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><times id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"></times><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.2">𝑦</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.3">2</cn></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.4.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.4">𝑙</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.5.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.5">𝑜</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.6.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.6">𝑔</ci><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.2">𝑝</ci><cn type="integer" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">L=-\frac{1}{N}\sum_{i=1}^{2}y_{i}log(p_{i})=-\frac{1}{N}[y_{1}log(p_{1})+y_{2}log(p_{2})]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Évaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Base De Données</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">En général, il est difficile de collecter des données de haute qualité auprès de personnes qui présentent des symptômes de TSPT. Il peut y avoir aussi des considérations éthiques qui limitent la collecte et l’utilisation de données en milieu naturel. Cela peut être particulièrement difficile dans le contexte d’un trouble sensible comme le TSPT, où les participants peuvent être réticents à divulguer des informations personnelles.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Par conséquent, seules quatre bases de données non structurées pour la détection du TSPT existent: eDAIC-WOZ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, FEMH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Aurora <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> et PTSD in-the-wild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Parmi elles, seules les bases eDAIC-WOZ et PTSD in-the-wild disposent à la fois de modalités audio et vidéo. Nous avons choisi d’appliquer notre modèle <code id="S3.SS1.p2.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> sur la base PTSD in-the-wild pour son caractère réel en milieu naturel. La base de données PTSD in-the-wild (EULA) contient 634 vidéos équilibrées : 317 vidéos de sujets avec TSPT et 317 vidéos de sujets témoins sains avec aucun symptôme de TSPT.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Résultats</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Nous nous intéressons à l’évaluation d’une classification binaire avec deux classes : TSPT (positif) et Non-TSPT (négatif) avec différentes métriques de classification populaires : l’accuracy, la précision, le rappel. Nous avons suivi le processus train/validation/test proposé par <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (80%/10%/10%) pour entraîner (<code id="S3.SS2.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code>) sur une carte GPU NVIDIA A100 SXM 40Go avec taille de batch de 8, un taux d’apprentissage de 0.001, et un optimiseur Adam pour 50 époques. Ces paramètres sont optimaux pour éviter le surapprentissage lié à la taille des données. De plus, nous avons utilisé différentes méthodes de régularisation pour le classement audio (Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Résultats ‣ 3 Évaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Résultats du classement sur les données test</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Modalité</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Régularisation</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Accuracy</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Précision</span></th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Rappel</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Vidéo</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">0,89</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">0,84</span></td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">0,84</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">0,72</span></td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">0,68</span></td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">0,81</span></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">0,73</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.4.1" class="ltx_text" style="font-size:90%;">0,67</span></td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.4.3.5.1" class="ltx_text" style="font-size:90%;">0,90</span></td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">L2</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">0,75</span></td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.4.1" class="ltx_text" style="font-size:90%;">0,72</span></td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">0,81</span></td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Vidéo + Audio</span></th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">L1</span></td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">0,89</span></td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">0,90</span></td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">0,87</span></td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Vidéo + Audio</span></th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">L2</span></td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">0,92</span></td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">0,88</span></td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">0,97</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discussion</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Le meilleur des modèles unimodaux est le classement vidéo avec une accuracy de 0,89. En revanche, le classement audio de base ne donne pas de très bons résultats même si les régularisations L1 et L2 l’améliorent respectivement à 0,73 et 0,75. Notre approche de fusion tardive des modalités apporte de réelles améliorations par rapport aux classements unimodaux car <code id="S3.SS3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> donne la meilleure accuracy (0,92), avec une régularisation L2, et le meilleur rappel (0,97). Le principal avantage d’une fusion tardive est la prise en charge des différentes modalités non alignées ainsi nous ne dépendons pas de l’interopérabilité des capteurs. De plus, l’entrainement indépendant des deux sous-modèles permet de gagner du temps en effectuant des tâches en parallèle. Enfin, cette fusion permet d’apporter la flexibilité pour le choix des sous-modèles adaptés à chaque modalité. Il est à noter que la différence de taille des fichiers de la base PTSD in-the-wild (le plus court : 0 min 35 s et le plus long : 44 min 40 s) peut créer des difficultés pour l’extraction des variables en entrée du modèle de convolution.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Nous proposons <code id="S4.p1.1.1" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code>, un modèle qui fusionne tardivement des modalités audio et vidéo pour détecter le TSPT. Grâce à un mécanisme de correction d’erreur, notre modèle surpasse les modèles unimodaux. En plus d’être non invasif, <code id="S4.p1.1.2" class="ltx_verbatim ltx_font_typewriter">PTSD-MDNN</code> traite les informations sensibles sur les patients à très bas niveau (pixel, MFCC), ce qui permet de garder une certaine confidentialité pour les patients par rapport aux approches type NLP où les paroles sont transcrites.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Ce travail ouvre une multitude de travaux futurs. Premièrement, nous avons l’intention d’extraire des variables de haut niveau à partir d’aspects comportementaux subtils, tels que les mouvements du corps, les expressions faciales pour la vision ainsi que la prosodie et la parole pour l’audio. Deuxièment, d’autres directions concernent la fusion des modalités à travers le mécanisme de l’attention inter-modalité.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">American Psychiatric Association</span>,
Marc-Antoine <span id="bib.bib1.2.2" class="ltx_text ltx_font_smallcaps">Crocq</span>, Julien-Daniel
<span id="bib.bib1.3.3" class="ltx_text ltx_font_smallcaps">Guelfi</span>, Patrice
<span id="bib.bib1.4.4" class="ltx_text ltx_font_smallcaps">Boyer</span>, Marie-Claire
<span id="bib.bib1.5.5" class="ltx_text ltx_font_smallcaps">Pull</span> et Charles-Bernard
<span id="bib.bib1.6.6" class="ltx_text ltx_font_smallcaps">Pull</span> :

</span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text ltx_font_italic">DSM-5 - Manuel diagnostique et statistique des troubles
mentaux</span>.

</span>
<span class="ltx_bibblock">Elsevier Masson, juin 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Debrup <span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">Banerjee</span>, Kazi
<span id="bib.bib2.2.2" class="ltx_text ltx_font_smallcaps">Islam</span>, Keyi
<span id="bib.bib2.3.3" class="ltx_text ltx_font_smallcaps">Xue</span>, Gang
<span id="bib.bib2.4.4" class="ltx_text ltx_font_smallcaps">Mei</span>, Lemin
<span id="bib.bib2.5.5" class="ltx_text ltx_font_smallcaps">Xiao</span>, Guangfan
<span id="bib.bib2.6.6" class="ltx_text ltx_font_smallcaps">Zhang</span>, Roger
<span id="bib.bib2.7.7" class="ltx_text ltx_font_smallcaps">Xu</span>, Cai
<span id="bib.bib2.8.8" class="ltx_text ltx_font_smallcaps">Lei</span>, Shuiwang
<span id="bib.bib2.9.9" class="ltx_text ltx_font_smallcaps">Ji</span> et Jiang
<span id="bib.bib2.10.10" class="ltx_text ltx_font_smallcaps">Li</span> :

</span>
<span class="ltx_bibblock">A deep transfer learning approach for improved post-traumatic stress
disorder diagnosis.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.11.1" class="ltx_text ltx_font_italic">Knowl Inf Syst</span>, 60(3):1693–1724,
septembre 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jeroen <span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">Breebaart</span> et Martin F.
<span id="bib.bib3.2.2" class="ltx_text ltx_font_smallcaps">McKinney</span> :

</span>
<span class="ltx_bibblock">Features for Audio Classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">In</em> Wim F. J. <span id="bib.bib3.4.2" class="ltx_text ltx_font_smallcaps">Verhaegh</span>, Emile <span id="bib.bib3.5.3" class="ltx_text ltx_font_smallcaps">Aarts</span> et Jan <span id="bib.bib3.6.4" class="ltx_text ltx_font_smallcaps">Korst</span>, éditeurs : <span id="bib.bib3.7.5" class="ltx_text ltx_font_italic">Algorithms in Ambient
Intelligence</span>, Philips Research, pages 113–129. Springer Netherlands,
Dordrecht, 2004.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Michael S. <span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">Breen</span>, Kevin G.F.
<span id="bib.bib4.2.2" class="ltx_text ltx_font_smallcaps">Thomas</span>, David S.
<span id="bib.bib4.3.3" class="ltx_text ltx_font_smallcaps">Baldwin</span> et Gosia
<span id="bib.bib4.4.4" class="ltx_text ltx_font_smallcaps">Lipinska</span> :

</span>
<span class="ltx_bibblock">Modelling PTSD diagnosis using sleep, memory, and adrenergic
metabolites: An exploratory machine-learning study.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text ltx_font_italic">Human Psychopharmacology: Clinical and Experimental</span>,
34(2):e2691, 2019.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hup.2691.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jonathan <span id="bib.bib5.1.1" class="ltx_text ltx_font_smallcaps">Gratch</span>, Ron
<span id="bib.bib5.2.2" class="ltx_text ltx_font_smallcaps">Artstein</span>, Gale
<span id="bib.bib5.3.3" class="ltx_text ltx_font_smallcaps">Lucas</span>, Giota
<span id="bib.bib5.4.4" class="ltx_text ltx_font_smallcaps">Stratou</span>, Stefan
<span id="bib.bib5.5.5" class="ltx_text ltx_font_smallcaps">Scherer</span>, Angela
<span id="bib.bib5.6.6" class="ltx_text ltx_font_smallcaps">Nazarian</span>, Rachel
<span id="bib.bib5.7.7" class="ltx_text ltx_font_smallcaps">Wood</span>, Jill
<span id="bib.bib5.8.8" class="ltx_text ltx_font_smallcaps">Boberg</span>, David
<span id="bib.bib5.9.9" class="ltx_text ltx_font_smallcaps">DeVault</span>, Stacy
<span id="bib.bib5.10.10" class="ltx_text ltx_font_smallcaps">Marsella</span>, David
<span id="bib.bib5.11.11" class="ltx_text ltx_font_smallcaps">Traum</span>, Skip
<span id="bib.bib5.12.12" class="ltx_text ltx_font_smallcaps">Rizzo</span> et Louis-Philippe
<span id="bib.bib5.13.13" class="ltx_text ltx_font_smallcaps">Morency</span> :

</span>
<span class="ltx_bibblock">The Distress Analysis Interview Corpus of human and computer
interviews.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.14.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib5.15.2" class="ltx_text ltx_font_italic">Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LREC’14)</span>,
Reykjavik, Iceland, mai 2014. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Marit <span id="bib.bib6.1.1" class="ltx_text ltx_font_smallcaps">Hauschildt</span>, Maarten J. V.
<span id="bib.bib6.2.2" class="ltx_text ltx_font_smallcaps">Peters</span>, Steffen
<span id="bib.bib6.3.3" class="ltx_text ltx_font_smallcaps">Moritz</span> et Lena
<span id="bib.bib6.4.4" class="ltx_text ltx_font_smallcaps">Jelinek</span> :

</span>
<span class="ltx_bibblock">Heart rate variability in response to affective scenes in
posttraumatic stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text ltx_font_italic">Biological Psychology</span>, 88(2):215–222,
décembre 2011.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Rebecca <span id="bib.bib7.1.1" class="ltx_text ltx_font_smallcaps">Hinrichs</span>, Vasiliki
<span id="bib.bib7.2.2" class="ltx_text ltx_font_smallcaps">Michopoulos</span>, Sterling
<span id="bib.bib7.3.3" class="ltx_text ltx_font_smallcaps">Winters</span>, Alex O.
<span id="bib.bib7.4.4" class="ltx_text ltx_font_smallcaps">Rothbaum</span>, Barbara O.
<span id="bib.bib7.5.5" class="ltx_text ltx_font_smallcaps">Rothbaum</span>, Kerry J.
<span id="bib.bib7.6.6" class="ltx_text ltx_font_smallcaps">Ressler</span> et Tanja
<span id="bib.bib7.7.7" class="ltx_text ltx_font_smallcaps">Jovanovic</span> :

</span>
<span class="ltx_bibblock">Mobile assessment of heightened skin conductance in posttraumatic
stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text ltx_font_italic">Depression and Anxiety</span>, 34(6):502–507, 2017.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/da.22610.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kazi Aminul <span id="bib.bib8.1.1" class="ltx_text ltx_font_smallcaps">Islam</span>, Daniel
<span id="bib.bib8.2.2" class="ltx_text ltx_font_smallcaps">Perez</span> et Jiang
<span id="bib.bib8.3.3" class="ltx_text ltx_font_smallcaps">Li</span> :

</span>
<span class="ltx_bibblock">A Transfer Learning Approach for the 2018 FEMH Voice Data
Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.4.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib8.5.2" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Big
Data (Big Data)</span>, pages 5252–5257, décembre 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Stefan <span id="bib.bib9.1.1" class="ltx_text ltx_font_smallcaps">Kahl</span>, Amanda
<span id="bib.bib9.2.2" class="ltx_text ltx_font_smallcaps">Navine</span>, Tom
<span id="bib.bib9.3.3" class="ltx_text ltx_font_smallcaps">Denton</span>, Holger
<span id="bib.bib9.4.4" class="ltx_text ltx_font_smallcaps">Klinck</span>, Patrick
<span id="bib.bib9.5.5" class="ltx_text ltx_font_smallcaps">Hart</span>, Hervé
<span id="bib.bib9.6.6" class="ltx_text ltx_font_smallcaps">Glotin</span>, Hervé
<span id="bib.bib9.7.7" class="ltx_text ltx_font_smallcaps">Goëau</span>, Willem-Pier
<span id="bib.bib9.8.8" class="ltx_text ltx_font_smallcaps">Vellinga</span>, Robert
<span id="bib.bib9.9.9" class="ltx_text ltx_font_smallcaps">Planqué</span> et Alexis
<span id="bib.bib9.10.10" class="ltx_text ltx_font_smallcaps">Joly</span> :

</span>
<span class="ltx_bibblock">Overview of BirdCLEF 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.11.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib9.12.2" class="ltx_text ltx_font_italic">Proceedings of the Working Notes of CLEF 2022 -
Conference and Labs of the Evaluation Forum</span>, Bologna, Italy,
septembre 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jan A. <span id="bib.bib10.1.1" class="ltx_text ltx_font_smallcaps">Lindsay</span>, Michael R.
<span id="bib.bib10.2.2" class="ltx_text ltx_font_smallcaps">Kauth</span>, Sonora
<span id="bib.bib10.3.3" class="ltx_text ltx_font_smallcaps">Hudson</span>, Lindsey A.
<span id="bib.bib10.4.4" class="ltx_text ltx_font_smallcaps">Martin</span>, David J.
<span id="bib.bib10.5.5" class="ltx_text ltx_font_smallcaps">Ramsey</span>, Lawrence
<span id="bib.bib10.6.6" class="ltx_text ltx_font_smallcaps">Daily</span> et John
<span id="bib.bib10.7.7" class="ltx_text ltx_font_smallcaps">Rader</span> :

</span>
<span class="ltx_bibblock">Implementation of Video Telehealth to Improve Access to
Evidence-Based Psychotherapy for Posttraumatic Stress Disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text ltx_font_italic">Telemedicine and e-Health</span>, 21(6):467–472, juin 2015.

</span>
<span class="ltx_bibblock">Publisher: Mary Ann Liebert, Inc., publishers.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Sevinç İlhan <span id="bib.bib11.1.1" class="ltx_text ltx_font_smallcaps">Omurca</span> et Ekin
<span id="bib.bib11.2.2" class="ltx_text ltx_font_smallcaps">Ekinci</span> :

</span>
<span class="ltx_bibblock">An alternative evaluation of post traumatic stress disorder with
machine learning methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib11.4.2" class="ltx_text ltx_font_italic">2015 International Symposium on Innovations in
Intelligent SysTems and Applications (INISTA)</span>, pages 1–7, septembre
2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Dhanesh <span id="bib.bib12.1.1" class="ltx_text ltx_font_smallcaps">Ramachandram</span> et Graham W.
<span id="bib.bib12.2.2" class="ltx_text ltx_font_smallcaps">Taylor</span> :

</span>
<span class="ltx_bibblock">Deep Multimodal Learning: A Survey on Recent Advances and
Trends.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 34(6):96–108, novembre 2017.

</span>
<span class="ltx_bibblock">Conference Name: IEEE Signal Processing Magazine.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. <span id="bib.bib13.1.1" class="ltx_text ltx_font_smallcaps">Rangaprakash</span>, Gopikrishna
<span id="bib.bib13.2.2" class="ltx_text ltx_font_smallcaps">Deshpande</span>, Thomas A.
<span id="bib.bib13.3.3" class="ltx_text ltx_font_smallcaps">Daniel</span>, Adam M.
<span id="bib.bib13.4.4" class="ltx_text ltx_font_smallcaps">Goodman</span>, Jennifer L.
<span id="bib.bib13.5.5" class="ltx_text ltx_font_smallcaps">Robinson</span>, Nouha
<span id="bib.bib13.6.6" class="ltx_text ltx_font_smallcaps">Salibi</span>, Jeffrey S.
<span id="bib.bib13.7.7" class="ltx_text ltx_font_smallcaps">Katz</span>, Thomas S.
<span id="bib.bib13.8.8" class="ltx_text ltx_font_smallcaps">Denney Jr.</span> et Michael N.
<span id="bib.bib13.9.9" class="ltx_text ltx_font_smallcaps">Dretsch</span> :

</span>
<span class="ltx_bibblock">Compromised hippocampus-striatum pathway as a potential imaging
biomarker of mild-traumatic brain injury and posttraumatic stress disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.10.1" class="ltx_text ltx_font_italic">Human Brain Mapping</span>, 38(6):2843–2864,
2017.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.23551.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Niels <span id="bib.bib14.1.1" class="ltx_text ltx_font_smallcaps">Rathlev</span> :

</span>
<span class="ltx_bibblock">Correction: The AURORA Study: a longitudinal, multimodal
library of brain biology and function after traumatic stress exposure.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text ltx_font_italic">All Scholarly Works</span>, septembre 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lauren E. <span id="bib.bib15.1.1" class="ltx_text ltx_font_smallcaps">Salminen</span>, Rajendra A.
<span id="bib.bib15.2.2" class="ltx_text ltx_font_smallcaps">Morey</span>, Brandalyn C.
<span id="bib.bib15.3.3" class="ltx_text ltx_font_smallcaps">Riedel</span>, Neda
<span id="bib.bib15.4.4" class="ltx_text ltx_font_smallcaps">Jahanshad</span>, Emily L.
<span id="bib.bib15.5.5" class="ltx_text ltx_font_smallcaps">Dennis</span> et Paul M.
<span id="bib.bib15.6.6" class="ltx_text ltx_font_smallcaps">Thompson</span> :

</span>
<span class="ltx_bibblock">Adaptive Identification of Cortical and Subcortical Imaging
Markers of Early Life Stress and Posttraumatic Stress Disorder.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text ltx_font_italic">Journal of Neuroimaging</span>, 29(3):335–343, 2019.

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jon.12600.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Moctar Abdoul Latif <span id="bib.bib16.1.1" class="ltx_text ltx_font_smallcaps">Sawadogo</span>,
Furkan <span id="bib.bib16.2.2" class="ltx_text ltx_font_smallcaps">Pala</span>, Gurkirat
<span id="bib.bib16.3.3" class="ltx_text ltx_font_smallcaps">Singh</span>, Imen
<span id="bib.bib16.4.4" class="ltx_text ltx_font_smallcaps">Selmi</span>, Pauline
<span id="bib.bib16.5.5" class="ltx_text ltx_font_smallcaps">Puteaux</span> et Alice
<span id="bib.bib16.6.6" class="ltx_text ltx_font_smallcaps">Othmani</span> :

</span>
<span class="ltx_bibblock">PTSD in the Wild: A Video Database for Studying
Post-Traumatic Stress Disorder Recognition in Unconstrained
Environments, septembre 2022.

</span>
<span class="ltx_bibblock">arXiv:2209.14085 [cs].

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jeff <span id="bib.bib17.1.1" class="ltx_text ltx_font_smallcaps">Sawalha</span>, Muhammad
<span id="bib.bib17.2.2" class="ltx_text ltx_font_smallcaps">Yousefnezhad</span>, Zehra
<span id="bib.bib17.3.3" class="ltx_text ltx_font_smallcaps">Shah</span>, Matthew R. G.
<span id="bib.bib17.4.4" class="ltx_text ltx_font_smallcaps">Brown</span>, Andrew J.
<span id="bib.bib17.5.5" class="ltx_text ltx_font_smallcaps">Greenshaw</span> et Russell
<span id="bib.bib17.6.6" class="ltx_text ltx_font_smallcaps">Greiner</span> :

</span>
<span class="ltx_bibblock">Detecting Presence of PTSD Using Sentiment Analysis From
Text Data.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text ltx_font_italic">Frontiers in Psychiatry</span>, 12, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Chappidi <span id="bib.bib18.1.1" class="ltx_text ltx_font_smallcaps">Suneetha</span> et Raju
<span id="bib.bib18.2.2" class="ltx_text ltx_font_smallcaps">Anitha</span> :

</span>
<span class="ltx_bibblock">A Survey Of Machine Learning Techniques OnSpeech Based
Emotion Recognition And Post Traumatic Stress
DisorderDetection.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">nq</span>, 20(14):1–11, décembre 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Du <span id="bib.bib19.1.1" class="ltx_text ltx_font_smallcaps">Tran</span>, Heng
<span id="bib.bib19.2.2" class="ltx_text ltx_font_smallcaps">Wang</span>, Lorenzo
<span id="bib.bib19.3.3" class="ltx_text ltx_font_smallcaps">Torresani</span>, Jamie
<span id="bib.bib19.4.4" class="ltx_text ltx_font_smallcaps">Ray</span>, Yann
<span id="bib.bib19.5.5" class="ltx_text ltx_font_smallcaps">LeCun</span> et Manohar
<span id="bib.bib19.6.6" class="ltx_text ltx_font_smallcaps">Paluri</span> :

</span>
<span class="ltx_bibblock">A Closer Look at Spatiotemporal Convolutions for Action
Recognition, avril 2018.

</span>
<span class="ltx_bibblock">arXiv:1711.11248 [cs].

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Weiyao <span id="bib.bib20.1.1" class="ltx_text ltx_font_smallcaps">Wang</span>,
Du <span id="bib.bib20.2.2" class="ltx_text ltx_font_smallcaps">Tran</span> et Matt
<span id="bib.bib20.3.3" class="ltx_text ltx_font_smallcaps">Feiszli</span> :

</span>
<span class="ltx_bibblock">What Makes Training Multi-Modal Classification Networks
Hard?

</span>
<span class="ltx_bibblock"><em id="bib.bib20.4.1" class="ltx_emph ltx_font_italic">In</em> <span id="bib.bib20.5.2" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</span>, pages 12695–12705, 2020.

</span>
</li>
</ul>
</section>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.10564" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.10565" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.10565">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.10565" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.10566" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:01:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
