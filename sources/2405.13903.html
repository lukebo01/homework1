<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13903] ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos</title><meta property="og:description" content="Emotion recognition is relevant for human behaviour understanding, where facial expression and speech recognition have been widely explored by the computer vision community.
Literature in the field of behavioural psych…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13903">

<!--Generated on Wed Jun  5 13:33:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">ST-Gait++: Leveraging spatio-temporal convolutions for
<br class="ltx_break">gait-based emotion recognition on videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Luísa Lima
<br class="ltx_break">Voxar Labs, Centro de Informática
<br class="ltx_break">Universidade Federal de Pernambuco
<br class="ltx_break">Av. Jorn. Aníbal Fernandes, Recife, Brazil
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mlll@cin.ufpe.br</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Willams de Lima Costa
<br class="ltx_break">Voxar Labs, Centro de Informática
<br class="ltx_break">Universidade Federal de Pernambuco
<br class="ltx_break">Av. Jorn. Aníbal Fernandes, Recife, Brazil
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">wlc2@cin.ufpe.br</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Estefania Talavera Martínez
<br class="ltx_break">University of Twente
<br class="ltx_break">Drienerlolaan 5, 7522 NB
<br class="ltx_break">Enschede, Netherlands
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">e.talaveramartinez@utwente.nl</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Veronica Teichrieb
<br class="ltx_break">Voxar Labs, Centro de Informática
<br class="ltx_break">Universidade Federal de Pernambuco
<br class="ltx_break">Av. Jorn. Aníbal Fernandes, Recife, Brazil
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">vt@cin.ufpe.br</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Emotion recognition is relevant for human behaviour understanding, where facial expression and speech recognition have been widely explored by the computer vision community.
Literature in the field of behavioural psychology indicates that gait, described as the way a person walks, is an additional indicator of emotions.
In this work, we propose a deep framework for emotion recognition through the analysis of gait. More specifically, our model is composed of a sequence of spatial-temporal Graph Convolutional Networks that produce a robust skeleton-based representation for the task of emotion classification. We evaluate our proposed framework on the E-Gait dataset, composed of a total of 2177 samples.
The results obtained represent an improvement of <math id="id1.1.m1.1" class="ltx_Math" alttext="\approx 5\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">≈</mo><mrow id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml"><mn id="id1.1.m1.1.1.3.2" xref="id1.1.m1.1.1.3.2.cmml">5</mn><mo id="id1.1.m1.1.1.3.1" xref="id1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><approx id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><apply id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"><csymbol cd="latexml" id="id1.1.m1.1.1.3.1.cmml" xref="id1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="id1.1.m1.1.1.3.2.cmml" xref="id1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\approx 5\%</annotation></semantics></math> in accuracy compared to the state of the art. In addition, during training we observed a faster convergence of our model compared to the state-of-the-art methodologies.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Humans have a natural perception capability that allows us to capture, process, and understand behavioral cues from other people naturally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. There are several biological triggers inside our brains that plan our interactions with other people based on this perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. However, human behavior is a very broad cognitive spectrum with multiple different nuances that can affect this planning procedure. When looking at social interactions, however, emotion is a specific part of behavior that plays a significant role. The ability to perceive and respond to emotional aspects is essential to develop and maintain links with peers in society.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In multiple contexts of applications, we could argue that understanding the emotions of users is also a significant aspect of the development of systems that are more inclusive and fair. These systems could adapt the way they perform according to how they perceive their users. However, to allow these systems to understand the emotions of their user, first, they need to be able to extract and process emotional information from them. Researchers have been studying how humans perceive and process these affective cues for a while, and evidence from the behavioral psychology literature suggests that a significant part of affective information is communicated naturally and intuitively through a medium known as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">nonverbal communication</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Among the vast list of nonverbal communication cues, some studies highlight the importance of bodily expression for emotion recognition. Early studies such as those by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wallbott and Scherer</span> [<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> suggested that there are specific body movements that lead to an accurate perception of emotions. Therefore, this strong evidence from the literature on behavioral psychology validates that the body acts as an outlet for the person’s emotional state, as well as signals that, by extracting and processing these cues, someone can perceive emotional aspects through observation of certain characteristics. Recent studies have shown significant success in encoding body language to recognize affect in humans through deep learning and computer vision, indicating that body expressions are a significant cue when building affect-aware technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, body language, as well as other affective features such as facial expressions, gaze, gestures, or physiological indicators such as cardiac frequency or respiratory rate, share a common limitation related to applications: they require the user to be facing toward a camera so that the affective sources (e.g., face, eyes, arms…) are visible at all times. Although this constraint is not harmful in some scenarios, such as when people are facing a computer or other affective agents (such as social robots), it is also a limiting aspect in ubiquitous applications since the user would not be able to freely interact with the environment. We could, however, communicate to the user this constraint and ask the user to face the system; however, this would alter the user’s behavior, changing how they would communicate their emotions and removing all naturality from this interaction, which would now be an artificial interaction.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A possible way to encode body language and still not limit the user is to look at the person’s gait to extract affective information. Gait is the description of the way that someone walks, and previous research in behavioral psychology has found that humans are able to identify multiple social aspects through gait-related parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Roether et al.</span> [<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> applied machine learning to a set of recorded gaits and found that some features, such as movement speed and limb flexion, were essential in correlating emotions and gaits. These pieces of evidence suggest that evaluating gait in a spatiotemporal manner can lead to strong classifications of emotion. With this, new applications in multiple scenarios are enabled, for example: we could leverage gait information to monitor freely moving citizens for public safety, where collective negative emotions could indicate a dangerous situation taking place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>;
wellbeing applications such as urbanism, where collective spaces can be changed according to what people experience in them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>; or even
healthcare where the psychological monitoring of a patient in internment can prove to be useful not only as an overall better treatment, especially if non-invasive, but also to enable mental health professionals to better identify mental health issues in their patients by having another information source to look into (<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Dhuheir et al.</span> [<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Based on this inspiration, we propose an approach for emotion recognition using gait. Through the use of spatiotemporal processing blocks, we overcome limitations present in the current state-of-the-art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, which possess a limited processing capability in this aspect. We also show that by overcoming this limitation, our results in the proposed quantitative metric are also better than the state-of-the-art. This work’s contributions are as follows:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An emotion recognition method through gait and body language, compatible with behavioral psychology studies, with a <math id="S1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\approx 5\%" display="inline"><semantics id="S1.I1.i1.p1.1.m1.1a"><mrow id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml"><mi id="S1.I1.i1.p1.1.m1.1.1.2" xref="S1.I1.i1.p1.1.m1.1.1.2.cmml"></mi><mo id="S1.I1.i1.p1.1.m1.1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S1.I1.i1.p1.1.m1.1.1.3" xref="S1.I1.i1.p1.1.m1.1.1.3.cmml"><mn id="S1.I1.i1.p1.1.m1.1.1.3.2" xref="S1.I1.i1.p1.1.m1.1.1.3.2.cmml">5</mn><mo id="S1.I1.i1.p1.1.m1.1.1.3.1" xref="S1.I1.i1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><apply id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1"><approx id="S1.I1.i1.p1.1.m1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S1.I1.i1.p1.1.m1.1.1.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.2">absent</csymbol><apply id="S1.I1.i1.p1.1.m1.1.1.3.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S1.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S1.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">\approx 5\%</annotation></semantics></math> increase in accuracy relative to the state-of-the-art</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A model that converges 3.63 times faster in training, saving time and computational resources, allowing for faster testing and experimentation.</p>
</div>
</li>
</ul>
<p id="S1.p7.1" class="ltx_p">The rest of the paper is organized as follows: Next, in Section <a href="#S2" title="2 Related Works ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we visit the state of the art. In Section <a href="#S3" title="3 Method ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present the methods utilized and in Section <a href="#S4" title="4 Experimental setup ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we expose our experimental setup. Later, in Section <a href="#S5" title="5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we present the results and discuss them. Finally, Section <a href="#S6" title="6 Conclusion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows our conclusions and present ideas for future research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Early works for emotion recognition through gaits were based on extracting features from motion capture data, which is acquired by using special clothing with landmark points and a motion capture camera, and using algorithms to calculate similarity indexes with databases. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Venture et al.</span> [<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> captured walks from four different performers and applied PCA to verify that these emotions could be distinguished through some affective features.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">A significant improvement was proposed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Daoudi et al.</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, which evaluated this task from a geometric approach. They have represented skeleton joints over time using covariance matrices, which were mapped to the Riemannian manifold of symmetric positive definite matrices. This allowed the authors to exploit multiple geometric properties for emotion classification. However, this approach is limited by how much temporal information can be encoded, imposing a limited sequence modeling.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The natural next step here was, then, related to improving the temporality aspect of these previous approaches. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Randhavane et al.</span> [<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> presented a new approach, now focused on RNNs, thus allowing improved spatiotemporal relationship. They combined affective features, such as the angles between joints and stride length, with deep features that were learned using a Long Short-Term Memory architecture.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">However, the advances on Graph Convolutional Networks at that time, especially the proposal of the ST-GCNs by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yan et al.</span> [<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> allowed for an even more robust way of learning these relationships. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bhattacharya et al.</span> [<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> proposed using such architecture to extract features from videos and classify the emotions in an implicit manner.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Still, while ST-GCNs provide an effective approach, there are some limitations that this work aims to address. The representational capacity of the base ST-GCN is predefined rather than learned, which was sufficient for its originally intended application of activity recognition. However, for perceiving emotional cues through nonverbal behaviors like gait, these cues are often more subtle than the movements used for activity recognition. Therefore, not learning the topology may limit the ability to capture these subtle movement patterns that are indicative of different emotions.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">This is solved by using the ST-GCN++ architecture, which brings the novelty of learning the topology during training. Another limitation is in the temporal aspect, with ST-GCNs having a fixed size temporal kernel, which was rendered adjustable in the ST-GCN++ architecture. Therefore, switching from ST-GCN to ST-GCN++, is very advantageous for the emotion recognition using gaits problem.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.8" class="ltx_p">Given a video <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="V\in\mathcal{R}^{\mathsf{n}\times\mathsf{h}\times\mathsf{w}\times 3}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">V</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">ℛ</mi><mrow id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml"><mi id="S3.p1.1.m1.1.1.3.3.2" xref="S3.p1.1.m1.1.1.3.3.2.cmml">𝗇</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.1.m1.1.1.3.3.3" xref="S3.p1.1.m1.1.1.3.3.3.cmml">𝗁</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1a" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.1.m1.1.1.3.3.4" xref="S3.p1.1.m1.1.1.3.3.4.cmml">𝗐</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.3.3.1b" xref="S3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.p1.1.m1.1.1.3.3.5" xref="S3.p1.1.m1.1.1.3.3.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><in id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></in><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝑉</ci><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">ℛ</ci><apply id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3"><times id="S3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.p1.1.m1.1.1.3.3.1"></times><ci id="S3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.p1.1.m1.1.1.3.3.2">𝗇</ci><ci id="S3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3.3">𝗁</ci><ci id="S3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.p1.1.m1.1.1.3.3.4">𝗐</ci><cn type="integer" id="S3.p1.1.m1.1.1.3.3.5.cmml" xref="S3.p1.1.m1.1.1.3.3.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">V\in\mathcal{R}^{\mathsf{n}\times\mathsf{h}\times\mathsf{w}\times 3}</annotation></semantics></math> with <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathsf{n}" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathsf{n}</annotation></semantics></math> frames, height <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathsf{h}" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">𝗁</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathsf{h}</annotation></semantics></math> width <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\mathsf{w}" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">𝗐</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝗐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathsf{w}</annotation></semantics></math> and a set of emotions <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="\mathbb{K}" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">𝕂</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝕂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\mathbb{K}</annotation></semantics></math>, our task is to classify the perceived emotion of a person present in such video by extracting features related to body language and gait. We first extract a set of 3D body keypoints <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{K}\in\mathcal{R}^{16\times 3}" display="inline"><semantics id="S3.p1.6.m6.1a"><mrow id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">𝐊</mi><mo id="S3.p1.6.m6.1.1.1" xref="S3.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.6.m6.1.1.3.2" xref="S3.p1.6.m6.1.1.3.2.cmml">ℛ</mi><mrow id="S3.p1.6.m6.1.1.3.3" xref="S3.p1.6.m6.1.1.3.3.cmml"><mn id="S3.p1.6.m6.1.1.3.3.2" xref="S3.p1.6.m6.1.1.3.3.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.6.m6.1.1.3.3.1" xref="S3.p1.6.m6.1.1.3.3.1.cmml">×</mo><mn id="S3.p1.6.m6.1.1.3.3.3" xref="S3.p1.6.m6.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><in id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1.1"></in><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">𝐊</ci><apply id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.3.1.cmml" xref="S3.p1.6.m6.1.1.3">superscript</csymbol><ci id="S3.p1.6.m6.1.1.3.2.cmml" xref="S3.p1.6.m6.1.1.3.2">ℛ</ci><apply id="S3.p1.6.m6.1.1.3.3.cmml" xref="S3.p1.6.m6.1.1.3.3"><times id="S3.p1.6.m6.1.1.3.3.1.cmml" xref="S3.p1.6.m6.1.1.3.3.1"></times><cn type="integer" id="S3.p1.6.m6.1.1.3.3.2.cmml" xref="S3.p1.6.m6.1.1.3.3.2">16</cn><cn type="integer" id="S3.p1.6.m6.1.1.3.3.3.cmml" xref="S3.p1.6.m6.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\mathbf{K}\in\mathcal{R}^{16\times 3}</annotation></semantics></math>, in which <math id="S3.p1.7.m7.4" class="ltx_Math" alttext="\mathbf{k}_{1},\mathbf{k}_{2},\ldots,\mathbf{k}_{16}" display="inline"><semantics id="S3.p1.7.m7.4a"><mrow id="S3.p1.7.m7.4.4.3" xref="S3.p1.7.m7.4.4.4.cmml"><msub id="S3.p1.7.m7.2.2.1.1" xref="S3.p1.7.m7.2.2.1.1.cmml"><mi id="S3.p1.7.m7.2.2.1.1.2" xref="S3.p1.7.m7.2.2.1.1.2.cmml">𝐤</mi><mn id="S3.p1.7.m7.2.2.1.1.3" xref="S3.p1.7.m7.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.p1.7.m7.4.4.3.4" xref="S3.p1.7.m7.4.4.4.cmml">,</mo><msub id="S3.p1.7.m7.3.3.2.2" xref="S3.p1.7.m7.3.3.2.2.cmml"><mi id="S3.p1.7.m7.3.3.2.2.2" xref="S3.p1.7.m7.3.3.2.2.2.cmml">𝐤</mi><mn id="S3.p1.7.m7.3.3.2.2.3" xref="S3.p1.7.m7.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.p1.7.m7.4.4.3.5" xref="S3.p1.7.m7.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">…</mi><mo id="S3.p1.7.m7.4.4.3.6" xref="S3.p1.7.m7.4.4.4.cmml">,</mo><msub id="S3.p1.7.m7.4.4.3.3" xref="S3.p1.7.m7.4.4.3.3.cmml"><mi id="S3.p1.7.m7.4.4.3.3.2" xref="S3.p1.7.m7.4.4.3.3.2.cmml">𝐤</mi><mn id="S3.p1.7.m7.4.4.3.3.3" xref="S3.p1.7.m7.4.4.3.3.3.cmml">16</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.4b"><list id="S3.p1.7.m7.4.4.4.cmml" xref="S3.p1.7.m7.4.4.3"><apply id="S3.p1.7.m7.2.2.1.1.cmml" xref="S3.p1.7.m7.2.2.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.2.2.1.1.1.cmml" xref="S3.p1.7.m7.2.2.1.1">subscript</csymbol><ci id="S3.p1.7.m7.2.2.1.1.2.cmml" xref="S3.p1.7.m7.2.2.1.1.2">𝐤</ci><cn type="integer" id="S3.p1.7.m7.2.2.1.1.3.cmml" xref="S3.p1.7.m7.2.2.1.1.3">1</cn></apply><apply id="S3.p1.7.m7.3.3.2.2.cmml" xref="S3.p1.7.m7.3.3.2.2"><csymbol cd="ambiguous" id="S3.p1.7.m7.3.3.2.2.1.cmml" xref="S3.p1.7.m7.3.3.2.2">subscript</csymbol><ci id="S3.p1.7.m7.3.3.2.2.2.cmml" xref="S3.p1.7.m7.3.3.2.2.2">𝐤</ci><cn type="integer" id="S3.p1.7.m7.3.3.2.2.3.cmml" xref="S3.p1.7.m7.3.3.2.2.3">2</cn></apply><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">…</ci><apply id="S3.p1.7.m7.4.4.3.3.cmml" xref="S3.p1.7.m7.4.4.3.3"><csymbol cd="ambiguous" id="S3.p1.7.m7.4.4.3.3.1.cmml" xref="S3.p1.7.m7.4.4.3.3">subscript</csymbol><ci id="S3.p1.7.m7.4.4.3.3.2.cmml" xref="S3.p1.7.m7.4.4.3.3.2">𝐤</ci><cn type="integer" id="S3.p1.7.m7.4.4.3.3.3.cmml" xref="S3.p1.7.m7.4.4.3.3.3">16</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.4c">\mathbf{k}_{1},\mathbf{k}_{2},\ldots,\mathbf{k}_{16}</annotation></semantics></math>, each <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{k}_{i}" display="inline"><semantics id="S3.p1.8.m8.1a"><msub id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">𝐤</mi><mi id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">𝐤</ci><ci id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\mathbf{k}_{i}</annotation></semantics></math> represents the location of a body joint in space related to the person in the video.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.13903/assets/images/st-gait++_2.drawio.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.18.9.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.16.8" class="ltx_text" style="font-size:90%;">The proposed architecture for ST-Gait++, composed of <math id="S3.F1.9.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.F1.9.1.m1.1b"><mn id="S3.F1.9.1.m1.1.1" xref="S3.F1.9.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.F1.9.1.m1.1c"><cn type="integer" id="S3.F1.9.1.m1.1.1.cmml" xref="S3.F1.9.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.9.1.m1.1d">3</annotation></semantics></math> ST-GCN++ blocks with outputs sized <math id="S3.F1.10.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.F1.10.2.m2.1b"><mn id="S3.F1.10.2.m2.1.1" xref="S3.F1.10.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.F1.10.2.m2.1c"><cn type="integer" id="S3.F1.10.2.m2.1.1.cmml" xref="S3.F1.10.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.10.2.m2.1d">32</annotation></semantics></math>, <math id="S3.F1.11.3.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.F1.11.3.m3.1b"><mn id="S3.F1.11.3.m3.1.1" xref="S3.F1.11.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.F1.11.3.m3.1c"><cn type="integer" id="S3.F1.11.3.m3.1.1.cmml" xref="S3.F1.11.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.11.3.m3.1d">64</annotation></semantics></math> and <math id="S3.F1.12.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.F1.12.4.m4.1b"><mn id="S3.F1.12.4.m4.1.1" xref="S3.F1.12.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.F1.12.4.m4.1c"><cn type="integer" id="S3.F1.12.4.m4.1.1.cmml" xref="S3.F1.12.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.12.4.m4.1d">64</annotation></semantics></math>, followed by a <math id="S3.F1.13.5.m5.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S3.F1.13.5.m5.1b"><mrow id="S3.F1.13.5.m5.1.1" xref="S3.F1.13.5.m5.1.1.cmml"><mn id="S3.F1.13.5.m5.1.1.2" xref="S3.F1.13.5.m5.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.F1.13.5.m5.1.1.1" xref="S3.F1.13.5.m5.1.1.1.cmml">​</mo><mi id="S3.F1.13.5.m5.1.1.3" xref="S3.F1.13.5.m5.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.13.5.m5.1c"><apply id="S3.F1.13.5.m5.1.1.cmml" xref="S3.F1.13.5.m5.1.1"><times id="S3.F1.13.5.m5.1.1.1.cmml" xref="S3.F1.13.5.m5.1.1.1"></times><cn type="integer" id="S3.F1.13.5.m5.1.1.2.cmml" xref="S3.F1.13.5.m5.1.1.2">2</cn><ci id="S3.F1.13.5.m5.1.1.3.cmml" xref="S3.F1.13.5.m5.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.13.5.m5.1d">2D</annotation></semantics></math> Average Pooling and a <math id="S3.F1.14.6.m6.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.F1.14.6.m6.1b"><mrow id="S3.F1.14.6.m6.1.1" xref="S3.F1.14.6.m6.1.1.cmml"><mn id="S3.F1.14.6.m6.1.1.2" xref="S3.F1.14.6.m6.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F1.14.6.m6.1.1.1" xref="S3.F1.14.6.m6.1.1.1.cmml">×</mo><mn id="S3.F1.14.6.m6.1.1.3" xref="S3.F1.14.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.14.6.m6.1c"><apply id="S3.F1.14.6.m6.1.1.cmml" xref="S3.F1.14.6.m6.1.1"><times id="S3.F1.14.6.m6.1.1.1.cmml" xref="S3.F1.14.6.m6.1.1.1"></times><cn type="integer" id="S3.F1.14.6.m6.1.1.2.cmml" xref="S3.F1.14.6.m6.1.1.2">1</cn><cn type="integer" id="S3.F1.14.6.m6.1.1.3.cmml" xref="S3.F1.14.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.14.6.m6.1d">1\times 1</annotation></semantics></math> Convolution, which reduces dimensionality from <math id="S3.F1.15.7.m7.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.F1.15.7.m7.1b"><mn id="S3.F1.15.7.m7.1.1" xref="S3.F1.15.7.m7.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.F1.15.7.m7.1c"><cn type="integer" id="S3.F1.15.7.m7.1.1.cmml" xref="S3.F1.15.7.m7.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.15.7.m7.1d">64</annotation></semantics></math> to <math id="S3.F1.16.8.m8.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.F1.16.8.m8.1b"><mn id="S3.F1.16.8.m8.1.1" xref="S3.F1.16.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.F1.16.8.m8.1c"><cn type="integer" id="S3.F1.16.8.m8.1.1.cmml" xref="S3.F1.16.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.16.8.m8.1d">4</annotation></semantics></math>, which is followed by a Softmax. Ideally, in an application scenario, this can be used along some Skeletal trajectory extraction which takes as input a video and outputs the gait sequences to be analysed by ST-Gait++ automatically. This work focuses on the Skeletal trajectory classification.</span></figcaption>
</figure>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">(a) Skeletal trajectory extraction.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.5" class="ltx_p">One of the possible ways to represent a skeleton is through a graph, since these representations can be considered analog. Each body joint, such as the right shoulder or right elbow can be considered a vertex, and the bones that connects these two joints can be considered edges. This is a clear indicative on why GCNs can be used to process these types of data. Therefore, at a given timestamp <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">t</annotation></semantics></math>, we extract the skeleton of the person visible on the scene and represent it as a graph: <math id="S3.SS0.SSS0.Px1.p1.2.m2.2" class="ltx_Math" alttext="\mathbb{G}=(\mathbb{V},\mathbb{E})" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2.cmml">𝔾</mi><mo id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml">=</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">𝕍</mi><mo id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml">𝔼</mi><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.2b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3"><eq id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1"></eq><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2">𝔾</ci><interval closure="open" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2"><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">𝕍</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2">𝔼</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.2c">\mathbb{G}=(\mathbb{V},\mathbb{E})</annotation></semantics></math>, where <math id="S3.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathbb{V}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">𝕍</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">𝕍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">\mathbb{V}</annotation></semantics></math> is the vertices (or joints) set and <math id="S3.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathbb{E}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">𝔼</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1">𝔼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.1c">\mathbb{E}</annotation></semantics></math> is the edge (or bones) set and <math id="S3.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\mathbb{N}=|\mathbb{V}|" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.5.m5.1a"><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.1.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.2.cmml">ℕ</mi><mo id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.1.cmml">=</mo><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.1.1.cmml">|</mo><mi id="S3.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">𝕍</mi><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.2.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2"><eq id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.1"></eq><ci id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.2">ℕ</ci><apply id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.2"><abs id="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.2.3.2.1"></abs><ci id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1">𝕍</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.5.m5.1c">\mathbb{N}=|\mathbb{V}|</annotation></semantics></math> the number of vertices.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">(b) Skeletal trajectory classification.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">We use this graph <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathbb{G}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">𝔾</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">𝔾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\mathbb{G}</annotation></semantics></math> as input for our gait processing model. We propose using ST-GCN++ blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> to learn the joint representations and discover movement patterns related to perceived emotions. This way, nonverbal cues such as step size, arm swinging, head angle relative to the shoulders, among others, can be extracted automatically and without user intervention.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.24" class="ltx_p">The extracted gait features are propagated from the body joints in the shape of <math id="S3.SS0.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\mathbb{X}\in\mathcal{R}^{\mathsf{n}\times\mathsf{f}}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml">𝕏</mi><mo id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">ℛ</mi><mrow id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml">𝗇</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml">𝖿</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1"><in id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2">𝕏</ci><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2">ℛ</ci><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2">𝗇</ci><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3">𝖿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.1.m1.1c">\mathbb{X}\in\mathcal{R}^{\mathsf{n}\times\mathsf{f}}</annotation></semantics></math> with <math id="S3.SS0.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="x_{i}\in\mathcal{R}^{\mathsf{f}}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.2.m2.1a"><mrow id="S3.SS0.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.cmml"><msub id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.2.cmml">x</mi><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2.cmml">ℛ</mi><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.cmml">𝖿</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1"><in id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1"></in><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.2">𝑥</ci><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2">ℛ</ci><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3">𝖿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.2.m2.1c">x_{i}\in\mathcal{R}^{\mathsf{f}}</annotation></semantics></math> representing a feature of the <math id="S3.SS0.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.3.m3.1a"><msup id="S3.SS0.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml">i</mi><mrow id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.1" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2">𝑖</ci><apply id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3"><times id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2">𝑡</ci><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.3.m3.1c">i^{th}</annotation></semantics></math> vertex. The propagation rule is done in the following manner: <math id="S3.SS0.SSS0.Px2.p2.4.m4.4" class="ltx_Math" alttext="\mathbb{Z}^{(l+1)}=\sigma(\mathbb{AZ}^{(l)}\mathbb{W}^{(l)})" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.4.m4.4a"><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.4.4" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.cmml"><msup id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.2.cmml">ℤ</mi><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.2.cmml">l</mi><mo id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.2.cmml">=</mo><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.2.cmml">​</mo><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.2.cmml">𝔸</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1.cmml">​</mo><msup id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.2.cmml">ℤ</mi><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.3.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.cmml">(</mo><mi id="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.1.cmml">l</mi><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.3.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1a" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1.cmml">​</mo><msup id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.cmml"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.2.cmml">𝕎</mi><mrow id="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.3.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.cmml">(</mo><mi id="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.1.cmml">l</mi><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.3.2" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.cmml">)</mo></mrow></msup></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.4.m4.4b"><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4"><eq id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.2"></eq><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.3.2">ℤ</ci><apply id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1"><plus id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.1"></plus><ci id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.2">𝑙</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1"><times id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.2"></times><ci id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.3">𝜎</ci><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1"><times id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.1"></times><ci id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.2">𝔸</ci><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.3.2">ℤ</ci><ci id="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.2.2.1.1">𝑙</ci></apply><apply id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.2.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.4.4.1.1.1.1.4.2">𝕎</ci><ci id="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.3.3.1.1">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.4.m4.4c">\mathbb{Z}^{(l+1)}=\sigma(\mathbb{AZ}^{(l)}\mathbb{W}^{(l)})</annotation></semantics></math>, where <math id="S3.SS0.SSS0.Px2.p2.5.m5.1" class="ltx_Math" alttext="\mathbb{Z}^{l}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.5.m5.1a"><msup id="S3.SS0.SSS0.Px2.p2.5.m5.1.1" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.2" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.2.cmml">ℤ</mi><mi id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.3" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.5.m5.1b"><apply id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.2">ℤ</ci><ci id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.5.m5.1c">\mathbb{Z}^{l}</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="\mathbb{Z}^{l+1}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.6.m6.1a"><msup id="S3.SS0.SSS0.Px2.p2.6.m6.1.1" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.2" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.2.cmml">ℤ</mi><mrow id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.2.cmml">l</mi><mo id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.1" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.1.cmml">+</mo><mn id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.6.m6.1b"><apply id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.2">ℤ</ci><apply id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3"><plus id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.1"></plus><ci id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.6.m6.1c">\mathbb{Z}^{l+1}</annotation></semantics></math> are the inputs to the network layers <math id="S3.SS0.SSS0.Px2.p2.7.m7.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.7.m7.1a"><mi id="S3.SS0.SSS0.Px2.p2.7.m7.1.1" xref="S3.SS0.SSS0.Px2.p2.7.m7.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.7.m7.1b"><ci id="S3.SS0.SSS0.Px2.p2.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.7.m7.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.7.m7.1c">l</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p2.8.m8.1" class="ltx_Math" alttext="l+1" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.8.m8.1a"><mrow id="S3.SS0.SSS0.Px2.p2.8.m8.1.1" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.2" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.2.cmml">l</mi><mo id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.1" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.1.cmml">+</mo><mn id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.3" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.8.m8.1b"><apply id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1"><plus id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.1"></plus><ci id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.2">𝑙</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.8.m8.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.8.m8.1c">l+1</annotation></semantics></math>, with <math id="S3.SS0.SSS0.Px2.p2.9.m9.1" class="ltx_Math" alttext="\mathbb{Z}^{0}=\mathbb{X}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.9.m9.1a"><mrow id="S3.SS0.SSS0.Px2.p2.9.m9.1.1" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.cmml"><msup id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.2" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.2.cmml">ℤ</mi><mn id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.3" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.3.cmml">0</mn></msup><mo id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.1" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.1.cmml">=</mo><mi id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.3" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.3.cmml">𝕏</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.9.m9.1b"><apply id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1"><eq id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.1"></eq><apply id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.2">ℤ</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.2.3">0</cn></apply><ci id="S3.SS0.SSS0.Px2.p2.9.m9.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m9.1.1.3">𝕏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.9.m9.1c">\mathbb{Z}^{0}=\mathbb{X}</annotation></semantics></math>. <math id="S3.SS0.SSS0.Px2.p2.10.m10.1" class="ltx_Math" alttext="\mathbb{W}^{l}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.10.m10.1a"><msup id="S3.SS0.SSS0.Px2.p2.10.m10.1.1" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.2" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1.2.cmml">𝕎</mi><mi id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.3" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.10.m10.1b"><apply id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1.2">𝕎</ci><ci id="S3.SS0.SSS0.Px2.p2.10.m10.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.10.m10.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.10.m10.1c">\mathbb{W}^{l}</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p2.11.m11.1" class="ltx_Math" alttext="\mathbb{W}^{l+1}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.11.m11.1a"><msup id="S3.SS0.SSS0.Px2.p2.11.m11.1.1" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.2" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.2.cmml">𝕎</mi><mrow id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.2.cmml">l</mi><mo id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.1" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.1.cmml">+</mo><mn id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.11.m11.1b"><apply id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.2">𝕎</ci><apply id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3"><plus id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.1"></plus><ci id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.11.m11.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.11.m11.1c">\mathbb{W}^{l+1}</annotation></semantics></math> are the weight matrices between layers <math id="S3.SS0.SSS0.Px2.p2.12.m12.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.12.m12.1a"><mi id="S3.SS0.SSS0.Px2.p2.12.m12.1.1" xref="S3.SS0.SSS0.Px2.p2.12.m12.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.12.m12.1b"><ci id="S3.SS0.SSS0.Px2.p2.12.m12.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.12.m12.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.12.m12.1c">l</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p2.13.m13.1" class="ltx_Math" alttext="l+1" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.13.m13.1a"><mrow id="S3.SS0.SSS0.Px2.p2.13.m13.1.1" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.2" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.2.cmml">l</mi><mo id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.1" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.1.cmml">+</mo><mn id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.3" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.13.m13.1b"><apply id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1"><plus id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.1"></plus><ci id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.2">𝑙</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p2.13.m13.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.13.m13.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.13.m13.1c">l+1</annotation></semantics></math>, and <math id="S3.SS0.SSS0.Px2.p2.14.m14.1" class="ltx_Math" alttext="\mathbb{A}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.14.m14.1a"><mi id="S3.SS0.SSS0.Px2.p2.14.m14.1.1" xref="S3.SS0.SSS0.Px2.p2.14.m14.1.1.cmml">𝔸</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.14.m14.1b"><ci id="S3.SS0.SSS0.Px2.p2.14.m14.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.14.m14.1.1">𝔸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.14.m14.1c">\mathbb{A}</annotation></semantics></math> is the adjacency matrix of <math id="S3.SS0.SSS0.Px2.p2.15.m15.1" class="ltx_Math" alttext="\mathbb{G}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.15.m15.1a"><mi id="S3.SS0.SSS0.Px2.p2.15.m15.1.1" xref="S3.SS0.SSS0.Px2.p2.15.m15.1.1.cmml">𝔾</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.15.m15.1b"><ci id="S3.SS0.SSS0.Px2.p2.15.m15.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.15.m15.1.1">𝔾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.15.m15.1c">\mathbb{G}</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p2.16.m16.1" class="ltx_math_unparsed" alttext="\sigma(.)" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.16.m16.1a"><mrow id="S3.SS0.SSS0.Px2.p2.16.m16.1b"><mi id="S3.SS0.SSS0.Px2.p2.16.m16.1.1">σ</mi><mrow id="S3.SS0.SSS0.Px2.p2.16.m16.1.2"><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.16.m16.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S3.SS0.SSS0.Px2.p2.16.m16.1.2.2">.</mo><mo stretchy="false" id="S3.SS0.SSS0.Px2.p2.16.m16.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.16.m16.1c">\sigma(.)</annotation></semantics></math> is a nonlinear activation function. Each weight matrix <math id="S3.SS0.SSS0.Px2.p2.17.m17.1" class="ltx_Math" alttext="\mathbb{W}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.17.m17.1a"><mi id="S3.SS0.SSS0.Px2.p2.17.m17.1.1" xref="S3.SS0.SSS0.Px2.p2.17.m17.1.1.cmml">𝕎</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.17.m17.1b"><ci id="S3.SS0.SSS0.Px2.p2.17.m17.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.17.m17.1.1">𝕎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.17.m17.1c">\mathbb{W}</annotation></semantics></math> represents a convolutional kernel which can be used to obtain features. For example, the application of <math id="S3.SS0.SSS0.Px2.p2.18.m18.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.18.m18.1a"><mi id="S3.SS0.SSS0.Px2.p2.18.m18.1.1" xref="S3.SS0.SSS0.Px2.p2.18.m18.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.18.m18.1b"><ci id="S3.SS0.SSS0.Px2.p2.18.m18.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.18.m18.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.18.m18.1c">k</annotation></semantics></math> kernels of dimension <math id="S3.SS0.SSS0.Px2.p2.19.m19.1" class="ltx_Math" alttext="\mathsf{f}\times\mathsf{d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.19.m19.1a"><mrow id="S3.SS0.SSS0.Px2.p2.19.m19.1.1" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.2" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.2.cmml">𝖿</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.1" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.3" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.3.cmml">𝖽</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.19.m19.1b"><apply id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1"><times id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.1"></times><ci id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.2">𝖿</ci><ci id="S3.SS0.SSS0.Px2.p2.19.m19.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.19.m19.1.1.3">𝖽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.19.m19.1c">\mathsf{f}\times\mathsf{d}</annotation></semantics></math> in an input <math id="S3.SS0.SSS0.Px2.p2.20.m20.1" class="ltx_Math" alttext="\mathbb{X}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.20.m20.1a"><mi id="S3.SS0.SSS0.Px2.p2.20.m20.1.1" xref="S3.SS0.SSS0.Px2.p2.20.m20.1.1.cmml">𝕏</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.20.m20.1b"><ci id="S3.SS0.SSS0.Px2.p2.20.m20.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.20.m20.1.1">𝕏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.20.m20.1c">\mathbb{X}</annotation></semantics></math>, the output corresponds to a feature vector of dimension <math id="S3.SS0.SSS0.Px2.p2.21.m21.1" class="ltx_Math" alttext="\mathsf{n}\times\mathsf{d}\times\mathsf{k}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.21.m21.1a"><mrow id="S3.SS0.SSS0.Px2.p2.21.m21.1.1" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.2" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.2.cmml">𝗇</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.3" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.3.cmml">𝖽</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1a" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.4" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.4.cmml">𝗄</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.21.m21.1b"><apply id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1"><times id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.1"></times><ci id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.2">𝗇</ci><ci id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.3">𝖽</ci><ci id="S3.SS0.SSS0.Px2.p2.21.m21.1.1.4.cmml" xref="S3.SS0.SSS0.Px2.p2.21.m21.1.1.4">𝗄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.21.m21.1c">\mathsf{n}\times\mathsf{d}\times\mathsf{k}</annotation></semantics></math>. There’s also the set of adjacent vertices <math id="S3.SS0.SSS0.Px2.p2.22.m22.1" class="ltx_Math" alttext="\mathit{A}^{t}_{i}\subseteq\mathbb{V}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.22.m22.1a"><mrow id="S3.SS0.SSS0.Px2.p2.22.m22.1.1" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.cmml"><msubsup id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.2" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.2.cmml">A</mi><mi id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.3" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.3.cmml">i</mi><mi id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.3" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.3.cmml">t</mi></msubsup><mo id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.1" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.1.cmml">⊆</mo><mi id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.3" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.3.cmml">𝕍</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.22.m22.1b"><apply id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1"><subset id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.1"></subset><apply id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2">subscript</csymbol><apply id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.1.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.2.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.2">𝐴</ci><ci id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.3.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.2.3">𝑡</ci></apply><ci id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.2.3">𝑖</ci></apply><ci id="S3.SS0.SSS0.Px2.p2.22.m22.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.22.m22.1.1.3">𝕍</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.22.m22.1c">\mathit{A}^{t}_{i}\subseteq\mathbb{V}</annotation></semantics></math> to <math id="S3.SS0.SSS0.Px2.p2.23.m23.1" class="ltx_Math" alttext="v^{t}_{i}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.23.m23.1a"><msubsup id="S3.SS0.SSS0.Px2.p2.23.m23.1.1" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.2" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.2.cmml">v</mi><mi id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.3" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.3.cmml">i</mi><mi id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.3" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.23.m23.1b"><apply id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1">subscript</csymbol><apply id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.2">𝑣</ci><ci id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.2.3">𝑡</ci></apply><ci id="S3.SS0.SSS0.Px2.p2.23.m23.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.23.m23.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.23.m23.1c">v^{t}_{i}</annotation></semantics></math> at time <math id="S3.SS0.SSS0.Px2.p2.24.m24.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.24.m24.1a"><mi id="S3.SS0.SSS0.Px2.p2.24.m24.1.1" xref="S3.SS0.SSS0.Px2.p2.24.m24.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.24.m24.1b"><ci id="S3.SS0.SSS0.Px2.p2.24.m24.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.24.m24.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.24.m24.1c">t</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p3.8" class="ltx_p">Our proposed model ST-Gait++ is composed of <math id="S3.SS0.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.1.m1.1a"><mn id="S3.SS0.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.1.m1.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.1.m1.1c">3</annotation></semantics></math> ST-GCN++ blocks with <math id="S3.SS0.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.2.m2.1a"><mn id="S3.SS0.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p3.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.2.m2.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.2.m2.1c">32</annotation></semantics></math>, <math id="S3.SS0.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.3.m3.1a"><mn id="S3.SS0.SSS0.Px2.p3.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p3.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.3.m3.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.3.m3.1c">64</annotation></semantics></math>, and <math id="S3.SS0.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.4.m4.1a"><mn id="S3.SS0.SSS0.Px2.p3.4.m4.1.1" xref="S3.SS0.SSS0.Px2.p3.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.4.m4.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.4.m4.1c">64</annotation></semantics></math> kernels each, followed by an average pooling, a <math id="S3.SS0.SSS0.Px2.p3.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.5.m5.1a"><mn id="S3.SS0.SSS0.Px2.p3.5.m5.1.1" xref="S3.SS0.SSS0.Px2.p3.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.5.m5.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.5.m5.1c">2</annotation></semantics></math>D <math id="S3.SS0.SSS0.Px2.p3.6.m6.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.6.m6.1a"><mn id="S3.SS0.SSS0.Px2.p3.6.m6.1.1" xref="S3.SS0.SSS0.Px2.p3.6.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.6.m6.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.6.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.6.m6.1c">1</annotation></semantics></math>x<math id="S3.SS0.SSS0.Px2.p3.7.m7.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.7.m7.1a"><mn id="S3.SS0.SSS0.Px2.p3.7.m7.1.1" xref="S3.SS0.SSS0.Px2.p3.7.m7.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.7.m7.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.7.m7.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.7.m7.1c">1</annotation></semantics></math> convolution layer, and a softmax layer for the <math id="S3.SS0.SSS0.Px2.p3.8.m8.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS0.SSS0.Px2.p3.8.m8.1a"><mn id="S3.SS0.SSS0.Px2.p3.8.m8.1.1" xref="S3.SS0.SSS0.Px2.p3.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p3.8.m8.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p3.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px2.p3.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p3.8.m8.1c">4</annotation></semantics></math> emotion categories. This design was chosen empirically to overcome the limitations presented previously in Section <a href="#S2" title="2 Related Works ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, but also because other works experiment with similar architectures, such as STEP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Also, according to the methodology described by the author <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, affective features can extracted from the data, so those are extracted and added to the used data as well. We overview our model in <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup </h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We used the Emotion-Gait (E-Gait) dataset in our experiments. The dataset that is currently available is a modified version of the original dataset made available by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bhattacharya et al.</span> [<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. The authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> do not share specifics of what has changed or when it did, so we try our best to keep a clear comparison with other techniques from the state of the art. We only used the real data from the E-Gait, because of some quality issues perceived during early experimentation with the synthetic data, alongside the issue of the changes on the dataset.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.8" class="ltx_p">The data consists on <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="342" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">342</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">342</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">342</annotation></semantics></math> samples collected by the authors and 1,835 samples collected by the Edinburgh Locomotion Mocap Database (ELMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> and its distribution per train/validation/test split and categories can be found in <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experimental setup ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>. The data is composed of already extracted and normalized skeleton sequences forming different labeled gaits in the <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">4</annotation></semantics></math> emotions targeted in this paper: Happy, Neutral, Sad and Angry. Each sample is shaped <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="T\times V" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">×</mo><mi id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><times id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></times><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑇</ci><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">T\times V</annotation></semantics></math> with <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">T</annotation></semantics></math> being the number of time steps and <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">V</annotation></semantics></math> the number of coordinates which is equal to <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="48" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mn id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><cn type="integer" id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">48</annotation></semantics></math> here since there are <math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mn id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><cn type="integer" id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">16</annotation></semantics></math> joins with <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mn id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><cn type="integer" id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">3</annotation></semantics></math> dimensions each. We show samples of the skeletons present in the dataset in <a href="#S4.F2" title="Figure 2 ‣ 4.1 Dataset ‣ 4 Experimental setup ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">E-Gait dataset sample distribution according to training, validation and testing splits and the four emotion labels present on E-Gait: Angry, Neutral, Happy and Sad.</span></figcaption>
<div id="S4.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:74.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.1pt,7.9pt) scale(0.824458379934055,0.824458379934055) ;">
<table id="S4.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.1.1" class="ltx_tr">
<td id="S4.T1.4.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S4.T1.4.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4"><span id="S4.T1.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Category</span></th>
<th id="S4.T1.4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T1.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Total</span></th>
</tr>
<tr id="S4.T1.4.1.2.2" class="ltx_tr">
<td id="S4.T1.4.1.2.2.1" class="ltx_td"></td>
<th id="S4.T1.4.1.2.2.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.3.1" class="ltx_text ltx_font_bold">Angry</span></th>
<th id="S4.T1.4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.4.1" class="ltx_text ltx_font_bold">Neutral</span></th>
<th id="S4.T1.4.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.5.1" class="ltx_text ltx_font_bold">Happy</span></th>
<th id="S4.T1.4.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.1.2.2.6.1" class="ltx_text ltx_font_bold">Sad</span></th>
</tr>
<tr id="S4.T1.4.1.3.3" class="ltx_tr">
<td id="S4.T1.4.1.3.3.1" class="ltx_td ltx_align_left ltx_border_b" rowspan="3"><span id="S4.T1.4.1.3.3.1.1" class="ltx_text ltx_font_bold">Split</span></td>
<td id="S4.T1.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.1.3.3.2.1" class="ltx_text ltx_font_bold">Train</span></td>
<td id="S4.T1.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">818</td>
<td id="S4.T1.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">332</td>
<td id="S4.T1.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">237</td>
<td id="S4.T1.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">136</td>
<td id="S4.T1.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">1523</td>
</tr>
<tr id="S4.T1.4.1.4.4" class="ltx_tr">
<td id="S4.T1.4.1.4.4.1" class="ltx_td ltx_align_center"><span id="S4.T1.4.1.4.4.1.1" class="ltx_text ltx_font_bold">Val</span></td>
<td id="S4.T1.4.1.4.4.2" class="ltx_td ltx_align_center">220</td>
<td id="S4.T1.4.1.4.4.3" class="ltx_td ltx_align_center">107</td>
<td id="S4.T1.4.1.4.4.4" class="ltx_td ltx_align_center">65</td>
<td id="S4.T1.4.1.4.4.5" class="ltx_td ltx_align_center">46</td>
<td id="S4.T1.4.1.4.4.6" class="ltx_td ltx_align_center">438</td>
</tr>
<tr id="S4.T1.4.1.5.5" class="ltx_tr">
<td id="S4.T1.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.4.1.5.5.1.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S4.T1.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b">122</td>
<td id="S4.T1.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b">48</td>
<td id="S4.T1.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b">30</td>
<td id="S4.T1.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b">16</td>
<td id="S4.T1.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_b">216</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/angry-42.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="143" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Angry</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/69-happy.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="116" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Happy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/151-neutral.png" id="S4.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Neutral</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/17-sad.png" id="S4.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Sad</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Examples of the four categories of the E-Gait dataset. Each item is a sample from one of the categories of the E-Gait dataset, with each frame of the six-frame sequence taken from the whole sample gait sequence. This was done to provide a sense of movement to the reader, so they can better understand E-Gait’s characteristics.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Validation metric</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We have used the <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">accuracy</span> metric, which is a common metric used in multiple emotion recognition baselines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. This metric is also employed in other works that use E-Gait as the evaluation benchmark.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">We have used PyTorch <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="float" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1.7</annotation></semantics></math> to build and train our model. We have used the publicly available implementation for the ST-GCN++ block published by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Duan et al.</span> [<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Available at <a target="_blank" href="https://github.com/kennymckormick/pyskl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kennymckormick/pyskl</a></span></span></span>. From this point, we have applied a Bayesian Search algorithm for hyperparameter tuning. Bayesian Search is a powerful yet simple technique for optimizing hyperparameters by modelling objective functions and updating its belief based on observed results. Therefore, instead of using a random set of hyperparameters, which would be computationally costly, this method will choose the set of hyperparameters that has the highest chance of leading to better results and will discard those with low chance. The search space we navigated using this algorithm is shown in <a href="#S4.T2" title="Table 2 ‣ 4.3 Implementation details ‣ 4 Experimental setup ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, and the set of chosen hyperparameters in comparison to those of STEP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> are shown in <a href="#S4.T3" title="Table 3 ‣ 4.3 Implementation details ‣ 4 Experimental setup ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. We have trained ST-Gait++ for <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">200</annotation></semantics></math> epochs.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.6.2" class="ltx_text" style="font-size:90%;">Search space defined for the Bayesian Search. For each parameter, a set of values are chosen as a search space. As the Bayesian Search finds an optimal value within the search space for each parameter, this value is represented on the third column of the table. These optimal values were used to train ST-Gait++.</span></figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:71.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.3pt,9.1pt) scale(0.798603023896389,0.798603023896389) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<td id="S4.T2.3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></td>
<td id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.3.3.4.1.2.1" class="ltx_text ltx_font_bold">Search Space</span></td>
<td id="S4.T2.3.3.4.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T2.3.3.4.1.3.1" class="ltx_text ltx_font_bold">Value</span></td>
</tr>
<tr id="S4.T2.3.3.5.2" class="ltx_tr">
<td id="S4.T2.3.3.5.2.1" class="ltx_td ltx_align_left ltx_border_t">Basic Learning Rate</td>
<td id="S4.T2.3.3.5.2.2" class="ltx_td ltx_align_left ltx_border_t">0.01, 0.1, 0.3</td>
<td id="S4.T2.3.3.5.2.3" class="ltx_td ltx_align_right ltx_border_t">0.01</td>
</tr>
<tr id="S4.T2.3.3.6.3" class="ltx_tr">
<td id="S4.T2.3.3.6.3.1" class="ltx_td ltx_align_left">GCN Initializer</td>
<td id="S4.T2.3.3.6.3.2" class="ltx_td ltx_align_left">Importance, Offset</td>
<td id="S4.T2.3.3.6.3.3" class="ltx_td ltx_align_right">Offset</td>
</tr>
<tr id="S4.T2.3.3.7.4" class="ltx_tr">
<td id="S4.T2.3.3.7.4.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="S4.T2.3.3.7.4.2" class="ltx_td ltx_align_left">Adam, RMSProp, SGD</td>
<td id="S4.T2.3.3.7.4.3" class="ltx_td ltx_align_right">RMSProp</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_left ltx_border_b">Weight Decay</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_b">
<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T2.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T2.1.1.1.1.m1.1.1.3.3a" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T2.1.1.1.1.m1.1.1.3.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">3</cn><apply id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3"><minus id="S4.T2.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">3\times 10^{-4}</annotation></semantics></math>, <math id="S4.T2.2.2.2.2.m2.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.T2.2.2.2.2.m2.1a"><mrow id="S4.T2.2.2.2.2.m2.1.1" xref="S4.T2.2.2.2.2.m2.1.1.cmml"><mn id="S4.T2.2.2.2.2.m2.1.1.2" xref="S4.T2.2.2.2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.2.2.2.2.m2.1.1.1" xref="S4.T2.2.2.2.2.m2.1.1.1.cmml">×</mo><msup id="S4.T2.2.2.2.2.m2.1.1.3" xref="S4.T2.2.2.2.2.m2.1.1.3.cmml"><mn id="S4.T2.2.2.2.2.m2.1.1.3.2" xref="S4.T2.2.2.2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.T2.2.2.2.2.m2.1.1.3.3" xref="S4.T2.2.2.2.2.m2.1.1.3.3.cmml"><mo id="S4.T2.2.2.2.2.m2.1.1.3.3a" xref="S4.T2.2.2.2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.T2.2.2.2.2.m2.1.1.3.3.2" xref="S4.T2.2.2.2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m2.1b"><apply id="S4.T2.2.2.2.2.m2.1.1.cmml" xref="S4.T2.2.2.2.2.m2.1.1"><times id="S4.T2.2.2.2.2.m2.1.1.1.cmml" xref="S4.T2.2.2.2.2.m2.1.1.1"></times><cn type="integer" id="S4.T2.2.2.2.2.m2.1.1.2.cmml" xref="S4.T2.2.2.2.2.m2.1.1.2">1</cn><apply id="S4.T2.2.2.2.2.m2.1.1.3.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m2.1.1.3.1.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.T2.2.2.2.2.m2.1.1.3.2.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3.2">10</cn><apply id="S4.T2.2.2.2.2.m2.1.1.3.3.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3.3"><minus id="S4.T2.2.2.2.2.m2.1.1.3.3.1.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.T2.2.2.2.2.m2.1.1.3.3.2.cmml" xref="S4.T2.2.2.2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m2.1c">1\times 10^{-4}</annotation></semantics></math>
</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_right ltx_border_b"><math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml"><mn id="S4.T2.3.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.3.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.3.3.3.3.m1.1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.1.cmml">×</mo><msup id="S4.T2.3.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.3.m1.1.1.3.cmml"><mn id="S4.T2.3.3.3.3.m1.1.1.3.2" xref="S4.T2.3.3.3.3.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T2.3.3.3.3.m1.1.1.3.3" xref="S4.T2.3.3.3.3.m1.1.1.3.3.cmml"><mo id="S4.T2.3.3.3.3.m1.1.1.3.3a" xref="S4.T2.3.3.3.3.m1.1.1.3.3.cmml">−</mo><mn id="S4.T2.3.3.3.3.m1.1.1.3.3.2" xref="S4.T2.3.3.3.3.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><times id="S4.T2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.1"></times><cn type="integer" id="S4.T2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2">3</cn><apply id="S4.T2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T2.3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.2">10</cn><apply id="S4.T2.3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.3"><minus id="S4.T2.3.3.3.3.m1.1.1.3.3.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.T2.3.3.3.3.m1.1.1.3.3.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">3\times 10^{-4}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;">Training hyperparameters for STEP and ST-Gait++. The learning rate decay happens after epochs 250, 375 and 438 and wasn’t used on ST-Gait++. The remaining parameters are the same as STEP, except GCN Initializer which doesn’t exist on STEP.</span></figcaption>
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.3.1" class="ltx_tr">
<th id="S4.T3.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T3.2.3.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></th>
<td id="S4.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T3.2.3.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="S4.T3.2.4.2" class="ltx_tr">
<td id="S4.T3.2.4.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.4.2.1.1" class="ltx_text ltx_font_bold">STEP</span></td>
<td id="S4.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.4.2.2.1" class="ltx_text ltx_font_bold">ST-Gait++</span></td>
</tr>
<tr id="S4.T3.2.5.3" class="ltx_tr">
<th id="S4.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Train/Val/Test</th>
<td id="S4.T3.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">7:2:1</td>
<td id="S4.T3.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">7:2:1</td>
</tr>
<tr id="S4.T3.2.6.4" class="ltx_tr">
<th id="S4.T3.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Epochs</th>
<td id="S4.T3.2.6.4.2" class="ltx_td ltx_align_center">500</td>
<td id="S4.T3.2.6.4.3" class="ltx_td ltx_align_center">200</td>
</tr>
<tr id="S4.T3.2.7.5" class="ltx_tr">
<th id="S4.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Batch Size</th>
<td id="S4.T3.2.7.5.2" class="ltx_td ltx_align_center">8</td>
<td id="S4.T3.2.7.5.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S4.T3.2.8.6" class="ltx_tr">
<th id="S4.T3.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Optimizer</th>
<td id="S4.T3.2.8.6.2" class="ltx_td ltx_align_center">Adam</td>
<td id="S4.T3.2.8.6.3" class="ltx_td ltx_align_center">RMSProp</td>
</tr>
<tr id="S4.T3.2.9.7" class="ltx_tr">
<th id="S4.T3.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Basic LR</th>
<td id="S4.T3.2.9.7.2" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T3.2.9.7.3" class="ltx_td ltx_align_center">0.01</td>
</tr>
<tr id="S4.T3.2.10.8" class="ltx_tr">
<th id="S4.T3.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LR Decay</th>
<td id="S4.T3.2.10.8.2" class="ltx_td ltx_align_center">1/10</td>
<td id="S4.T3.2.10.8.3" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S4.T3.2.11.9" class="ltx_tr">
<th id="S4.T3.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Momentum</th>
<td id="S4.T3.2.11.9.2" class="ltx_td ltx_align_center">0.9</td>
<td id="S4.T3.2.11.9.3" class="ltx_td ltx_align_center">0.9</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<th id="S4.T3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Weight Decay</th>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center"><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml"><mn id="S4.T3.1.1.1.m1.1.1.3.2" xref="S4.T3.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T3.1.1.1.m1.1.1.3.3" xref="S4.T3.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T3.1.1.1.m1.1.1.3.3a" xref="S4.T3.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T3.1.1.1.m1.1.1.3.3.2" xref="S4.T3.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">5</cn><apply id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.3.1.cmml" xref="S4.T3.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T3.1.1.1.m1.1.1.3.2.cmml" xref="S4.T3.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T3.1.1.1.m1.1.1.3.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3.3"><minus id="S4.T3.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T3.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T3.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T3.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">5\times 10^{-4}</annotation></semantics></math></td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center"><math id="S4.T3.2.2.2.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml"><mn id="S4.T3.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.2.2.2.m1.1.1.1" xref="S4.T3.2.2.2.m1.1.1.1.cmml">×</mo><msup id="S4.T3.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.m1.1.1.3.cmml"><mn id="S4.T3.2.2.2.m1.1.1.3.2" xref="S4.T3.2.2.2.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T3.2.2.2.m1.1.1.3.3" xref="S4.T3.2.2.2.m1.1.1.3.3.cmml"><mo id="S4.T3.2.2.2.m1.1.1.3.3a" xref="S4.T3.2.2.2.m1.1.1.3.3.cmml">−</mo><mn id="S4.T3.2.2.2.m1.1.1.3.3.2" xref="S4.T3.2.2.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1"><times id="S4.T3.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1.1"></times><cn type="integer" id="S4.T3.2.2.2.m1.1.1.2.cmml" xref="S4.T3.2.2.2.m1.1.1.2">3</cn><apply id="S4.T3.2.2.2.m1.1.1.3.cmml" xref="S4.T3.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.2.2.2.m1.1.1.3.1.cmml" xref="S4.T3.2.2.2.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T3.2.2.2.m1.1.1.3.2.cmml" xref="S4.T3.2.2.2.m1.1.1.3.2">10</cn><apply id="S4.T3.2.2.2.m1.1.1.3.3.cmml" xref="S4.T3.2.2.2.m1.1.1.3.3"><minus id="S4.T3.2.2.2.m1.1.1.3.3.1.cmml" xref="S4.T3.2.2.2.m1.1.1.3.3"></minus><cn type="integer" id="S4.T3.2.2.2.m1.1.1.3.3.2.cmml" xref="S4.T3.2.2.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">3\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.2.12.10" class="ltx_tr">
<th id="S4.T3.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">GCN Initializer</th>
<td id="S4.T3.2.12.10.2" class="ltx_td ltx_align_center ltx_border_b">N/A</td>
<td id="S4.T3.2.12.10.3" class="ltx_td ltx_align_center ltx_border_b">Offset</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The loss used during training for both STEP and ST-Gait++ was Cross Entropy Loss. We used the Pytorch implementation <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Available at: <a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html</a></span></span></span>. Since the predictions for each sample were in the form of probabilities for each class, the Cross Entropy Loss can be defined as:</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.11" class="ltx_Math" alttext="l(x,y)=\frac{\sum_{n=1}^{N}(-\sum_{c=1}^{C}w_{c}log\frac{exp(x_{n,c})}{\sum_{i=1}^{C}exp(x_{n,i})}y_{n,c})}{N}" display="block"><semantics id="S4.Ex1.m1.11a"><mrow id="S4.Ex1.m1.11.12" xref="S4.Ex1.m1.11.12.cmml"><mrow id="S4.Ex1.m1.11.12.2" xref="S4.Ex1.m1.11.12.2.cmml"><mi id="S4.Ex1.m1.11.12.2.2" xref="S4.Ex1.m1.11.12.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.11.12.2.1" xref="S4.Ex1.m1.11.12.2.1.cmml">​</mo><mrow id="S4.Ex1.m1.11.12.2.3.2" xref="S4.Ex1.m1.11.12.2.3.1.cmml"><mo stretchy="false" id="S4.Ex1.m1.11.12.2.3.2.1" xref="S4.Ex1.m1.11.12.2.3.1.cmml">(</mo><mi id="S4.Ex1.m1.10.10" xref="S4.Ex1.m1.10.10.cmml">x</mi><mo id="S4.Ex1.m1.11.12.2.3.2.2" xref="S4.Ex1.m1.11.12.2.3.1.cmml">,</mo><mi id="S4.Ex1.m1.11.11" xref="S4.Ex1.m1.11.11.cmml">y</mi><mo stretchy="false" id="S4.Ex1.m1.11.12.2.3.2.3" xref="S4.Ex1.m1.11.12.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.11.12.1" xref="S4.Ex1.m1.11.12.1.cmml">=</mo><mfrac id="S4.Ex1.m1.9.9" xref="S4.Ex1.m1.9.9.cmml"><mrow id="S4.Ex1.m1.9.9.9" xref="S4.Ex1.m1.9.9.9.cmml"><msubsup id="S4.Ex1.m1.9.9.9.10" xref="S4.Ex1.m1.9.9.9.10.cmml"><mo id="S4.Ex1.m1.9.9.9.10.2.2" xref="S4.Ex1.m1.9.9.9.10.2.2.cmml">∑</mo><mrow id="S4.Ex1.m1.9.9.9.10.2.3" xref="S4.Ex1.m1.9.9.9.10.2.3.cmml"><mi id="S4.Ex1.m1.9.9.9.10.2.3.2" xref="S4.Ex1.m1.9.9.9.10.2.3.2.cmml">n</mi><mo id="S4.Ex1.m1.9.9.9.10.2.3.1" xref="S4.Ex1.m1.9.9.9.10.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.9.9.9.10.2.3.3" xref="S4.Ex1.m1.9.9.9.10.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex1.m1.9.9.9.10.3" xref="S4.Ex1.m1.9.9.9.10.3.cmml">N</mi></msubsup><mrow id="S4.Ex1.m1.9.9.9.9.1" xref="S4.Ex1.m1.9.9.9.9.1.1.cmml"><mo lspace="0em" stretchy="false" id="S4.Ex1.m1.9.9.9.9.1.2" xref="S4.Ex1.m1.9.9.9.9.1.1.cmml">(</mo><mrow id="S4.Ex1.m1.9.9.9.9.1.1" xref="S4.Ex1.m1.9.9.9.9.1.1.cmml"><mo id="S4.Ex1.m1.9.9.9.9.1.1a" xref="S4.Ex1.m1.9.9.9.9.1.1.cmml">−</mo><mrow id="S4.Ex1.m1.9.9.9.9.1.1.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.cmml"><msubsup id="S4.Ex1.m1.9.9.9.9.1.1.2.1" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.cmml"><mo id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.2.cmml">∑</mo><mrow id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.cmml"><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.2.cmml">c</mi><mo id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.1" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.3" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.1.3" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.3.cmml">C</mi></msubsup><mrow id="S4.Ex1.m1.9.9.9.9.1.1.2.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.cmml"><msub id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.cmml"><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.2.cmml">w</mi><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.3" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml">​</mo><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.3" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1a" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml">​</mo><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.4" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1b" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml">​</mo><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.5" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1c" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml">​</mo><mfrac id="S4.Ex1.m1.6.6.6.6" xref="S4.Ex1.m1.6.6.6.6.cmml"><mrow id="S4.Ex1.m1.3.3.3.3.3" xref="S4.Ex1.m1.3.3.3.3.3.cmml"><mi id="S4.Ex1.m1.3.3.3.3.3.5" xref="S4.Ex1.m1.3.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.3.3.3.3.3.4" xref="S4.Ex1.m1.3.3.3.3.3.4.cmml">​</mo><mi id="S4.Ex1.m1.3.3.3.3.3.6" xref="S4.Ex1.m1.3.3.3.3.3.6.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.3.3.3.3.3.4a" xref="S4.Ex1.m1.3.3.3.3.3.4.cmml">​</mo><mi id="S4.Ex1.m1.3.3.3.3.3.7" xref="S4.Ex1.m1.3.3.3.3.3.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.3.3.3.3.3.4b" xref="S4.Ex1.m1.3.3.3.3.3.4.cmml">​</mo><mrow id="S4.Ex1.m1.3.3.3.3.3.3.1" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.cmml"><mo stretchy="false" id="S4.Ex1.m1.3.3.3.3.3.3.1.2" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.cmml">(</mo><msub id="S4.Ex1.m1.3.3.3.3.3.3.1.1" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.cmml"><mi id="S4.Ex1.m1.3.3.3.3.3.3.1.1.2" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.2.cmml">x</mi><mrow id="S4.Ex1.m1.2.2.2.2.2.2.2.4" xref="S4.Ex1.m1.2.2.2.2.2.2.2.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml">n</mi><mo id="S4.Ex1.m1.2.2.2.2.2.2.2.4.1" xref="S4.Ex1.m1.2.2.2.2.2.2.2.3.cmml">,</mo><mi id="S4.Ex1.m1.2.2.2.2.2.2.2.2" xref="S4.Ex1.m1.2.2.2.2.2.2.2.2.cmml">c</mi></mrow></msub><mo stretchy="false" id="S4.Ex1.m1.3.3.3.3.3.3.1.3" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.cmml">)</mo></mrow></mrow><mrow id="S4.Ex1.m1.6.6.6.6.6" xref="S4.Ex1.m1.6.6.6.6.6.cmml"><mstyle displaystyle="false" id="S4.Ex1.m1.6.6.6.6.6.4" xref="S4.Ex1.m1.6.6.6.6.6.4.cmml"><msubsup id="S4.Ex1.m1.6.6.6.6.6.4a" xref="S4.Ex1.m1.6.6.6.6.6.4.cmml"><mo id="S4.Ex1.m1.6.6.6.6.6.4.2.2" xref="S4.Ex1.m1.6.6.6.6.6.4.2.2.cmml">∑</mo><mrow id="S4.Ex1.m1.6.6.6.6.6.4.2.3" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.cmml"><mi id="S4.Ex1.m1.6.6.6.6.6.4.2.3.2" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.2.cmml">i</mi><mo id="S4.Ex1.m1.6.6.6.6.6.4.2.3.1" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.6.6.6.6.6.4.2.3.3" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex1.m1.6.6.6.6.6.4.3" xref="S4.Ex1.m1.6.6.6.6.6.4.3.cmml">C</mi></msubsup></mstyle><mrow id="S4.Ex1.m1.6.6.6.6.6.3" xref="S4.Ex1.m1.6.6.6.6.6.3.cmml"><mi id="S4.Ex1.m1.6.6.6.6.6.3.3" xref="S4.Ex1.m1.6.6.6.6.6.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.6.3.2" xref="S4.Ex1.m1.6.6.6.6.6.3.2.cmml">​</mo><mi id="S4.Ex1.m1.6.6.6.6.6.3.4" xref="S4.Ex1.m1.6.6.6.6.6.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.6.3.2a" xref="S4.Ex1.m1.6.6.6.6.6.3.2.cmml">​</mo><mi id="S4.Ex1.m1.6.6.6.6.6.3.5" xref="S4.Ex1.m1.6.6.6.6.6.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.6.3.2b" xref="S4.Ex1.m1.6.6.6.6.6.3.2.cmml">​</mo><mrow id="S4.Ex1.m1.6.6.6.6.6.3.1.1" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.cmml"><mo stretchy="false" id="S4.Ex1.m1.6.6.6.6.6.3.1.1.2" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.cmml">(</mo><msub id="S4.Ex1.m1.6.6.6.6.6.3.1.1.1" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.cmml"><mi id="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.2" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.2.cmml">x</mi><mrow id="S4.Ex1.m1.5.5.5.5.5.2.2.4" xref="S4.Ex1.m1.5.5.5.5.5.2.2.3.cmml"><mi id="S4.Ex1.m1.4.4.4.4.4.1.1.1" xref="S4.Ex1.m1.4.4.4.4.4.1.1.1.cmml">n</mi><mo id="S4.Ex1.m1.5.5.5.5.5.2.2.4.1" xref="S4.Ex1.m1.5.5.5.5.5.2.2.3.cmml">,</mo><mi id="S4.Ex1.m1.5.5.5.5.5.2.2.2" xref="S4.Ex1.m1.5.5.5.5.5.2.2.2.cmml">i</mi></mrow></msub><mo stretchy="false" id="S4.Ex1.m1.6.6.6.6.6.3.1.1.3" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1d" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml">​</mo><msub id="S4.Ex1.m1.9.9.9.9.1.1.2.2.6" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.cmml"><mi id="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.2" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.2.cmml">y</mi><mrow id="S4.Ex1.m1.8.8.8.8.2.4" xref="S4.Ex1.m1.8.8.8.8.2.3.cmml"><mi id="S4.Ex1.m1.7.7.7.7.1.1" xref="S4.Ex1.m1.7.7.7.7.1.1.cmml">n</mi><mo id="S4.Ex1.m1.8.8.8.8.2.4.1" xref="S4.Ex1.m1.8.8.8.8.2.3.cmml">,</mo><mi id="S4.Ex1.m1.8.8.8.8.2.2" xref="S4.Ex1.m1.8.8.8.8.2.2.cmml">c</mi></mrow></msub></mrow></mrow></mrow><mo stretchy="false" id="S4.Ex1.m1.9.9.9.9.1.3" xref="S4.Ex1.m1.9.9.9.9.1.1.cmml">)</mo></mrow></mrow><mi id="S4.Ex1.m1.9.9.11" xref="S4.Ex1.m1.9.9.11.cmml">N</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.11b"><apply id="S4.Ex1.m1.11.12.cmml" xref="S4.Ex1.m1.11.12"><eq id="S4.Ex1.m1.11.12.1.cmml" xref="S4.Ex1.m1.11.12.1"></eq><apply id="S4.Ex1.m1.11.12.2.cmml" xref="S4.Ex1.m1.11.12.2"><times id="S4.Ex1.m1.11.12.2.1.cmml" xref="S4.Ex1.m1.11.12.2.1"></times><ci id="S4.Ex1.m1.11.12.2.2.cmml" xref="S4.Ex1.m1.11.12.2.2">𝑙</ci><interval closure="open" id="S4.Ex1.m1.11.12.2.3.1.cmml" xref="S4.Ex1.m1.11.12.2.3.2"><ci id="S4.Ex1.m1.10.10.cmml" xref="S4.Ex1.m1.10.10">𝑥</ci><ci id="S4.Ex1.m1.11.11.cmml" xref="S4.Ex1.m1.11.11">𝑦</ci></interval></apply><apply id="S4.Ex1.m1.9.9.cmml" xref="S4.Ex1.m1.9.9"><divide id="S4.Ex1.m1.9.9.10.cmml" xref="S4.Ex1.m1.9.9"></divide><apply id="S4.Ex1.m1.9.9.9.cmml" xref="S4.Ex1.m1.9.9.9"><apply id="S4.Ex1.m1.9.9.9.10.cmml" xref="S4.Ex1.m1.9.9.9.10"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.10.1.cmml" xref="S4.Ex1.m1.9.9.9.10">superscript</csymbol><apply id="S4.Ex1.m1.9.9.9.10.2.cmml" xref="S4.Ex1.m1.9.9.9.10"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.10.2.1.cmml" xref="S4.Ex1.m1.9.9.9.10">subscript</csymbol><sum id="S4.Ex1.m1.9.9.9.10.2.2.cmml" xref="S4.Ex1.m1.9.9.9.10.2.2"></sum><apply id="S4.Ex1.m1.9.9.9.10.2.3.cmml" xref="S4.Ex1.m1.9.9.9.10.2.3"><eq id="S4.Ex1.m1.9.9.9.10.2.3.1.cmml" xref="S4.Ex1.m1.9.9.9.10.2.3.1"></eq><ci id="S4.Ex1.m1.9.9.9.10.2.3.2.cmml" xref="S4.Ex1.m1.9.9.9.10.2.3.2">𝑛</ci><cn type="integer" id="S4.Ex1.m1.9.9.9.10.2.3.3.cmml" xref="S4.Ex1.m1.9.9.9.10.2.3.3">1</cn></apply></apply><ci id="S4.Ex1.m1.9.9.9.10.3.cmml" xref="S4.Ex1.m1.9.9.9.10.3">𝑁</ci></apply><apply id="S4.Ex1.m1.9.9.9.9.1.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1"><minus id="S4.Ex1.m1.9.9.9.9.1.1.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1"></minus><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2"><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.9.1.1.2.1.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1">superscript</csymbol><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1">subscript</csymbol><sum id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.2"></sum><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3"><eq id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.1"></eq><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.2">𝑐</ci><cn type="integer" id="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.3.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.1.3.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.1.3">𝐶</ci></apply><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2"><times id="S4.Ex1.m1.9.9.9.9.1.1.2.2.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.1"></times><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2">subscript</csymbol><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.2">𝑤</ci><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.3.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.2.3">𝑐</ci></apply><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.3.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.3">𝑙</ci><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.4.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.4">𝑜</ci><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.5.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.5">𝑔</ci><apply id="S4.Ex1.m1.6.6.6.6.cmml" xref="S4.Ex1.m1.6.6.6.6"><divide id="S4.Ex1.m1.6.6.6.6.7.cmml" xref="S4.Ex1.m1.6.6.6.6"></divide><apply id="S4.Ex1.m1.3.3.3.3.3.cmml" xref="S4.Ex1.m1.3.3.3.3.3"><times id="S4.Ex1.m1.3.3.3.3.3.4.cmml" xref="S4.Ex1.m1.3.3.3.3.3.4"></times><ci id="S4.Ex1.m1.3.3.3.3.3.5.cmml" xref="S4.Ex1.m1.3.3.3.3.3.5">𝑒</ci><ci id="S4.Ex1.m1.3.3.3.3.3.6.cmml" xref="S4.Ex1.m1.3.3.3.3.3.6">𝑥</ci><ci id="S4.Ex1.m1.3.3.3.3.3.7.cmml" xref="S4.Ex1.m1.3.3.3.3.3.7">𝑝</ci><apply id="S4.Ex1.m1.3.3.3.3.3.3.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.3.3.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.3.3.3.3.3.3.1.1.1.cmml" xref="S4.Ex1.m1.3.3.3.3.3.3.1">subscript</csymbol><ci id="S4.Ex1.m1.3.3.3.3.3.3.1.1.2.cmml" xref="S4.Ex1.m1.3.3.3.3.3.3.1.1.2">𝑥</ci><list id="S4.Ex1.m1.2.2.2.2.2.2.2.3.cmml" xref="S4.Ex1.m1.2.2.2.2.2.2.2.4"><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1">𝑛</ci><ci id="S4.Ex1.m1.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2.2.2.2.2">𝑐</ci></list></apply></apply><apply id="S4.Ex1.m1.6.6.6.6.6.cmml" xref="S4.Ex1.m1.6.6.6.6.6"><apply id="S4.Ex1.m1.6.6.6.6.6.4.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.6.6.6.6.6.4.1.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4">superscript</csymbol><apply id="S4.Ex1.m1.6.6.6.6.6.4.2.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.6.6.6.6.6.4.2.1.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4">subscript</csymbol><sum id="S4.Ex1.m1.6.6.6.6.6.4.2.2.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.2.2"></sum><apply id="S4.Ex1.m1.6.6.6.6.6.4.2.3.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3"><eq id="S4.Ex1.m1.6.6.6.6.6.4.2.3.1.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.1"></eq><ci id="S4.Ex1.m1.6.6.6.6.6.4.2.3.2.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.2">𝑖</ci><cn type="integer" id="S4.Ex1.m1.6.6.6.6.6.4.2.3.3.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.2.3.3">1</cn></apply></apply><ci id="S4.Ex1.m1.6.6.6.6.6.4.3.cmml" xref="S4.Ex1.m1.6.6.6.6.6.4.3">𝐶</ci></apply><apply id="S4.Ex1.m1.6.6.6.6.6.3.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3"><times id="S4.Ex1.m1.6.6.6.6.6.3.2.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.2"></times><ci id="S4.Ex1.m1.6.6.6.6.6.3.3.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.3">𝑒</ci><ci id="S4.Ex1.m1.6.6.6.6.6.3.4.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.4">𝑥</ci><ci id="S4.Ex1.m1.6.6.6.6.6.3.5.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.5">𝑝</ci><apply id="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.1.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1">subscript</csymbol><ci id="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.2.cmml" xref="S4.Ex1.m1.6.6.6.6.6.3.1.1.1.2">𝑥</ci><list id="S4.Ex1.m1.5.5.5.5.5.2.2.3.cmml" xref="S4.Ex1.m1.5.5.5.5.5.2.2.4"><ci id="S4.Ex1.m1.4.4.4.4.4.1.1.1.cmml" xref="S4.Ex1.m1.4.4.4.4.4.1.1.1">𝑛</ci><ci id="S4.Ex1.m1.5.5.5.5.5.2.2.2.cmml" xref="S4.Ex1.m1.5.5.5.5.5.2.2.2">𝑖</ci></list></apply></apply></apply></apply><apply id="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.6"><csymbol cd="ambiguous" id="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.1.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.6">subscript</csymbol><ci id="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.2.cmml" xref="S4.Ex1.m1.9.9.9.9.1.1.2.2.6.2">𝑦</ci><list id="S4.Ex1.m1.8.8.8.8.2.3.cmml" xref="S4.Ex1.m1.8.8.8.8.2.4"><ci id="S4.Ex1.m1.7.7.7.7.1.1.cmml" xref="S4.Ex1.m1.7.7.7.7.1.1">𝑛</ci><ci id="S4.Ex1.m1.8.8.8.8.2.2.cmml" xref="S4.Ex1.m1.8.8.8.8.2.2">𝑐</ci></list></apply></apply></apply></apply></apply><ci id="S4.Ex1.m1.9.9.11.cmml" xref="S4.Ex1.m1.9.9.11">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.11c">l(x,y)=\frac{\sum_{n=1}^{N}(-\sum_{c=1}^{C}w_{c}log\frac{exp(x_{n,c})}{\sum_{i=1}^{C}exp(x_{n,i})}y_{n,c})}{N}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.6" class="ltx_p">Where <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mi id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><ci id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">l</annotation></semantics></math> is the loss, <math id="S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS3.p4.2.m2.1a"><mi id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><ci id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">x</annotation></semantics></math> is the input, <math id="S4.SS3.p4.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS3.p4.3.m3.1a"><mi id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><ci id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">y</annotation></semantics></math> is the target, <math id="S4.SS3.p4.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S4.SS3.p4.4.m4.1a"><mi id="S4.SS3.p4.4.m4.1.1" xref="S4.SS3.p4.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.4.m4.1b"><ci id="S4.SS3.p4.4.m4.1.1.cmml" xref="S4.SS3.p4.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.4.m4.1c">w</annotation></semantics></math> is the weight <math id="S4.SS3.p4.5.m5.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.p4.5.m5.1a"><mi id="S4.SS3.p4.5.m5.1.1" xref="S4.SS3.p4.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.5.m5.1b"><ci id="S4.SS3.p4.5.m5.1.1.cmml" xref="S4.SS3.p4.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.5.m5.1c">C</annotation></semantics></math> is the number of classes and <math id="S4.SS3.p4.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p4.6.m6.1a"><mi id="S4.SS3.p4.6.m6.1.1" xref="S4.SS3.p4.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.6.m6.1b"><ci id="S4.SS3.p4.6.m6.1.1.cmml" xref="S4.SS3.p4.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.6.m6.1c">N</annotation></semantics></math> is the minibatch size.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and discussion</h2>

<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/cm-stgait.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="574" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/cm-stepb.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="501" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Confusion matrices generated from evaluating the models on the test set for (a) ST-Gait++ and (b) STEP. As can be seen, there is a more pronounced diagonal on (a), emphasizing the better accuracy of ST-Gait++. Also, there is less confusion between the emotions Neutral and Happy on ST-Gait++ than on STEP. However, there is some increase in confusion between Happy and Sad on ST-Gait++.</span></figcaption>
</figure>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Quantitative analysis.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.3" class="ltx_p">We compare our results with other different approaches in <a href="#S5.T5" title="Table 5 ‣ Quantitative analysis. ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a>. Our proposed method outperforms other graph-related methods, such as STEP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. It is interesting to notice that our implementation of this work has led to an increased accuracy than their reported results. In this case, our model has had an increase of <math id="S5.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="5.4\%" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">5.4</mn><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">5.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">5.4\%</annotation></semantics></math> regarding their result, and <math id="S5.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="4.2\%" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">4.2</mn><mo id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">4.2\%</annotation></semantics></math> regarding our implementation. Besides the accuracy increase, our model was also able to converge faster, highlighting several improvements, such as fewer requirements for computational resources or training time, increased possibilities for scaling, and better generalization of data. ST-Gait++ converged on epoch #127, while STEP converged on epoch #462, a reduction of <math id="S5.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="72\%" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S5.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">72</mn><mo id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2">72</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.3.m3.1c">72\%</annotation></semantics></math> in training time. A runtime analysis was also performed, over the test set, and can be found on table <a href="#S5.T4" title="Table 4 ‣ Quantitative analysis. ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>. As observed, STEP has a better time for inference, which can be explained by the simpler architecture.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.6.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.7.2" class="ltx_text" style="font-size:90%;">Runtime analysis on the test set, which has a total of 216 samples. The test was performed on a laptop with a NVIDIA GeForce RTX 4050 GPU.</span></figcaption>
<div id="S5.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:37.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.9pt,8.2pt) scale(0.69785409592919,0.69785409592919) ;">
<table id="S5.T4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.4.4.5.1" class="ltx_tr">
<th id="S5.T4.4.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T4.4.4.5.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S5.T4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.4.5.1.2.1" class="ltx_text ltx_font_bold">Min Inference Time</span></th>
<th id="S5.T4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T4.4.4.5.1.3.1" class="ltx_text ltx_font_bold">Average Inference Time</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2.2" class="ltx_tr">
<th id="S5.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">STEP</th>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">0.99ms (<math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\sim</annotation></semantics></math>1001 fps)</td>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">3.28ms (<math id="S5.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T4.2.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.2.m1.1.1" xref="S5.T4.2.2.2.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\sim</annotation></semantics></math>304 fps)</td>
</tr>
<tr id="S5.T4.4.4.4" class="ltx_tr">
<th id="S5.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">ST-Gait++ (ours)</th>
<td id="S5.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_b">4.64ms (<math id="S5.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T4.3.3.3.1.m1.1a"><mo id="S5.T4.3.3.3.1.m1.1.1" xref="S5.T4.3.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\sim</annotation></semantics></math>215 fps)</td>
<td id="S5.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_b">15.08ms (<math id="S5.T4.4.4.4.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T4.4.4.4.2.m1.1a"><mo id="S5.T4.4.4.4.2.m1.1.1" xref="S5.T4.4.4.4.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S5.T4.4.4.4.2.m1.1.1.cmml" xref="S5.T4.4.4.4.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.2.m1.1c">\sim</annotation></semantics></math>66 fps)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">Our model also highly outperforms other approaches with a temporal focus, such as <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Randhavane et al.</span> [<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>. In this case, we report an accuracy increase of <math id="S5.SS0.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="6.8\%" display="inline"><semantics id="S5.SS0.SSS0.Px1.p2.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml">6.8</mn><mo id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.2">6.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.1.m1.1c">6.8\%</annotation></semantics></math>. Overall, this quantitative evaluation highlights that our model not only has increased accuracy in relation to the current state-of-the-art, but is also more optimized towards training requirements.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.3.2" class="ltx_text" style="font-size:90%;">Quantitative analysis of methods for emotion recognition using the E-Gait dataset.</span></figcaption>
<div id="S5.T5.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:98.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-110.4pt,49.9pt) scale(0.495501662626894,0.495501662626894) ;">
<table id="S5.T5.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.4.1.1.1" class="ltx_tr">
<th id="S5.T5.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.1.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#FFFFFF;">Methods</span></th>
<th id="S5.T5.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.1.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#FFFFFF;">Acc (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.4.1.2.1" class="ltx_tr">
<th id="S5.T5.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Venture et al.</span></a></cite><span id="S5.T5.4.1.2.1.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.2.1.2.1" class="ltx_text" style="background-color:#FFFFFF;">30.8</span></td>
</tr>
<tr id="S5.T5.4.1.3.2" class="ltx_tr">
<th id="S5.T5.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Daoudi et al.</span></a></cite><span id="S5.T5.4.1.3.2.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.3.2.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.3.2.2.1" class="ltx_text" style="background-color:#FFFFFF;">42.5</span></td>
</tr>
<tr id="S5.T5.4.1.4.3" class="ltx_tr">
<th id="S5.T5.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Li et al.</span></a></cite><span id="S5.T5.4.1.4.3.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.4.3.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.4.3.2.1" class="ltx_text" style="background-color:#FFFFFF;">53.7</span></td>
</tr>
<tr id="S5.T5.4.1.5.4" class="ltx_tr">
<th id="S5.T5.4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Crenn et al.</span></a></cite><span id="S5.T5.4.1.5.4.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.5.4.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.5.4.2.1" class="ltx_text" style="background-color:#FFFFFF;">66.2</span></td>
</tr>
<tr id="S5.T5.4.1.6.5" class="ltx_tr">
<th id="S5.T5.4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Randhavane et al.</span></a></cite><span id="S5.T5.4.1.6.5.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019a</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.6.5.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.6.5.2.1" class="ltx_text" style="background-color:#FFFFFF;">80.7</span></td>
</tr>
<tr id="S5.T5.4.1.7.6" class="ltx_tr">
<th id="S5.T5.4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Narayanan et al.</span></a></cite><span id="S5.T5.4.1.7.6.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.7.6.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.7.6.2.1" class="ltx_text" style="background-color:#FFFFFF;">82.4</span></td>
</tr>
<tr id="S5.T5.4.1.8.7" class="ltx_tr">
<th id="S5.T5.4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Bhattacharya et al.</span></a></cite><span id="S5.T5.4.1.8.7.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a></cite>) (STEP)</span>
</th>
<td id="S5.T5.4.1.8.7.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.8.7.2.1" class="ltx_text" style="background-color:#FFFFFF;">82.1</span></td>
</tr>
<tr id="S5.T5.4.1.9.8" class="ltx_tr">
<th id="S5.T5.4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.9.8.1.1" class="ltx_text" style="background-color:#FFFFFF;">Bhattacharya et al. (2020) (STEP) (Our implementation)</span></th>
<td id="S5.T5.4.1.9.8.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.9.8.2.1" class="ltx_text" style="background-color:#FFFFFF;">83.3</span></td>
</tr>
<tr id="S5.T5.4.1.10.9" class="ltx_tr">
<th id="S5.T5.4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="background-color:#FFFFFF;">
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">YuMeng et al.</span></a></cite><span id="S5.T5.4.1.10.9.1.1" class="ltx_text" style="background-color:#FFFFFF;"> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a></cite>)</span>
</th>
<td id="S5.T5.4.1.10.9.2" class="ltx_td ltx_align_center" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.10.9.2.1" class="ltx_text" style="background-color:#FFFFFF;">85.2</span></td>
</tr>
<tr id="S5.T5.4.1.11.10" class="ltx_tr">
<th id="S5.T5.4.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.11.10.1.1" class="ltx_text ltx_font_bold" style="background-color:#FFFFFF;">ST-Gait++ (Ours)</span></th>
<td id="S5.T5.4.1.11.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="background-color:#FFFFFF;"><span id="S5.T5.4.1.11.10.2.1" class="ltx_text ltx_font_bold" style="background-color:#FFFFFF;">87.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p3.1" class="ltx_p">We have also generated confusion matrices for ST-Gait++ and STEP in order to compare how the accuracy of these two models can be represented in how they perceive the emotional classes differently. We show these results in <a href="#S5.F3" title="Figure 3 ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. As expected, this result reaffirm the results we have discussed before; STEP has a higher confusion between <span id="S5.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_italic">Neutral</span> and <span id="S5.SS0.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_italic">Happy</span> than our model. In the case of facial expressions, the literature on behavioral psychology has shown that there is a structural resemblance between neutral and happy faces that could lead to confusion, and it is common to have this type of ambiguity in these tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>; however, there is still no evidence that the same could happen to gait perception. Therefore, it is difficult to judge if STEP’s ambiguity between these two classes could be justified. In our model, we also notice an ambiguity between Sad and Happy. We do not have any insights regarding this curious behavior at this point, but we also highlight that our accuracy for the Sad class is still higher than the reported for STEP.</p>
</div>
<div id="S5.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p4.1" class="ltx_p">Finally, we have also applied a t-SNE representation of the features extracted from the last layer of the models to visualize and analyze the learned representations of the data. The last layer usually contains higher-level, abstract features of the input data, which could usually be represented by activation maps or feature maps. In this case, since we are not directly working with images at that point, applying t-SNE could reveal clusters or groups that could highlight the ability of the model to distinguish classes or categories effectively, as well as to identify outliers or anomalies in the data. We show in <a href="#S5.F4" title="Figure 4 ‣ Qualitative analysis. ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> that the Angry class has a very distinct and separated clustering, which explains the better performance in this class as we have shown before in <a href="#S5.F3" title="Figure 3 ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. This better performance can also be attributed to the far larger amount of samples for this category, making the model better able to separate it from the rest. Neutral and Happy categories are distinguishable enough to show why they also show a good performance, while Sad, the least numerous category, is very mingled with the others.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Qualitative analysis.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">In <a href="#S5.F5" title="Figure 5 ‣ Qualitative analysis. ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a> we show some qualitative visualizations with examples of correct and incorrect predictions. By looking just at the skeletons it’s impossible to distinguish the emotions. For some, we may agree with the annotation provided and understand why the model made a correct prediction. For others, it’s dubious to infer the perceived emotion with just the skeleton, and we can understand why the model made a mistake. On (a) we can see the wide stride, arm swinging and attribute that to happiness, but we can also see the somewhat lowered head, which could indicate sadness. On (b) we can se a fast stride and arm swinging that could indicate some more energetic emotion, but the model still correctly classifies it as sadness. (c) gives us a lowered head, which could have lead the model to infer on sadness, even though the swinging arms and big stride indicate anger. Lastly, (d) shows a fast stride with slightly swinging arms and upperbody which could have misled the model into inferring happiness and not some neutral emotion. Overall, we can understand that there is some dubiousness in the data that leads to explainable mistakes.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">However, given the temporality aspect of this evaluation, we are limited to what we can show in this research paper. We have prepared a video with a more in-depth overview available at to be made available upon publication.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.13903/assets/images/output-features-step.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="468" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.13903/assets/images/output-features-stgait.png" id="S5.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="470" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">T-SNE representation of the features extracted from the last convolutional layer of ST-Gait++ and STEP on the test set, The inner color represents the ground truth labels and the outer circle represents the model’s inference.</span></figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/GT_Happy_STGAIT_Happy_37-crop.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Ground Truth: Happy — ST-Gait++ Prediction: Happy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/GT_Sad_STGAIT_Sad_53-crop.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Ground Truth: Sad — ST-Gait++ Prediction: Sad</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/GT_Angry_STGAIT_Sad_121-crop.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Ground Truth: Angry — ST-Gait++ Prediction: Sad</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13903/assets/images/GT_Neutral_STGAIT_Happy_34-crop.png" id="S5.F5.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">Ground Truth: Neutral — ST-Gait++ Prediction: Happy</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Examples of correct and incorrect inferences by ST-Gait++ on the test set.</span></figcaption>
</figure>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations of the E-Gait dataset.</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">To the best of our knowledge, E-Gait is one of the only public datasets of emotion recognition through gait, alongside Emotion-Walk (E-Walk) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>. However, since they have data overlap, we decided to test on E-Gait, given that it is the one used by the state-of-the-art approaches of emotion recognition from gait.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p2.1" class="ltx_p">In addition, the E-Gait dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> is very obscure, with only the skeletons being available to researchers. Because of this, the annotations cannot be confirmed and neither can we know the transformations taken to process the videos and the skeletons which are already normalized. This can be observed in <a href="#S5.F5" title="Figure 5 ‣ Qualitative analysis. ‣ 5 Results and discussion ‣ ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>. Furthermore, the dataset does not disclose the demographics of the subjects and a great part of the data comes from the capture with a single individual and adds a heavy bias. For an application in other contexts, such as Latin American contexts, for example, it would be very interesting to have a dataset that could show the local cultural emotion expression. Also, publishing an open dataset also containing the original videos, not only the skeletons, would give researchers a higher freedom for experimenting, validating ideas and checking for biases to correct.</p>
</div>
<div id="S5.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p3.1" class="ltx_p">The first step to making such a dataset would be to gather other available datasets for emotion recognition through gait, or curate new ones, focusing on bringing high quality data that is representative, diverse and as unbiased as possible. Testing ST-Gait++ on more data will certainly point to new paths of improvement for this research.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Diversity and Bias in emotion recognition related datasets.</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">There are many studies focused on trying to find biases in intelligent systems. One such study is <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Rhue</span> [<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, which found racial disparities in facial emotion recognition and raised the question of whether artificial inteligence could in fact determine emotion better than people. To answer this question, we need to understand that demographics such as gender, race and ethnicity heavily influence the perception of human characteristics, such as in facial emotion recognition algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. But this is also due to the fact that we as humans perceive the world based on our own biases. To annotate the data that will be fed into algorithms is to accept that the data will have biases that the model will propagate. Studies such as <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Pahl et al.</span> [<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> bring to attention the age bias, besides the ethnicity bias, in prominent Action Unit Datasets. The algorithms studied had problems with glasses but not with beards. This shows data collection problem: Are there no diverse people available or are these people not even considered as a possible variation in the target user public? These limitations can be extended to gaits, as it is a characteristic and emotion expression outlet that can vary across different demographics.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p2.1" class="ltx_p">Furthermore, although some emotion expression may have universal features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, it is noted that cultural particularities are very influential in affective cues encoding <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kleinsmith and Bianchi-Berthouze</span> [<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>. For a general application, having an analysis on the demographic characteristics can help researchers gain more insight on the limitations of the data and, as a consequence, of the real world application of their research. As such, having such a skewed dataset, such as ELMD (<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Habibie et al.</span> [<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>), being a considerable part of the total data available, brings to question the applicability of the entire dataset in in-the-wild scenarios.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p3.2" class="ltx_p">Given the ELMD dataset, of the <math id="S5.SS0.SSS0.Px4.p3.1.m1.1" class="ltx_Math" alttext="342" display="inline"><semantics id="S5.SS0.SSS0.Px4.p3.1.m1.1a"><mn id="S5.SS0.SSS0.Px4.p3.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p3.1.m1.1.1.cmml">342</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p3.1.m1.1b"><cn type="integer" id="S5.SS0.SSS0.Px4.p3.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p3.1.m1.1.1">342</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p3.1.m1.1c">342</annotation></semantics></math> samples recorded, 90 participants were involved, with no demographic of this public being disclosed. It is disclosed that the data, all <math id="S5.SS0.SSS0.Px4.p3.2.m2.2" class="ltx_Math" alttext="1,835" display="inline"><semantics id="S5.SS0.SSS0.Px4.p3.2.m2.2a"><mrow id="S5.SS0.SSS0.Px4.p3.2.m2.2.3.2" xref="S5.SS0.SSS0.Px4.p3.2.m2.2.3.1.cmml"><mn id="S5.SS0.SSS0.Px4.p3.2.m2.1.1" xref="S5.SS0.SSS0.Px4.p3.2.m2.1.1.cmml">1</mn><mo id="S5.SS0.SSS0.Px4.p3.2.m2.2.3.2.1" xref="S5.SS0.SSS0.Px4.p3.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS0.SSS0.Px4.p3.2.m2.2.2" xref="S5.SS0.SSS0.Px4.p3.2.m2.2.2.cmml">835</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p3.2.m2.2b"><list id="S5.SS0.SSS0.Px4.p3.2.m2.2.3.1.cmml" xref="S5.SS0.SSS0.Px4.p3.2.m2.2.3.2"><cn type="integer" id="S5.SS0.SSS0.Px4.p3.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px4.p3.2.m2.1.1">1</cn><cn type="integer" id="S5.SS0.SSS0.Px4.p3.2.m2.2.2.cmml" xref="S5.SS0.SSS0.Px4.p3.2.m2.2.2">835</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p3.2.m2.2c">1,835</annotation></semantics></math> samples, was collected using a sole male actor. With this in mind, it’s important to point out the importance of diversity and fairness in this data collection.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p4" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p4.1" class="ltx_p">Also, it’s important to emphasize that what is being dealt with here are the perceived emotions, since whatever actual emotion was being felt by the person at the time of data capture is only available if the person is questioned (in real scenarios) or if we know what they are trying to convey (actors portraying emotions). Even in real scenarios, the reported emotion after questioning could be biased due to the type of question asked or recall biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we proposed ST-Gait++, a novel framework for emotion recognition from gait. Its skeleton-based spatio-temporal representation approach results in state-of-the-art classification performance on the E-Gait dataset. We also discussed some of the limitations of the field with the objective of presenting research opportunities.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In addition, given the faster training convergence on a consumer grade laptop, we expect ST-Gait++ to provide new research opportunities in the field of human behaviour analysis for researchers with a lower budget or limited resources.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Future work will explore the relevance of the different body parts for recognising emotion and the inclusion of additional gait descriptors.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bhattacharya et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Step: Spatial temporal graph convolutional networks for emotion perception from gaits.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 1342–1350, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.4.4.1" class="ltx_text" style="font-size:90%;">Buck [1991]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">
R Buck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">Motivation, emotion and cognition: A developmental-interactionist view.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International review of studies on emotion</em><span id="bib.bib2.9.2" class="ltx_text" style="font-size:90%;">, 1:101–142, 1991.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">Buolamwini [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
Joy Adowaa Buolamwini.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers</em><span id="bib.bib3.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">PhD thesis, Massachusetts Institute of Technology, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Costa et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Willams Costa, David Macêdo, Cleber Zanchettin, Estefanía Talavera, Lucas Silva Figueiredo, and Veronica Teichrieb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">A fast multiple cue fusing approach for human emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Available at SSRN 4255748</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Costa et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Willams Costa, Estefanía Talavera, Renato Oliveira, Lucas Figueiredo, João Marcelo Teixeira, João Paulo Lima, and Veronica Teichrieb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">A survey on datasets for emotion recognition from vision: Limitations and in-the-wild applicability.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Sciences</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 13(9):5697, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Crenn et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Arthur Crenn, Rizwan Ahmed Khan, Alexandre Meyer, and Saida Bouakaz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Body expression recognition from animated 3d skeleton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 International Conference on 3D Imaging (IC3D)</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 1–7. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Daoudi et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Mohamed Daoudi, Stefano Berretti, Pietro Pala, Yvonne Delevoye, and Alberto Del Bimbo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Image Analysis and Processing-ICIAP 2017: 19th International Conference, Catania, Italy, September 11-15, 2017, Proceedings, Part I 19</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 550–560. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Dhuheir et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Marwan Dhuheir, Abdullatif Albaseer, Emna Baccour, Aiman Erbad, Mohamed Abdallah, and Mounir Hamdi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Emotion recognition for healthcare surveillance systems using neural networks: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 International Wireless Communications and Mobile Computing (IWCMC)</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 681–687, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Duan et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Haodong Duan, Jiaqi Wang, Kai Chen, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Pyskl: Towards good practices for skeleton action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 30th ACM International Conference on Multimedia</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 7351–7354, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Grant and Williams [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
Dawn Grant and David Williams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">The importance of perceiving social contexts when predicting crime and antisocial behaviour in cctv images.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Legal and Criminological Psychology</em><span id="bib.bib10.9.2" class="ltx_text" style="font-size:90%;">, 16(2):307–322, 2011.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Habibie et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and Taku Komura.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">A recurrent variational autoencoder for human motion synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference (BMVC)</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Hernandez et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Javier Hernandez, Josh Lovejoy, Daniel McDuff, Jina Suh, Tim O’Brien, Arathi Sethumadhavan, Gretchen Greene, Rosalind Picard, and Mary Czerwinski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Guidelines for assessing and minimizing risks of emotion recognition applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 9th International conference on affective computing and intelligent interaction (ACII)</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 1–8. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Jacob et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Heike Jacob, Benjamin Kreifelts, Sophia Nizielski, Astrid Schütz, and Dirk Wildgruber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Effects of emotional intelligence on the impression of irony created by the mismatch between verbal and nonverbal cues.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">PloS one</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 11(10):e0163211, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">Kleinsmith and Bianchi-Berthouze [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Andrea Kleinsmith and Nadia Bianchi-Berthouze.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Affective body expression perception and recognition: A survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib14.9.2" class="ltx_text" style="font-size:90%;">, 4(1):15–33, 2012.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">Lhommet and Marsella [2014a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
Margaux Lhommet and Stacy C Marsella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">19 expressing emotion through posture and gesture.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Oxford handbook of affective computing</em><span id="bib.bib15.9.2" class="ltx_text" style="font-size:90%;">, page 273, 2014a.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.4.4.1" class="ltx_text" style="font-size:90%;">Lhommet and Marsella [2014b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">
Margaux Lhommet and Stacy C Marsella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">Expressing emotion through posture.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Oxford handbook of affective computing</em><span id="bib.bib16.9.2" class="ltx_text" style="font-size:90%;">, 273, 2014b.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Baobin Li, Changye Zhu, Shun Li, and Tingshao Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Identifying emotions from non-contact gaits information based on microsoft kinects.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 9(4):585–591, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Montepare et al. [1987]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Joann M Montepare, Sabra B Goldstein, and Annmarie Clausen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">The identification of emotions from gait information.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Nonverbal Behavior</em><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 11:33–42, 1987.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Narayanan et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Venkatraman Narayanan, Bala Murali Manoghar, Vishnu Sashank Dorbala, Dinesh Manocha, and Aniket Bera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 8200–8207. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Nes et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Anders Nes, Kristoffer Sundberg, and Sebastian Watzl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">The perception/cognition distinction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Inquiry</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 66(2):165–195, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Pahl et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Jaspar Pahl, Ines Rieger, Anna Möller, Thomas Wittenberg, and Ute Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Female, white, 27? bias evaluation on data and algorithms for affect recognition in faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages 973–987, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Randhavane et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray, Aniket Bera, and Dinesh Manocha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Identifying emotions from walking using affective and deep features.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.11884</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Randhavane et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray, Aniket Bera, and Dinesh Manocha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Learning perceived emotion using affective and deep features for mental health applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 395–399. IEEE, 2019b.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.4.4.1" class="ltx_text" style="font-size:90%;">Rhue [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">
Lauren Rhue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">Racial influence on automated perceptions of emotions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Available at SSRN 3281765</em><span id="bib.bib24.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Roether et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Claire L Roether, Lars Omlor, Andrea Christensen, and Martin A Giese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Critical features for the perception of emotion from gait.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of vision</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 9(6):15–15, 2009.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Said et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Christopher P Said, Nicu Sebe, and Alexander Todorov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Structural resemblance to emotional expressions predicts evaluation of emotionally neutral faces.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Emotion</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 9(2):260, 2009.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Thuseethan et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Selvarajah Thuseethan, Sutharshan Rajasegarar, and John Yearwood.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Emosec: Emotion recognition from scene context.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, 492:174–187, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.4.4.1" class="ltx_text" style="font-size:90%;">Tracy and Matsumoto [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">
Jessica L Tracy and David Matsumoto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">The spontaneous expression of pride and shame: Evidence for biologically innate nonverbal displays.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the National Academy of Sciences</em><span id="bib.bib28.9.2" class="ltx_text" style="font-size:90%;">, 105(33):11655–11660, 2008.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Venture et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Gentiane Venture, Hideki Kadone, Tianxiang Zhang, Julie Grèzes, Alain Berthoz, and Halim Hicheur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Recognizing emotions conveyed by human gait.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Social Robotics</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 6:621–632, 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.4.4.1" class="ltx_text" style="font-size:90%;">Viderman and Knierbein [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">
Tihomir Viderman and Sabine Knierbein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">Correction to: affective urbanism: towards inclusive design praxis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Urban Design International</em><span id="bib.bib30.9.2" class="ltx_text" style="font-size:90%;">, 25:192–202, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.4.4.1" class="ltx_text" style="font-size:90%;">Wallbott and Scherer [1986]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">
Harald G Wallbott and Klaus R Scherer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">Cues and channels in emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of personality and social psychology</em><span id="bib.bib31.9.2" class="ltx_text" style="font-size:90%;">, 51(4):690, 1986.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Yan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Sijie Yan, Yuanjun Xiong, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Spatial temporal graph convolutional networks for skeleton-based action recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 32(1), 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Dingkang Yang, Shuai Huang, Shunli Wang, Yang Liu, Peng Zhai, Liuzhen Su, Mingcheng Li, and Lihua Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Emotion recognition for multiple context awareness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 144–162. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">YuMeng et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Zhao YuMeng, Liu Zhen, Liu TingTing, Wang YuanYi, and Chai YanJie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Affective-pose gait: perceiving emotions from gaits with body pose and human affective prior knowledge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Multimedia Tools and Applications</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 83(2):5327–5350, 2024.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13902" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13903" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13903">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13903" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13904" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 13:33:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
