<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.10488] Joint Multimodal Transformer for Emotion Recognition in the Wild</title><meta property="og:description" content="Systems for multimodal emotion recognition (MMER) can typically outperform unimodal systems by leveraging the inter- and intra-modal relationships between, e.g., visual, textual, physiological, and auditory modalities.…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Joint Multimodal Transformer for Emotion Recognition in the Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Joint Multimodal Transformer for Emotion Recognition in the Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.10488">

<!--Generated on Fri Apr  5 17:00:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Joint Multimodal Transformer for Emotion Recognition in the Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Paul Waligora<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup>,
 Haseeb Aslam<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">1</span></sup>,
 Osama Zeeshan<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">1</span></sup>,
 Soufiane Belharbi<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">1</span></sup>,
 Alessandro Lameiras Koerich<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1</span></sup>,

<br class="ltx_break"> <span id="id6.6.1" class="ltx_text ltx_font_bold">Marco Pedersoli<sup id="id6.6.1.1" class="ltx_sup"><span id="id6.6.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>,
 <span id="id7.7.2" class="ltx_text ltx_font_bold">Simon Bacon<sup id="id7.7.2.1" class="ltx_sup"><span id="id7.7.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>, and
 <span id="id9.9.4" class="ltx_text ltx_font_bold">Eric Granger<sup id="id9.9.4.1" class="ltx_sup"><span id="id9.9.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id9.9.4.2" class="ltx_sup"><span id="id9.9.4.2.1" class="ltx_text ltx_font_medium">1</span></sup></span> LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada 
<br class="ltx_break"><sup id="id16.16.id6" class="ltx_sup">2</sup> Dept. of Health, Kinesiology &amp; Applied Physiology, Concordia University, Montreal, Canada
<br class="ltx_break"><span id="id17.17.id7" class="ltx_text ltx_font_typewriter" style="color:#000000;">paul.waligora.1@ens.etsmtl.ca<span id="id17.17.id7.1" class="ltx_text" style="color:#000000;"> </span></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">Systems for multimodal emotion recognition (MMER) can typically outperform unimodal systems by leveraging the inter- and intra-modal relationships between, e.g., visual, textual, physiological, and auditory modalities.
In this paper, an MMER method is proposed that relies on a joint multimodal transformer for fusion with key-based cross-attention. This framework aims to exploit the diverse and complementary nature of different modalities to improve predictive accuracy. Separate backbones capture intra-modal spatiotemporal dependencies within each modality over video sequences. Subsequently, a joint multimodal transformer fusion architecture integrates the individual modality embeddings, allowing the model to capture inter-modal and intra-modal relationships effectively.
Extensive experiments<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code: <span id="footnote1.1" class="ltx_ERROR undefined">\href</span>https://github.com/PoloWlg/Joint-Multimodal-Transformer-6th-ABAWhttps://github.com/PoloWlg/Joint-Multimodal-Transformer-6th-ABAW.</span></span></span> on two challenging expression recognition tasks: (1) dimensional emotion recognition on the Affwild2 dataset (with face and voice), and (2) pain estimation on the Biovid dataset (with face and biosensors), indicate that the proposed method can work effectively with different modalities. Empirical results show that MMER systems with our proposed fusion method allow us to outperform relevant baseline and state-of-the-art methods.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold">Keywords:</span> Affective Computing, Multi-Modal Fusion, Audio-Visual Fusion, Transformers, Cross-Attention, Valence-Arousal Estimation.</p>
</div>
<figure id="S0.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S0.F1.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\includegraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S0.F1.2" class="ltx_p ltx_figure_panel ltx_align_center">[scale=0.8]JMT_top_4.png</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(Top) Illustration of the vanilla multimodal transformer fusion architecture. (Bottom) Multimodal transformer fusion with joint representation (in red) for two input sources, A and B.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Human-computer interaction is applied in a wide range of real-world scenarios, e.g., health care, IoT, and autonomous driving. Researchers have classified human emotions in different ways, most notably according to discrete categories, ordinal intensity levels, and the valence/arousal circumplex <cite class="ltx_cite ltx_citemacro_cite">Anagnostopoulos et al., (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite>. With the recent advancements in deep learning and sensor technologies, research in affective computing has evolved from lab-controlled to real-world (in the wild) scenarios. In the latter, human emotions are usually expressed over a broader spectrum beyond the six basic categorical expressions - anger, disgust, fear, happy, sad, and surprise <cite class="ltx_cite ltx_citemacro_cite">Ekman, (<a href="#bib.bib9" title="" class="ltx_ref">1992</a>)</cite>. Therefore, there is much interest in analyzing and modeling complex and subtle expressions of emotions in real-world scenarios. For instance, the spectrum of emotions can be formulated as dimensional emotional recognition (ER), where complex human emotions are represented along the axes of arousal (intensity) and valence (positiveness).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Multimodal fusion has been widely explored for the problem of video-based ER in the literature <cite class="ltx_cite ltx_citemacro_cite">Schoneveld et al., (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>); <a href="#bib.bib28" title="" class="ltx_ref">Kuhnke et al., 2020a </a></cite>. For instance, audio and visual modalities may provide complementary and redundant information over a video sequence. These relationships must be captured to model the intricacies of human emotions effectively. Furthermore, effectively capturing both the intra-modal temporal dependencies within the audio and visual modalities and the inter-modal association across the audio and visual modalities is crucial in developing an effective AER system<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">Praveen et al., 2022b </a></cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Several methods have been proposed for video-based ER and recurrent networks have been employed to capture the intra-modal temporal dependencies from video sequences <cite class="ltx_cite ltx_citemacro_cite">Schoneveld et al., (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>); <a href="#bib.bib28" title="" class="ltx_ref">Kuhnke et al., 2020a </a>; de Melo et al., (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. Recently, attention-based methods have been introduced to extract features that are the most relevant to downstream tasks. Cross-attention-based methods have also been <cite class="ltx_cite ltx_citemacro_cite">Rajasekhar et al., (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> employed to capture the inter-modal association between the audio, visual, and other modalities. Lu et al. <cite class="ltx_cite ltx_citemacro_cite">Lu et al., (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> proposed ViLBERT, which was the seminal work in multimodal co-attention. Since then, many transformer-based cross-attention methods have been proposed <cite class="ltx_cite ltx_citemacro_cite">Parthasarathy and Sundaram, (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>); Wei et al., (<a href="#bib.bib57" title="" class="ltx_ref">2020</a>)</cite>. These methods, however, cannot effectively capture the intra-modal temporal dynamics. Further, they specialize in capturing the complementary information among the modalities but do not have the mechanism to capture the redundant information explicitly.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The proposed method introduces the third branch with the joint representation of the multiple modalities, as shown in Figure <a href="#S0.F1" title="Figure 1 ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. By incorporating a joint representation branch, the model can access additional contextual information that might not be fully captured by cross-attention alone. This can help improve the model’s understanding of complex relationships between the input sequences. Further, the proposed method becomes more robust to noise or irrelevant information present in individual sequences. This helps in mitigating the sensitivity of cross-attention to noisy inputs and improves the overall performance of the system.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Our main contributions are summarized as follows</span>.
<br class="ltx_break"><span id="S1.p5.1.2" class="ltx_text ltx_font_bold">(1)</span> This paper proposes a joint multimodal transformer fusion framework that leverages joint modality representations. It captures inter- and intra-modal information in videos using key-based cross-attention.
<span id="S1.p5.1.3" class="ltx_text ltx_font_bold">(2)</span> A joint multimodal transformer multimodal fusion that exploits the redundant and complementary associations among modalities.
<span id="S1.p5.1.4" class="ltx_text ltx_font_bold">(3)</span> An extensive set of experiments on the challenging emotion recognition (pain estimation on BioVid and dimension valence-arousal assessment on Affwild2) datasets indicate that our proposed multimodal fusion architecture can outperform relevant baseline and state-of-the-art methods.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work in Emotion Recognition</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multimodal Methods</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">MMER refers to the integration of multiple sources of information (modalities) to improve the accuracy and robustness of automated emotion recognition systems, at the expense of complexity. These modalities typically include visual, audio, textual, and physiological. The seminal work in multimodal deep learning was proposed by Ngiam et al. <cite class="ltx_cite ltx_citemacro_cite">Ngiam et al., (<a href="#bib.bib39" title="" class="ltx_ref">2011</a>)</cite>, where the features from the audio and visual modalities were extracted separately, and then autoencoders and Restricted Boltzmann Machines were used to feature fusion. Tzirakis et al. <cite class="ltx_cite ltx_citemacro_cite">Tzirakis et al., (<a href="#bib.bib55" title="" class="ltx_ref">2017</a>)</cite> proposed one of the early approaches for A-V fusion for dimensional emotion recognition, in which the visual features were extracted using a ResNet50 and the audio features were obtained using a 1D convolutional neural network (CNN). The modality-specific features were concatenated and fed to a recurrent net for simultaneous temporal modeling and modality fusion. An empirical study was presented by Juan et al. <cite class="ltx_cite ltx_citemacro_cite">Ortega et al., (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, where the authors studied the impact of fine-tuning multiple layers in a pretrained CNN for the visual modality. A two-stream autoencoder with a long short-term memory (LSTM) network was proposed by Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al., (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> to jointly learn and compact representative features from the visual and audio modalities. A knowledge distillation-based approach was investigated by Schonevald et al. <cite class="ltx_cite ltx_citemacro_cite">Schoneveld et al., (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite> for visual modality. For the audio modality, spectrograms were obtained and fed to a CNN model, and the two modalities were fused using a recurrent net. A novel self-distillation scheme was put forward by Deng et al. <cite class="ltx_cite ltx_citemacro_cite">Deng et al., (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> to overcome the problem of noisy labels in a multitasking setting. A two-stream aural visual (TSAV) network was proposed by Kuhnke et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">Kuhnke et al., 2020a </a></cite>, in which the audio features were extracted using a ResNet18, and the visual features were extracted using a 3D-CNN. The obtained embeddings were fed to a specially designed TSAV network for information fusion.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Pain classification is one of the primary problems in affective computing. Researchers have proposed many multimodal datasets for the pain estimation task. The facial activity descriptors method for pain estimation was introduced by Werner et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">Werner et al., 2014a </a></cite>. Dragomir et al. <cite class="ltx_cite ltx_citemacro_cite">Dragomir et al., (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> propose a subject-independent method from facial images with a residual learning technique. A Sparse LSTM-based method was proposed by Zhi et al. <cite class="ltx_cite ltx_citemacro_cite">Zhi et al., (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite> to solve the problem of vanishing gradients in temporal learning. Morabit et al. <cite class="ltx_cite ltx_citemacro_cite">Morabit and Rivenq, (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> proposed a data-efficient image transformer. To process multiscale electrodermal activity signals, a SE-Net-based network was proposed by Lu et al. <cite class="ltx_cite ltx_citemacro_cite">Lu et al., (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>. Multimodal solutions to fuse the physiological and visual modalities were proposed by Werner et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">Werner et al., 2014a </a></cite>, Kachele et al. <cite class="ltx_cite ltx_citemacro_cite">Kächele et al., (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>, and Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">Zhi et al., (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>. Physiological signals are more discriminative for pain classification than the visual modality.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Attention-Based and Transformer Methods</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Since its inception, attention models have shown extraordinary performance in many applications. These models have been extensively investigated for capturing the inter and intra-modal associations between the audio and visual modalities for tasks like action localization <cite class="ltx_cite ltx_citemacro_cite">Lee et al., (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, A-V event localization <cite class="ltx_cite ltx_citemacro_cite">Duan et al., (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>, and multimodal emotion recognition <cite class="ltx_cite ltx_citemacro_cite">Parthasarathy and Sundaram, (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite>. An attention-based fusion mechanism was proposed by Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">Zhang et al., (<a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, 3D-CNNs and 2D-CNNs were used to extract multi-features in the visual modality, and for the audio modality, a 2D-CNN was used to learn representation from spectrograms. Specialized scoring functions were used to re-weight the audio and visual features. Recently, cross-modal attention has shown promising results because of its ability to model inter-modal relationships. Srinivas et al. <cite class="ltx_cite ltx_citemacro_cite">Parthasarathy and Sundaram, (<a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> explored a transformer network with encoder layers, where cross-modal attention is used to fuse audio and visual features for continuous arousal/valence prediction in the wild. Tzirakis et al. <cite class="ltx_cite ltx_citemacro_cite">Tzirakis et al., (<a href="#bib.bib54" title="" class="ltx_ref">2021</a>)</cite> explored the idea of cross-attention in conjunction with self-attention. The authors proposed a transformer-based fusion architecture. Although the methods mentioned above have used cross-modal attention with transformers, they do not have any explicit mechanism to capture semantic relevance between the A-V features, particularly the intra-modal correlations. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">Zhang et al., (<a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite> proposed a method for A-V fusion using leader-follower attentive fusion for continuous arousal/valence prediction. Attention weights are combined with the encoded visual and audio features. Cross attention presented in Praveen et al. <cite class="ltx_cite ltx_citemacro_cite">Rajasekhar et al., (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> has shown a substantial increase in performance by using cross-correlation across the individual features. In contrast, our proposed method uses key-based cross-attention in multimodal transformers and explores the idea of feeding the joint A-V feature vector. By feeding the joint A-V feature representation, the proposed method effectively captures the inter- and intra-modal relationships simultaneously by interacting across itself and the other modalities.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Huang et al. <cite class="ltx_cite ltx_citemacro_cite">Huang et al., (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> investigated the idea of multi-head attention in transformer-based fusion architecture, which was further combined with LSTM to capture the high-level representations. Tran et al. <cite class="ltx_cite ltx_citemacro_cite">Tran and Soleymani, (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> proposed a cross-modal transformer architecture that consisted of a multimodal cross-modal attention block, where the Queries were generated from one modality and the key-values were generated from the other modality. Le et al. <cite class="ltx_cite ltx_citemacro_cite">Le et al., (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> put forward an end-to-end transformer-based fusion mechanism for multilabel multimodal emotion classification; the model consisted of three parts: i) three backbone networks for visual, audio, and textual feature extractor, ii) a transformer network for information fusion, and iii) classification network. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">Zhou et al., (<a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite> proposed a transformer-based fusion scheme along with the temporal convolutional network (TCN); the audio and visual features were extracted using pretrained backbones followed by a TCN, the output of TCN was concatenated and fed to a transformer encoder block. A multilayer perceptron (MLP) was then used for the final prediction.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">All the aforementioned transformer-based fusion architectures primarily focus on intermodality correlation. In contrast, the proposed method, in addition to modeling the intermodality relationships to capture the complementarity between modalities, we explicitly feed the joint (combined) features to the multimodal transformer to introduce redundancy. By incorporating this third joint representation branch, the proposed model has access to enhanced contextual information that might not be fully captured by cross-attention alone. Doing this improves the model’s understanding of complex relationships between the input sequences. Further, the proposed method becomes more robust to noise or irrelevant information present in individual sequences. Having this third joint representation allows the model to dynamically focus on this newly introduced information in sequences where both modalities are noisy simultaneously. This helps in mitigating the sensitivity of cross-attention to noisy inputs and improves the overall performance of the system.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S2.F2.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\includegraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F2.2" class="ltx_p ltx_figure_panel ltx_align_center">[scale=0.78]JMT_3.png</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of the proposed joint multimodal transformer model for A-V fusion. The audio and visual modalities are cross-attended using transformer blocks. The JMT block also takes in the joint representation (shown with red arrows). The output of the cross-attended features is concatenated, and an FC layer is used for valence/arousal prediction.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.3" class="ltx_p">The proposed method is a hierarchical fusion mechanism, where the intra-modality features are combined using transformer-based self-attention, and cross-modality features are fused using transformer-based cross-attention. Further, we feed a third joint representation to the joint transformer module (JTM) to enhance robustness. The <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">K</annotation></semantics></math> (key matrix), <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">V</annotation></semantics></math> (value matrix), and <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">Q</annotation></semantics></math> (query matrix) vectors are shared among the six transformer blocks. In the end, the output of these six blocks is again fed to a transformer self-attention block to dynamically weigh the most relevant representations. The final prediction is made using fully connected (FC) layers.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Modality Specific Feature Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In the first step, modality-specific features are extracted using backbones. The proposed method allows to combine multiple backbones for each modality to improve system robustness. The extracted feature vectors from each backbone are fed to a transformer self-attention block. The combined feature vector is the representation of the particular modality.
For example, to capture information about a person’s emotional state, we can use an R(2+1)D CNN pretrained on the Kinetics-400 dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">Kuhnke et al., 2020b </a></cite> to extract visual features. For the audio modality, we could extract features using a ResNet18 <cite class="ltx_cite ltx_citemacro_cite">He et al., (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> CNN with a GRU <cite class="ltx_cite ltx_citemacro_cite">Cho et al., (<a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-transformer Attention Fusion</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.6" class="ltx_p">We define <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{F_{A}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2b.cmml"><mtext id="S3.SS2.p1.1.m1.1.1.2a" xref="S3.SS2.p1.1.m1.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">A</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2b.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><mtext id="S3.SS2.p1.1.m1.1.1.2a.cmml" xref="S3.SS2.p1.1.m1.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\boldsymbol{F_{A}}</annotation></semantics></math> as the deep features extracted from backbone A, and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{F_{B}}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2b.cmml"><mtext id="S3.SS2.p1.2.m2.1.1.2a" xref="S3.SS2.p1.2.m2.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2b.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><mtext id="S3.SS2.p1.2.m2.1.1.2a.cmml" xref="S3.SS2.p1.2.m2.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\boldsymbol{F_{B}}</annotation></semantics></math> as the deep features extracted from the backbone B in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Attention-Based and Transformer Methods ‣ 2 Related Work in Emotion Recognition ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Given <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\boldsymbol{F_{B}}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2b.cmml"><mtext id="S3.SS2.p1.3.m3.1.1.2a" xref="S3.SS2.p1.3.m3.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><times id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></times><ci id="S3.SS2.p1.3.m3.1.1.2b.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2"><mtext id="S3.SS2.p1.3.m3.1.1.2a.cmml" xref="S3.SS2.p1.3.m3.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\boldsymbol{F_{B}}</annotation></semantics></math> and <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\boldsymbol{F_{A}}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2b.cmml"><mtext id="S3.SS2.p1.4.m4.1.1.2a" xref="S3.SS2.p1.4.m4.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">A</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></times><ci id="S3.SS2.p1.4.m4.1.1.2b.cmml" xref="S3.SS2.p1.4.m4.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"><mtext id="S3.SS2.p1.4.m4.1.1.2a.cmml" xref="S3.SS2.p1.4.m4.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\boldsymbol{F_{A}}</annotation></semantics></math>, the joint feature representation is obtained by concatenating <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\boldsymbol{F_{B}}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2b.cmml"><mtext id="S3.SS2.p1.5.m5.1.1.2a" xref="S3.SS2.p1.5.m5.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><times id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></times><ci id="S3.SS2.p1.5.m5.1.1.2b.cmml" xref="S3.SS2.p1.5.m5.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2"><mtext id="S3.SS2.p1.5.m5.1.1.2a.cmml" xref="S3.SS2.p1.5.m5.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\boldsymbol{F_{B}}</annotation></semantics></math> and <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="\boldsymbol{F_{A}}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2b.cmml"><mtext id="S3.SS2.p1.6.m6.1.1.2a" xref="S3.SS2.p1.6.m6.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">​</mo><msub id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml">A</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><times id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1"></times><ci id="S3.SS2.p1.6.m6.1.1.2b.cmml" xref="S3.SS2.p1.6.m6.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2"><mtext id="S3.SS2.p1.6.m6.1.1.2a.cmml" xref="S3.SS2.p1.6.m6.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">𝐹</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">\boldsymbol{F_{A}}</annotation></semantics></math> feature vectors:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="{\boldsymbol J}=[{\boldsymbol F}_{\mathbf{B}};{\boldsymbol F}_{\mathbf{A}}]\;," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.2b.cmml"><mtext id="S3.E1.m1.1.1.1.1.4.2a" xref="S3.E1.m1.1.1.1.1.4.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.4.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.4.3.cmml">J</mi></mrow><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">[</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2b.cmml"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">𝐁</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.2.3.cmml">;</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2b.cmml"><mtext id="S3.E1.m1.1.1.1.1.2.2.2.2a" xref="S3.E1.m1.1.1.1.1.2.2.2.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.2.1" xref="S3.E1.m1.1.1.1.1.2.2.2.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml">F</mi><mi id="S3.E1.m1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml">𝐀</mi></msub></mrow><mo rspace="0.280em" stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><times id="S3.E1.m1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2b.cmml" xref="S3.E1.m1.1.1.1.1.4.2"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2"><mtext id="S3.E1.m1.1.1.1.1.4.2a.cmml" xref="S3.E1.m1.1.1.1.1.4.2">\boldsymbol</mtext></merror></ci><ci id="S3.E1.m1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.4.3">𝐽</ci></apply><list id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.2b.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.2a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">𝐁</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2"><times id="S3.E1.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.1"></times><ci id="S3.E1.m1.1.1.1.1.2.2.2.2b.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><merror class="ltx_ERROR undefined undefined" id="S3.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><mtext id="S3.E1.m1.1.1.1.1.2.2.2.2a.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2">\boldsymbol</mtext></merror></ci><apply id="S3.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2">𝐹</ci><ci id="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3">𝐀</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">{\boldsymbol J}=[{\boldsymbol F}_{\mathbf{B}};{\boldsymbol F}_{\mathbf{A}}]\;,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.6" class="ltx_p">where <math id="S3.SS2.p3.1.m1.2" class="ltx_Math" alttext="{[\cdot;\cdot]}" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.1" xref="S3.SS2.p3.1.m1.2.3.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">⋅</mo><mo rspace="0em" id="S3.SS2.p3.1.m1.2.3.2.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml">;</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.3" xref="S3.SS2.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><list id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3.2"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">⋅</ci><ci id="S3.SS2.p3.1.m1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2">⋅</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">{[\cdot;\cdot]}</annotation></semantics></math> denotes a concatenation operation.
The concatenated <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="{\boldsymbol J}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2b.cmml"><mtext id="S3.SS2.p3.2.m2.1.1.2a" xref="S3.SS2.p3.2.m2.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">J</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><times id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></times><ci id="S3.SS2.p3.2.m2.1.1.2b.cmml" xref="S3.SS2.p3.2.m2.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2"><mtext id="S3.SS2.p3.2.m2.1.1.2a.cmml" xref="S3.SS2.p3.2.m2.1.1.2">\boldsymbol</mtext></merror></ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝐽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">{\boldsymbol J}</annotation></semantics></math> features are then fed to an FC layer for dimensionality reduction of the joint feature representation to yield <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\boldsymbol{J_{AB}}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2b.cmml"><mtext id="S3.SS2.p3.3.m3.1.1.2a" xref="S3.SS2.p3.3.m3.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.3.2.cmml">J</mi><mrow id="S3.SS2.p3.3.m3.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.3.2" xref="S3.SS2.p3.3.m3.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.3.3.1" xref="S3.SS2.p3.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p3.3.m3.1.1.3.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.3.cmml">B</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></times><ci id="S3.SS2.p3.3.m3.1.1.2b.cmml" xref="S3.SS2.p3.3.m3.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2"><mtext id="S3.SS2.p3.3.m3.1.1.2a.cmml" xref="S3.SS2.p3.3.m3.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.2">𝐽</ci><apply id="S3.SS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3"><times id="S3.SS2.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS2.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3.2">𝐴</ci><ci id="S3.SS2.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\boldsymbol{J_{AB}}</annotation></semantics></math>. We now have three key sources of information: <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="{\boldsymbol F}_{\mathbf{B}}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2b.cmml"><mtext id="S3.SS2.p3.4.m4.1.1.2a" xref="S3.SS2.p3.4.m4.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.cmml">​</mo><msub id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p3.4.m4.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.3.3.cmml">𝐁</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></times><ci id="S3.SS2.p3.4.m4.1.1.2b.cmml" xref="S3.SS2.p3.4.m4.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2"><mtext id="S3.SS2.p3.4.m4.1.1.2a.cmml" xref="S3.SS2.p3.4.m4.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.3.2">𝐹</ci><ci id="S3.SS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3.3">𝐁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">{\boldsymbol F}_{\mathbf{B}}</annotation></semantics></math>, <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="{\boldsymbol F}_{\mathbf{A}}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2b.cmml"><mtext id="S3.SS2.p3.5.m5.1.1.2a" xref="S3.SS2.p3.5.m5.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.cmml">​</mo><msub id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3.2" xref="S3.SS2.p3.5.m5.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p3.5.m5.1.1.3.3" xref="S3.SS2.p3.5.m5.1.1.3.3.cmml">𝐀</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><times id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1"></times><ci id="S3.SS2.p3.5.m5.1.1.2b.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><mtext id="S3.SS2.p3.5.m5.1.1.2a.cmml" xref="S3.SS2.p3.5.m5.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2">𝐹</ci><ci id="S3.SS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3">𝐀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">{\boldsymbol F}_{\mathbf{A}}</annotation></semantics></math> and <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="\boldsymbol{J_{AB}}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2b.cmml"><mtext id="S3.SS2.p3.6.m6.1.1.2a" xref="S3.SS2.p3.6.m6.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.cmml">​</mo><msub id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3.2" xref="S3.SS2.p3.6.m6.1.1.3.2.cmml">J</mi><mrow id="S3.SS2.p3.6.m6.1.1.3.3" xref="S3.SS2.p3.6.m6.1.1.3.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3.3.2" xref="S3.SS2.p3.6.m6.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1.3.3.1" xref="S3.SS2.p3.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p3.6.m6.1.1.3.3.3" xref="S3.SS2.p3.6.m6.1.1.3.3.3.cmml">B</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><times id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1"></times><ci id="S3.SS2.p3.6.m6.1.1.2b.cmml" xref="S3.SS2.p3.6.m6.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2"><mtext id="S3.SS2.p3.6.m6.1.1.2a.cmml" xref="S3.SS2.p3.6.m6.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.3.2">𝐽</ci><apply id="S3.SS2.p3.6.m6.1.1.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3"><times id="S3.SS2.p3.6.m6.1.1.3.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3.1"></times><ci id="S3.SS2.p3.6.m6.1.1.3.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3.2">𝐴</ci><ci id="S3.SS2.p3.6.m6.1.1.3.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\boldsymbol{J_{AB}}</annotation></semantics></math>, which have the same dimensionality.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.13" class="ltx_p">Each representation is then fed to a specific encoder. Our model is composed of three different encoders, one for each type of feature <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="{\boldsymbol F}_{\mathbf{B}}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2b.cmml"><mtext id="S3.SS2.p4.1.m1.1.1.2a" xref="S3.SS2.p4.1.m1.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">​</mo><msub id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.cmml">𝐁</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><times id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></times><ci id="S3.SS2.p4.1.m1.1.1.2b.cmml" xref="S3.SS2.p4.1.m1.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2"><mtext id="S3.SS2.p4.1.m1.1.1.2a.cmml" xref="S3.SS2.p4.1.m1.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2">𝐹</ci><ci id="S3.SS2.p4.1.m1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3">𝐁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">{\boldsymbol F}_{\mathbf{B}}</annotation></semantics></math>, <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="{\boldsymbol F}_{\mathbf{A}}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2b.cmml"><mtext id="S3.SS2.p4.2.m2.1.1.2a" xref="S3.SS2.p4.2.m2.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml"><mi id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml">F</mi><mi id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.cmml">𝐀</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><times id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></times><ci id="S3.SS2.p4.2.m2.1.1.2b.cmml" xref="S3.SS2.p4.2.m2.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2"><mtext id="S3.SS2.p4.2.m2.1.1.2a.cmml" xref="S3.SS2.p4.2.m2.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2">𝐹</ci><ci id="S3.SS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3">𝐀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">{\boldsymbol F}_{\mathbf{A}}</annotation></semantics></math>, and <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\boldsymbol{J_{AB}}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2b.cmml"><mtext id="S3.SS2.p4.3.m3.1.1.2a" xref="S3.SS2.p4.3.m3.1.1.2b.cmml">\boldsymbol</mtext></merror><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml"><mi id="S3.SS2.p4.3.m3.1.1.3.2" xref="S3.SS2.p4.3.m3.1.1.3.2.cmml">J</mi><mrow id="S3.SS2.p4.3.m3.1.1.3.3" xref="S3.SS2.p4.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.p4.3.m3.1.1.3.3.2" xref="S3.SS2.p4.3.m3.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.1.1.3.3.1" xref="S3.SS2.p4.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p4.3.m3.1.1.3.3.3" xref="S3.SS2.p4.3.m3.1.1.3.3.3.cmml">B</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><times id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1"></times><ci id="S3.SS2.p4.3.m3.1.1.2b.cmml" xref="S3.SS2.p4.3.m3.1.1.2"><merror class="ltx_ERROR undefined undefined" id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2"><mtext id="S3.SS2.p4.3.m3.1.1.2a.cmml" xref="S3.SS2.p4.3.m3.1.1.2">\boldsymbol</mtext></merror></ci><apply id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.3.1.cmml" xref="S3.SS2.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.3.2.cmml" xref="S3.SS2.p4.3.m3.1.1.3.2">𝐽</ci><apply id="S3.SS2.p4.3.m3.1.1.3.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3.3"><times id="S3.SS2.p4.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p4.3.m3.1.1.3.3.1"></times><ci id="S3.SS2.p4.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p4.3.m3.1.1.3.3.2">𝐴</ci><ci id="S3.SS2.p4.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\boldsymbol{J_{AB}}</annotation></semantics></math>.
Each encoder consists of a multi-head-self-attention (Eq. <a href="#S3.E2" title="In 3.2 Multi-transformer Attention Fusion ‣ 3 Proposed Approach ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) followed by a fully connected feed-forward network. Residual connection and layer normalization are performed around both of these layers.
The key is used to associate a sequence to a key value, the value matrix holds information that is ultimately used to compute the output of the attention mechanism, and the query matrix represents a set of vectors used to query the key-value pairs. The <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="{K}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">{K}</annotation></semantics></math>, <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="{V}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">{V}</annotation></semantics></math>, <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="{Q}" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">{Q}</annotation></semantics></math> matrix are calculated this way: <math id="S3.SS2.p4.7.m7.1" class="ltx_Math" alttext="{K={X}{W_{K}}}" display="inline"><semantics id="S3.SS2.p4.7.m7.1a"><mrow id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">K</mi><mo id="S3.SS2.p4.7.m7.1.1.1" xref="S3.SS2.p4.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml"><mi id="S3.SS2.p4.7.m7.1.1.3.2" xref="S3.SS2.p4.7.m7.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.7.m7.1.1.3.1" xref="S3.SS2.p4.7.m7.1.1.3.1.cmml">​</mo><msub id="S3.SS2.p4.7.m7.1.1.3.3" xref="S3.SS2.p4.7.m7.1.1.3.3.cmml"><mi id="S3.SS2.p4.7.m7.1.1.3.3.2" xref="S3.SS2.p4.7.m7.1.1.3.3.2.cmml">W</mi><mi id="S3.SS2.p4.7.m7.1.1.3.3.3" xref="S3.SS2.p4.7.m7.1.1.3.3.3.cmml">K</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><eq id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1.1"></eq><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">𝐾</ci><apply id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3"><times id="S3.SS2.p4.7.m7.1.1.3.1.cmml" xref="S3.SS2.p4.7.m7.1.1.3.1"></times><ci id="S3.SS2.p4.7.m7.1.1.3.2.cmml" xref="S3.SS2.p4.7.m7.1.1.3.2">𝑋</ci><apply id="S3.SS2.p4.7.m7.1.1.3.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3.2">𝑊</ci><ci id="S3.SS2.p4.7.m7.1.1.3.3.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">{K={X}{W_{K}}}</annotation></semantics></math>, <math id="S3.SS2.p4.8.m8.1" class="ltx_Math" alttext="{Q={X}{W_{Q}}}" display="inline"><semantics id="S3.SS2.p4.8.m8.1a"><mrow id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml"><mi id="S3.SS2.p4.8.m8.1.1.2" xref="S3.SS2.p4.8.m8.1.1.2.cmml">Q</mi><mo id="S3.SS2.p4.8.m8.1.1.1" xref="S3.SS2.p4.8.m8.1.1.1.cmml">=</mo><mrow id="S3.SS2.p4.8.m8.1.1.3" xref="S3.SS2.p4.8.m8.1.1.3.cmml"><mi id="S3.SS2.p4.8.m8.1.1.3.2" xref="S3.SS2.p4.8.m8.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.8.m8.1.1.3.1" xref="S3.SS2.p4.8.m8.1.1.3.1.cmml">​</mo><msub id="S3.SS2.p4.8.m8.1.1.3.3" xref="S3.SS2.p4.8.m8.1.1.3.3.cmml"><mi id="S3.SS2.p4.8.m8.1.1.3.3.2" xref="S3.SS2.p4.8.m8.1.1.3.3.2.cmml">W</mi><mi id="S3.SS2.p4.8.m8.1.1.3.3.3" xref="S3.SS2.p4.8.m8.1.1.3.3.3.cmml">Q</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1"><eq id="S3.SS2.p4.8.m8.1.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1.1"></eq><ci id="S3.SS2.p4.8.m8.1.1.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2">𝑄</ci><apply id="S3.SS2.p4.8.m8.1.1.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3"><times id="S3.SS2.p4.8.m8.1.1.3.1.cmml" xref="S3.SS2.p4.8.m8.1.1.3.1"></times><ci id="S3.SS2.p4.8.m8.1.1.3.2.cmml" xref="S3.SS2.p4.8.m8.1.1.3.2">𝑋</ci><apply id="S3.SS2.p4.8.m8.1.1.3.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m8.1.1.3.3.1.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p4.8.m8.1.1.3.3.2.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3.2">𝑊</ci><ci id="S3.SS2.p4.8.m8.1.1.3.3.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3.3">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">{Q={X}{W_{Q}}}</annotation></semantics></math>, <math id="S3.SS2.p4.9.m9.1" class="ltx_Math" alttext="{V={X}{W_{V}}}" display="inline"><semantics id="S3.SS2.p4.9.m9.1a"><mrow id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml">V</mi><mo id="S3.SS2.p4.9.m9.1.1.1" xref="S3.SS2.p4.9.m9.1.1.1.cmml">=</mo><mrow id="S3.SS2.p4.9.m9.1.1.3" xref="S3.SS2.p4.9.m9.1.1.3.cmml"><mi id="S3.SS2.p4.9.m9.1.1.3.2" xref="S3.SS2.p4.9.m9.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.9.m9.1.1.3.1" xref="S3.SS2.p4.9.m9.1.1.3.1.cmml">​</mo><msub id="S3.SS2.p4.9.m9.1.1.3.3" xref="S3.SS2.p4.9.m9.1.1.3.3.cmml"><mi id="S3.SS2.p4.9.m9.1.1.3.3.2" xref="S3.SS2.p4.9.m9.1.1.3.3.2.cmml">W</mi><mi id="S3.SS2.p4.9.m9.1.1.3.3.3" xref="S3.SS2.p4.9.m9.1.1.3.3.3.cmml">V</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1"><eq id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1.1"></eq><ci id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2">𝑉</ci><apply id="S3.SS2.p4.9.m9.1.1.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3"><times id="S3.SS2.p4.9.m9.1.1.3.1.cmml" xref="S3.SS2.p4.9.m9.1.1.3.1"></times><ci id="S3.SS2.p4.9.m9.1.1.3.2.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2">𝑋</ci><apply id="S3.SS2.p4.9.m9.1.1.3.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.3.3.1.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.3.3.2.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3.2">𝑊</ci><ci id="S3.SS2.p4.9.m9.1.1.3.3.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3.3">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">{V={X}{W_{V}}}</annotation></semantics></math>. <math id="S3.SS2.p4.10.m10.1" class="ltx_Math" alttext="{X}" display="inline"><semantics id="S3.SS2.p4.10.m10.1a"><mi id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><ci id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">{X}</annotation></semantics></math> corresponds to one of the sources of information and <math id="S3.SS2.p4.11.m11.1" class="ltx_Math" alttext="{W_{K}}" display="inline"><semantics id="S3.SS2.p4.11.m11.1a"><msub id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml"><mi id="S3.SS2.p4.11.m11.1.1.2" xref="S3.SS2.p4.11.m11.1.1.2.cmml">W</mi><mi id="S3.SS2.p4.11.m11.1.1.3" xref="S3.SS2.p4.11.m11.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><apply id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.11.m11.1.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p4.11.m11.1.1.2.cmml" xref="S3.SS2.p4.11.m11.1.1.2">𝑊</ci><ci id="S3.SS2.p4.11.m11.1.1.3.cmml" xref="S3.SS2.p4.11.m11.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">{W_{K}}</annotation></semantics></math>, <math id="S3.SS2.p4.12.m12.1" class="ltx_Math" alttext="{W_{Q}}" display="inline"><semantics id="S3.SS2.p4.12.m12.1a"><msub id="S3.SS2.p4.12.m12.1.1" xref="S3.SS2.p4.12.m12.1.1.cmml"><mi id="S3.SS2.p4.12.m12.1.1.2" xref="S3.SS2.p4.12.m12.1.1.2.cmml">W</mi><mi id="S3.SS2.p4.12.m12.1.1.3" xref="S3.SS2.p4.12.m12.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m12.1b"><apply id="S3.SS2.p4.12.m12.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m12.1.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p4.12.m12.1.1.2.cmml" xref="S3.SS2.p4.12.m12.1.1.2">𝑊</ci><ci id="S3.SS2.p4.12.m12.1.1.3.cmml" xref="S3.SS2.p4.12.m12.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m12.1c">{W_{Q}}</annotation></semantics></math>, <math id="S3.SS2.p4.13.m13.1" class="ltx_Math" alttext="{W_{V}}" display="inline"><semantics id="S3.SS2.p4.13.m13.1a"><msub id="S3.SS2.p4.13.m13.1.1" xref="S3.SS2.p4.13.m13.1.1.cmml"><mi id="S3.SS2.p4.13.m13.1.1.2" xref="S3.SS2.p4.13.m13.1.1.2.cmml">W</mi><mi id="S3.SS2.p4.13.m13.1.1.3" xref="S3.SS2.p4.13.m13.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.13.m13.1b"><apply id="S3.SS2.p4.13.m13.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.13.m13.1.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p4.13.m13.1.1.2.cmml" xref="S3.SS2.p4.13.m13.1.1.2">𝑊</ci><ci id="S3.SS2.p4.13.m13.1.1.3.cmml" xref="S3.SS2.p4.13.m13.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.13.m13.1c">{W_{V}}</annotation></semantics></math> are the weights of the key, query, value matrices respectively.
The output values of the self-attention layers are given by:</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="{Attention(Q,K,V)=softmax({K}{Q^{T}}/{d_{k}})V}" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml"><mi id="S3.E2.m1.4.4.3.2" xref="S3.E2.m1.4.4.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1a" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.4" xref="S3.E2.m1.4.4.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1b" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.5" xref="S3.E2.m1.4.4.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1c" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.6" xref="S3.E2.m1.4.4.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1d" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.7" xref="S3.E2.m1.4.4.3.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1e" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.8" xref="S3.E2.m1.4.4.3.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1f" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.9" xref="S3.E2.m1.4.4.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1g" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.3.10" xref="S3.E2.m1.4.4.3.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.1h" xref="S3.E2.m1.4.4.3.1.cmml">​</mo><mrow id="S3.E2.m1.4.4.3.11.2" xref="S3.E2.m1.4.4.3.11.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.3.11.2.1" xref="S3.E2.m1.4.4.3.11.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">Q</mi><mo id="S3.E2.m1.4.4.3.11.2.2" xref="S3.E2.m1.4.4.3.11.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">K</mi><mo id="S3.E2.m1.4.4.3.11.2.3" xref="S3.E2.m1.4.4.3.11.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E2.m1.4.4.3.11.2.4" xref="S3.E2.m1.4.4.3.11.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml">=</mo><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.cmml"><mi id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.4.4.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.4" xref="S3.E2.m1.4.4.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2a" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.5" xref="S3.E2.m1.4.4.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2b" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.6" xref="S3.E2.m1.4.4.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2c" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.7" xref="S3.E2.m1.4.4.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2d" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.8" xref="S3.E2.m1.4.4.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2e" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.9" xref="S3.E2.m1.4.4.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2f" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.2.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.1.1.2.1" xref="S3.E2.m1.4.4.1.1.1.1.2.1.cmml">​</mo><msup id="S3.E2.m1.4.4.1.1.1.1.2.3" xref="S3.E2.m1.4.4.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.2.3.2" xref="S3.E2.m1.4.4.1.1.1.1.2.3.2.cmml">Q</mi><mi id="S3.E2.m1.4.4.1.1.1.1.2.3.3" xref="S3.E2.m1.4.4.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.cmml">/</mo><msub id="S3.E2.m1.4.4.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.3.2.cmml">d</mi><mi id="S3.E2.m1.4.4.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.2g" xref="S3.E2.m1.4.4.1.2.cmml">​</mo><mi id="S3.E2.m1.4.4.1.10" xref="S3.E2.m1.4.4.1.10.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"></eq><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"><times id="S3.E2.m1.4.4.3.1.cmml" xref="S3.E2.m1.4.4.3.1"></times><ci id="S3.E2.m1.4.4.3.2.cmml" xref="S3.E2.m1.4.4.3.2">𝐴</ci><ci id="S3.E2.m1.4.4.3.3.cmml" xref="S3.E2.m1.4.4.3.3">𝑡</ci><ci id="S3.E2.m1.4.4.3.4.cmml" xref="S3.E2.m1.4.4.3.4">𝑡</ci><ci id="S3.E2.m1.4.4.3.5.cmml" xref="S3.E2.m1.4.4.3.5">𝑒</ci><ci id="S3.E2.m1.4.4.3.6.cmml" xref="S3.E2.m1.4.4.3.6">𝑛</ci><ci id="S3.E2.m1.4.4.3.7.cmml" xref="S3.E2.m1.4.4.3.7">𝑡</ci><ci id="S3.E2.m1.4.4.3.8.cmml" xref="S3.E2.m1.4.4.3.8">𝑖</ci><ci id="S3.E2.m1.4.4.3.9.cmml" xref="S3.E2.m1.4.4.3.9">𝑜</ci><ci id="S3.E2.m1.4.4.3.10.cmml" xref="S3.E2.m1.4.4.3.10">𝑛</ci><vector id="S3.E2.m1.4.4.3.11.1.cmml" xref="S3.E2.m1.4.4.3.11.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑄</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐾</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑉</ci></vector></apply><apply id="S3.E2.m1.4.4.1.cmml" xref="S3.E2.m1.4.4.1"><times id="S3.E2.m1.4.4.1.2.cmml" xref="S3.E2.m1.4.4.1.2"></times><ci id="S3.E2.m1.4.4.1.3.cmml" xref="S3.E2.m1.4.4.1.3">𝑠</ci><ci id="S3.E2.m1.4.4.1.4.cmml" xref="S3.E2.m1.4.4.1.4">𝑜</ci><ci id="S3.E2.m1.4.4.1.5.cmml" xref="S3.E2.m1.4.4.1.5">𝑓</ci><ci id="S3.E2.m1.4.4.1.6.cmml" xref="S3.E2.m1.4.4.1.6">𝑡</ci><ci id="S3.E2.m1.4.4.1.7.cmml" xref="S3.E2.m1.4.4.1.7">𝑚</ci><ci id="S3.E2.m1.4.4.1.8.cmml" xref="S3.E2.m1.4.4.1.8">𝑎</ci><ci id="S3.E2.m1.4.4.1.9.cmml" xref="S3.E2.m1.4.4.1.9">𝑥</ci><apply id="S3.E2.m1.4.4.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><divide id="S3.E2.m1.4.4.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"></divide><apply id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2"><times id="S3.E2.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.1"></times><ci id="S3.E2.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.2">𝐾</ci><apply id="S3.E2.m1.4.4.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.3">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.3.2">𝑄</ci><ci id="S3.E2.m1.4.4.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3.2">𝑑</ci><ci id="S3.E2.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.3.3">𝑘</ci></apply></apply><ci id="S3.E2.m1.4.4.1.10.cmml" xref="S3.E2.m1.4.4.1.10">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">{Attention(Q,K,V)=softmax({K}{Q^{T}}/{d_{k}})V}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p">With self-attention layers, each encoder focuses independently on important cues related to its respective source of information. Afterward, each encoder embedding is combined by utilizing six cross-modal attention layers where the query matrix Q is shared with the key K and value V matrix of the other source of information. Sharing this matrix between each source of information helps the model add redundancy and complement the visual and audio modalities, thus improving its performance. At the output of each of the six cross-attention modules, the feature vector of dimension 512 is output. These six feature vectors are then stacked to form a sequence, which is then fed to a transformer self-attention block. This block dynamically selects and weighs these feature vectors. The final attended features are fed to an FC layer for final prediction.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p">The model aims to maximize the Concordance Correlation Coefficient (CCC) <cite class="ltx_cite ltx_citemacro_cite">Lawrence and Lin, (<a href="#bib.bib30" title="" class="ltx_ref">1989</a>)</cite>, which is common in dimensional emotion recognition. To achieve this, we minimize the following loss:</p>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\\
{L}_{c}=1-\rho_{c}=\ 1-\frac{2\rho_{xy}^{2}}{\rho_{x}^{2}+\rho_{y}^{2}+(\mu_{x}-\mu_{y})^{2}}\ " display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.2" xref="S3.E3.m1.1.2.cmml"><msub id="S3.E3.m1.1.2.2" xref="S3.E3.m1.1.2.2.cmml"><mi id="S3.E3.m1.1.2.2.2" xref="S3.E3.m1.1.2.2.2.cmml">L</mi><mi id="S3.E3.m1.1.2.2.3" xref="S3.E3.m1.1.2.2.3.cmml">c</mi></msub><mo id="S3.E3.m1.1.2.3" xref="S3.E3.m1.1.2.3.cmml">=</mo><mrow id="S3.E3.m1.1.2.4" xref="S3.E3.m1.1.2.4.cmml"><mn id="S3.E3.m1.1.2.4.2" xref="S3.E3.m1.1.2.4.2.cmml">1</mn><mo id="S3.E3.m1.1.2.4.1" xref="S3.E3.m1.1.2.4.1.cmml">−</mo><msub id="S3.E3.m1.1.2.4.3" xref="S3.E3.m1.1.2.4.3.cmml"><mi id="S3.E3.m1.1.2.4.3.2" xref="S3.E3.m1.1.2.4.3.2.cmml">ρ</mi><mi id="S3.E3.m1.1.2.4.3.3" xref="S3.E3.m1.1.2.4.3.3.cmml">c</mi></msub></mrow><mo id="S3.E3.m1.1.2.5" xref="S3.E3.m1.1.2.5.cmml">=</mo><mrow id="S3.E3.m1.1.2.6" xref="S3.E3.m1.1.2.6.cmml"><mn id="S3.E3.m1.1.2.6.2" xref="S3.E3.m1.1.2.6.2.cmml"> 1</mn><mo id="S3.E3.m1.1.2.6.1" xref="S3.E3.m1.1.2.6.1.cmml">−</mo><mfrac id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mn id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><msubsup id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml">ρ</mi><mrow id="S3.E3.m1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.3.2" xref="S3.E3.m1.1.1.3.3.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.3.1" xref="S3.E3.m1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.3.3" xref="S3.E3.m1.1.1.3.3.2.3.3.cmml">y</mi></mrow><mn id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">2</mn></msubsup></mrow><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml">ρ</mi><mi id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml">x</mi><mn id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><msubsup id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.4.2.2" xref="S3.E3.m1.1.1.1.4.2.2.cmml">ρ</mi><mi id="S3.E3.m1.1.1.1.4.2.3" xref="S3.E3.m1.1.1.1.4.2.3.cmml">y</mi><mn id="S3.E3.m1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.4.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.1.1.1.2a" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><msup id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.2.cmml">μ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.3.cmml">x</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml">μ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml">y</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.2.cmml" xref="S3.E3.m1.1.2"><and id="S3.E3.m1.1.2a.cmml" xref="S3.E3.m1.1.2"></and><apply id="S3.E3.m1.1.2b.cmml" xref="S3.E3.m1.1.2"><eq id="S3.E3.m1.1.2.3.cmml" xref="S3.E3.m1.1.2.3"></eq><apply id="S3.E3.m1.1.2.2.cmml" xref="S3.E3.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.2.2.1.cmml" xref="S3.E3.m1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.1.2.2.2.cmml" xref="S3.E3.m1.1.2.2.2">𝐿</ci><ci id="S3.E3.m1.1.2.2.3.cmml" xref="S3.E3.m1.1.2.2.3">𝑐</ci></apply><apply id="S3.E3.m1.1.2.4.cmml" xref="S3.E3.m1.1.2.4"><minus id="S3.E3.m1.1.2.4.1.cmml" xref="S3.E3.m1.1.2.4.1"></minus><cn type="integer" id="S3.E3.m1.1.2.4.2.cmml" xref="S3.E3.m1.1.2.4.2">1</cn><apply id="S3.E3.m1.1.2.4.3.cmml" xref="S3.E3.m1.1.2.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.2.4.3.1.cmml" xref="S3.E3.m1.1.2.4.3">subscript</csymbol><ci id="S3.E3.m1.1.2.4.3.2.cmml" xref="S3.E3.m1.1.2.4.3.2">𝜌</ci><ci id="S3.E3.m1.1.2.4.3.3.cmml" xref="S3.E3.m1.1.2.4.3.3">𝑐</ci></apply></apply></apply><apply id="S3.E3.m1.1.2c.cmml" xref="S3.E3.m1.1.2"><eq id="S3.E3.m1.1.2.5.cmml" xref="S3.E3.m1.1.2.5"></eq><share href="#S3.E3.m1.1.2.4.cmml" id="S3.E3.m1.1.2d.cmml" xref="S3.E3.m1.1.2"></share><apply id="S3.E3.m1.1.2.6.cmml" xref="S3.E3.m1.1.2.6"><minus id="S3.E3.m1.1.2.6.1.cmml" xref="S3.E3.m1.1.2.6.1"></minus><cn type="integer" id="S3.E3.m1.1.2.6.2.cmml" xref="S3.E3.m1.1.2.6.2">1</cn><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><divide id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1"></divide><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><cn type="integer" id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">2</cn><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">superscript</csymbol><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">𝜌</ci><apply id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3"><times id="S3.E3.m1.1.1.3.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2">𝑥</ci><ci id="S3.E3.m1.1.1.3.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3.3">𝑦</ci></apply></apply><cn type="integer" id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">2</cn></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2">𝜌</ci><ci id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3">𝑥</ci></apply><cn type="integer" id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">2</cn></apply><apply id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.4">superscript</csymbol><apply id="S3.E3.m1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.4.2.1.cmml" xref="S3.E3.m1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.4.2.2.cmml" xref="S3.E3.m1.1.1.1.4.2.2">𝜌</ci><ci id="S3.E3.m1.1.1.1.4.2.3.cmml" xref="S3.E3.m1.1.1.1.4.2.3">𝑦</ci></apply><cn type="integer" id="S3.E3.m1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.4.3">2</cn></apply><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.2">𝜇</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.2">𝜇</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3.3">𝑦</ci></apply></apply><cn type="integer" id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\\
{L}_{c}=1-\rho_{c}=\ 1-\frac{2\rho_{xy}^{2}}{\rho_{x}^{2}+\rho_{y}^{2}+(\mu_{x}-\mu_{y})^{2}}\ </annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.5" class="ltx_p">where <math id="S3.SS2.p9.1.m1.1" class="ltx_Math" alttext="{\rho_{xy}^{2}}" display="inline"><semantics id="S3.SS2.p9.1.m1.1a"><msubsup id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2.2" xref="S3.SS2.p9.1.m1.1.1.2.2.cmml">ρ</mi><mrow id="S3.SS2.p9.1.m1.1.1.2.3" xref="S3.SS2.p9.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2.3.2" xref="S3.SS2.p9.1.m1.1.1.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.1.m1.1.1.2.3.1" xref="S3.SS2.p9.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p9.1.m1.1.1.2.3.3" xref="S3.SS2.p9.1.m1.1.1.2.3.3.cmml">y</mi></mrow><mn id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.2.1.cmml" xref="S3.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2.2">𝜌</ci><apply id="S3.SS2.p9.1.m1.1.1.2.3.cmml" xref="S3.SS2.p9.1.m1.1.1.2.3"><times id="S3.SS2.p9.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p9.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.p9.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2.3.2">𝑥</ci><ci id="S3.SS2.p9.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p9.1.m1.1.1.2.3.3">𝑦</ci></apply></apply><cn type="integer" id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">{\rho_{xy}^{2}}</annotation></semantics></math> is the covariance between the predictions and the ground truth, <math id="S3.SS2.p9.2.m2.1" class="ltx_Math" alttext="{\rho_{x}^{2}}" display="inline"><semantics id="S3.SS2.p9.2.m2.1a"><msubsup id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml"><mi id="S3.SS2.p9.2.m2.1.1.2.2" xref="S3.SS2.p9.2.m2.1.1.2.2.cmml">ρ</mi><mi id="S3.SS2.p9.2.m2.1.1.2.3" xref="S3.SS2.p9.2.m2.1.1.2.3.cmml">x</mi><mn id="S3.SS2.p9.2.m2.1.1.3" xref="S3.SS2.p9.2.m2.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><apply id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.2.m2.1.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p9.2.m2.1.1.2.cmml" xref="S3.SS2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.2.m2.1.1.2.1.cmml" xref="S3.SS2.p9.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p9.2.m2.1.1.2.2.cmml" xref="S3.SS2.p9.2.m2.1.1.2.2">𝜌</ci><ci id="S3.SS2.p9.2.m2.1.1.2.3.cmml" xref="S3.SS2.p9.2.m2.1.1.2.3">𝑥</ci></apply><cn type="integer" id="S3.SS2.p9.2.m2.1.1.3.cmml" xref="S3.SS2.p9.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">{\rho_{x}^{2}}</annotation></semantics></math> and <math id="S3.SS2.p9.3.m3.1" class="ltx_Math" alttext="{\rho_{y}^{2}}" display="inline"><semantics id="S3.SS2.p9.3.m3.1a"><msubsup id="S3.SS2.p9.3.m3.1.1" xref="S3.SS2.p9.3.m3.1.1.cmml"><mi id="S3.SS2.p9.3.m3.1.1.2.2" xref="S3.SS2.p9.3.m3.1.1.2.2.cmml">ρ</mi><mi id="S3.SS2.p9.3.m3.1.1.2.3" xref="S3.SS2.p9.3.m3.1.1.2.3.cmml">y</mi><mn id="S3.SS2.p9.3.m3.1.1.3" xref="S3.SS2.p9.3.m3.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.3.m3.1b"><apply id="S3.SS2.p9.3.m3.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.3.m3.1.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.p9.3.m3.1.1.2.cmml" xref="S3.SS2.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.3.m3.1.1.2.1.cmml" xref="S3.SS2.p9.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p9.3.m3.1.1.2.2.cmml" xref="S3.SS2.p9.3.m3.1.1.2.2">𝜌</ci><ci id="S3.SS2.p9.3.m3.1.1.2.3.cmml" xref="S3.SS2.p9.3.m3.1.1.2.3">𝑦</ci></apply><cn type="integer" id="S3.SS2.p9.3.m3.1.1.3.cmml" xref="S3.SS2.p9.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.3.m3.1c">{\rho_{y}^{2}}</annotation></semantics></math> the variance of the prediction and the ground truth, <math id="S3.SS2.p9.4.m4.1" class="ltx_Math" alttext="{\mu_{x}}" display="inline"><semantics id="S3.SS2.p9.4.m4.1a"><msub id="S3.SS2.p9.4.m4.1.1" xref="S3.SS2.p9.4.m4.1.1.cmml"><mi id="S3.SS2.p9.4.m4.1.1.2" xref="S3.SS2.p9.4.m4.1.1.2.cmml">μ</mi><mi id="S3.SS2.p9.4.m4.1.1.3" xref="S3.SS2.p9.4.m4.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.4.m4.1b"><apply id="S3.SS2.p9.4.m4.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.4.m4.1.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p9.4.m4.1.1.2.cmml" xref="S3.SS2.p9.4.m4.1.1.2">𝜇</ci><ci id="S3.SS2.p9.4.m4.1.1.3.cmml" xref="S3.SS2.p9.4.m4.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.4.m4.1c">{\mu_{x}}</annotation></semantics></math> and <math id="S3.SS2.p9.5.m5.1" class="ltx_Math" alttext="{\mu_{y}}" display="inline"><semantics id="S3.SS2.p9.5.m5.1a"><msub id="S3.SS2.p9.5.m5.1.1" xref="S3.SS2.p9.5.m5.1.1.cmml"><mi id="S3.SS2.p9.5.m5.1.1.2" xref="S3.SS2.p9.5.m5.1.1.2.cmml">μ</mi><mi id="S3.SS2.p9.5.m5.1.1.3" xref="S3.SS2.p9.5.m5.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.5.m5.1b"><apply id="S3.SS2.p9.5.m5.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.5.m5.1.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p9.5.m5.1.1.2.cmml" xref="S3.SS2.p9.5.m5.1.1.2">𝜇</ci><ci id="S3.SS2.p9.5.m5.1.1.3.cmml" xref="S3.SS2.p9.5.m5.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.5.m5.1c">{\mu_{y}}</annotation></semantics></math> the mean of prediction and the ground truth.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with the state-of-the-art pain estimation methods on the Biovid Heat Pain Database. The highest score is indicated in bold. </figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Modality</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">CV Scheme</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p" style="width:170.7pt;">Werner et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref">Werner et al., 2014b </a></cite> <span id="S3.T1.1.2.1.1.1.1.1" class="ltx_text ltx_font_italic">ICPR 2014</span></span>
</span>
</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">EDA, ECG, EMG + Video</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5-FOLD</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.6</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p" style="width:170.7pt;">Werner et al. <cite class="ltx_cite ltx_citemacro_cite">Werner et al., (<a href="#bib.bib58" title="" class="ltx_ref">2016</a>)</cite> <span id="S3.T1.1.3.2.1.1.1.1" class="ltx_text ltx_font_italic">IEEE TAC 2016</span></span>
</span>
</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Video</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.4</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1.1" class="ltx_p" style="width:170.7pt;">Kachele et al. <cite class="ltx_cite ltx_citemacro_cite">Kächele et al., (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> <span id="S3.T1.1.4.3.1.1.1.1" class="ltx_text ltx_font_italic">IEEE IJSTSP 2016</span></span>
</span>
</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA, ECG, EMG</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.73</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p" style="width:170.7pt;">Lopez et al. <cite class="ltx_cite ltx_citemacro_cite">Lopez-Martinez and Picard, (<a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> <span id="S3.T1.1.5.4.1.1.1.1" class="ltx_text ltx_font_italic">ACIIW 2017</span></span>
</span>
</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA, ECG</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10-FOLD</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.75</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.1.1.1" class="ltx_p" style="width:170.7pt;">Lopez et al. <cite class="ltx_cite ltx_citemacro_cite">Lopez-Martinez and Picard, (<a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> <span id="S3.T1.1.6.5.1.1.1.1" class="ltx_text ltx_font_italic">EMBC 2018</span></span>
</span>
</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.21</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p" style="width:170.7pt;">Thiam et al. <cite class="ltx_cite ltx_citemacro_cite">Thiam et al., (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite> <span id="S3.T1.1.7.6.1.1.1.1" class="ltx_text ltx_font_italic">Sensors 2019</span></span>
</span>
</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.57</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.1.1.1" class="ltx_p" style="width:170.7pt;">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">Wang et al., (<a href="#bib.bib56" title="" class="ltx_ref">2020</a>)</cite> <span id="S3.T1.1.8.7.1.1.1.1" class="ltx_text ltx_font_italic">EMBC 2020</span></span>
</span>
</th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA, ECG, EMG</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.3</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.1.1.1" class="ltx_p" style="width:170.7pt;">Pouromran et al. <cite class="ltx_cite ltx_citemacro_cite">Pouromran et al., (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite> <span id="S3.T1.1.9.8.1.1.1.1" class="ltx_text ltx_font_italic">PLoS ONE 2021</span></span>
</span>
</th>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA</td>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.3</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.9.1.1.1" class="ltx_p" style="width:170.7pt;">Thiam et al. <cite class="ltx_cite ltx_citemacro_cite">Thiam et al., (<a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite> <span id="S3.T1.1.10.9.1.1.1.1" class="ltx_text ltx_font_italic">Frontiers 2021</span></span>
</span>
</th>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA, ECG, EMG</td>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.25</td>
</tr>
<tr id="S3.T1.1.11.10" class="ltx_tr">
<th id="S3.T1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.10.1.1.1" class="ltx_p" style="width:170.7pt;">Phan et al. <cite class="ltx_cite ltx_citemacro_cite">Phan et al., (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> <span id="S3.T1.1.11.10.1.1.1.1" class="ltx_text ltx_font_italic">IEEE Access 2023</span></span>
</span>
</th>
<td id="S3.T1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA, ECG, EMG</td>
<td id="S3.T1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LOSO</td>
<td id="S3.T1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.8</td>
</tr>
<tr id="S3.T1.1.12.11" class="ltx_tr">
<th id="S3.T1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">
<span id="S3.T1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.11.1.1.1" class="ltx_p" style="width:170.7pt;">1D CNN</span>
</span>
</th>
<td id="S3.T1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">EDA</td>
<td id="S3.T1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5-FOLD</td>
<td id="S3.T1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.2</td>
</tr>
<tr id="S3.T1.1.13.12" class="ltx_tr">
<th id="S3.T1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.12.1.1.1" class="ltx_p" style="width:170.7pt;">R3D model</span>
</span>
</th>
<td id="S3.T1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Video</td>
<td id="S3.T1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5-FOLD</td>
<td id="S3.T1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.9</td>
</tr>
<tr id="S3.T1.1.14.13" class="ltx_tr">
<th id="S3.T1.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.13.1.1.1" class="ltx_p" style="width:170.7pt;">Feature concatenation</span>
</span>
</th>
<td id="S3.T1.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA + Video</td>
<td id="S3.T1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5-FOLD</td>
<td id="S3.T1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.5</td>
</tr>
<tr id="S3.T1.1.15.14" class="ltx_tr">
<th id="S3.T1.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.14.1.1.1" class="ltx_p" style="width:170.7pt;">Vanilla transformer fusion</span>
</span>
</th>
<td id="S3.T1.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EDA + Video</td>
<td id="S3.T1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5-FOLD</td>
<td id="S3.T1.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.8</td>
</tr>
<tr id="S3.T1.1.16.15" class="ltx_tr">
<th id="S3.T1.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.16.15.1.1.1" class="ltx_p" style="width:170.7pt;">Joint Multimodal Transformer (<span id="S3.T1.1.16.15.1.1.1.1" class="ltx_text ltx_font_bold">ours</span>)</span>
</span>
</th>
<td id="S3.T1.1.16.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">EDA + Video</td>
<td id="S3.T1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5-FOLD</td>
<td id="S3.T1.1.16.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.16.15.4.1" class="ltx_text ltx_font_bold">89.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Methodology</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Affwild2</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">was put forward by <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">Kollias et al., 2019b </a></cite>. It is one of the largest and most comprehensive datasets for affective computing focused in-the-wild scenarios. The dataset is also a part of the Affective Behavior Analysis in the Wild (ABAW) challenge <cite class="ltx_cite ltx_citemacro_cite">Aslam et al., (<a href="#bib.bib2" title="" class="ltx_ref">2024</a>); Kollias et al., (<a href="#bib.bib22" title="" class="ltx_ref">2024</a>; <a href="#bib.bib21" title="" class="ltx_ref">2023</a>); <a href="#bib.bib17" title="" class="ltx_ref">Kollias, 2023b </a>; <a href="#bib.bib16" title="" class="ltx_ref">Kollias, 2023a </a>; Aslam et al., (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Kollias, (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); <a href="#bib.bib27" title="" class="ltx_ref">Kollias and Zafeiriou, 2021b </a>; <a href="#bib.bib26" title="" class="ltx_ref">Kollias and Zafeiriou, 2021a </a>; Kollias et al., (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>; <a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Kollias and Zafeiriou, (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>); <a href="#bib.bib24" title="" class="ltx_ref">Kollias et al., 2019c </a>; <a href="#bib.bib19" title="" class="ltx_ref">Kollias et al., 2019a </a>; Zafeiriou et al., (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>. The dataset comprises <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="564" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">564</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">564</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">564</annotation></semantics></math> videos that were collected from YouTube. These videos are labeled for three main affective computing tasks: i) categorical expression recognition, ii) continuous valence arousal prediction, and iii) action unit detection. The dataset comes with a train, validation, and test split with 341, 71, and 152 videos, respectively. The continuous valence/arousal annotation set is used to validate the proposed method.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Biovid Heat Pain Database:</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The BioVid Heat Pain Database <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref">Werner et al., 2014b </a></cite> comprises 87 subjects where heat pain was induced experimentally at the right arm with four different intensities. The data comprises video and depth map video from a Kinect camera, galvanic skin response (EDA), electromyograph (EMG) on trapezius muscle, and electrocardiogram (ECG). The Biovid dataset has various partitions from Part A through E. These partitions differ in the modalities, annotations, and tasks. We use Part A of the dataset and utilize the video and raw EDA data to validate our proposed method.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Affwild2:</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px1.p1.4" class="ltx_p">In the visual modality, cropped and aligned facial images provided with the dataset are used <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">Kollias and Zafeiriou, 2021b </a></cite>. Black frames (zero pixels) replace missing frames in the visual modality. These images are then fed to a 3D network that takes the input size of 224<math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><times id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">\times</annotation></semantics></math>224.
A clip length of 8 is used, which makes up a sub-sequence of 64 frames. Each sub-sequence contains eight clips. A dropout with a value of 0.8 was used in the linear layers for network regularization. The network was optimized using the stochastic gradient descent (SGD) iterative method with an initial learning rate (LR) of <math id="S4.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><msup id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mo id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3a" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2">10</cn><apply id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3"><minus id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">10^{-3}</annotation></semantics></math>. The batch size used for the visual modality was 8. For further generalization, random cropping and random horizontal flips were added as data augmentation. The maximum number of epochs is 50, with early stopping for model selection. In the audio modality, the audio from each video is separated and resampled for 44 kHz. Following the segmentation in the visual modality, small vocal segments are extracted corresponding to the length of the clip.
A ResNet18 is used to extract the features from the spectrograms. A Discrete Fourier transform with a length of 1024, a hop length of 10 msec, and a window length of 20 msec is used to obtain spectrograms of each vocal segment. The spectrograms have a resolution of 64<math id="S4.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.1a"><mo id="S4.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.1b"><times id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.1c">\times</annotation></semantics></math>107 pixels, corresponding to a single clip in the visual modality. Other preprocessing of spectrograms include mean and variance normalization, as well as conversion to log-power spectrum.
The first convolutional layer in the pretrained ResNet model is adapted to take in the single-channel spectrograms. The learning rate of <math id="S4.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="1\times 10^{-2}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S4.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.1" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mn id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml"><mo id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3a" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.2" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1"><times id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2">1</cn><apply id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.2">10</cn><apply id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3"><minus id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.4.m4.1c">1\times 10^{-2}</annotation></semantics></math> is used and Adam optimizer is used to optimize the network. 64 batch size is set for the audio modality.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px1.p2.3" class="ltx_p">The audio and visual backbones are frozen to train the A-V fusion network and only train the whole transformer-based fusion model. Each of these backbones outputs deep feature vectors of dimension 512. These features are concatenated (to obtain the joint feature representation, a feature vector of dimension 1024) and then fed to an FC layer to reduce dimensionality to 512. Each of these features is fed to the fusion model, as we can see in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Attention-Based and Transformer Methods ‣ 2 Related Work in Emotion Recognition ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. At the output of each of the six cross-attention modules, the feature vector of dimension 512 is outputted. These six feature vectors are then stacked to form a sequence, which is then fed to a transformer self-attention block. This block dynamically selects and weighs these feature vectors. The final attended features are fed to a FC layer for final prediction.
We perform a grid search to find the optimal learning rate and batch size for our fusion network. Thus, we use a list of learning rates: [<math id="S4.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="8\times 10^{-4}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mn id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.2" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3a" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.2" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1"><times id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2">8</cn><apply id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3"><minus id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.1.m1.1c">8\times 10^{-4}</annotation></semantics></math>, <math id="S4.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="6\times 10^{-4}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S4.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.2" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3a" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.2" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1"><times id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2">6</cn><apply id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3"><minus id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.2.m2.1c">6\times 10^{-4}</annotation></semantics></math>, <math id="S4.SS2.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.3.m3.1a"><mrow id="S4.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"><mn id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.cmml"><mn id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.2" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.cmml"><mo id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3a" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.2" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1"><times id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2">3</cn><apply id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.2">10</cn><apply id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3"><minus id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.3.m3.1c">3\times 10^{-4}</annotation></semantics></math>] and an SGD optimizer to optimize the fusion network. We use a batch size of 32, and the maximum number of epochs is 5, with early stopping for fusion model selection. We select the best model among the learning rates listed before.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Biovid:</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">In the visual modality, the faces are cropped and aligned using an MTCNN. We apply the frame retention strategy for missing frames where the face is not visible, and the MTCNN cannot capture any frame. Further, to ensure noise-free input to the visual model, we clip the first 2 seconds of the video and the last 0.5 seconds at the end because the subjects show no signs of pain during this duration. The total number of frames is 75. The extracted faces are fed to an R3D model for a visual feature extractor. The batch size is set to 64. The network is separately optimized using the SGD optimizer and the learning rate of <math id="S4.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><msup id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3a" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2">10</cn><apply id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3"><minus id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">10^{-3}</annotation></semantics></math>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Description of the architecture of the custom 1D-CNN for the physiological backbone.</figcaption>
<table id="S4.T2.32" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.32.33.1" class="ltx_tr">
<td id="S4.T2.32.33.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.32.33.1.1.1" class="ltx_text ltx_font_bold">Layer</span></td>
<td id="S4.T2.32.33.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.32.33.1.2.1" class="ltx_text ltx_font_bold">No. of</span></td>
<td id="S4.T2.32.33.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.32.33.1.3.1" class="ltx_text ltx_font_bold">Size of</span></td>
<td id="S4.T2.32.33.1.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.32.33.1.5" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T2.32.34.2" class="ltx_tr">
<td id="S4.T2.32.34.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.T2.32.34.2.1.1" class="ltx_text ltx_font_bold">type</span></td>
<td id="S4.T2.32.34.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.32.34.2.2.1" class="ltx_text ltx_font_bold">Filters</span></td>
<td id="S4.T2.32.34.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.32.34.2.3.1" class="ltx_text ltx_font_bold">Kernel</span></td>
<td id="S4.T2.32.34.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.32.34.2.4.1" class="ltx_text ltx_font_bold">Stride</span></td>
<td id="S4.T2.32.34.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.32.34.2.5.1" class="ltx_text ltx_font_bold">Output</span></td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">
<span id="S4.T2.3.3.4.1" class="ltx_ERROR undefined">\makecell</span>Input</td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="2816" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mn id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">2816</mn><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><cn type="integer" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">2816</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">2816</annotation></semantics></math> <math id="S4.T2.2.2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.2.m2.1a"><mo id="S4.T2.2.2.2.m2.1.1" xref="S4.T2.2.2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m2.1b"><times id="S4.T2.2.2.2.m2.1.1.cmml" xref="S4.T2.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.3.3.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.T2.3.3.3.m3.1a"><mn id="S4.T2.3.3.3.m3.1.1" xref="S4.T2.3.3.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m3.1b"><cn type="integer" id="S4.T2.3.3.3.m3.1.1.cmml" xref="S4.T2.3.3.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m3.1c">1</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.7" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1st Conv</td>
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mn id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><cn type="integer" id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">32</annotation></semantics></math></td>
<td id="S4.T2.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.5.5.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T2.5.5.2.m1.1a"><mn id="S4.T2.5.5.2.m1.1.1" xref="S4.T2.5.5.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.2.m1.1b"><cn type="integer" id="S4.T2.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.2.m1.1c">5</annotation></semantics></math></td>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.6.6.3.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T2.6.6.3.m1.1a"><mn id="S4.T2.6.6.3.m1.1.1" xref="S4.T2.6.6.3.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.3.m1.1b"><cn type="integer" id="S4.T2.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.3.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.3.m1.1c">2</annotation></semantics></math></td>
<td id="S4.T2.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S4.T2.7.7.4.m1.1" class="ltx_Math" alttext="1406" display="inline"><semantics id="S4.T2.7.7.4.m1.1a"><mn id="S4.T2.7.7.4.m1.1.1" xref="S4.T2.7.7.4.m1.1.1.cmml">1406</mn><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.4.m1.1b"><cn type="integer" id="S4.T2.7.7.4.m1.1.1.cmml" xref="S4.T2.7.7.4.m1.1.1">1406</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.4.m1.1c">1406</annotation></semantics></math> <math id="S4.T2.8.8.5.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.8.8.5.m2.1a"><mo id="S4.T2.8.8.5.m2.1.1" xref="S4.T2.8.8.5.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.5.m2.1b"><times id="S4.T2.8.8.5.m2.1.1.cmml" xref="S4.T2.8.8.5.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.5.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.9.9.6.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.T2.9.9.6.m3.1a"><mn id="S4.T2.9.9.6.m3.1.1" xref="S4.T2.9.9.6.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.6.m3.1b"><cn type="integer" id="S4.T2.9.9.6.m3.1.1.cmml" xref="S4.T2.9.9.6.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.6.m3.1c">32</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.12.12" class="ltx_tr">
<td id="S4.T2.12.12.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ReLU</td>
<td id="S4.T2.12.12.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.12.12.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.12.12.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.12.12.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S4.T2.10.10.1.m1.1" class="ltx_Math" alttext="1406" display="inline"><semantics id="S4.T2.10.10.1.m1.1a"><mn id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml">1406</mn><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><cn type="integer" id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1">1406</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">1406</annotation></semantics></math> <math id="S4.T2.11.11.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.11.11.2.m2.1a"><mo id="S4.T2.11.11.2.m2.1.1" xref="S4.T2.11.11.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.2.m2.1b"><times id="S4.T2.11.11.2.m2.1.1.cmml" xref="S4.T2.11.11.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.2.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.12.12.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.T2.12.12.3.m3.1a"><mn id="S4.T2.12.12.3.m3.1.1" xref="S4.T2.12.12.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.3.m3.1b"><cn type="integer" id="S4.T2.12.12.3.m3.1.1.cmml" xref="S4.T2.12.12.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.3.m3.1c">32</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.16.16" class="ltx_tr">
<td id="S4.T2.16.16.5" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Max Pooling</td>
<td id="S4.T2.16.16.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.13.13.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.13.13.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T2.13.13.1.m1.1a"><mn id="S4.T2.13.13.1.m1.1.1" xref="S4.T2.13.13.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.1.m1.1b"><cn type="integer" id="S4.T2.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.1.m1.1c">2</annotation></semantics></math></td>
<td id="S4.T2.16.16.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.16.16.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S4.T2.14.14.2.m1.1" class="ltx_Math" alttext="703" display="inline"><semantics id="S4.T2.14.14.2.m1.1a"><mn id="S4.T2.14.14.2.m1.1.1" xref="S4.T2.14.14.2.m1.1.1.cmml">703</mn><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.2.m1.1b"><cn type="integer" id="S4.T2.14.14.2.m1.1.1.cmml" xref="S4.T2.14.14.2.m1.1.1">703</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.2.m1.1c">703</annotation></semantics></math> <math id="S4.T2.15.15.3.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.15.15.3.m2.1a"><mo id="S4.T2.15.15.3.m2.1.1" xref="S4.T2.15.15.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.3.m2.1b"><times id="S4.T2.15.15.3.m2.1.1.cmml" xref="S4.T2.15.15.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.3.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.16.16.4.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.T2.16.16.4.m3.1a"><mn id="S4.T2.16.16.4.m3.1.1" xref="S4.T2.16.16.4.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.4.m3.1b"><cn type="integer" id="S4.T2.16.16.4.m3.1.1.cmml" xref="S4.T2.16.16.4.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.4.m3.1c">32</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.22.22" class="ltx_tr">
<td id="S4.T2.22.22.7" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">2nd Conv</td>
<td id="S4.T2.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.17.17.1.m1.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.T2.17.17.1.m1.1a"><mn id="S4.T2.17.17.1.m1.1.1" xref="S4.T2.17.17.1.m1.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.1.m1.1b"><cn type="integer" id="S4.T2.17.17.1.m1.1.1.cmml" xref="S4.T2.17.17.1.m1.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.1.m1.1c">64</annotation></semantics></math></td>
<td id="S4.T2.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.18.18.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T2.18.18.2.m1.1a"><mn id="S4.T2.18.18.2.m1.1.1" xref="S4.T2.18.18.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.2.m1.1b"><cn type="integer" id="S4.T2.18.18.2.m1.1.1.cmml" xref="S4.T2.18.18.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.2.m1.1c">5</annotation></semantics></math></td>
<td id="S4.T2.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.19.19.3.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.T2.19.19.3.m1.1a"><mn id="S4.T2.19.19.3.m1.1.1" xref="S4.T2.19.19.3.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.3.m1.1b"><cn type="integer" id="S4.T2.19.19.3.m1.1.1.cmml" xref="S4.T2.19.19.3.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.3.m1.1c">1</annotation></semantics></math></td>
<td id="S4.T2.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S4.T2.20.20.4.m1.1" class="ltx_Math" alttext="699" display="inline"><semantics id="S4.T2.20.20.4.m1.1a"><mn id="S4.T2.20.20.4.m1.1.1" xref="S4.T2.20.20.4.m1.1.1.cmml">699</mn><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.4.m1.1b"><cn type="integer" id="S4.T2.20.20.4.m1.1.1.cmml" xref="S4.T2.20.20.4.m1.1.1">699</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.4.m1.1c">699</annotation></semantics></math> <math id="S4.T2.21.21.5.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.21.21.5.m2.1a"><mo id="S4.T2.21.21.5.m2.1.1" xref="S4.T2.21.21.5.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.5.m2.1b"><times id="S4.T2.21.21.5.m2.1.1.cmml" xref="S4.T2.21.21.5.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.5.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.22.22.6.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.T2.22.22.6.m3.1a"><mn id="S4.T2.22.22.6.m3.1.1" xref="S4.T2.22.22.6.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.6.m3.1b"><cn type="integer" id="S4.T2.22.22.6.m3.1.1.cmml" xref="S4.T2.22.22.6.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.6.m3.1c">64</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.25.25" class="ltx_tr">
<td id="S4.T2.25.25.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ReLU</td>
<td id="S4.T2.25.25.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.25.25.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.25.25.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.25.25.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S4.T2.23.23.1.m1.1" class="ltx_Math" alttext="699" display="inline"><semantics id="S4.T2.23.23.1.m1.1a"><mn id="S4.T2.23.23.1.m1.1.1" xref="S4.T2.23.23.1.m1.1.1.cmml">699</mn><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.1.m1.1b"><cn type="integer" id="S4.T2.23.23.1.m1.1.1.cmml" xref="S4.T2.23.23.1.m1.1.1">699</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.1.m1.1c">699</annotation></semantics></math> <math id="S4.T2.24.24.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.24.24.2.m2.1a"><mo id="S4.T2.24.24.2.m2.1.1" xref="S4.T2.24.24.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.2.m2.1b"><times id="S4.T2.24.24.2.m2.1.1.cmml" xref="S4.T2.24.24.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.2.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.25.25.3.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.T2.25.25.3.m3.1a"><mn id="S4.T2.25.25.3.m3.1.1" xref="S4.T2.25.25.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.3.m3.1b"><cn type="integer" id="S4.T2.25.25.3.m3.1.1.cmml" xref="S4.T2.25.25.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.3.m3.1c">64</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.29.29" class="ltx_tr">
<td id="S4.T2.29.29.5" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Max Pooling</td>
<td id="S4.T2.29.29.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.26.26.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.26.26.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T2.26.26.1.m1.1a"><mn id="S4.T2.26.26.1.m1.1.1" xref="S4.T2.26.26.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.1.m1.1b"><cn type="integer" id="S4.T2.26.26.1.m1.1.1.cmml" xref="S4.T2.26.26.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.1.m1.1c">2</annotation></semantics></math></td>
<td id="S4.T2.29.29.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.29.29.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S4.T2.27.27.2.m1.1" class="ltx_Math" alttext="349" display="inline"><semantics id="S4.T2.27.27.2.m1.1a"><mn id="S4.T2.27.27.2.m1.1.1" xref="S4.T2.27.27.2.m1.1.1.cmml">349</mn><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.2.m1.1b"><cn type="integer" id="S4.T2.27.27.2.m1.1.1.cmml" xref="S4.T2.27.27.2.m1.1.1">349</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.2.m1.1c">349</annotation></semantics></math> <math id="S4.T2.28.28.3.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.28.28.3.m2.1a"><mo id="S4.T2.28.28.3.m2.1.1" xref="S4.T2.28.28.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.28.28.3.m2.1b"><times id="S4.T2.28.28.3.m2.1.1.cmml" xref="S4.T2.28.28.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.28.3.m2.1c">\times</annotation></semantics></math> <math id="S4.T2.29.29.4.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.T2.29.29.4.m3.1a"><mn id="S4.T2.29.29.4.m3.1.1" xref="S4.T2.29.29.4.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.4.m3.1b"><cn type="integer" id="S4.T2.29.29.4.m3.1.1.cmml" xref="S4.T2.29.29.4.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.4.m3.1c">64</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.30.30" class="ltx_tr">
<td id="S4.T2.30.30.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1st FC</td>
<td id="S4.T2.30.30.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.30.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.30.30.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.30.30.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.30.30.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.T2.30.30.1.m1.1a"><mn id="S4.T2.30.30.1.m1.1.1" xref="S4.T2.30.30.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.1.m1.1b"><cn type="integer" id="S4.T2.30.30.1.m1.1.1.cmml" xref="S4.T2.30.30.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.1.m1.1c">512</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.31.31" class="ltx_tr">
<td id="S4.T2.31.31.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ReLU</td>
<td id="S4.T2.31.31.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.31.31.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.31.31.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.31.31.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.31.31.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.T2.31.31.1.m1.1a"><mn id="S4.T2.31.31.1.m1.1.1" xref="S4.T2.31.31.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.T2.31.31.1.m1.1b"><cn type="integer" id="S4.T2.31.31.1.m1.1.1.cmml" xref="S4.T2.31.31.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.31.1.m1.1c">512</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.32.32" class="ltx_tr">
<td id="S4.T2.32.32.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">2nd FC</td>
<td id="S4.T2.32.32.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.32.32.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.32.32.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.32.32.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.32.32.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T2.32.32.1.m1.1a"><mn id="S4.T2.32.32.1.m1.1.1" xref="S4.T2.32.32.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.1.m1.1b"><cn type="integer" id="S4.T2.32.32.1.m1.1.1.cmml" xref="S4.T2.32.32.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.1.m1.1c">2</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">For the physiological modality, the EDA is used. The signal is clipped to correspond to the visual modality and fed to a 1D CNN. The architecture of the custom 1D-CNN is shown in Table <a href="#S4.T2" title="Table 2 ‣ Biovid: ‣ 4.2 Implementation Details ‣ 4 Experimental Methodology ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The CNN outputs a 512-dimensional feature vector. The model is optimized using the SGD optimizer with a learning rate of <math id="S4.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.SSS0.Px2.p2.1.m1.1a"><msup id="S4.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mo id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3a" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.2">10</cn><apply id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3"><minus id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px2.p2.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p2.1.m1.1c">10^{-4}</annotation></semantics></math>. The batch size for the physiological backbone is set to 1024.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F3.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\includegraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F3.2" class="ltx_p ltx_figure_panel ltx_align_center">[scale=0.90]biovid-img2.png</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the proposed joint multimodal transformer architecture used for the Biovid pain estimation task. The blue branch shows the visual backbone, and the yellow branch is the physiological backbone. The joint representation is shown with a red block. The three feature vectors are fed into the joint transformer block.</figcaption>
</figure>
<div id="S4.SS2.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p3.1" class="ltx_p">For the modality fusion, the two feature vectors and the joint representation are fed to the joint transformer block, as show in Figure <a href="#S4.F3" title="Figure 3 ‣ Biovid: ‣ 4.2 Implementation Details ‣ 4 Experimental Methodology ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The FC layers are removed from both backbones that were added in the backbone training phase. 512-dimensional feature vectors from visual and physiological backbones are obtained and fed to the joint multimodal transformer module. The backbones are frozen, and the joint transformer block is optimized using the ADAM optimizer with a learning rate of <math id="S4.SS2.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="5\times 10^{-6}" display="inline"><semantics id="S4.SS2.SSS0.Px2.p3.1.m1.1a"><mrow id="S4.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.2" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.1" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.2" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3a" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.2" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p3.1.m1.1b"><apply id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1"><times id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.2">5</cn><apply id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3"><minus id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.SSS0.Px2.p3.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p3.1.m1.1c">5\times 10^{-6}</annotation></semantics></math>, and the batch size is set to 128.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with the State-of-the-Art</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Multi-transformer Attention Fusion ‣ 3 Proposed Approach ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the performance of our fusion model on the Biovid Heat Pain Database. We performed 5-fold cross-validation to pick up the best average fusion model. It can be seen from the table that the proposed model can achieve state-of-the-art performance with multimodal input while using 5-fold cross-validation. The physiological modality is stronger in the Biovid database. Many studies have validated the models on physiological modality. Our empirical results show that the EDA-only accuracy is 77.2%, whereas the visual-only accuracy is 72.9%. The proposed model can improve over unimodal performance and achieves state-of-the-art performance on the Biovid dataset. We also compare it with standard fusion techniques like feature concatenation. For a fair comparison, we keep all the parameters the same. The proposed model improves 6% over simple feature concatenation and 1.3% over a vanilla multimodal transformer i.e. without a joint representation. Figure <a href="#S5.F4" title="Figure 4 ‣ 5.1 Comparison with the State-of-the-Art ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the visualization of the attention weights generated by the joint transformer model. It can be seen that the model gives more weightage to the physiological modality.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F4.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\includegraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F4.2" class="ltx_p ltx_figure_panel ltx_align_center">[scale=0.36]awv_1.png</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Visualization of attention weights for visual and physiological modalities on the Biovid heat pain database. The facial frames are taken 1400 msec each.</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Comparison with the State-of-the-Art ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the valence and arousal CCC values on the Affwild2 official validation set and custom-defined folds to increase generalizability. On the official validation set, the proposed method achieves a 0.666 average with 0.717 valence and 0.614 arousal.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>CCC for valence and arousal of the fusion model trained on different folds of the Affwild2 validation set. Highest scores are indicated in bold.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Validation Set</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">Official</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.717</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.614</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.666</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">fold-1</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.705</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">0.683</span></td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.694</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">fold-2</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.623</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.682</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">fold-3</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.657</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.637</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.647</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<td id="S5.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">fold-4</td>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.6.5.2.1" class="ltx_text ltx_font_bold">0.760</span></td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.666</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.6.5.4.1" class="ltx_text ltx_font_bold">0.713</span></td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<td id="S5.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">fold-5</td>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.684</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.629</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.657</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Comparison with the State-of-the-Art ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the proposed method with the state-of-the-art on Affwild2 development set. Our method achieves a 0.722 average with 0.76 valence and 0.68 arousal.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>CCC performance of the proposed and state-of-the-art methods for A-V fusion on the Affwild2 development set.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_tt">
<span id="S5.T4.1.2.2.1.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Multimodal</th>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.1.2.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.1.2.2.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<th id="S5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l">Transformer (<span id="S5.T4.1.3.3.1.1" class="ltx_text ltx_font_bold">ours</span>)</th>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.3.3.2.1" class="ltx_text ltx_font_bold">0.760</span></td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.684</td>
<td id="S5.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.3.3.4.1" class="ltx_text ltx_font_bold">0.722</span></td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<th id="S5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Situ-RUCAIM3 <cite class="ltx_cite ltx_citemacro_cite">Meng et al., (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.655</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.4.4.3.1" class="ltx_text ltx_font_bold">0.709</span></td>
<td id="S5.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.682</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<th id="S5.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t">
<span id="S5.T4.1.5.5.1.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Cross-</th>
<td id="S5.T4.1.5.5.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T4.1.5.5.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T4.1.5.5.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T4.1.6.6" class="ltx_tr">
<th id="S5.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l">Attention <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">Praveen et al., 2022a </a></cite>
</th>
<td id="S5.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">0.670</td>
<td id="S5.T4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">0.590</td>
<td id="S5.T4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">0.630</td>
</tr>
<tr id="S5.T4.1.7.7" class="ltx_tr">
<th id="S5.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">FlyingPigs <cite class="ltx_cite ltx_citemacro_cite">Zhang et al., (<a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.559</td>
<td id="S5.T4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.671</td>
<td id="S5.T4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.615</td>
</tr>
<tr id="S5.T4.1.8.8" class="ltx_tr">
<th id="S5.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">PRL <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al., (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.390</td>
<td id="S5.T4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.540</td>
<td id="S5.T4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.465</td>
</tr>
<tr id="S5.T4.1.9.9" class="ltx_tr">
<th id="S5.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">AU-NO <cite class="ltx_cite ltx_citemacro_cite">Karas et al., (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.361</td>
<td id="S5.T4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.551</td>
<td id="S5.T4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.456</td>
</tr>
<tr id="S5.T4.1.10.10" class="ltx_tr">
<th id="S5.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">HSE-NN <cite class="ltx_cite ltx_citemacro_cite">Savchenko, (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.408</td>
<td id="S5.T4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.477</td>
<td id="S5.T4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.443</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5.1 Comparison with the State-of-the-Art ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the proposed method with the state-of-the-art on the Affwild2 test set. The proposed method can achieve 0.42 valence and 0.46 arousal. The average of valence and arousal is 0.443. The proposed method significantly improves over the baseline, which is 0.17 average. It is essential to mention here that the other methods that achieve higher performance are due to extensive pertaining, the use of additional modalities like text, and the use of more robust backbones. For a fair comparison, we use a similar setting to Praveen et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">Praveen et al., 2022a </a></cite>, which includes similar pretraining and identical backbones for audio and visual modalities. They used joint cross-attention to fuse the two modalities and achieve a 0.369 average. On the other hand, the proposed model uses a joint multimodal transformer-based fusion method and can achieve a 0.443 average. The proposed method improves 7% over the joint cross-attention-based method.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>CCC performance of the proposed method and state-of-the-art methods for A-V fusion on the Affwild2 test set.</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T5.1.2.2" class="ltx_tr">
<th id="S5.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Situ-RUCAIM3 <cite class="ltx_cite ltx_citemacro_cite">Meng et al., (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T5.1.2.2.2.1" class="ltx_text ltx_font_bold">0.606</span></td>
<td id="S5.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.596</td>
<td id="S5.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T5.1.2.2.4.1" class="ltx_text ltx_font_bold">0.601</span></td>
</tr>
<tr id="S5.T5.1.3.3" class="ltx_tr">
<th id="S5.T5.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">FlyingPigs <cite class="ltx_cite ltx_citemacro_cite">Zhang et al., (<a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.520</td>
<td id="S5.T5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.3.3.3.1" class="ltx_text ltx_font_bold">0.602</span></td>
<td id="S5.T5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.561</td>
</tr>
<tr id="S5.T5.1.4.4" class="ltx_tr">
<th id="S5.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">PRL <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al., (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.450</td>
<td id="S5.T5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.445</td>
<td id="S5.T5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.448</td>
</tr>
<tr id="S5.T5.1.5.5" class="ltx_tr">
<th id="S5.T5.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t">
<span id="S5.T5.1.5.5.1.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Multimodal</th>
<td id="S5.T5.1.5.5.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.1.5.5.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.1.5.5.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T5.1.6.6" class="ltx_tr">
<th id="S5.T5.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l">Transformer (<span id="S5.T5.1.6.6.1.1" class="ltx_text ltx_font_bold">ours</span>)</th>
<td id="S5.T5.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">0.420</td>
<td id="S5.T5.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">0.467</td>
<td id="S5.T5.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">0.443</td>
</tr>
<tr id="S5.T5.1.7.7" class="ltx_tr">
<th id="S5.T5.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">HSE-NN <cite class="ltx_cite ltx_citemacro_cite">Savchenko, (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T5.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.417</td>
<td id="S5.T5.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.454</td>
<td id="S5.T5.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.436</td>
</tr>
<tr id="S5.T5.1.8.8" class="ltx_tr">
<th id="S5.T5.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">AU-NO <cite class="ltx_cite ltx_citemacro_cite">Karas et al., (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T5.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.418</td>
<td id="S5.T5.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.407</td>
<td id="S5.T5.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.413</td>
</tr>
<tr id="S5.T5.1.9.9" class="ltx_tr">
<th id="S5.T5.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t">
<span id="S5.T5.1.9.9.1.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Cross-</th>
<td id="S5.T5.1.9.9.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.1.9.9.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.1.9.9.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T5.1.10.10" class="ltx_tr">
<th id="S5.T5.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l">Attention <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">Praveen et al., 2022a </a></cite>
</th>
<td id="S5.T5.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">0.374</td>
<td id="S5.T5.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">0.363</td>
<td id="S5.T5.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">0.369</td>
</tr>
<tr id="S5.T5.1.11.11" class="ltx_tr">
<th id="S5.T5.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Baseline</th>
<td id="S5.T5.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.180</td>
<td id="S5.T5.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.170</td>
<td id="S5.T5.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.175</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablations</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">To see the effectiveness of the joint representation of the proposed method, we performed an ablation study. Table <a href="#S5.T6" title="Table 6 ‣ 5.2 Ablations ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results of the proposed method with and without the joint representation. On the Biovid dataset, the joint multimodal transformer improves by 1.3% over the vanilla multimodal transformer.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of the proposed approach compared to the vanilla multimodal transformer on test set. Valence and arousal for the Affwild2 dataset and accuracy for the Biovid dataset. I3D + ResNet18 backbones are used for the Affwild2 dataset and R3D + 1D CNN are used for the BioVid dataset. Default training/validation split is used in the Affwild2 dataset and 5-fold cross validation is performed on the BioVid dataset.</figcaption>
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T6.1.1.1.1.1" class="ltx_ERROR undefined">\multirow</span>1*<span id="S5.T6.1.1.1.1.2" class="ltx_text ltx_font_bold">Database</span>
</th>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S5.T6.1.2.2" class="ltx_tr">
<th id="S5.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">
<span id="S5.T6.1.2.2.1.1" class="ltx_ERROR undefined">\multirow</span>3*<span id="S5.T6.1.2.2.1.2" class="ltx_text ltx_font_bold">Affwild2</span>
</th>
<td id="S5.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt">
<span id="S5.T6.1.2.2.2.1" class="ltx_ERROR undefined">\makecell</span>[l]Vanilla Multimodal</td>
<td id="S5.T6.1.2.2.3" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T6.1.3.3" class="ltx_tr">
<th id="S5.T6.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Transformer</th>
<td id="S5.T6.1.3.3.2" class="ltx_td ltx_align_left">
<span id="S5.T6.1.3.3.2.1" class="ltx_ERROR undefined">\makecell</span>[r]V: <span id="S5.T6.1.3.3.2.2" class="ltx_text ltx_font_bold">0.432</span>
</td>
<td id="S5.T6.1.3.3.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.4.4" class="ltx_tr">
<th id="S5.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">A: 0.410</th>
<td id="S5.T6.1.4.4.2" class="ltx_td"></td>
<td id="S5.T6.1.4.4.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.5.5" class="ltx_tr">
<th id="S5.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Avg: 0.421</th>
<td id="S5.T6.1.5.5.2" class="ltx_td"></td>
<td id="S5.T6.1.5.5.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.6.6" class="ltx_tr">
<th id="S5.T6.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S5.T6.1.6.6.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T6.1.6.6.2.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Multimodal</td>
<td id="S5.T6.1.6.6.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.1.7.7" class="ltx_tr">
<th id="S5.T6.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Transformer</th>
<td id="S5.T6.1.7.7.2" class="ltx_td ltx_align_left">
<span id="S5.T6.1.7.7.2.1" class="ltx_ERROR undefined">\makecell</span>[r]V: 0.425</td>
<td id="S5.T6.1.7.7.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.8.8" class="ltx_tr">
<th id="S5.T6.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">A: <span id="S5.T6.1.8.8.1.1" class="ltx_text ltx_font_bold">0.450</span>
</th>
<td id="S5.T6.1.8.8.2" class="ltx_td"></td>
<td id="S5.T6.1.8.8.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.9.9" class="ltx_tr">
<th id="S5.T6.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Avg: <span id="S5.T6.1.9.9.1.1" class="ltx_text ltx_font_bold">0.438</span>
</th>
<td id="S5.T6.1.9.9.2" class="ltx_td"></td>
<td id="S5.T6.1.9.9.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.10.10" class="ltx_tr">
<th id="S5.T6.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T6.1.10.10.1.1" class="ltx_ERROR undefined">\multirow</span>3*<span id="S5.T6.1.10.10.1.2" class="ltx_text ltx_font_bold">Biovid</span>
</th>
<td id="S5.T6.1.10.10.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T6.1.10.10.2.1" class="ltx_ERROR undefined">\makecell</span>[l]Vanilla Multimodal</td>
<td id="S5.T6.1.10.10.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.1.11.11" class="ltx_tr">
<th id="S5.T6.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Transformer</th>
<td id="S5.T6.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r">87.8</td>
<td id="S5.T6.1.11.11.3" class="ltx_td"></td>
</tr>
<tr id="S5.T6.1.12.12" class="ltx_tr">
<th id="S5.T6.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="S5.T6.1.12.12.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T6.1.12.12.2.1" class="ltx_ERROR undefined">\makecell</span>[l]Joint Multimodal</td>
<td id="S5.T6.1.12.12.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T6.1.13.13" class="ltx_tr">
<th id="S5.T6.1.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Transformer</th>
<td id="S5.T6.1.13.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T6.1.13.13.2.1" class="ltx_text ltx_font_bold">89.1</span></td>
<td id="S5.T6.1.13.13.3" class="ltx_td ltx_border_b"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Table <a href="#S5.T7" title="Table 7 ‣ 5.2 Ablations ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the proposed model improves accuracy over the vanilla multimodal transformer by 1.8% with a R2D1 and ResNet18 backbone on the Affwild2 dataset. With an I3D and a ResNet18 backbone, results are improved by 1.7%, as shown in Table <a href="#S5.T8" title="Table 8 ‣ 5.2 Ablations ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Notice that the results on the test set are different from Table <a href="#S5.T5" title="Table 5 ‣ 5.1 Comparison with the State-of-the-Art ‣ 5 Results and Discussion ‣ Joint Multimodal Transformer for Emotion Recognition in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> because we used a different train/validation split.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>CCC performance of the backbones alone, and combined using baselines and the proposed A-V fusion method on the Affwild2 test set. Experiments are performed on the default training-validation split. R2D1 (visual) and ResNet18 (audio) backbones are used for all cases.</figcaption>
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">Fusion Model</span></td>
<td id="S5.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></td>
<td id="S5.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></td>
<td id="S5.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.1.1.4.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T7.1.2.2" class="ltx_tr">
<td id="S5.T7.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt">
<span id="S5.T7.1.2.2.1.1" class="ltx_ERROR undefined">\makecell</span>[c]R2D1 model only</td>
<td id="S5.T7.1.2.2.2" class="ltx_td ltx_border_tt"></td>
<td id="S5.T7.1.2.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T7.1.2.2.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T7.1.3.3" class="ltx_tr">
<td id="S5.T7.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l">(visual)</td>
<td id="S5.T7.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">0.194</td>
<td id="S5.T7.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.310</td>
<td id="S5.T7.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">0.252</td>
</tr>
<tr id="S5.T7.1.4.4" class="ltx_tr">
<td id="S5.T7.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T7.1.4.4.1.1" class="ltx_ERROR undefined">\makecell</span>[c]ResNet18 model only</td>
<td id="S5.T7.1.4.4.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T7.1.4.4.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T7.1.4.4.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T7.1.5.5" class="ltx_tr">
<td id="S5.T7.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l">(audio)</td>
<td id="S5.T7.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">0.273</td>
<td id="S5.T7.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">0.246</td>
<td id="S5.T7.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">0.260</td>
</tr>
<tr id="S5.T7.1.6.6" class="ltx_tr">
<td id="S5.T7.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.1.6.6.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Concat + FC layers</td>
<td id="S5.T7.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.320</td>
<td id="S5.T7.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.327</td>
<td id="S5.T7.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.323</td>
</tr>
<tr id="S5.T7.1.7.7" class="ltx_tr">
<td id="S5.T7.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T7.1.7.7.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Vanilla Transformer</td>
<td id="S5.T7.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T7.1.7.7.2.1" class="ltx_text ltx_font_bold">0.376</span></td>
<td id="S5.T7.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.334</td>
<td id="S5.T7.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.355</td>
</tr>
<tr id="S5.T7.1.8.8" class="ltx_tr">
<td id="S5.T7.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T7.1.8.8.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Joint Multimodal</td>
<td id="S5.T7.1.8.8.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T7.1.8.8.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T7.1.8.8.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T7.1.9.9" class="ltx_tr">
<td id="S5.T7.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l">Transformer (<span id="S5.T7.1.9.9.1.1" class="ltx_text ltx_font_bold">ours</span>)</td>
<td id="S5.T7.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.366</td>
<td id="S5.T7.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T7.1.9.9.3.1" class="ltx_text ltx_font_bold">0.379</span></td>
<td id="S5.T7.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T7.1.9.9.4.1" class="ltx_text ltx_font_bold">0.373</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>CCC performance of the backbones alone, and combined using baselines and the proposed A-V fusion method on the Affwild2 test set. Experiments are performed on the default training-validation split. I3D (visual) and ResNet18 (audio) backbones are used for all cases.</figcaption>
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<td id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Fusion Model</span></td>
<td id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></td>
<td id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></td>
<td id="S5.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T8.1.2.2" class="ltx_tr">
<td id="S5.T8.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt">
<span id="S5.T8.1.2.2.1.1" class="ltx_ERROR undefined">\makecell</span>[c]I3D model only</td>
<td id="S5.T8.1.2.2.2" class="ltx_td ltx_border_tt"></td>
<td id="S5.T8.1.2.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T8.1.2.2.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T8.1.3.3" class="ltx_tr">
<td id="S5.T8.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l">(visual)</td>
<td id="S5.T8.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">0.336</td>
<td id="S5.T8.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.422</td>
<td id="S5.T8.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">0.379</td>
</tr>
<tr id="S5.T8.1.4.4" class="ltx_tr">
<td id="S5.T8.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T8.1.4.4.1.1" class="ltx_ERROR undefined">\makecell</span>[c]ResNet18 model only</td>
<td id="S5.T8.1.4.4.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T8.1.4.4.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T8.1.4.4.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T8.1.5.5" class="ltx_tr">
<td id="S5.T8.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l">(audio)</td>
<td id="S5.T8.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">0.273</td>
<td id="S5.T8.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">0.246</td>
<td id="S5.T8.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">0.260</td>
</tr>
<tr id="S5.T8.1.6.6" class="ltx_tr">
<td id="S5.T8.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T8.1.6.6.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Concat + FC layers</td>
<td id="S5.T8.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.387</td>
<td id="S5.T8.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.453</td>
<td id="S5.T8.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.420</td>
</tr>
<tr id="S5.T8.1.7.7" class="ltx_tr">
<td id="S5.T8.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T8.1.7.7.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Vanilla Transformer</td>
<td id="S5.T8.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T8.1.7.7.2.1" class="ltx_text ltx_font_bold">0.432</span></td>
<td id="S5.T8.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.410</td>
<td id="S5.T8.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.421</td>
</tr>
<tr id="S5.T8.1.8.8" class="ltx_tr">
<td id="S5.T8.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S5.T8.1.8.8.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Joint Multimodal</td>
<td id="S5.T8.1.8.8.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T8.1.8.8.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T8.1.8.8.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T8.1.9.9" class="ltx_tr">
<td id="S5.T8.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l">Transformer (<span id="S5.T8.1.9.9.1.1" class="ltx_text ltx_font_bold">ours</span>)</td>
<td id="S5.T8.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">0.425</td>
<td id="S5.T8.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T8.1.9.9.3.1" class="ltx_text ltx_font_bold">0.450</span></td>
<td id="S5.T8.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T8.1.9.9.4.1" class="ltx_text ltx_font_bold">0.438</span></td>
</tr>
<tr id="S5.T8.1.10.10" class="ltx_tr">
<td id="S5.T8.1.10.10.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T8.1.10.10.1.1" class="ltx_ERROR undefined">\makecell</span>[c]Joint Cross Attention <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">Praveen et al., 2022a </a></cite>
</td>
<td id="S5.T8.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.374</td>
<td id="S5.T8.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.363</td>
<td id="S5.T8.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.369</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Multimodal emotion recognition systems outperform their unimodal counterparts, especially in the wild environment. The missing and noisy modality is a prevalent issue with in-the-wild emotion recognition systems. Many attention-based methods have been proposed in the literature to overcome this problem. These methods aim to weigh the modalities dynamically. This paper introduces a joint multimodal transformer for dimensional emotional recognition in the wild. This transformer-based architecture introduces a joint feature representation in order to add more redundancy and complementary between audio and visual data. The proposed model outperforms the state of the art on the Biovid dataset and improves 6% on the Affwild2 dataset. In the future, we aim to introduce more modalities and sophisticated backbones for effective feature extraction.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<span id="S6.p2.1" class="ltx_ERROR undefined">\FloatBarrier</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anagnostopoulos et al.,  (2015)</span>
<span class="ltx_bibblock">
Anagnostopoulos, C., Iliou, T., and Giannoukos, I. (2015).

</span>
<span class="ltx_bibblock">Features and classifiers for emotion recognition from speech: a
survey from 2000 to 2011.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Artif Intell Rev</span>, 43(6):155–177.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aslam et al.,  (2024)</span>
<span class="ltx_bibblock">
Aslam, M. H., Zeeshan, M. O., Belharbi, S., Pedersoli, M., Koerich, A. L.,
Bacon, S., and Granger, E. (2024).

</span>
<span class="ltx_bibblock">Distilling privileged multimodal information for expression
recognition using optimal transport.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">18th IEEE International Conference on Automatic Face and
Gesture Recognition</span>. arXiv preprint arXiv:2401.15489.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aslam et al.,  (2023)</span>
<span class="ltx_bibblock">
Aslam, M. H., Zeeshan, O., Pedersoli, M., Koerich, A. L., Bacon, S., and
Granger, E. (2023).

</span>
<span class="ltx_bibblock">Privileged knowledge distillation for dimensional emotion recognition
in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">CVPRw 2023: IEEE/CVF Conference on Computer Vision and
Pattern Recognition, Vancouver, Canada.</span>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al.,  (2014)</span>
<span class="ltx_bibblock">
Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., and Bengio, Y. (2014).

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Melo et al.,  (2021)</span>
<span class="ltx_bibblock">
de Melo, W. C., Granger, E., and Lopez, M. B. (2021).

</span>
<span class="ltx_bibblock">Mdn: A deep maximization-differentiation network for spatio-temporal
depression detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Tran. on Affective Computing</span>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al.,  (2021)</span>
<span class="ltx_bibblock">
Deng, D., Wu, L., and Shi, B. E. (2021).

</span>
<span class="ltx_bibblock">Iterative distillation for better uncertainty estimates in multitask
emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ICCVW</span>, pages 3550–3559.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dragomir et al.,  (2020)</span>
<span class="ltx_bibblock">
Dragomir, M.-C., Florea, C., and Pupezescu, V. (2020).

</span>
<span class="ltx_bibblock">Automatic subject independent pain intensity estimation using a deep
learning approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2020 International Conference on e-Health and Bioengineering
(EHB)</span>, pages 1–4.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al.,  (2021)</span>
<span class="ltx_bibblock">
Duan, B., Tang, H., Wang, W., Zong, Z., Yang, G., and Yan, Y. (2021).

</span>
<span class="ltx_bibblock">Audio-visual event localization via recursive fusion by joint
co-attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">WACV</span>, pages 4012–4021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman,  (1992)</span>
<span class="ltx_bibblock">
Ekman, P. (1992).

</span>
<span class="ltx_bibblock">An argument for basic emotions.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Cognition and Emotion</span>, 6(3-4):169–200.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al.,  (2016)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J. (2016).

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al.,  (2020)</span>
<span class="ltx_bibblock">
Huang, J., Tao, J., Liu, B., Lian, Z., and Niu, M. (2020).

</span>
<span class="ltx_bibblock">Multimodal transformer fusion for continuous emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 3507–3511.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kächele et al.,  (2016)</span>
<span class="ltx_bibblock">
Kächele, M., Thiam, P., Amirian, M., Schwenker, F., and Palm, G. (2016).

</span>
<span class="ltx_bibblock">Methods for person-centered continuous pain intensity assessment from
bio-physiological channels.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>,
10(5):854–864.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kächele et al.,  (2015)</span>
<span class="ltx_bibblock">
Kächele, M., Werner, P., Al-Hamadi, A., Palm, G., Walter, S., and
Schwenker, F. (2015).

</span>
<span class="ltx_bibblock">Bio-visual fusion for person-independent recognition of pain
intensity.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">International Workshop on Multiple Classifier Systems</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karas et al.,  (2022)</span>
<span class="ltx_bibblock">
Karas, V., Tellamekala, M. K., Mallol-Ragolta, A., Valstar, M., and Schuller,
B. W. (2022).

</span>
<span class="ltx_bibblock">Continuous-time audiovisual fusion with recurrence vs. attention for
in-the-wild affect recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.13285</span>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias,  (2022)</span>
<span class="ltx_bibblock">
Kollias, D. (2022).

</span>
<span class="ltx_bibblock">Abaw: Valence-arousal estimation, expression recognition, action unit
detection &amp; multi-task learning challenges.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2328–2336.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Kollias, D. (2023a).

</span>
<span class="ltx_bibblock">Abaw: learning from synthetic data &amp; multi-task learning challenges.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 157–172.
Springer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Kollias, D. (2023b).

</span>
<span class="ltx_bibblock">Multi-label compound expression recognition: C-expr database &amp;
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5589–5598.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias et al.,  (2020)</span>
<span class="ltx_bibblock">
Kollias, D., Schulc, A., Hajiyev, E., and Zafeiriou, S. (2020).

</span>
<span class="ltx_bibblock">Analysing affective behavior in the first abaw competition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">FG 2020</span>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
Kollias, D., Sharmanska, V., and Zafeiriou, S. (2019a).

</span>
<span class="ltx_bibblock">Face behavior a la carte: Expressions, affect and action units in a
single network.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.11111</span>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias et al.,  (2021)</span>
<span class="ltx_bibblock">
Kollias, D., Sharmanska, V., and Zafeiriou, S. (2021).

</span>
<span class="ltx_bibblock">Distribution matching for heterogeneous multi-task learning: a
large-scale face study.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.03790</span>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias et al.,  (2023)</span>
<span class="ltx_bibblock">
Kollias, D., Tzirakis, P., Baird, A., Cowen, A., and Zafeiriou, S. (2023).

</span>
<span class="ltx_bibblock">Abaw: Valence-arousal estimation, expression recognition, action unit
detection &amp; emotional reaction intensity estimation challenges.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 5888–5897.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias et al.,  (2024)</span>
<span class="ltx_bibblock">
Kollias, D., Tzirakis, P., Cowen, A., Zafeiriou, S., Shao, C., and Hu, G.
(2024).

</span>
<span class="ltx_bibblock">The 6th affective behavior analysis in-the-wild (abaw) competition.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.19344</span>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
Kollias, D., Tzirakis, P., Nicolaou, M. A., Papaioannou, A., Zhao, G.,
Schuller, B., Kotsia, I., and Zafeiriou, S. (2019b).

</span>
<span class="ltx_bibblock">Deep affect prediction in-the-wild: Aff-wild database and challenge,
deep architectures, and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IJCV</span>, 127:907–929.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
Kollias, D., Tzirakis, P., Nicolaou, M. A., Papaioannou, A., Zhao, G.,
Schuller, B., Kotsia, I., and Zafeiriou, S. (2019c).

</span>
<span class="ltx_bibblock">Deep affect prediction in-the-wild: Aff-wild database and challenge,
deep architectures, and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, pages 1–23.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kollias and Zafeiriou,  (2019)</span>
<span class="ltx_bibblock">
Kollias, D. and Zafeiriou, S. (2019).

</span>
<span class="ltx_bibblock">Expression, affect, action unit recognition: Aff-wild2, multi-task
learning and arcface.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.04855</span>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
Kollias, D. and Zafeiriou, S. (2021a).

</span>
<span class="ltx_bibblock">Affect analysis in-the-wild: Valence-arousal, expressions, action
units and a unified framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2103.15792</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
Kollias, D. and Zafeiriou, S. (2021b).

</span>
<span class="ltx_bibblock">Analysing affective behavior in the second abaw2 competition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ICCVW</span>, pages 3652–3660.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Kuhnke, F., Rumberg, L., and Ostermann, J. (2020a).

</span>
<span class="ltx_bibblock">Two-stream aural-visual affect analysis in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">FG Workshop</span>, pages 600–605.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
Kuhnke, F., Rumberg, L., and Ostermann, J. (2020b).

</span>
<span class="ltx_bibblock">Two-stream aural-visual affect analysis in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2020 15th IEEE International Conference on Automatic Face and
Gesture Recognition (FG 2020)</span>, pages 600–605. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lawrence and Lin,  (1989)</span>
<span class="ltx_bibblock">
Lawrence, I. and Lin, K. (1989).

</span>
<span class="ltx_bibblock">A concordance correlation coefficient to evaluate reproducibility.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Biometrics</span>, pages 255–268.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al.,  (2023)</span>
<span class="ltx_bibblock">
Le, H.-D., Lee, G.-S., Kim, S.-H., Kim, S., and Yang, H.-J. (2023).

</span>
<span class="ltx_bibblock">Multi-label multimodal emotion recognition with transformer-based
fusion and emotion-level representation learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 11:14742–14751.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al.,  (2021)</span>
<span class="ltx_bibblock">
Lee, J.-T., Jain, M., Park, H., and Yun, S. (2021).

</span>
<span class="ltx_bibblock">Cross-attentional audio-visual fusion for weakly-supervised action
localization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">ICLR</span>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Martinez and Picard,  (2017)</span>
<span class="ltx_bibblock">
Lopez-Martinez, D. and Picard, R. (2017).

</span>
<span class="ltx_bibblock">Multi-task neural networks for personalized pain recognition from
physiological signals.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2017 Seventh International Conference on Affective Computing
and Intelligent Interaction Workshops and Demos (ACIIW)</span>, pages 181–184.
IEEE.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Martinez and Picard,  (2018)</span>
<span class="ltx_bibblock">
Lopez-Martinez, D. and Picard, R. (2018).

</span>
<span class="ltx_bibblock">Continuous pain intensity estimation from autonomic signals with
recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">2018 40th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC)</span>, pages 5624–5627. IEEE.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al.,  (2019)</span>
<span class="ltx_bibblock">
Lu, J., Batra, D., Parikh, D., and Lee, S. (2019).

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al.,  (2023)</span>
<span class="ltx_bibblock">
Lu, Z., Ozek, B., and Kamarthi, S. (2023).

</span>
<span class="ltx_bibblock">Transformer encoder with multiscale deep learning for pain
classification using physiological signals.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.06845</span>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al.,  (2022)</span>
<span class="ltx_bibblock">
Meng, L., Liu, Y., Liu, X., Huang, Z., Jiang, W., Zhang, T., Deng, Y., Li, R.,
Wu, Y., Zhao, J., et al. (2022).

</span>
<span class="ltx_bibblock">Multi-modal emotion estimation for in-the-wild videos.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.13032</span>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morabit and Rivenq,  (2022)</span>
<span class="ltx_bibblock">
Morabit, S. E. and Rivenq, A. (2022).

</span>
<span class="ltx_bibblock">Pain detection from facial expressions based on transformers and
distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">2022 11th International Symposium on Signal, Image, Video and
Communications (ISIVC)</span>, pages 1–5.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ngiam et al.,  (2011)</span>
<span class="ltx_bibblock">
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A. Y. (2011).

</span>
<span class="ltx_bibblock">Multimodal deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th International Conference on
International Conference on Machine Learning</span>, ICML’11, page 689–696,
Madison, WI, USA. Omnipress.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al.,  (2021)</span>
<span class="ltx_bibblock">
Nguyen, D., Nguyen, D. T., Zeng, R., Nguyen, T. T., Tran, S., Nguyen, T. K.,
Sridharan, S., and Fookes, C. (2021).

</span>
<span class="ltx_bibblock">Deep auto-encoders with sequential learning for multimodal
dimensional emotion recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Multimedia</span>, pages 1–1.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al.,  (2022)</span>
<span class="ltx_bibblock">
Nguyen, H.-H., Huynh, V.-T., and Kim, S.-H. (2022).

</span>
<span class="ltx_bibblock">An ensemble approach for facial expression analysis in video.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.12891</span>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortega et al.,  (2019)</span>
<span class="ltx_bibblock">
Ortega, J. D. S., Cardinal, P., and Koerich, A. L. (2019).

</span>
<span class="ltx_bibblock">Emotion recognition using fusion of audio and video features.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">SMC</span>, pages 3847–3852.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parthasarathy and Sundaram,  (2020)</span>
<span class="ltx_bibblock">
Parthasarathy, S. and Sundaram, S. (2020).

</span>
<span class="ltx_bibblock">Training strategies to handle missing modalities for audio-visual
expression recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">ICMI</span>, page 400–404.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al.,  (2023)</span>
<span class="ltx_bibblock">
Phan, K. N., Iyortsuun, N. K., Pant, S., Yang, H.-J., and Kim, S.-H. (2023).

</span>
<span class="ltx_bibblock">Pain recognition with physiological signals using multi-level context
information.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 11:20114–20127.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pouromran et al.,  (2021)</span>
<span class="ltx_bibblock">
Pouromran, F., Radhakrishnan, S., and Kamarthi, S. (2021).

</span>
<span class="ltx_bibblock">Exploration of physiological sensors, features, and machine learning
models for pain intensity estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Pone Journal</span>, 16(7):e0254108.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
Praveen, R. G., de Melo, W. C., Ullah, N., Aslam, H., Zeeshan, O., Denorme, T.,
Pedersoli, M., Koerich, A. L., Bacon, S., Cardinal, P., et al. (2022a).

</span>
<span class="ltx_bibblock">A joint cross-attention model for audio-visual fusion in dimensional
emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">IEEE/CVF CVPR</span>, pages 2486–2495.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
Praveen, R. G., de Melo, W. C., Ullah, N., Aslam, H., Zeeshan, O., Denorme, T.,
Pedersoli, M., Koerich, A. L., Bacon, S., Cardinal, P., and Granger, E.
(2022b).

</span>
<span class="ltx_bibblock">A joint cross-attention model for audio-visual fusion in dimensional
emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">CVPRW</span>, pages 2485–2494.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajasekhar et al.,  (2021)</span>
<span class="ltx_bibblock">
Rajasekhar, G. P., Granger, E., and Cardinal, P. (2021).

</span>
<span class="ltx_bibblock">Cross attentional audio-visual fusion for dimensional emotion
recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">FG</span>, pages 1–8.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savchenko,  (2022)</span>
<span class="ltx_bibblock">
Savchenko, A. V. (2022).

</span>
<span class="ltx_bibblock">Frame-level prediction of facial expressions, valence, arousal and
action units for mobile devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.13436</span>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schoneveld et al.,  (2021)</span>
<span class="ltx_bibblock">
Schoneveld, L., Othmani, A., and Abdelkawy, H. (2021).

</span>
<span class="ltx_bibblock">Leveraging recent advances in deep learning for audio-visual emotion
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 146:1–7.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thiam et al.,  (2019)</span>
<span class="ltx_bibblock">
Thiam, P., Bellmann, P., Kestler, H. A., and Schwenker, F. (2019).

</span>
<span class="ltx_bibblock">Exploring deep physiological models for nociceptive pain recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 19(20):4503.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thiam et al.,  (2021)</span>
<span class="ltx_bibblock">
Thiam, P., Hihn, H., Braun, D. A., Kestler, H. A., and Schwenker, F. (2021).

</span>
<span class="ltx_bibblock">Multi-modal pain intensity assessment based on physiological signals:
A deep learning perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Frontiers in Physiology</span>, 12:720464.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran and Soleymani,  (2022)</span>
<span class="ltx_bibblock">
Tran, M. and Soleymani, M. (2022).

</span>
<span class="ltx_bibblock">A pre-trained audio-visual transformer for emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 4698–4702.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tzirakis et al.,  (2021)</span>
<span class="ltx_bibblock">
Tzirakis, P., Chen, J., Zafeiriou, S., and Schuller, B. (2021).

</span>
<span class="ltx_bibblock">End-to-end multimodal affect recognition in real-world environments.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Information Fusion</span>, 68:46–53.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tzirakis et al.,  (2017)</span>
<span class="ltx_bibblock">
Tzirakis, P., Trigeorgis, G., Nicolaou, M. A., Schuller, B. W., and Zafeiriou,
S. (2017).

</span>
<span class="ltx_bibblock">End-to-end multimodal emotion recognition using deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">IEEE J. of Selected Topics in Signal Processing</span>,
11(8):1301–1309.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.,  (2020)</span>
<span class="ltx_bibblock">
Wang, R., Xu, K., Feng, H., and Chen, W. (2020).

</span>
<span class="ltx_bibblock">Hybrid rnn-ann based deep physiological network for pain recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">2020 42nd Annual International Conference of the IEEE
Engineering in Medicine &amp; Biology Society (EMBC)</span>, pages 5584–5587. IEEE.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al.,  (2020)</span>
<span class="ltx_bibblock">
Wei, X., Zhang, T., Li, Y., Zhang, Y., and Wu, F. (2020).

</span>
<span class="ltx_bibblock">Multi-modality cross attention network for image and sentence
matching.

</span>
<span class="ltx_bibblock">In <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, pages 10938–10947.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Werner et al.,  (2016)</span>
<span class="ltx_bibblock">
Werner, P., Al-Hamadi, A., Limbrecht-Ecklundt, K., Walter, S., Gruss, S., and
Traue, H. C. (2016).

</span>
<span class="ltx_bibblock">Automatic pain assessment with facial activity descriptors.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing</span>, 8(3):286–299.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
Werner, P., Al-Hamadi, A., Niese, R., Walter, S., Gruss, S., and Traue, H. C.
(2014a).

</span>
<span class="ltx_bibblock">Automatic pain recognition from video and biomedical signals.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">2014 22nd International Conference on Pattern Recognition</span>,
pages 4582–4587.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Werner, P., Al-Hamadi, A., Niese, R., Walter, S., Gruss, S., and Traue, H. C.
(2014b).

</span>
<span class="ltx_bibblock">Automatic pain recognition from video and biomedical signals.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">2014 22nd international conference on pattern recognition</span>,
pages 4582–4587. IEEE.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zafeiriou et al.,  (2017)</span>
<span class="ltx_bibblock">
Zafeiriou, S., Kollias, D., Nicolaou, M. A., Papaioannou, A., Zhao, G., and
Kotsia, I. (2017).

</span>
<span class="ltx_bibblock">Aff-wild: Valence and arousal ‘in-the-wild’challenge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition Workshops (CVPRW),
2017 IEEE Conference on</span>, pages 1980–1987. IEEE.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2022)</span>
<span class="ltx_bibblock">
Zhang, S., An, R., Ding, Y., and Guan, C. (2022).

</span>
<span class="ltx_bibblock">Continuous emotion recognition using visual-audio-linguistic
information: A technical report for abaw3.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2376–2381.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2021)</span>
<span class="ltx_bibblock">
Zhang, S., Ding, Y., Wei, Z., and Guan, C. (2021).

</span>
<span class="ltx_bibblock">Continuous emotion recognition with audio-visual leader-follower
attentive fusion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">ICCVW</span>, pages 3560–3567.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.,  (2020)</span>
<span class="ltx_bibblock">
Zhang, Y.-H., Huang, R., Zeng, J., and Shan, S. (2020).

</span>
<span class="ltx_bibblock">Multi-modal continuous valence-arousal estimation in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">IEEE FG</span>, pages 632–636.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhi et al.,  (2021)</span>
<span class="ltx_bibblock">
Zhi, R., Zhou, C., Yu, J., Li, T., and Zamzmi, G. (2021).

</span>
<span class="ltx_bibblock">Multimodal-based stream integrated neural networks for pain
assessment.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">IEICE Trans. Inf. Syst.</span>, 104-D:2184–2194.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al.,  (2023)</span>
<span class="ltx_bibblock">
Zhou, W., Lu, J., Xiong, Z., and Wang, W. (2023).

</span>
<span class="ltx_bibblock">Leveraging tcn and transformer for effective visual-audio fusion in
continuous emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</span>, pages 5756–5763.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.10487" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.10488" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.10488">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.10488" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.10489" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:00:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
