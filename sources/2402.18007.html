<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.18007] Mixer is more than just a model</title><meta property="og:description" content="Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mixer is more than just a model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Mixer is more than just a model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.18007">

<!--Generated on Tue Mar  5 17:39:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Audio Classification,  MLP-Mixer,  Audio Spectrogram Mixer,  FFT,  RollBlock
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_ERROR undefined">\ul</span>

</p>
</div>
<h1 class="ltx_title ltx_title_document">Mixer is more than just a model
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

1<sup id="id1.1.id1" class="ltx_sup">st</sup> Qingfeng Ji1,
2<sup id="id2.2.id2" class="ltx_sup">nd</sup> Yuxin Wang1,
3<sup id="id3.3.id3" class="ltx_sup">rd</sup> Letong Sun1,






</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1School of Computer Science and Technology, Dalian University of Technology, Dalian, PRC
</span>
<span class="ltx_contact ltx_role_affiliation">1<sup id="id4.4.id1" class="ltx_sup">st</sup> Email: 15640414255@mail.dlut.edu.cn
</span>
<span class="ltx_contact ltx_role_affiliation">2<sup id="id5.5.id1" class="ltx_sup">nd</sup> Email: wyx@dlut.edu.cn, corresponding author
</span>
<span class="ltx_contact ltx_role_affiliation">3<sup id="id6.6.id1" class="ltx_sup">rd</sup> Email: 3038847812@mail.dlut.edu.cn
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of ”mixing” in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains. Experimental results demonstrate that ASM-RH is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks. The models and optimal weights files will be published.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Audio Classification, MLP-Mixer, Audio Spectrogram Mixer, FFT, RollBlock

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the realm of deep learning, Transformers have emerged as dominant players across various domains, exemplified by ViT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in computer vision and the refined versions like DeiT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, Swin Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and Swin Transformer V2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The rise of ChatGPT further solidifies the Transformer’s influence. Yet, the resource-intensive nature of Transformers has prompted a reevaluation of their intricate architectures. Google’s MLP-Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> presents a compelling case, showcasing that pure MLP structures can rival Transformers in computer vision. Similarly, gMLP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and ResMLP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> have delivered impressive results, underscoring the efficacy of alternative approaches. Furthermore, the introduction of MTS-Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> by the Huawei team for multivariate temporal prediction has demonstrated remarkable capabilities across diverse tasks. Weihao Yu et al. extended this concept by introducing the MetaFormer model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, emphasizing that deep learning models featuring a MetaFormer macro-architecture possess significant potential across a wide spectrum of computer vision tasks. Their proposed PoolFormer outperformed several prominent models, including ResNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, ViT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, DeiT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and Swin Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, in the ImageNet-1K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> benchmark. Guangting Wang’s integration of the Shift operation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, replacing the attention module with no additional FLOP or parameters, provides a slight edge over the benchmark model Swin Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Yuan Gong et al. ventured into the realm of audio classification by introducing the Audio Spectrogram Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, yielding notable outcomes. They later advanced this work by proposing the SSAST model in an unsupervised fashion<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Furthermore, Jiu Feng et al. introduced FlexiAST<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as an extension of AST, enhancing the model’s adaptability by offering flexibility in patch sizes.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Qingfeng Ji et al. introduced the Audio Spectrogram Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> by merging the AST model with MLP-Mixer and integrating the RGB to grayscale mapping formula. This innovative approach demonstrated outstanding performance across three datasets: SpeechCommands (for audio classification), UrbanSound8K (for environmental classification), and CASIA Chinese Sentiment Corpus (for audio sentiment classification). This introduction represents a significant milestone in bringing the Mixer methodology into the domain of audio classification.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this study, drawing inspiration from the Shift operation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in ShiftViT, we introduce the RollBlock module, leading to the development of the Roll-Time-mixing module aimed at enhancing the capture of time-domain information within the speech graph. Furthermore, leveraging the Hermit property for frequency domain information extraction, we introduce the Hermit-Frequency-mixing module. These components are seamlessly integrated with the Audio Spectrogram Mixer, resulting in the creation of the Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH). ASM-RH embodies the essence of the Mixer concept by shifting away from the conventional computer vision approach of analyzing spectrograms through Channel and Token perspectives. Instead, ASM-RH adopts a perspective that views spectrogram data from the time and frequency domain angles, aligning more closely with the requirements of audio applications. ASM-RH delivers impressive outcomes across three tasks - SpeechCommand<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, UrbanSound8K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and CASIA Chinese Emotion Corpus<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, surpassing ERANNs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to establish a new state-of-the-art (SOTA) performance in the RAVDESS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> audio classification task solely based on audio data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">ASM-RH Model</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The structure of Audio Spectrogram Mixer with Roll-Time and Hermit FFT is shown in Fig. 1. The spectrogram data is fed into RH-MixerBlocks after slicing and projection, and then output using an MLP output layer.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2402.18007/assets/ASM-RH.jpg" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_square" width="492" height="591" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Audio Spectrogram Mixer with Roll-Time and Hermit FFT</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Removal of useless structures</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Qingfeng Ji et al., in their proposal of the ASM model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, aimed to showcase that Mixer possesses capabilities on par with Transformer in the realm of audio processing. As a result, many of the structures from the AST model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> were retained in the ASM model, although they appeared to be non-essential. Conversely, Ilya Tolstikhin et al., when introducing the MLP-Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, emphasized that the Mixer architecture eliminates the need for positional embedding. Therefore, in ASM-RH, the cls-token and dist-token were omitted accordingly.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">RollBlock and Roll-Time-mixing</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The ShiftViT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposed by Guangting Wang et al. leverages the Shift operation to efficiently extract information without adding to the parameter complexity or computational load. The Shift operation involves spatially shifting the data matrix by a small increment in four directions while keeping the remaining channels unchanged, and filling the empty spaces with zeros. This operation bears resemblance to convolution. As every segment of time-domain information in the spectrogram is crucial, to maintain the integrity of the time-domain data, the discarded data from the Shift operation is reinstated in the empty positions. This process resembles rolling the data in place, leading to the term RollBlock. As the module’s depth increases, we limit the range and distance over which the data scrolls. As shown in Fig. 2.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<p id="S2.F2.1" class="ltx_p ltx_align_center"><span id="S2.F2.1.1" class="ltx_text"><img src="/html/2402.18007/assets/RollBlock.jpg" id="S2.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="497" height="140" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>RollBlock</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In detail, the data initially enters the RollBlock in the form of (BatchSize, Height, Width). It is then reshaped into (BatchSize, C, Height//C_a, Width*C_a//C) before being incrementally scrolled along the four spatial directions. Ultimately, the output is presented as (BatchSize, Height, Width). For this study, we set C=16 and C_a=4. The pseudo code is presented in Algorithms 1. Inserting a RollBlock between LayerNorm and FeedForward operation results in Roll-Time-mixing, as shown in Fig. 3.</p>
</div>
<figure id="S2.SS2.tab1" class="ltx_table">
<table id="S2.SS2.tab1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.SS2.tab1.1.1.1" class="ltx_tr">
<td id="S2.SS2.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.SS2.tab1.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm 1: Pytorch-like pseudo code of Roll</span></td>
</tr>
<tr id="S2.SS2.tab1.1.2.2" class="ltx_tr">
<td id="S2.SS2.tab1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">def Roll(feat, gamma=1/(1+alpha), step=ModelDepth-alpha):</td>
</tr>
<tr id="S2.SS2.tab1.1.3.3" class="ltx_tr">
<td id="S2.SS2.tab1.1.3.3.1" class="ltx_td ltx_align_left">    C, C_a = 16, 4</td>
</tr>
<tr id="S2.SS2.tab1.1.4.4" class="ltx_tr">
<td id="S2.SS2.tab1.1.4.4.1" class="ltx_td ltx_align_left">    B, H, W = feat.shape</td>
</tr>
<tr id="S2.SS2.tab1.1.5.5" class="ltx_tr">
<td id="S2.SS2.tab1.1.5.5.1" class="ltx_td ltx_align_left">    feat = feat.reshape(B, C, H//C_a. W//(C//C_a))</td>
</tr>
<tr id="S2.SS2.tab1.1.6.6" class="ltx_tr">
<td id="S2.SS2.tab1.1.6.6.1" class="ltx_td ltx_align_left">    g = int(gamma * C)</td>
</tr>
<tr id="S2.SS2.tab1.1.7.7" class="ltx_tr">
<td id="S2.SS2.tab1.1.7.7.1" class="ltx_td ltx_align_left">    out = feat</td>
</tr>
<tr id="S2.SS2.tab1.1.8.8" class="ltx_tr">
<td id="S2.SS2.tab1.1.8.8.1" class="ltx_td ltx_align_left">
<table id="S2.SS2.tab1.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.tab1.1.8.8.1.1.1" class="ltx_tr">
<td id="S2.SS2.tab1.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">    out[:, 0*g:1*g, :, :] = torch.roll(out[:, 0*g:1*g, :, :],</td>
</tr>
<tr id="S2.SS2.tab1.1.8.8.1.1.2" class="ltx_tr">
<td id="S2.SS2.tab1.1.8.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">      shifts=step, dim=3)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.tab1.1.9.9" class="ltx_tr">
<td id="S2.SS2.tab1.1.9.9.1" class="ltx_td ltx_align_left">
<table id="S2.SS2.tab1.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.tab1.1.9.9.1.1.1" class="ltx_tr">
<td id="S2.SS2.tab1.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">    out[:, 1*g:2*g, :, :] = torch.roll(out[:, 1*g:2*g, :, :],</td>
</tr>
<tr id="S2.SS2.tab1.1.9.9.1.1.2" class="ltx_tr">
<td id="S2.SS2.tab1.1.9.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">      shifts=-step, dim=3)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.tab1.1.10.10" class="ltx_tr">
<td id="S2.SS2.tab1.1.10.10.1" class="ltx_td ltx_align_left">
<table id="S2.SS2.tab1.1.10.10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.tab1.1.10.10.1.1.1" class="ltx_tr">
<td id="S2.SS2.tab1.1.10.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">    out[:, 2*g:3*g, :, :] = torch.roll(out[:, 2*g:3*g, :, :],</td>
</tr>
<tr id="S2.SS2.tab1.1.10.10.1.1.2" class="ltx_tr">
<td id="S2.SS2.tab1.1.10.10.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">      shifts=step, dim=2)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.tab1.1.11.11" class="ltx_tr">
<td id="S2.SS2.tab1.1.11.11.1" class="ltx_td ltx_align_left">
<table id="S2.SS2.tab1.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.tab1.1.11.11.1.1.1" class="ltx_tr">
<td id="S2.SS2.tab1.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">    out[:, 3*g:4*g, :, :] = torch.roll(out[:, 3*g:4*g, :, :],</td>
</tr>
<tr id="S2.SS2.tab1.1.11.11.1.1.2" class="ltx_tr">
<td id="S2.SS2.tab1.1.11.11.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">      shifts=-step, dim=2)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.tab1.1.12.12" class="ltx_tr">
<td id="S2.SS2.tab1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_b">    return out.reshape(B, H, W)</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F3" class="ltx_figure">
<p id="S2.F3.1" class="ltx_p ltx_align_center"><span id="S2.F3.1.1" class="ltx_text"><img src="/html/2402.18007/assets/Roll-Time-mixing.jpg" id="S2.F3.1.1.g1" class="ltx_graphics ltx_img_square" width="246" height="294" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Roll-Time-mixing</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Hermit-Frequency-mixing</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Drawing inspiration from ActiveMLP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Active Token Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and Adaptive Frequency Filters<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we introduce the concept of Hermit-Frequency-mixing in this context. The Hermit Fast Fourier Transform (FFT) is integrated before the initial data transposition, while the Inverse Real Fast Fourier Transform (IRFFT) is included after the second data transposition to establish the Hermit-Frequency-mixing, depicted in Fig. 4. The Hermit Fast Fourier Transform (HFFT) enables the model to capture frequency domain characteristics and Hermit properties of the data. The Inverse Real Fast Fourier Transform aids in restoring the data to its original domain. Notably, both transformations ensure that real inputs produce real outputs without the need for complex numbers.</p>
</div>
<figure id="S2.F4" class="ltx_figure">
<p id="S2.F4.1" class="ltx_p ltx_align_center"><span id="S2.F4.1.1" class="ltx_text"><img src="/html/2402.18007/assets/Hermit-Frequency-mixing.jpg" id="S2.F4.1.1.g1" class="ltx_graphics ltx_img_portrait" width="235" height="529" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Hermit-Frequency-mixing</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Audio Spectrogram Mixer has proven its efficacy in audio classification tasks and comparisons with Transformers. This paper adopts ASM as a benchmark for evaluation and primarily focuses on four datasets: SpeechCommands, UrbanSound8K, CASIA Chinese Sentiment Corpus, and RAVDESS for experimentation. The first three datasets align with those used in ASM, while RAVDESS serves as the comparison target against the established state-of-the-art (SOTA) model ERANNs.All experiments were evaluated on a single NVidia RTX3080 Ti GPU.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For experimental replication purposes, here are additional details: The data shape prior to slicing was [batchsize, 600, 768] and retained this structure until reaching the MLP output layer. Specifically, 12 RH-MixerBlocks were stacked in the model architecture. Following the RH-MixerBlocks, an MLP output layer was incorporated to process the data for final predictions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Throughout this paper, the notation ”-I” signifies pre-trained models that utilize DeiT partially, where ImageNet=True, as outlined in the Audio Spectrogram Mixer paper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The notation ”-A” signifies pre-trained models that utilize AST partially, where AudioSet=True, as outlined in the Audio Spectrogram Transformer paper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">SpeechCommands</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Speech Commands V2 is an audio dataset with 35 classes, comprising 84,843 training samples, 9,981 validation samples, and 11,005 test samples, each lasting 1 second. Following ASM settings, the initial learning rate was set to 2.5e-4, with decay starting after the 5th epoch. Experiments were conducted for 30 epochs with ImageNet set to False.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The best model was selected based on metrics from the validation dataset, and the evaluated metrics were reported for the test set. Three different random seeds were used for the validation process of each model. The evaluation metrics encompassed accuracy (ACC, primary metric) and area under the curve (AUC, secondary metric). The mean values of the experimental results are shown in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">I</span></span>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>SpeechCommand Result</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">ASM(%)</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">ASM-I(%)</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">ASM-RH(%)</span></th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">v-acc</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">92.92</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">94.70</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">96.62</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center">v-auc</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center">99.77</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center">99.87</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center">99.84</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center">t-acc</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center">91.89</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center">94.01</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center">96.51</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b">t-auc</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b">99.73</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b">99.83</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b">99.89</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">UrbanSound8K</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Urbansound8K is a widely used public dataset for automatic urban environmental sound classification research. This dataset contains a total of 8732 annotated sound fragments (¡=4s), including 10 categories: air conditioning sound, car honking sound, children’s playing sound, dog barking, drilling sound, engine idling sound, gunfire, drill, siren sound, and street music sound. Following ASM settings, the initial learning rate was set to 2.5e-4, with decay starting after the 5th epoch. Experiments were conducted for 25 epochs with ImageNet set to False.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The best model was selected based on metrics from the validation dataset, and the evaluated metrics were reported for the test set. Three different random seeds were used for the validation process of each model. The evaluation metrics encompassed accuracy (ACC, primary metric) and area under the curve (AUC, secondary metric). The mean values of the experimental results are shown in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">II</span></span>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>UrbanSound8K Result</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">ASM(%)</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">ASM-I(%)</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">ASM-RH(%)</span></th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">v-acc</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">88.52</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">92.30</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">96.49</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center">v-auc</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center">99.06</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center">99.44</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center">99.80</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center">t-acc</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center">89.51</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center">91.76</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center">95.80</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b">t-auc</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b">99.03</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b">99.44</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b">99.83</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To compare with the current state-of-the-art (SOTA) methods, we performed a 10-fold cross-validation experiment on the UrbanSound 8K dataset. The average and best results are presented in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">III</span></span> for reference.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>UrbanSound8K 10-fold Result</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Avg acc(%)</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Best acc(%)</span></th>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<td id="S3.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">EAT-M</td>
<td id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">90</td>
<td id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T3.1.3.3" class="ltx_tr">
<td id="S3.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_b">ASM-RH-I</td>
<td id="S3.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_b">97.96</td>
<td id="S3.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_b">98.63</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">CASIA Chinese Sentiment Corpus</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">CASIA Chinese Sentiment Corpus covers speech material recorded by four professional pronouncers, including six emotions: angry, happy, fear, sad, surprised and neutral. A total of 9600 speech samples with different pronunciations are included. Following ASM settings, the initial learning rate was set to 2.5e-4, with decay starting after the 5th epoch. Experiments were conducted for 25 epochs with ImageNet set to False.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The best model was selected based on metrics from the validation dataset, and the evaluated metrics were reported for the test set. Three different random seeds were used for the validation process of each model. The evaluation metrics encompassed accuracy (ACC, primary metric) and area under the curve (AUC, secondary metric). The mean values of the experimental results are shown in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">IV</span></span>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>CASIA Result</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">ASM(%)</span></th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">ASM-I(%)</span></th>
<th id="S3.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">ASM-RH(%)</span></th>
</tr>
<tr id="S3.T4.1.2.2" class="ltx_tr">
<td id="S3.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">v-acc</td>
<td id="S3.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">90.25</td>
<td id="S3.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">92.47</td>
<td id="S3.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">93.15</td>
</tr>
<tr id="S3.T4.1.3.3" class="ltx_tr">
<td id="S3.T4.1.3.3.1" class="ltx_td ltx_align_center">v-auc</td>
<td id="S3.T4.1.3.3.2" class="ltx_td ltx_align_center">99.48</td>
<td id="S3.T4.1.3.3.3" class="ltx_td ltx_align_center">99.62</td>
<td id="S3.T4.1.3.3.4" class="ltx_td ltx_align_center">99.60</td>
</tr>
<tr id="S3.T4.1.4.4" class="ltx_tr">
<td id="S3.T4.1.4.4.1" class="ltx_td ltx_align_center">t-acc</td>
<td id="S3.T4.1.4.4.2" class="ltx_td ltx_align_center">89.97</td>
<td id="S3.T4.1.4.4.3" class="ltx_td ltx_align_center">91.76</td>
<td id="S3.T4.1.4.4.4" class="ltx_td ltx_align_center">92.19</td>
</tr>
<tr id="S3.T4.1.5.5" class="ltx_tr">
<td id="S3.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b">t-auc</td>
<td id="S3.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b">99.43</td>
<td id="S3.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b">99.45</td>
<td id="S3.T4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b">99.45</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">RAVDESS</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) is an emotional speech and song database contributed by participants from Ryerson University, Canada. This dataset includes recordings of audio clips featuring various emotional states. It comprises 24 performers, evenly split between males and females, who deliver speech and songs depicting emotions such as happiness, neutrality, sadness, anger, surprise, fear, disgust, and calmness. In this section, we employ a 10-fold cross-validation method and consider the best performance achieved across the folds as the final result. The result is in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">V</span></span>.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>RAVDESS Result</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">acc(%)</span></th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">auc(%)</span></th>
</tr>
<tr id="S3.T5.1.2.2" class="ltx_tr">
<td id="S3.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.2.2.1.1" class="ltx_text ltx_font_bold">ERANN-1-3</span></td>
<td id="S3.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">73.1</td>
<td id="S3.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.2.2.3.1" class="ltx_text ltx_font_bold">-</span></td>
</tr>
<tr id="S3.T5.1.3.3" class="ltx_tr">
<td id="S3.T5.1.3.3.1" class="ltx_td ltx_align_center"><span id="S3.T5.1.3.3.1.1" class="ltx_text ltx_font_bold">ERANN-0-4</span></td>
<td id="S3.T5.1.3.3.2" class="ltx_td ltx_align_center">74.8</td>
<td id="S3.T5.1.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T5.1.3.3.3.1" class="ltx_text ltx_font_bold">-</span></td>
</tr>
<tr id="S3.T5.1.4.4" class="ltx_tr">
<td id="S3.T5.1.4.4.1" class="ltx_td ltx_align_center"><span id="S3.T5.1.4.4.1.1" class="ltx_text ltx_font_bold">ERANN-1-4</span></td>
<td id="S3.T5.1.4.4.2" class="ltx_td ltx_align_center">74.1</td>
<td id="S3.T5.1.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T5.1.4.4.3.1" class="ltx_text ltx_font_bold">-</span></td>
</tr>
<tr id="S3.T5.1.5.5" class="ltx_tr">
<td id="S3.T5.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T5.1.5.5.1.1" class="ltx_text ltx_font_bold">ASM-RH-A</span></td>
<td id="S3.T5.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">75.4</td>
<td id="S3.T5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">94.98</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">To assess the effectiveness of Roll-Time-mixing and Hermit-Frequency-mixing, ablation experiments were conducted on the UrbanSound8K dataset. Channel-mixing and Tokens-mixing in the MLP-Mixer were implemented as alternatives to Roll-Time-mixing and Hermit-Frequency-mixing, respectively. The experiment maintained consistency by utilizing the same three random seeds, ensuring reproducibility and reliability. The results were averaged and are presented in Table <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">VI</span></span> for a comprehensive comparison and analysis. In the table, ”-H” represents ASM-RH with Hermit-Frequency-mixing only, while ”-R” signifies ASM-RH with Roll-Time-mixing only.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">The best model was chosen based on the metrics obtained from the validation dataset, and the performance metrics were then reported for the test set. The evaluation metrics encompassed accuracy (ACC, primary metric) and area under the curve (AUC, secondary metric).</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>UrbanSound8K Ablation Experiment Result</figcaption>
<table id="S3.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<td id="S3.T6.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">ASM-RH(%)</span></th>
<th id="S3.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">ASM-H(%)</span></th>
<th id="S3.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">ASM-R(%)</span></th>
</tr>
<tr id="S3.T6.1.2.2" class="ltx_tr">
<td id="S3.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">v-acc</td>
<td id="S3.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">96.49</td>
<td id="S3.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">95.61</td>
<td id="S3.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">96.19</td>
</tr>
<tr id="S3.T6.1.3.3" class="ltx_tr">
<td id="S3.T6.1.3.3.1" class="ltx_td ltx_align_center">v-auc</td>
<td id="S3.T6.1.3.3.2" class="ltx_td ltx_align_center">99.80</td>
<td id="S3.T6.1.3.3.3" class="ltx_td ltx_align_center">99.76</td>
<td id="S3.T6.1.3.3.4" class="ltx_td ltx_align_center">99.77</td>
</tr>
<tr id="S3.T6.1.4.4" class="ltx_tr">
<td id="S3.T6.1.4.4.1" class="ltx_td ltx_align_center">t-acc</td>
<td id="S3.T6.1.4.4.2" class="ltx_td ltx_align_center">95.80</td>
<td id="S3.T6.1.4.4.3" class="ltx_td ltx_align_center">95.19</td>
<td id="S3.T6.1.4.4.4" class="ltx_td ltx_align_center">95.04</td>
</tr>
<tr id="S3.T6.1.5.5" class="ltx_tr">
<td id="S3.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b">t-auc</td>
<td id="S3.T6.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b">99.83</td>
<td id="S3.T6.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b">99.75</td>
<td id="S3.T6.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b">99.79</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.4.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.5.2" class="ltx_text ltx_font_italic">Discussion</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">In this paper, we introduce the Audio Spectrogram Mixer with Roll-Time and Hermit FFT model, which mix time-domain and frequency-domain information. We compare this model with the Audio Spectrogram Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which integrates Mixer into audio classification tasks, across three datasets: SpeechCommands, UrbanSound8K, and the CASIA Chinese sentiment corpus. The experimental findings reveal that, even without pre-training or additional information, the Audio Spectrogram Mixer with Roll-Time and Hermit FFT model outperforms the Audio Spectrogram Mixer significantly. Indeed, the results demonstrate that the performance of the Audio Spectrogram Mixer with Roll-Time and Hermit FFT model surpasses that of the Audio Spectrogram Mixer, even when the latter is enhanced with visual information. This underscores the effectiveness and promising outcomes of the proposed ASM-RH model.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Furthermore, our evaluation on the RAVDESS dataset illustrated that the Audio Spectrogram Mixer with Roll-Time and Hermit FFT model outperforms the state-of-the-art (SOTA) ERANN models, showcasing its superior performance in audio classification tasks.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">Lastly, ablation experiments conducted on the UrbanSound 8K dataset validated that Roll-Time-mixing and Hermit-Frequency-mixing effectively capture both time-domain and frequency-domain information, further reinforcing the efficacy of Mixer in audio data processing and classification tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Mixer is more than just a model</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we introduce two novel structures, Roll-Time-mixing and Hermit-Frequency-mixing, designed to capture time-domain and frequency-domain information, respectively. These structures are integrated into Mixer to create the ASM-RH model for audio classification tasks. Experimental results validate the effectiveness of these structures and the capability of ASM-RH.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The audio categorization task serves as a demonstration of Mixer’s potential, but the essence of this paper is to urge researchers to delve deeper into the study of MLP and Mixer. As previously mentioned, Mixer transcends being merely a model structure; it represents a mind and approach to handling and interpreting data. Just as CNNs blend information from local and global viewpoints, and spatio-temporal models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> merge temporal and spatial information, they can be viewed as a form of Mixer. Just as MLP-Mixer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> looks at visual data from Channel and Tokens perspectives, and ASM-RH looks at spectrogram data from time and frequency domain perspectives, we hope that more researchers will develop high-quality models that capture and MIX information from more perspectives. All are Mixers.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.,

</span>
<span class="ltx_bibblock">“An image is worth 16x16 words: Transformers for image recognition at scale,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, and Hervé Jégou,

</span>
<span class="ltx_bibblock">“Training data-efficient image transformers &amp; distillation through attention,” 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo,

</span>
<span class="ltx_bibblock">“Swin transformer: Hierarchical vision transformer using shifted windows,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</span>, 2021, pp. 10012–10022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.,

</span>
<span class="ltx_bibblock">“Swin transformer v2: Scaling up capacity and resolution,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, 2022, pp. 12009–12019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.,

</span>
<span class="ltx_bibblock">“Mlp-mixer: An all-mlp architecture for vision,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 34, pp. 24261–24272, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hanxiao Liu, Zihang Dai, David So, and Quoc V Le,

</span>
<span class="ltx_bibblock">“Pay attention to mlps,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 34, pp. 9204–9215, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al.,

</span>
<span class="ltx_bibblock">“Resmlp: Feedforward networks for image classification with data-efficient training,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, vol. 45, no. 4, pp. 5314–5321, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu,

</span>
<span class="ltx_bibblock">“Mts-mixers: Multivariate time series forecasting via factorized temporal and channel mixing,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.04501</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan,

</span>
<span class="ltx_bibblock">“Metaformer is actually what you need for vision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, 2022, pp. 10819–10829.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,

</span>
<span class="ltx_bibblock">“Deep residual learning for image recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein,

</span>
<span class="ltx_bibblock">“Imagenet large scale visual recognition challenge,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, vol. 115, no. 3, pp. 211–252, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Guangting Wang, Yucheng Zhao, Chuanxin Tang, Chong Luo, and Wenjun Zeng,

</span>
<span class="ltx_bibblock">“When shift operation meets vision transformer: An extremely simple alternative to attention mechanism,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 2022, vol. 36, pp. 2423–2430.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yuan Gong, Yu-An Chung, and James Glass,

</span>
<span class="ltx_bibblock">“Ast: Audio spectrogram transformer,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.01778</span>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass,

</span>
<span class="ltx_bibblock">“Ssast: Self-supervised audio spectrogram transformer,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 2022, vol. 36, pp. 10699–10709.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, and Arda Senocak,

</span>
<span class="ltx_bibblock">“Flexiast: Flexibility is what ast needs,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/2307.09286, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Qingfeng Ji, Jicun Zhang, and Yuxin Wang,

</span>
<span class="ltx_bibblock">“Asm: Audio spectrogram mixer,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/2401.11102, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad, Joseph Gonzalez, and Kurt Keutzer,

</span>
<span class="ltx_bibblock">“Shift: A zero flop, zero parameter alternative to spatial convolutions,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, Jun 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yunho Jeon and Junmo Kim,

</span>
<span class="ltx_bibblock">“Constructing fast network through deconstruction of convolution,”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition</span>, May 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tan Yu, Li Xu, Yunfeng Cai, Mingming Sun, and Ping Li,

</span>
<span class="ltx_bibblock">“S<sup id="bib.bib19.2.1" class="ltx_sup">2</sup>-mlp: Spatial-shift mlp architecture for vision,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">Cornell University - arXiv,Cornell University - arXiv</span>, Jun 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Pete Warden,

</span>
<span class="ltx_bibblock">“Speech commands: A dataset for limited-vocabulary speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.03209</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Kaustumbh Jaiswal and Dhairya Kalpeshbhai Patel,

</span>
<span class="ltx_bibblock">“Sound classification using convolutional neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)</span>. IEEE, 2018, pp. 81–84.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xianxin Ke, Yujiao Zhu, Lei Wen, and Wenzhen Zhang,

</span>
<span class="ltx_bibblock">“Speech emotion recognition based on svm and ann,”

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Journal of Machine Learning and Computing</span>, vol. 8, no. 3, pp. 198–202, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sergey Verbitskiy, Vladimir Berikov, and Viacheslav Vyshegorodtsev,

</span>
<span class="ltx_bibblock">“Eranns: Efficient residual audio neural networks for audio pattern recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, vol. 161, pp. 38–44, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Steven R. Livingstone and Frank A. Russo,

</span>
<span class="ltx_bibblock">“The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">PLOS ONE</span>, vol. 13, no. 5, pp. 1–35, 05 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Guoqiang Wei, Zhizheng Zhang, Cuiling Lan, Yan Lu, and Zhibo Chen,

</span>
<span class="ltx_bibblock">“Activemlp: An mlp-like architecture with active token mixer,”

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv: 2203.06108</span>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Guoqiang Wei, Zhizheng Zhang, Cuiling Lan, Yan Lu, and Zhibo Chen,

</span>
<span class="ltx_bibblock">“Active token mixer,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 2023, vol. 37, pp. 2759–2767.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhipeng Huang, Zhizheng Zhang, Cuiling Lan, Zheng-Jun Zha, Yan Lu, and Baining Guo,

</span>
<span class="ltx_bibblock">“Adaptive frequency filters as efficient global token mixers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, 2023, pp. 6049–6059.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, and Yanming Shen,

</span>
<span class="ltx_bibblock">“How can large language models understand spatial-temporal data?,”

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/2401.14192, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.18006" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.18007" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.18007">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.18007" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.18008" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 17:39:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
