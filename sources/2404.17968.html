<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.17968] Usefulness of Emotional Prosody in Neural Machine Translation</title><meta property="og:description" content="Neural Machine Translation (NMT) is the task of translating a text from one language to another with the use of a trained neural network. Several existing works aim at incorporating external information into NMT models…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Usefulness of Emotional Prosody in Neural Machine Translation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Usefulness of Emotional Prosody in Neural Machine Translation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.17968">

<!--Generated on Sun May  5 16:41:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">Usefulness of Emotional Prosody in Neural Machine Translation</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">Neural Machine Translation (NMT) is the task of translating a text from one language to another with the use of a trained neural network. Several existing works aim at incorporating external information into NMT models to improve or control predicted translations (e.g. sentiment, politeness, gender). In this work, we propose to improve translation quality by adding another external source of information: the automatically recognized emotion in the voice. This work is motivated by the assumption that each emotion is associated with a specific lexicon that can overlap between emotions. Our proposed method follows a two-stage procedure. At first, we select a state-of-the-art Speech Emotion Recognition (SER) model to predict dimensional emotion values from all input audio in the dataset. Then, we use these predicted emotions as source tokens added at the beginning of input texts to train our NMT model. We show that integrating emotion information, especially arousal, into NMT systems leads to better translations.</span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Index Terms</span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">: Neural Machine Translation, Speech Emotion Recognition, Neural Networks.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Machine Translation (MT) is the task of translating a text from one language to another. Existing methods are originally based on rule-based or statistical approaches. More recently, end-to-end neural networks have demonstrated their efficiency in the task </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">, outperforming earlier methods </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;">, and leading to the emergence of the Neural Machine Translation (NMT) task.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Transformer-based architectures </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;"> are widely used in NMT, leading to state-of-the-art performances for several language pairs. Transformers are sequence-to-sequence models composed of an encoder that reads the input sequence and generates a representation of it, a decoder that generates the output sequence based on the input representation, and self-attention mechanisms that enable the use of the entire input sequence for each output prediction.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">Several works in the domain focused on the control of the translation given by existing NMT models. In fact, most NMT models aim to generate a single correct translation of a given sentence. However, the translation can be dependent on specific information that may be missing in the input sentence. For example, translations are dependent on the speaker’s gender that may not be specified in the source language, or even some words can be translated differently depending on the context. For this purpose, several models propose solutions to improve translation quality by controlling sentiment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.4" class="ltx_text" style="font-size:90%;">, politeness </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.7" class="ltx_text" style="font-size:90%;">, gender </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.p3.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.10" class="ltx_text" style="font-size:90%;">, style of translators </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.p3.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.13" class="ltx_text" style="font-size:90%;">, or output language in the case of a multilingual setup </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.p3.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.16" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">In this work, we propose to improve translation quality by adding another external source of information: the automatically recognized emotion in the voice. This work is motivated by the assumption that each emotion is associated with a specific lexicon that can overlap between emotions. One possible approach to estimate the emotion of a specific sentence is to use affective word lists that classify individual words into emotion categories </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S1.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.4" class="ltx_text" style="font-size:90%;"> or rate them on emotion dimension scales for arousal, dominance, and valence </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S1.p4.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.7" class="ltx_text" style="font-size:90%;">. However, the emotion of words is dependent on the entire sentence context </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.p4.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.10" class="ltx_text" style="font-size:90%;">. Additionally, word lists are not exhaustive and are specific to particular languages. In the following, we focus on the prosody of the pronounced sentences present in audio recordings to estimate emotion.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="font-size:90%;">To this end, we first select a state-of-the-art Speech Emotion Recognition (SER) model to predict dimensional emotion values from audio recordings. Then, we use these predicted emotions as source tokens added at the beginning of input texts to train our NMT model. We show that integrating emotion information, especially arousal, into NMT systems leads to better translations.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="font-size:90%;">In this paper, existing works adding external information to NMT models will be presented in Section </span><a href="#S2" title="2 Related works ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.p6.1.2" class="ltx_text" style="font-size:90%;">. The selected state-of-the-art SER model and our proposed emotion-aware NMT pipeline will be described in Section </span><a href="#S3" title="3 Emotion Aware Neural Machine Translation ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S1.p6.1.3" class="ltx_text" style="font-size:90%;">. Experiments and results will be presented and discussed in Section </span><a href="#S4" title="4 Experiments and results ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S1.p6.1.4" class="ltx_text" style="font-size:90%;">. Section </span><a href="#S5" title="5 Conclusion ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S1.p6.1.5" class="ltx_text" style="font-size:90%;"> will conclude the work.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">Existing works aiming at incorporating external information into NMT models to improve or control translation quality are numerous. They generally involve a two-stage procedure. At first, the external information is translated into a special token with the help of manual or automatic annotations. Then, the special token is added into the NMT model to control translation.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="font-size:90%;">In the case of sentiment, Si et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.4" class="ltx_text" style="font-size:90%;"> use a trained sentiment classifier to label ambiguous words with positive or negative tags. Then, the authors incorporate the sentiment information into the source sentence either at the beginning or directly before the ambiguous word. They also try to use two different embedding vectors for each ambiguous word and select the correct embedding depending on the desired sentiment.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text" style="font-size:90%;">In the case of politeness, Sennrich et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S2.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.4" class="ltx_text" style="font-size:90%;"> automatically annotate politeness (informal/polite) using linguistic rules and add the special token at the end of the source sentence.</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="font-size:90%;">In the case of gender, Vanmassenhove et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p4.1.4" class="ltx_text" style="font-size:90%;"> extract gender information (male/female) from meta data present in their dataset and use it as additional token added at the beginning of the source sentence.</span></p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text" style="font-size:90%;">In the case of translation style, Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p5.1.4" class="ltx_text" style="font-size:90%;"> use the data translated by 10 translators and incorporate the translator token at the beginning of input sentences, or add the embedded translator token to every token in the encoder. They also try to add the translator information directly into the decoder but this solution seems ineffective.</span></p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text" style="font-size:90%;">In the case of a multilingual scenario, Johnson et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.p6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p6.1.4" class="ltx_text" style="font-size:90%;"> add an artificial token at the beginning of input sentences to specify the desired target language.</span></p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text" style="font-size:90%;">Our approach aims at enhancing translation quality by adding emotion information into text sentences. Emotion is automatically extracted from audio data (recordings or synthesized text) with the help of a trained SER model and added to source sentences at the beginning as an additional emotion token.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Emotion Aware Neural Machine Translation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">In this work, we propose to combine SER classifier and NMT models to improve translation quality.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text" style="font-size:90%;">To the best of our knowledge, the only work that combines an emotion classifier with an NMT model is presented by Troiano et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.1.4" class="ltx_text" style="font-size:90%;">. The authors introduce a method to preserve emotion during automatic translation by re-ranking the best translations generated by an NMT model, according to a trained emotion classifier. Due to the lack of comparable emotion classifiers for different languages, they adopt a back-translation setup that re-translates the best translation hypotheses and re-rank them to preserve the same emotion between the input sentence and its translated translations. The re-ranking is performed as a post-processing step and not as a fine-tuning procedure for the NMT model.</span></p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text" style="font-size:90%;">In contrast to </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.4" class="ltx_text" style="font-size:90%;">, we aim at improving translation quality by using the output of a pre-trained emotion classifier in the NMT training step.</span></p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text" style="font-size:90%;">Our proposed method follows a two-stage procedure. At first, a trained Speech Emotion Recognition (SER) model predicts dimensional emotion values from all input audio in the dataset. Then, these predicted values are converted into discrete tokens and added at the beginning of input texts. Finally, we train various NMT models by varying the utilized emotion token and extraction method (whether it is predicted from original or synthesized recordings).</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Speech Emotion Recognition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Automatic Speech Emotion Recognition (SER) is the task of predicting emotion from speech recordings. SER models can predict discrete categories such as happy, sad, etc., or continuous emotional dimensions like arousal, valence, and dominance. The three emotional dimensions are described as follows:</span></p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Arousal refers to the level of stimulation or activation associated with an emotion. It ranges from low arousal (calm, relaxed) to high arousal (excited, agitated). Emotions like excitement, anxiety, and surprise typically have higher arousal levels, while calmness and relaxation have lower arousal levels.</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">Valence represents the positive or negative nature of an emotion. Emotions can be categorized on a scale from positive (happy, joyful) to negative (sad, angry). Valence indicates whether an emotion is pleasant or unpleasant, with positive valence indicating positive emotions and negative valence indicating negative emotions.</span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text" style="font-size:90%;">Dominance refers to the level of control or power associated with an emotion. It measures whether an emotion makes an individual feel dominant or submissive in a particular situation. Emotions with high dominance might include feelings of confidence, assertiveness, or control, while low dominance emotions might involve feelings of submission or helplessness.</span></p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">These dimensions are often used in psychology and neuroscience to help understand and categorize emotions, providing a more nuanced view of the complex nature of human feelings and experiences.</span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">For our experiments, we select a state-of-the-art transformer-based SER model proposed by Wagner et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.SS1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p3.1.4" class="ltx_text" style="font-size:90%;">. This model is based on the wav2vec2.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.SS1.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p3.1.7" class="ltx_text" style="font-size:90%;"> model which learns powerful speech representations from unlabelled speech data. Wav2vec2.0 is composed of 7 Convolutional Neural Networks (CNN) layers of feature encoder and 12 transformer layers of encoder including multi-head self-attention modules and fully-connected layers. Wagner et al. add one average pooling layer, one hidden layer, and one output layer of length 3 at the end of the network to classify each raw waveform into three emotion dimensions corresponding to arousal, dominance, and valence. The three output values range from 0 to 1.</span></p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text" style="font-size:90%;">During training, the authors freeze the CNN layers and only fine-tune transformer layers with the added layers on top of the model. The fine-tuning step is performed on the MSP-Podcast </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S3.SS1.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p4.1.4" class="ltx_text" style="font-size:90%;"> dataset (train split) for training, and tested on MSP-Podcast (test-1 split), IEMOCAP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p4.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.SS1.p4.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p4.1.7" class="ltx_text" style="font-size:90%;">, and MOSI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p4.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S3.SS1.p4.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p4.1.10" class="ltx_text" style="font-size:90%;">. All three corpora contain audio recordings with corresponding dimensional emotion values that are normalized between 0 and 1. Model performances are evaluated with the Concordance Correlation Coefficient (CCC) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p4.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.SS1.p4.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p4.1.13" class="ltx_text" style="font-size:90%;">. The authors report a CCC of .744 for arousal, .655 for dominance, and .638 for valence, on the MSP-Podcast dataset.</span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Neural Machine Translation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Neural Machine Translation (NMT) is the task of translating a text from one language to another. As model for our experiments, we selected a transformer-based encoder-decoder architecture developed with the ESPnet toolkit </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">. The model is composed of an encoder of 6 transformer layers, a decoder of 6 transformer layers, and 4 attention heads in each self-attention layers. The model is end-to-end meaning that it receives text as input and outputs text. All textual sentences are tokenized into sub-word sequences using a byte-pair encoding algorithm with 1000 units.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">The model is trained on the Libri-trans </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p2.1.4" class="ltx_text" style="font-size:90%;"> dataset, a dataset that contains speech recordings and corresponding transcripts in English and French. The dataset is divided into train, dev, and test sets whose durations are respectively 230, 2, and 3.5 hours. In this work, we focus on text-to-text translation from English to French.</span></p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">The model is evaluated with BiLingual Evaluation Understudy (BLEU) score, a metric that measures the similarity between generated translations and corresponding groundtruth. Its calculation includes a brevity penalty, penalizing generated translations shorter than their references, and a n-gram precision factor that considers various n-grams (including unigrams, bigrams, etc.) and counts the number of n-grams from the generated translation that also appear in the reference translation. The BLEU score is a value between 0 (poor translation) and 100 (perfect translation). Note that a translation with a BLEU score higher than 40 is generally considered as a high-quality translation. We report BLEU using SacreBLEU </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.SS2.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p3.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Emotion Aware Neural Machine Translation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.12" class="ltx_p"><span id="S3.SS3.p1.12.1" class="ltx_text" style="font-size:90%;">To improve the translation quality of our NMT model, we propose to incorporate the emotion information extracted from input recordings into their corresponding input text sequences. The emotion information is represented as an additional token representing the polarity over a specific emotion dimension. We use the tokens </span><math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mo mathsize="90%" id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><lt id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.2" class="ltx_text" style="font-size:90%;">AroNeg</span><math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mo mathsize="90%" id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><gt id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.3" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mo mathsize="90%" id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><lt id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.4" class="ltx_text" style="font-size:90%;">AroPos</span><math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mo mathsize="90%" id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><gt id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.5" class="ltx_text" style="font-size:90%;"> for recordings that have an arousal score lower or higher than 0.5 respectively, as well as </span><math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mo mathsize="90%" id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><lt id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.6" class="ltx_text" style="font-size:90%;">DomNeg</span><math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mo mathsize="90%" id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><gt id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.7" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><mo mathsize="90%" id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><lt id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.8" class="ltx_text" style="font-size:90%;">DomPos</span><math id="S3.SS3.p1.8.m8.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.8.m8.1a"><mo mathsize="90%" id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><gt id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.9" class="ltx_text" style="font-size:90%;"> for dominance, and </span><math id="S3.SS3.p1.9.m9.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.9.m9.1a"><mo mathsize="90%" id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><lt id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.10" class="ltx_text" style="font-size:90%;">ValNeg</span><math id="S3.SS3.p1.10.m10.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.10.m10.1a"><mo mathsize="90%" id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><gt id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.11" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS3.p1.11.m11.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p1.11.m11.1a"><mo mathsize="90%" id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><lt id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p1.12.12" class="ltx_text" style="font-size:90%;">ValPos</span><math id="S3.SS3.p1.12.m12.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p1.12.m12.1a"><mo mathsize="90%" id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.1b"><gt id="S3.SS3.p1.12.m12.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.1c">&gt;</annotation></semantics></math><span id="S3.SS3.p1.12.13" class="ltx_text" style="font-size:90%;"> for valence.</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.5" class="ltx_p"><span id="S3.SS3.p2.5.1" class="ltx_text" style="font-size:90%;">For example, consider the following pair of English and French sentences:
</span>
<br class="ltx_break"></p>
<p id="S3.SS3.p2.1" class="ltx_p ltx_align_center"><span id="S3.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">
<span id="S3.SS3.p2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;">I am quite foolish</span> <math id="S3.SS3.p2.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.1.1.m1.1a"><mo stretchy="false" id="S3.SS3.p2.1.1.m1.1.1" xref="S3.SS3.p2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.1.m1.1b"><ci id="S3.SS3.p2.1.1.m1.1.1.cmml" xref="S3.SS3.p2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.1.m1.1c">\rightarrow</annotation></semantics></math> <span id="S3.SS3.p2.1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:89%;">Je suis toute sotte</span></span></p>
<p id="S3.SS3.p2.6" class="ltx_p"><span id="S3.SS3.p2.6.1" class="ltx_text" style="font-size:90%;">In the case of valence, the pair will be modified to:
</span>
<br class="ltx_break"></p>
<p id="S3.SS3.p2.4" class="ltx_p ltx_align_center"><span id="S3.SS3.p2.4.3" class="ltx_text" style="font-size:90%;">
<math id="S3.SS3.p2.2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p2.2.1.m1.1a"><mo mathsize="89%" id="S3.SS3.p2.2.1.m1.1.1" xref="S3.SS3.p2.2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.1.m1.1b"><lt id="S3.SS3.p2.2.1.m1.1.1.cmml" xref="S3.SS3.p2.2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.1.m1.1c">&lt;</annotation></semantics></math><span id="S3.SS3.p2.3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;">ValNeg<math id="S3.SS3.p2.3.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p2.3.2.1.m1.1a"><mo id="S3.SS3.p2.3.2.1.m1.1.1" xref="S3.SS3.p2.3.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.2.1.m1.1b"><gt id="S3.SS3.p2.3.2.1.m1.1.1.cmml" xref="S3.SS3.p2.3.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.2.1.m1.1c">&gt;</annotation></semantics></math></span> <span id="S3.SS3.p2.4.3.2" class="ltx_text ltx_font_typewriter" style="font-size:89%;">I am quite foolish</span> <math id="S3.SS3.p2.4.3.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.4.3.m2.1a"><mo stretchy="false" id="S3.SS3.p2.4.3.m2.1.1" xref="S3.SS3.p2.4.3.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.3.m2.1b"><ci id="S3.SS3.p2.4.3.m2.1.1.cmml" xref="S3.SS3.p2.4.3.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.3.m2.1c">\rightarrow</annotation></semantics></math> <span id="S3.SS3.p2.4.3.3" class="ltx_text ltx_font_typewriter" style="font-size:89%;">Je suis toute sotte</span></span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text" style="font-size:90%;">The proposed method offers several advantages. Firstly, it does not necessitate any changes in the model architecture nor in the set of translated sequences. Model performances with and without the additional token can be directly compared. Furthermore, the approach is simple yet effective, showcasing progress in various domains </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.SS3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p3.1.4" class="ltx_text" style="font-size:90%;">. It only involves the addition of an extra token at the beginning of each input sentence. Lastly, the method does not require any additional manual annotation, as the emotion is automatically extracted from the SER model.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">In this section, we conduct a set of experiments to compare NMT performances under different configurations. Firstly, we analyze the statistical distribution of values provided by the SER model on the Libri-trans dataset for the three emotion dimensions. Secondly, we train four different NMT models that are a baseline model, and three different models that use the arousal, dominance, and valence information respectively.</span></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Emotion recognition on Libri-trans</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Our proposed method starts with the computation of emotional dimensions for each English sentence in the Libri-trans dataset. These values will be later used for the English-to-French text-to-text translation task.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">Due to the modality mismatch between the text-to-text task and the required raw audio waveform as input for our SER model, we propose two different ways to compute emotion values. The first method involves using audio recordings that are already present in the Libri-trans dataset to evaluate emotion values. This represents the most accurate method to estimate emotion; however, it implies the use of additional data in the translation pipeline. Statistical distributions of emotion values on original audio recordings from the Libri-trans dataset are illustrated in Figure </span><a href="#S4.F1" title="Figure 1 ‣ 4.1 Emotion recognition on Libri-trans ‣ 4 Experiments and results ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.1.2" class="ltx_text" style="font-size:90%;">. We showcase statistical distributions for each train, dev, and test subset of the Libri-trans dataset. We remark that, for the three emotional dimensions and each subset, statistical distributions are balanced, medians are around 0.5 and values range from 0.1 to 0.9. Due to the acceptable balance of dimensional emotion scores provided by our selected SER, we consider the initial train, dev, and test splits as good candidates for the translation task conditioned with emotion information.</span></p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2404.17968/assets/emotion.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="585" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Statistical distribution of emotion values on original audio from the Libri-trans dataset.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">The second method involves synthesizing each English text with the help of the Google Text-to-Speech API</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://gtts.readthedocs.io/en/latest/</span></span></span></span><span id="S4.SS1.p3.1.2" class="ltx_text" style="font-size:90%;">. This method has the advantage of using only text data; however, the original emotion can be altered with a neutral rendering. Statistical distributions of emotion values on synthesized audio of all English text are illustrated in Figure </span><a href="#S4.F2" title="Figure 2 ‣ 4.1 Emotion recognition on Libri-trans ‣ 4 Experiments and results ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p3.1.3" class="ltx_text" style="font-size:90%;">. We observe that arousal values are concentrated around 0.5, indicating that synthesizing audio files produces neutral arousal. Also, we remark that almost all dominance values are above 0.5. Thus, using synthesized audio to estimate dominance is inefficient. However, valence values are more distributed, reflecting the fact that valence is captured in the vocabulary used in each English sentence. As an additional observation, the medians of arousal and valence values across all Libri-trans subsets are close to 0.5, enabling an automatically balanced dataset for training.</span></p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2404.17968/assets/emotion_synth.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="585" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Statistical distribution of emotion values on synthesized audio from the Libri-trans dataset.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Neural Machine Translation on Libri-trans</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">The computation of emotion values on English data from Libri-trans enables the assignment of a specific dimension polarity for each input sentence. This is reflected by the addition of an extra token at the beginning of each input text.</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">In the following experiments, we compare four different NMT models. The first model, </span><span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">baseline</span><span id="S4.SS2.p2.1.3" class="ltx_text" style="font-size:90%;">, is trained without the use of an emotion token and serves as a reference to improve its translation score. The second, third, and fourth models, namely </span><span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_italic" style="font-size:90%;">arousal</span><span id="S4.SS2.p2.1.5" class="ltx_text" style="font-size:90%;">, </span><span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">dominance</span><span id="S4.SS2.p2.1.7" class="ltx_text" style="font-size:90%;">, and </span><span id="S4.SS2.p2.1.8" class="ltx_text ltx_font_italic" style="font-size:90%;">valence</span><span id="S4.SS2.p2.1.9" class="ltx_text" style="font-size:90%;">, are NMT models trained with the addition of a special token at the beginning of each English sentence representing the polarity (positive or negative) of their corresponding emotion dimension.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">We also conduct two sets of experiments, one using tokens extracted from original audio recordings and the other using those extracted from synthesized audio.</span></p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text" style="font-size:90%;">All models have the exact same structure. They are trained with a label-smoothing loss, a Noam optimizer, a dropout rate of 0.1, an inverse square root scheduler with 8000 warm-up steps, and a batch size of 96. Each transformer model is trained during 250 epochs and the 5 best MT models are averaged to create the best model. Each training session takes approximately 15 hours on a single NVIDIA A100 GPU.</span></p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S4.T1" title="Table 1 ‣ 4.2 Neural Machine Translation on Libri-trans ‣ 4 Experiments and results ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p5.1.2" class="ltx_text" style="font-size:90%;"> showcases the results of our different experiments. In this table, we report BLEU scores of 8 different experiments. It gathers the evaluation of our four proposed models, evaluated on the development and test sets of Libri-trans, with the emotion token computed either from the synthesized text or the audio recordings directly.</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>BLEU scores of our four proposed models on dev and test sets of Libri-trans. The emotion token is computed either from the audio recordings of Libri-trans (Recor) or the synthesized text (Synth).</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">baseline</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">arousal</span></th>
<th id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">dominance</span></th>
<th id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">valence</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<th id="S4.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Recor dev</span></th>
<td id="S4.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
<td id="S4.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.1.3.1" class="ltx_text" style="font-size:90%;">20.2</span></td>
<td id="S4.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.1.4.1" class="ltx_text" style="font-size:90%;">20.5</span></td>
<td id="S4.T1.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.1.5.1" class="ltx_text" style="font-size:90%;">20.6</span></td>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<th id="S4.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.3.2.1.1" class="ltx_text" style="font-size:90%;">Recor test</span></th>
<td id="S4.T1.3.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.2.2.1" class="ltx_text" style="font-size:90%;">18.2</span></td>
<td id="S4.T1.3.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">18.6</span></td>
<td id="S4.T1.3.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.2.4.1" class="ltx_text" style="font-size:90%;">18.3</span></td>
<td id="S4.T1.3.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.2.5.1" class="ltx_text" style="font-size:90%;">18.2</span></td>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<th id="S4.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T1.3.4.3.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T1.3.4.3.1.2" class="ltx_text" style="font-size:90%;">Synth dev</span>
</th>
<td id="S4.T1.3.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.3.2.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.3.3.1" class="ltx_text" style="font-size:90%;">20.2</span></td>
<td id="S4.T1.3.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.3.4.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
<td id="S4.T1.3.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.3.5.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
</tr>
<tr id="S4.T1.3.5.4" class="ltx_tr">
<th id="S4.T1.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.3.5.4.1.1" class="ltx_text" style="font-size:90%;">Synth test</span></th>
<td id="S4.T1.3.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.5.4.2.1" class="ltx_text" style="font-size:90%;">18.2</span></td>
<td id="S4.T1.3.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">18.3</span></td>
<td id="S4.T1.3.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.5.4.4.1" class="ltx_text" style="font-size:90%;">18.0</span></td>
<td id="S4.T1.3.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.5.4.5.1" class="ltx_text" style="font-size:90%;">17.9</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text" style="font-size:90%;">Regarding the baseline model, we emphasize that it does not include any additional token, and thus, BLEU scores are insensitive to the type of modality used to extract the emotion token and are respectively 20.1 and 18.2 on the dev and test sets.</span></p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text" style="font-size:90%;">In comparison with the baseline performances, we observe that the addition of the emotion token computed from real speech leads to a noticeable improvement for arousal, with a BLEU score of 18.6 on the test set of Libri-trans, and there is no degradation in BLEU score for dominance and valence, with BLEU scores of 18.3 and 18.2 respectively. These results show that the arousal information seems to be a promising factor to enhance translation quality. The best performances are achieved when using the arousal information, which has been automatically estimated from real speech, resulting in a 0.4-point improvement in the BLEU score.</span></p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text" style="font-size:90%;">When the emotion token is computed from synthesized text, we observe a slight improvement in the BLEU score for arousal. However, for dominance, we notice a slight degradation in the BLEU score. As mentioned earlier, dominance value estimations on synthesized audio almost always provide a value higher than 0.5. Thus, we anticipated a BLEU score lower or equal to the baseline, as no additional information is introduced to the NMT model. Finally, the highest deterioration in BLEU score is observed for valence.</span></p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p"><span id="S4.SS2.p9.1.1" class="ltx_text" style="font-size:90%;">Using emotion tokens extracted from real speech always leads to better BLEU scores than when using tokens from synthesized speech. We observed that, with synthesized audio, emotion value estimations become neutral in the case of arousal and dominance, and translation quality is degraded. Therefore, it appears crucial to use real audio recordings to estimate emotion and improve translation quality.
</span></p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p"><span id="S4.SS2.p10.1.1" class="ltx_text" style="font-size:90%;">In addition, we note that all BLEU scores presented in Table </span><a href="#S4.T1" title="Table 1 ‣ 4.2 Neural Machine Translation on Libri-trans ‣ 4 Experiments and results ‣ Usefulness of Emotional Prosody in Neural Machine Translation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p10.1.2" class="ltx_text" style="font-size:90%;"> are consistently low, indicating poor translation quality for all proposed models, including the baseline. This is due to the nature of the data within the Libri-trans dataset, which comprises English utterances extracted from books. Audios are recorded from readers, and the vocabulary in the texts differs significantly from the language commonly spoken in real-world interactions.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">We proposed an innovative method that combines SER with NMT models to improve translation quality. Our method is fully automatic and does not require any manual annotation. The proposed method is simple, effective, and enables the direct comparison between models. The best performances were achieved by adding the arousal token extracted from real speech at the start of each input sentence.</span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:90%;">This study only presents the first experiment on combining SER and NMT models to improve translation. Additional experiments can be conducted, such as using the three emotional dimensions to condition each source sentence, switching languages (translating from French to English), testing the method on other multilingual datasets including MuST-C, or visualizing the embedding of the NMT model to get a better understanding on how vocabulary related to a specific emotion is clustered. Moreover, different methods to include the emotion information can be tested, such as adding the embedding of the emotion token to all token embeddings.</span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text" style="font-size:90%;">In this work, we incorporate real speech recordings as an additional modality to estimate emotion. Introducing the audio modality into the translation task also enables to perform speech-to-text translation, known as Speech Translation. Consequently, exploring the conditioning of Speech Translation models with emotion as additional information would be valuable.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">The research presented in this paper is conducted as part of the project FVLLMONTI, which has received funding from the European Union’s Horizon 2020 Research and Innovation action under grant agreement No 101016776.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to Sequence Learning with Neural Networks,” in </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Annual Conference on Neural Information Processing Systems (NIPS)</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, Montreal, Canada, 2014, pp. 3104–3112.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the International Conference on Learning Representation (ICLR)</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, San Diego, USA, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
T. Kocmi, E. Avramidis, R. Bawden, O. Bojar, A. Dvorkovich, C. Federmann, M. Fishel, M. Freitag, T. Gowda, R. Grundkiewicz, B. Haddow, P. Koehn, B. Marie, C. Monz, M. Morishita, K. Murray, M. Nagata, T. Nakazawa, M. Popel, M. Popovic, and M. Shmatova, “Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here But Not Quite There Yet,” in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Conference on Machine Translation (WMT)</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, Singapore, 2023, pp. 1–42.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Annual Conference on Neural Information Processing Systems (NIPS)</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, Long Beach, USA, 2017, pp. 5998–6008.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
C. Si, K. Wu, A. Aw, and M.-Y. Kan, “Sentiment Aware Neural Machine Translation,” in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Workshop on Asian Translation (WAT)</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, Hong Kong, China, 2019, pp. 200–206.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
R. Sennrich, B. Haddow, and A. Birch, “Controlling Politeness in Neural Machine Translation via Side Constraints,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, San Diego, USA, 2019, pp. 35–40.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
E. Vanmassenhove, C. Hardmeier, and A. Way, “Getting Gender Right in Neural Machine Translation,” in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Conference on Empirical Methods in Natural Language Processing (CEMNLP)</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, Brussels, Belgium, 2018, pp. 3003–3008.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
M. Gaido, D. Fucci, M. Negri, and L. Bentivogli, “How to Build Competitive Multi-gender Speech Translation Models for Controlling Speaker Gender Translation,” in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Italian Conference on Computational Linguistics (CLiC-it)</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Y. Wang, C. Hoang, and M. Federico, “Towards Modeling the Style of Translators in Neural Machine Translation,” in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, Online, 2021, pp. 1193–1199.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Viégas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean, “Google’s Multilingual Neural Machine Translation System: Enabling Zero-shot Translation,” </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, vol. 5, pp. 339–351, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
J. Pennebaker, M. Francis, and R. Booth, “Linguistic Inquiry and Word Count: LIWC 2001,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Mahway: Lawrence Erlbaum Associates</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, vol. 71, 2001.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
A. Warriner, V. Kuperman, and M. Brysbaert, “Norms of Valence, Arousal, and Dominance for 13,915 English Lemmas,” </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Behavior Research Methods</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, vol. 45, no. 4, pp. 1191–1207, 2013.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
N. Braun, M. Goudbeek, and E. Krahmer, “Affective Words and the Company They Keep: Studying the Accuracy of Affective Word Lists in Determining Sentence and Word Valence in a Domain-Specific Corpus,” </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on Affective Computing</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, vol. 13, no. 3, pp. 1440–1451, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
E. Troiano, R. Klinger, and S. Padó, “Lost in Back-translation: Emotion Preservation in Neural Machine Translation,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the International Conference on Computational Linguistics (COLING)</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, Barcelona, Spain (Online), 2020, pp. 4340–4354.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap,” </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, vol. 45, pp. 10 745–10 759, 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec2.0: A Framework for Self-supervised Learning of Speech Representations,” in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Annual Conference on Neural Information Processing Systems (NeurIPS)</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, Online, 2020, pp. 12 449–12 460.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
R. Lotfian and C. Busso, “Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings,” </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, vol. 10, no. 4, pp. 471–483, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive Emotional dyadic MOtion CAPture database,” </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Language Resources and Evaluation</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, vol. 42, pp. 335–359, 2008.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages,” </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Intelligent Systems</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, vol. 31, no. 6, pp. 82–88, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
B. T. Atmaja and M. Akagi, “Evaluation of Error-and Correlation-based Loss Functions for Multitask Learning Dimensional Speech Emotion Recognition,” </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Physics: Conference Series</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, vol. 1896, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, “ESPnet: End-to-end Speech Processing Toolkit,” in </span><em id="bib.bib21.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Annual Conference of the International Speech Communication Association (Interspeech)</em><span id="bib.bib21.5.5" class="ltx_text" style="font-size:90%;">, Hyderabad, India, 2018, pp. 2207–2211.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, “Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation,” in </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the International Conference on Language Resources and Evaluation (LREC)</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, Miyazaki, Japan, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Post, “A Call for Clarity in Reporting BLEU Scores,” in </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the Conference on Machine Translation: Research Papers (WMT)</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, Brussels, Belgium, 2018, pp. 186–191.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.17967" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.17968" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.17968">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.17968" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.17969" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 16:41:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
