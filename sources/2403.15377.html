<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.15377] InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><meta property="og:description" content="We introduce InternVideo2, a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue.
Our approach employs a progressive trai…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.15377">

<!--Generated on Fri Apr  5 17:39:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id25.id1" class="ltx_text ltx_font_bold">InternVideo2</span>: Scaling Video Foundation Models for Multimodal Video Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yi Wang<sup id="id26.25.id1" class="ltx_sup"><span id="id26.25.id1.1" class="ltx_text ltx_font_italic">∗1</span></sup>, Kunchang Li<sup id="id27.26.id2" class="ltx_sup"><span id="id27.26.id2.1" class="ltx_text ltx_font_italic">∗6,1</span></sup>, Xinhao Li<sup id="id28.27.id3" class="ltx_sup"><span id="id28.27.id3.1" class="ltx_text ltx_font_italic">∗2,1</span></sup>, Jiashuo Yu<sup id="id29.28.id4" class="ltx_sup"><span id="id29.28.id4.1" class="ltx_text ltx_font_italic">∗1</span></sup>, Yinan He<sup id="id30.29.id5" class="ltx_sup"><span id="id30.29.id5.1" class="ltx_text ltx_font_italic">∗1</span></sup>
<br class="ltx_break"><span id="id18.18.13" class="ltx_text ltx_font_bold">Guo Chen<sup id="id18.18.13.1" class="ltx_sup"><span id="id18.18.13.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2,1</span></sup>, Baoqi Pei<sup id="id18.18.13.2" class="ltx_sup"><span id="id18.18.13.2.1" class="ltx_text ltx_font_medium ltx_font_italic">3,1</span></sup>, Rongkun Zheng<sup id="id18.18.13.3" class="ltx_sup"><span id="id18.18.13.3.1" class="ltx_text ltx_font_medium ltx_font_italic">4,1</span></sup>, Jilan Xu<sup id="id18.18.13.4" class="ltx_sup"><span id="id18.18.13.4.1" class="ltx_text ltx_font_medium ltx_font_italic">5,1</span></sup>, Zun Wang<sup id="id18.18.13.5" class="ltx_sup"><span id="id18.18.13.5.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
<br class="ltx_break">Yansong Shi<sup id="id18.18.13.6" class="ltx_sup"><span id="id18.18.13.6.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Tianxiang Jiang<sup id="id18.18.13.7" class="ltx_sup"><span id="id18.18.13.7.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Songze Li<sup id="id18.18.13.8" class="ltx_sup"><span id="id18.18.13.8.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Hongjie Zhang<sup id="id18.18.13.9" class="ltx_sup"><span id="id18.18.13.9.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Yifei Huang<sup id="id18.18.13.10" class="ltx_sup"><span id="id18.18.13.10.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
<br class="ltx_break">Yu Qiao<sup id="id18.18.13.11" class="ltx_sup"><span id="id18.18.13.11.1" class="ltx_text ltx_font_medium ltx_font_italic">†1</span></sup>, Yali Wang<sup id="id18.18.13.12" class="ltx_sup"><span id="id18.18.13.12.1" class="ltx_text ltx_font_medium ltx_font_italic">†6,1</span></sup>, Limin Wang<sup id="id18.18.13.13" class="ltx_sup"><span id="id18.18.13.13.1" class="ltx_text ltx_font_medium ltx_font_italic">†2,1</span></sup>
<br class="ltx_break"></span> <sup id="id31.30.id6" class="ltx_sup">1</sup>OpenGVLab, Shanghai AI Laboratory  <sup id="id32.31.id7" class="ltx_sup">2</sup>Nanjing University 
<br class="ltx_break"><sup id="id33.32.id8" class="ltx_sup">3</sup>Zhejiang University <sup id="id34.33.id9" class="ltx_sup">4</sup> The University of Hong Kong  <sup id="id35.34.id10" class="ltx_sup">5</sup>Fudan University 
<br class="ltx_break"><sup id="id36.35.id11" class="ltx_sup">6</sup>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 
<br class="ltx_break">
<br class="ltx_break"><a target="_blank" href="https://github.com/OpenGVLab/InternVideo2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenGVLab/InternVideo2</a> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id37.id1" class="ltx_p">We introduce <span id="id37.id1.1" class="ltx_text ltx_font_bold">InternVideo2</span>, a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue.
Our approach employs a progressive training paradigm that unifies the different self- or weakly-supervised learning frameworks of masked video token reconstruction, cross-modal contrastive learning, and next token prediction.
Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks.
At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our <span id="id37.id1.2" class="ltx_text ltx_font_bold">InternVideo2</span>.
Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts.</p>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span>*Equal contribution. <math id="footnotex1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="footnotex1.m1.1b"><mo id="footnotex1.m1.1.1" xref="footnotex1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="footnotex1.m1.1c"><ci id="footnotex1.m1.1.1.cmml" xref="footnotex1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.m1.1d">\dagger</annotation></semantics></math>Corresponding authors.</span></span></span>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2403.15377/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.2.1" class="ltx_text ltx_font_bold">InternVideo2</span> yields strong transferable visual and visual-linguistic representations across a total of 70 video understanding tasks, ranging from action recognition, video-text understanding, to video-centric dialogue. It also exhibits capability of long-form video understanding and procedure-aware reasoning.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The pursuit of transferable spatiotemporal representations is a critical research area within vision understanding, holding diverse applications across domains such as video searching <cite class="ltx_cite ltx_citemacro_cite">Gabeur et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, game control <cite class="ltx_cite ltx_citemacro_cite">Bruce et al. (<a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite>, robotic learning <cite class="ltx_cite ltx_citemacro_cite">Driess et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>, self-driving <cite class="ltx_cite ltx_citemacro_cite">Zablocki et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, and scientific studies <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. Recently, the advancement of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); OpenAI (<a href="#bib.bib7" title="" class="ltx_ref">2023a</a>); Touvron et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib9" title="" class="ltx_ref">b</a>)</cite> and their multimodal variations (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib10" title="" class="ltx_ref">2023b</a>); Gong et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>); Zhu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite> has had a profound impact on vision research and other disciplines, thanks to their learned world models <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib14" title="" class="ltx_ref">2023c</a>); Yang et al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite>. Embedding videos effectively into these world models and harnessing their capabilities to enhance video embedding have emerged as pivotal tasks for advancing video understanding.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Previous research has identified several effective learning schemes for video representation, including reconstructing videos with masked inputs (MAE <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> and VideoMAE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>), aligning videos with languages <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>); Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite>, and predicting the next token using videos <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Sun et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023a</a>); Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>. These approaches have been shown to be complementary and can be unified through a progressive training scheme. Notably, methods such as UMT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite>, InternVideo <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, and VideoPrism <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite> have utilized a two-stage training approach involving masked reconstruction and multimodal contrastive learning, leading to improved performance in downstream tasks. These studies have demonstrated the benefits of joint learning for enhancing spatiotemporal perception and video-language alignment.
This paper extends upon this scheme by incorporating video-based next token prediction and scaling the entire training process, including models and data.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The proposed video foundation model <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>, is built through a progressive learning scheme. The learning involves three stages aimed at improving the model’s spatiotemporal perception, aligning it with semantics from other modalities, and enhancing its world modeling through next token prediction <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>.
In the initial stage of masked video token reconstruction, the model learns to reconstruct masked video tokens, allowing the video encoder to develop basic spatiotemporal perception. To estimate the masked tokens, vision encoders (InternViT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite> and VideoMAE-g <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>) trained differently are employed as proxies.
In the next stage of multimodal learning, the architecture is expanded to include audio and text encoders. This not only improves the alignment between videos and text but also grants <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> the ability to handle video-audio tasks. By incorporating these additional modalities, the model’s understanding of videos is enriched and aligned with the semantics provided by audios.
Finally, in the next-token prediction stage, a video-centric dialogue system and the corresponding instruction-finetuning dataset are leveraged to train <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">InternVideo2</span>. This transfer learning process allows the model to benefit from the knowledge acquired by LLM and others. By connecting <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">InternVideo2</span> to LLMs, the video encoder is further updated through next-token prediction training, enhancing its ability to generate contextually appropriate next tokens.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In pretraining, we emphasize incorporating spatiotemporal consistency in the data. We utilize a large-scale multimodal video-centric dataset consisting of 412M data entries, which includes 2M videos, 50M video-text pairs, 60M video-audio-speech-text pairs, and 300M image-text pairs.
In multimodal pretraining data, we segment videos into clips semantically, and we focus on recalibrating the descriptions with videos using three modalities: audio, video, and speech. To this end, we generate captions for these three modalities separately. Then individual captions are fused together to create a more comprehensive description.
By incorporating audio, video, and speech modalities in the training process, we provide a richer and more detailed understanding of the videos. This holistic data approach to video-text alignment improves the model’s ability to comprehend and interpret the video accurately.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We evaluate <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> across a range of video-related tasks. These tasks span from basic spatiotemporal perception, such as action recognition, to more complex reasoning tasks, such as long video or procedure-aware question-answering (QA), as given in Fig. <a href="#S0.F1" title="Figure 1 ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The results (in Sec. <a href="#S5" title="5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) demonstrate that <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> achieves state-of-the-art performance on multiple tasks, able to analyzes and reasons sequences of actions.
These highlight that <span id="S1.p5.1.3" class="ltx_text ltx_font_bold">InternVideo2</span> excels not only in video perception and video-language alignment but also in modeling the world. Its top performance on various benchmarks in the field of multimodal language models (MLLM) signifies its capability to effectively capture and understand video content. These empirical findings validate that <span id="S1.p5.1.4" class="ltx_text ltx_font_bold">InternVideo2</span> serves as a qualified video encoder for future exploration in video understanding.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions to video understanding are as follows.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix1.1.1.m1.1b"><mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c"><ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">This paper introduces <span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>, a competitive video foundation model that leverages masked reconstruction, crossmodal contrastive learning, and next token prediction to make model perceptive, semantic, and capable of reasoning in video understanding. It achieves the state-of-the-art performance in 65 out of 74 video / audio tasks. Our model demonstrates superior performance in video-related dialogue and long video understanding, highlighting its potential in various world model studies and applications.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix2.1.1.m1.1b"><mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c"><ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.ix2.p1.1" class="ltx_p">We provide an enhanced dataset to train <span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>. This includes the validation and incorporation of audio data during training, as well as the implementation of annotation techniques. These improvements result in significant enhancements in model generalization and performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Video Understanding.</span>
Studies on learning video foundation models becomes increasingly crucial considering its wide applications <cite class="ltx_cite ltx_citemacro_cite">Li and Wang (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>); Xu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib28" title="" class="ltx_ref">2022a</a>); Hu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>); Dou et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>); Shen et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>); Yao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>); Sun et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Zhu and Yang (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>); Wang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>); Chen et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022a</a>); Zellers et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>); Zeng et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib39" title="" class="ltx_ref">b</a>); Chen et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib41" title="" class="ltx_ref">c</a>); He et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>); Chen et al. (<a href="#bib.bib43" title="" class="ltx_ref">2024a</a>); Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>); Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>); Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>); Feichtenhofer et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023c</a>); Miech et al. (<a href="#bib.bib47" title="" class="ltx_ref">2020</a>); Tong et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>); Li et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023c</a>)</cite>. Typical methods in building video foundation models (ViFM) include video-text contrastive learning <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Wang et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023c</a>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, masked video modeling <cite class="ltx_cite ltx_citemacro_cite">Tong et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>); Fu et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>, and next token prediction <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Sun et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib51" title="" class="ltx_ref">b</a>)</cite>. Notably, All-in-one <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023c</a>)</cite> utilizes a single backbone with unified multiple pretraining objectives. On the other hand, UMT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite> combines masked modeling with video-text contrastive learning, demonstrating strong performance in both action recognition and video-language tasks. Another approach is mPLUG-2 <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>, which introduces a new design for modulating different modalities. It shares a common module across modalities to enhance relations while incorporating modality-specific modules for discrimination.
In addition to video-text pretraining, researchers have also explored the use of audio information in videos to improve performance. MERLOT Reserve <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite> learns video representations using a large-scale dataset of video-speech-transcript pairs. VALOR <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023c</a>)</cite> employs independent encoders for video, audio, and text and trains a joint visual-audio-text representation. VAST <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib43" title="" class="ltx_ref">2024a</a>)</cite> constructs an audio-visual-speech dataset and develops a multimodal backbone that excels in video-audio-related tasks. VideoPrism <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite> combines video-text contrastive learning and video token reconstruction on a combination of public and proprietary videos, achieving leading results across various video tasks.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multimodal Large Language Models.</span> With advances in large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>); Raffel et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>); Brown et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); Wei et al. (<a href="#bib.bib55" title="" class="ltx_ref">2021</a>); Chowdhery et al. (<a href="#bib.bib56" title="" class="ltx_ref">2022</a>); Sun et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib51" title="" class="ltx_ref">b</a>)</cite>, their multimodal versions (MLLMs) gain a lot heat considering its open-world visual task execution by simple instructions. Seminal works like Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> shows outstanding zero/few-shots performances over a range of multimodal tasks <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib57" title="" class="ltx_ref">2017a</a>); Plummer et al. (<a href="#bib.bib58" title="" class="ltx_ref">2015</a>); Xu et al. (<a href="#bib.bib59" title="" class="ltx_ref">2016</a>); Marino et al. (<a href="#bib.bib60" title="" class="ltx_ref">2019</a>)</cite>. Public MLLMs <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib61" title="" class="ltx_ref">2023b</a>); Liu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>); Gong et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
e.g. LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a href="#bib.bib62" title="" class="ltx_ref">2023</a>)</cite> give visual instruction-tuning data for dialogue experience. Some video-related MLLM (VideoLLM) VideoChatGPT <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. (<a href="#bib.bib63" title="" class="ltx_ref">2023a</a>)</cite> and Valley <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite> utilize ChatGPT to generate video instruction-tuning data, aiming to enhance instruction-following capabilities for real-world video comprehension.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.15377/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Framework of <span id="S2.F2.2.1" class="ltx_text ltx_font_bold">InternVideo2</span>. It consists of three consecutive training phases: unmasked video token reconstruction, multimodal contrastive learning, and next token prediction. In stage 1, the video encoder is trained from scratch, while in stages 2 and 3, it is initialized from the version used in the previous stage.
</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We learn <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> in three stages, illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The stages include spatiotemporal token reconstruction, video-audio-speech-language contrastive learning, and joint training of a large language model (LLM).</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Video Encoder.</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p1.2" class="ltx_p">The video encoder used in <span id="S3.SS0.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_bold">InternVideo2</span> follows the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite> and includes additional projection layers for distillation. Inspired by previous works <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>); Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite>, we introduce attention pooling to the ViT.
For input videos, we sparsely sample 8 frames and perform a 14<math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">\times</annotation></semantics></math>14 (<math id="S3.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="h\times w" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1"><times id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1"></times><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2">ℎ</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.1c">h\times w</annotation></semantics></math>) spatial downsampling. These spatiotemporal tokens are then concatenated with a class token and combined with 3D position embeddings. The hyperparameters used for ViT-1B and -6B are the same as CoCa-1B <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite> and InternViT-6B <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite>, respectively. The details of ViT-6B architecture are given in Supp. Tab. <a href="#A1.T21" title="Table 21 ‣ Appendix A Model ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>.</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stage1: Reconstructing Masked Video Tokens</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.10" class="ltx_p">We exploit two expert models to guide the video encoder to conduct masked token-level reconstruction.
Specifically, we adopt InternVL-6B <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite> and VideoMAEv2-g <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite> to transfer unmasked knowledge via simple projection layers.
When training, we input the full videos into different teachers and masked out 80% of the tokens frame by frame, under the semantic guidance of the multimodal model InternVL and motion-aware model VideoMAEv2. We only align the related unmasked tokens, by minimizing their mean squared error (MSE). The learning objective of this stage is formed by reconstructing the remaining tokens as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathcal{L}=\frac{1}{Z}\sum{(\alpha_{1}|f^{V}(\mathbf{V}_{p})-h(\mathbf{V}_{p})|^{2}+\alpha_{2}|f^{V}(\mathbf{V}_{p})-g(\mathbf{V}_{p})|^{2})}," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">ℒ</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.3.3.cmml">Z</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">α</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">V</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐕</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">𝐕</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">α</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.cmml">V</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml">𝐕</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.2.cmml">𝐕</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">|</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.3.cmml">2</mn></msup></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.3">𝑍</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><sum id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></sum><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑉</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐕</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑝</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">ℎ</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.2">𝐕</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.3">𝑝</ci></apply></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2">𝛼</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.3">𝑉</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2">𝐕</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3">𝑝</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3">𝑔</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.2">𝐕</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.1.1.3">𝑝</ci></apply></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}=\frac{1}{Z}\sum{(\alpha_{1}|f^{V}(\mathbf{V}_{p})-h(\mathbf{V}_{p})|^{2}+\alpha_{2}|f^{V}(\mathbf{V}_{p})-g(\mathbf{V}_{p})|^{2})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.9" class="ltx_p">where <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="f^{V}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msup id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">V</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">f^{V}</annotation></semantics></math>, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">h</annotation></semantics></math>, and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">g</annotation></semantics></math> are our video encoder, InternViT-6B <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite>, and ViT-g of VideoMAEv2, respectively. <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">p</annotation></semantics></math> stands for the token index and <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="f(\mathbf{V}_{p})" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS1.p1.5.m5.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p1.5.m5.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2.cmml">𝐕</mi><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.SS1.p1.5.m5.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2"></times><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑓</ci><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.2">𝐕</ci><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">f(\mathbf{V}_{p})</annotation></semantics></math> is the corresponding token extracted from <span id="S3.SS1.p1.9.1" class="ltx_text ltx_font_bold">InternVideo2</span> of input video <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathbf{V}</annotation></semantics></math>. <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">Z</annotation></semantics></math> is the normalization factor. <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="\alpha_{1}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">α</mi><mn id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝛼</ci><cn type="integer" id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\alpha_{1}</annotation></semantics></math> and <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="\alpha_{2}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">α</mi><mn id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝛼</ci><cn type="integer" id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\alpha_{2}</annotation></semantics></math> balance the influence between the employed models.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.2" class="ltx_p">In implementation, we align the last <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">6</annotation></semantics></math> layers of InternVL, last <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">4</annotation></semantics></math> layers of VideoMAE, and the final output token of InternVL. Different losses are simply summed for optimization. After pretraining, we drop those projection layers and only use the basic encoder. Compared with only using the multimodal model in UMT and VideoPrism, our strategy makes the vision encoder multimodal-friendly as well as enhances its temporal sensitivity for action modeling.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stage 2: Aligning Video to Audio-Speech-Text</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">We exploit the correspondence between video and audio, speech, and text to align <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> to semantics explicitly.
In structure, though <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> has a huge video encoder, its employed audio and text encoders are relatively lightweight. The used audio encoder is a 12-layer transformer initialized with BEATs <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib67" title="" class="ltx_ref">2023d</a>)</cite> (90M). It takes in audio features, which are 64-dimensional log Mel filterbank spectrograms using a 25ms Hamming window, transformed from 10-second-long clips, padding with zeros.
For the text and speech encoders, we initialize the text encoder and multimodal decoder using Bert-Large <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>)</cite>. Specifically, we utilize the initial 19 layers of Bert-Large as the text encoder, with the subsequent 5 layers equipped with cross-attention layers serving as the multimodal decoder.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.6" class="ltx_p">For pretraining objectives,
we establish alignment and fusion across modalities via text, including video, audio, image, and speech. We employ crossmodal contrastive and matching losses with masked language reconstruction one as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}=\beta_{1}\mathcal{L}_{\text{CON}}+\beta_{2}\mathcal{L}_{\text{MAC}}+\beta_{3}\mathcal{L}_{\text{MLM}}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><msub id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.2.cmml">β</mi><mn id="S3.E2.m1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.3.2.1.cmml">​</mo><msub id="S3.E2.m1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.1.3.2.3.3a.cmml">CON</mtext></msub></mrow><mo id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><msub id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.1.1.3.3.2.2.cmml">β</mi><mn id="S3.E2.m1.1.1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.1.1.3.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.3a.cmml">MAC</mtext></msub></mrow><mo id="S3.E2.m1.1.1.1.1.3.1a" xref="S3.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.1.1.1.1.3.4.2" xref="S3.E2.m1.1.1.1.1.3.4.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.4.2.2" xref="S3.E2.m1.1.1.1.1.3.4.2.2.cmml">β</mi><mn id="S3.E2.m1.1.1.1.1.3.4.2.3" xref="S3.E2.m1.1.1.1.1.3.4.2.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.4.1" xref="S3.E2.m1.1.1.1.1.3.4.1.cmml">​</mo><msub id="S3.E2.m1.1.1.1.1.3.4.3" xref="S3.E2.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.4.3.2" xref="S3.E2.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.1.1.1.1.3.4.3.3" xref="S3.E2.m1.1.1.1.1.3.4.3.3a.cmml">MLM</mtext></msub></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2">ℒ</ci><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><plus id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><times id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1"></times><apply id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2">𝛽</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3">1</cn></apply><apply id="S3.E2.m1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2">ℒ</ci><ci id="S3.E2.m1.1.1.1.1.3.2.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.3">CON</mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><apply id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.2">𝛽</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.3">2</cn></apply><apply id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3">MAC</mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.4"><times id="S3.E2.m1.1.1.1.1.3.4.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4.1"></times><apply id="S3.E2.m1.1.1.1.1.3.4.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2.2">𝛽</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2.3">3</cn></apply><apply id="S3.E2.m1.1.1.1.1.3.4.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.2">ℒ</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.4.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3.3">MLM</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}=\beta_{1}\mathcal{L}_{\text{CON}}+\beta_{2}\mathcal{L}_{\text{MAC}}+\beta_{3}\mathcal{L}_{\text{MLM}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.5" class="ltx_p">where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">β</mi><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝛽</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\beta_{1}</annotation></semantics></math>, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">β</mi><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝛽</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\beta_{2}</annotation></semantics></math>, and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\beta_{3}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">β</mi><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝛽</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\beta_{3}</annotation></semantics></math> are hyperparameters to tradeoff different losses. The employed <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{MAC}}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3a.cmml">MAC</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ℒ</ci><ci id="S3.SS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">MAC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathcal{L}_{\text{MAC}}</annotation></semantics></math> and <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{\text{MLM}}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3a.cmml">MLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ℒ</ci><ci id="S3.SS2.p2.5.m5.1.1.3a.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">MLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathcal{L}_{\text{MLM}}</annotation></semantics></math> are standard and referenced from <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib68" title="" class="ltx_ref">2022</a>)</cite>. Specifically, the crossmodal contrastive learning is given as:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<div id="S3.SS2.p3.7" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:30.4pt;vertical-align:-12.2pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<div id="S3.SS2.p3.7.1" class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<table id="A3.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</span></div>
<p id="S3.SS2.p3.6" class="ltx_p">where <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="f^{V}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">V</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑓</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">f^{V}</annotation></semantics></math> and <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="f^{T}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝑓</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">f^{T}</annotation></semantics></math> denote the learned video and text embeddings, respectively. <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">M</annotation></semantics></math> and <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="T_{M^{\prime}}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">T</mi><msup id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.3.2.cmml">M</mi><mo id="S3.SS2.p3.4.m4.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.3.2">𝑀</ci><ci id="S3.SS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">T_{M^{\prime}}</annotation></semantics></math> indicates the modality of input signals and the text descriptions describing, respectively.
<math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="\text{sim}(\cdot)" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.2" xref="S3.SS2.p3.5.m5.1.2.cmml"><mtext id="S3.SS2.p3.5.m5.1.2.2" xref="S3.SS2.p3.5.m5.1.2.2a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.2.1" xref="S3.SS2.p3.5.m5.1.2.1.cmml">​</mo><mrow id="S3.SS2.p3.5.m5.1.2.3.2" xref="S3.SS2.p3.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.1.2.3.2.1" xref="S3.SS2.p3.5.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p3.5.m5.1.2.3.2.2" xref="S3.SS2.p3.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.2.cmml" xref="S3.SS2.p3.5.m5.1.2"><times id="S3.SS2.p3.5.m5.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.2.1"></times><ci id="S3.SS2.p3.5.m5.1.2.2a.cmml" xref="S3.SS2.p3.5.m5.1.2.2"><mtext id="S3.SS2.p3.5.m5.1.2.2.cmml" xref="S3.SS2.p3.5.m5.1.2.2">sim</mtext></ci><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\text{sim}(\cdot)</annotation></semantics></math> computes the cosine similarity between two features. <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\tau</annotation></semantics></math> is the learnable temperature.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.7" class="ltx_p">For the matching part, it is given as:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.6" class="ltx_Math" alttext="\mathcal{L}_{\text{MAC}}=-y\log f_{p}(\mathbf{V},\mathbf{T})-(1-y)\log(1-f_{p}(\mathbf{V},\mathbf{T}))," display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><msub id="S3.E4.m1.6.6.1.1.4" xref="S3.E4.m1.6.6.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.6.6.1.1.4.2" xref="S3.E4.m1.6.6.1.1.4.2.cmml">ℒ</mi><mtext id="S3.E4.m1.6.6.1.1.4.3" xref="S3.E4.m1.6.6.1.1.4.3a.cmml">MAC</mtext></msub><mo id="S3.E4.m1.6.6.1.1.3" xref="S3.E4.m1.6.6.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.6.6.1.1.2" xref="S3.E4.m1.6.6.1.1.2.cmml"><mrow id="S3.E4.m1.6.6.1.1.2.4" xref="S3.E4.m1.6.6.1.1.2.4.cmml"><mo id="S3.E4.m1.6.6.1.1.2.4a" xref="S3.E4.m1.6.6.1.1.2.4.cmml">−</mo><mrow id="S3.E4.m1.6.6.1.1.2.4.2" xref="S3.E4.m1.6.6.1.1.2.4.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.4.2.2" xref="S3.E4.m1.6.6.1.1.2.4.2.2.cmml">y</mi><mo lspace="0.167em" rspace="0em" id="S3.E4.m1.6.6.1.1.2.4.2.1" xref="S3.E4.m1.6.6.1.1.2.4.2.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.2.4.2.3" xref="S3.E4.m1.6.6.1.1.2.4.2.3.cmml"><mi id="S3.E4.m1.6.6.1.1.2.4.2.3.1" xref="S3.E4.m1.6.6.1.1.2.4.2.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E4.m1.6.6.1.1.2.4.2.3a" xref="S3.E4.m1.6.6.1.1.2.4.2.3.cmml">⁡</mo><msub id="S3.E4.m1.6.6.1.1.2.4.2.3.2" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.4.2.3.2.2" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2.2.cmml">f</mi><mi id="S3.E4.m1.6.6.1.1.2.4.2.3.2.3" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2.3.cmml">p</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.2.4.2.1a" xref="S3.E4.m1.6.6.1.1.2.4.2.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.2.4.2.4.2" xref="S3.E4.m1.6.6.1.1.2.4.2.4.1.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.4.2.4.2.1" xref="S3.E4.m1.6.6.1.1.2.4.2.4.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">𝐕</mi><mo id="S3.E4.m1.6.6.1.1.2.4.2.4.2.2" xref="S3.E4.m1.6.6.1.1.2.4.2.4.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">𝐓</mi><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.4.2.4.2.3" xref="S3.E4.m1.6.6.1.1.2.4.2.4.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.6.6.1.1.2.3" xref="S3.E4.m1.6.6.1.1.2.3.cmml">−</mo><mrow id="S3.E4.m1.6.6.1.1.2.2" xref="S3.E4.m1.6.6.1.1.2.2.cmml"><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml"><mn id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.E4.m1.6.6.1.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E4.m1.6.6.1.1.2.2.3" xref="S3.E4.m1.6.6.1.1.2.2.3.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">log</mi><mo id="S3.E4.m1.6.6.1.1.2.2.2.1a" xref="S3.E4.m1.6.6.1.1.2.2.2.2.cmml">⁡</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.cmml">(</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.cmml"><mn id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.2" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.1" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.1.cmml">−</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.cmml"><msub id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.2" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.2.cmml">f</mi><mi id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.3" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.1" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.2.1" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.1.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">𝐕</mi><mo id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.2.2" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">𝐓</mi><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.2.3" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.3" xref="S3.E4.m1.6.6.1.1.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><eq id="S3.E4.m1.6.6.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.3"></eq><apply id="S3.E4.m1.6.6.1.1.4.cmml" xref="S3.E4.m1.6.6.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.1.cmml" xref="S3.E4.m1.6.6.1.1.4">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.2">ℒ</ci><ci id="S3.E4.m1.6.6.1.1.4.3a.cmml" xref="S3.E4.m1.6.6.1.1.4.3"><mtext mathsize="70%" id="S3.E4.m1.6.6.1.1.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.3">MAC</mtext></ci></apply><apply id="S3.E4.m1.6.6.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2"><minus id="S3.E4.m1.6.6.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.3"></minus><apply id="S3.E4.m1.6.6.1.1.2.4.cmml" xref="S3.E4.m1.6.6.1.1.2.4"><minus id="S3.E4.m1.6.6.1.1.2.4.1.cmml" xref="S3.E4.m1.6.6.1.1.2.4"></minus><apply id="S3.E4.m1.6.6.1.1.2.4.2.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2"><times id="S3.E4.m1.6.6.1.1.2.4.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.1"></times><ci id="S3.E4.m1.6.6.1.1.2.4.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.2">𝑦</ci><apply id="S3.E4.m1.6.6.1.1.2.4.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3"><log id="S3.E4.m1.6.6.1.1.2.4.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3.1"></log><apply id="S3.E4.m1.6.6.1.1.2.4.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.4.2.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.4.2.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2.2">𝑓</ci><ci id="S3.E4.m1.6.6.1.1.2.4.2.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.3.2.3">𝑝</ci></apply></apply><interval closure="open" id="S3.E4.m1.6.6.1.1.2.4.2.4.1.cmml" xref="S3.E4.m1.6.6.1.1.2.4.2.4.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝐕</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝐓</ci></interval></apply></apply><apply id="S3.E4.m1.6.6.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2"><times id="S3.E4.m1.6.6.1.1.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.3"></times><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1"><minus id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3">𝑦</ci></apply><apply id="S3.E4.m1.6.6.1.1.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1"><log id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5"></log><apply id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1"><minus id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.2">1</cn><apply id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3"><times id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.1"></times><apply id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.2">𝑓</ci><ci id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.2.3">𝑝</ci></apply><interval closure="open" id="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.1.1.1.3.3.2"><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝐕</ci><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">𝐓</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\mathcal{L}_{\text{MAC}}=-y\log f_{p}(\mathbf{V},\mathbf{T})-(1-y)\log(1-f_{p}(\mathbf{V},\mathbf{T})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.6" class="ltx_p">where <math id="S3.SS2.p4.1.m1.2" class="ltx_Math" alttext="f_{p}(\mathbf{V},\mathbf{T})" display="inline"><semantics id="S3.SS2.p4.1.m1.2a"><mrow id="S3.SS2.p4.1.m1.2.3" xref="S3.SS2.p4.1.m1.2.3.cmml"><msub id="S3.SS2.p4.1.m1.2.3.2" xref="S3.SS2.p4.1.m1.2.3.2.cmml"><mi id="S3.SS2.p4.1.m1.2.3.2.2" xref="S3.SS2.p4.1.m1.2.3.2.2.cmml">f</mi><mi id="S3.SS2.p4.1.m1.2.3.2.3" xref="S3.SS2.p4.1.m1.2.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.2.3.1" xref="S3.SS2.p4.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS2.p4.1.m1.2.3.3.2" xref="S3.SS2.p4.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.2.3.3.2.1" xref="S3.SS2.p4.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">𝐕</mi><mo id="S3.SS2.p4.1.m1.2.3.3.2.2" xref="S3.SS2.p4.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS2.p4.1.m1.2.2" xref="S3.SS2.p4.1.m1.2.2.cmml">𝐓</mi><mo stretchy="false" id="S3.SS2.p4.1.m1.2.3.3.2.3" xref="S3.SS2.p4.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.2b"><apply id="S3.SS2.p4.1.m1.2.3.cmml" xref="S3.SS2.p4.1.m1.2.3"><times id="S3.SS2.p4.1.m1.2.3.1.cmml" xref="S3.SS2.p4.1.m1.2.3.1"></times><apply id="S3.SS2.p4.1.m1.2.3.2.cmml" xref="S3.SS2.p4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.2.3.2.1.cmml" xref="S3.SS2.p4.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS2.p4.1.m1.2.3.2.2.cmml" xref="S3.SS2.p4.1.m1.2.3.2.2">𝑓</ci><ci id="S3.SS2.p4.1.m1.2.3.2.3.cmml" xref="S3.SS2.p4.1.m1.2.3.2.3">𝑝</ci></apply><interval closure="open" id="S3.SS2.p4.1.m1.2.3.3.1.cmml" xref="S3.SS2.p4.1.m1.2.3.3.2"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐕</ci><ci id="S3.SS2.p4.1.m1.2.2.cmml" xref="S3.SS2.p4.1.m1.2.2">𝐓</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.2c">f_{p}(\mathbf{V},\mathbf{T})</annotation></semantics></math> computes the matching likelihood between video <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\mathbf{V}</annotation></semantics></math> and text <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{T}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\mathbf{T}</annotation></semantics></math>, respectively. <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">y</annotation></semantics></math> denotes whether the given video and text are paired (<math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="y=1" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mrow id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">y</mi><mo id="S3.SS2.p4.5.m5.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><eq id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1.1"></eq><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">𝑦</ci><cn type="integer" id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">y=1</annotation></semantics></math>) or not (<math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="y=0" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><mrow id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mi id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">y</mi><mo id="S3.SS2.p4.6.m6.1.1.1" xref="S3.SS2.p4.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><eq id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1.1"></eq><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">𝑦</ci><cn type="integer" id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">y=0</annotation></semantics></math>).</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.4" class="ltx_p">The employed masked language modeling loss is:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{MLM}}=-\log f_{p}^{T}(\mathbf{T}_{j}|\mathbf{T}_{&lt;j})," display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3a.cmml">MLM</mtext></msub><mo id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mo rspace="0.167em" id="S3.E5.m1.1.1.1.1.1a" xref="S3.E5.m1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E5.m1.1.1.1.1.1.1.3a" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">⁡</mo><msubsup id="S3.E5.m1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.3.2.2.2" xref="S3.E5.m1.1.1.1.1.1.1.3.2.2.2.cmml">f</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.3.2.2.3.cmml">p</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.1.1.3.2.3.cmml">T</mi></msubsup></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐓</mi><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐓</mi><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">j</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2">ℒ</ci><ci id="S3.E5.m1.1.1.1.1.3.3a.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3">MLM</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><minus id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1"></minus><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3"><log id="S3.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.1"></log><apply id="S3.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2">superscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2.2.2">𝑓</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2.2.3">𝑝</ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2.3">𝑇</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2">𝐓</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2">𝐓</ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathcal{L}_{\text{MLM}}=-\log f_{p}^{T}(\mathbf{T}_{j}|\mathbf{T}_{&lt;j}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.3" class="ltx_p">where <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="f_{p}^{T}(\mathbf{T}_{j}|\mathbf{T}_{&lt;j})" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><msubsup id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.3.2.2" xref="S3.SS2.p5.1.m1.1.1.3.2.2.cmml">f</mi><mi id="S3.SS2.p5.1.m1.1.1.3.2.3" xref="S3.SS2.p5.1.m1.1.1.3.2.3.cmml">p</mi><mi id="S3.SS2.p5.1.m1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.3.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS2.p5.1.m1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2.2.cmml">𝐓</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S3.SS2.p5.1.m1.1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS2.p5.1.m1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.2.cmml">𝐓</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.3.cmml">j</mi></mrow></msub></mrow><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><times id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2"></times><apply id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3">superscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2.2">𝑓</ci><ci id="S3.SS2.p5.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2.3">𝑝</ci></apply><ci id="S3.SS2.p5.1.m1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.3">𝑇</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2.2">𝐓</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.2">𝐓</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3"><lt id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.3.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">f_{p}^{T}(\mathbf{T}_{j}|\mathbf{T}_{&lt;j})</annotation></semantics></math> computes the likelihood of the <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="j_{\text{th}}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">j</mi><mtext id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3a.cmml">th</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝑗</ci><ci id="S3.SS2.p5.2.m2.1.1.3a.cmml" xref="S3.SS2.p5.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">j_{\text{th}}</annotation></semantics></math> text token based on the previous ones. Here <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\mathbf{T}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\mathbf{T}</annotation></semantics></math> refers to video captions.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p">For training efficiency, we employ masking learning, aligning unmasked video tokens to tokens from other modalities first, then using full video tokens reconstruction shortly. Specifically, it consists of two steps as follows:</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Aligning Masked Visual-Language-Audio.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We freeze the audio encoder, focusing primarily on aligning visual and text features. We employ the complete set of image, video, and audio-video data for pre-training, where <math id="S3.SS2.SSS0.Px1.p1.1.m1.10" class="ltx_Math" alttext="\{M,T_{M^{\prime}}\}\in\{\{I,T_{I}\},\{V,T_{V}\},\{V,T_{\textit{VAS}}\},\{\textit{VA},T_{\textit{VAS}}\}\}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.10a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.10.10" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.2.cmml">{</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">M</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.2.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.2.cmml">T</mi><msup id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.3.cmml">′</mo></msup></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.6" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.6.cmml">∈</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.5" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml">{</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.2.cmml">{</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">I</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.2.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.3.cmml">I</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.6" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.2.cmml">{</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">V</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.2.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.3.cmml">V</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.7" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.2.cmml">{</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml">V</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.2.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.2.cmml">T</mi><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3a.cmml">VAS</mtext></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.2.cmml">}</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.8" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.2.cmml">{</mo><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px1.p1.1.m1.5.5" xref="S3.SS2.SSS0.Px1.p1.1.m1.5.5a.cmml">VA</mtext><mo id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.2.cmml">,</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.2.cmml">T</mi><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3a.cmml">VAS</mtext></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.2.cmml">}</mo></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.9" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.10b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10"><in id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.6.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.6"></in><set id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑀</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.2">𝑇</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.2">𝑀</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.6.6.1.1.1.3.3">′</ci></apply></apply></set><set id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.5.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4"><set id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2">𝐼</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.7.7.2.1.1.1.1.3">𝐼</ci></apply></set><set id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3">𝑉</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.8.8.3.2.2.1.1.3">𝑉</ci></apply></set><set id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4">𝑉</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.9.9.4.3.3.1.1.3">VAS</mtext></ci></apply></set><set id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.5.5a.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.5.5"><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px1.p1.1.m1.5.5.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.5.5">VA</mtext></ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.10.10.5.4.4.1.1.3">VAS</mtext></ci></apply></set></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.10c">\{M,T_{M^{\prime}}\}\in\{\{I,T_{I}\},\{V,T_{V}\},\{V,T_{\textit{VAS}}\},\{\textit{VA},T_{\textit{VAS}}\}\}</annotation></semantics></math> in this training (the combinations denote the concatenated features from the used modalities).</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Unmasked Visual-Audio-Language Post-Pretraining.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We freeze the vision encoder to achieve joint alignment of audio, visual, and text features. We conduct post-pretraining using a small subset of image and video data (25M), along with the complete set of audio (0.5M) and audio-video data (50M). Given that the parameters of the largest ViT-6B model are frozen, we abandon masking strategies here for the consistency with the inference, minimizing the degradations on downstream performance brought by masking. Here <math id="S3.SS2.SSS0.Px2.p1.1.m1.6" class="ltx_math_unparsed" alttext="\{M,T_{M^{\prime}}\}\in\{\{I,T_{I}\},\{V,T_{V}\},\{A,T_{A}\},\{V,T_{\textit{VAS}}\},\{\textit{VA},T_{\textit{VA}}\}\}\}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.6a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6b"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.7"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.1">{</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1">M</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.3.2">T</mi><msup id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.3.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.3.3.2">M</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.3.3.3">′</mo></msup></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.7.4">}</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.8">∈</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.1">{</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.1">{</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.2.2">I</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.3.2">T</mi><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.3.3">I</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.2.4">}</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.3">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.1">{</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.3.3">V</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.3.2">T</mi><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.3.3">V</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.4.4">}</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.5">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.1">{</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.4.4">A</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.3.2">T</mi><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.3.3">A</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.6.4">}</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.7">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.1">{</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.5.5">V</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.3.2">T</mi><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.3.3">VAS</mtext></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.8.4">}</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.9">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.1">{</mo><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px2.p1.1.m1.6.6">VA</mtext><mo id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.2">,</mo><msub id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.3"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.3.2">T</mi><mtext class="ltx_mathvariant_italic" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.3.3">VA</mtext></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.10.4">}</mo></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.9.11">}</mo></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.6.10">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.6c">\{M,T_{M^{\prime}}\}\in\{\{I,T_{I}\},\{V,T_{V}\},\{A,T_{A}\},\{V,T_{\textit{VAS}}\},\{\textit{VA},T_{\textit{VA}}\}\}\}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Stage3: Predicting Next Token with Video-Centric Inputs</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">To further enrich the semantics embedded in <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> and improve its user-friendly in human communications, we tune it by incorporating it into a VideoLLM with a LLM and a video BLIP <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib69" title="" class="ltx_ref">2022b</a>, <a href="#bib.bib70" title="" class="ltx_ref">c</a>)</cite>. We employ the progressive learning scheme in <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite> by using <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> as the video encoder and train a video blip for communicating with open-sourced LLM <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>); Jiang et al. (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite>. In training, the video encoder will be updated. Detailed training recipts are in <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of datasets used in <span id="S3.T1.2.1" class="ltx_text ltx_font_bold">InternVideo2</span> pretraining process.</figcaption>
<div id="S3.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:312.2pt;height:103.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.9pt,20.4pt) scale(0.716138271490517,0.716138271490517) ;">
<table id="S3.T1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<td id="S3.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Pretraining Stage</td>
<td id="S3.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S3.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Domain</td>
<td id="S3.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"># of clips</td>
<td id="S3.T1.3.1.1.5" class="ltx_td ltx_align_left ltx_border_tt">Annotation</td>
</tr>
<tr id="S3.T1.3.1.2" class="ltx_tr">
<td id="S3.T1.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Stage 1</td>
<td id="S3.T1.3.1.2.2" class="ltx_td ltx_align_left ltx_border_t">KMash</td>
<td id="S3.T1.3.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Web Video</td>
<td id="S3.T1.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">2M</td>
<td id="S3.T1.3.1.2.5" class="ltx_td ltx_align_left ltx_border_t">Action Label</td>
</tr>
<tr id="S3.T1.3.1.3" class="ltx_tr">
<td id="S3.T1.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Stage 2 (img-txt)</td>
<td id="S3.T1.3.1.3.2" class="ltx_td ltx_align_left ltx_border_t">LAION, etc</td>
<td id="S3.T1.3.1.3.3" class="ltx_td ltx_align_left ltx_border_t">Web Image</td>
<td id="S3.T1.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t">300M</td>
<td id="S3.T1.3.1.3.5" class="ltx_td ltx_align_left ltx_border_t">Alt-text / Generated Caps</td>
</tr>
<tr id="S3.T1.3.1.4" class="ltx_tr">
<td id="S3.T1.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S3.T1.3.1.4.1.1" class="ltx_text">Stage 2 (vid-txt)</span></td>
<td id="S3.T1.3.1.4.2" class="ltx_td ltx_align_left ltx_border_t">WebVid2M</td>
<td id="S3.T1.3.1.4.3" class="ltx_td ltx_align_left ltx_border_t">Web Video</td>
<td id="S3.T1.3.1.4.4" class="ltx_td ltx_align_center ltx_border_t">250k</td>
<td id="S3.T1.3.1.4.5" class="ltx_td ltx_align_left ltx_border_t">Alt-text</td>
</tr>
<tr id="S3.T1.3.1.5" class="ltx_tr">
<td id="S3.T1.3.1.5.1" class="ltx_td ltx_align_left">WebVid10M</td>
<td id="S3.T1.3.1.5.2" class="ltx_td ltx_align_left">Web Video</td>
<td id="S3.T1.3.1.5.3" class="ltx_td ltx_align_center">9.7M</td>
<td id="S3.T1.3.1.5.4" class="ltx_td ltx_align_left">Alt-text</td>
</tr>
<tr id="S3.T1.3.1.6" class="ltx_tr">
<td id="S3.T1.3.1.6.1" class="ltx_td ltx_align_left">InternVid</td>
<td id="S3.T1.3.1.6.2" class="ltx_td ltx_align_left">Youtube Video</td>
<td id="S3.T1.3.1.6.3" class="ltx_td ltx_align_center">40M</td>
<td id="S3.T1.3.1.6.4" class="ltx_td ltx_align_left">Generated Caption</td>
</tr>
<tr id="S3.T1.3.1.7" class="ltx_tr">
<td id="S3.T1.3.1.7.1" class="ltx_td ltx_align_left">Self-collected</td>
<td id="S3.T1.3.1.7.2" class="ltx_td ltx_align_left">Youtube Video</td>
<td id="S3.T1.3.1.7.3" class="ltx_td ltx_align_center">60M</td>
<td id="S3.T1.3.1.7.4" class="ltx_td ltx_align_left">Generated Caption</td>
</tr>
<tr id="S3.T1.3.1.8" class="ltx_tr">
<td id="S3.T1.3.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Stage 3</td>
<td id="S3.T1.3.1.8.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">LLaVA, etc</td>
<td id="S3.T1.3.1.8.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Web Image/Video</td>
<td id="S3.T1.3.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.1M</td>
<td id="S3.T1.3.1.8.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Conversation, QA</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Multimodal Video Data</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We prepare our training data (video-only, video multimodal versions, and visual instructions) as given in Tab. <a href="#S3.T1" title="Table 1 ‣ 3.3 Stage3: Predicting Next Token with Video-Centric Inputs ‣ 3 Method ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Video-only Data for Masked AutoEncoders</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We curate a video collection without labels and text named <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">K-Mash</span> from
renowned action recognition datasets <cite class="ltx_cite ltx_citemacro_cite">Carreira and Zisserman (<a href="#bib.bib73" title="" class="ltx_ref">2017</a>); Goyal et al. (<a href="#bib.bib74" title="" class="ltx_ref">2017b</a>); Monfort et al. (<a href="#bib.bib75" title="" class="ltx_ref">2020</a>); Heilbron et al. (<a href="#bib.bib76" title="" class="ltx_ref">2015</a>); Zhao et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>, as detailed in Supp. It encompasses a wide range of video types, including first- and third-person perspectives, with both short and long durations, and featuring various characters and settings. Further, we give K-Mash<sub id="S4.SS1.p1.1.2" class="ltx_sub"><span id="S4.SS1.p1.1.2.1" class="ltx_text ltx_font_italic">2M</span></sub> with additionally sourced and meticulously selected 844K videos from YouTube for diversity. We observe notable action related task performance improvements in finetuned settings using K-Mash, as given in Sec. <a href="#S5.SS4.SSS2" title="5.4.2 Training Data and used Teachers in Stage 1 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4.2</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.15377/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The framework of our video multimodal annotation system, called <span id="S4.F3.2.1" class="ltx_text ltx_font_bold">VidCap</span>, consists of four main components: video, audio, and speech captioners, along with a LLM for integrating captions from these modalities.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Videos with Audio-Speech Modalities for Contrastive Learning</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We build a multimodal video dataset <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">MVid</span> with video-audio-speech information and their descriptions for pushing video perception via other modalities. It consists of 110M videos along with their VAS captions.
In <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">MVid</span>, we collect videos from several sources (detailed in the supplementary material), segment them into clips, and automatically annotate them based on their solo unimodal or crossmodal inputs.
We highlight the importance of temporal segmentation in clip generation and our video mutimodal annotation system. We find refining them leads to notable downstream improvements.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temporal Consistency Matters.</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We employ a temporal boundary detection model AutoShot <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib78" title="" class="ltx_ref">2023c</a>)</cite> to segment videos into clips instead of SceneDet filter from FFMPEG. It predicts boundaries based on temporal semantic variations rather than pixel differences, leading to generating semantically complete cuts without mixing extra frames with inconsistent context.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Video Multimodal Annotation.</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">We design a video multimodal annotation system <span id="S4.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">VidCap</span> to give proper unimodal and crossmodal descriptions for textualizing videos from different perceptions. It automatically captions visual, audio, and speech of <span id="S4.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_bold">MVid</span>, then it corrects them and fuses them for cross-modal captions via LLM. The system frame is given in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1 Video-only Data for Masked AutoEncoders ‣ 4 Multimodal Video Data ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. <span id="S4.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_bold">VidCap</span> has independent video, audio, speech captioner, and a LLM for caption refinement and fusion. For video, speech, and caption postprocessing, we employ existing methods as the video captioning pipeline in <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023d</a>)</cite>, WhisperV2-large model <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib80" title="" class="ltx_ref">2023</a>)</cite>, and Vicuna-1.5 <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. For audio, we craft a audio captioner upon VideoChat <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib81" title="" class="ltx_ref">2023d</a>)</cite>, as we find no open-sourced one. It extracts audio features from inputs by Beats <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib67" title="" class="ltx_ref">2023d</a>)</cite>. We learn it by only tuning its Qformer using a combination of the large-scale audio-text corpus WavCaps <cite class="ltx_cite ltx_citemacro_cite">Mei et al. (<a href="#bib.bib82" title="" class="ltx_ref">2023</a>)</cite> dataset. Details are given in the supplementary material.</p>
</div>
</section>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Video Instruction-Tuning Data for Next Token Prediction</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We employ the training part of MVBench <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>. It comprises around 2M samples (both images and videos) from 34 distinct sources. This training dataset covers key features of image and video understanding across crucial tasks, including 1) conversation, 2) caption, 3) visual question anwser, 4) reasoning, and 5) classification.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In our evaluation of <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>, we assess the model performance of the three learning stages. The evaluation covers a wide range of tasks, including action recognition, video retrieval, question-answering, and more.
It includes various scenarios such as zero-shot learning, finetuning, and others.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.5" class="ltx_p">For <span id="S5.p2.5.6" class="ltx_text ltx_font_bold">InternVideo2</span> trained in stage 1, 2, and 3, we denote them with <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.p2.1.1.1" class="ltx_sub"><span id="S5.p2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>, <span id="S5.p2.2.2" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.p2.2.2.1" class="ltx_sub"><span id="S5.p2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>, and <span id="S5.p2.3.3" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.p2.3.3.1" class="ltx_sub"><span id="S5.p2.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s3</span></sub></span>, respectively. We also learn a CLIP-style <span id="S5.p2.5.7" class="ltx_text ltx_font_bold">InternVideo2</span> indicated by <span id="S5.p2.4.4" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.p2.4.4.1" class="ltx_sub"><span id="S5.p2.4.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">clip</span></sub></span>. It is post-pretrained from <span id="S5.p2.5.5" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.p2.5.5.1" class="ltx_sub"><span id="S5.p2.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span> by only preserving video and text encoders and contrastive loss.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">Each training stage of <span id="S5.p3.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>-6B utilizes different configurations. In the first stage, we employ 256 NVIDIA A100 GPUs and train the model for 18 days. The second stage also utilizes 256 A100 GPUs and spans a training period of 14 days. Finally, in the third stage, we use 64 A100 GPUs and train the model for 3 days. We introduce DeepSpeed and FlashAttention <cite class="ltx_cite ltx_citemacro_cite">Dao et al. (<a href="#bib.bib83" title="" class="ltx_ref">2022</a>)</cite> for training and inference.
Details and more results are given in the supp.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Video Classification</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Action Recognition</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">We test <span id="S5.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> on Kinetics (<span id="S5.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span>,
K400, 600 and 700 <cite class="ltx_cite ltx_citemacro_cite">Carreira and Zisserman (<a href="#bib.bib73" title="" class="ltx_ref">2017</a>); Carreira et al. (<a href="#bib.bib84" title="" class="ltx_ref">2018</a>, <a href="#bib.bib85" title="" class="ltx_ref">2019</a>)</cite>), Moments in Time V1 (MiT) <cite class="ltx_cite ltx_citemacro_cite">Monfort et al. (<a href="#bib.bib75" title="" class="ltx_ref">2020</a>)</cite>, Something-Something V2 (SSv2) <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib74" title="" class="ltx_ref">2017b</a>)</cite>, UCF <cite class="ltx_cite ltx_citemacro_cite">Soomro et al. (<a href="#bib.bib86" title="" class="ltx_ref">2012</a>)</cite>, HMDB <cite class="ltx_cite ltx_citemacro_cite">Kuehne et al. (<a href="#bib.bib87" title="" class="ltx_ref">2011</a>)</cite>, Charades <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib88" title="" class="ltx_ref">2017</a>)</cite>, ActivityNet <cite class="ltx_cite ltx_citemacro_cite">Heilbron et al. (<a href="#bib.bib76" title="" class="ltx_ref">2015</a>)</cite> (ANet) and HACS <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>. We evaluate in four settings:
<span id="S5.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_bold">(a) end-to-end finetuning</span> the whole backbone;
<span id="S5.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_bold">(b) attentive probing</span> is similar to linear pooling, but extra trains the attention pooling layer <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite>.
<span id="S5.SS1.SSS1.p1.1.5" class="ltx_text ltx_font_bold">(c) linear probing</span> which freezes the backbone and only trains the task head; and
<span id="S5.SS1.SSS1.p1.1.6" class="ltx_text ltx_font_bold">(d) zero-shot</span>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>End-to-end finetuning action recognition results (top-1 accuracy) on Kinetics, SomethingSomething, and Moments in Time. <sup id="S5.T2.23.1" class="ltx_sup"><span id="S5.T2.23.1.1" class="ltx_text ltx_font_italic">†</span></sup> denotes that the result is achieved with different resolutions or frame rates.</figcaption>
<div id="S5.T2.21" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:143.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-164.9pt,54.4pt) scale(0.567930723680135,0.567930723680135) ;">
<table id="S5.T2.21.19" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.21.19.20" class="ltx_tr">
<td id="S5.T2.21.19.20.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Method</td>
<td id="S5.T2.21.19.20.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Training Data</td>
<td id="S5.T2.21.19.20.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Setting</td>
<td id="S5.T2.21.19.20.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K400</td>
<td id="S5.T2.21.19.20.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K600</td>
<td id="S5.T2.21.19.20.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K700</td>
<td id="S5.T2.21.19.20.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">SthSthv2</td>
<td id="S5.T2.21.19.20.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">MiT</td>
<td id="S5.T2.21.19.20.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">ANet</td>
<td id="S5.T2.21.19.20.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">HACS</td>
</tr>
<tr id="S5.T2.3.1.1" class="ltx_tr">
<td id="S5.T2.3.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">CoVeR <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S5.T2.3.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">IV</span>-3B</td>
<td id="S5.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.3.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.1.1.1.m1.1a"><mo id="S5.T2.3.1.1.1.m1.1.1" xref="S5.T2.3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.1.1.m1.1b"><times id="S5.T2.3.1.1.1.m1.1.1.cmml" xref="S5.T2.3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.1.1.m1.1c">\times</annotation></semantics></math> 448</td>
<td id="S5.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">87.1</td>
<td id="S5.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">87.9</td>
<td id="S5.T2.3.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">79.8</td>
<td id="S5.T2.3.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">70.8</td>
<td id="S5.T2.3.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">46.1</td>
<td id="S5.T2.3.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.3.1.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.4.2.2" class="ltx_tr">
<td id="S5.T2.4.2.2.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">Hiera-H <cite class="ltx_cite ltx_citemacro_cite">Ryali et al. (<a href="#bib.bib90" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T2.4.2.2.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.4.2.2.3.1" class="ltx_text ltx_font_bold">V</span>-0.25M</td>
<td id="S5.T2.4.2.2.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.4.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.4.2.2.1.m1.1a"><mo id="S5.T2.4.2.2.1.m1.1.1" xref="S5.T2.4.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.2.1.m1.1b"><times id="S5.T2.4.2.2.1.m1.1.1.cmml" xref="S5.T2.4.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.2.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.4.2.2.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">87.8</td>
<td id="S5.T2.4.2.2.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">88.8</td>
<td id="S5.T2.4.2.2.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">81.1</td>
<td id="S5.T2.4.2.2.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">76.5</td>
<td id="S5.T2.4.2.2.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.4.2.2.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.4.2.2.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.5.3.3" class="ltx_tr">
<td id="S5.T2.5.3.3.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">CoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T2.5.3.3.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.5.3.3.3.1" class="ltx_text ltx_font_bold">I</span>-3B</td>
<td id="S5.T2.5.3.3.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.5.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.5.3.3.1.m1.1a"><mo id="S5.T2.5.3.3.1.m1.1.1" xref="S5.T2.5.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.3.3.1.m1.1b"><times id="S5.T2.5.3.3.1.m1.1.1.cmml" xref="S5.T2.5.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.3.3.1.m1.1c">\times</annotation></semantics></math> 576</td>
<td id="S5.T2.5.3.3.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">88.9</td>
<td id="S5.T2.5.3.3.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">89.4</td>
<td id="S5.T2.5.3.3.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">82.7</td>
<td id="S5.T2.5.3.3.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.5.3.3.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">49.0</td>
<td id="S5.T2.5.3.3.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.5.3.3.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.6.4.4" class="ltx_tr">
<td id="S5.T2.6.4.4.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">MTV-H <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T2.6.4.4.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.6.4.4.3.1" class="ltx_text ltx_font_bold">IV</span>-370M</td>
<td id="S5.T2.6.4.4.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">32 <math id="S5.T2.6.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.6.4.4.1.m1.1a"><mo id="S5.T2.6.4.4.1.m1.1.1" xref="S5.T2.6.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.4.4.1.m1.1b"><times id="S5.T2.6.4.4.1.m1.1.1.cmml" xref="S5.T2.6.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.4.4.1.m1.1c">\times</annotation></semantics></math> 280</td>
<td id="S5.T2.6.4.4.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">89.9</td>
<td id="S5.T2.6.4.4.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">90.3</td>
<td id="S5.T2.6.4.4.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">83.4</td>
<td id="S5.T2.6.4.4.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.6.4.4.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.6.4.4.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.6.4.4.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.8.6.6" class="ltx_tr">
<td id="S5.T2.8.6.6.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">VideoMAEv2-g <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T2.8.6.6.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.8.6.6.4.1" class="ltx_text ltx_font_bold">V</span>-1.35M</td>
<td id="S5.T2.7.5.5.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">64 <math id="S5.T2.7.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.7.5.5.1.m1.1a"><mo id="S5.T2.7.5.5.1.m1.1.1" xref="S5.T2.7.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.5.5.1.m1.1b"><times id="S5.T2.7.5.5.1.m1.1.1.cmml" xref="S5.T2.7.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.5.5.1.m1.1c">\times</annotation></semantics></math> 266</td>
<td id="S5.T2.8.6.6.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">90.0</td>
<td id="S5.T2.8.6.6.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">89.9</td>
<td id="S5.T2.8.6.6.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.8.6.6.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">77.0<sup id="S5.T2.8.6.6.2.1" class="ltx_sup"><span id="S5.T2.8.6.6.2.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S5.T2.8.6.6.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.8.6.6.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.8.6.6.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.9.7.7" class="ltx_tr">
<td id="S5.T2.9.7.7.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">V-JEPA-H <cite class="ltx_cite ltx_citemacro_cite">Bardes et al. (<a href="#bib.bib91" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T2.9.7.7.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.9.7.7.3.1" class="ltx_text ltx_font_bold">V</span>-2M</td>
<td id="S5.T2.9.7.7.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.9.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.9.7.7.1.m1.1a"><mo id="S5.T2.9.7.7.1.m1.1.1" xref="S5.T2.9.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.7.7.1.m1.1b"><times id="S5.T2.9.7.7.1.m1.1.1.cmml" xref="S5.T2.9.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.7.7.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.9.7.7.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.9.7.7.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.9.7.7.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.9.7.7.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">77.0</td>
<td id="S5.T2.9.7.7.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.9.7.7.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.9.7.7.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.10.8.8" class="ltx_tr">
<td id="S5.T2.10.8.8.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">MVD-H <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S5.T2.10.8.8.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.10.8.8.3.1" class="ltx_text ltx_font_bold">IV</span>-1.25M</td>
<td id="S5.T2.10.8.8.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.10.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.10.8.8.1.m1.1a"><mo id="S5.T2.10.8.8.1.m1.1.1" xref="S5.T2.10.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.8.8.1.m1.1b"><times id="S5.T2.10.8.8.1.m1.1.1.cmml" xref="S5.T2.10.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.8.8.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.10.8.8.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.10.8.8.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.10.8.8.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.10.8.8.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">77.3</td>
<td id="S5.T2.10.8.8.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.10.8.8.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.10.8.8.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.13.11.11" class="ltx_tr">
<td id="S5.T2.13.11.11.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">UniFormerV2-L <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S5.T2.13.11.11.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.13.11.11.5.1" class="ltx_text ltx_font_bold">IV</span>-401M</td>
<td id="S5.T2.11.9.9.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">64 <math id="S5.T2.11.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.11.9.9.1.m1.1a"><mo id="S5.T2.11.9.9.1.m1.1.1" xref="S5.T2.11.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.9.9.1.m1.1b"><times id="S5.T2.11.9.9.1.m1.1.1.cmml" xref="S5.T2.11.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.9.9.1.m1.1c">\times</annotation></semantics></math> 336</td>
<td id="S5.T2.13.11.11.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">90.0</td>
<td id="S5.T2.13.11.11.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">90.1</td>
<td id="S5.T2.13.11.11.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">82.7</td>
<td id="S5.T2.12.10.10.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">73.0<sup id="S5.T2.12.10.10.2.1" class="ltx_sup"><span id="S5.T2.12.10.10.2.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S5.T2.13.11.11.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">47.8<sup id="S5.T2.13.11.11.3.1" class="ltx_sup"><span id="S5.T2.13.11.11.3.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S5.T2.13.11.11.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">94.7</td>
<td id="S5.T2.13.11.11.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">95.4</td>
</tr>
<tr id="S5.T2.21.19.21" class="ltx_tr">
<td id="S5.T2.21.19.21.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.1.1" class="ltx_text" style="color:#808080;">InternVideo <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite></span></td>
<td id="S5.T2.21.19.21.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.21.19.21.2.1" class="ltx_text ltx_font_bold">V</span>-12M</td>
<td id="S5.T2.21.19.21.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.3.1" class="ltx_text ltx_font_italic" style="color:#808080;">ensemble</span></td>
<td id="S5.T2.21.19.21.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.4.1" class="ltx_text" style="color:#808080;">91.1</span></td>
<td id="S5.T2.21.19.21.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.5.1" class="ltx_text" style="color:#808080;">91.3</span></td>
<td id="S5.T2.21.19.21.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.6.1" class="ltx_text" style="color:#808080;">84.0</span></td>
<td id="S5.T2.21.19.21.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.21.7.1" class="ltx_text" style="color:#808080;">77.2</span></td>
<td id="S5.T2.21.19.21.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.21.19.21.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.21.19.21.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.15.13.13" class="ltx_tr">
<td id="S5.T2.14.12.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.14.12.12.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T2.14.12.12.1.1.1" class="ltx_sub"><span id="S5.T2.14.12.12.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-1B</td>
<td id="S5.T2.15.13.13.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.15.13.13.3.1" class="ltx_text ltx_font_bold">IV</span>-1.1M</td>
<td id="S5.T2.15.13.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">8 <math id="S5.T2.15.13.13.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.15.13.13.2.m1.1a"><mo id="S5.T2.15.13.13.2.m1.1.1" xref="S5.T2.15.13.13.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.13.13.2.m1.1b"><times id="S5.T2.15.13.13.2.m1.1.1.cmml" xref="S5.T2.15.13.13.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.13.13.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.15.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">91.3</td>
<td id="S5.T2.15.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">91.4</td>
<td id="S5.T2.15.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">85.0</td>
<td id="S5.T2.15.13.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">77.1</td>
<td id="S5.T2.15.13.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">50.8</td>
<td id="S5.T2.15.13.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.15.13.13.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.17.15.15" class="ltx_tr">
<td id="S5.T2.16.14.14.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.16.14.14.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T2.16.14.14.1.1.1" class="ltx_sub"><span id="S5.T2.16.14.14.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-1B</td>
<td id="S5.T2.17.15.15.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.17.15.15.3.1" class="ltx_text ltx_font_bold">IV</span>-1.1M</td>
<td id="S5.T2.17.15.15.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.17.15.15.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.17.15.15.2.m1.1a"><mo id="S5.T2.17.15.15.2.m1.1.1" xref="S5.T2.17.15.15.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.17.15.15.2.m1.1b"><times id="S5.T2.17.15.15.2.m1.1.1.cmml" xref="S5.T2.17.15.15.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.15.15.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.17.15.15.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">91.6</td>
<td id="S5.T2.17.15.15.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">91.6</td>
<td id="S5.T2.17.15.15.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">85.4</td>
<td id="S5.T2.17.15.15.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">77.1</td>
<td id="S5.T2.17.15.15.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">50.9</td>
<td id="S5.T2.17.15.15.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.17.15.15.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.19.17.17" class="ltx_tr">
<td id="S5.T2.18.16.16.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.18.16.16.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T2.18.16.16.1.1.1" class="ltx_sub"><span id="S5.T2.18.16.16.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-6B</td>
<td id="S5.T2.19.17.17.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.19.17.17.3.1" class="ltx_text ltx_font_bold">IV</span>-2M</td>
<td id="S5.T2.19.17.17.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">8 <math id="S5.T2.19.17.17.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.19.17.17.2.m1.1a"><mo id="S5.T2.19.17.17.2.m1.1.1" xref="S5.T2.19.17.17.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.19.17.17.2.m1.1b"><times id="S5.T2.19.17.17.2.m1.1.1.cmml" xref="S5.T2.19.17.17.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.17.17.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.19.17.17.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">91.9</td>
<td id="S5.T2.19.17.17.5" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">91.7</td>
<td id="S5.T2.19.17.17.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">85.7</td>
<td id="S5.T2.19.17.17.7" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.19.17.17.7.1" class="ltx_text ltx_font_bold">77.5</span></td>
<td id="S5.T2.19.17.17.8" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">51.0</td>
<td id="S5.T2.19.17.17.9" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T2.19.17.17.10" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T2.21.19.19" class="ltx_tr">
<td id="S5.T2.20.18.18.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.20.18.18.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T2.20.18.18.1.1.1" class="ltx_sub"><span id="S5.T2.20.18.18.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-6B</td>
<td id="S5.T2.21.19.19.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T2.21.19.19.3.1" class="ltx_text ltx_font_bold">IV</span>-2M</td>
<td id="S5.T2.21.19.19.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T2.21.19.19.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.21.19.19.2.m1.1a"><mo id="S5.T2.21.19.19.2.m1.1.1" xref="S5.T2.21.19.19.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.21.19.19.2.m1.1b"><times id="S5.T2.21.19.19.2.m1.1.1.cmml" xref="S5.T2.21.19.19.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.19.19.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T2.21.19.19.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.4.1" class="ltx_text ltx_font_bold">92.1</span></td>
<td id="S5.T2.21.19.19.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.5.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S5.T2.21.19.19.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.6.1" class="ltx_text ltx_font_bold">85.9</span></td>
<td id="S5.T2.21.19.19.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">77.4</td>
<td id="S5.T2.21.19.19.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.8.1" class="ltx_text ltx_font_bold">51.2</span></td>
<td id="S5.T2.21.19.19.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.9.1" class="ltx_text ltx_font_bold">95.9</span></td>
<td id="S5.T2.21.19.19.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T2.21.19.19.10.1" class="ltx_text ltx_font_bold">97.0</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">End-to-end Finetuning.</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px1.p1.1" class="ltx_p">Tab. <a href="#S5.T2" title="Table 2 ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows <span id="S5.SS1.SSS1.Px1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>-6B obtains new state-of-the-art (SOTA) results on Kinetics (<span id="S5.SS1.SSS1.Px1.p1.1.2" class="ltx_text ltx_font_bold">92.1%/91.9%/85.9%</span> on K400/600/700, respectively), SthSthv2, MiT, ANet, and HACS with only 16 frames, while the previous SOTAs require larger resolution (224 <span id="S5.SS1.SSS1.Px1.p1.1.3" class="ltx_text ltx_font_italic">vs.</span> 576) or model ensemble. As for MiT in Table <a href="#S5.T2" title="Table 2 ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<span id="S5.SS1.SSS1.Px1.p1.1.4" class="ltx_text ltx_font_bold">InternVideo2</span>-6B exceeds the previous SOTA, CoCa-g, by a significant margin of 2.2% (<span id="S5.SS1.SSS1.Px1.p1.1.5" class="ltx_text ltx_font_bold">51.2%</span> <span id="S5.SS1.SSS1.Px1.p1.1.6" class="ltx_text ltx_font_italic">vs.</span> 49.0%).
Regarding the temporal-related actions in Table <a href="#S5.T2" title="Table 2 ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
our <span id="S5.SS1.SSS1.Px1.p1.1.7" class="ltx_text ltx_font_bold">InternVideo2</span>-6B also surpasses MVD <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite> on SSv2 (<span id="S5.SS1.SSS1.Px1.p1.1.8" class="ltx_text ltx_font_bold">77.5%</span> <span id="S5.SS1.SSS1.Px1.p1.1.9" class="ltx_text ltx_font_italic">vs.</span> 77.3%).
Moreover, our <span id="S5.SS1.SSS1.Px1.p1.1.10" class="ltx_text ltx_font_bold">InternVideo2</span>-6B showcases top performance on untrimmed video analysis, as indicated in Table <a href="#S5.T2" title="Table 2 ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
with 95.9% on ActivityNet and 97.0% on HACS.
These results affirm our model’s superior capability for robustly identifying complex actions across varied scenes.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Attentive probing recognition results (top-1 accuracy) on Kinetics-400/600/700, Moments in Time and Something-Something V2.</figcaption>
<div id="S5.T3.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:117.6pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.2pt,40.5pt) scale(0.590735401164181,0.590735401164181) ;">
<table id="S5.T3.10.10" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.10.10.11" class="ltx_tr">
<td id="S5.T3.10.10.11.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Method</td>
<td id="S5.T3.10.10.11.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Training Data</td>
<td id="S5.T3.10.10.11.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Setting</td>
<td id="S5.T3.10.10.11.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K400</td>
<td id="S5.T3.10.10.11.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K600</td>
<td id="S5.T3.10.10.11.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K700</td>
<td id="S5.T3.10.10.11.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">MiT</td>
<td id="S5.T3.10.10.11.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">SSV2</td>
</tr>
<tr id="S5.T3.10.10.12" class="ltx_tr">
<td id="S5.T3.10.10.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">UMT-L</td>
<td id="S5.T3.10.10.12.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.10.10.12.2.1" class="ltx_text ltx_font_bold">IV</span>-25M</td>
<td id="S5.T3.10.10.12.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.12.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">82.8</td>
<td id="S5.T3.10.10.12.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.12.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.12.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">40.3</td>
<td id="S5.T3.10.10.12.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">54.5</td>
</tr>
<tr id="S5.T3.10.10.13" class="ltx_tr">
<td id="S5.T3.10.10.13.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">VideoMAEv2-g</td>
<td id="S5.T3.10.10.13.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.10.10.13.2.1" class="ltx_text ltx_font_bold">V</span>-1.35M</td>
<td id="S5.T3.10.10.13.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.13.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">82.1</td>
<td id="S5.T3.10.10.13.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.13.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.10.10.13.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">35.0</td>
<td id="S5.T3.10.10.13.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">56.1</td>
</tr>
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">V-JEPA-H <cite class="ltx_cite ltx_citemacro_cite">Bardes et al. (<a href="#bib.bib91" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">V</span>-2M</td>
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><times id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\times</annotation></semantics></math> 384</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">81.9</td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.1.1.1.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.1.1.1.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.1.1.1.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T3.1.1.1.8.1" class="ltx_text ltx_font_bold">72.2</span></td>
</tr>
<tr id="S5.T3.2.2.2" class="ltx_tr">
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">DINOv2-g <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib92" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T3.2.2.2.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">I</span>-142M</td>
<td id="S5.T3.2.2.2.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.2.2.2.1.m1.1a"><mo id="S5.T3.2.2.2.1.m1.1.1" xref="S5.T3.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.m1.1b"><times id="S5.T3.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T3.2.2.2.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">83.4</td>
<td id="S5.T3.2.2.2.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.2.2.2.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.2.2.2.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.2.2.2.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">50.0</td>
</tr>
<tr id="S5.T3.3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.3.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">VideoPrism-g <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.3.3.3.3.1" class="ltx_text ltx_font_bold">V</span>-619M</td>
<td id="S5.T3.3.3.3.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.3.3.3.1.m1.1a"><mo id="S5.T3.3.3.3.1.m1.1.1" xref="S5.T3.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.1.m1.1b"><times id="S5.T3.3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.1.m1.1c">\times</annotation></semantics></math> 288</td>
<td id="S5.T3.3.3.3.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">87.2</td>
<td id="S5.T3.3.3.3.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.3.3.3.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.3.3.3.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">45.5</td>
<td id="S5.T3.3.3.3.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">68.5</td>
</tr>
<tr id="S5.T3.4.4.4" class="ltx_tr">
<td id="S5.T3.4.4.4.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-e <cite class="ltx_cite ltx_citemacro_cite">Dehghani et al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T3.4.4.4.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.4.4.4.3.1" class="ltx_text ltx_font_bold">I</span>-4B</td>
<td id="S5.T3.4.4.4.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">128 <math id="S5.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.4.4.4.1.m1.1a"><mo id="S5.T3.4.4.4.1.m1.1.1" xref="S5.T3.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.1.m1.1b"><times id="S5.T3.4.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">86.5</td>
<td id="S5.T3.4.4.4.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.4.4.4.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.4.4.4.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">43.6</td>
<td id="S5.T3.4.4.4.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T3.5.5.5" class="ltx_tr">
<td id="S5.T3.5.5.5.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-22B <cite class="ltx_cite ltx_citemacro_cite">Dehghani et al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T3.5.5.5.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.5.5.5.3.1" class="ltx_text ltx_font_bold">I</span>-4B</td>
<td id="S5.T3.5.5.5.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">128 <math id="S5.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.5.5.5.1.m1.1a"><mo id="S5.T3.5.5.5.1.m1.1.1" xref="S5.T3.5.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.1.m1.1b"><times id="S5.T3.5.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T3.5.5.5.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">88.0</td>
<td id="S5.T3.5.5.5.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.5.5.5.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T3.5.5.5.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">44.9</td>
<td id="S5.T3.5.5.5.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T3.6.6.6" class="ltx_tr">
<td id="S5.T3.6.6.6.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">CoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T3.6.6.6.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.6.6.6.3.1" class="ltx_text ltx_font_bold">I</span>-3B</td>
<td id="S5.T3.6.6.6.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.6.6.6.1.m1.1a"><mo id="S5.T3.6.6.6.1.m1.1.1" xref="S5.T3.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.1.m1.1b"><times id="S5.T3.6.6.6.1.m1.1.1.cmml" xref="S5.T3.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.1.m1.1c">\times</annotation></semantics></math> 576</td>
<td id="S5.T3.6.6.6.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">88.0</td>
<td id="S5.T3.6.6.6.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">88.5</td>
<td id="S5.T3.6.6.6.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T3.6.6.6.6.1" class="ltx_text ltx_font_bold">81.1</span></td>
<td id="S5.T3.6.6.6.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">47.4</td>
<td id="S5.T3.6.6.6.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T3.8.8.8" class="ltx_tr">
<td id="S5.T3.7.7.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.7.7.7.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T3.7.7.7.1.1.1" class="ltx_sub"><span id="S5.T3.7.7.7.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="S5.T3.8.8.8.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.8.8.8.3.1" class="ltx_text ltx_font_bold">IV</span>-25.5M</td>
<td id="S5.T3.8.8.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.8.8.8.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.8.8.8.2.m1.1a"><mo id="S5.T3.8.8.8.2.m1.1.1" xref="S5.T3.8.8.8.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.2.m1.1b"><times id="S5.T3.8.8.8.2.m1.1.1.cmml" xref="S5.T3.8.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T3.8.8.8.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">87.9</td>
<td id="S5.T3.8.8.8.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">88.0</td>
<td id="S5.T3.8.8.8.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">79.5</td>
<td id="S5.T3.8.8.8.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">46.3</td>
<td id="S5.T3.8.8.8.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">67.3</td>
</tr>
<tr id="S5.T3.10.10.10" class="ltx_tr">
<td id="S5.T3.9.9.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.9.9.9.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T3.9.9.9.1.1.1" class="ltx_sub"><span id="S5.T3.9.9.9.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T3.10.10.10.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T3.10.10.10.3.1" class="ltx_text ltx_font_bold">IV</span>-400M</td>
<td id="S5.T3.10.10.10.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T3.10.10.10.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.10.10.10.2.m1.1a"><mo id="S5.T3.10.10.10.2.m1.1.1" xref="S5.T3.10.10.10.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.2.m1.1b"><times id="S5.T3.10.10.10.2.m1.1.1.cmml" xref="S5.T3.10.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T3.10.10.10.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T3.10.10.10.4.1" class="ltx_text ltx_font_bold">88.8</span></td>
<td id="S5.T3.10.10.10.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T3.10.10.10.5.1" class="ltx_text ltx_font_bold">89.1</span></td>
<td id="S5.T3.10.10.10.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">81.0</td>
<td id="S5.T3.10.10.10.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T3.10.10.10.7.1" class="ltx_text ltx_font_bold">47.8</span></td>
<td id="S5.T3.10.10.10.8" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">67.7</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attentive Probing.</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p1.1" class="ltx_p">As in Tab. <a href="#S5.T3" title="Table 3 ‣ End-to-end Finetuning. ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <span id="S5.SS1.SSS1.Px2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>-6B not only outperforms ViT-22B <cite class="ltx_cite ltx_citemacro_cite">Dehghani et al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite> and CoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite> in scene-focused datasets but also surpasses or matches the performance of the latest video foundation model <cite class="ltx_cite ltx_citemacro_cite">Bardes et al. (<a href="#bib.bib91" title="" class="ltx_ref">2024</a>); Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>, on datasets emphasizing temporal dynamics (SthSthV2).
This underscores our model’s exceptional ability to understand and interpret both spatial and temporal information effectively.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Linear probing action recognition results (top-1 accuracy) on Kinetics-400, Something-Something V2, UCF-101 and HMDB-51.</figcaption>
<div id="S5.T4.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:355.6pt;height:87.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.8pt,28.4pt) scale(0.605589385457729,0.605589385457729) ;">
<table id="S5.T4.10.10" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.10.10.11" class="ltx_tr">
<td id="S5.T4.10.10.11.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Method</td>
<td id="S5.T4.10.10.11.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Setting</td>
<td id="S5.T4.10.10.11.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K400</td>
<td id="S5.T4.10.10.11.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">SSV2</td>
<td id="S5.T4.10.10.11.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">UCF-101</td>
<td id="S5.T4.10.10.11.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">HMDB-51</td>
</tr>
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">VideoMAEv2-H <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">12 <math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><times id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">25.8</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">56.4</td>
<td id="S5.T4.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">34.1</td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">TVTSv2-H <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S5.T4.2.2.2.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">12 <math id="S5.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.2.2.2.1.m1.1a"><mo id="S5.T4.2.2.2.1.m1.1.1" xref="S5.T4.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><times id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">73.1</td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.8</td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">65.7</td>
</tr>
<tr id="S5.T4.3.3.3" class="ltx_tr">
<td id="S5.T4.3.3.3.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">OpenCLIP-G <cite class="ltx_cite ltx_citemacro_cite">Cherti et al. (<a href="#bib.bib94" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T4.3.3.3.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">8 <math id="S5.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.3.3.3.1.m1.1a"><mo id="S5.T4.3.3.3.1.m1.1.1" xref="S5.T4.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><times id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.3.3.3.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">78.3</td>
<td id="S5.T4.3.3.3.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">35.8</td>
<td id="S5.T4.3.3.3.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">90.7</td>
<td id="S5.T4.3.3.3.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T4.4.4.4" class="ltx_tr">
<td id="S5.T4.4.4.4.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">DINOv2-g <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib92" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T4.4.4.4.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">8 <math id="S5.T4.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.4.4.4.1.m1.1a"><mo id="S5.T4.4.4.4.1.m1.1.1" xref="S5.T4.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.1.m1.1b"><times id="S5.T4.4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.1.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.4.4.4.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">78.4</td>
<td id="S5.T4.4.4.4.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">38.3</td>
<td id="S5.T4.4.4.4.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.2</td>
<td id="S5.T4.4.4.4.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">-</td>
</tr>
<tr id="S5.T4.6.6.6" class="ltx_tr">
<td id="S5.T4.5.5.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T4.5.5.5.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T4.5.5.5.1.1.1" class="ltx_sub"><span id="S5.T4.5.5.5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-1B</td>
<td id="S5.T4.6.6.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.6.6.6.2.m1.1a"><mo id="S5.T4.6.6.6.2.m1.1.1" xref="S5.T4.6.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.2.m1.1b"><times id="S5.T4.6.6.6.2.m1.1.1.cmml" xref="S5.T4.6.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.6.6.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">81.6</td>
<td id="S5.T4.6.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">46.3</td>
<td id="S5.T4.6.6.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">96.0</td>
<td id="S5.T4.6.6.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">71.6</td>
</tr>
<tr id="S5.T4.8.8.8" class="ltx_tr">
<td id="S5.T4.7.7.7.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T4.7.7.7.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T4.7.7.7.1.1.1" class="ltx_sub"><span id="S5.T4.7.7.7.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-6B</td>
<td id="S5.T4.8.8.8.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T4.8.8.8.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.8.8.8.2.m1.1a"><mo id="S5.T4.8.8.8.2.m1.1.1" xref="S5.T4.8.8.8.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.8.2.m1.1b"><times id="S5.T4.8.8.8.2.m1.1.1.cmml" xref="S5.T4.8.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.8.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.8.8.8.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">82.0</td>
<td id="S5.T4.8.8.8.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">47.8</td>
<td id="S5.T4.8.8.8.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">96.3</td>
<td id="S5.T4.8.8.8.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">71.8</td>
</tr>
<tr id="S5.T4.10.10.10" class="ltx_tr">
<td id="S5.T4.9.9.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">
<span id="S5.T4.9.9.9.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T4.9.9.9.1.1.1" class="ltx_sub"><span id="S5.T4.9.9.9.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T4.10.10.10.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">16 <math id="S5.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.10.10.10.2.m1.1a"><mo id="S5.T4.10.10.10.2.m1.1.1" xref="S5.T4.10.10.10.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.10.10.10.2.m1.1b"><times id="S5.T4.10.10.10.2.m1.1.1.cmml" xref="S5.T4.10.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.10.10.2.m1.1c">\times</annotation></semantics></math> 224</td>
<td id="S5.T4.10.10.10.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T4.10.10.10.3.1" class="ltx_text ltx_font_bold">84.2</span></td>
<td id="S5.T4.10.10.10.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T4.10.10.10.4.1" class="ltx_text ltx_font_bold">56.7</span></td>
<td id="S5.T4.10.10.10.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T4.10.10.10.5.1" class="ltx_text ltx_font_bold">97.3</span></td>
<td id="S5.T4.10.10.10.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T4.10.10.10.6.1" class="ltx_text ltx_font_bold">80.7</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Linear Probing.</h5>

<div id="S5.SS1.SSS1.Px3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px3.p1.1" class="ltx_p">In Tab. <a href="#S5.T4" title="Table 4 ‣ Attentive Probing. ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <span id="S5.SS1.SSS1.Px3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>-1B significantly outperforms the previous SOTA, DINOv2-g <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib92" title="" class="ltx_ref">2023</a>)</cite>, by notable margins: +3.2% on K400, +8.0% on SthSthV2, and +4.8% on UCF-101. As we scale the model, an upward trend in results is observed, underscoring the benefits of model enhancement. Notably, the integration of multimodal pretraining (stage 2) yields further rise in results. We suppose stage 2 enhances feature discrimination.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Zero-shot action recognition results (top-1 acc) on UCF, HMDB, MiTv1, SSv2-MC, and Charades.</figcaption>
<div id="S5.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:338.2pt;height:78.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-104.9pt,24.1pt) scale(0.617108233967072,0.617108233967072) ;">
<table id="S5.T5.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.2.2.3" class="ltx_tr">
<td id="S5.T5.2.2.3.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Method</td>
<td id="S5.T5.2.2.3.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">#F</td>
<td id="S5.T5.2.2.3.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Training Data</td>
<td id="S5.T5.2.2.3.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">UCF</td>
<td id="S5.T5.2.2.3.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">HMDB</td>
<td id="S5.T5.2.2.3.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">MiT</td>
<td id="S5.T5.2.2.3.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">SSv2-MC</td>
<td id="S5.T5.2.2.3.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Charades</td>
</tr>
<tr id="S5.T5.2.2.4" class="ltx_tr">
<td id="S5.T5.2.2.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T5.2.2.4.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">12</td>
<td id="S5.T5.2.2.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.4.3.1" class="ltx_text ltx_font_bold">I</span>-400M</td>
<td id="S5.T5.2.2.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">68.9</td>
<td id="S5.T5.2.2.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">43.2</td>
<td id="S5.T5.2.2.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">29.6</td>
<td id="S5.T5.2.2.4.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
</tr>
<tr id="S5.T5.2.2.5" class="ltx_tr">
<td id="S5.T5.2.2.5.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">TVTSv2 <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S5.T5.2.2.5.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">12</td>
<td id="S5.T5.2.2.5.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.5.3.1" class="ltx_text ltx_font_bold">V</span>-8.5M</td>
<td id="S5.T5.2.2.5.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">78.0</td>
<td id="S5.T5.2.2.5.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">52.1</td>
<td id="S5.T5.2.2.5.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.5.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">48.4</td>
<td id="S5.T5.2.2.5.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
</tr>
<tr id="S5.T5.2.2.6" class="ltx_tr">
<td id="S5.T5.2.2.6.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">VideoCoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T5.2.2.6.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">16</td>
<td id="S5.T5.2.2.6.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.6.3.1" class="ltx_text ltx_font_bold">V</span>-145M</td>
<td id="S5.T5.2.2.6.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.6.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.6.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.6.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.6.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">25.8</td>
</tr>
<tr id="S5.T5.2.2.7" class="ltx_tr">
<td id="S5.T5.2.2.7.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">VideoPrism-g <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T5.2.2.7.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">16</td>
<td id="S5.T5.2.2.7.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.7.3.1" class="ltx_text ltx_font_bold">V</span>-619M</td>
<td id="S5.T5.2.2.7.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.7.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.7.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.7.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T5.2.2.7.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">32.4</td>
</tr>
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T5.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T5.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">clip</span></sub></span>-1B</td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">IV</span>-25.5M</td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">88.8</td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">53.9</td>
<td id="S5.T5.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">31.6</td>
<td id="S5.T5.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">61.5</td>
<td id="S5.T5.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">32.9</td>
</tr>
<tr id="S5.T5.2.2.2" class="ltx_tr">
<td id="S5.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T5.2.2.2.1.1.1" class="ltx_sub"><span id="S5.T5.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">clip</span></sub></span>-6B</td>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T5.2.2.2.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T5.2.2.2.3.1" class="ltx_text ltx_font_bold">IV</span>-400M</td>
<td id="S5.T5.2.2.2.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T5.2.2.2.4.1" class="ltx_text ltx_font_bold">89.5</span></td>
<td id="S5.T5.2.2.2.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T5.2.2.2.5.1" class="ltx_text ltx_font_bold">56.7</span></td>
<td id="S5.T5.2.2.2.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T5.2.2.2.6.1" class="ltx_text ltx_font_bold">32.9</span></td>
<td id="S5.T5.2.2.2.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T5.2.2.2.7.1" class="ltx_text ltx_font_bold">63.5</span></td>
<td id="S5.T5.2.2.2.8" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T5.2.2.2.8.1" class="ltx_text ltx_font_bold">34.6</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Zero-shot action recognition results on Kinetics. </figcaption>
<div id="S5.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:116.7pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-111.8pt,49.9pt) scale(0.537881929796441,0.537881929796441) ;">
<table id="S5.T6.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.2.2.3" class="ltx_tr">
<td id="S5.T6.2.2.3.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S5.T6.2.2.3.1.1" class="ltx_text">Method</span></td>
<td id="S5.T6.2.2.3.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S5.T6.2.2.3.2.1" class="ltx_text">#F</span></td>
<td id="S5.T6.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">K400</td>
<td id="S5.T6.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">K600</td>
<td id="S5.T6.2.2.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;" colspan="2">K700</td>
</tr>
<tr id="S5.T6.2.2.4" class="ltx_tr">
<td id="S5.T6.2.2.4.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">top-1</td>
<td id="S5.T6.2.2.4.2" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">AVG</td>
<td id="S5.T6.2.2.4.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">top-1</td>
<td id="S5.T6.2.2.4.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">AVG</td>
<td id="S5.T6.2.2.4.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">top-1</td>
<td id="S5.T6.2.2.4.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">AVG</td>
</tr>
<tr id="S5.T6.2.2.5" class="ltx_tr">
<td id="S5.T6.2.2.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T6.2.2.5.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T6.2.2.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">58.4</td>
<td id="S5.T6.2.2.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">70.1</td>
<td id="S5.T6.2.2.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">55.1</td>
<td id="S5.T6.2.2.5.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">67.2</td>
<td id="S5.T6.2.2.5.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">46.1</td>
<td id="S5.T6.2.2.5.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">58.4</td>
</tr>
<tr id="S5.T6.2.2.6" class="ltx_tr">
<td id="S5.T6.2.2.6.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">EVA-CLIP-L <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib96" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S5.T6.2.2.6.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">1</td>
<td id="S5.T6.2.2.6.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.6.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">65.0</td>
<td id="S5.T6.2.2.6.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.6.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">64.9</td>
<td id="S5.T6.2.2.6.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.6.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">59.1</td>
</tr>
<tr id="S5.T6.2.2.7" class="ltx_tr">
<td id="S5.T6.2.2.7.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">EVA-CLIP-E <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib96" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S5.T6.2.2.7.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">1</td>
<td id="S5.T6.2.2.7.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.7.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">69.8</td>
<td id="S5.T6.2.2.7.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.7.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">69.3</td>
<td id="S5.T6.2.2.7.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.7.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">63.4</td>
</tr>
<tr id="S5.T6.2.2.8" class="ltx_tr">
<td id="S5.T6.2.2.8.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">ViCLIP-L <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S5.T6.2.2.8.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T6.2.2.8.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">64.8</td>
<td id="S5.T6.2.2.8.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">75.7</td>
<td id="S5.T6.2.2.8.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">62.2</td>
<td id="S5.T6.2.2.8.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">73.5</td>
<td id="S5.T6.2.2.8.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">54.3</td>
<td id="S5.T6.2.2.8.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">66.4</td>
</tr>
<tr id="S5.T6.2.2.9" class="ltx_tr">
<td id="S5.T6.2.2.9.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">VideoCoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T6.2.2.9.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">16</td>
<td id="S5.T6.2.2.9.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">72.0</td>
<td id="S5.T6.2.2.9.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">81.3</td>
<td id="S5.T6.2.2.9.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.9.6" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T6.2.2.9.7" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T6.2.2.9.8" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr id="S5.T6.2.2.10" class="ltx_tr">
<td id="S5.T6.2.2.10.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">InternVL-6B <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T6.2.2.10.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T6.2.2.10.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">69.1</td>
<td id="S5.T6.2.2.10.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">79.4</td>
<td id="S5.T6.2.2.10.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">68.9</td>
<td id="S5.T6.2.2.10.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">78.8</td>
<td id="S5.T6.2.2.10.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">60.6</td>
<td id="S5.T6.2.2.10.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">71.5</td>
</tr>
<tr id="S5.T6.2.2.11" class="ltx_tr">
<td id="S5.T6.2.2.11.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">EVA-CLIP-18B <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib97" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T6.2.2.11.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">16</td>
<td id="S5.T6.2.2.11.3" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T6.2.2.11.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">79.4</td>
<td id="S5.T6.2.2.11.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.11.6" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">79.4</td>
<td id="S5.T6.2.2.11.7" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.11.8" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">72.2</td>
</tr>
<tr id="S5.T6.2.2.12" class="ltx_tr">
<td id="S5.T6.2.2.12.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">VideoPrism-g <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T6.2.2.12.2" class="ltx_td ltx_align_right" style="padding-left:2.8pt;padding-right:2.8pt;">16</td>
<td id="S5.T6.2.2.12.3" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.2.2.12.3.1" class="ltx_text ltx_font_bold">76.4</span></td>
<td id="S5.T6.2.2.12.4" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.2.2.12.4.1" class="ltx_text ltx_font_bold">85.4</span></td>
<td id="S5.T6.2.2.12.5" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">-</td>
<td id="S5.T6.2.2.12.6" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T6.2.2.12.7" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td id="S5.T6.2.2.12.8" class="ltx_td" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T6.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T6.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">clip</span></sub></span>-1B</td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">73.1</span></td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">82.4</span></td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">72.8</span></td>
<td id="S5.T6.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">81.8</span></td>
<td id="S5.T6.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">64.9</span></td>
<td id="S5.T6.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S5.T6.1.1.1.8.1" class="ltx_text ltx_font_bold">75.2</span></td>
</tr>
<tr id="S5.T6.2.2.2" class="ltx_tr">
<td id="S5.T6.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S5.T6.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T6.2.2.2.1.1.1" class="ltx_sub"><span id="S5.T6.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">clip</span></sub></span>-6B</td>
<td id="S5.T6.2.2.2.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">8</td>
<td id="S5.T6.2.2.2.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">72.7</td>
<td id="S5.T6.2.2.2.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">82.2</td>
<td id="S5.T6.2.2.2.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">71.7</td>
<td id="S5.T6.2.2.2.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">81.2</td>
<td id="S5.T6.2.2.2.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">64.2</td>
<td id="S5.T6.2.2.2.8" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">75.2</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Zero-shot.</h5>

<div id="S5.SS1.SSS1.Px4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px4.p1.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ Linear Probing. ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.T6" title="Table 6 ‣ Linear Probing. ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> show <span id="S5.SS1.SSS1.Px4.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> gets 72.7% / 71.7% / 64.2% on K400/600/700, respectively, outperforming others but VideoPrism on K400 (76.4%). On UCF <cite class="ltx_cite ltx_citemacro_cite">Soomro et al. (<a href="#bib.bib86" title="" class="ltx_ref">2012</a>)</cite>, HMDB <cite class="ltx_cite ltx_citemacro_cite">Kuehne et al. (<a href="#bib.bib87" title="" class="ltx_ref">2011</a>)</cite>, MiT <cite class="ltx_cite ltx_citemacro_cite">Monfort et al. (<a href="#bib.bib75" title="" class="ltx_ref">2020</a>)</cite>, SSv2-MC, and Charades, <span id="S5.SS1.SSS1.Px4.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> gives an cutting edges over others. The clear gap between VideoPrism and <span id="S5.SS1.SSS1.Px4.p1.1.3" class="ltx_text ltx_font_bold">InternVideo2</span> on K400 may signify the importance of pretraining corpus in VideoPrism (311M videos with text and 36.1M of them are manually labeled) for K400 in zero-shot. Note that on the datasets of Kinetics, UCF101 and HMDB51, which have a distribution closer to the pre-training dataset used in stage1, the performance of Internvideo2-6B is slightly inferior to that of Internvideo2-1B. We suppose this is caused by Internvideo2-6B uses a more abundant pretraining dataset in stage2, leading to the forgetting of pretraining data in stage1.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Finetuned temporal action localization results on THUMOS14 <cite class="ltx_cite ltx_citemacro_cite">Idrees et al. (<a href="#bib.bib98" title="" class="ltx_ref">2017</a>)</cite>, ActivityNet <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib99" title="" class="ltx_ref">2017</a>)</cite>, HACS Segment <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> and FineAction <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>. We report average mAP. “Flow” denotes the ensembling I3D flow feature. * denotes the result is achieved with Flow.</figcaption>
<div id="S5.T7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:75pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-105.3pt,25.8pt) scale(0.590337885003704,0.590337885003704) ;">
<table id="S5.T7.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.3.3.4" class="ltx_tr">
<td id="S5.T7.3.3.4.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Backbone</td>
<td id="S5.T7.3.3.4.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">THUMOS14</td>
<td id="S5.T7.3.3.4.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">HACS</td>
<td id="S5.T7.3.3.4.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">ActivityNet</td>
<td id="S5.T7.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">FineAction</td>
</tr>
<tr id="S5.T7.3.3.5" class="ltx_tr">
<td id="S5.T7.3.3.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">I3D <cite class="ltx_cite ltx_citemacro_cite">Carreira and Zisserman (<a href="#bib.bib73" title="" class="ltx_ref">2017</a>)</cite> + Flow</td>
<td id="S5.T7.3.3.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td id="S5.T7.3.3.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.3.3.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td id="S5.T7.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.3.3.6" class="ltx_tr">
<td id="S5.T7.3.3.6.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">R(2+1)D <cite class="ltx_cite ltx_citemacro_cite">Tran et al. (<a href="#bib.bib101" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T7.3.3.6.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">55.6</td>
<td id="S5.T7.3.3.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.3.3.6.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">36.6</td>
<td id="S5.T7.3.3.6.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">InternVideo</td>
<td id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">71.6<sup id="S5.T7.1.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T7.1.1.1.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.3</td>
<td id="S5.T7.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">39.0</td>
<td id="S5.T7.1.1.1.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">17.6</td>
</tr>
<tr id="S5.T7.3.3.7" class="ltx_tr">
<td id="S5.T7.3.3.7.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">VideoMAEv2-g <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T7.3.3.7.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.5</td>
<td id="S5.T7.3.3.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.3.3.7.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.3.3.7.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.2.2.2" class="ltx_tr">
<td id="S5.T7.2.2.2.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T7.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T7.2.2.2.1.1.1" class="ltx_sub"><span id="S5.T7.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-1B</td>
<td id="S5.T7.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.8</td>
<td id="S5.T7.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">42.4</td>
<td id="S5.T7.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">40.4</td>
<td id="S5.T7.2.2.2.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.2</td>
</tr>
<tr id="S5.T7.3.3.3" class="ltx_tr">
<td id="S5.T7.3.3.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S5.T7.3.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T7.3.3.3.1.1.1" class="ltx_sub"><span id="S5.T7.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>-6B</td>
<td id="S5.T7.3.3.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.3.3.3.2.1" class="ltx_text ltx_font_bold">72.0</span></td>
<td id="S5.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.3.3.3.3.1" class="ltx_text ltx_font_bold">43.3</span></td>
<td id="S5.T7.3.3.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.3.3.3.4.1" class="ltx_text ltx_font_bold">41.2</span></td>
<td id="S5.T7.3.3.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.3.3.3.5.1" class="ltx_text ltx_font_bold">27.7</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Temporal Action Localization</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">We evaluate models on four temporal action localization (TAL) datasets: THUMOS14 <cite class="ltx_cite ltx_citemacro_cite">Idrees et al. (<a href="#bib.bib98" title="" class="ltx_ref">2017</a>)</cite>, ActivityNet <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib99" title="" class="ltx_ref">2017</a>)</cite>, HACS Segment <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> and FineAction <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite> in a feature-based manner with finetuning. We employ output of the 7-th layer from <span id="S5.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> for inputs as the corresponding features without fusing anything else. ActionFormer <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib102" title="" class="ltx_ref">2022</a>)</cite> is used as the detection head. We report mean Average Precision(mAP) under multiple tIoU as in <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib103" title="" class="ltx_ref">2019</a>); Chen et al. (<a href="#bib.bib104" title="" class="ltx_ref">2022b</a>); Zhang et al. (<a href="#bib.bib102" title="" class="ltx_ref">2022</a>); Yang et al. (<a href="#bib.bib105" title="" class="ltx_ref">2023</a>)</cite>. In Table <a href="#S5.T7" title="Table 7 ‣ Zero-shot. ‣ 5.1.1 Action Recognition ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <span id="S5.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>-6B gets the highest mAP among all comparisons in all datasets, while <span id="S5.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_bold">InternVideo2</span>-1B nearly surpass other methods except in THUMOS14. We find <span id="S5.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_bold">InternVideo2</span>-6B almost consistently improves mAP with a notable margin from <span id="S5.SS1.SSS2.p1.1.5" class="ltx_text ltx_font_bold">InternVideo2</span>-1B except in FineAction. We suppose scaling model capacity without data refinement cannot nontrivially improve fine-grained discrimination abilities of models. Scaling detailed annotations in training may address this issue.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Video instance segmentation performance (mAP) on YouTube-VIS19 <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib106" title="" class="ltx_ref">2019</a>)</cite>. </figcaption>
<div id="S5.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:41.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.8pt,15.6pt) scale(0.565869398198928,0.565869398198928) ;">
<table id="S5.T8.1.1" class="ltx_tabular ltx_align_top">
<tr id="S5.T8.1.1.2" class="ltx_tr">
<td id="S5.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="S5.T8.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">Backbone</td>
<td id="S5.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">#Params</td>
<td id="S5.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">YouTubeVIS19</td>
</tr>
<tr id="S5.T8.1.1.3" class="ltx_tr">
<td id="S5.T8.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Mask2Former</td>
<td id="S5.T8.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">Swin-L (image) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib107" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S5.T8.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">219M</td>
<td id="S5.T8.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">60.3</td>
</tr>
<tr id="S5.T8.1.1.4" class="ltx_tr">
<td id="S5.T8.1.1.4.1" class="ltx_td ltx_align_left">Mask2Former</td>
<td id="S5.T8.1.1.4.2" class="ltx_td ltx_align_center">InternViT (image)</td>
<td id="S5.T8.1.1.4.3" class="ltx_td ltx_align_center">6B</td>
<td id="S5.T8.1.1.4.4" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="S5.T8.1.1.1" class="ltx_tr">
<td id="S5.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Mask2Former</td>
<td id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T8.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T8.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span></td>
<td id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">6B</td>
<td id="S5.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">64.2</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Video Instance Segmentation</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">We evaluate on the popular Video Instance Segmentation (VIS) dataset, Youtube-VIS 2019 <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib106" title="" class="ltx_ref">2019</a>)</cite>. Built upon Mask2Former <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib108" title="" class="ltx_ref">2021</a>)</cite>, we employ the video encoder of <span id="S5.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> as backbone with ViT-adapter <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib109" title="" class="ltx_ref">2022c</a>)</cite> for features. We also try InternViT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite> for comparisons. In Table <a href="#S5.T8" title="Table 8 ‣ 5.1.2 Temporal Action Localization ‣ 5.1 Video Classification ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <span id="S5.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> gets the highest mAP among all. It improves mAP with a nontrivial margin over InternViT and Swin-L. This validates its effectiveness in relatively fine-grained spatiotemporal perception.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Video-Language Tasks</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">We evaluate <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> on video retrieval, captioning, and multi-choice question-answersing (QA). The former two tasks are conducted by matching video representation and the candidate text ones using the text encoder in stage 2. The latter is tested by the VideoLLM learned in stage 3. We also test audio tasks.</p>
</div>
<figure id="S5.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Results of zero-shot video retrieval in both text-to-video (T2V) and video-to-text (V2T) on MSR-VTT, LSMDC, DiDeMo, MSVD, ActivityNet (ANet), and VATEX.</figcaption>
<div id="S5.T9.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:117.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.7pt,40.6pt) scale(0.58976527752534,0.58976527752534) ;">
<table id="S5.T9.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S5.T9.2.2.3" class="ltx_tr">
<td id="S5.T9.2.2.3.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" rowspan="2"><span id="S5.T9.2.2.3.1.1" class="ltx_text">Method</span></td>
<td id="S5.T9.2.2.3.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">MSR-VTT</td>
<td id="S5.T9.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">LSMDC</td>
<td id="S5.T9.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">DiDeMo</td>
<td id="S5.T9.2.2.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">MSVD</td>
<td id="S5.T9.2.2.3.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">ANet</td>
<td id="S5.T9.2.2.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">VATEX</td>
</tr>
<tr id="S5.T9.2.2.4" class="ltx_tr">
<td id="S5.T9.2.2.4.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T9.2.2.4.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T9.2.2.4.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T9.2.2.4.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T9.2.2.4.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T9.2.2.4.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T9.2.2.4.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
</tr>
<tr id="S5.T9.2.2.5" class="ltx_tr">
<td id="S5.T9.2.2.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T9.2.2.5.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">30.4</td>
<td id="S5.T9.2.2.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">24.2</td>
<td id="S5.T9.2.2.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">13.9</td>
<td id="S5.T9.2.2.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">11.9</td>
<td id="S5.T9.2.2.5.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">12.7</td>
<td id="S5.T9.2.2.5.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">18.7</td>
<td id="S5.T9.2.2.5.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">40.5</td>
<td id="S5.T9.2.2.5.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">57.2</td>
<td id="S5.T9.2.2.5.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">9.1</td>
<td id="S5.T9.2.2.5.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">13.2</td>
<td id="S5.T9.2.2.5.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.5.13" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T9.2.2.6" class="ltx_tr">
<td id="S5.T9.2.2.6.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP4Clip <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T9.2.2.6.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">32.0</td>
<td id="S5.T9.2.2.6.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">15.1</td>
<td id="S5.T9.2.2.6.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">38.5</td>
<td id="S5.T9.2.2.6.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.6.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T9.2.2.7" class="ltx_tr">
<td id="S5.T9.2.2.7.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">ViCLIP <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S5.T9.2.2.7.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">42.4</td>
<td id="S5.T9.2.2.7.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">41.3</td>
<td id="S5.T9.2.2.7.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">20.1</td>
<td id="S5.T9.2.2.7.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">16.9</td>
<td id="S5.T9.2.2.7.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">18.4</td>
<td id="S5.T9.2.2.7.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">27.9</td>
<td id="S5.T9.2.2.7.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.1</td>
<td id="S5.T9.2.2.7.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">75.1</td>
<td id="S5.T9.2.2.7.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">15.1</td>
<td id="S5.T9.2.2.7.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">24.0</td>
<td id="S5.T9.2.2.7.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.7.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T9.2.2.8" class="ltx_tr">
<td id="S5.T9.2.2.8.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">InternVideo-L <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T9.2.2.8.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">40.7</td>
<td id="S5.T9.2.2.8.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">39.6</td>
<td id="S5.T9.2.2.8.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">17.6</td>
<td id="S5.T9.2.2.8.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">13.2</td>
<td id="S5.T9.2.2.8.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">31.5</td>
<td id="S5.T9.2.2.8.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">33.5</td>
<td id="S5.T9.2.2.8.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">43.4</td>
<td id="S5.T9.2.2.8.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">67.6</td>
<td id="S5.T9.2.2.8.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">30.7</td>
<td id="S5.T9.2.2.8.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">31.4</td>
<td id="S5.T9.2.2.8.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.5</td>
<td id="S5.T9.2.2.8.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">69.5</td>
</tr>
<tr id="S5.T9.2.2.9" class="ltx_tr">
<td id="S5.T9.2.2.9.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">UMT-L <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T9.2.2.9.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">40.7</td>
<td id="S5.T9.2.2.9.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">37.1</td>
<td id="S5.T9.2.2.9.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">24.9</td>
<td id="S5.T9.2.2.9.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">21.9</td>
<td id="S5.T9.2.2.9.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">48.6</td>
<td id="S5.T9.2.2.9.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.9</td>
<td id="S5.T9.2.2.9.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.0</td>
<td id="S5.T9.2.2.9.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">74.5</td>
<td id="S5.T9.2.2.9.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">41.9</td>
<td id="S5.T9.2.2.9.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">39.4</td>
<td id="S5.T9.2.2.9.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.9.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T9.2.2.10" class="ltx_tr">
<td id="S5.T9.2.2.10.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VideoCoCa-g <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T9.2.2.10.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">34.4</td>
<td id="S5.T9.2.2.10.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">64.7</td>
<td id="S5.T9.2.2.10.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.10.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">34.5</td>
<td id="S5.T9.2.2.10.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">33.0</td>
<td id="S5.T9.2.2.10.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">53.2</td>
<td id="S5.T9.2.2.10.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">73.6</td>
</tr>
<tr id="S5.T9.2.2.11" class="ltx_tr">
<td id="S5.T9.2.2.11.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VideoPrism-g <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S5.T9.2.2.11.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">39.7</td>
<td id="S5.T9.2.2.11.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.11.3.1" class="ltx_text ltx_font_bold">71.0</span></td>
<td id="S5.T9.2.2.11.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T9.2.2.11.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">52.7</td>
<td id="S5.T9.2.2.11.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">50.3</td>
<td id="S5.T9.2.2.11.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">62.5</td>
<td id="S5.T9.2.2.11.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">77.1</td>
</tr>
<tr id="S5.T9.1.1.1" class="ltx_tr">
<td id="S5.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T9.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T9.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T9.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="S5.T9.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">51.9</td>
<td id="S5.T9.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">50.9</td>
<td id="S5.T9.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">32.0</td>
<td id="S5.T9.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">27.3</td>
<td id="S5.T9.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">57.0</td>
<td id="S5.T9.1.1.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">54.3</td>
<td id="S5.T9.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">58.1</td>
<td id="S5.T9.1.1.1.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">83.3</td>
<td id="S5.T9.1.1.1.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">60.4</td>
<td id="S5.T9.1.1.1.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">54.8</td>
<td id="S5.T9.1.1.1.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">70.4</td>
<td id="S5.T9.1.1.1.13" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">85.4</td>
</tr>
<tr id="S5.T9.2.2.2" class="ltx_tr">
<td id="S5.T9.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T9.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T9.2.2.2.1.1.1" class="ltx_sub"><span id="S5.T9.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T9.2.2.2.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.2.1" class="ltx_text ltx_font_bold">55.9</span></td>
<td id="S5.T9.2.2.2.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">53.7</td>
<td id="S5.T9.2.2.2.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.4.1" class="ltx_text ltx_font_bold">33.8</span></td>
<td id="S5.T9.2.2.2.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.5.1" class="ltx_text ltx_font_bold">30.1</span></td>
<td id="S5.T9.2.2.2.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.6.1" class="ltx_text ltx_font_bold">57.9</span></td>
<td id="S5.T9.2.2.2.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.7.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="S5.T9.2.2.2.8" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.8.1" class="ltx_text ltx_font_bold">59.3</span></td>
<td id="S5.T9.2.2.2.9" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.9.1" class="ltx_text ltx_font_bold">83.1</span></td>
<td id="S5.T9.2.2.2.10" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.10.1" class="ltx_text ltx_font_bold">63.2</span></td>
<td id="S5.T9.2.2.2.11" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.11.1" class="ltx_text ltx_font_bold">56.5</span></td>
<td id="S5.T9.2.2.2.12" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.12.1" class="ltx_text ltx_font_bold">71.5</span></td>
<td id="S5.T9.2.2.2.13" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T9.2.2.2.13.1" class="ltx_text ltx_font_bold">85.3</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Results of finetuning video retrieval in both text-to-video (T2V) and video-to-text (V2T) on MSR-VTT, LSMDC, DiDeMo, MSVD, ActivityNet (ANet), and VATEX. </figcaption>
<div id="S5.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:79.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-117.3pt,23.7pt) scale(0.624528861260845,0.624528861260845) ;">
<table id="S5.T10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T10.1.1.2" class="ltx_tr">
<td id="S5.T10.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" rowspan="2"><span id="S5.T10.1.1.2.1.1" class="ltx_text">Method</span></td>
<td id="S5.T10.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">MSR-VTT</td>
<td id="S5.T10.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">LSMDC</td>
<td id="S5.T10.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">DiDeMo</td>
<td id="S5.T10.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">MSVD</td>
<td id="S5.T10.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">ANet</td>
<td id="S5.T10.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">VATEX</td>
</tr>
<tr id="S5.T10.1.1.3" class="ltx_tr">
<td id="S5.T10.1.1.3.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T10.1.1.3.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T10.1.1.3.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T10.1.1.3.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T10.1.1.3.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
<td id="S5.T10.1.1.3.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">T2V</td>
<td id="S5.T10.1.1.3.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">V2T</td>
</tr>
<tr id="S5.T10.1.1.4" class="ltx_tr">
<td id="S5.T10.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T10.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">38.2</td>
<td id="S5.T10.1.1.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">38.7</td>
<td id="S5.T10.1.1.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">22.5</td>
<td id="S5.T10.1.1.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">22.6</td>
<td id="S5.T10.1.1.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">32.2</td>
<td id="S5.T10.1.1.4.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">33.9</td>
<td id="S5.T10.1.1.4.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.4.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.4.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">26.1</td>
<td id="S5.T10.1.1.4.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">26.9</td>
<td id="S5.T10.1.1.4.12" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.4.13" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T10.1.1.5" class="ltx_tr">
<td id="S5.T10.1.1.5.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP4Clip <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib110" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T10.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">45.6</td>
<td id="S5.T10.1.1.5.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">45.9</td>
<td id="S5.T10.1.1.5.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">24.3</td>
<td id="S5.T10.1.1.5.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">23.8</td>
<td id="S5.T10.1.1.5.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">43.0</td>
<td id="S5.T10.1.1.5.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">43.6</td>
<td id="S5.T10.1.1.5.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">45.2</td>
<td id="S5.T10.1.1.5.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">48.4</td>
<td id="S5.T10.1.1.5.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">40.3</td>
<td id="S5.T10.1.1.5.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">41.6</td>
<td id="S5.T10.1.1.5.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.5.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T10.1.1.6" class="ltx_tr">
<td id="S5.T10.1.1.6.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">ViCLIP <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib79" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S5.T10.1.1.6.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">52.5</td>
<td id="S5.T10.1.1.6.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">51.8</td>
<td id="S5.T10.1.1.6.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">33.0</td>
<td id="S5.T10.1.1.6.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">32.5</td>
<td id="S5.T10.1.1.6.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.4</td>
<td id="S5.T10.1.1.6.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">50.2</td>
<td id="S5.T10.1.1.6.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.6.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.6.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">49.8</td>
<td id="S5.T10.1.1.6.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">48.1</td>
<td id="S5.T10.1.1.6.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T10.1.1.6.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T10.1.1.7" class="ltx_tr">
<td id="S5.T10.1.1.7.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">UMT-L <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T10.1.1.7.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">58.8</td>
<td id="S5.T10.1.1.7.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">58.6</td>
<td id="S5.T10.1.1.7.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">43.0</td>
<td id="S5.T10.1.1.7.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">41.4</td>
<td id="S5.T10.1.1.7.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">70.4</td>
<td id="S5.T10.1.1.7.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">65.7</td>
<td id="S5.T10.1.1.7.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">58.2</td>
<td id="S5.T10.1.1.7.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">82.4</td>
<td id="S5.T10.1.1.7.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">66.8</td>
<td id="S5.T10.1.1.7.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">64.4</td>
<td id="S5.T10.1.1.7.12" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">72.0</td>
<td id="S5.T10.1.1.7.13" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">86.0</td>
</tr>
<tr id="S5.T10.1.1.1" class="ltx_tr">
<td id="S5.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T10.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T10.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T10.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.2.1" class="ltx_text ltx_font_bold">62.8</span></td>
<td id="S5.T10.1.1.1.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.3.1" class="ltx_text ltx_font_bold">60.2</span></td>
<td id="S5.T10.1.1.1.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.4.1" class="ltx_text ltx_font_bold">46.4</span></td>
<td id="S5.T10.1.1.1.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.5.1" class="ltx_text ltx_font_bold">46.7</span></td>
<td id="S5.T10.1.1.1.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.6.1" class="ltx_text ltx_font_bold">74.2</span></td>
<td id="S5.T10.1.1.1.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.7.1" class="ltx_text ltx_font_bold">71.9</span></td>
<td id="S5.T10.1.1.1.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.8.1" class="ltx_text ltx_font_bold">61.4</span></td>
<td id="S5.T10.1.1.1.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.9.1" class="ltx_text ltx_font_bold">85.2</span></td>
<td id="S5.T10.1.1.1.10" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.10.1" class="ltx_text ltx_font_bold">74.1</span></td>
<td id="S5.T10.1.1.1.11" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.11.1" class="ltx_text ltx_font_bold">69.7</span></td>
<td id="S5.T10.1.1.1.12" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.12.1" class="ltx_text ltx_font_bold">75.5</span></td>
<td id="S5.T10.1.1.1.13" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T10.1.1.1.13.1" class="ltx_text ltx_font_bold">89.3</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Video Retrieval</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">We evaluate the video retrieval on six popular benchmarks <cite class="ltx_cite ltx_citemacro_cite">Heilbron et al. (<a href="#bib.bib76" title="" class="ltx_ref">2015</a>); Xu et al. (<a href="#bib.bib59" title="" class="ltx_ref">2016</a>); Rohrbach et al. (<a href="#bib.bib111" title="" class="ltx_ref">2015</a>); Anne Hendricks et al. (<a href="#bib.bib112" title="" class="ltx_ref">2017</a>); Chen and Dolan (<a href="#bib.bib113" title="" class="ltx_ref">2011</a>)</cite>, as shown in Tab. <a href="#S5.T9" title="Table 9 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#S5.T10" title="Table 10 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. In eval, eight frames from the input videos are uniformly sampled. <span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">We report R@1 scores for both text-to-video (t2v) and video-to-text (v2t) tasks</span> in Tab. <a href="#S5.T9" title="Table 9 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#S5.T10" title="Table 10 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. R@5 and R@10 are given in Supp.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">Table <a href="#S5.T9" title="Table 9 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#S5.T10" title="Table 10 ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> demonstrate that <span id="S5.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> outperforms other state-of-the-arts with a notable margin in both t2v and v2t of all used datasets no matter in zero-shot or finetuned settings, except for the v2t of MSR-VTT, where VideoPrism gives the best result. This shows the video-language semantic alignment of transferrity of <span id="S5.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>.</p>
</div>
<figure id="S5.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>The CIDEr scores (<math id="S5.T11.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T11.2.m1.1b"><mo stretchy="false" id="S5.T11.2.m1.1.1" xref="S5.T11.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T11.2.m1.1c"><ci id="S5.T11.2.m1.1.1.cmml" xref="S5.T11.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.2.m1.1d">\uparrow</annotation></semantics></math>) of zero-shot video captioning on MSR-VTT, VATEX, MSVD, and ANet Captions. </figcaption>
<div id="S5.T11.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:108.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.9pt,36.1pt) scale(0.598816111729986,0.598816111729986) ;">
<table id="S5.T11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T11.3.1.2" class="ltx_tr">
<td id="S5.T11.3.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Method</td>
<td id="S5.T11.3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">MSR-VTT</td>
<td id="S5.T11.3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">VATEX</td>
<td id="S5.T11.3.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">MSVD</td>
<td id="S5.T11.3.1.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">ANet Cap</td>
</tr>
<tr id="S5.T11.3.1.3" class="ltx_tr">
<td id="S5.T11.3.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">BLIP2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib69" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S5.T11.3.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">19.0</td>
<td id="S5.T11.3.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">27.4</td>
<td id="S5.T11.3.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.4" class="ltx_tr">
<td id="S5.T11.3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Video-LLaMA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T11.3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">29.1</td>
<td id="S5.T11.3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.4.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">47.0</td>
<td id="S5.T11.3.1.4.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.5" class="ltx_tr">
<td id="S5.T11.3.1.5.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Student Captioning Model <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib115" title="" class="ltx_ref">2024b</a>)</cite>
</td>
<td id="S5.T11.3.1.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">31.5</td>
<td id="S5.T11.3.1.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">49.2</td>
<td id="S5.T11.3.1.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.6" class="ltx_tr">
<td id="S5.T11.3.1.6.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VideoCoCa <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T11.3.1.6.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">27.1</td>
<td id="S5.T11.3.1.6.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">22.8</td>
<td id="S5.T11.3.1.6.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.6.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">19.3</td>
</tr>
<tr id="S5.T11.3.1.7" class="ltx_tr">
<td id="S5.T11.3.1.7.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Flamingo-3B <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T11.3.1.7.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.7.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">40.1</td>
<td id="S5.T11.3.1.7.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.7.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.8" class="ltx_tr">
<td id="S5.T11.3.1.8.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Flamingo-9B <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T11.3.1.8.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.8.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">39.5</td>
<td id="S5.T11.3.1.8.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.8.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.9" class="ltx_tr">
<td id="S5.T11.3.1.9.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VideoPrism-B <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite> w/ PaLM-1B</td>
<td id="S5.T11.3.1.9.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">40.3</td>
<td id="S5.T11.3.1.9.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">24.2</td>
<td id="S5.T11.3.1.9.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.9.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.10" class="ltx_tr">
<td id="S5.T11.3.1.10.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VideoPrism-B <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib25" title="" class="ltx_ref">2024</a>)</cite> w/ PaLM-8B</td>
<td id="S5.T11.3.1.10.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">38.5</td>
<td id="S5.T11.3.1.10.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">31.7</td>
<td id="S5.T11.3.1.10.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T11.3.1.10.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T11.3.1.1" class="ltx_tr">
<td id="S5.T11.3.1.1.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T11.3.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T11.3.1.1.1.1.1" class="ltx_sub"><span id="S5.T11.3.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s3</span></sub></span>-1B</td>
<td id="S5.T11.3.1.1.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T11.3.1.1.2.1" class="ltx_text ltx_font_bold">43.5</span></td>
<td id="S5.T11.3.1.1.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T11.3.1.1.3.1" class="ltx_text ltx_font_bold">49.2</span></td>
<td id="S5.T11.3.1.1.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T11.3.1.1.4.1" class="ltx_text ltx_font_bold">93.1</span></td>
<td id="S5.T11.3.1.1.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T11.3.1.1.5.1" class="ltx_text ltx_font_bold">19.6</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>The top-1 accuracy of zero-shot video QA (multi-choice) on MSR-VTT, LSMDC, and EgoSchema. Finetuned results are marked in gray.</figcaption>
<div id="S5.T12.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:268.8pt;height:55.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.1pt,17.2pt) scale(0.617961897735765,0.617961897735765) ;">
<table id="S5.T12.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T12.1.1.2" class="ltx_tr">
<td id="S5.T12.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Method</td>
<td id="S5.T12.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">MSR-VTT</td>
<td id="S5.T12.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">LSMDC</td>
<td id="S5.T12.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">EgoSchema</td>
</tr>
<tr id="S5.T12.1.1.3" class="ltx_tr">
<td id="S5.T12.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib116" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T12.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T12.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T12.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">31.1</td>
</tr>
<tr id="S5.T12.1.1.4" class="ltx_tr">
<td id="S5.T12.1.1.4.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">VIOLET <cite class="ltx_cite ltx_citemacro_cite">Fu et al. (<a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S5.T12.1.1.4.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T12.1.1.4.2.1" class="ltx_text" style="color:#808080;">91.9</span></td>
<td id="S5.T12.1.1.4.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T12.1.1.4.3.1" class="ltx_text" style="color:#808080;">82.8</span></td>
<td id="S5.T12.1.1.4.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">19.9</td>
</tr>
<tr id="S5.T12.1.1.5" class="ltx_tr">
<td id="S5.T12.1.1.5.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">InternVideo</td>
<td id="S5.T12.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">93.4</td>
<td id="S5.T12.1.1.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T12.1.1.5.3.1" class="ltx_text ltx_font_bold">77.3</span></td>
<td id="S5.T12.1.1.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">32.1</td>
</tr>
<tr id="S5.T12.1.1.1" class="ltx_tr">
<td id="S5.T12.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T12.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T12.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T12.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T12.1.1.1.2.1" class="ltx_text ltx_font_bold">94.4</span></td>
<td id="S5.T12.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">76.9</td>
<td id="S5.T12.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T12.1.1.1.4.1" class="ltx_text ltx_font_bold">41.1</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Video Captioning</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">As shown in Tab. <a href="#S5.T11" title="Table 11 ‣ 5.2.1 Video Retrieval ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>,we evaluate zero-shot video captioning using <span id="S5.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> learned in stage 3 on MSR-VTT, VATEX, MSVD, and ANet Captions. <span id="S5.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> delivers superior CIDEr scores than VideoCoCa, Flamingo, and VideoPrism. Despite models scale, it may suggest a progressive learning scheme involving masked reconstruction, crossmodal contrastive learning, and next token prediction would present better video-to-text generation performance than those which only utilize some of them.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Multi-Choice Video Question Answering</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">We evaluate zero-shot multi-choice (MC) video QA using <span id="S5.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> in stage 2 on MSR-VTT, VATEX, and EgoSchema <cite class="ltx_cite ltx_citemacro_cite">Mangalam et al. (<a href="#bib.bib117" title="" class="ltx_ref">2023</a>)</cite>.
Note EgoSchema is a long-form egocentric video benchmark in the form of video QA. Tab. <a href="#S5.T12" title="Table 12 ‣ 5.2.1 Video Retrieval ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows <span id="S5.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> consistently improves MC accuracy compared with previous SOTAs except on LSMDC, where it gets a comparable result with InternVideo. Its zero-shot result on EgoSchema increases previous state-of-the-art by 9 points, suggesting its better generalization on long video understanding than others.</p>
</div>
<figure id="S5.T13" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>Finetuned temporal grounding on QVHighlight <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite> and Charade-STA <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib88" title="" class="ltx_ref">2017</a>)</cite>.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.T13.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:195.5pt;vertical-align:-195.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.0pt,0.0pt) scale(0.97745821830567,0.97745821830567) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T13.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span>QVHighlight</figcaption>
<table id="S5.T13.st1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T13.st1.2.3" class="ltx_tr">
<td id="S5.T13.st1.2.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">Feature</td>
<td id="S5.T13.st1.2.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">R1@0.5</td>
<td id="S5.T13.st1.2.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">R1@0.7</td>
<td id="S5.T13.st1.2.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">mAP</td>
<td id="S5.T13.st1.2.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">mAP</td>
<td id="S5.T13.st1.2.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">HiT@1</td>
</tr>
<tr id="S5.T13.st1.2.4" class="ltx_tr">
<td id="S5.T13.st1.2.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T13.st1.2.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">64.97</td>
<td id="S5.T13.st1.2.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">48.65</td>
<td id="S5.T13.st1.2.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">42.96</td>
<td id="S5.T13.st1.2.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">39.83</td>
<td id="S5.T13.st1.2.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">64.19</td>
</tr>
<tr id="S5.T13.st1.2.5" class="ltx_tr">
<td id="S5.T13.st1.2.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.5pt;padding-right:0.5pt;">CLIP+SlowFast <cite class="ltx_cite ltx_citemacro_cite">Feichtenhofer et al. (<a href="#bib.bib119" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T13.st1.2.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">65.43</td>
<td id="S5.T13.st1.2.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">48.38</td>
<td id="S5.T13.st1.2.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.5pt;padding-right:0.5pt;">42.86</td>
<td id="S5.T13.st1.2.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">40.33</td>
<td id="S5.T13.st1.2.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">66.21</td>
</tr>
<tr id="S5.T13.st1.1.1" class="ltx_tr">
<td id="S5.T13.st1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.5pt;padding-right:0.5pt;">
<span id="S5.T13.st1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T13.st1.1.1.1.1.1" class="ltx_sub"><span id="S5.T13.st1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="S5.T13.st1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.1.1.2.1" class="ltx_text ltx_framed ltx_framed_underline">70.00</span></td>
<td id="S5.T13.st1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.1.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">54.45</span></td>
<td id="S5.T13.st1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">47.02</span></td>
<td id="S5.T13.st1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.1.1.5.1" class="ltx_text ltx_framed ltx_framed_underline">42.36</span></td>
<td id="S5.T13.st1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.1.1.6.1" class="ltx_text ltx_framed ltx_framed_underline">69.74</span></td>
</tr>
<tr id="S5.T13.st1.2.2" class="ltx_tr">
<td id="S5.T13.st1.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;">
<span id="S5.T13.st1.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T13.st1.2.2.1.1.1" class="ltx_sub"><span id="S5.T13.st1.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T13.st1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.2.2.2.1" class="ltx_text ltx_font_bold">71.42</span></td>
<td id="S5.T13.st1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.2.2.3.1" class="ltx_text ltx_font_bold">56.45</span></td>
<td id="S5.T13.st1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.2.2.4.1" class="ltx_text ltx_font_bold">49.24</span></td>
<td id="S5.T13.st1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.2.2.5.1" class="ltx_text ltx_font_bold">42.90</span></td>
<td id="S5.T13.st1.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st1.2.2.6.1" class="ltx_text ltx_font_bold">72.00</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T13.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span>Charade-STA</figcaption>
<table id="S5.T13.st2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T13.st2.2.3" class="ltx_tr">
<td id="S5.T13.st2.2.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">Feature</td>
<td id="S5.T13.st2.2.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">R1@0.3</td>
<td id="S5.T13.st2.2.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">R1@0.5</td>
<td id="S5.T13.st2.2.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">R1@0.7</td>
<td id="S5.T13.st2.2.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.5pt;padding-right:0.5pt;">mIoU</td>
</tr>
<tr id="S5.T13.st2.2.4" class="ltx_tr">
<td id="S5.T13.st2.2.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib95" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="S5.T13.st2.2.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">65.62</td>
<td id="S5.T13.st2.2.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">52.77</td>
<td id="S5.T13.st2.2.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">30.16</td>
<td id="S5.T13.st2.2.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.5pt;padding-right:0.5pt;">45.85</td>
</tr>
<tr id="S5.T13.st2.2.5" class="ltx_tr">
<td id="S5.T13.st2.2.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.5pt;padding-right:0.5pt;">CLIP+SlowFast <cite class="ltx_cite ltx_citemacro_cite">Feichtenhofer et al. (<a href="#bib.bib119" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T13.st2.2.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">70.43</td>
<td id="S5.T13.st2.2.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">58.44</td>
<td id="S5.T13.st2.2.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">36.34</td>
<td id="S5.T13.st2.2.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;">50.13</td>
</tr>
<tr id="S5.T13.st2.1.1" class="ltx_tr">
<td id="S5.T13.st2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:0.5pt;padding-right:0.5pt;">
<span id="S5.T13.st2.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T13.st2.1.1.1.1.1" class="ltx_sub"><span id="S5.T13.st2.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="S5.T13.st2.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.1.1.2.1" class="ltx_text ltx_framed ltx_framed_underline">78.41</span></td>
<td id="S5.T13.st2.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.1.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">68.36</span></td>
<td id="S5.T13.st2.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.1.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">45.03</span></td>
<td id="S5.T13.st2.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.1.1.5.1" class="ltx_text ltx_framed ltx_framed_underline">57.12</span></td>
</tr>
<tr id="S5.T13.st2.2.2" class="ltx_tr">
<td id="S5.T13.st2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;">
<span id="S5.T13.st2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T13.st2.2.2.1.1.1" class="ltx_sub"><span id="S5.T13.st2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T13.st2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.2.2.2.1" class="ltx_text ltx_font_bold">79.70</span></td>
<td id="S5.T13.st2.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.2.2.3.1" class="ltx_text ltx_font_bold">70.03</span></td>
<td id="S5.T13.st2.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.2.2.4.1" class="ltx_text ltx_font_bold">48.95</span></td>
<td id="S5.T13.st2.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.5pt;padding-right:0.5pt;"><span id="S5.T13.st2.2.2.5.1" class="ltx_text ltx_font_bold">58.79</span></td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4 </span>Video Temporal Grounding</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">We evaluate <span id="S5.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> on two temporal grounding (VTG) datasets: Qvhighlight <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite> and Charade-STA <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib88" title="" class="ltx_ref">2017</a>)</cite>. The eval setting and used features are the same as in TAL.
We use CG-DETR <cite class="ltx_cite ltx_citemacro_cite">Moon et al. (<a href="#bib.bib120" title="" class="ltx_ref">2023</a>)</cite> as the grounding head.
We report R1@0.3, R1@0.5, R1@0.7, and mAP for moment retrieval as in <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>); Moon et al. (<a href="#bib.bib120" title="" class="ltx_ref">2023</a>); Lin et al. (<a href="#bib.bib121" title="" class="ltx_ref">2023</a>)</cite>. Highlight Detection is evaluated in terms of “Very Good” mAP and HiT@1.
In Table <a href="#S5.T13.st2" title="In Table 13 ‣ 5.2.3 Multi-Choice Video Question Answering ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>, <span id="S5.SS2.SSS4.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>-1B and <span id="S5.SS2.SSS4.p1.1.3" class="ltx_text ltx_font_bold">InternVideo2</span>-6B bring gradual performance improvements compared to CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib122" title="" class="ltx_ref">2021b</a>)</cite> and CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib122" title="" class="ltx_ref">2021b</a>)</cite>+Slowfast <cite class="ltx_cite ltx_citemacro_cite">Feichtenhofer et al. (<a href="#bib.bib119" title="" class="ltx_ref">2019</a>)</cite>. This suggests that a larger spatiotemporal model is more beneficial to short-term video semantic alignment capabilities.</p>
</div>
<figure id="S5.T14" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 14: </span>Audio retrieval results on AudioCaps <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib123" title="" class="ltx_ref">2019</a>)</cite>, Clothov1, and Clothov2 <cite class="ltx_cite ltx_citemacro_cite">Drossos et al. (<a href="#bib.bib124" title="" class="ltx_ref">2020</a>)</cite>. We report text-to-audio R@1 accuracy in zero-shot and finetuning settings. </figcaption>
<div id="S5.T14.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:66.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-125.4pt,21.1pt) scale(0.608847265019444,0.608847265019444) ;">
<table id="S5.T14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T14.1.1.2" class="ltx_tr">
<td id="S5.T14.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" rowspan="2"><span id="S5.T14.1.1.2.1.1" class="ltx_text">Method</span></td>
<td id="S5.T14.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="3">Zero-shot</td>
<td id="S5.T14.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="3">Finetuning</td>
</tr>
<tr id="S5.T14.1.1.3" class="ltx_tr">
<td id="S5.T14.1.1.3.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">AudioCaps</td>
<td id="S5.T14.1.1.3.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">ClothoV1</td>
<td id="S5.T14.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">ClothoV2</td>
<td id="S5.T14.1.1.3.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">AudioCaps</td>
<td id="S5.T14.1.1.3.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">ClothoV1</td>
<td id="S5.T14.1.1.3.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">ClothoV2</td>
</tr>
<tr id="S5.T14.1.1.4" class="ltx_tr">
<td id="S5.T14.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">VIP-ANT <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib125" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S5.T14.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">27.7</td>
<td id="S5.T14.1.1.4.3" class="ltx_td ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T14.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.4.6" class="ltx_td ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T14.1.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T14.1.1.5" class="ltx_tr">
<td id="S5.T14.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">VAST <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib43" title="" class="ltx_ref">2024a</a>)</cite>
</td>
<td id="S5.T14.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">52.0</td>
<td id="S5.T14.1.1.5.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">25.1</td>
<td id="S5.T14.1.1.5.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">26.9</td>
</tr>
<tr id="S5.T14.1.1.6" class="ltx_tr">
<td id="S5.T14.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">LanguageBind <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib126" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S5.T14.1.1.6.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.6.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">12.1</td>
<td id="S5.T14.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">12.1</td>
<td id="S5.T14.1.1.6.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.6.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T14.1.1.6.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T14.1.1.1" class="ltx_tr">
<td id="S5.T14.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T14.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T14.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T14.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T14.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.2.1" class="ltx_text ltx_font_bold">37.1</span></td>
<td id="S5.T14.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.3.1" class="ltx_text ltx_font_bold">17.4</span></td>
<td id="S5.T14.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.4.1" class="ltx_text ltx_font_bold">17.4</span></td>
<td id="S5.T14.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.5.1" class="ltx_text ltx_font_bold">55.2</span></td>
<td id="S5.T14.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.6.1" class="ltx_text ltx_font_bold">25.3</span></td>
<td id="S5.T14.1.1.1.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T14.1.1.1.7.1" class="ltx_text ltx_font_bold">27.2</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T15" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 15: </span>Results of AudioQA on ClothoAQA <cite class="ltx_cite ltx_citemacro_cite">Lipping et al. (<a href="#bib.bib127" title="" class="ltx_ref">2022</a>)</cite> and Audio-MusicAVQA (AMAVQA) <cite class="ltx_cite ltx_citemacro_cite">Behera et al. (<a href="#bib.bib128" title="" class="ltx_ref">2023</a>)</cite>, and audio classification on the ESC-50 <cite class="ltx_cite ltx_citemacro_cite">Piczak (<a href="#bib.bib129" title="" class="ltx_ref">2015</a>)</cite>. Both in the finetuning setting. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.T15.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:216.8pt;height:187.2pt;vertical-align:-187.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.3pt,0.0pt) scale(0.970168443221065,0.970168443221065) ;">
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T15.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span>ClothoAQA and AMAVQA</figcaption>
<table id="S5.T15.st1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T15.st1.1.2" class="ltx_tr">
<td id="S5.T15.st1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 2.0pt;">Backbone</td>
<td id="S5.T15.st1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;">ClothQA</td>
<td id="S5.T15.st1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;">AMAVQA</td>
</tr>
<tr id="S5.T15.st1.1.3" class="ltx_tr">
<td id="S5.T15.st1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">AquaNet <cite class="ltx_cite ltx_citemacro_cite">Lipping et al. (<a href="#bib.bib127" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S5.T15.st1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">14.78</td>
<td id="S5.T15.st1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">65.59</td>
</tr>
<tr id="S5.T15.st1.1.4" class="ltx_tr">
<td id="S5.T15.st1.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.0pt;">MWAFM <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib130" title="" class="ltx_ref">2023e</a>)</cite>
</td>
<td id="S5.T15.st1.1.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">22.24</td>
<td id="S5.T15.st1.1.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">67.54</td>
</tr>
<tr id="S5.T15.st1.1.1" class="ltx_tr">
<td id="S5.T15.st1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S5.T15.st1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T15.st1.1.1.1.1.1" class="ltx_sub"><span id="S5.T15.st1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span></td>
<td id="S5.T15.st1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;"><span id="S5.T15.st1.1.1.2.1" class="ltx_text ltx_font_bold">30.14</span></td>
<td id="S5.T15.st1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;"><span id="S5.T15.st1.1.1.3.1" class="ltx_text ltx_font_bold">80.51</span></td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.T15.2" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:216.8pt;height:164pt;vertical-align:-164.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.3pt,0.0pt) scale(0.970168443221065,0.970168443221065) ;">
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T15.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span>ESC-50</figcaption>
<table id="S5.T15.st2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T15.st2.1.2" class="ltx_tr">
<td id="S5.T15.st2.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding:0.5pt 2.0pt;">Method</td>
<td id="S5.T15.st2.1.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;">Top-1 Acc</td>
</tr>
<tr id="S5.T15.st2.1.3" class="ltx_tr">
<td id="S5.T15.st2.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;">AST <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S5.T15.st2.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">95.60</td>
</tr>
<tr id="S5.T15.st2.1.4" class="ltx_tr">
<td id="S5.T15.st2.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:0.5pt 2.0pt;">BEATs <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib67" title="" class="ltx_ref">2023d</a>)</cite>
</td>
<td id="S5.T15.st2.1.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">98.10</td>
</tr>
<tr id="S5.T15.st2.1.1" class="ltx_tr">
<td id="S5.T15.st2.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.5pt 2.0pt;"><span id="S5.T15.st2.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T15.st2.1.1.1.1.1" class="ltx_sub"><span id="S5.T15.st2.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span></td>
<td id="S5.T15.st2.1.1.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;"><span id="S5.T15.st2.1.1.2.1" class="ltx_text ltx_font_bold">98.60</span></td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.5 </span>Audio-related Tasks</h4>

<div id="S5.SS2.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p1.1" class="ltx_p">We evaluate <span id="S5.SS2.SSS5.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>’s audio and text encoders on audio tasks, including audio-text retrieval on AudioCaps <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib123" title="" class="ltx_ref">2019</a>)</cite>, Clothov1, and Clothov2 <cite class="ltx_cite ltx_citemacro_cite">Drossos et al. (<a href="#bib.bib124" title="" class="ltx_ref">2020</a>)</cite>; audioQA on ClothoAQA <cite class="ltx_cite ltx_citemacro_cite">Lipping et al. (<a href="#bib.bib127" title="" class="ltx_ref">2022</a>)</cite> and Audio-MusicAVQA <cite class="ltx_cite ltx_citemacro_cite">Behera et al. (<a href="#bib.bib128" title="" class="ltx_ref">2023</a>)</cite>; and audio classification on the ESC-50 <cite class="ltx_cite ltx_citemacro_cite">Piczak (<a href="#bib.bib129" title="" class="ltx_ref">2015</a>)</cite>. As shown in Tab. <a href="#S5.T14" title="Table 14 ‣ 5.2.4 Video Temporal Grounding ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>,  <a href="#S5.T15.st1" title="In Table 15 ‣ 5.2.4 Video Temporal Grounding ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15(a)</span></a>, and <a href="#S5.T15.st2" title="In Table 15 ‣ 5.2.4 Video Temporal Grounding ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15(b)</span></a>, our model achieves state-of-the-art performance on all downstream tasks. Considering the limited size of the used audio and text encoders, these audio-related results show crossmodal contrastive learning’s benefits are mutual to the used modalities. Audio and the corresponding text models also gain from this learning.</p>
</div>
<figure id="S5.T16" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 16: </span>
Video-centric dialogue evaluations on MVBench <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>, VideoChatGPT-Eval <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. (<a href="#bib.bib63" title="" class="ltx_ref">2023a</a>)</cite>, and MoVQA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2023b</a>)</cite>.
</figcaption>
<div id="S5.T16.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:103.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.0pt,29.5pt) scale(0.63542398267227,0.63542398267227) ;">
<table id="S5.T16.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T16.1.1.2" class="ltx_tr">
<td id="S5.T16.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T16.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.2.1" class="ltx_text ltx_font_bold">ViEncoder</span></td>
<td id="S5.T16.1.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.3.1" class="ltx_text ltx_font_bold">LLM</span></td>
<td id="S5.T16.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.4.1" class="ltx_text ltx_font_bold">MVBench</span></td>
<td id="S5.T16.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.5.1" class="ltx_text ltx_font_bold">VideoChatGPT</span></td>
<td id="S5.T16.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.2.6.1" class="ltx_text ltx_font_bold">MoVQA</span></td>
</tr>
<tr id="S5.T16.1.1.3" class="ltx_tr">
<td id="S5.T16.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib10" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S5.T16.1.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">GPT-4</td>
<td id="S5.T16.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">43.5</td>
<td id="S5.T16.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T16.1.1.4" class="ltx_tr">
<td id="S5.T16.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">Gemini Pro <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T16.1.1.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.4.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">37.7</td>
<td id="S5.T16.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.4.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T16.1.1.5" class="ltx_tr">
<td id="S5.T16.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S5.T16.1.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP-L</td>
<td id="S5.T16.1.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Vicuna-7B</td>
<td id="S5.T16.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">36.0</td>
<td id="S5.T16.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T16.1.1.6" class="ltx_tr">
<td id="S5.T16.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">mPLUG-Owl-V <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib116" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T16.1.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP-L</td>
<td id="S5.T16.1.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">LLaMA-7B</td>
<td id="S5.T16.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">29.7</td>
<td id="S5.T16.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S5.T16.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">24.8</td>
</tr>
<tr id="S5.T16.1.1.7" class="ltx_tr">
<td id="S5.T16.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">VideoChatGPT <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. (<a href="#bib.bib63" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T16.1.1.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">CLIP-L</td>
<td id="S5.T16.1.1.7.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">Vicuna-7B</td>
<td id="S5.T16.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">32.7</td>
<td id="S5.T16.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">2.38</td>
<td id="S5.T16.1.1.7.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">22.9</td>
</tr>
<tr id="S5.T16.1.1.8" class="ltx_tr">
<td id="S5.T16.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">VideoLLaMA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib133" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S5.T16.1.1.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">EVA-CLIP-g</td>
<td id="S5.T16.1.1.8.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">Vicuna-7B</td>
<td id="S5.T16.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">34.1</td>
<td id="S5.T16.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">1.98</td>
<td id="S5.T16.1.1.8.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr id="S5.T16.1.1.9" class="ltx_tr">
<td id="S5.T16.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.9.1.1" class="ltx_text ltx_font_bold">VideoChat2</span></td>
<td id="S5.T16.1.1.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">UMT-L</td>
<td id="S5.T16.1.1.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Vicuna-7B</td>
<td id="S5.T16.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">51.1</td>
<td id="S5.T16.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.9.5.1" class="ltx_text ltx_font_bold">2.98</span></td>
<td id="S5.T16.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">34.7</td>
</tr>
<tr id="S5.T16.1.1.1" class="ltx_tr">
<td id="S5.T16.1.1.1.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.1.2.1" class="ltx_text ltx_font_bold">VideoChat2</span></td>
<td id="S5.T16.1.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T16.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T16.1.1.1.1.1.1" class="ltx_sub"><span id="S5.T16.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s3</span></sub></span>-1B</td>
<td id="S5.T16.1.1.1.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">Mistral-7B</td>
<td id="S5.T16.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.1.4.1" class="ltx_text ltx_font_bold">60.9</span></td>
<td id="S5.T16.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.1.5.1" class="ltx_text ltx_framed ltx_framed_underline">2.67</span></td>
<td id="S5.T16.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T16.1.1.1.6.1" class="ltx_text ltx_font_bold">41.0</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T17" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 17: </span>Average top-1 accuracy of action recognition (K400, SSv2, and MiT) and video retrieval (MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, and VATEX in t2v) using zero-shot and finetuning settings. * denotes results are from <span id="S5.T17.2.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T17.2.1.1" class="ltx_sub"><span id="S5.T17.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>.</figcaption>
<div id="S5.T17.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:61pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,5.5pt) scale(0.846642868941457,0.846642868941457) ;">
<table id="S5.T17.7.5" class="ltx_tabular ltx_align_middle">
<tr id="S5.T17.7.5.6" class="ltx_tr">
<td id="S5.T17.7.5.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" rowspan="2"><span id="S5.T17.7.5.6.1.1" class="ltx_text">Model</span></td>
<td id="S5.T17.7.5.6.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2">Zero-shot</td>
<td id="S5.T17.7.5.6.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Finetuning</td>
</tr>
<tr id="S5.T17.7.5.7" class="ltx_tr">
<td id="S5.T17.7.5.7.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">Action Recognition</td>
<td id="S5.T17.7.5.7.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">Video Retrieval</td>
<td id="S5.T17.7.5.7.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;">Action Recognition</td>
</tr>
<tr id="S5.T17.3.1.1" class="ltx_tr">
<td id="S5.T17.3.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T17.3.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T17.3.1.1.1.1.1" class="ltx_sub"><span id="S5.T17.3.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="S5.T17.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">55.5</td>
<td id="S5.T17.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">55.0</td>
<td id="S5.T17.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">73.2*</td>
</tr>
<tr id="S5.T17.7.5.5" class="ltx_tr">
<td id="S5.T17.4.2.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T17.4.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="S5.T17.4.2.2.1.1.1" class="ltx_sub"><span id="S5.T17.4.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="S5.T17.5.3.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T17.5.3.3.2.1" class="ltx_text ltx_font_bold">56.9<sub id="S5.T17.5.3.3.2.1.1" class="ltx_sub"><span id="S5.T17.5.3.3.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">(+1.4)</span></sub></span></td>
<td id="S5.T17.6.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T17.6.4.4.3.1" class="ltx_text ltx_font_bold">56.9<sub id="S5.T17.6.4.4.3.1.1" class="ltx_sub"><span id="S5.T17.6.4.4.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">(+1.9)</span></sub></span></td>
<td id="S5.T17.7.5.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T17.7.5.5.4.1" class="ltx_text ltx_font_bold">73.6<sub id="S5.T17.7.5.5.4.1.1" class="ltx_sub"><span id="S5.T17.7.5.5.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">(+0.4)</span></sub></span>*</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T18" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 18: </span>Ablation of Stage1, conducted with finetuned action recognition on Kinetics, MiT, and SthSthv2. All models are tested with 8<math id="S5.T18.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T18.3.m1.1b"><mo id="S5.T18.3.m1.1.1" xref="S5.T18.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T18.3.m1.1c"><times id="S5.T18.3.m1.1.1.cmml" xref="S5.T18.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T18.3.m1.1d">\times</annotation></semantics></math>224<math id="S5.T18.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T18.4.m2.1b"><mo id="S5.T18.4.m2.1.1" xref="S5.T18.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T18.4.m2.1c"><times id="S5.T18.4.m2.1.1.cmml" xref="S5.T18.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T18.4.m2.1d">\times</annotation></semantics></math>224 input.</figcaption>
<div id="S5.T18.11" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:128.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,8.1pt) scale(0.887484844480786,0.887484844480786) ;">
<table id="S5.T18.11.7" class="ltx_tabular ltx_align_middle">
<tr id="S5.T18.11.7.8" class="ltx_tr">
<td id="S5.T18.11.7.8.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Model</td>
<td id="S5.T18.11.7.8.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Teacher</td>
<td id="S5.T18.11.7.8.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Data</td>
<td id="S5.T18.11.7.8.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K400</td>
<td id="S5.T18.11.7.8.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K600</td>
<td id="S5.T18.11.7.8.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">K700</td>
<td id="S5.T18.11.7.8.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">MiT</td>
<td id="S5.T18.11.7.8.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">SSv2</td>
<td id="S5.T18.11.7.8.9" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Avg</td>
</tr>
<tr id="S5.T18.11.7.9" class="ltx_tr">
<td id="S5.T18.11.7.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-L</td>
<td id="S5.T18.11.7.9.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">CLIP-L</td>
<td id="S5.T18.11.7.9.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">K710</td>
<td id="S5.T18.11.7.9.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">90.3</td>
<td id="S5.T18.11.7.9.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">90.4</td>
<td id="S5.T18.11.7.9.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">83.2</td>
<td id="S5.T18.11.7.9.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">48.0</td>
<td id="S5.T18.11.7.9.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">74.7</td>
<td id="S5.T18.11.7.9.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">77.3</td>
</tr>
<tr id="S5.T18.5.1.1" class="ltx_tr">
<td id="S5.T18.5.1.1.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-L</td>
<td id="S5.T18.5.1.1.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">CLIP-L</td>
<td id="S5.T18.5.1.1.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">K-Mash<sub id="S5.T18.5.1.1.1.1" class="ltx_sub"><span id="S5.T18.5.1.1.1.1.1" class="ltx_text ltx_font_italic">1.1M</span></sub>
</td>
<td id="S5.T18.5.1.1.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">90.5</td>
<td id="S5.T18.5.1.1.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">90.4</td>
<td id="S5.T18.5.1.1.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">83.4</td>
<td id="S5.T18.5.1.1.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">48.1</td>
<td id="S5.T18.5.1.1.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">74.7</td>
<td id="S5.T18.5.1.1.9" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">77.4</td>
</tr>
<tr id="S5.T18.11.7.10" class="ltx_tr">
<td id="S5.T18.11.7.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-1B</td>
<td id="S5.T18.11.7.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">InternVL-6B</td>
<td id="S5.T18.11.7.10.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">K710</td>
<td id="S5.T18.11.7.10.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">90.9</td>
<td id="S5.T18.11.7.10.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">91.0</td>
<td id="S5.T18.11.7.10.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">84.7</td>
<td id="S5.T18.11.7.10.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">49.8</td>
<td id="S5.T18.11.7.10.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">75.9</td>
<td id="S5.T18.11.7.10.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">78.5</td>
</tr>
<tr id="S5.T18.6.2.2" class="ltx_tr">
<td id="S5.T18.6.2.2.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-1B</td>
<td id="S5.T18.6.2.2.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">InternVL-6B</td>
<td id="S5.T18.6.2.2.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">K-Mash<sub id="S5.T18.6.2.2.1.1" class="ltx_sub"><span id="S5.T18.6.2.2.1.1.1" class="ltx_text ltx_font_italic">1.1M</span></sub>
</td>
<td id="S5.T18.6.2.2.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.4</td>
<td id="S5.T18.6.2.2.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.5</td>
<td id="S5.T18.6.2.2.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">85.1</td>
<td id="S5.T18.6.2.2.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">50.5</td>
<td id="S5.T18.6.2.2.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">76.5</td>
<td id="S5.T18.6.2.2.9" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">79.0</td>
</tr>
<tr id="S5.T18.8.4.4" class="ltx_tr">
<td id="S5.T18.8.4.4.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-1B</td>
<td id="S5.T18.7.3.3.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">InternVL-6B<math id="S5.T18.7.3.3.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S5.T18.7.3.3.1.m1.1a"><mo id="S5.T18.7.3.3.1.m1.1.1" xref="S5.T18.7.3.3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T18.7.3.3.1.m1.1b"><plus id="S5.T18.7.3.3.1.m1.1.1.cmml" xref="S5.T18.7.3.3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T18.7.3.3.1.m1.1c">+</annotation></semantics></math>VideoMAE-g</td>
<td id="S5.T18.8.4.4.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">K-Mash<sub id="S5.T18.8.4.4.2.1" class="ltx_sub"><span id="S5.T18.8.4.4.2.1.1" class="ltx_text ltx_font_italic">1.1M</span></sub>
</td>
<td id="S5.T18.8.4.4.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.3</td>
<td id="S5.T18.8.4.4.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.4</td>
<td id="S5.T18.8.4.4.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">85.0</td>
<td id="S5.T18.8.4.4.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">50.8</td>
<td id="S5.T18.8.4.4.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">77.1</td>
<td id="S5.T18.8.4.4.9" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">79.1</td>
</tr>
<tr id="S5.T18.9.5.5" class="ltx_tr">
<td id="S5.T18.9.5.5.2" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-1B</td>
<td id="S5.T18.9.5.5.3" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">InternVL-6B</td>
<td id="S5.T18.9.5.5.1" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">K-Mash<sub id="S5.T18.9.5.5.1.1" class="ltx_sub"><span id="S5.T18.9.5.5.1.1.1" class="ltx_text ltx_font_italic">2M</span></sub>
</td>
<td id="S5.T18.9.5.5.4" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.3</td>
<td id="S5.T18.9.5.5.5" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">91.5</td>
<td id="S5.T18.9.5.5.6" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">85.1</td>
<td id="S5.T18.9.5.5.7" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">50.6</td>
<td id="S5.T18.9.5.5.8" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">76.6</td>
<td id="S5.T18.9.5.5.9" class="ltx_td ltx_align_left" style="padding-left:8.5pt;padding-right:8.5pt;">79.0</td>
</tr>
<tr id="S5.T18.11.7.7" class="ltx_tr">
<td id="S5.T18.11.7.7.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">ViT-6B</td>
<td id="S5.T18.10.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">InternVL-6B<math id="S5.T18.10.6.6.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S5.T18.10.6.6.1.m1.1a"><mo id="S5.T18.10.6.6.1.m1.1.1" xref="S5.T18.10.6.6.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T18.10.6.6.1.m1.1b"><plus id="S5.T18.10.6.6.1.m1.1.1.cmml" xref="S5.T18.10.6.6.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T18.10.6.6.1.m1.1c">+</annotation></semantics></math>VideoMAE-g</td>
<td id="S5.T18.11.7.7.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">K-Mash<sub id="S5.T18.11.7.7.2.1" class="ltx_sub"><span id="S5.T18.11.7.7.2.1.1" class="ltx_text ltx_font_italic">2M</span></sub>
</td>
<td id="S5.T18.11.7.7.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S5.T18.11.7.7.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.5.1" class="ltx_text ltx_font_bold">91.7</span></td>
<td id="S5.T18.11.7.7.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.6.1" class="ltx_text ltx_font_bold">85.7</span></td>
<td id="S5.T18.11.7.7.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.7.1" class="ltx_text ltx_font_bold">51.0</span></td>
<td id="S5.T18.11.7.7.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.8.1" class="ltx_text ltx_font_bold">77.5</span></td>
<td id="S5.T18.11.7.7.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T18.11.7.7.9.1" class="ltx_text ltx_font_bold">79.6</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2403.15377/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.2.1" class="ltx_text ltx_font_bold">Temporal action recognition tasks.</span> In questions about an action before it happens, Gemini Pro and InternVideo2-Chat both describe accurately the action, while GPT-4V hallucinates.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2403.15377/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S5.F5.2.1" class="ltx_text ltx_font_bold">Confused action recognition.</span> The person in the video is performing a misleading action while holding a banana. Gemini Pro gives a wrong answer. GPT-4V identifies the misleading action but doesn’t give a correct answer. InternVideo2-Chat gives a correct answer.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2403.15377/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S5.F6.2.1" class="ltx_text ltx_font_bold">Video Object Temporal Recognition.</span> The person in this video takes out different letters in the order of time. Gemini Pro recognizes 4 letters, but the order is totally reversed; GPT-4V recognizes only 3 letters, and the result is mixed with wrong answers; InternVideo2-Chat has the fewest errors among them and the order is correct.</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2403.15377/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S5.F7.2.1" class="ltx_text ltx_font_bold">Event counting task.</span> Both GPT-4V and InternVideo2-Chat are able to correctly capture the times of actions and not be confused by redundant frames and other actions.</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2403.15377/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S5.F8.2.1" class="ltx_text ltx_font_bold">Unexpected Action Recognition tasks.</span> The model needs to recognize the magical parts of the video. Both Gemini Pro and InternVideo2-Chat can capture part of the transition in the video and infer the shooting technique of the video. GPT-4V recognizes the transition but fails to successfully explain the process of the transition.</figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2403.15377/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S5.F9.2.1" class="ltx_text ltx_font_bold">Visual Language Navigation Tasks.</span> GPT-4V and InternVideo2-Chat are able to understand the instruction and make decisions about next steps based on the content of the video, while Gemini Pro is subject to hallucination.</figcaption>
</figure>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Video-centric Dialogue and its Applications</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Tab. <a href="#S5.T16" title="Table 16 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows results on MVBench <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite>, VideoChatGPT-Eval <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. (<a href="#bib.bib63" title="" class="ltx_ref">2023a</a>)</cite>, and MoVQA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib132" title="" class="ltx_ref">2023b</a>)</cite> from VideoChat2 (a video-based large language model) <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023b</a>)</cite> with different backbones, compared with other MLLMs. Note VideoChat-<span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> outperforms other systems with a notable margin, not only on the average score but also in each subtask (in the Supp), except in VideoChatGPT-Eval. Considering these benchmarks not only involve perception but also reasoning, these results suggest <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> does embed knowledge to partially model the world, at least compared with other models.
It also validates the importance of learning a transferrable video representation for current video-related MLLM. We also give several qualitative evaluations with popular GPT-4V and GeminiPro, including dialogues on action sequence (Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), confused action (Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), temporal order understanding (Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), temporal event counting (Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, unexpected action reasoning (Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), and vision-language navigation (Figure <a href="#S5.F9" title="Figure 9 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Studies</h3>

<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1 </span>Scaling Video Encoder</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">Tab. <a href="#S5.T17" title="Table 17 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> gives <span id="S5.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>’s average performance on action recognition and video retrieval. It shows that scaling video encoder from 1B to 6B still leads to notable improvements in generalization of action recognition and video-retrieval by 1.4% and 1.9% (in zero-shot), respectively. Meanwhile, the increase in finetuned action recognition result is relatively marginal (0.4%) with the growth of model scale.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2 </span>Training Data and used Teachers in Stage 1</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS2.p1.2" class="ltx_p">In Tab. <a href="#S5.T18" title="Table 18 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, we examine the impact of distillation teachers and used dataset sizes on model performance.
<span id="S5.SS4.SSS2.p1.2.1" class="ltx_text ltx_font_italic">(a) Data Scale:</span>
Note that pretraining data scale for MAE should grow with the increasing model scale, otherwise the downstream performance would be saturated, such as K710 (0.66M videos) for ViT-L, K-Mash<sub id="S5.SS4.SSS2.p1.2.2" class="ltx_sub"><span id="S5.SS4.SSS2.p1.2.2.1" class="ltx_text ltx_font_italic">1.1M</span></sub> for ViT-1B, and K-Mash<sub id="S5.SS4.SSS2.p1.2.3" class="ltx_sub"><span id="S5.SS4.SSS2.p1.2.3.1" class="ltx_text ltx_font_italic">2M</span></sub> for ViT-6B.
<span id="S5.SS4.SSS2.p1.2.4" class="ltx_text ltx_font_italic">(b) Teacher:</span> Tab. <a href="#S5.T18" title="Table 18 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> reveals that the synergy between a multimodal teacher (e.g., CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib122" title="" class="ltx_ref">2021b</a>)</cite>) and a motion-aware teacher (e.g., MAE <cite class="ltx_cite ltx_citemacro_cite">Tong et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>) markedly boosts performance,
especially on SthSthV2. It highlights the importance of strategic teacher model selection in the distillation process.</p>
</div>
<figure id="S5.T19" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 19: </span>Ablation of Stage2. All models are tested with 8<math id="S5.T19.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T19.3.m1.1b"><mo id="S5.T19.3.m1.1.1" xref="S5.T19.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T19.3.m1.1c"><times id="S5.T19.3.m1.1.1.cmml" xref="S5.T19.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T19.3.m1.1d">\times</annotation></semantics></math>224<math id="S5.T19.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T19.4.m2.1b"><mo id="S5.T19.4.m2.1.1" xref="S5.T19.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T19.4.m2.1c"><times id="S5.T19.4.m2.1.1.cmml" xref="S5.T19.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T19.4.m2.1d">\times</annotation></semantics></math>224 input.
</figcaption>
<div id="S5.T19.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:338.2pt;height:89.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.2pt,9.3pt) scale(0.827784535382118,0.827784535382118) ;">
<table id="S5.T19.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T19.5.1.1" class="ltx_tr">
<td id="S5.T19.5.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Method</td>
<td id="S5.T19.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">MSR-VTT</td>
</tr>
<tr id="S5.T19.5.1.2" class="ltx_tr">
<td id="S5.T19.5.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">Baseline (video &amp; text encoders w/ video-text learning)</td>
<td id="S5.T19.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">24.7</td>
</tr>
<tr id="S5.T19.5.1.3" class="ltx_tr">
<td id="S5.T19.5.1.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">Baseline + audio encoder w/ audio-text learning</td>
<td id="S5.T19.5.1.3.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">24.0</td>
</tr>
<tr id="S5.T19.5.1.4" class="ltx_tr">
<td id="S5.T19.5.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">Baseline + speech encoder w/ video-speech-text learning</td>
<td id="S5.T19.5.1.4.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">24.9</td>
</tr>
<tr id="S5.T19.5.1.5" class="ltx_tr">
<td id="S5.T19.5.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">Baseline + audio encoder w/ video-audio-text learning</td>
<td id="S5.T19.5.1.5.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T19.5.1.5.2.1" class="ltx_text ltx_font_bold">27.8</span></td>
</tr>
<tr id="S5.T19.5.1.6" class="ltx_tr">
<td id="S5.T19.5.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">Baseline + audio &amp; speech encoders + video-audio-speech-text learning</td>
<td id="S5.T19.5.1.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">25.7</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T20" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 20: </span>Zero-shot t2v retrieval on MSR-VTT with different training captions.
</figcaption>
<div id="S5.T20.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:338.2pt;height:78pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.1pt,6.0pt) scale(0.86615968651641,0.86615968651641) ;">
<table id="S5.T20.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T20.1.1.1" class="ltx_tr">
<td id="S5.T20.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">SceneDet</td>
<td id="S5.T20.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">AutoShot</td>
<td id="S5.T20.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Video Cap</td>
<td id="S5.T20.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Audio Cap</td>
<td id="S5.T20.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Speech Cap</td>
<td id="S5.T20.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">MSR-VTT</td>
</tr>
<tr id="S5.T20.1.1.2" class="ltx_tr">
<td id="S5.T20.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.2.2" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.2.4" class="ltx_td ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.2.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">24.7</td>
</tr>
<tr id="S5.T20.1.1.3" class="ltx_tr">
<td id="S5.T20.1.1.3.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.3.2" class="ltx_td" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.3.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.3.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.3.5" class="ltx_td ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.3.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">26.6</td>
</tr>
<tr id="S5.T20.1.1.4" class="ltx_tr">
<td id="S5.T20.1.1.4.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.4.2" class="ltx_td" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.4.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.4.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.4.6" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">27.1</td>
</tr>
<tr id="S5.T20.1.1.5" class="ltx_tr">
<td id="S5.T20.1.1.5.1" class="ltx_td ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"></td>
<td id="S5.T20.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;">✓</td>
<td id="S5.T20.1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T20.1.1.5.6.1" class="ltx_text ltx_font_bold">34.8</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.3 </span>Training Arch, Method, and Data in Stage 2</h4>

<div id="S5.SS4.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS3.p1.1" class="ltx_p">We ablate the necessity of introducing an audio encoder in Stage 2. We employ ViT-B and Bert-B for video and text encoders, respectively. The used text are simple video captions. The baseline is conducting video-text contrastive and matching as well as masked language generation loss for training with only video and text encoders. Other settings including adding audio or speech encoder or them both, and how to update newly added encoders i.e., whether train them with only text encoder or both video and text encoders. Tab. <a href="#S5.T19" title="Table 19 ‣ 5.4.2 Training Data and used Teachers in Stage 1 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> shows that only introducing audio encoder and learn it along with both video and text encoders can best improve video-text retrieval performance. The speech encoder harms such effectiveness more or less.</p>
</div>
<div id="S5.SS4.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.SSS3.p2.1" class="ltx_p">Further, we verify the impact of video temporal segmentation and the used captions as text inputs in Stage 2 in Tab. <a href="#S5.T20" title="Table 20 ‣ 5.4.2 Training Data and used Teachers in Stage 1 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>.
We still use ViT-B and Bert-B for video-text training.
Tab. <a href="#S5.T20" title="Table 20 ‣ 5.4.2 Training Data and used Teachers in Stage 1 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> finds the fused text from video-audio-speech works best for retrieval tasks compard with others, rising zero-shot t2v R1 of MSR-VTT from 24.7 to 27.1. Moreover, using AutoShot instead of SceneDet notably improves t2v retrieval (increasing by nearly 7 points). It validates the effectiveness of the introduced video-text dataset and its annotation systems.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Discussion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We introduce a novel video foundation model called <span id="S6.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>, which achieves state-of-the-art performance across various video and audio tasks. In <span id="S6.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>, we combine masked video token reconstruction, video-audio-text contrastive learning, and next token prediction into a unified framework. Additionally, we create a new video-text dataset that incorporates video-audio-speech fused captions as descriptions. The dataset contains temporally segmented clips with semantic differences.
These designs in <span id="S6.p1.1.3" class="ltx_text ltx_font_bold">InternVideo2</span> contribute to enhancing video understanding in both perception and reasoning tasks. Notably, <span id="S6.p1.1.4" class="ltx_text ltx_font_bold">InternVideo2</span> excels in video-related dialogue and long video understanding, demonstrating its effectiveness in various video-related tasks.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Limitations and Discussions.</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">Despite its achievements, <span id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> does not introduce novel training methods or architectural advances. Instead, it leverages existing learning techniques for scheme exploration while focusing on improving data processing to enhance spatiotemporal perception, semantic alignment, and basic knowledge embedding. Similarly to previous studies <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>); Ye et al. (<a href="#bib.bib134" title="" class="ltx_ref">2023b</a>)</cite>, the model still grapples with limitations stemming from fixed input resolutions, sampling rates, and highly compressed tokens, which restrict its ability to express rich video information and capture fine-grained details.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px1.p2.1" class="ltx_p">The progressive learning scheme adopted by <span id="S6.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> strikes a balance between model capabilities and training compute. While jointly learning the three optimization objectives simultaneously is computationally feasible, scalability becomes an issue when confronted with limited resources.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px1.p3.1" class="ltx_p">Although <span id="S6.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> has demonstrated leading performance in long video understanding and reasoning benchmarks, it cannot guarantee an implicit world model that ensures consistency in visual reasoning. The inherent constraints imposed by fixed input representations, coupled with the complexity of visual reasoning tasks, present challenges in achieving a comprehensive and consistent understanding of the visual world.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Broader Impact</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">It is important to acknowledge that, similar to other foundational models, <span id="S7.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> has the potential to embed biases present in its training data and the associated models used during training, such as neural teachers <cite class="ltx_cite ltx_citemacro_cite">Tong et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>); Chen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023a</a>)</cite> and language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>); Zheng et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. These biases may emerge due to a variety of factors, including the personal ideas, preferences, values, and perspectives of the data creators and the training corpus utilized.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">The presence of biases in AI models can have societal implications and reinforce existing inequalities or prejudices. Biases within <span id="S7.p2.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> could manifest in the form of unfair or discriminatory outputs, potentially perpetuating social biases or stereotypes present in the training data. Consequently, it is crucial to carefully consider the potential impact of deploying <span id="S7.p2.1.2" class="ltx_text ltx_font_bold">InternVideo2</span> in real-world applications and take proactive measures to mitigate biases and ensure fairness.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabeur et al. [2020]</span>
<span class="ltx_bibblock">
Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Multi-modal transformer for video retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16</em>, pages 214–229. Springer, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bruce et al. [2024]</span>
<span class="ltx_bibblock">
Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al.

</span>
<span class="ltx_bibblock">Genie: Generative interactive environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.15391</em>, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. [2023]</span>
<span class="ltx_bibblock">
Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zablocki et al. [2022]</span>
<span class="ltx_bibblock">
Éloi Zablocki, Hédi Ben-Younes, Patrick Pérez, and Matthieu Cord.

</span>
<span class="ltx_bibblock">Explainability of deep vision-based autonomous driving systems: Review and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 130(10):2425–2452, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. [2023]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023a]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.08774, 2023a.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023a]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2302.13971, 2023a.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023b]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2307.09288, 2023b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023b]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4v(ision) system card.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:263218031" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:263218031</a>, 2023b.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2023]</span>
<span class="ltx_bibblock">
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qianmengke Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.

</span>
<span class="ltx_bibblock">Multimodal-gpt: A vision and language model for dialogue with humans.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.04790, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023a]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10592</em>, 2023a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023c]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt/</a>, 2023c.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2024]</span>
<span class="ltx_bibblock">
Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans.

</span>
<span class="ltx_bibblock">Video as the new language for real-world decision making.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.17139</em>, 2024.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2022]</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023a]</span>
<span class="ltx_bibblock">
Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Videomae v2: Scaling video masked autoencoders with dual masking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023a.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2021]</span>
<span class="ltx_bibblock">
Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Videoclip: Contrastive pre-training for zero-shot video-text understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.14084</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2022]</span>
<span class="ltx_bibblock">
Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu.

</span>
<span class="ltx_bibblock">Video-text modeling with zero-shot transfer from contrastive captioners.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2212.04979, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Unmasked teacher: Towards training-efficient video foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16058</em>, 2023a.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. [2022]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2204.14198, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023a]</span>
<span class="ltx_bibblock">
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Generative pretraining in multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.05222</em>, 2023a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.

</span>
<span class="ltx_bibblock">Mvbench: A comprehensive multi-modal video understanding benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17005</em>, 2023b.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Internvideo: General video foundation models via generative and discriminative learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.03191</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2024]</span>
<span class="ltx_bibblock">
Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong.

</span>
<span class="ltx_bibblock">Videoprism: A foundational visual encoder for video understanding, 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023a]</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14238</em>, 2023a.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang [2020]</span>
<span class="ltx_bibblock">
Tianhao Li and Limin Wang.

</span>
<span class="ltx_bibblock">Learning spatiotemporal features via video and text pair discrimination.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2001.05691, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2001.05691" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2001.05691</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022a]</span>
<span class="ltx_bibblock">
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09552</em>, 2022a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2022]</span>
<span class="ltx_bibblock">
Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Scaling up vision-language pre-training for image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 17980–17989, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou et al. [2022]</span>
<span class="ltx_bibblock">
Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al.

</span>
<span class="ltx_bibblock">An empirical study of training end-to-end vision-and-language transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2021]</span>
<span class="ltx_bibblock">
Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">How much can clip benefit vision-and-language tasks?

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.06383</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2021]</span>
<span class="ltx_bibblock">
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.

</span>
<span class="ltx_bibblock">Filip: Fine-grained interactive language-image pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.07783</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2019]</span>
<span class="ltx_bibblock">
Chen Sun, Austin Myers, Carl Vondrick, Kevin P. Murphy, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Videobert: A joint model for video and language representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Yang [2020]</span>
<span class="ltx_bibblock">
Linchao Zhu and Yi Yang.

</span>
<span class="ltx_bibblock">Actbert: Learning global-local video-text representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022a]</span>
<span class="ltx_bibblock">
Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, et al.

</span>
<span class="ltx_bibblock">Internvideo-ego4d: A pack of champion solutions to ego4d challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09529</em>, 2022a.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. [2021]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Merlot: Multimodal neural script knowledge models.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 34:23634–23651, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. [2022]</span>
<span class="ltx_bibblock">
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Merlot reserve: Neural script knowledge through vision and language and sound.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 16375–16387, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023a]</span>
<span class="ltx_bibblock">
Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, and Yixiao Ge.

</span>
<span class="ltx_bibblock">Learning transferable spatiotemporal representations from natural script knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 23079–23089, 2023a.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023b]</span>
<span class="ltx_bibblock">
Ziyun Zeng, Yixiao Ge, Zhan Tong, Xihui Liu, Shu-Tao Xia, and Ying Shan.

</span>
<span class="ltx_bibblock">Tvtsv2: Learning out-of-the-box spatiotemporal visual representations at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14173</em>, 2023b.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023b]</span>
<span class="ltx_bibblock">
Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al.

</span>
<span class="ltx_bibblock">Videollm: Modeling video sequence with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13292</em>, 2023b.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023c]</span>
<span class="ltx_bibblock">
Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu.

</span>
<span class="ltx_bibblock">Valor: Vision-audio-language omni-perception pretraining model and dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08345</em>, 2023c.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2023]</span>
<span class="ltx_bibblock">
Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing Liu, and Jiashi Feng.

</span>
<span class="ltx_bibblock">Vlab: Enhancing video language pre-training by feature adapting and blending.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13167</em>, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024a]</span>
<span class="ltx_bibblock">
Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu.

</span>
<span class="ltx_bibblock">Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023b]</span>
<span class="ltx_bibblock">
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang.

</span>
<span class="ltx_bibblock">Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 6312–6322, 2023b.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer et al. [2022]</span>
<span class="ltx_bibblock">
Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He.

</span>
<span class="ltx_bibblock">Masked autoencoders as spatiotemporal learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.09113, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023c]</span>
<span class="ltx_bibblock">
Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et al.

</span>
<span class="ltx_bibblock">All in one: Exploring unified video-language pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 6598–6608, 2023c.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miech et al. [2020]</span>
<span class="ltx_bibblock">
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">End-to-end learning of visual representations from uncurated instructional videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong et al. [2022]</span>
<span class="ltx_bibblock">
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.

</span>
<span class="ltx_bibblock">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35:10078–10093, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023c]</span>
<span class="ltx_bibblock">
Yizhuo Li, Kunchang Li, Yinan He, Yi Wang, Yali Wang, Limin Wang, Yu Qiao, and Ping Luo.

</span>
<span class="ltx_bibblock">Harvest video foundation models via efficient post-pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.19554</em>, 2023c.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2021]</span>
<span class="ltx_bibblock">
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu.

</span>
<span class="ltx_bibblock">Violet: End-to-end video-language transformers with masked visual-token modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.12681</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023b]</span>
<span class="ltx_bibblock">
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al.

</span>
<span class="ltx_bibblock">Generative multimodal models are in-context learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.13286</em>, 2023b.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al.

</span>
<span class="ltx_bibblock">mplug-2: A modularized multi-modal foundation model across text, image and video.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.00402</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2018]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1810.04805, 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">JMLR</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2021]</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. [2022]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">JMLR</em>, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017a]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017a.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. [2015]</span>
<span class="ltx_bibblock">
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2016]</span>
<span class="ltx_bibblock">
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.

</span>
<span class="ltx_bibblock">Msr-vtt: A large video description dataset for bridging video and language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 5288–5296, 2016.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. [2019]</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023b]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2304.10592, 2023b.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2023]</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2023.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al. [2023a]</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Shahbaz Khan.

</span>
<span class="ltx_bibblock">Video-chatgpt: Towards detailed video understanding via large vision and language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2306.05424, 2023a.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2023]</span>
<span class="ltx_bibblock">
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-Hui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei.

</span>
<span class="ltx_bibblock">Valley: Video assistant with large language model enhanced ability.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2306.07207, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2020]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.11929, 2020.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022]</span>
<span class="ltx_bibblock">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01917</em>, 2022.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023d]</span>
<span class="ltx_bibblock">
Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei.

</span>
<span class="ltx_bibblock">BEATs: Audio pre-training with acoustic tokenizers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em id="bib.bib67.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 5178–5193. PMLR, 23–29 Jul 2023d.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2022]</span>
<span class="ltx_bibblock">
Feng Cheng, Xizi Wang, Jie Lei, David J. Crandall, Mohit Bansal, and Gedas Bertasius.

</span>
<span class="ltx_bibblock">Vindlu: A recipe for effective video-and-language pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2212.05051, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022b]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022b.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022c]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022c.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023]</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05685</em>, 2023.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira and Zisserman [2017]</span>
<span class="ltx_bibblock">
Joao Carreira and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Quo vadis, action recognition? a new model and the kinetics dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017b]</span>
<span class="ltx_bibblock">
Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic.

</span>
<span class="ltx_bibblock">The “something something” video database for learning and evaluating visual common sense.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017b.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monfort et al. [2020]</span>
<span class="ltx_bibblock">
Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva.

</span>
<span class="ltx_bibblock">Moments in time dataset: One million videos for event understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">TPAMI</em>, 2020.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heilbron et al. [2015]</span>
<span class="ltx_bibblock">
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.

</span>
<span class="ltx_bibblock">Activitynet: A large-scale video benchmark for human activity understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2019]</span>
<span class="ltx_bibblock">
Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan.

</span>
<span class="ltx_bibblock">Hacs: Human action clips and segments dataset for recognition and temporal localization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023c]</span>
<span class="ltx_bibblock">
Wentao Zhu, Yufang Huang, Xiufeng Xie, Wenxian Liu, Jincan Deng, Debing Zhang, Zhangyang Wang, and Ji Liu.

</span>
<span class="ltx_bibblock">Autoshot: A short video dataset and state-of-the-art shot boundary detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 2237–2246, 2023c.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023d]</span>
<span class="ltx_bibblock">
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.06942</em>, 2023d.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2023]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 28492–28518. PMLR, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023d]</span>
<span class="ltx_bibblock">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Videochat: Chat-centric video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06355</em>, 2023d.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et al. [2023]</span>
<span class="ltx_bibblock">
Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang.

</span>
<span class="ltx_bibblock">Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. [2022]</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 35:16344–16359, 2022.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2018]</span>
<span class="ltx_bibblock">
João Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">A short note about kinetics-600.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1808.01340, 2018.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2019]</span>
<span class="ltx_bibblock">
João Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">A short note on the kinetics-700 human action dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1907.06987, 2019.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soomro et al. [2012]</span>
<span class="ltx_bibblock">
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.

</span>
<span class="ltx_bibblock">Ucf101: A dataset of 101 human actions classes from videos in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1212.0402</em>, 2012.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuehne et al. [2011]</span>
<span class="ltx_bibblock">
Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre.

</span>
<span class="ltx_bibblock">Hmdb: a large video database for human motion recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">2011 International conference on computer vision</em>, pages 2556–2563. IEEE, 2011.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2017]</span>
<span class="ltx_bibblock">
J. Gao, Chen Sun, Zhenheng Yang, and Ramakant Nevatia.

</span>
<span class="ltx_bibblock">Tall: Temporal activity localization via language query.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021]</span>
<span class="ltx_bibblock">
Bowen Zhang, Jiahui Yu, Christopher Fifty, Wei Han, Andrew M. Dai, Ruoming Pang, and Fei Sha.

</span>
<span class="ltx_bibblock">Co-training transformer with videos and images improves action recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2112.07175, 2021.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryali et al. [2023]</span>
<span class="ltx_bibblock">
Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Hiera: A hierarchical vision transformer without the bells-and-whistles, 2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bardes et al. [2024]</span>
<span class="ltx_bibblock">
Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas.

</span>
<span class="ltx_bibblock">V-JEPA: Latent video prediction for visual representation learning, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=WFYbBOEOtv" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=WFYbBOEOtv</a>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. [2023]</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al. [2023]</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim M. Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti’c, Dustin Tran, Thomas Kipf, Mario Luvci’c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby.

</span>
<span class="ltx_bibblock">Scaling vision transformers to 22 billion parameters.

</span>
<span class="ltx_bibblock">In <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherti et al. [2023]</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">Reproducible scaling laws for contrastive language-image learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 2818–2829, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021a]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">ICML</em>, pages 8748–8763. PMLR, 2021a.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023c]</span>
<span class="ltx_bibblock">
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.

</span>
<span class="ltx_bibblock">Eva-clip: Improved training techniques for clip at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.15389</em>, 2023c.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2024]</span>
<span class="ltx_bibblock">
Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Eva-clip-18b: Scaling clip to 18 billion parameters.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.04252</em>, 2024.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Idrees et al. [2017]</span>
<span class="ltx_bibblock">
Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah.

</span>
<span class="ltx_bibblock">The thumos challenge on action recognition for videos “in the wild”.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 2017.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. [2017]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.

</span>
<span class="ltx_bibblock">Dense-captioning events in videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao.

</span>
<span class="ltx_bibblock">Fineaction: A fine-grained video dataset for temporal action localization.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, 31:6937–6950, 2022.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2018]</span>
<span class="ltx_bibblock">
Du Tran, Hong xiu Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri.

</span>
<span class="ltx_bibblock">A closer look at spatiotemporal convolutions for action recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022]</span>
<span class="ltx_bibblock">
Chen-Lin Zhang, Jian Zhai Wu, and Yin Li.

</span>
<span class="ltx_bibblock">Actionformer: Localizing moments of actions with transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2202.07925, 2022.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2019]</span>
<span class="ltx_bibblock">
Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.

</span>
<span class="ltx_bibblock">Bmn: Boundary-matching network for temporal action proposal generation, 2019.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022b]</span>
<span class="ltx_bibblock">
Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu.

</span>
<span class="ltx_bibblock">Dcan: improving temporal action detection via dual context aggregation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, volume 36, pages 248–257, 2022b.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023]</span>
<span class="ltx_bibblock">
Min Yang, Guo Chen, Yin-Dong Zheng, Tong Lu, and Limin Wang.

</span>
<span class="ltx_bibblock">Basictad: an astounding rgb-only baseline for temporal action detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, page 103692, 2023.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Linjie Yang, Yuchen Fan, and Ning Xu.

</span>
<span class="ltx_bibblock">Video instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2021]</span>
<span class="ltx_bibblock">
Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov.

</span>
<span class="ltx_bibblock">Per-pixel classification is not all you need for semantic segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022c]</span>
<span class="ltx_bibblock">
Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao.

</span>
<span class="ltx_bibblock">Vision transformer adapter for dense predictions.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.08534</em>, 2022c.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2022]</span>
<span class="ltx_bibblock">
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.

</span>
<span class="ltx_bibblock">Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 2022.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. [2015]</span>
<span class="ltx_bibblock">
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele.

</span>
<span class="ltx_bibblock">A dataset for movie description.

</span>
<span class="ltx_bibblock">In <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 3202–3212, 2015.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anne Hendricks et al. [2017]</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.

</span>
<span class="ltx_bibblock">Localizing moments in video with natural language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, pages 5803–5812, 2017.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Dolan [2011]</span>
<span class="ltx_bibblock">
David L Chen and William B Dolan.

</span>
<span class="ltx_bibblock">Collecting highly parallel data for paraphrase evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1</em>, pages 190–200. Association for Computational Linguistics, 2011.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02858</em>, 2023a.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024b]</span>
<span class="ltx_bibblock">
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al.

</span>
<span class="ltx_bibblock">Panda-70m: Captioning 70m videos with multiple cross-modality teachers.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.19479</em>, 2024b.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023a]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2304.14178, 2023a.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangalam et al. [2023]</span>
<span class="ltx_bibblock">
Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Egoschema: A diagnostic benchmark for very long-form video language understanding.

</span>
<span class="ltx_bibblock">In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 36, pages 46212–46244. Curran Associates, Inc., 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf</a>.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. [2021]</span>
<span class="ltx_bibblock">
Jie Lei, Tamara L. Berg, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Qvhighlights: Detecting moments and highlights in videos via natural language queries, 2021.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer et al. [2019]</span>
<span class="ltx_bibblock">
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.

</span>
<span class="ltx_bibblock">Slowfast networks for video recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. [2023]</span>
<span class="ltx_bibblock">
WonJun Moon, Sangeek Hyun, SuBeen Lee, and Jae-Pil Heo.

</span>
<span class="ltx_bibblock">Correlation-guided query-dependency calibration in video representation learning for temporal grounding.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08835</em>, 2023.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023]</span>
<span class="ltx_bibblock">
Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou.

</span>
<span class="ltx_bibblock">Univtg: Towards unified video-language temporal grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 2794–2804, 2023.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021b]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021b.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2019]</span>
<span class="ltx_bibblock">
Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.

</span>
<span class="ltx_bibblock">Audiocaps: Generating captions for audios in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 119–132, 2019.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drossos et al. [2020]</span>
<span class="ltx_bibblock">
Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen.

</span>
<span class="ltx_bibblock">Clotho: An audio captioning dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 736–740. IEEE, 2020.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2021]</span>
<span class="ltx_bibblock">
Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, and Yejin Choi.

</span>
<span class="ltx_bibblock">Connecting the dots between audio and text without parallel data through visual knowledge transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.08995</em>, 2021.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023d]</span>
<span class="ltx_bibblock">
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al.

</span>
<span class="ltx_bibblock">Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01852</em>, 2023d.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipping et al. [2022]</span>
<span class="ltx_bibblock">
Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen.

</span>
<span class="ltx_bibblock">Clotho-aqa: A crowdsourced dataset for audio question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">2022 30th European Signal Processing Conference (EUSIPCO)</em>, pages 1140–1144. IEEE, 2022.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Behera et al. [2023]</span>
<span class="ltx_bibblock">
Swarup Ranjan Behera, Krishna Mohan Injeti, Jaya Sai Kiran Patibandla, Praveen Kumar Pokala, and Balakrishna Reddy Pailla.

</span>
<span class="ltx_bibblock">Aquallm: Audio question answering data generation using large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.17343</em>, 2023.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piczak [2015]</span>
<span class="ltx_bibblock">
Karol J Piczak.

</span>
<span class="ltx_bibblock">Esc: Dataset for environmental sound classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM international conference on Multimedia</em>, pages 1015–1018, 2015.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023e]</span>
<span class="ltx_bibblock">
Guangyao Li, Yixin Xu, and Di Hu.

</span>
<span class="ltx_bibblock">Multi-scale attention for audio question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.17993</em>, 2023e.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2021]</span>
<span class="ltx_bibblock">
Yuan Gong, Yu-An Chung, and James Glass.

</span>
<span class="ltx_bibblock">Ast: Audio spectrogram transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.01778</em>, 2021.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">Movqa: A benchmark of versatile question-answering for long-form movie understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.04817</em>, 2023b.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023c]</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2306.02858, 2023c.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023b]</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality, 2023b.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich [2019]</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et al. [2021]</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, pages 1728–1738, 2021.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et al. [2016]</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.01759</em>, 2016.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et al. [2023]</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff
Wang, and Skyler Wang.

</span>
<span class="ltx_bibblock">Seamlessm4t: Massively multilingual &amp; multimodal machine translation, 2023.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. [2023]</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023f]</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Otter: A multi-modal model with in-context instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.03726</em>, 2023f.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al. [2023b]</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.

</span>
<span class="ltx_bibblock">Video-chatgpt: Towards detailed video understanding via large vision and language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05424</em>, 2023b.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patraucean et al. [2024]</span>
<span class="ltx_bibblock">
Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al.

</span>
<span class="ltx_bibblock">Perception test: A diagnostic benchmark for multimodal video models.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p">In this supplementary file, we present more details about 1) the used video encoder architecture; 2) pretraining data and annotations; and 3) additional experiments.</p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Model</h2>

<figure id="A1.T21" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 21: </span>Architecture of vision encoder (6B).</figcaption>
<div id="A1.T21.44" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:187.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.6pt,19.9pt) scale(0.824268767620931,0.824268767620931) ;">
<table id="A1.T21.44.44" class="ltx_tabular ltx_align_middle">
<tr id="A1.T21.44.44.45" class="ltx_tr">
<td id="A1.T21.44.44.45.1" class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<span id="A1.T21.44.44.45.1.1" class="ltx_text ltx_font_bold">Stage</span>
</td>
<td id="A1.T21.44.44.45.2" class="ltx_td ltx_align_center ltx_border_r"><span id="A1.T21.44.44.45.2.1" class="ltx_text ltx_font_bold">ViT-6B</span></td>
<td id="A1.T21.44.44.45.3" class="ltx_td ltx_align_center"><span id="A1.T21.44.44.45.3.1" class="ltx_text ltx_font_bold">Output Size</span></td>
</tr>
<tr id="A1.T21.3.3.3" class="ltx_tr">
<td id="A1.T21.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
Video</td>
<td id="A1.T21.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r">sparse sampling</td>
<td id="A1.T21.3.3.3.3" class="ltx_td ltx_align_center"><span id="A1.T21.3.3.3.3.3" class="ltx_text" style="color:#800080;">3<math id="A1.T21.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.1.1.1.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.1.1.1.1.1.m1.1.1" xref="A1.T21.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.1.1.1.1.1.m1.1b"><times id="A1.T21.1.1.1.1.1.m1.1.1.cmml" xref="A1.T21.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.1.1.1.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.3.3.3.3.3.2" class="ltx_text" style="color:#5C946E;">8<math id="A1.T21.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.2.2.2.2.2.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.2.2.2.2.2.1.m1.1.1" xref="A1.T21.2.2.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.2.2.2.2.2.1.m1.1b"><times id="A1.T21.2.2.2.2.2.1.m1.1.1.cmml" xref="A1.T21.2.2.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.2.2.2.2.2.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.3.3.3.3.3.2.1" class="ltx_text" style="color:#0E79B2;">224<math id="A1.T21.3.3.3.3.3.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.3.3.3.3.3.2.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.3.3.3.3.3.2.1.m1.1.1" xref="A1.T21.3.3.3.3.3.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.3.3.3.3.3.2.1.m1.1b"><times id="A1.T21.3.3.3.3.3.2.1.m1.1.1.cmml" xref="A1.T21.3.3.3.3.3.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.3.3.3.3.3.2.1.m1.1c">\times</annotation></semantics></math>224</span></span></span></td>
</tr>
<tr id="A1.T21.7.7.7" class="ltx_tr">
<td id="A1.T21.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Patch</td>
<td id="A1.T21.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A1.T21.5.5.5.2.2" class="ltx_text" style="color:#5C946E;">1<math id="A1.T21.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.4.4.4.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.4.4.4.1.1.m1.1.1" xref="A1.T21.4.4.4.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.4.4.4.1.1.m1.1b"><times id="A1.T21.4.4.4.1.1.m1.1.1.cmml" xref="A1.T21.4.4.4.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.4.4.4.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.5.5.5.2.2.1" class="ltx_text" style="color:#0E79B2;">14<math id="A1.T21.5.5.5.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.5.5.5.2.2.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.5.5.5.2.2.1.m1.1.1" xref="A1.T21.5.5.5.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.5.5.5.2.2.1.m1.1b"><times id="A1.T21.5.5.5.2.2.1.m1.1.1.cmml" xref="A1.T21.5.5.5.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.5.5.5.2.2.1.m1.1c">\times</annotation></semantics></math>14</span></span>, <span id="A1.T21.5.5.5.2.3" class="ltx_text" style="color:#800080;">3200</span>
</td>
<td id="A1.T21.7.7.7.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A1.T21.7.7.7.4.2" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.6.6.6.3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.6.6.6.3.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.6.6.6.3.1.1.m1.1.1" xref="A1.T21.6.6.6.3.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.6.6.6.3.1.1.m1.1b"><times id="A1.T21.6.6.6.3.1.1.m1.1.1.cmml" xref="A1.T21.6.6.6.3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.6.6.6.3.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.7.7.7.4.2.2.1" class="ltx_text" style="color:#5C946E;">8<math id="A1.T21.7.7.7.4.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.7.7.7.4.2.2.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.7.7.7.4.2.2.1.m1.1.1" xref="A1.T21.7.7.7.4.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.7.7.7.4.2.2.1.m1.1b"><times id="A1.T21.7.7.7.4.2.2.1.m1.1.1.cmml" xref="A1.T21.7.7.7.4.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.7.7.7.4.2.2.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.7.7.7.4.2.2.1.1" class="ltx_text" style="color:#FF8000;">256</span></span></span></td>
</tr>
<tr id="A1.T21.9.9.9" class="ltx_tr">
<td id="A1.T21.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">Embedding</td>
<td id="A1.T21.9.9.9.2" class="ltx_td ltx_align_center ltx_border_r">stride <span id="A1.T21.9.9.9.2.2" class="ltx_text" style="color:#5C946E;">1<math id="A1.T21.8.8.8.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.8.8.8.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.8.8.8.1.1.m1.1.1" xref="A1.T21.8.8.8.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.8.8.8.1.1.m1.1b"><times id="A1.T21.8.8.8.1.1.m1.1.1.cmml" xref="A1.T21.8.8.8.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.8.8.8.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.9.9.9.2.2.1" class="ltx_text" style="color:#0E79B2;">14<math id="A1.T21.9.9.9.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.9.9.9.2.2.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.9.9.9.2.2.1.m1.1.1" xref="A1.T21.9.9.9.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.9.9.9.2.2.1.m1.1b"><times id="A1.T21.9.9.9.2.2.1.m1.1.1.cmml" xref="A1.T21.9.9.9.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.9.9.9.2.2.1.m1.1c">\times</annotation></semantics></math>14</span></span>
</td>
</tr>
<tr id="A1.T21.10.10.10" class="ltx_tr">
<td id="A1.T21.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Position</td>
<td id="A1.T21.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">learnable, 3D sine-cosine initialization</td>
<td id="A1.T21.10.10.10.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A1.T21.10.10.10.1.1" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.10.10.10.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.10.10.10.1.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.10.10.10.1.1.1.m1.1.1" xref="A1.T21.10.10.10.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.10.10.10.1.1.1.m1.1b"><times id="A1.T21.10.10.10.1.1.1.m1.1.1.cmml" xref="A1.T21.10.10.10.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.10.10.10.1.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.10.10.10.1.1.2" class="ltx_text" style="color:#FF8000;">2048</span></span></td>
</tr>
<tr id="A1.T21.11.11.11" class="ltx_tr">
<td id="A1.T21.11.11.11.2" class="ltx_td ltx_align_center ltx_border_r">Embedding</td>
<td id="A1.T21.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r"><span id="A1.T21.11.11.11.1.1" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.11.11.11.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.11.11.11.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.11.11.11.1.1.m1.1.1" xref="A1.T21.11.11.11.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.11.11.11.1.1.m1.1b"><times id="A1.T21.11.11.11.1.1.m1.1.1.cmml" xref="A1.T21.11.11.11.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.11.11.11.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.11.11.11.1.1.1" class="ltx_text" style="color:#FF8000;">2048</span></span></td>
</tr>
<tr id="A1.T21.16.16.16" class="ltx_tr">
<td id="A1.T21.16.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mask</td>
<td id="A1.T21.13.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">semantic mask w/ <span id="A1.T21.13.13.13.2.1" class="ltx_text ltx_font_italic">mask ratio</span> <math id="A1.T21.12.12.12.1.m1.1" class="ltx_Math" alttext="=" display="inline"><semantics id="A1.T21.12.12.12.1.m1.1a"><mo id="A1.T21.12.12.12.1.m1.1.1" xref="A1.T21.12.12.12.1.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="A1.T21.12.12.12.1.m1.1b"><eq id="A1.T21.12.12.12.1.m1.1.1.cmml" xref="A1.T21.12.12.12.1.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.12.12.12.1.m1.1c">=</annotation></semantics></math> <math id="A1.T21.13.13.13.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A1.T21.13.13.13.2.m2.1a"><mi id="A1.T21.13.13.13.2.m2.1.1" xref="A1.T21.13.13.13.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A1.T21.13.13.13.2.m2.1b"><ci id="A1.T21.13.13.13.2.m2.1.1.cmml" xref="A1.T21.13.13.13.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.13.13.13.2.m2.1c">\rho</annotation></semantics></math>
</td>
<td id="A1.T21.16.16.16.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T21.16.16.16.5.3" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.14.14.14.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.14.14.14.3.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.14.14.14.3.1.m1.1.1" xref="A1.T21.14.14.14.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.14.14.14.3.1.m1.1b"><times id="A1.T21.14.14.14.3.1.m1.1.1.cmml" xref="A1.T21.14.14.14.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.14.14.14.3.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.16.16.16.5.3.2" class="ltx_text" style="color:#FF8000;">2048<math id="A1.T21.15.15.15.4.2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="A1.T21.15.15.15.4.2.1.m1.1a"><mo mathcolor="#FF8000" id="A1.T21.15.15.15.4.2.1.m1.1.1" xref="A1.T21.15.15.15.4.2.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="A1.T21.15.15.15.4.2.1.m1.1b"><ci id="A1.T21.15.15.15.4.2.1.m1.1.1.cmml" xref="A1.T21.15.15.15.4.2.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.15.15.15.4.2.1.m1.1c">\cdot</annotation></semantics></math>(1-<math id="A1.T21.16.16.16.5.3.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A1.T21.16.16.16.5.3.2.m2.1a"><mi mathcolor="#FF8000" id="A1.T21.16.16.16.5.3.2.m2.1.1" xref="A1.T21.16.16.16.5.3.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A1.T21.16.16.16.5.3.2.m2.1b"><ci id="A1.T21.16.16.16.5.3.2.m2.1.1.cmml" xref="A1.T21.16.16.16.5.3.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.16.16.16.5.3.2.m2.1c">\rho</annotation></semantics></math>)</span></span></td>
</tr>
<tr id="A1.T21.23.23.23" class="ltx_tr">
<td id="A1.T21.23.23.23.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Encoder</td>
<td id="A1.T21.19.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="A1.T21.17.17.17.1.m1.1" class="ltx_Math" alttext="\left[\begin{array}[]{c}\text{MHSA({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}12800})}\end{array}\right]" display="inline"><semantics id="A1.T21.17.17.17.1.m1.1a"><mrow id="A1.T21.17.17.17.1.m1.1.2.2" xref="A1.T21.17.17.17.1.m1.1.2.1.cmml"><mo id="A1.T21.17.17.17.1.m1.1.2.2.1" xref="A1.T21.17.17.17.1.m1.1.2.1.1.cmml">[</mo><mtable rowspacing="0pt" id="A1.T21.17.17.17.1.m1.1.1" xref="A1.T21.17.17.17.1.m1.1.1.cmml"><mtr id="A1.T21.17.17.17.1.m1.1.1a" xref="A1.T21.17.17.17.1.m1.1.1.cmml"><mtd id="A1.T21.17.17.17.1.m1.1.1b" xref="A1.T21.17.17.17.1.m1.1.1.cmml"><mrow id="A1.T21.17.17.17.1.m1.1.1.1.1.1" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1d.cmml"><mtext id="A1.T21.17.17.17.1.m1.1.1.1.1.1a" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1d.cmml">MHSA(</mtext><mtext mathcolor="#800080" id="A1.T21.17.17.17.1.m1.1.1.1.1.1b" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1d.cmml">3200</mtext><mtext id="A1.T21.17.17.17.1.m1.1.1.1.1.1c" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1d.cmml">)</mtext></mrow></mtd></mtr><mtr id="A1.T21.17.17.17.1.m1.1.1c" xref="A1.T21.17.17.17.1.m1.1.1.cmml"><mtd id="A1.T21.17.17.17.1.m1.1.1d" xref="A1.T21.17.17.17.1.m1.1.1.cmml"><mrow id="A1.T21.17.17.17.1.m1.1.1.2.1.1" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1d.cmml"><mtext id="A1.T21.17.17.17.1.m1.1.1.2.1.1a" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1d.cmml">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.17.17.17.1.m1.1.1.2.1.1b" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1d.cmml">12800</mtext><mtext id="A1.T21.17.17.17.1.m1.1.1.2.1.1c" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1d.cmml">)</mtext></mrow></mtd></mtr></mtable><mo id="A1.T21.17.17.17.1.m1.1.2.2.2" xref="A1.T21.17.17.17.1.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.T21.17.17.17.1.m1.1b"><apply id="A1.T21.17.17.17.1.m1.1.2.1.cmml" xref="A1.T21.17.17.17.1.m1.1.2.2"><csymbol cd="latexml" id="A1.T21.17.17.17.1.m1.1.2.1.1.cmml" xref="A1.T21.17.17.17.1.m1.1.2.2.1">delimited-[]</csymbol><matrix id="A1.T21.17.17.17.1.m1.1.1.cmml" xref="A1.T21.17.17.17.1.m1.1.1"><matrixrow id="A1.T21.17.17.17.1.m1.1.1a.cmml" xref="A1.T21.17.17.17.1.m1.1.1"><ci id="A1.T21.17.17.17.1.m1.1.1.1.1.1d.cmml" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1"><mrow id="A1.T21.17.17.17.1.m1.1.1.1.1.1.cmml" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1"><mtext id="A1.T21.17.17.17.1.m1.1.1.1.1.1a.cmml" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1">MHSA(</mtext><mtext mathcolor="#800080" id="A1.T21.17.17.17.1.m1.1.1.1.1.1b.cmml" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1">3200</mtext><mtext id="A1.T21.17.17.17.1.m1.1.1.1.1.1c.cmml" xref="A1.T21.17.17.17.1.m1.1.1.1.1.1">)</mtext></mrow></ci></matrixrow><matrixrow id="A1.T21.17.17.17.1.m1.1.1b.cmml" xref="A1.T21.17.17.17.1.m1.1.1"><ci id="A1.T21.17.17.17.1.m1.1.1.2.1.1d.cmml" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1"><mrow id="A1.T21.17.17.17.1.m1.1.1.2.1.1.cmml" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1"><mtext id="A1.T21.17.17.17.1.m1.1.1.2.1.1a.cmml" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.17.17.17.1.m1.1.1.2.1.1b.cmml" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1">12800</mtext><mtext id="A1.T21.17.17.17.1.m1.1.1.2.1.1c.cmml" xref="A1.T21.17.17.17.1.m1.1.1.2.1.1">)</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.17.17.17.1.m1.1c">\left[\begin{array}[]{c}\text{MHSA({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}12800})}\end{array}\right]</annotation></semantics></math><math id="A1.T21.18.18.18.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.18.18.18.2.m2.1a"><mo id="A1.T21.18.18.18.2.m2.1.1" xref="A1.T21.18.18.18.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.18.18.18.2.m2.1b"><times id="A1.T21.18.18.18.2.m2.1.1.cmml" xref="A1.T21.18.18.18.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.18.18.18.2.m2.1c">\times</annotation></semantics></math>48 <math id="A1.T21.19.19.19.3.m3.1" class="ltx_Math" alttext="+" display="inline"><semantics id="A1.T21.19.19.19.3.m3.1a"><mo id="A1.T21.19.19.19.3.m3.1.1" xref="A1.T21.19.19.19.3.m3.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="A1.T21.19.19.19.3.m3.1b"><plus id="A1.T21.19.19.19.3.m3.1.1.cmml" xref="A1.T21.19.19.19.3.m3.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.19.19.19.3.m3.1c">+</annotation></semantics></math> AttnPool(<span id="A1.T21.19.19.19.3.1" class="ltx_text" style="color:#800080;">768</span>)</td>
<td id="A1.T21.23.23.23.7" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T21.23.23.23.7.5" class="ltx_text"></span> <span id="A1.T21.23.23.23.7.4" class="ltx_text">
<span id="A1.T21.23.23.23.7.4.4" class="ltx_tabular ltx_align_middle">
<span id="A1.T21.22.22.22.6.3.3.3" class="ltx_tr">
<span id="A1.T21.22.22.22.6.3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A1.T21.22.22.22.6.3.3.3.3.3" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1.1" xref="A1.T21.20.20.20.4.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1b"><times id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1.1.cmml" xref="A1.T21.20.20.20.4.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.20.20.20.4.1.1.1.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.22.22.22.6.3.3.3.3.3.2" class="ltx_text" style="color:#FF8000;">2048<math id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1a"><mo mathcolor="#FF8000" id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1.1" xref="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1b"><ci id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1.1.cmml" xref="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.21.21.21.5.2.2.2.2.2.1.m1.1c">\cdot</annotation></semantics></math>(1-<math id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1a"><mi mathcolor="#FF8000" id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1.1" xref="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1b"><ci id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1.1.cmml" xref="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.22.22.22.6.3.3.3.3.3.2.m2.1c">\rho</annotation></semantics></math>)</span></span></span></span>
<span id="A1.T21.23.23.23.7.4.4.4" class="ltx_tr">
<span id="A1.T21.23.23.23.7.4.4.4.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A1.T21.23.23.23.7.4.4.4.1.1" class="ltx_text" style="color:#800080;">768<math id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1.1" xref="A1.T21.23.23.23.7.4.4.4.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1b"><times id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1.1.cmml" xref="A1.T21.23.23.23.7.4.4.4.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.23.23.23.7.4.4.4.1.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.23.23.23.7.4.4.4.1.1.1" class="ltx_text" style="color:#FF8000;">1</span></span></span></span>
</span></span><span id="A1.T21.23.23.23.7.6" class="ltx_text"></span></td>
</tr>
<tr id="A1.T21.44.44.44" class="ltx_tr">
<td id="A1.T21.44.44.44.22" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Projection</td>
<td id="A1.T21.31.31.31.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<math id="A1.T21.24.24.24.1.m1.1" class="ltx_Math" alttext="\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\end{array}\right]" display="inline"><semantics id="A1.T21.24.24.24.1.m1.1a"><mrow id="A1.T21.24.24.24.1.m1.1.2.2" xref="A1.T21.24.24.24.1.m1.1.2.1.cmml"><mo id="A1.T21.24.24.24.1.m1.1.2.2.1" xref="A1.T21.24.24.24.1.m1.1.2.1.1.cmml">[</mo><mtable rowspacing="0pt" id="A1.T21.24.24.24.1.m1.1.1" xref="A1.T21.24.24.24.1.m1.1.1.cmml"><mtr id="A1.T21.24.24.24.1.m1.1.1a" xref="A1.T21.24.24.24.1.m1.1.1.cmml"><mtd id="A1.T21.24.24.24.1.m1.1.1b" xref="A1.T21.24.24.24.1.m1.1.1.cmml"><mrow id="A1.T21.24.24.24.1.m1.1.1.1.1.1" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1d.cmml"><mtext id="A1.T21.24.24.24.1.m1.1.1.1.1.1a" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1d.cmml">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.24.24.24.1.m1.1.1.1.1.1b" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1d.cmml">3200</mtext><mtext id="A1.T21.24.24.24.1.m1.1.1.1.1.1c" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1d.cmml">)</mtext></mrow></mtd></mtr><mtr id="A1.T21.24.24.24.1.m1.1.1c" xref="A1.T21.24.24.24.1.m1.1.1.cmml"><mtd id="A1.T21.24.24.24.1.m1.1.1d" xref="A1.T21.24.24.24.1.m1.1.1.cmml"><mrow id="A1.T21.24.24.24.1.m1.1.1.2.1.1" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1d.cmml"><mtext id="A1.T21.24.24.24.1.m1.1.1.2.1.1a" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1d.cmml">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.24.24.24.1.m1.1.1.2.1.1b" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1d.cmml">3200</mtext><mtext id="A1.T21.24.24.24.1.m1.1.1.2.1.1c" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1d.cmml">)</mtext></mrow></mtd></mtr></mtable><mo id="A1.T21.24.24.24.1.m1.1.2.2.2" xref="A1.T21.24.24.24.1.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.T21.24.24.24.1.m1.1b"><apply id="A1.T21.24.24.24.1.m1.1.2.1.cmml" xref="A1.T21.24.24.24.1.m1.1.2.2"><csymbol cd="latexml" id="A1.T21.24.24.24.1.m1.1.2.1.1.cmml" xref="A1.T21.24.24.24.1.m1.1.2.2.1">delimited-[]</csymbol><matrix id="A1.T21.24.24.24.1.m1.1.1.cmml" xref="A1.T21.24.24.24.1.m1.1.1"><matrixrow id="A1.T21.24.24.24.1.m1.1.1a.cmml" xref="A1.T21.24.24.24.1.m1.1.1"><ci id="A1.T21.24.24.24.1.m1.1.1.1.1.1d.cmml" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1"><mrow id="A1.T21.24.24.24.1.m1.1.1.1.1.1.cmml" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1"><mtext id="A1.T21.24.24.24.1.m1.1.1.1.1.1a.cmml" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.24.24.24.1.m1.1.1.1.1.1b.cmml" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1">3200</mtext><mtext id="A1.T21.24.24.24.1.m1.1.1.1.1.1c.cmml" xref="A1.T21.24.24.24.1.m1.1.1.1.1.1">)</mtext></mrow></ci></matrixrow><matrixrow id="A1.T21.24.24.24.1.m1.1.1b.cmml" xref="A1.T21.24.24.24.1.m1.1.1"><ci id="A1.T21.24.24.24.1.m1.1.1.2.1.1d.cmml" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1"><mrow id="A1.T21.24.24.24.1.m1.1.1.2.1.1.cmml" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1"><mtext id="A1.T21.24.24.24.1.m1.1.1.2.1.1a.cmml" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.24.24.24.1.m1.1.1.2.1.1b.cmml" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1">3200</mtext><mtext id="A1.T21.24.24.24.1.m1.1.1.2.1.1c.cmml" xref="A1.T21.24.24.24.1.m1.1.1.2.1.1">)</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.24.24.24.1.m1.1c">\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\end{array}\right]</annotation></semantics></math><math id="A1.T21.25.25.25.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.25.25.25.2.m2.1a"><mo id="A1.T21.25.25.25.2.m2.1.1" xref="A1.T21.25.25.25.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.25.25.25.2.m2.1b"><times id="A1.T21.25.25.25.2.m2.1.1.cmml" xref="A1.T21.25.25.25.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.25.25.25.2.m2.1c">\times</annotation></semantics></math><math id="A1.T21.26.26.26.3.m3.1" class="ltx_Math" alttext="K_{CLIP}" display="inline"><semantics id="A1.T21.26.26.26.3.m3.1a"><msub id="A1.T21.26.26.26.3.m3.1.1" xref="A1.T21.26.26.26.3.m3.1.1.cmml"><mi id="A1.T21.26.26.26.3.m3.1.1.2" xref="A1.T21.26.26.26.3.m3.1.1.2.cmml">K</mi><mrow id="A1.T21.26.26.26.3.m3.1.1.3" xref="A1.T21.26.26.26.3.m3.1.1.3.cmml"><mi id="A1.T21.26.26.26.3.m3.1.1.3.2" xref="A1.T21.26.26.26.3.m3.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="A1.T21.26.26.26.3.m3.1.1.3.1" xref="A1.T21.26.26.26.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.T21.26.26.26.3.m3.1.1.3.3" xref="A1.T21.26.26.26.3.m3.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="A1.T21.26.26.26.3.m3.1.1.3.1a" xref="A1.T21.26.26.26.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.T21.26.26.26.3.m3.1.1.3.4" xref="A1.T21.26.26.26.3.m3.1.1.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="A1.T21.26.26.26.3.m3.1.1.3.1b" xref="A1.T21.26.26.26.3.m3.1.1.3.1.cmml">​</mo><mi id="A1.T21.26.26.26.3.m3.1.1.3.5" xref="A1.T21.26.26.26.3.m3.1.1.3.5.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.T21.26.26.26.3.m3.1b"><apply id="A1.T21.26.26.26.3.m3.1.1.cmml" xref="A1.T21.26.26.26.3.m3.1.1"><csymbol cd="ambiguous" id="A1.T21.26.26.26.3.m3.1.1.1.cmml" xref="A1.T21.26.26.26.3.m3.1.1">subscript</csymbol><ci id="A1.T21.26.26.26.3.m3.1.1.2.cmml" xref="A1.T21.26.26.26.3.m3.1.1.2">𝐾</ci><apply id="A1.T21.26.26.26.3.m3.1.1.3.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3"><times id="A1.T21.26.26.26.3.m3.1.1.3.1.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3.1"></times><ci id="A1.T21.26.26.26.3.m3.1.1.3.2.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3.2">𝐶</ci><ci id="A1.T21.26.26.26.3.m3.1.1.3.3.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3.3">𝐿</ci><ci id="A1.T21.26.26.26.3.m3.1.1.3.4.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3.4">𝐼</ci><ci id="A1.T21.26.26.26.3.m3.1.1.3.5.cmml" xref="A1.T21.26.26.26.3.m3.1.1.3.5">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.26.26.26.3.m3.1c">K_{CLIP}</annotation></semantics></math>,
<math id="A1.T21.27.27.27.4.m4.1" class="ltx_Math" alttext="\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}1408})}\end{array}\right]" display="inline"><semantics id="A1.T21.27.27.27.4.m4.1a"><mrow id="A1.T21.27.27.27.4.m4.1.2.2" xref="A1.T21.27.27.27.4.m4.1.2.1.cmml"><mo id="A1.T21.27.27.27.4.m4.1.2.2.1" xref="A1.T21.27.27.27.4.m4.1.2.1.1.cmml">[</mo><mtable rowspacing="0pt" id="A1.T21.27.27.27.4.m4.1.1" xref="A1.T21.27.27.27.4.m4.1.1.cmml"><mtr id="A1.T21.27.27.27.4.m4.1.1a" xref="A1.T21.27.27.27.4.m4.1.1.cmml"><mtd id="A1.T21.27.27.27.4.m4.1.1b" xref="A1.T21.27.27.27.4.m4.1.1.cmml"><mrow id="A1.T21.27.27.27.4.m4.1.1.1.1.1" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1d.cmml"><mtext id="A1.T21.27.27.27.4.m4.1.1.1.1.1a" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1d.cmml">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.27.27.27.4.m4.1.1.1.1.1b" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1d.cmml">3200</mtext><mtext id="A1.T21.27.27.27.4.m4.1.1.1.1.1c" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1d.cmml">)</mtext></mrow></mtd></mtr><mtr id="A1.T21.27.27.27.4.m4.1.1c" xref="A1.T21.27.27.27.4.m4.1.1.cmml"><mtd id="A1.T21.27.27.27.4.m4.1.1d" xref="A1.T21.27.27.27.4.m4.1.1.cmml"><mrow id="A1.T21.27.27.27.4.m4.1.1.2.1.1" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1d.cmml"><mtext id="A1.T21.27.27.27.4.m4.1.1.2.1.1a" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1d.cmml">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.27.27.27.4.m4.1.1.2.1.1b" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1d.cmml">1408</mtext><mtext id="A1.T21.27.27.27.4.m4.1.1.2.1.1c" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1d.cmml">)</mtext></mrow></mtd></mtr></mtable><mo id="A1.T21.27.27.27.4.m4.1.2.2.2" xref="A1.T21.27.27.27.4.m4.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.T21.27.27.27.4.m4.1b"><apply id="A1.T21.27.27.27.4.m4.1.2.1.cmml" xref="A1.T21.27.27.27.4.m4.1.2.2"><csymbol cd="latexml" id="A1.T21.27.27.27.4.m4.1.2.1.1.cmml" xref="A1.T21.27.27.27.4.m4.1.2.2.1">delimited-[]</csymbol><matrix id="A1.T21.27.27.27.4.m4.1.1.cmml" xref="A1.T21.27.27.27.4.m4.1.1"><matrixrow id="A1.T21.27.27.27.4.m4.1.1a.cmml" xref="A1.T21.27.27.27.4.m4.1.1"><ci id="A1.T21.27.27.27.4.m4.1.1.1.1.1d.cmml" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1"><mrow id="A1.T21.27.27.27.4.m4.1.1.1.1.1.cmml" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1"><mtext id="A1.T21.27.27.27.4.m4.1.1.1.1.1a.cmml" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.27.27.27.4.m4.1.1.1.1.1b.cmml" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1">3200</mtext><mtext id="A1.T21.27.27.27.4.m4.1.1.1.1.1c.cmml" xref="A1.T21.27.27.27.4.m4.1.1.1.1.1">)</mtext></mrow></ci></matrixrow><matrixrow id="A1.T21.27.27.27.4.m4.1.1b.cmml" xref="A1.T21.27.27.27.4.m4.1.1"><ci id="A1.T21.27.27.27.4.m4.1.1.2.1.1d.cmml" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1"><mrow id="A1.T21.27.27.27.4.m4.1.1.2.1.1.cmml" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1"><mtext id="A1.T21.27.27.27.4.m4.1.1.2.1.1a.cmml" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.27.27.27.4.m4.1.1.2.1.1b.cmml" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1">1408</mtext><mtext id="A1.T21.27.27.27.4.m4.1.1.2.1.1c.cmml" xref="A1.T21.27.27.27.4.m4.1.1.2.1.1">)</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.27.27.27.4.m4.1c">\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}1408})}\end{array}\right]</annotation></semantics></math><math id="A1.T21.28.28.28.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.28.28.28.5.m5.1a"><mo id="A1.T21.28.28.28.5.m5.1.1" xref="A1.T21.28.28.28.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.28.28.28.5.m5.1b"><times id="A1.T21.28.28.28.5.m5.1.1.cmml" xref="A1.T21.28.28.28.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.28.28.28.5.m5.1c">\times</annotation></semantics></math><math id="A1.T21.29.29.29.6.m6.1" class="ltx_Math" alttext="K_{MAE}" display="inline"><semantics id="A1.T21.29.29.29.6.m6.1a"><msub id="A1.T21.29.29.29.6.m6.1.1" xref="A1.T21.29.29.29.6.m6.1.1.cmml"><mi id="A1.T21.29.29.29.6.m6.1.1.2" xref="A1.T21.29.29.29.6.m6.1.1.2.cmml">K</mi><mrow id="A1.T21.29.29.29.6.m6.1.1.3" xref="A1.T21.29.29.29.6.m6.1.1.3.cmml"><mi id="A1.T21.29.29.29.6.m6.1.1.3.2" xref="A1.T21.29.29.29.6.m6.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="A1.T21.29.29.29.6.m6.1.1.3.1" xref="A1.T21.29.29.29.6.m6.1.1.3.1.cmml">​</mo><mi id="A1.T21.29.29.29.6.m6.1.1.3.3" xref="A1.T21.29.29.29.6.m6.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="A1.T21.29.29.29.6.m6.1.1.3.1a" xref="A1.T21.29.29.29.6.m6.1.1.3.1.cmml">​</mo><mi id="A1.T21.29.29.29.6.m6.1.1.3.4" xref="A1.T21.29.29.29.6.m6.1.1.3.4.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.T21.29.29.29.6.m6.1b"><apply id="A1.T21.29.29.29.6.m6.1.1.cmml" xref="A1.T21.29.29.29.6.m6.1.1"><csymbol cd="ambiguous" id="A1.T21.29.29.29.6.m6.1.1.1.cmml" xref="A1.T21.29.29.29.6.m6.1.1">subscript</csymbol><ci id="A1.T21.29.29.29.6.m6.1.1.2.cmml" xref="A1.T21.29.29.29.6.m6.1.1.2">𝐾</ci><apply id="A1.T21.29.29.29.6.m6.1.1.3.cmml" xref="A1.T21.29.29.29.6.m6.1.1.3"><times id="A1.T21.29.29.29.6.m6.1.1.3.1.cmml" xref="A1.T21.29.29.29.6.m6.1.1.3.1"></times><ci id="A1.T21.29.29.29.6.m6.1.1.3.2.cmml" xref="A1.T21.29.29.29.6.m6.1.1.3.2">𝑀</ci><ci id="A1.T21.29.29.29.6.m6.1.1.3.3.cmml" xref="A1.T21.29.29.29.6.m6.1.1.3.3">𝐴</ci><ci id="A1.T21.29.29.29.6.m6.1.1.3.4.cmml" xref="A1.T21.29.29.29.6.m6.1.1.3.4">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.29.29.29.6.m6.1c">K_{MAE}</annotation></semantics></math>,
<math id="A1.T21.30.30.30.7.m7.1" class="ltx_Math" alttext="\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}768})}\end{array}\right]" display="inline"><semantics id="A1.T21.30.30.30.7.m7.1a"><mrow id="A1.T21.30.30.30.7.m7.1.2.2" xref="A1.T21.30.30.30.7.m7.1.2.1.cmml"><mo id="A1.T21.30.30.30.7.m7.1.2.2.1" xref="A1.T21.30.30.30.7.m7.1.2.1.1.cmml">[</mo><mtable rowspacing="0pt" id="A1.T21.30.30.30.7.m7.1.1" xref="A1.T21.30.30.30.7.m7.1.1.cmml"><mtr id="A1.T21.30.30.30.7.m7.1.1a" xref="A1.T21.30.30.30.7.m7.1.1.cmml"><mtd id="A1.T21.30.30.30.7.m7.1.1b" xref="A1.T21.30.30.30.7.m7.1.1.cmml"><mrow id="A1.T21.30.30.30.7.m7.1.1.1.1.1" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1d.cmml"><mtext id="A1.T21.30.30.30.7.m7.1.1.1.1.1a" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1d.cmml">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.30.30.30.7.m7.1.1.1.1.1b" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1d.cmml">3200</mtext><mtext id="A1.T21.30.30.30.7.m7.1.1.1.1.1c" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1d.cmml">)</mtext></mrow></mtd></mtr><mtr id="A1.T21.30.30.30.7.m7.1.1c" xref="A1.T21.30.30.30.7.m7.1.1.cmml"><mtd id="A1.T21.30.30.30.7.m7.1.1d" xref="A1.T21.30.30.30.7.m7.1.1.cmml"><mrow id="A1.T21.30.30.30.7.m7.1.1.2.1.1" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1d.cmml"><mtext id="A1.T21.30.30.30.7.m7.1.1.2.1.1a" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1d.cmml">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.30.30.30.7.m7.1.1.2.1.1b" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1d.cmml">768</mtext><mtext id="A1.T21.30.30.30.7.m7.1.1.2.1.1c" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1d.cmml">)</mtext></mrow></mtd></mtr></mtable><mo id="A1.T21.30.30.30.7.m7.1.2.2.2" xref="A1.T21.30.30.30.7.m7.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.T21.30.30.30.7.m7.1b"><apply id="A1.T21.30.30.30.7.m7.1.2.1.cmml" xref="A1.T21.30.30.30.7.m7.1.2.2"><csymbol cd="latexml" id="A1.T21.30.30.30.7.m7.1.2.1.1.cmml" xref="A1.T21.30.30.30.7.m7.1.2.2.1">delimited-[]</csymbol><matrix id="A1.T21.30.30.30.7.m7.1.1.cmml" xref="A1.T21.30.30.30.7.m7.1.1"><matrixrow id="A1.T21.30.30.30.7.m7.1.1a.cmml" xref="A1.T21.30.30.30.7.m7.1.1"><ci id="A1.T21.30.30.30.7.m7.1.1.1.1.1d.cmml" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1"><mrow id="A1.T21.30.30.30.7.m7.1.1.1.1.1.cmml" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1"><mtext id="A1.T21.30.30.30.7.m7.1.1.1.1.1a.cmml" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1">LN(</mtext><mtext mathcolor="#800080" id="A1.T21.30.30.30.7.m7.1.1.1.1.1b.cmml" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1">3200</mtext><mtext id="A1.T21.30.30.30.7.m7.1.1.1.1.1c.cmml" xref="A1.T21.30.30.30.7.m7.1.1.1.1.1">)</mtext></mrow></ci></matrixrow><matrixrow id="A1.T21.30.30.30.7.m7.1.1b.cmml" xref="A1.T21.30.30.30.7.m7.1.1"><ci id="A1.T21.30.30.30.7.m7.1.1.2.1.1d.cmml" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1"><mrow id="A1.T21.30.30.30.7.m7.1.1.2.1.1.cmml" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1"><mtext id="A1.T21.30.30.30.7.m7.1.1.2.1.1a.cmml" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1">MLP(</mtext><mtext mathcolor="#800080" id="A1.T21.30.30.30.7.m7.1.1.2.1.1b.cmml" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1">768</mtext><mtext id="A1.T21.30.30.30.7.m7.1.1.2.1.1c.cmml" xref="A1.T21.30.30.30.7.m7.1.1.2.1.1">)</mtext></mrow></ci></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.30.30.30.7.m7.1c">\left[\begin{array}[]{c}\text{LN({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}3200})}\\
\text{MLP({\color[rgb]{.5,0,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,0,.5}768})}\end{array}\right]</annotation></semantics></math><math id="A1.T21.31.31.31.8.m8.1" class="ltx_Math" alttext="\times 1" display="inline"><semantics id="A1.T21.31.31.31.8.m8.1a"><mrow id="A1.T21.31.31.31.8.m8.1.1" xref="A1.T21.31.31.31.8.m8.1.1.cmml"><mi id="A1.T21.31.31.31.8.m8.1.1.2" xref="A1.T21.31.31.31.8.m8.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="A1.T21.31.31.31.8.m8.1.1.1" xref="A1.T21.31.31.31.8.m8.1.1.1.cmml">×</mo><mn id="A1.T21.31.31.31.8.m8.1.1.3" xref="A1.T21.31.31.31.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T21.31.31.31.8.m8.1b"><apply id="A1.T21.31.31.31.8.m8.1.1.cmml" xref="A1.T21.31.31.31.8.m8.1.1"><times id="A1.T21.31.31.31.8.m8.1.1.1.cmml" xref="A1.T21.31.31.31.8.m8.1.1.1"></times><csymbol cd="latexml" id="A1.T21.31.31.31.8.m8.1.1.2.cmml" xref="A1.T21.31.31.31.8.m8.1.1.2">absent</csymbol><cn type="integer" id="A1.T21.31.31.31.8.m8.1.1.3.cmml" xref="A1.T21.31.31.31.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.31.31.31.8.m8.1c">\times 1</annotation></semantics></math>
</td>
<td id="A1.T21.44.44.44.21" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="A1.T21.44.44.44.21.14" class="ltx_text"></span> <span id="A1.T21.44.44.44.21.13" class="ltx_text">
<span id="A1.T21.44.44.44.21.13.13" class="ltx_tabular ltx_align_middle">
<span id="A1.T21.36.36.36.13.5.5.5" class="ltx_tr">
<span id="A1.T21.36.36.36.13.5.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A1.T21.32.32.32.9.1.1.1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A1.T21.32.32.32.9.1.1.1.1.m1.1a"><mi id="A1.T21.32.32.32.9.1.1.1.1.m1.1.1" xref="A1.T21.32.32.32.9.1.1.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A1.T21.32.32.32.9.1.1.1.1.m1.1b"><ci id="A1.T21.32.32.32.9.1.1.1.1.m1.1.1.cmml" xref="A1.T21.32.32.32.9.1.1.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.32.32.32.9.1.1.1.1.m1.1c">K</annotation></semantics></math><math id="A1.T21.33.33.33.10.2.2.2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.33.33.33.10.2.2.2.2.m2.1a"><mo id="A1.T21.33.33.33.10.2.2.2.2.m2.1.1" xref="A1.T21.33.33.33.10.2.2.2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.33.33.33.10.2.2.2.2.m2.1b"><times id="A1.T21.33.33.33.10.2.2.2.2.m2.1.1.cmml" xref="A1.T21.33.33.33.10.2.2.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.33.33.33.10.2.2.2.2.m2.1c">\times</annotation></semantics></math><span id="A1.T21.36.36.36.13.5.5.5.5.3" class="ltx_text" style="color:#800080;">3200<math id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1.1" xref="A1.T21.34.34.34.11.3.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1b"><times id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1.1.cmml" xref="A1.T21.34.34.34.11.3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.34.34.34.11.3.3.3.3.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.36.36.36.13.5.5.5.5.3.2" class="ltx_text" style="color:#FF8000;">2048<math id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1a"><mo mathcolor="#FF8000" id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1.1" xref="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1b"><ci id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1.1.cmml" xref="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.35.35.35.12.4.4.4.4.2.1.m1.1c">\cdot</annotation></semantics></math>(1-<math id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1a"><mi mathcolor="#FF8000" id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1.1" xref="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1b"><ci id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1.1.cmml" xref="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.36.36.36.13.5.5.5.5.3.2.m2.1c">\rho</annotation></semantics></math>)</span></span></span></span>
<span id="A1.T21.41.41.41.18.10.10.10" class="ltx_tr">
<span id="A1.T21.41.41.41.18.10.10.10.5" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A1.T21.37.37.37.14.6.6.6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A1.T21.37.37.37.14.6.6.6.1.m1.1a"><mi id="A1.T21.37.37.37.14.6.6.6.1.m1.1.1" xref="A1.T21.37.37.37.14.6.6.6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A1.T21.37.37.37.14.6.6.6.1.m1.1b"><ci id="A1.T21.37.37.37.14.6.6.6.1.m1.1.1.cmml" xref="A1.T21.37.37.37.14.6.6.6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.37.37.37.14.6.6.6.1.m1.1c">K</annotation></semantics></math><math id="A1.T21.38.38.38.15.7.7.7.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.38.38.38.15.7.7.7.2.m2.1a"><mo id="A1.T21.38.38.38.15.7.7.7.2.m2.1.1" xref="A1.T21.38.38.38.15.7.7.7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.38.38.38.15.7.7.7.2.m2.1b"><times id="A1.T21.38.38.38.15.7.7.7.2.m2.1.1.cmml" xref="A1.T21.38.38.38.15.7.7.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.38.38.38.15.7.7.7.2.m2.1c">\times</annotation></semantics></math><span id="A1.T21.41.41.41.18.10.10.10.5.3" class="ltx_text" style="color:#800080;">1408<math id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1.1" xref="A1.T21.39.39.39.16.8.8.8.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1b"><times id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1.1.cmml" xref="A1.T21.39.39.39.16.8.8.8.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.39.39.39.16.8.8.8.3.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.41.41.41.18.10.10.10.5.3.2" class="ltx_text" style="color:#FF8000;">2048<math id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1a"><mo mathcolor="#FF8000" id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1.1" xref="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1b"><ci id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1.1.cmml" xref="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.40.40.40.17.9.9.9.4.2.1.m1.1c">\cdot</annotation></semantics></math>(1-<math id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1a"><mi mathcolor="#FF8000" id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1.1" xref="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1b"><ci id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1.1.cmml" xref="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.41.41.41.18.10.10.10.5.3.2.m2.1c">\rho</annotation></semantics></math>)</span></span></span></span>
<span id="A1.T21.44.44.44.21.13.13.13" class="ltx_tr">
<span id="A1.T21.44.44.44.21.13.13.13.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A1.T21.42.42.42.19.11.11.11.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A1.T21.42.42.42.19.11.11.11.1.m1.1a"><mi id="A1.T21.42.42.42.19.11.11.11.1.m1.1.1" xref="A1.T21.42.42.42.19.11.11.11.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A1.T21.42.42.42.19.11.11.11.1.m1.1b"><ci id="A1.T21.42.42.42.19.11.11.11.1.m1.1.1.cmml" xref="A1.T21.42.42.42.19.11.11.11.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.42.42.42.19.11.11.11.1.m1.1c">K</annotation></semantics></math><math id="A1.T21.43.43.43.20.12.12.12.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.43.43.43.20.12.12.12.2.m2.1a"><mo id="A1.T21.43.43.43.20.12.12.12.2.m2.1.1" xref="A1.T21.43.43.43.20.12.12.12.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.43.43.43.20.12.12.12.2.m2.1b"><times id="A1.T21.43.43.43.20.12.12.12.2.m2.1.1.cmml" xref="A1.T21.43.43.43.20.12.12.12.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.43.43.43.20.12.12.12.2.m2.1c">\times</annotation></semantics></math><span id="A1.T21.44.44.44.21.13.13.13.3.1" class="ltx_text" style="color:#800080;">768<math id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1a"><mo mathcolor="#000000" id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1.1" xref="A1.T21.44.44.44.21.13.13.13.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1b"><times id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1.1.cmml" xref="A1.T21.44.44.44.21.13.13.13.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T21.44.44.44.21.13.13.13.3.1.m1.1c">\times</annotation></semantics></math><span id="A1.T21.44.44.44.21.13.13.13.3.1.1" class="ltx_text" style="color:#FF8000;">1</span></span></span></span>
</span></span><span id="A1.T21.44.44.44.21.15" class="ltx_text"></span></td>
</tr>
</table>
</span></div>
</figure>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Video Encoder.</h5>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS0.SSS0.Px1.p1.2" class="ltx_p">In Tab. <a href="#A1.T21" title="Table 21 ‣ Appendix A Model ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>, we take ViT-6B as an example and omit the class token for a simple presentation. “MHSA”, “MLP”, “AttnPool”, and “LN” refer to spatiotemporal multi-head self-attention, multi-layer perception, attention pooling <cite class="ltx_cite ltx_citemacro_cite">Yu et al. [<a href="#bib.bib66" title="" class="ltx_ref">2022</a>]</cite> and root mean square layer normalization <cite class="ltx_cite ltx_citemacro_cite">Zhang and Sennrich [<a href="#bib.bib135" title="" class="ltx_ref">2019</a>]</cite>. <math id="A1.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="K_{CLIP}" display="inline"><semantics id="A1.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="A1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">K</mi><mrow id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.2" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.3" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1a" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.4" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1b" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.5" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml">P</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝐾</ci><apply id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3"><times id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.1"></times><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.2">𝐶</ci><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.3">𝐿</ci><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.4.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.4">𝐼</ci><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.5.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.5">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.m1.1c">K_{CLIP}</annotation></semantics></math> and <math id="A1.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="K_{MAE}" display="inline"><semantics id="A1.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="A1.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">K</mi><mrow id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.2" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.3" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1a" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.4" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2">𝐾</ci><apply id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3"><times id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.1"></times><ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.2">𝑀</ci><ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.3">𝐴</ci><ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.4">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.2.m2.1c">K_{MAE}</annotation></semantics></math> means the layer number for unmasked token alignment with multimodal and motion-aware teachers. We mark the <span id="A1.SS0.SSS0.Px1.p1.2.1" class="ltx_text" style="color:#800080;">channel number</span>, <span id="A1.SS0.SSS0.Px1.p1.2.2" class="ltx_text" style="color:#5C946E;">frame number</span>, <span id="A1.SS0.SSS0.Px1.p1.2.3" class="ltx_text" style="color:#0E79B2;">spatial size</span>, and <span id="A1.SS0.SSS0.Px1.p1.2.4" class="ltx_text" style="color:#FF8000;">token number</span> by different colors. The projection layers are dropped after stage 1 training.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Video-centric Multimodal Data</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">We prepare our training data according to the learning objectives of the three stages in our learning scheme. Specifically, it consists of video-only pretraining set for masked video token reconstruction, Video-Audio-Speech-Text one for multimodal alignment, and video instruction dataset for human-computer interaction alignment. They are detailed in the following.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Video-only Data</h3>

<figure id="A2.T22" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 22: </span>Statistics of Stage1 data.
All the videos are used without any labels.
</figcaption>
<table id="A2.T22.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T22.2.3" class="ltx_tr">
<td id="A2.T22.2.3.1" class="ltx_td ltx_align_left ltx_border_r">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<span id="A2.T22.2.3.1.1" class="ltx_text ltx_font_bold">Dataset</span>
</td>
<td id="A2.T22.2.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="A2.T22.2.3.2.1" class="ltx_text ltx_font_bold">K710</span></td>
<td id="A2.T22.2.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="A2.T22.2.3.3.1" class="ltx_text ltx_font_bold">SthSthV2</span></td>
<td id="A2.T22.2.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="A2.T22.2.3.4.1" class="ltx_text ltx_font_bold">HACS</span></td>
<td id="A2.T22.2.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="A2.T22.2.3.5.1" class="ltx_text ltx_font_bold">ANet</span></td>
<td id="A2.T22.2.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="A2.T22.2.3.6.1" class="ltx_text ltx_font_bold">MiT</span></td>
<td id="A2.T22.2.3.7" class="ltx_td ltx_align_center"><span id="A2.T22.2.3.7.1" class="ltx_text ltx_font_bold">Self-collected</span></td>
</tr>
<tr id="A2.T22.1.1" class="ltx_tr">
<td id="A2.T22.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A2.T22.1.1.1.1" class="ltx_text ltx_font_bold">K-Mash<sub id="A2.T22.1.1.1.1.1" class="ltx_sub"><span id="A2.T22.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1.1M</span></sub></span></td>
<td id="A2.T22.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">658K</td>
<td id="A2.T22.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">169K</td>
<td id="A2.T22.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">106K</td>
<td id="A2.T22.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15K</td>
<td id="A2.T22.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">152K</td>
<td id="A2.T22.1.1.7" class="ltx_td ltx_align_center ltx_border_t">0</td>
</tr>
<tr id="A2.T22.2.2" class="ltx_tr">
<td id="A2.T22.2.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A2.T22.2.2.1.1" class="ltx_text ltx_font_bold">K-Mash<sub id="A2.T22.2.2.1.1.1" class="ltx_sub"><span id="A2.T22.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2M</span></sub></span></td>
<td id="A2.T22.2.2.2" class="ltx_td ltx_align_center ltx_border_r">658K</td>
<td id="A2.T22.2.2.3" class="ltx_td ltx_align_center ltx_border_r">169K</td>
<td id="A2.T22.2.2.4" class="ltx_td ltx_align_center ltx_border_r">106K</td>
<td id="A2.T22.2.2.5" class="ltx_td ltx_align_center ltx_border_r">15K</td>
<td id="A2.T22.2.2.6" class="ltx_td ltx_align_center ltx_border_r">207K</td>
<td id="A2.T22.2.2.7" class="ltx_td ltx_align_center">844K</td>
</tr>
<tr id="A2.T22.2.4" class="ltx_tr">
<td id="A2.T22.2.4.1" class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td id="A2.T22.2.4.2" class="ltx_td"></td>
<td id="A2.T22.2.4.3" class="ltx_td"></td>
<td id="A2.T22.2.4.4" class="ltx_td"></td>
<td id="A2.T22.2.4.5" class="ltx_td"></td>
<td id="A2.T22.2.4.6" class="ltx_td"></td>
<td id="A2.T22.2.4.7" class="ltx_td"></td>
</tr>
</table>
</figure>
<div id="A2.SS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS1.p1.1" class="ltx_p">To create the curated collection of videos, named <span id="A2.SS1.p1.1.1" class="ltx_text ltx_font_bold">K-Mash</span>, we source videos from renowned action recognition datasets such as Kinetics-400 (K400) <cite class="ltx_cite ltx_citemacro_cite">Carreira and Zisserman [<a href="#bib.bib73" title="" class="ltx_ref">2017</a>]</cite>, Something-Something (Sth) <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. [<a href="#bib.bib74" title="" class="ltx_ref">2017b</a>]</cite>, Moments in Time (MIT) <cite class="ltx_cite ltx_citemacro_cite">Monfort et al. [<a href="#bib.bib75" title="" class="ltx_ref">2020</a>]</cite>, ActivityNet <cite class="ltx_cite ltx_citemacro_cite">Heilbron et al. [<a href="#bib.bib76" title="" class="ltx_ref">2015</a>]</cite>, and HACS <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. [<a href="#bib.bib77" title="" class="ltx_ref">2019</a>]</cite>. These datasets provide a wide range of video types, including both first-person and third-person perspectives, short and long durations, and featuring a rich variety of characters and settings.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para ltx_noindent">
<p id="A2.SS1.p2.1" class="ltx_p">For the enhanced version of the dataset, called K-Mash<sub id="A2.SS1.p2.1.1" class="ltx_sub"><span id="A2.SS1.p2.1.1.1" class="ltx_text ltx_font_italic">2M</span></sub>, we push a step further and meticulously selected an additional 844,000 videos from YouTube to further enhance the diversity of the dataset. It’s important to note that all videos in this dataset are utilized for training without any labels, allowing the model to learn from the unlabeled data in an unsupervised manner. This approach helps to broaden the model’s understanding of different visual concepts and improves its performance on various video-related tasks.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Videos with Audio-Video-Speech Modalities</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.1" class="ltx_p">In addtion to publicly available video-text datasets (e.g. InternVid <cite class="ltx_cite ltx_citemacro_citep">[Wang et al., <a href="#bib.bib79" title="" class="ltx_ref">2023d</a>]</cite> and WebVid <cite class="ltx_cite ltx_citemacro_citep">[Bain et al., <a href="#bib.bib136" title="" class="ltx_ref">2021</a>]</cite>), we introduce a new video dataset that incorporates both audio-visual-speech information and their corresponding textual descriptions. This dataset is included in the training process of <span id="A2.SS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span>. To create this multimodal dataset, named <span id="A2.SS2.p1.1.2" class="ltx_text ltx_font_bold">MVid</span>, we leverage several video sources and provide detailed annotations. <span id="A2.SS2.p1.1.3" class="ltx_text ltx_font_bold">MVid</span> includes videos with synchronized audio, visual, and speech information, along with their corresponding textual descriptions. This multimodal dataset enables the training of <span id="A2.SS2.p1.1.4" class="ltx_text ltx_font_bold">InternVideo2</span> to better understand and capture the connections between different modalities, enhancing its performance in various video-related tasks that require audio, visual, and speech understanding.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS2.p2.1" class="ltx_p"><span id="A2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Collection.</span>
In the <span id="A2.SS2.p2.1.2" class="ltx_text ltx_font_bold">MVid</span> dataset, approximately half of the videos are sourced from YouTube, while the remaining videos are gathered from anonymous sources. This is to improve the diversity of dataset, as relying solely on YouTube may result in limited depth of the dataset.
Furthermore, to study the impact of video culture backgrounds on the learned models, a small portion of the dataset consists of Chinese data. These videos were collected with proper permissions for academic usage, ensuring compliance with legal and ethical considerations.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para ltx_noindent">
<p id="A2.SS2.p3.1" class="ltx_p">By incorporating videos from various sources and including a subset of Chinese data, <span id="A2.SS2.p3.1.1" class="ltx_text ltx_font_bold">MVid</span> provides a more diverse and representative dataset for training <span id="A2.SS2.p3.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>. This approach allows the model to learn from a wide range of video content, encompassing different cultural backgrounds and further enhancing its ability to understand and process videos from various sources.</p>
</div>
<div id="A2.SS2.p4" class="ltx_para ltx_noindent">
<p id="A2.SS2.p4.1" class="ltx_p"><span id="A2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Trimming.</span>
In our approach, instead of relying on the widely-used SceneDet filter of FFMPEG, we employ a temporal boundary detection model called AutoShot <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. [<a href="#bib.bib78" title="" class="ltx_ref">2023c</a>]</cite> to segment videos into clips. AutoShot is capable of predicting clip boundaries based on temporal semantic variations, as opposed to pixel differences. This leads to the generation of semantically complete cuts without mixing extra frames that may contain inconsistent context. By using AutoShot, we aim to reduce captioning errors by producing fewer clips with obvious transitions, resulting in a more coherent input for video captioning models. In the inference of AutoShot, we use a threshold of 0.5 to determine the shot boundaries for AutoShot’s estimations.</p>
</div>
<div id="A2.SS2.p5" class="ltx_para ltx_noindent">
<p id="A2.SS2.p5.1" class="ltx_p">For the video dataset, we first preserve clips longer than 2 seconds. For video clips longer than 30 seconds, as the segments within the clip are from the same shot, we randomly choose a 30-second segment. During this process, we also abandon clips with still or extreme dynamics, such as browsing a photo gallery.</p>
</div>
<figure id="A2.T23" class="ltx_table">
<div id="A2.T23.1" class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" style="width:429.3pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_block">Table 23: </span><span id="A2.T23.1.2.1" class="ltx_text ltx_font_bold">Fusion prompt.</span>The above prompt is to generate 2 multi-modal captions, the following prompt is to generate 3 multi-modal captions.</figcaption>
<div id="A2.T23.1.p1" class="ltx_para ltx_noindent ltx_align_center">
<svg id="A2.T23.1.p1.pic1" class="ltx_picture" height="198.43" overflow="visible" version="1.1" width="600"><g transform="translate(0,198.43) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 192.53 C 0 195.79 2.64 198.43 5.91 198.43 L 594.09 198.43 C 597.36 198.43 600 195.79 600 192.53 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 192.53 C 1.97 194.7 3.73 196.46 5.91 196.46 L 594.09 196.46 C 596.27 196.46 598.03 194.7 598.03 192.53 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="170.87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p" style="width:429.3pt;"> 

<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:429.3pt;">
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p">You are a text analysis expert. About one video, here is 1 vision caption: <span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter">vid_cap</span>, 1 audio caption:<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_text ltx_font_typewriter">aud_cap</span>. You need to understand and encode them into 1 sentence. Do not simply concatenate them together. The weights of video/audio are equaled. Considering dropping audio caption if it is incomprehensible. The output must be a complete and natural sentence. The sentence is:
<math id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="..." display="inline"><semantics id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">...</annotation></semantics></math></span>
<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_p"><span class="ltx_rule" style="width:433.6pt;height:1.0pt;position:relative; bottom:3.0pt;background:black;display:inline-block;"> </span>
You are a text analysis expert. About one video, here is 1 vision caption: <span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_typewriter">vid_cap</span>, 1 audio caption:<span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" class="ltx_text ltx_font_typewriter">aud_cap</span>, and one speech subtitle: <span id="A2.T23.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" class="ltx_text ltx_font_typewriter">asr_cap</span>,. You need to understand and encode them into 1 compelete sentence. The weights of video/audio/speech are equaled. Considering dropping audio caption or speech subtitle if it is incomprehensible. The output must be a complete and natural sentence, do not simply concatenate them together. The complete sentence is:</span>
</span></span>
</span></span></span>
</span>
</span></foreignObject></g></g></svg>
</div>
</div>
</figure>
<figure id="A2.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.15377/assets/x10.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.15377/assets/x11.png" id="A2.F10.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="84" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.15377/assets/x12.png" id="A2.F10.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.15377/assets/x13.png" id="A2.F10.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="83" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Annotation examples using our captioning approach.</figcaption>
</figure>
<div id="A2.SS2.p6" class="ltx_para ltx_noindent">
<p id="A2.SS2.p6.1" class="ltx_p"><span id="A2.SS2.p6.1.1" class="ltx_text ltx_font_bold">Annotation.</span> We automatically caption visual, audio, and speech of <span id="A2.SS2.p6.1.2" class="ltx_text ltx_font_bold">MVid</span>. Then we correct them and fuse them for cross-modal captions for training using LLM. We list several annotation examples of our method in Fig. <a href="#A2.F10" title="Figure 10 ‣ B.2 Videos with Audio-Video-Speech Modalities ‣ Appendix B Video-centric Multimodal Data ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A2.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="A2.I1.ix1.1.1.m1.1b"><mo id="A2.I1.ix1.1.1.m1.1.1" xref="A2.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="A2.I1.ix1.1.1.m1.1c"><ci id="A2.I1.ix1.1.1.m1.1.1.cmml" xref="A2.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="A2.I1.ix1.p1" class="ltx_para">
<p id="A2.I1.ix1.p1.1" class="ltx_p"><span id="A2.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Vision Captioner.</span> We employ the video captioning pipeline in InternVid to annotate our data. Rather than using VideoLLM <cite class="ltx_cite ltx_citemacro_citep">[Li et al., <a href="#bib.bib81" title="" class="ltx_ref">2023d</a>, Maaz et al., <a href="#bib.bib63" title="" class="ltx_ref">2023a</a>]</cite> to describe videos, we choose this validated method due to its better downstream results.</p>
</div>
</li>
<li id="A2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A2.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="A2.I1.ix2.1.1.m1.1b"><mo id="A2.I1.ix2.1.1.m1.1.1" xref="A2.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="A2.I1.ix2.1.1.m1.1c"><ci id="A2.I1.ix2.1.1.m1.1.1.cmml" xref="A2.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="A2.I1.ix2.p1" class="ltx_para">
<p id="A2.I1.ix2.p1.1" class="ltx_p"><span id="A2.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Audio Captioner.</span> We craft a audio captioner upon VideoChat <cite class="ltx_cite ltx_citemacro_citep">[Li et al., <a href="#bib.bib81" title="" class="ltx_ref">2023d</a>]</cite>, as we find no reliable ones. It extracts audio features from inputs by Beats <cite class="ltx_cite ltx_citemacro_citep">[Chen et al., <a href="#bib.bib67" title="" class="ltx_ref">2023d</a>]</cite>. We learn it by only tuning its Qformer (the interface between audio encoder and LLM) using a combination of the large-scale audio-text corpus WavCaps <cite class="ltx_cite ltx_citemacro_citep">[Mei et al., <a href="#bib.bib82" title="" class="ltx_ref">2023</a>]</cite> dataset.</p>
</div>
</li>
<li id="A2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A2.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="A2.I1.ix3.1.1.m1.1b"><mo id="A2.I1.ix3.1.1.m1.1.1" xref="A2.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="A2.I1.ix3.1.1.m1.1c"><ci id="A2.I1.ix3.1.1.m1.1.1.cmml" xref="A2.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="A2.I1.ix3.p1" class="ltx_para">
<p id="A2.I1.ix3.p1.1" class="ltx_p"><span id="A2.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Speech Captioner.</span> We utilize the audio transcription model Whisper <cite class="ltx_cite ltx_citemacro_citep">[Radford et al., <a href="#bib.bib80" title="" class="ltx_ref">2023</a>]</cite> to obtain speech from videos.Concretely, we use the WhisperV2-large model since its concurrent state-of-the-art performance. During the data collection process, a portion of the data is directly adopted from YT-Temporal-180M, which already has well-aligned timestamps and adjusted speech recognition content. The remaining data is first passed through a pre-trained language identification model Fasttext-lid <cite class="ltx_cite ltx_citemacro_citep">[Joulin et al., <a href="#bib.bib137" title="" class="ltx_ref">2016</a>]</cite> to determine the language category, and then transcribed the non-English text into English using the pretrained Seamless M4T <cite class="ltx_cite ltx_citemacro_citep">[Communication et al., <a href="#bib.bib138" title="" class="ltx_ref">2023</a>]</cite> model. For the text with language identification confidence less than 0.95, we use Vicuna-1.5 as a translation alternative.</p>
</div>
</li>
<li id="A2.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="A2.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="A2.I1.ix4.1.1.m1.1b"><mo id="A2.I1.ix4.1.1.m1.1.1" xref="A2.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="A2.I1.ix4.1.1.m1.1c"><ci id="A2.I1.ix4.1.1.m1.1.1.cmml" xref="A2.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="A2.I1.ix4.p1" class="ltx_para ltx_noindent">
<p id="A2.I1.ix4.p1.1" class="ltx_p"><span id="A2.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Caption Triming &amp; Fusion with LLM.</span> After obtaining the captions of audio, video, and speech from the given video, we utilize an LLM (Vicuna-1.5 <cite class="ltx_cite ltx_citemacro_citep">[Zheng et al., <a href="#bib.bib71" title="" class="ltx_ref">2023</a>]</cite>) to integrate the uni-modal captions into the multimodal ones. To fulfill the request for multiple contrastive objectives, we combine the audio caption with the video caption as the audio-visual caption, as well as integrate the audio, video, and speech captions as the audio-visual-subtitle captions. In this way, we acquire 5 types of captions (3 uni-modal captions (A, V, S) and 2 multi-modal captions (AV, AVS)) for each video automatically. Specifically, we have carefully designed prompt templates (Fig. <a href="#A2.T23" title="Table 23 ‣ B.2 Videos with Audio-Video-Speech Modalities ‣ Appendix B Video-centric Multimodal Data ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>) and employed vLLM <cite class="ltx_cite ltx_citemacro_citep">[Kwon et al., <a href="#bib.bib139" title="" class="ltx_ref">2023</a>]</cite> for inference acceleration, effectively get the visual caption, audio caption, subtitle, audio-visual caption and audio-visual-speech caption while maintaining a natural human-like subtitle style.</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS2.p7" class="ltx_para ltx_noindent">
<p id="A2.SS2.p7.1" class="ltx_p"><span id="A2.SS2.p7.1.1" class="ltx_text ltx_font_bold">Filtering &amp; Sampling.</span> After obtaining the captions, we calculate the CLIP similarity between the video segments and captions. We select the top 60 million data as the video segment data for <span id="A2.SS2.p7.1.2" class="ltx_text ltx_font_bold">MVid</span>. For LAION-2B, we only select samples with CLIP similarity in the top 158 million for training.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Instruction-Tuning Data for Video Dialogues</h3>

<div id="A2.SS3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.p1.1" class="ltx_p">We employ the training part of MVBench <cite class="ltx_cite ltx_citemacro_citep">[Li et al., <a href="#bib.bib23" title="" class="ltx_ref">2023b</a>]</cite>. It comprises 1.9M samples (both images and videos) from 34 distinct sources. They are sampled from the public image / video datasets and reorganized in a standard question-anwser format. This training dataset covers key features of image and video understanding across crucial tasks, including 1) conversation, 2) caption, 3) visual question anwser, 4) reasoning, and 5) classification.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experiments</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Ablations</h3>

<section id="A3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.1.1 </span>How <span id="A3.SS1.SSS1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> Works in Feature-based Tasks.</h4>

<div id="A3.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A3.SS1.SSS1.p1.1" class="ltx_p">We study which part of <span id="A3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.SS1.SSS1.p1.1.1.1" class="ltx_sub"><span id="A3.SS1.SSS1.p1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s1</span></sub></span>’s predictions are suitable for feature-based tasks, i.e. temporal action localition. We adhere the same train and test protocols as in the main paper.</p>
</div>
<div id="A3.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="A3.SS1.SSS1.p2.1" class="ltx_p">In Tab. <a href="#A3.SS1.SSS1" title="C.1.1 How InternVideo2 Works in Feature-based Tasks. ‣ C.1 Ablations ‣ Appendix C Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1.1</span></a>, the most effective feature tends to be located within the last few layers of the video encoder. This observation aligns with the similarity between feature-based temporal tasks and linear probing classification, which is reasonable. We undertake comprehensive experiments to investigate the impact of features from various layers, as detailed in Table <a href="#A3.T24" title="Table 24 ‣ C.1.1 How InternVideo2 Works in Feature-based Tasks. ‣ C.1 Ablations ‣ Appendix C Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>. The results reveal the best features appear between the last 5-th layer and 7-th layer.</p>
</div>
<figure id="A3.T24" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 24: </span>Effect of feature extracted from the last 7 layers.</figcaption>
<div id="A3.T24.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:146.3pt;vertical-align:-6.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.9pt,11.2pt) scale(0.861304720689232,0.861304720689232) ;">
<table id="A3.T24.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A3.T24.1.1.1" class="ltx_tr">
<td id="A3.T24.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Layer Index</td>
<td id="A3.T24.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">THUMOS-14</td>
<td id="A3.T24.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">ActivityNet</td>
<td id="A3.T24.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">HACS Segment</td>
<td id="A3.T24.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">FineAction</td>
</tr>
<tr id="A3.T24.1.1.2" class="ltx_tr">
<td id="A3.T24.1.1.2.1" class="ltx_td ltx_border_r"></td>
<td id="A3.T24.1.1.2.2" class="ltx_td ltx_align_center">1B@mAP</td>
<td id="A3.T24.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r">6B@mAP</td>
<td id="A3.T24.1.1.2.4" class="ltx_td ltx_align_center">1B@mAP</td>
<td id="A3.T24.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r">6B@mAP</td>
<td id="A3.T24.1.1.2.6" class="ltx_td ltx_align_center">1B@mAP</td>
<td id="A3.T24.1.1.2.7" class="ltx_td ltx_align_center ltx_border_r">6B@mAP</td>
<td id="A3.T24.1.1.2.8" class="ltx_td ltx_align_center">1B@mAP</td>
<td id="A3.T24.1.1.2.9" class="ltx_td ltx_align_center">6B@mAP</td>
</tr>
<tr id="A3.T24.1.1.3" class="ltx_tr">
<td id="A3.T24.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-1</td>
<td id="A3.T24.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">67.9</td>
<td id="A3.T24.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.3</td>
<td id="A3.T24.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">39.0</td>
<td id="A3.T24.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.7</td>
<td id="A3.T24.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">39.5</td>
<td id="A3.T24.1.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.1</td>
<td id="A3.T24.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">25.4</td>
<td id="A3.T24.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">25.3</td>
</tr>
<tr id="A3.T24.1.1.4" class="ltx_tr">
<td id="A3.T24.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">-2</td>
<td id="A3.T24.1.1.4.2" class="ltx_td ltx_align_center">68.4</td>
<td id="A3.T24.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">71.0</td>
<td id="A3.T24.1.1.4.4" class="ltx_td ltx_align_center">39.3</td>
<td id="A3.T24.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">40.5</td>
<td id="A3.T24.1.1.4.6" class="ltx_td ltx_align_center">41.0</td>
<td id="A3.T24.1.1.4.7" class="ltx_td ltx_align_center ltx_border_r">42.7</td>
<td id="A3.T24.1.1.4.8" class="ltx_td ltx_align_center">26.2</td>
<td id="A3.T24.1.1.4.9" class="ltx_td ltx_align_center">26.4</td>
</tr>
<tr id="A3.T24.1.1.5" class="ltx_tr">
<td id="A3.T24.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">-3</td>
<td id="A3.T24.1.1.5.2" class="ltx_td ltx_align_center">69.0</td>
<td id="A3.T24.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">71.3</td>
<td id="A3.T24.1.1.5.4" class="ltx_td ltx_align_center">39.7</td>
<td id="A3.T24.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">40.5</td>
<td id="A3.T24.1.1.5.6" class="ltx_td ltx_align_center">41.2</td>
<td id="A3.T24.1.1.5.7" class="ltx_td ltx_align_center ltx_border_r">42.7</td>
<td id="A3.T24.1.1.5.8" class="ltx_td ltx_align_center">27.0</td>
<td id="A3.T24.1.1.5.9" class="ltx_td ltx_align_center">26.6</td>
</tr>
<tr id="A3.T24.1.1.6" class="ltx_tr">
<td id="A3.T24.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">-4</td>
<td id="A3.T24.1.1.6.2" class="ltx_td ltx_align_center">69.3</td>
<td id="A3.T24.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">71.4</td>
<td id="A3.T24.1.1.6.4" class="ltx_td ltx_align_center">39.6</td>
<td id="A3.T24.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r">41.1</td>
<td id="A3.T24.1.1.6.6" class="ltx_td ltx_align_center">41.3</td>
<td id="A3.T24.1.1.6.7" class="ltx_td ltx_align_center ltx_border_r">43.1</td>
<td id="A3.T24.1.1.6.8" class="ltx_td ltx_align_center">27.1</td>
<td id="A3.T24.1.1.6.9" class="ltx_td ltx_align_center">27.1</td>
</tr>
<tr id="A3.T24.1.1.7" class="ltx_tr">
<td id="A3.T24.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">-5</td>
<td id="A3.T24.1.1.7.2" class="ltx_td ltx_align_center"><span id="A3.T24.1.1.7.2.1" class="ltx_text ltx_font_bold">69.9</span></td>
<td id="A3.T24.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">71.8</td>
<td id="A3.T24.1.1.7.4" class="ltx_td ltx_align_center">39.7</td>
<td id="A3.T24.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r">40.9</td>
<td id="A3.T24.1.1.7.6" class="ltx_td ltx_align_center"><span id="A3.T24.1.1.7.6.1" class="ltx_text ltx_font_bold">41.4</span></td>
<td id="A3.T24.1.1.7.7" class="ltx_td ltx_align_center ltx_border_r">43.1</td>
<td id="A3.T24.1.1.7.8" class="ltx_td ltx_align_center"><span id="A3.T24.1.1.7.8.1" class="ltx_text ltx_font_bold">27.2</span></td>
<td id="A3.T24.1.1.7.9" class="ltx_td ltx_align_center"><span id="A3.T24.1.1.7.9.1" class="ltx_text ltx_font_bold">27.7</span></td>
</tr>
<tr id="A3.T24.1.1.8" class="ltx_tr">
<td id="A3.T24.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r">-6</td>
<td id="A3.T24.1.1.8.2" class="ltx_td ltx_align_center">69.6</td>
<td id="A3.T24.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="A3.T24.1.1.8.3.1" class="ltx_text ltx_font_bold">72.0</span></td>
<td id="A3.T24.1.1.8.4" class="ltx_td ltx_align_center">39.6</td>
<td id="A3.T24.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="A3.T24.1.1.8.5.1" class="ltx_text ltx_font_bold">41.2</span></td>
<td id="A3.T24.1.1.8.6" class="ltx_td ltx_align_center">40.7</td>
<td id="A3.T24.1.1.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="A3.T24.1.1.8.7.1" class="ltx_text ltx_font_bold">43.3</span></td>
<td id="A3.T24.1.1.8.8" class="ltx_td ltx_align_center">27.0</td>
<td id="A3.T24.1.1.8.9" class="ltx_td ltx_align_center">27.7</td>
</tr>
<tr id="A3.T24.1.1.9" class="ltx_tr">
<td id="A3.T24.1.1.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-7</td>
<td id="A3.T24.1.1.9.2" class="ltx_td ltx_align_center ltx_border_bb">69.5</td>
<td id="A3.T24.1.1.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">71.9</td>
<td id="A3.T24.1.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T24.1.1.9.4.1" class="ltx_text ltx_font_bold">40.0</span></td>
<td id="A3.T24.1.1.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">41.1</td>
<td id="A3.T24.1.1.9.6" class="ltx_td ltx_align_center ltx_border_bb">40.6</td>
<td id="A3.T24.1.1.9.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">42.8</td>
<td id="A3.T24.1.1.9.8" class="ltx_td ltx_align_center ltx_border_bb">26.9</td>
<td id="A3.T24.1.1.9.9" class="ltx_td ltx_align_center ltx_border_bb">27.7</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Video Retrieval</h3>

<div id="A3.SS2.p1" class="ltx_para ltx_noindent">
<p id="A3.SS2.p1.1" class="ltx_p">We detail R@1, R@5, and R@10 of zero-shot video retrieval from <span id="A3.SS2.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> in Tab. <a href="#A3.T25.st3" title="In Table 25 ‣ C.2 Video Retrieval ‣ Appendix C Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25(c)</span></a>
-<a href="#A3.T25.st6" title="In Table 25 ‣ C.2 Video Retrieval ‣ Appendix C Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25(f)</span></a> for reference.</p>
</div>
<figure id="A3.T25" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 25: </span>Video retrieval results on MSR-VTT, DiDeMo, LSMDC, ActivityNet, VATEX, and MSVD. We report R@1, R@5, and R@10. #F denotes input frame number in eval.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.T25.st3" class="ltx_table ltx_figure_panel">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>MSR-VTT</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st3.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:178.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(85.7pt,-35.3pt) scale(1.65393041946622,1.65393041946622) ;">
<table id="A3.T25.st3.4.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st3.4.4.5" class="ltx_tr">
<td id="A3.T25.st3.4.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.4.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st3.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.4.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st3.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st3.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st3.4.4.6" class="ltx_tr">
<td id="A3.T25.st3.4.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.4.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.4.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st3.4.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.4.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.4.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st3.1.1.1" class="ltx_tr">
<td id="A3.T25.st3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.1.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st3.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">51.9</td>
<td id="A3.T25.st3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">74.6</td>
<td id="A3.T25.st3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">81.7</td>
<td id="A3.T25.st3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">49.6</td>
<td id="A3.T25.st3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">73.6</td>
<td id="A3.T25.st3.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">81.2</td>
</tr>
<tr id="A3.T25.st3.2.2.2" class="ltx_tr">
<td id="A3.T25.st3.2.2.2.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.2.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st3.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.2.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.2.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">51.9</td>
<td id="A3.T25.st3.2.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">75.3</td>
<td id="A3.T25.st3.2.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">82.5</td>
<td id="A3.T25.st3.2.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">50.9</td>
<td id="A3.T25.st3.2.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">73.4</td>
<td id="A3.T25.st3.2.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">81.8</td>
</tr>
<tr id="A3.T25.st3.3.3.3" class="ltx_tr">
<td id="A3.T25.st3.3.3.3.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.3.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.3.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st3.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.3.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.3.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">54.5</td>
<td id="A3.T25.st3.3.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">77.5</td>
<td id="A3.T25.st3.3.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.7</td>
<td id="A3.T25.st3.3.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">52.3</td>
<td id="A3.T25.st3.3.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">75.3</td>
<td id="A3.T25.st3.3.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.5</td>
</tr>
<tr id="A3.T25.st3.4.4.4" class="ltx_tr">
<td id="A3.T25.st3.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.4.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.4.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st3.4.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">55.9</td>
<td id="A3.T25.st3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">78.3</td>
<td id="A3.T25.st3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">85.1</td>
<td id="A3.T25.st3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">53.7</td>
<td id="A3.T25.st3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">77.5</td>
<td id="A3.T25.st3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">84.1</td>
</tr>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>LSMDC</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st3.8" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:152.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.9pt,-22.1pt) scale(1.40883287125328,1.40883287125328) ;">
<table id="A3.T25.st3.8.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st3.8.4.5" class="ltx_tr">
<td id="A3.T25.st3.8.4.5.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.8.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st3.8.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.8.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st3.8.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st3.8.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st3.8.4.6" class="ltx_tr">
<td id="A3.T25.st3.8.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.8.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.8.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st3.8.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.8.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.8.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st3.5.1.1" class="ltx_tr">
<td id="A3.T25.st3.5.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.5.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.5.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st3.5.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.5.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">31.5</td>
<td id="A3.T25.st3.5.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">51.3</td>
<td id="A3.T25.st3.5.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">59.5</td>
<td id="A3.T25.st3.5.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">27.1</td>
<td id="A3.T25.st3.5.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">44.8</td>
<td id="A3.T25.st3.5.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">51.8</td>
</tr>
<tr id="A3.T25.st3.6.2.2" class="ltx_tr">
<td id="A3.T25.st3.6.2.2.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.6.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.6.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st3.6.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.6.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.6.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">32.0</td>
<td id="A3.T25.st3.6.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">52.4</td>
<td id="A3.T25.st3.6.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">59.4</td>
<td id="A3.T25.st3.6.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">27.3</td>
<td id="A3.T25.st3.6.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">44.2</td>
<td id="A3.T25.st3.6.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">51.6</td>
</tr>
<tr id="A3.T25.st3.7.3.3" class="ltx_tr">
<td id="A3.T25.st3.7.3.3.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.7.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.7.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st3.7.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.7.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.7.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">34.8</td>
<td id="A3.T25.st3.7.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">54.0</td>
<td id="A3.T25.st3.7.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">61.6</td>
<td id="A3.T25.st3.7.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">30.1</td>
<td id="A3.T25.st3.7.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">48.0</td>
<td id="A3.T25.st3.7.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">55.0</td>
</tr>
<tr id="A3.T25.st3.8.4.4" class="ltx_tr">
<td id="A3.T25.st3.8.4.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.8.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.8.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st3.8.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.8.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.8.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">33.8</td>
<td id="A3.T25.st3.8.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">55.9</td>
<td id="A3.T25.st3.8.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">62.2</td>
<td id="A3.T25.st3.8.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">30.1</td>
<td id="A3.T25.st3.8.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">47.7</td>
<td id="A3.T25.st3.8.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">54.8</td>
</tr>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(c) </span>VATEX</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st3.12" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:153pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.8pt,-22.5pt) scale(1.41676025246879,1.41676025246879) ;">
<table id="A3.T25.st3.12.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st3.12.4.5" class="ltx_tr">
<td id="A3.T25.st3.12.4.5.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.12.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st3.12.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st3.12.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st3.12.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st3.12.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st3.12.4.6" class="ltx_tr">
<td id="A3.T25.st3.12.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.12.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.12.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st3.12.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st3.12.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st3.12.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st3.9.1.1" class="ltx_tr">
<td id="A3.T25.st3.9.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.9.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.9.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st3.9.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.9.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.9.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">70.7</td>
<td id="A3.T25.st3.9.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">93.7</td>
<td id="A3.T25.st3.9.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">96.9</td>
<td id="A3.T25.st3.9.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">85.9</td>
<td id="A3.T25.st3.9.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">97.6</td>
<td id="A3.T25.st3.9.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">99.2</td>
</tr>
<tr id="A3.T25.st3.10.2.2" class="ltx_tr">
<td id="A3.T25.st3.10.2.2.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.10.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.10.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st3.10.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st3.10.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.10.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">70.4</td>
<td id="A3.T25.st3.10.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">93.4</td>
<td id="A3.T25.st3.10.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">96.9</td>
<td id="A3.T25.st3.10.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">85.4</td>
<td id="A3.T25.st3.10.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">97.6</td>
<td id="A3.T25.st3.10.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">99.1</td>
</tr>
<tr id="A3.T25.st3.11.3.3" class="ltx_tr">
<td id="A3.T25.st3.11.3.3.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.11.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.11.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st3.11.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.11.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st3.11.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">71.1</td>
<td id="A3.T25.st3.11.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">93.8</td>
<td id="A3.T25.st3.11.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">97.0</td>
<td id="A3.T25.st3.11.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">85.2</td>
<td id="A3.T25.st3.11.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">97.7</td>
<td id="A3.T25.st3.11.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">99.4</td>
</tr>
<tr id="A3.T25.st3.12.4.4" class="ltx_tr">
<td id="A3.T25.st3.12.4.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st3.12.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st3.12.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st3.12.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st3.12.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st3.12.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">71.5</td>
<td id="A3.T25.st3.12.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">94.0</td>
<td id="A3.T25.st3.12.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">97.1</td>
<td id="A3.T25.st3.12.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">85.3</td>
<td id="A3.T25.st3.12.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">97.9</td>
<td id="A3.T25.st3.12.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">99.3</td>
</tr>
</table>
</span></div>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A3.T25.st6" class="ltx_table ltx_figure_panel">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(d) </span>DiDeMo</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st6.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:178.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(85.7pt,-35.3pt) scale(1.65393041946622,1.65393041946622) ;">
<table id="A3.T25.st6.4.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st6.4.4.5" class="ltx_tr">
<td id="A3.T25.st6.4.4.5.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.4.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st6.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.4.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st6.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st6.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st6.4.4.6" class="ltx_tr">
<td id="A3.T25.st6.4.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.4.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.4.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st6.4.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.4.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.4.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st6.1.1.1" class="ltx_tr">
<td id="A3.T25.st6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.1.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st6.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">56.7</td>
<td id="A3.T25.st6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">78.7</td>
<td id="A3.T25.st6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">83.9</td>
<td id="A3.T25.st6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">54.4</td>
<td id="A3.T25.st6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">74.4</td>
<td id="A3.T25.st6.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">80.6</td>
</tr>
<tr id="A3.T25.st6.2.2.2" class="ltx_tr">
<td id="A3.T25.st6.2.2.2.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.2.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.2.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st6.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.2.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.2.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">57.0</td>
<td id="A3.T25.st6.2.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">80.0</td>
<td id="A3.T25.st6.2.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">85.1</td>
<td id="A3.T25.st6.2.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">54.3</td>
<td id="A3.T25.st6.2.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">77.2</td>
<td id="A3.T25.st6.2.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.5</td>
</tr>
<tr id="A3.T25.st6.3.3.3" class="ltx_tr">
<td id="A3.T25.st6.3.3.3.1" class="ltx_td ltx_align_left" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.3.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.3.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st6.3.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.3.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.3.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">56.2</td>
<td id="A3.T25.st6.3.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">77.6</td>
<td id="A3.T25.st6.3.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.6</td>
<td id="A3.T25.st6.3.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">53.2</td>
<td id="A3.T25.st6.3.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">76.8</td>
<td id="A3.T25.st6.3.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">82.7</td>
</tr>
<tr id="A3.T25.st6.4.4.4" class="ltx_tr">
<td id="A3.T25.st6.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.4.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.4.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st6.4.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">57.9</td>
<td id="A3.T25.st6.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">80.0</td>
<td id="A3.T25.st6.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">84.6</td>
<td id="A3.T25.st6.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">57.1</td>
<td id="A3.T25.st6.4.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">79.9</td>
<td id="A3.T25.st6.4.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">85.0</td>
</tr>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(e) </span>ActivityNet</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st6.8" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:155.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(66.0pt,-23.6pt) scale(1.43763669584345,1.43763669584345) ;">
<table id="A3.T25.st6.8.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st6.8.4.5" class="ltx_tr">
<td id="A3.T25.st6.8.4.5.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.8.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st6.8.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.8.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st6.8.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st6.8.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st6.8.4.6" class="ltx_tr">
<td id="A3.T25.st6.8.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.8.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.8.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st6.8.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.8.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.8.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st6.5.1.1" class="ltx_tr">
<td id="A3.T25.st6.5.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.5.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.5.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st6.5.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.5.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">56.9</td>
<td id="A3.T25.st6.5.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">81.7</td>
<td id="A3.T25.st6.5.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">89.8</td>
<td id="A3.T25.st6.5.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">53.6</td>
<td id="A3.T25.st6.5.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">80.0</td>
<td id="A3.T25.st6.5.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">88.5</td>
</tr>
<tr id="A3.T25.st6.6.2.2" class="ltx_tr">
<td id="A3.T25.st6.6.2.2.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.6.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.6.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st6.6.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.6.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.6.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">60.4</td>
<td id="A3.T25.st6.6.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.9</td>
<td id="A3.T25.st6.6.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">90.8</td>
<td id="A3.T25.st6.6.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">54.8</td>
<td id="A3.T25.st6.6.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">81.5</td>
<td id="A3.T25.st6.6.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">89.5</td>
</tr>
<tr id="A3.T25.st6.7.3.3" class="ltx_tr">
<td id="A3.T25.st6.7.3.3.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.7.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.7.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st6.7.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.7.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.7.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">59.4</td>
<td id="A3.T25.st6.7.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.2</td>
<td id="A3.T25.st6.7.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">90.3</td>
<td id="A3.T25.st6.7.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">53.7</td>
<td id="A3.T25.st6.7.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">80.5</td>
<td id="A3.T25.st6.7.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">88.9</td>
</tr>
<tr id="A3.T25.st6.8.4.4" class="ltx_tr">
<td id="A3.T25.st6.8.4.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.8.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.8.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st6.8.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.8.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.8.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">63.2</td>
<td id="A3.T25.st6.8.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">85.6</td>
<td id="A3.T25.st6.8.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">92.5</td>
<td id="A3.T25.st6.8.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">56.5</td>
<td id="A3.T25.st6.8.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">82.8</td>
<td id="A3.T25.st6.8.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">90.3</td>
</tr>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(f) </span>MSVD</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A3.T25.st6.12" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:153.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.9pt,-22.6pt) scale(1.41778968909798,1.41778968909798) ;">
<table id="A3.T25.st6.12.4" class="ltx_tabular ltx_align_middle">
<tr id="A3.T25.st6.12.4.5" class="ltx_tr">
<td id="A3.T25.st6.12.4.5.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.12.4.5.1.1" class="ltx_text">Method</span></td>
<td id="A3.T25.st6.12.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" rowspan="2"><span id="A3.T25.st6.12.4.5.2.1" class="ltx_text">#F</span></td>
<td id="A3.T25.st6.12.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Text-to-Video</td>
<td id="A3.T25.st6.12.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Video-to-Text</td>
</tr>
<tr id="A3.T25.st6.12.4.6" class="ltx_tr">
<td id="A3.T25.st6.12.4.6.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.12.4.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.12.4.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
<td id="A3.T25.st6.12.4.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@1</td>
<td id="A3.T25.st6.12.4.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@5</td>
<td id="A3.T25.st6.12.4.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">R@10</td>
</tr>
<tr id="A3.T25.st6.9.1.1" class="ltx_tr">
<td id="A3.T25.st6.9.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.9.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.9.1.1.1.1.1" class="ltx_sub"><span id="A3.T25.st6.9.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.9.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.9.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">58.9</td>
<td id="A3.T25.st6.9.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">83.0</td>
<td id="A3.T25.st6.9.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">88.7</td>
<td id="A3.T25.st6.9.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">83.6</td>
<td id="A3.T25.st6.9.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">94.8</td>
<td id="A3.T25.st6.9.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 2.0pt;">97.0</td>
</tr>
<tr id="A3.T25.st6.10.2.2" class="ltx_tr">
<td id="A3.T25.st6.10.2.2.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.10.2.2.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.10.2.2.1.1.1" class="ltx_sub"><span id="A3.T25.st6.10.2.2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-1B</td>
<td id="A3.T25.st6.10.2.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.10.2.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">58.1</td>
<td id="A3.T25.st6.10.2.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.0</td>
<td id="A3.T25.st6.10.2.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">88.4</td>
<td id="A3.T25.st6.10.2.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">83.3</td>
<td id="A3.T25.st6.10.2.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">94.3</td>
<td id="A3.T25.st6.10.2.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">96.9</td>
</tr>
<tr id="A3.T25.st6.11.3.3" class="ltx_tr">
<td id="A3.T25.st6.11.3.3.1" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.11.3.3.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.11.3.3.1.1.1" class="ltx_sub"><span id="A3.T25.st6.11.3.3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.11.3.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">4</td>
<td id="A3.T25.st6.11.3.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">59.8</td>
<td id="A3.T25.st6.11.3.3.4" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">84.2</td>
<td id="A3.T25.st6.11.3.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">89.7</td>
<td id="A3.T25.st6.11.3.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">82.5</td>
<td id="A3.T25.st6.11.3.3.7" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">94.6</td>
<td id="A3.T25.st6.11.3.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 2.0pt;">97.2</td>
</tr>
<tr id="A3.T25.st6.12.4.4" class="ltx_tr">
<td id="A3.T25.st6.12.4.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">
<span id="A3.T25.st6.12.4.4.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T25.st6.12.4.4.1.1.1" class="ltx_sub"><span id="A3.T25.st6.12.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s2</span></sub></span>-6B</td>
<td id="A3.T25.st6.12.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">8</td>
<td id="A3.T25.st6.12.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">59.3</td>
<td id="A3.T25.st6.12.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">84.4</td>
<td id="A3.T25.st6.12.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">89.6</td>
<td id="A3.T25.st6.12.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">83.1</td>
<td id="A3.T25.st6.12.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">94.2</td>
<td id="A3.T25.st6.12.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 2.0pt;">97.0</td>
</tr>
</table>
</span></div>
</div>
</div>
</figure>
</div>
</div>
</figure>
<figure id="A3.T26" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 26: </span>Performance of multimodal LLMs on different question types and scenes of MoVQA.
</figcaption>
<div id="A3.T26.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:553.3pt;height:65.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-184.4pt,21.6pt) scale(0.6,0.6) ;">
<table id="A3.T26.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A3.T26.1.1.2" class="ltx_tr">
<td id="A3.T26.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="A3.T26.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Backbone</td>
<td id="A3.T26.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">Synopsis</td>
<td id="A3.T26.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">Temporal</td>
<td id="A3.T26.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">Spatial</td>
<td id="A3.T26.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">Causal</td>
<td id="A3.T26.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt">Hypothetical</td>
<td id="A3.T26.1.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Knowledge</td>
<td id="A3.T26.1.1.2.9" class="ltx_td ltx_align_center ltx_border_tt">Single-Scene</td>
<td id="A3.T26.1.1.2.10" class="ltx_td ltx_align_center ltx_border_tt">Multi-Scene</td>
<td id="A3.T26.1.1.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Full-Scene</td>
<td id="A3.T26.1.1.2.12" class="ltx_td ltx_align_center ltx_border_tt">Overall</td>
</tr>
<tr id="A3.T26.1.1.3" class="ltx_tr">
<td id="A3.T26.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mplug-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. [<a href="#bib.bib134" title="" class="ltx_ref">2023b</a>]</cite>
</td>
<td id="A3.T26.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP ViT-L</td>
<td id="A3.T26.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">25.1</td>
<td id="A3.T26.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">19.9</td>
<td id="A3.T26.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">25.3</td>
<td id="A3.T26.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">21.9</td>
<td id="A3.T26.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">23.5</td>
<td id="A3.T26.1.1.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.5</td>
<td id="A3.T26.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">25.2</td>
<td id="A3.T26.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">23.5</td>
<td id="A3.T26.1.1.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.1</td>
<td id="A3.T26.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">24.8</td>
</tr>
<tr id="A3.T26.1.1.4" class="ltx_tr">
<td id="A3.T26.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Otter <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib140" title="" class="ltx_ref">2023f</a>]</cite>
</td>
<td id="A3.T26.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">CLIP ViT-L</td>
<td id="A3.T26.1.1.4.3" class="ltx_td ltx_align_center">22.6</td>
<td id="A3.T26.1.1.4.4" class="ltx_td ltx_align_center">20.7</td>
<td id="A3.T26.1.1.4.5" class="ltx_td ltx_align_center">19.6</td>
<td id="A3.T26.1.1.4.6" class="ltx_td ltx_align_center">26.1</td>
<td id="A3.T26.1.1.4.7" class="ltx_td ltx_align_center">24.2</td>
<td id="A3.T26.1.1.4.8" class="ltx_td ltx_align_center ltx_border_r">21.8</td>
<td id="A3.T26.1.1.4.9" class="ltx_td ltx_align_center">23.1</td>
<td id="A3.T26.1.1.4.10" class="ltx_td ltx_align_center">22.1</td>
<td id="A3.T26.1.1.4.11" class="ltx_td ltx_align_center ltx_border_r">21.3</td>
<td id="A3.T26.1.1.4.12" class="ltx_td ltx_align_center">22.6</td>
</tr>
<tr id="A3.T26.1.1.5" class="ltx_tr">
<td id="A3.T26.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">VideoChatGPT <cite class="ltx_cite ltx_citemacro_cite">Maaz et al. [<a href="#bib.bib141" title="" class="ltx_ref">2023b</a>]</cite>
</td>
<td id="A3.T26.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">CLIP ViT-L</td>
<td id="A3.T26.1.1.5.3" class="ltx_td ltx_align_center">23.8</td>
<td id="A3.T26.1.1.5.4" class="ltx_td ltx_align_center">20.2</td>
<td id="A3.T26.1.1.5.5" class="ltx_td ltx_align_center">22.1</td>
<td id="A3.T26.1.1.5.6" class="ltx_td ltx_align_center">22.1</td>
<td id="A3.T26.1.1.5.7" class="ltx_td ltx_align_center">21.4</td>
<td id="A3.T26.1.1.5.8" class="ltx_td ltx_align_center ltx_border_r">24.1</td>
<td id="A3.T26.1.1.5.9" class="ltx_td ltx_align_center">23.4</td>
<td id="A3.T26.1.1.5.10" class="ltx_td ltx_align_center">22.7</td>
<td id="A3.T26.1.1.5.11" class="ltx_td ltx_align_center ltx_border_r">22.3</td>
<td id="A3.T26.1.1.5.12" class="ltx_td ltx_align_center">22.9</td>
</tr>
<tr id="A3.T26.1.1.6" class="ltx_tr">
<td id="A3.T26.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">VideoChat <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib81" title="" class="ltx_ref">2023d</a>]</cite>
</td>
<td id="A3.T26.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">Eva-g</td>
<td id="A3.T26.1.1.6.3" class="ltx_td ltx_align_center">33.6</td>
<td id="A3.T26.1.1.6.4" class="ltx_td ltx_align_center">24.3</td>
<td id="A3.T26.1.1.6.5" class="ltx_td ltx_align_center">34.5</td>
<td id="A3.T26.1.1.6.6" class="ltx_td ltx_align_center">36.6</td>
<td id="A3.T26.1.1.6.7" class="ltx_td ltx_align_center">35.5</td>
<td id="A3.T26.1.1.6.8" class="ltx_td ltx_align_center ltx_border_r">32.0</td>
<td id="A3.T26.1.1.6.9" class="ltx_td ltx_align_center">35.3</td>
<td id="A3.T26.1.1.6.10" class="ltx_td ltx_align_center">32.9</td>
<td id="A3.T26.1.1.6.11" class="ltx_td ltx_align_center ltx_border_r">33.3</td>
<td id="A3.T26.1.1.6.12" class="ltx_td ltx_align_center">34.7</td>
</tr>
<tr id="A3.T26.1.1.1" class="ltx_tr">
<td id="A3.T26.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">VideoChat2</td>
<td id="A3.T26.1.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="A3.T26.1.1.1.1.1" class="ltx_text ltx_font_bold">InternVideo2<sub id="A3.T26.1.1.1.1.1.1" class="ltx_sub"><span id="A3.T26.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">s3</span></sub></span></td>
<td id="A3.T26.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.3.1" class="ltx_text ltx_font_bold">42.6</span></td>
<td id="A3.T26.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.4.1" class="ltx_text ltx_font_bold">27.8</span></td>
<td id="A3.T26.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.5.1" class="ltx_text ltx_font_bold">39.9</span></td>
<td id="A3.T26.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.6.1" class="ltx_text ltx_font_bold">44.3</span></td>
<td id="A3.T26.1.1.1.7" class="ltx_td ltx_align_center ltx_border_bb">4<span id="A3.T26.1.1.1.7.1" class="ltx_text ltx_font_bold">42.5</span>
</td>
<td id="A3.T26.1.1.1.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="A3.T26.1.1.1.8.1" class="ltx_text ltx_font_bold">41.2</span></td>
<td id="A3.T26.1.1.1.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.9.1" class="ltx_text ltx_font_bold">40.9</span></td>
<td id="A3.T26.1.1.1.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.10.1" class="ltx_text ltx_font_bold">39.3</span></td>
<td id="A3.T26.1.1.1.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="A3.T26.1.1.1.11.1" class="ltx_text ltx_font_bold">38.6</span></td>
<td id="A3.T26.1.1.1.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="A3.T26.1.1.1.12.1" class="ltx_text ltx_font_bold">40.1</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Movie Understanding.</h3>

<div id="A3.SS3.p1" class="ltx_para ltx_noindent">
<p id="A3.SS3.p1.1" class="ltx_p">We evaluate <span id="A3.SS3.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> on the MoVQA for movie understanding. It is a long-form movie question-answering dataset<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. [<a href="#bib.bib132" title="" class="ltx_ref">2023b</a>]</cite>. MoVQA assesses the diverse cognitive capabilities of multimodal systems by considering both video length and clue length, relying on multi-level temporal lengths (single-scene, multi-scene and full-scene).
There are six types of QAs, including information synopsis, temporal perception, spatial perception, causal reasoning, hypothetical reasoning and external knowledge.
We evaluate <span id="A3.SS3.p1.1.2" class="ltx_text ltx_font_bold">InternVideo2</span>-6B in the form of open-ended QAs, and detailed results are shown on Table <a href="#A3.T26" title="Table 26 ‣ C.2 Video Retrieval ‣ Appendix C Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">26</span></a>.</p>
</div>
</section>
<section id="A3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Video-Centric Dialogue System</h3>

<div id="A3.SS4.p1" class="ltx_para ltx_noindent">
<p id="A3.SS4.p1.1" class="ltx_p">We conduct qualitative evaluations of <span id="A3.SS4.p1.1.1" class="ltx_text ltx_font_bold">InternVideo2</span> on several challenging tasks (most videos are from Perception Test <cite class="ltx_cite ltx_citemacro_cite">Patraucean et al. [<a href="#bib.bib142" title="" class="ltx_ref">2024</a>]</cite>) such as action sequence (Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), confused action (Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), temporal order understanding (Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), temporal event counting (Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, unexpected action reasoning (Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), and vision-language navigation (Figure <a href="#S5.F9" title="Figure 9 ‣ 5.2.5 Audio-related Tasks ‣ 5.2 Video-Language Tasks ‣ 5 Experiments ‣ InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Compared to other models, our model provides more concise and accurate answers.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="David S. Hippocampus, Elias D. Striatum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Video representation, Multimodal learning, "></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="q-bio.NC, q-bio.QM"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="A template for the arxiv style"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.15376" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.15377" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.15377">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.15377" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.15378" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:39:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
