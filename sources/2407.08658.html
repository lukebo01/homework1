<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.08658] Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks</title><meta property="og:description" content="This paper presents the development and comparative evaluation of three voice command pipelines for controlling a Tello drone, using speech recognition and deep learning techniques. The aim is to enhance human-machine …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.08658">

<!--Generated on Mon Aug  5 19:07:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Lucca Emmanuel Pineli Simões 
<br class="ltx_break">Instituto de Informática (INF)
<br class="ltx_break">Universidade Federal de Goiás
<br class="ltx_break">Goiânia, Brazil 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">lucca.pineli@discente.ufg.br</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Lucas Brandão Rodrigues 
<br class="ltx_break">Instituto de Informática (INF)
<br class="ltx_break">Universidade Federal de Goiás
<br class="ltx_break">Goiânia, Brazil 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">brandao.brandao@discente.ufg.br</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Rafaela Mota Silva 
<br class="ltx_break">Instituto de Informática (INF)
<br class="ltx_break">Universidade Federal de Goiás
<br class="ltx_break">Goiânia, Brazil 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">rafaelamota@discente.ufg.br</span> 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_ERROR undefined">\And</span>Gustavo Rodrigues da Silva 
<br class="ltx_break">Instituto de Informática (INF)
<br class="ltx_break">Universidade Federal de Goiás
<br class="ltx_break">Goiânia, Brazil 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">rodrigues_da@discente.ufg.br</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">This paper presents the development and comparative evaluation of three voice command pipelines for controlling a Tello drone, using speech recognition and deep learning techniques. The aim is to enhance human-machine interaction by enabling intuitive voice control of drone actions. The pipelines developed include: (1) a traditional Speech-to-Text (STT) followed by a Large Language Model (LLM) approach, (2) a direct voice-to-function mapping model, and (3) a Siamese neural network-based system. Each pipeline was evaluated based on inference time, accuracy, efficiency, and flexibility. Detailed methodologies, dataset preparation, and evaluation metrics are provided, offering a comprehensive analysis of each pipeline’s strengths and applicability across different scenarios.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.7" class="ltx_p"><em id="p1.7.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.7.2" class="ltx_text ltx_font_bold">eywords</span> Command Mapping  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Drone Control  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Function Calling  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
LLM  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
NLP  <math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
Siamese Networks  <math id="p1.6.m6.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation></semantics></math>
Speech Recognition  <math id="p1.7.m7.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation></semantics></math>
STT</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The integration of automation and voice control in drone systems has received significant attention in recent research, driven by the need for more intuitive and efficient human-machine interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This project focuses on developing a voice command system for the Tello drone, utilizing speech recognition and deep learning models to translate voice commands into precise drone actions.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">The primary challenge addressed by this project is the accurate and efficient translation of voice commands into specific drone operations. This is particularly crucial in scenarios where traditional control interfaces are impractical or where operators require hands-free operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To address this challenge, we developed and evaluated three distinct pipelines. The first pipeline uses a traditional Speech-to-Text (STT) model followed by a Large Language Model (LLM) for command interpretation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The second pipeline involves a direct mapping model that predicts drone commands from audio inputs without intermediate text conversion. The third pipeline employs a Siamese neural network to generalize new commands by comparing audio inputs to pre-trained examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Each pipeline was designed to balance performance, flexibility, and ease of maintenance. The methodologies employed include speech recognition techniques to convert audio to text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, natural language processing (NLP) for command analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and neural network models for direct audio-to-command mapping and similarity-based command recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The pipelines’ effectiveness was evaluated based on accuracy, inference time, and the system’s ability to generalize to new commands. The dataset for this project was prepared by recording a variety of voice commands, ensuring diversity in speech patterns and environmental conditions. Data augmentation techniques were applied to enhance model robustness. Evaluation metrics included precision, recall, F1-score, and inference time, providing a comprehensive assessment of each pipeline’s performance.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">This paper presents a comparative analysis of three voice command pipelines for drone control, highlighting their strengths and potential applications in various operational contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">The purpose of the dataset is to provide comprehensive samples of voice commands for controlling the drone. Each command, such as moving the drone "right," "left," "forward," "backward," "up," and "down," was recorded multiple times to capture variations in pronunciation and intonation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Augmentation</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">The augmentation step employed various techniques to enhance model robustness and artificially expand the dataset, making it five times larger. This expansion allowed the models to learn more effectively and generalize across diverse scenarios, thereby improving the overall performance and reliability of the voice command system. The data augmentation techniques used were:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Noise Addition</span>: Simulating different environmental sounds to mimic real-world conditions.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Tanh Distortion</span>: Utilizing Tanh activation to normalize audio signals.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Masking</span>: Applying temporal and frequency masking to obscure parts of the audio, promoting generalization.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Pitch-Shifting</span>: Altering the pitch to represent various vocal tones.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">These techniques ensured that the models could generalize well across different audio conditions. After applying data augmentation, the dataset sizes for each class were increased significantly. The relevant numbers of samples for each class before and after data augmentation are shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Data Augmentation ‣ 2 Dataset ‣ Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Without Data Augmentation</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">With Data Augmentation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">RIGHT</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">211</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1055</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">BACKWARD</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">205</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">1025</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">FORWARD</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">202</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">1010</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">LEFT</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">202</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">1010</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">UP</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">193</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">965</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<th id="S2.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">DOWN</th>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">177</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">885</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of samples per class before and after data augmentation.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The development of the voice control system for the Tello drone involved the design and evaluation of three distinct pipelines, each with a specific approach to mapping voice commands to drone actions. The selection and development process of these pipelines were driven by the need to balance performance, flexibility, and ease of maintenance. All three pipelines follow a general workflow to control the Tello drone using voice commands, which involves capturing audio, preprocessing it, and mapping it to specific drone commands, which are then executed. The steps are as follows:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Voice Recording</span>: Audio is recorded using integrated or external microphones. This step ensures high-quality input suitable for further processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Processing</span>: The captured audio is pre-processed to remove noise and improve signal clarity. This step includes padding the waveform to a consistent length, which is crucial for maintaining uniformity across inputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Model Application</span>: Each pipeline utilizes a different approach to interpret the pre-processed audio and map it to drone commands. The specifics of this step vary across the pipelines:</p>
<ul id="S3.I1.i3.I1" class="ltx_itemize">
<li id="S3.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i3.I1.i1.p1.1" class="ltx_p">Pipeline 1 uses a Speech-to-Text (STT) model followed by a Large Language Model (LLM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i3.I1.i2.p1.1" class="ltx_p">Pipeline 2 employs a direct sequence classification model to predict commands from audio inputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i3.I1.i3.p1.1" class="ltx_p">Pipeline 3 utilizes a Siamese neural network to compare audio commands with pre-trained examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Function Call</span>: The identified or predicted command is then mapped to a specific function that is executed by the drone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">We began with Pipeline 1, as it represents the "standard" approach for this type of problem, utilizing Automatic Speech Recognition (ASR) followed by a Large Language Model (LLM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This approach allowed us to transform voice commands into text and then map that text to specific drone commands. However, we identified opportunities to improve performance and reduce latency by streamlining the process.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">This realization led us to develop Pipeline 2, which employs a direct voice-to-function approach. Although this approach is more efficient in terms of latency, it proved less flexible. While ASR and an LLM enable easy addition of new functions, our direct classification model faced challenges in accommodating new commands without significant modifications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">To address these limitations, we devised Pipeline 3, which utilizes Siamese neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Siamese networks facilitate the introduction of new functions at a low cost, without necessitating a complete retraining of the model. They project voice commands into a latent space, where similar commands are grouped closely together. This method emerged as the most flexible and efficient solution, offering an ideal balance between performance, flexibility, and ease of maintenance.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preprocessing</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Preprocessing involves several key steps to ensure uniformity and compatibility of the audio data with the models used in all three pipelines. These steps include loading the audio files, standardizing the length of the waveforms, and extracting features to create input values compatible with the Wav2Vec2 model. In all three pipelines, the audio processing involves the following steps:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Padding</span>: Ensuring all waveforms have the same length by adding zeros to the end of waveforms that are shorter than the target length <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Feature Extraction</span>: Loading the audio files, standardizing the length of the waveform, and processing the waveform to obtain input values compatible with the Wav2Vec2 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Batch Padding</span>: Padding the input values and labels to ensure that all sequences in a batch have the same length <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">These steps ensure that the audio data is uniformly processed and compatible with the Wav2Vec2 model used in all pipelines. Additionally, in Pipeline 3, an extra method is implemented to select pairs of audio samples based on the similarity of their classes for audio comparison tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pipeline 1: STT and LLM</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.08658/assets/fig/pipeline1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the STT and LLM pipeline</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The first approach to controlling the Tello drone through voice commands involves converting spoken language into written text using Speech-to-Text (STT) technology, followed by interpreting the text with a Large Language Model (LLM) to generate the appropriate drone commands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Speech recognition, or STT, serves as the cornerstone of this pipeline. Our system leverages the "facebook/wav2vec2-large-xlsr-53-portuguese" model developed by Facebook. This model is renowned for its effectiveness in handling sequential data and its robustness against noise and variations in speech. Initially, the model undergoes pre-training on a vast corpus of unlabeled audio data, allowing it to learn general audio features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Subsequently, it is fine-tuned with a smaller, labeled dataset to accurately recognize specific speech patterns in Portuguese.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">In this pipeline, the audio input is processed and transcribed into text using the wav2vec2 model. Once transcribed, the next step is to interpret the text to discern specific drone commands. We utilize a large language model (LLM) pretrained from Llama3 for this task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The LLM processes the transcribed text within the context of predefined drone commands: UP, DOWN, FORWARD, BACKWARD, RIGHT, and LEFT.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">To achieve this, we employ a structured approach where the LLM generates responses in JSON format to specify the drone’s direction. The process involves a prompt template that provides context about the possible directions the drone can take, and the transcription of the audio command. The response generated by the LLM is structured as a JSON object that specifies the drone’s direction, ensuring clear and unambiguous command interpretation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Pipeline 2: Classification Model</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.08658/assets/fig/pipeline2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the Classification Model pipeline</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">The second approach involves a direct mapping of audio inputs to drone commands, eliminating the intermediate step of converting speech to text. This pipeline also leverages the "facebook/wav2vec2-large-xlsr-53-portuguese" model, fine-tuned specifically for our project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The wav2vec2 model is adept at handling sequential data and effectively processes variations in the audio input.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">In this pipeline, audio is captured using integrated or external microphones and undergoes pre-processing to remove noise and improve clarity, including padding the waveform to a consistent length. The pre-processed audio is then fed directly into the wav2vec2 model, which extracts relevant features from the audio waveform and directly predicts the appropriate drone command without requiring a text intermediary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">For evaluation, a custom dataset of voice commands was prepared, including audio files and their corresponding labels. The model’s performance is assessed on a test set, with key metrics including classification accuracy, mean inference time, and the percentage of unknown commands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The evaluation process involves measuring the model’s inference time for each prediction and calculating statistical metrics to assess overall performance. A classification report provides detailed insights into the model’s accuracy across different command classes.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">By using the same base model, the "facebook/wav2vec2-large-xlsr-53-portuguese", both pipelines demonstrate different methodologies for drone command recognition. Pipeline 1 uses an intermediate STT step followed by LLM for command interpretation, while Pipeline 2 employs direct audio-to-command mapping, showcasing the versatility and robustness of the wav2vec2 model in handling various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Pipeline 3: Siamese Network</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.08658/assets/fig/pipeline3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of the Siamese Network pipeline</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">The third approach explores the use of Siamese neural networks, which are particularly adept at comparing pairs of inputs to determine their similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This characteristic is invaluable for tasks that require the system to generalize to new examples, such as recognizing new voice commands without necessitating extensive retraining.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p">A Siamese network consists of two or more identical sub-networks that share the same weights and architecture. During training, pairs of examples are fed into these sub-networks. The network learns to project similar examples close to each other in a latent space while ensuring that dissimilar examples are projected farther apart <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This process is facilitated by the use of contrastive loss, which minimizes the distance between similar examples and maximizes the distance between dissimilar ones. This training approach enables the network to effectively distinguish between various commands based on their latent representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p">In our system, the Siamese network includes multiple convolutional layers to extract features from the input audio signals. The network is trained to encode audio files into fixed-size vectors. These vectors are stored in a vector database, where each vector is associated with a corresponding command label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. When a new voice command is received, it is encoded into a vector and compared against the stored vectors using a K-Nearest Neighbors (KNN) model. The KNN model identifies the closest matching command based on the vector similarity, thus determining the appropriate action for the drone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p">This methodology allows for flexible and efficient recognition of new commands, significantly reducing the need for extensive retraining and enabling quick adaptation to new instructions. By leveraging the Siamese network’s ability to generalize from a relatively small set of training examples, we achieve robust and scalable voice command recognition for the Tello drone.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we present the results obtained after training and evaluating the proposed pipelines. The results include accuracy, precision, recall, F1-score, inference times, and a summary comparison of all three pipelines.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Pipeline 1: STT and LLM</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">The first pipeline involves converting spoken language into written text using Speech-to-Text (STT) technology, followed by interpreting the text with a Large Language Model (LLM) to generate the appropriate drone commands. The performance metrics for this pipeline are presented in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Pipeline 1: STT and LLM ‣ 4 Results ‣ Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Metric</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Without Fine-Tuning</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">With Fine-Tuning</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.73</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.81</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Precision</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.74</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.78</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Recall</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.63</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.70</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">F1-Score</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.66</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.73</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance Metrics for Pipeline 1</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">The accuracy and precision metrics indicate a significant improvement with fine-tuning. The model achieved an accuracy of 0.81 and an F1-score of 0.73 after fine-tuning, demonstrating its capability to accurately transcribe and interpret voice commands.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Pipeline 2: Direct Model</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">The second pipeline involves a direct mapping of audio inputs to drone commands, bypassing the intermediate step of converting speech to text. The performance metrics for this pipeline are presented in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Pipeline 2: Direct Model ‣ 4 Results ‣ Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Metric</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Without Fine-Tuning</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">With Fine-Tuning</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.94</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.99</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Precision</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.91</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.98</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Recall</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.93</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.99</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">F1-Score</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.92</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.98</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance Metrics for Pipeline 2</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">This pipeline also shows notable improvements with fine-tuning, achieving an accuracy of 0.99 and an F1-score of 0.98. The direct classification approach proves to be effective in interpreting voice commands without the need for an intermediate text representation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pipeline 3: Siamese Network</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">The third pipeline utilizes a Siamese neural network to compare pairs of inputs and determine their similarity. The performance metrics for this pipeline are presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Pipeline 3: Siamese Network ‣ 4 Results ‣ Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Metric</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Without Fine-Tuning</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">With Fine-Tuning</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.74</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Precision</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.54</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.74</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<td id="S4.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Recall</td>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.54</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<td id="S4.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">F1-Score</td>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.54</td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.74</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance Metrics for Pipeline 3</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">The Siamese network approach showed significant improvement with fine-tuning, achieving an accuracy of 0.74. The ability to generalize to new commands was enhanced, making this pipeline particularly useful for scenarios requiring flexibility and adaptability.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Summary of Results</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Summary of Results ‣ 4 Results ‣ Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a summary comparison of the accuracy and inference times for all three pipelines.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Pipeline</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Inference Time (in seconds)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">STT and LLM</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.81</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.233</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Direct Model</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.99</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.021</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Siamese Network</td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.74</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.006</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Summary of Results for All Pipelines</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">The results indicate that the Direct Model pipeline achieved the highest accuracy of 0.99. However, the Siamese Network pipeline significantly outperformed the others in terms of inference time, making it the most efficient for real-time applications. Despite being slightly lower in accuracy at 0.74, the Siamese Network demonstrated the best performance in inference time and the ability to generalize to new commands.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Discussion</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p">The evaluation of the three pipelines highlights the trade-offs between accuracy, inference time, and flexibility. The STT and LLM pipeline, while highly accurate, has a longer inference time due to the sequential nature of the tasks involved. The Direct Model pipeline provides the highest accuracy and a balance between precision and efficiency, making it highly suitable for real-time applications. The Siamese Network pipeline offers the best generalization capabilities and the shortest inference time, which is advantageous in dynamic environments where new commands might be introduced.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p">Future work will focus on further improving the models, expanding the dataset, and exploring additional techniques to enhance the performance and reliability of the voice command system for drone control.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">The development and evaluation of the three voice command pipelines for controlling the Tello drone demonstrate the effectiveness of using different approaches: STT followed by LLM, direct classification, and Siamese networks. Each pipeline has its unique strengths and potential applications, depending on the specific requirements of inference time, accuracy, efficiency, and flexibility. Through a comparative analysis, we have highlighted the trade-offs between these approaches.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">The results indicate that Pipeline 1 (STT and LLM) showed high accuracy and precision, but with a longer inference time compared to other pipelines. Pipeline 2 (Direct Model) proved to have the highest accuracy and precision, with a balance between precision and efficiency. Pipeline 3 (Siamese Network) showed promise in generalizing to new commands, offering the best inference time and flexibility. These findings suggest that the ideal pipeline choice depends on the specific application context and requirements. In situations requiring high precision, Pipeline 2 is most suitable, whereas Pipeline 3 is preferable where speed and flexibility are crucial. Pipeline 1 offers a balanced solution, especially useful for applications requiring high accuracy in command interpretation.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">The implementation of this voice-controlled drone system demonstrates the potential of utilizing STT, NLP, and LLM technologies to create intuitive and efficient interfaces for drones. In the future, improving models and collecting more extensive datasets can further enhance the system’s performance and applicability.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ruben Contreras, Angel Ayala, and Francisco Cruz.

</span>
<span class="ltx_bibblock">Unmanned aerial vehicle control through domain-based automatic speech
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Computers</span>, 9(3):75, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J De Curtó, I De Zarza, and Carlos T Calafate.

</span>
<span class="ltx_bibblock">Semantic scene understanding with large language models on unmanned
aerial vehicles.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Drones</span>, 7(2):114, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Abdur Razzaq Fayjie, Amir Ramezani, Doukhi Oualid, and Deok Jin Lee.

</span>
<span class="ltx_bibblock">Voice enabled smart drone control.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2017 Ninth international conference on ubiquitous and future
networks (ICUFN)</span>, pages 119–121. IEEE, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Julia Hermann, Moritz Plückthun, Aysegül Dogangün, and Marc
Hesenius.

</span>
<span class="ltx_bibblock">User-defined gesture and voice control in human-drone interaction for
police operations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Nordic Conference on Human-Computer Interaction</span>, pages
1–11, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
DN Krishna, Pinyi Wang, and Bruno Bozza.

</span>
<span class="ltx_bibblock">Using large self-supervised models for low-resource speech
recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, pages 2436–2440, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Saumya Kumaar, Toshit Bazaz, Sumeet Kour, Disha Gupta, Ravi M Vishwanath,
SN Omkar, et al.

</span>
<span class="ltx_bibblock">A deep learning approach to speech based control of unmanned aerial
vehicles (uavs).

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">CS &amp; IT Conf. Proc</span>, volume 8, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Erica L Meszaros, Meghan Chandarana, Anna Trujillo, and B Danette Allen.

</span>
<span class="ltx_bibblock">Speech-based natural language interface for uav trajectory
generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2017 International Conference on Unmanned Aircraft Systems
(ICUAS)</span>, pages 46–55. IEEE, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yang Xie, Zhenchuan Zhang, and Yingchun Yang.

</span>
<span class="ltx_bibblock">Siamese network with wav2vec feature for fake speech detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, pages 4269–4273, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Shuyuan Xu, Zelong Li, Kai Mei, and Yongfeng Zhang.

</span>
<span class="ltx_bibblock">Core: Llm as interpreter for natural language programming,
pseudo-code programming, and flow programming of ai agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2405.06907</span>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cengizhan Yapicioğlu, Zümray Dokur, and Tamer Ölmez.

</span>
<span class="ltx_bibblock">Voice command recognition for drone control by deep neural networks
on embedded system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2021 8th International Conference on Electrical and
Electronics Engineering (ICEEE)</span>, pages 65–72. IEEE, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jing Zhao and Wei-Qiang Zhang.

</span>
<span class="ltx_bibblock">Improving automatic speech recognition performance for low-resource
languages with self-supervised models.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>,
16(6):1227–1241, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.08657" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.08658" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.08658">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.08658" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.08659" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 19:07:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
