<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.06060] MERGE - A Bimodal Dataset For Static Music Emotion Recognition</title><meta property="og:description" content="The Music Emotion Recognition (MER) field has seen steady developments in recent years, with contributions from feature engineering, machine learning, and deep learning. The landscape has also shifted from audio-centri…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MERGE - A Bimodal Dataset For Static Music Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MERGE - A Bimodal Dataset For Static Music Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.06060">

<!--Generated on Mon Aug  5 13:01:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
music emotion recognition,  bimodal datasets,  feature extraction,  music information retrieval,  audio analysis,  lyrics analysis,  feature engineering,  machine learning,  deep learning.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MERGE - A Bimodal Dataset For Static Music Emotion Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pedro Lima Louro, Hugo Redinho, Ricardo Santos, Ricardo Malheiro, Renato Panda and Rui Pedro Paiva
</span><span class="ltx_author_notes">P. L. Louro, H. Redinho, R. Santos, and R. P. Paiva are with the University of Coimbra, Centre for Informatics and Systems of the University of Coimbra (CISUC), Department of Informatics Engineering, and LASI. E-mail: pedrolouro, redinho, ricardocorreia,ruipedro@dei.uc.pt.R. Malheiro is with CISUC, LASI, and Polytechnic Institute of Leiria - School of Technology and Management. E-mail: rsmal@dei.uc.pt.R. Panda is with CISUC, LASI, and Ci2 - Smart Cities Research Center, Polytechnic Institute of Tomar. Email: panda@dei.uc.pt.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The Music Emotion Recognition (MER) field has seen steady developments in recent years, with contributions from feature engineering, machine learning, and deep learning. The landscape has also shifted from audio-centric systems to bimodal ensembles that combine audio and lyrics. However, a severe lack of public and sizeable bimodal databases has hampered the development and improvement of bimodal audio-lyrics systems. This article proposes three new audio, lyrics, and bimodal MER research datasets, collectively called MERGE, created using a semi-automatic approach. To comprehensively assess the proposed datasets and establish a baseline for benchmarking, we conducted several experiments for each modality, using feature engineering, machine learning, and deep learning methodologies. In addition, we propose and validate fixed train-validate-test splits. The obtained results confirm the viability of the proposed datasets, achieving the best overall result of 79.21% F1-score for bimodal classification using a deep neural network.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
music emotion recognition, bimodal datasets, feature extraction, music information retrieval, audio analysis, lyrics analysis, feature engineering, machine learning, deep learning.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatically classifying the predominant emotion in a musical piece has seen increasing interest, pushed mainly by the popularity of music streaming platforms and the necessity for organizing and recommending music to its users. The field of Music Emotion Recognition (MER) has, thus, seen several methodologies proposed to address this problem, although there are many differences regarding problem paradigms and employed data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To our knowledge, the first problem addressed in MER (in 2003 and yet to be solved) was a static single-label classification of audio samples by Feng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. By static, we mean the identification of the dominant single emotion, typically using samples with a uniform emotion (hereafter termed static MER), in contrast to the Music Emotion Variation Detection (MEVD) problem, where emotion fluctuations throughout songs are analyzed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Following Feng’s pioneering work, several approaches for static MER have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, current MER solutions can still not accurately solve fundamental problems such as static MER into a few emotion classes (e.g., four to five). Current results are still low (top at around 70 to 75% F1-score) and limited by a so-called “glass ceiling” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Both existing studies support this <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and the stagnation in the Music Information Retrieval Evaluation eXchange (MIREX) Audio Mood Classification (AMC) task (a tentative benchmark in the field) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The best-performing method achieved 69.8% accuracy in a task comprising five categories. Moreover, this score has remained stable for several years, which calls for ways to help break this glass ceiling.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In previous works, we demonstrated this glass ceiling to be partly due to two core problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>: i) absence of public, sizeable, and quality datasets; ii) and the lack of emotionally relevant features. This paper focuses on contributions to the first problem, i.e., the creation of MER datasets.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this respect, tentative benchmarks were previously proposed in the context of MER challenges. Examples of this are the abovementioned MIREX AMC dataset (for static MER) and later the DEAM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (more focused on MEVD), resulting from the successive benchmarks for the 2013, 2014, and 2015 MediaEval’s Emotion in Music tasks. However, these datasets, as well as other MER datasets created over the years, suffer from several limitations, e.g., the inadequacy of emotion taxonomies, emotion classes with acoustic and semantic overlap, low-quality annotations, limited size or noise, and poor handling of emotion subjectivity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Hence, no public, sizeable, widely accepted, and adequately validated benchmarks exist.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Another critical point in the development of MER datasets is the available modalities. Despite some contributions to the creation of bimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, most current datasets only provide annotations for audio clips and exclude contextual information, most notably lyrics. While audio can accurately predict arousal, the same cannot be said for valence. It is valence that lyrics considerably outperform audio-only modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Considering this, training a classifier based on a bimodal paradigm consisting of audio clips with corresponding lyrics can, in theory, significantly improve MER systems.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, we propose new audio, lyrics, and bimodal static MER datasets, collectively called MERGE, annotated according to emotion perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> based on Russell’s four emotion quadrants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. We created these datasets following a semi-automatic protocol that considerably accelerates the annotation stage while promoting annotation quality. The new MERGE audio and lyrics datasets are significantly larger than previous efforts by our team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. MERGE audio-lyrics (the main contribution of this work) is, to the best of our knowledge, the largest publicly available bimodal MER dataset.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In addition, we performed an experimental validation of the proposed datasets using state-of-the-art classical Machine Learning (ML) and Deep Learning (DL) methodologies. The attained results and analysis confirm the viability of the proposed datasets for benchmarking further MER studies. The best-performing model (a bimodal neural network combining audio and lyrics) attained an F1-score of 79.21%.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The document is organized as follows. Section 2 reviews the relevant background and related work regarding publicly available audio, lyrics, and bimodal MER datasets and systems. Section 3 presents the proposed semi-automatic creation protocol, generation of Train-Validation-Test (TVT) splits, and contents of each dataset. Section 4 describes the methodologies, pre-processing steps, and optimization strategies followed for evaluating the proposed datasets and establishing a baseline for benchmarking. The attained results and gathered insights are discussed in Section 5. Finally, Section 6 draws this study’s main conclusions and final thoughts.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section starts with a brief review of common emotion taxonomies. Then, it reviews the primary data collection and annotation approaches employed in the creation of MER datasets. It then provides a critical overview of the current MER datasets, followed by a review of state-of-the-art audio, lyrics, and bimodal MER systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Emotion Taxonomies</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Psychology researchers have discussed for a long time how emotions can be represented and classified. This study has led to the proposal of several emotion taxonomies over the last century, which can be grouped into two major paradigms: categorical (or discrete) models and dimensional models. In the categorical paradigm, emotions are represented as a set of discrete categories or emotional descriptors identified by adjectives, e.g., Hevner’s adjective circle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In the dimensional models, emotions are organized along, typically, two or three axes, as discrete adjectives or as continuous values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Among these, Russell’s circumplex model of emotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has gained particular acceptance in the MER community. Supporters of this idea suggest that emotional states arise from the combination of two distinct neurophysiological systems: one for valence (pleasure-displeasure, i.e., the polarity of emotion in terms of positive and negative states, also known as pleasantness) and another for arousal or activity (aroused-not aroused, also known as activity, energy, or stimulation level). Russell even claimed that valence and arousal are the “core processes” of affect, constituting the raw material or primitive of emotional experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The result, illustrated in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A Emotion Taxonomies ‣ II Background and Related Work ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is a two-dimensional plane, also termed arousal-valence (AV) plane, where the X-axis represents valence and the Y-axis represents arousal, resulting in four quadrants that can be roughly defined as: 1) positive valence and arousal, i.e., happy and energetic emotions such as excitement or enthusiasm (Quadrant 1 - Q1); 2) negative valence and positive arousal, i.e., frantic and energetic ones such as anxiety, fear or anger (Q2); 3. negative valence and arousal, i.e., melancholic and sad emotions such as depression (Q3); 4) and positive valence and negative arousal, representing calm and positive emotions such as contentment or serenity (Q4).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The circumplex model has received broad support from several music psychology studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and has been adopted in several MER works, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This is the model we employ in this article.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.06060/assets/figs/Russell_Circumplex_Model.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Russell’s circumplex model of emotion as seen in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Data Collection and Annotation Approaches</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Regarding data collection, researchers typically collect audio clips and song lyrics from music platforms, e.g., AllMusic<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>AllMusic is “a popular music database that provides professional reviews and metadata for albums, songs and artists” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. URL: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.allmusic.com/</span></span></span></span>, and lyrics web crawlers, e.g., lyrics.com, ChartLyrics<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://www.chartlyrics.com/</span></span></span></span>, MaxiLyrics<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://www.lyricsmania.com/maxi_lyrics.html</span></span></span></span> or MusixMatch<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>http://https://www.musixmatch.com/</span></span></span>, respectively. Such platforms usually offer Application Programming Interfaces (API).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The collected audio clips and song lyrics are then pre-processed, e.g., to uniformize audio samples in terms of sampling, frequency, bit depth, and number of channels, remove noise or bad quality clips, clean lyrics for grammatical or metadata and other descriptions inside the text (e.g., name of the artist, title, identification of the chorus), etc.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">One important constraint associated with data collection in MER is that the music files and song lyrics are usually subject to very restrictive copyright laws. This issue limits the public distribution of datasets to their annotations and extracted features. Although some authors assume that audio samples under 30 seconds can be shared as “fair use” without copyright obligation, the subject is complex<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://smallbusiness.chron.com/copyright-laws-30-seconds-music-61149.html</span></span></span></span> and many datasets remain private. Due to this, numerous studies had to invest limited resources to build and use private datasets, making it harder to compare to their peers’ work. Other works, including our own, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> provide the URL from where to obtain the corresponding songs (song lyrics, in this example).</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">After data collection, song annotation (the most challenging part) must be performed. To this end, different approaches have been employed in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, e.g., manual annotation or annotation based on social tagging, music platforms, or annotation games.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">In manual annotation, each song is annotated by several subjects (typically more than 10), and the most prevalent opinion is selected. Manual annotations require hiring subjects, which can be expensive in terms of financial cost and time. Moreover, the process is time-consuming, tedious, and, thus, error-prone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. To minimize the impact of low-quality annotations, several strategies are employed, e.g., discarding outlier annotators and songs with low annotator agreement using statistical tools (e.g., average and standard deviation metrics) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and inter-coder agreement metrics such as the Krippendorph’s alpha <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">To reduce the impact of tiredness or lack of commitment related to manual annotation, another method to annotate emotions in songs is through collaborative games on the web, also termed Games With A Purpose (GWAP), e.g., MoodSwings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The idea of these games is to increase the commitment and motivation of annotators through the context of games.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">Another alternative to tackle the difficulties with manual annotation is to employ social tags obtained directly from music social networks such as Last.fm, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Compared to manual annotation, this method makes collecting the ground truth data easier and faster. However, there are several problems with the obtained social tags: sparsity due to the cold-start problem and popularity bias, multiple spellings of tags, malicious tagging or ad-hoc labeling techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For example, when a subject uses the tag “hate” in Last.fm, this might either mean that the song is about “hate” or that the person hates the song.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">Compared to the previous approach, a potentially more robust alternative is to employ annotations provided by music platforms such as AllMusic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. For example, through the AllMusic web service, we can obtain song clips (typically 25 to 30-second excerpts) and their respective emotion tags (AllMusic defines 289 distinct emotion tags). However, these tags are not part of any known supported taxonomy. In addition, the annotation process in AllMusic is unclear: all we know is that the employed tags were “created and assigned to music works by professional editors” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Also, the audio clips provided by the platform often contain noise (e.g., claps or silence) or segments that do not match the assigned emotion labels. Hence, both the provided music excerpts and the associated emotion tags require post-processing and validation.</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p">To partly overcome the described limitations, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we proposed a semi-automatic data collection and annotation strategy based on AllMusic annotations. The basic idea was to map the provided multi-label annotations of each to a single emotion quadrant (according to Russell’s model). In this article, we adapt our original approach, as described in Section <a href="#S3" title="III Proposed Datasets ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Critical Overview of Current MER Datasets</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Several MER datasets have been introduced in the literature over the years. However, as mentioned above, there are no publicly available, well-validated, widely accepted, and sufficiently large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">First attempts towards the proposal of benchmarks consisted of challenge-related datasets, such as the Music Information Retrieval eXchange (MIREX) 2007 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> (for static MER), and later the DEAM dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (more focused on MEVD), resulting from the successive benchmarks for the 2013, 2014, and 2015 MediaEval’s Emotion in Music tasks. Here, the differences between the emotion taxonomies applied are noticeable. The former uses a custom 5-cluster categorical taxonomy derived from a previous study on the available emotion tags from AllMusic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, in contrast to the latter usage of continuous arousal-valence values based on Russell’s Circumplex Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, a dimensional taxonomy.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">One of the main problems in MER is the lack of uniformity in the selected taxonomies and datasets (some of the employed taxonomies, e.g., MIREX’s, are not validated by music psychology research). This and the different employed datasets make it difficult to benchmark different approaches.
Another common problem is that, in several works, private datasets are employed (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>). Moreover, other datasets, even if public, are small, focus on a specific genre or style (e.g., Western classical music, limiting their use in real-life scenarios) or show low agreement among annotators (often a result of inadequate handling of subjectivity or low-quality control in the annotation process), as pointed out in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Still, others only provide audio or lyrics features (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>). When actual samples are provided, some are noisy (e.g., claps or silence), or the provided segments do not match the assigned emotion labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Finally, some datasets are focused on induced rather than perceived emotion (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>).</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In the following, we present a brief overview of popular audio, lyrics, and bimodal audio-lyrics MER datasets, emphasizing public datasets. For each dataset, we briefly discuss the data collection and annotation process and their strengths and shortcomings.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS1.4.1.1" class="ltx_text">II-C</span>1 </span>Audio Datasets</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">As previously mentioned, in the audio domain, the MIREX AMC challenge has contributed with one dataset with 600 audio clips. However, several significant issues have been identified: i) the defined emotion taxonomy is not grounded in psychology studies; ii) and some of the defined emotion clusters show semantic and acoustic overlap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Therefore, this tentative benchmark failed to accomplish its promise.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p">Before that, in early MER studies, researchers tended to create their own small datasets with little regard for the source, distribution, and emotion taxonomy employed, such as in work by Feng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. There, a dataset of 353 popular songs was proposed, with no information regarding content or metadata. The annotations were generated using a set of musical features (e.g., tempo, articulation) extracted from the songs.</p>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p">Yang et al. presented one of the first public audio datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The dataset comprised 25-second excerpts from 195 popular songs (representing the predominant emotion present, mainly the chorus) taken from Western, Chinese, and Japanese albums. The fully manual annotation process involved 253 subjects, many of whom were college students. Each subject labeled ten random samples with arousal-valence (AV) values corresponding to the axes of Russell’s Circumplex Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> in a [-1.0, 1.0] interval. The final AV values were obtained by averaging all annotations. The dataset quality was deemed acceptable based on test-retest, where the annotation process was repeated two months after the initial annotation. However, the dataset presents some flaws, the most striking being its significant imbalance. For example, only 12% of all samples belong to the second quadrant.</p>
</div>
<div id="S2.SS3.SSS1.p4" class="ltx_para">
<p id="S2.SS3.SSS1.p4.1" class="ltx_p">More recent datasets introduce different approaches to dealing with the annotation process, such as the gamified annotation process of MagnaTagATune<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Available at: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset</span>.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. A total of 25.877 samples are provided, accompanied by 30-second audio clips, with 168 unique tags across them, ranging from common high-level descriptors such as genre, mood, and era to instruments and specific performing techniques (e.g., plucking). Despite presenting higher quality than other datasets, the data has many drawbacks regarding tag distribution, according to the analysis in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/keunwoochoi/magnatagatune-list</span>.</span></span></span>.</p>
</div>
<div id="S2.SS3.SSS1.p5" class="ltx_para">
<p id="S2.SS3.SSS1.p5.1" class="ltx_p">Also, in the same year, Bertin-Mahieux et al. proposed the Million Song Dataset (MSD)<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>More detailed information at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://millionsongdataset.com/</span>.</span></span></span> to address the small size of available datasets. This is still the largest dataset in the MIR) field. It aggregates smaller datasets compiled from different sources, such as The Echo Nest (Spotify’s metadata provider), MusicBrainz<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://musicbrainz.org/</span>.</span></span></span>, Last.FM<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.last.fm/</span>.</span></span></span> and more. Data annotation is based on the collection of tags provided by the users of these platforms. However, it suffers from the previously mentioned limitations of approaches based on social tags, namely the lack of tag validation and the associated ambiguities.</p>
</div>
<div id="S2.SS3.SSS1.p6" class="ltx_para">
<p id="S2.SS3.SSS1.p6.1" class="ltx_p">Most recent research employing the MSD employs a specific subset created by Choi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, hereafter referred to as MSD Last.FM split, containing 241889 samples. There, a train-validate-test split of 201680-11774-28435, respectively, was defined. It was obtained by only considering samples whose metadata presented at least 50 unique tags. However, despite supposedly being available to the public, there are no means to acquire the audio files from the original provider, 7digital<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.7digital.com/</span>.</span></span></span>, as the API is no longer available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS1.p7" class="ltx_para">
<p id="S2.SS3.SSS1.p7.1" class="ltx_p">Regarding our team’s efforts, the most recent dataset released to the public is the 4-Quadrant Audio Emotion Dataset (4QAED) dataset presented by Panda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. As previously mentioned, the creation of this dataset followed a semi-automatic data collection and annotation approach based on AllMusic annotations. The main idea was to map the provided multi-label annotations of each song to an emotion quadrant (according to Russell’s model). The dataset<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://mir.dei.uc.pt</span>.</span></span></span> contains 900 audio samples with accompanying metadata, equally distributed between the four quadrants. The sample data consists of 30-second song excerpts, their respective categorical labels (Russell quadrants and the original AllMusic tags), and 1714 extracted features.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS2.4.1.1" class="ltx_text">II-C</span>2 </span>Lyrics Datasets</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">Lyrics MER has not received the same attention as audio MER, as reflected in the smaller pool of available datasets.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">The most prominent dataset consisting solely of lyrics is MoodyLyrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. It comprises 2595 samples annotated using several affective lexicons based on Russell’s circumplex model. The song information was gathered from various sources, namely Playlist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, the abovementioned MSD Last.FM split, Cal500 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and The Beatles<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Available at: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/TheBeatlesHDF5.tar.gz</span>.</span></span></span> datasets. Regarding annotations, arousal, and valence values were computed directly from each lyric using a previously obtained affective lexicon. One limitation of this dataset is that the first quadrant is over-represented.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p">Our team developed and made available two lyrics datasets (termed LED, for Lyrics Emotion Datasets), as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, with a total of 942 lyrics. The first consisted of 180 manually annotated samples, and the second followed a semi-automatic annotation process akin to 4QAED, resulting in 771 samples. Some of the drawbacks of this dataset are the small size and the slightly unbalanced quadrant distribution.</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para">
<p id="S2.SS3.SSS2.p4.1" class="ltx_p">In summary, not only are datasets focused on this modality scarce, but copyright issues are even more pressing than for audio. Thus, only links to the employed lyrics are typically provided.</p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS3.4.1.1" class="ltx_text">II-C</span>3 </span>Bimodal Audio-Lyrics Datasets</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">Most lyrics MER datasets were created in the context of bimodal MER. One of the main limitations of these datasets is that the annotations from those datasets were mainly acquired from music social networks such as Last.fm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Besides the previously discussed difficulties with social tagging, it is unclear whether subjects annotated songs using only audio, lyrics, or a combination of both. As discussed later, audio and lyrics should be annotated separately to individually assess their contribution to music emotion recognition.</p>
</div>
<div id="S2.SS3.SSS3.p2" class="ltx_para">
<p id="S2.SS3.SSS3.p2.1" class="ltx_p">Hu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> created one of the first bimodal audio-lyrics datasets. It includes 5585 audio and lyrics retrieved from the Last.FM platform, along with their emotion tags. Similar tags were clustered into larger groups, resulting in 18 emotion categories that formed a data-driven emotion taxonomy not validated by music psychology. Manual validation is not mentioned, and its quality cannot be assessed since it is private.</p>
</div>
<div id="S2.SS3.SSS3.p3" class="ltx_para">
<p id="S2.SS3.SSS3.p3.1" class="ltx_p">Other bimodal datasets include the 1000-song dataset created by Laurier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. It uses Last.FM as the source for audio and lyrics samples. The same platform provides the employed emotion tags, which are mapped to the four Russell quadrants and manually validated by 17 human subjects.</p>
</div>
<div id="S2.SS3.SSS3.p4" class="ltx_para">
<p id="S2.SS3.SSS3.p4.1" class="ltx_p">Another contribution is the sizeable 18644-song dataset by Delbouys et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which used the MSD as its source. However, the annotations were heavily biased towards audio, leading to possible conflicts with the emotional content of the lyrics, and, as was the case for the dataset from Hu, its quality cannot be assessed because it is private. Adding to the lack of manual validation, it is unsurprising that the attained low classification results point to low-quality samples and annotations akin to the drawbacks of the MSD dataset.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">MER Systems</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">To evaluate the proposed datasets (see Section <a href="#S3" title="III Proposed Datasets ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>), in this work, we conducted a set of experiments using classical ML and DL-based methodologies for each modality. Here, we briefly discuss state-of-the-art methods for each modality, particularly the ones we employ to establish a baseline for the proposed dataset (see Section <a href="#S4" title="IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>). Given the scope of our datasets, only methodologies tackling static MER are considered.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS1.4.1.1" class="ltx_text">II-D</span>1 </span>Audio MER Systems</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Regarding audio MER, several approaches have been proposed in the literature, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.p2.1" class="ltx_p">Among these, our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is currently state-of-the-art on classical feature engineering and machine learning methods. We proposed a set of novel audio features related to melody, rhythm, dynamics, articulation, and musical texture. The latter two dimensions were severely underrepresented in the literature before this work. In addition, all features extracted for each sample were also extracted from voice-only stems obtained through a voice separation approach. The complete set of features, comprehending already existing features in the literature and the novel set, were ranked using the ReliefF algorithm. Classification (employing the four Russell quadrants on the 4QAED dataset) was performed using a Support Vector Machine (SVM) trained with the top 100 features obtained from the previous step, achieving an F1-score of 76.4%.</p>
</div>
<div id="S2.SS4.SSS1.p3" class="ltx_para">
<p id="S2.SS4.SSS1.p3.1" class="ltx_p">Regarding deep learning approaches, although not focused on MER, we highlight the work by Choi et al., where the authors proposed a fully convolutional architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and later the Convolutional Recurrent Neural Network (CRNN) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for music multi-tag classification. The latter learns features from a Mel spectrogram representation, which is the input to the network. The learned features are further processed using a recurrent portion to process time-related information, finally outputting probabilities for 50 tags. The evaluation was conducted on the already mentioned MSD Last.FM split, reaching an Area Under the ROC curve (ROC) of 0.852, surpassing the previous state-of-the-art systems.</p>
</div>
<div id="S2.SS4.SSS1.p4" class="ltx_para">
<p id="S2.SS4.SSS1.p4.1" class="ltx_p">As previously mentioned, audio was also shown to falter when predicting the valence of a song, regardless of the methodology in question. Researchers have found that lyrics provide the necessary information to predict this dimension more accurately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. However, research in this direction is lacking compared to audio, as discussed in the following.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS2.4.1.1" class="ltx_text">II-D</span>2 </span>Lyrics MER Systems</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">Compared to audio, few lyrics-only MER systems are found in the literature, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.p2.1" class="ltx_p">In the approach presented by our team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, novel features related to lyrics content, structure, style, and semantics were proposed and used as input to an SVM optimized with a grid search strategy. A first evaluation effort considered only the 180-lyrics subset of the LED dataset, which achieved a 77.1% F1-score. To further validate the previous model (based on 180 lyrics), we tested it using the 771-lyrics subset, attaining 73.6% F1-score.</p>
</div>
<div id="S2.SS4.SSS2.p3" class="ltx_para">
<p id="S2.SS4.SSS2.p3.1" class="ltx_p">As for lyrics-only DL-based systems, Abdillah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> proposed to use
lyrics directly as input to three classical ML algorithms (Naive Bayes, K-Nearest Neighbor (KNN), and SVM) and two neural network models. The first comprises a Convolutional Neural Network (CNN) feature extraction portion, outputting one of Russell’s quadrants after applying softmax to the last layer, while the second embeds the input using a pre-trained GloVe embedder and process these using a Bidirectional Long Short Term Memory (Bi-LSTM) recurrent unit. The Bi-LSTM model exploiting GloVe embeddings performs the best out of the five methodologies, achieving a 91% F1-score on the MoodyLyrics Dataset.</p>
</div>
<div id="S2.SS4.SSS2.p4" class="ltx_para">
<p id="S2.SS4.SSS2.p4.1" class="ltx_p">The most relevant takeaway from these methodologies is the transversal conclusion that lyrics predict valence more accurately than audio, as previously mentioned. This is not surprising since lyrics are known to carry important emotional cues for adequately assessing the overall feel of a song. Due to this, bimodal systems have been proposed to leverage audio and lyrics to improve arousal and valence prediction.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS3.4.1.1" class="ltx_text">II-D</span>3 </span>Bimodal Audio-Lyrics MER Systems</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">Few systems for bimodal audio-lyrics MER have been proposed, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS3.p2" class="ltx_para">
<p id="S2.SS4.SSS3.p2.1" class="ltx_p">Regarding classical approaches, Laurier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed one of the first bimodal MER systems. The author created a private bimodal dataset of 1000 songs annotated on the four Russell quadrants. The authors report over 80% accuracy for each single modality.
By combining audio and lyrics with a majority voting and a late-feature fusion approach, the bimodal approach significantly outperformed the accuracy attained by single modalities.</p>
</div>
<div id="S2.SS4.SSS3.p3" class="ltx_para">
<p id="S2.SS4.SSS3.p3.1" class="ltx_p">Regarding deep learning approaches, we highlight the methodology proposed in Delbouys et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The authors proposed a model leveraging both learned features from Mel spectrogram representations of audio and word2vec lyrics embeddings. The learned information is concatenated before outputting continuous AV values corresponding to Russell’s model. Both mid- and late-information fusion is experimented with as a regression problem, achieving the best results with the later, 0.232 and 0.219 <math id="S2.SS4.SSS3.p3.1.m1.1" class="ltx_Math" alttext="R^{2}" display="inline"><semantics id="S2.SS4.SSS3.p3.1.m1.1a"><msup id="S2.SS4.SSS3.p3.1.m1.1.1" xref="S2.SS4.SSS3.p3.1.m1.1.1.cmml"><mi id="S2.SS4.SSS3.p3.1.m1.1.1.2" xref="S2.SS4.SSS3.p3.1.m1.1.1.2.cmml">R</mi><mn id="S2.SS4.SSS3.p3.1.m1.1.1.3" xref="S2.SS4.SSS3.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p3.1.m1.1b"><apply id="S2.SS4.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p3.1.m1.1.1.1.cmml" xref="S2.SS4.SSS3.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS4.SSS3.p3.1.m1.1.1.2.cmml" xref="S2.SS4.SSS3.p3.1.m1.1.1.2">𝑅</ci><cn type="integer" id="S2.SS4.SSS3.p3.1.m1.1.1.3.cmml" xref="S2.SS4.SSS3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p3.1.m1.1c">R^{2}</annotation></semantics></math> scores for arousal and valence, respectively, on a private dataset containing 18644 songs (which used the MSD as its source). As previously mentioned, the low attained results suggest limitations in the employed dataset.</p>
</div>
<div id="S2.SS4.SSS3.p4" class="ltx_para">
<p id="S2.SS4.SSS3.p4.1" class="ltx_p">Despite the few available methods, bimodal approaches are very promising for solving the shortcomings of single modality, particularly the difficulties of audio in predicting valence and lyrics in predicting arousal. Most studies in the literature report improved results when bimodal approaches were followed.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Datasets</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The proposed MERGE dataset comprises audio, lyrics, and bimodal modalities, enabling both single and bimodal research. Each modality includes two different variations: i) complete, i.e., all songs with disregard for any balancing; ii) and balanced, in terms of both quadrant and genre distribution, built similarly to the protocol followed by Panda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The datasets, metadata, and features (audio and lyrics) are publicly available<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>URL under preparation.</span></span></span>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The rest of this section describes the building process and contents of the abovementioned datasets. Before that, we propose a set of requirements to be considered in the creation of MER datasets.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Requirements for MER Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">After reviewing the available datasets for MER and considering the state of the art of the field, we have defined the following requirements for creating new datasets:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">R1. Simple and validated taxonomy</span>: Datasets should be based on simple, psychologically validated taxonomies. For simplicity, a reduced set of emotional terms (e.g., Russell’s four emotional quadrants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>) should be employed. Current MER research is still unable to properly solve classification problems with four to five classes with high accuracy. Thus, at this moment, there are few advantages to tackling problems with higher granularity.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">R2. Variety and balance</span>: Datasets should be varied, balanced, and not limited to a single musical genre, style, or era.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">R3. Care in annotation</span>: It has been shown that datasets annotated with recourse to platforms such as the Amazon Mechanical Turk (MTurk) tend to lack annotation quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">R4. Reduced ambiguity</span>: At least good annotator agreement should be achieved, minimizing the mentioned ambiguity issues. This would lead to datasets with reasonably clear emotions, a key need at the current stage of MER research.</p>
</div>
</li>
<li id="S3.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix5.p1" class="ltx_para">
<p id="S3.I1.ix5.p1.1" class="ltx_p"><span id="S3.I1.ix5.p1.1.1" class="ltx_text ltx_font_bold">R5. Separate annotation between audio and lyrics</span>: In the creation of bimodal MER datasets (containing audio and lyrics), care should be taken to isolate the two sources in the annotation process so that the impact of each modality might be properly assessed.</p>
</div>
</li>
<li id="S3.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix6.p1" class="ltx_para">
<p id="S3.I1.ix6.p1.1" class="ltx_p"><span id="S3.I1.ix6.p1.1.1" class="ltx_text ltx_font_bold">R6. Publicly available</span>: It is necessary that the datasets be public to permit a comparative analysis of different methods.</p>
</div>
</li>
<li id="S3.I1.ix7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I1.ix7.p1" class="ltx_para">
<p id="S3.I1.ix7.p1.1" class="ltx_p"><span id="S3.I1.ix7.p1.1.1" class="ltx_text ltx_font_bold">R7. Large size</span>: Sizeable datasets are required to exploit ML and DL solutions better.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We also defined two additional secondary requirements:</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I2.ix1.p1" class="ltx_para">
<p id="S3.I2.ix1.p1.1" class="ltx_p"><span id="S3.I2.ix1.p1.1.1" class="ltx_text ltx_font_bold">S1. Prepared for a wide range of research works</span>: Besides emotion annotations, datasets should provide metadata such as genre, artist, album, year, and complete emotion tags. These would make the dataset relevant for the broader Music Information Retrieval (MIR) field and might be useful for later, more advanced tasks such as multi-label emotion classification.</p>
</div>
</li>
<li id="S3.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I2.ix2.p1" class="ltx_para">
<p id="S3.I2.ix2.p1.1" class="ltx_p"><span id="S3.I2.ix2.p1.1.1" class="ltx_text ltx_font_bold">S2. Semi-automatic construction process</span>: Probably, the main difficulty with the previous six requirements is that at least part of the annotation process must involve manual human validation. This calls for semi-automatic construction approaches, reducing the resources needed to build a sizeable dataset, as discussed below.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Creation Protocol</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We guided the creation of the new datasets by the above requirements. As a result, the dataset creation procedure described in Algorithm <a href="#alg1" title="Algorithm 1 ‣ III-B Creation Protocol ‣ III Proposed Datasets ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> was followed (adapted and improved from our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>). The main ideas of the proposed algorithm are discussed in the following paragraphs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">After gathering audio clips from AllMusic using the provided API<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Available audio samples and corresponding metadata were retrieved through <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://tivo.stoplight.io/docs/music-metadata-api</span>.</span></span></span>, a key component of our approach is the mapping of AllMusic emotion tags (curated by AllMusic experts) to Russell’s quadrants. To this end, we employ Warriner’s adjectives list <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which contains a list of 13915 emotion adjectives (in English) with affective ratings in three dimensions: arousal, valence, and dominance (AVD). Then, each song is mapped into a point in the AV plane by averaging the original emotion tags based on Warriner’s scores. In addition, to reduce ambiguity, songs placed close to the center of the plane, namely in the [-0,2, 0.2] interval (on a [-1, 1] scale), are discarded. In addition, genre variability in each quadrant is maximized.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Following the audio collection, their corresponding lyrics are retrieved from platforms such as lyrics.com, Chart-Lyrics, MaxiLyrics, and MusixMatch. In this process, lyrics could not be found for some of the audio samples.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Another crucial step of our approach is manual validation of the acquired songs regarding the assigned quadrant and the quality of the audio clip/lyrics. Hence, a manual blind inspection of the candidate set is carried out. Subjects were given sets of randomly distributed audio clips and song lyrics and asked to annotate them according to Russell’s quadrants or, should the sample present poor quality (noise, claps, silence, etc.), discard it entirely. If the annotated emotion matches the one proposed by AllMusic, the song is kept; otherwise, it is discarded. As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we considered a song valid if at least one annotator confirmed the tag. This step is essential to avoid the overload of a fully manual annotation process: assuming that the expert annotations were carefully obtained, validating them requires only a few human resources. A total of 8 subjects conducted validation. Overall, this approach is a good trade-off between the rigor and cost of fully manual annotation.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Based on the validated audio and lyrics datasets, the bimodal dataset will comprise the songs where the audio and lyrics quadrants match. These form the abovementioned ”complete” datasets. Finally, the ”balanced” audio, lyrics, and bimodal datasets are composed by discarding samples from the more represented quadrants to form equally represented quadrants, respecting genre balancing again.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In addition to the described procedure, the original 4QAED and LED datasets were used as a foundation for the MERGE dataset: whenever possible, lyrics were retrieved for the audio-only samples from 4QAED and, conversely, audio for the lyrics-only samples from LED.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">The following paragraphs discuss the resulting number and distribution of samples across quadrants for each dataset.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.9.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Dataset creation algorithm.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.10" class="ltx_p ltx_figure_panel">1. Gather songs and emotion data from AllMusic services. According to several authors, AllMusic data was curated by experts.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I3" class="ltx_itemize ltx_figure_panel">
<li id="S3.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I3.ix1.p1" class="ltx_para">
<p id="S3.I3.ix1.p1.1" class="ltx_p">1.1. Retrieve the list of 289 emotion tags, <math id="S3.I3.ix1.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I3.ix1.p1.1.m1.1a"><mi id="S3.I3.ix1.p1.1.m1.1.1" xref="S3.I3.ix1.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I3.ix1.p1.1.m1.1b"><ci id="S3.I3.ix1.p1.1.m1.1.1.cmml" xref="S3.I3.ix1.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.ix1.p1.1.m1.1c">E</annotation></semantics></math>, using the AllMusic API.</p>
</div>
</li>
<li id="S3.I3.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I3.ix2.p1" class="ltx_para">
<p id="S3.I3.ix2.p1.2" class="ltx_p">1.2. For each emotion tag gathered, <math id="S3.I3.ix2.p1.1.m1.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="S3.I3.ix2.p1.1.m1.1a"><msub id="S3.I3.ix2.p1.1.m1.1.1" xref="S3.I3.ix2.p1.1.m1.1.1.cmml"><mi id="S3.I3.ix2.p1.1.m1.1.1.2" xref="S3.I3.ix2.p1.1.m1.1.1.2.cmml">E</mi><mi id="S3.I3.ix2.p1.1.m1.1.1.3" xref="S3.I3.ix2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I3.ix2.p1.1.m1.1b"><apply id="S3.I3.ix2.p1.1.m1.1.1.cmml" xref="S3.I3.ix2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I3.ix2.p1.1.m1.1.1.1.cmml" xref="S3.I3.ix2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I3.ix2.p1.1.m1.1.1.2.cmml" xref="S3.I3.ix2.p1.1.m1.1.1.2">𝐸</ci><ci id="S3.I3.ix2.p1.1.m1.1.1.3.cmml" xref="S3.I3.ix2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.ix2.p1.1.m1.1c">E_{i}</annotation></semantics></math>, query the API for the top 10000 songs related to it, <math id="S3.I3.ix2.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.I3.ix2.p1.2.m2.1a"><mi id="S3.I3.ix2.p1.2.m2.1.1" xref="S3.I3.ix2.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.I3.ix2.p1.2.m2.1b"><ci id="S3.I3.ix2.p1.2.m2.1.1.cmml" xref="S3.I3.ix2.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.ix2.p1.2.m2.1c">S</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.11" class="ltx_p ltx_figure_panel">2. Bridge the emotional data from AllMusic (based on an unvalidated emotional taxonomy) with Warriner’s list.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I4" class="ltx_itemize ltx_figure_panel">
<li id="S3.I4.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I4.ix1.p1" class="ltx_para">
<p id="S3.I4.ix1.p1.2" class="ltx_p">2.1. For each emotion tag, <math id="S3.I4.ix1.p1.1.m1.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="S3.I4.ix1.p1.1.m1.1a"><msub id="S3.I4.ix1.p1.1.m1.1.1" xref="S3.I4.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I4.ix1.p1.1.m1.1.1.2" xref="S3.I4.ix1.p1.1.m1.1.1.2.cmml">E</mi><mi id="S3.I4.ix1.p1.1.m1.1.1.3" xref="S3.I4.ix1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I4.ix1.p1.1.m1.1b"><apply id="S3.I4.ix1.p1.1.m1.1.1.cmml" xref="S3.I4.ix1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I4.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I4.ix1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I4.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I4.ix1.p1.1.m1.1.1.2">𝐸</ci><ci id="S3.I4.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I4.ix1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.ix1.p1.1.m1.1c">E_{i}</annotation></semantics></math>, retrieve the associated AVD (arousal, valence, and dominance) values from Warriner’s dictionary of English words. If the word is missing, remove it from the set of tags, <math id="S3.I4.ix1.p1.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I4.ix1.p1.2.m2.1a"><mi id="S3.I4.ix1.p1.2.m2.1.1" xref="S3.I4.ix1.p1.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I4.ix1.p1.2.m2.1b"><ci id="S3.I4.ix1.p1.2.m2.1.1.cmml" xref="S3.I4.ix1.p1.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.ix1.p1.2.m2.1c">E</annotation></semantics></math>.</p>
</div>
</li>
<li id="S3.I4.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I4.ix2.p1" class="ltx_para">
<p id="S3.I4.ix2.p1.1" class="ltx_p">2.2. Using the retrieved AV values, map each emotion tag, <math id="S3.I4.ix2.p1.1.m1.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="S3.I4.ix2.p1.1.m1.1a"><msub id="S3.I4.ix2.p1.1.m1.1.1" xref="S3.I4.ix2.p1.1.m1.1.1.cmml"><mi id="S3.I4.ix2.p1.1.m1.1.1.2" xref="S3.I4.ix2.p1.1.m1.1.1.2.cmml">E</mi><mi id="S3.I4.ix2.p1.1.m1.1.1.3" xref="S3.I4.ix2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I4.ix2.p1.1.m1.1b"><apply id="S3.I4.ix2.p1.1.m1.1.1.cmml" xref="S3.I4.ix2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I4.ix2.p1.1.m1.1.1.1.cmml" xref="S3.I4.ix2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I4.ix2.p1.1.m1.1.1.2.cmml" xref="S3.I4.ix2.p1.1.m1.1.1.2">𝐸</ci><ci id="S3.I4.ix2.p1.1.m1.1.1.3.cmml" xref="S3.I4.ix2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.ix2.p1.1.m1.1c">E_{i}</annotation></semantics></math>, onto one of the four Russel’s quadrants.</p>
</div>
</li>
<li id="S3.I4.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I4.ix3.p1" class="ltx_para">
<p id="S3.I4.ix3.p1.2" class="ltx_p">2.3. Assign a quadrant to each song, <math id="S3.I4.ix3.p1.1.m1.1" class="ltx_Math" alttext="S_{i}" display="inline"><semantics id="S3.I4.ix3.p1.1.m1.1a"><msub id="S3.I4.ix3.p1.1.m1.1.1" xref="S3.I4.ix3.p1.1.m1.1.1.cmml"><mi id="S3.I4.ix3.p1.1.m1.1.1.2" xref="S3.I4.ix3.p1.1.m1.1.1.2.cmml">S</mi><mi id="S3.I4.ix3.p1.1.m1.1.1.3" xref="S3.I4.ix3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I4.ix3.p1.1.m1.1b"><apply id="S3.I4.ix3.p1.1.m1.1.1.cmml" xref="S3.I4.ix3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I4.ix3.p1.1.m1.1.1.1.cmml" xref="S3.I4.ix3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I4.ix3.p1.1.m1.1.1.2.cmml" xref="S3.I4.ix3.p1.1.m1.1.1.2">𝑆</ci><ci id="S3.I4.ix3.p1.1.m1.1.1.3.cmml" xref="S3.I4.ix3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.ix3.p1.1.m1.1c">S_{i}</annotation></semantics></math>, based on the quadrant where the majority of the emotion tags, <math id="S3.I4.ix3.p1.2.m2.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="S3.I4.ix3.p1.2.m2.1a"><msub id="S3.I4.ix3.p1.2.m2.1.1" xref="S3.I4.ix3.p1.2.m2.1.1.cmml"><mi id="S3.I4.ix3.p1.2.m2.1.1.2" xref="S3.I4.ix3.p1.2.m2.1.1.2.cmml">E</mi><mi id="S3.I4.ix3.p1.2.m2.1.1.3" xref="S3.I4.ix3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I4.ix3.p1.2.m2.1b"><apply id="S3.I4.ix3.p1.2.m2.1.1.cmml" xref="S3.I4.ix3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I4.ix3.p1.2.m2.1.1.1.cmml" xref="S3.I4.ix3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I4.ix3.p1.2.m2.1.1.2.cmml" xref="S3.I4.ix3.p1.2.m2.1.1.2">𝐸</ci><ci id="S3.I4.ix3.p1.2.m2.1.1.3.cmml" xref="S3.I4.ix3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.ix3.p1.2.m2.1c">E_{i}</annotation></semantics></math>, fall.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.1" class="ltx_p ltx_figure_panel">3. Perform data pre-processing and filtering to reduce the massive amount of gathered data to a more balanced but still sizeable set, <math id="alg1.1.m1.1" class="ltx_Math" alttext="FS" display="inline"><semantics id="alg1.1.m1.1a"><mrow id="alg1.1.m1.1.1" xref="alg1.1.m1.1.1.cmml"><mi id="alg1.1.m1.1.1.2" xref="alg1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="alg1.1.m1.1.1.1" xref="alg1.1.m1.1.1.1.cmml">​</mo><mi id="alg1.1.m1.1.1.3" xref="alg1.1.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.1.m1.1b"><apply id="alg1.1.m1.1.1.cmml" xref="alg1.1.m1.1.1"><times id="alg1.1.m1.1.1.1.cmml" xref="alg1.1.m1.1.1.1"></times><ci id="alg1.1.m1.1.1.2.cmml" xref="alg1.1.m1.1.1.2">𝐹</ci><ci id="alg1.1.m1.1.1.3.cmml" xref="alg1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.m1.1c">FS</annotation></semantics></math>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I5" class="ltx_itemize ltx_figure_panel">
<li id="S3.I5.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I5.ix1.p1" class="ltx_para">
<p id="S3.I5.ix1.p1.1" class="ltx_p">3.1. Filter ambiguous songs (where a dominant emotional quadrant is not present).</p>
<ul id="S3.I5.ix1.I1" class="ltx_itemize">
<li id="S3.I5.ix1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I5.ix1.I1.ix1.p1" class="ltx_para">
<p id="S3.I5.ix1.I1.ix1.p1.2" class="ltx_p">3.1.1. For all the songs in <math id="S3.I5.ix1.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="S_{i}" display="inline"><semantics id="S3.I5.ix1.I1.ix1.p1.1.m1.1a"><msub id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.2" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.2.cmml">S</mi><mi id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.3" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I5.ix1.I1.ix1.p1.1.m1.1b"><apply id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.cmml" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.2">𝑆</ci><ci id="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I5.ix1.I1.ix1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I5.ix1.I1.ix1.p1.1.m1.1c">S_{i}</annotation></semantics></math>, calculate the average arousal and valence values of all the emotion tags gathered, <math id="S3.I5.ix1.I1.ix1.p1.2.m2.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="S3.I5.ix1.I1.ix1.p1.2.m2.1a"><msub id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.cmml"><mi id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.2" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.2.cmml">E</mi><mi id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.3" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I5.ix1.I1.ix1.p1.2.m2.1b"><apply id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.cmml" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.1.cmml" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.2.cmml" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.2">𝐸</ci><ci id="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.3.cmml" xref="S3.I5.ix1.I1.ix1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I5.ix1.I1.ix1.p1.2.m2.1c">E_{i}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S3.I5.ix1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I5.ix1.I1.ix2.p1" class="ltx_para">
<p id="S3.I5.ix1.I1.ix2.p1.1" class="ltx_p">3.1.2. If the average value of valence or arousal is in the range [-0.2, 0.2], remove the song from the dataset.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S3.I5.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I5.ix2.p1" class="ltx_para">
<p id="S3.I5.ix2.p1.1" class="ltx_p">3.2. Remove duplicated or very similar versions of the same songs by the same artists (e.g., different albums) by using approximate string matching against the combination of artist and title metadata.</p>
</div>
</li>
<li id="S3.I5.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I5.ix3.p1" class="ltx_para">
<p id="S3.I5.ix3.p1.1" class="ltx_p">3.3. Remove songs without genre information. This ensures that the algorithms that ensure maximum genre diversity can function correctly.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.3" class="ltx_p ltx_figure_panel">4. Generate a subset, <math id="alg1.2.m1.1" class="ltx_Math" alttext="GS" display="inline"><semantics id="alg1.2.m1.1a"><mrow id="alg1.2.m1.1.1" xref="alg1.2.m1.1.1.cmml"><mi id="alg1.2.m1.1.1.2" xref="alg1.2.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.2.m1.1.1.1" xref="alg1.2.m1.1.1.1.cmml">​</mo><mi id="alg1.2.m1.1.1.3" xref="alg1.2.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.2.m1.1b"><apply id="alg1.2.m1.1.1.cmml" xref="alg1.2.m1.1.1"><times id="alg1.2.m1.1.1.1.cmml" xref="alg1.2.m1.1.1.1"></times><ci id="alg1.2.m1.1.1.2.cmml" xref="alg1.2.m1.1.1.2">𝐺</ci><ci id="alg1.2.m1.1.1.3.cmml" xref="alg1.2.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.m1.1c">GS</annotation></semantics></math>, maximizing genre variability in each quadrant.
<br class="ltx_break">5. Obtain the manually validated audio dataset, <math id="alg1.3.m2.1" class="ltx_Math" alttext="ASV" display="inline"><semantics id="alg1.3.m2.1a"><mrow id="alg1.3.m2.1.1" xref="alg1.3.m2.1.1.cmml"><mi id="alg1.3.m2.1.1.2" xref="alg1.3.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="alg1.3.m2.1.1.1" xref="alg1.3.m2.1.1.1.cmml">​</mo><mi id="alg1.3.m2.1.1.3" xref="alg1.3.m2.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="alg1.3.m2.1.1.1a" xref="alg1.3.m2.1.1.1.cmml">​</mo><mi id="alg1.3.m2.1.1.4" xref="alg1.3.m2.1.1.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.3.m2.1b"><apply id="alg1.3.m2.1.1.cmml" xref="alg1.3.m2.1.1"><times id="alg1.3.m2.1.1.1.cmml" xref="alg1.3.m2.1.1.1"></times><ci id="alg1.3.m2.1.1.2.cmml" xref="alg1.3.m2.1.1.2">𝐴</ci><ci id="alg1.3.m2.1.1.3.cmml" xref="alg1.3.m2.1.1.3">𝑆</ci><ci id="alg1.3.m2.1.1.4.cmml" xref="alg1.3.m2.1.1.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.m2.1c">ASV</annotation></semantics></math>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I6" class="ltx_itemize ltx_figure_panel">
<li id="S3.I6.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I6.ix1.p1" class="ltx_para">
<p id="S3.I6.ix1.p1.1" class="ltx_p">5.1. Distribute all the songs in the set <math id="S3.I6.ix1.p1.1.m1.1" class="ltx_Math" alttext="GS" display="inline"><semantics id="S3.I6.ix1.p1.1.m1.1a"><mrow id="S3.I6.ix1.p1.1.m1.1.1" xref="S3.I6.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I6.ix1.p1.1.m1.1.1.2" xref="S3.I6.ix1.p1.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.I6.ix1.p1.1.m1.1.1.1" xref="S3.I6.ix1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I6.ix1.p1.1.m1.1.1.3" xref="S3.I6.ix1.p1.1.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.ix1.p1.1.m1.1b"><apply id="S3.I6.ix1.p1.1.m1.1.1.cmml" xref="S3.I6.ix1.p1.1.m1.1.1"><times id="S3.I6.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I6.ix1.p1.1.m1.1.1.1"></times><ci id="S3.I6.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I6.ix1.p1.1.m1.1.1.2">𝐺</ci><ci id="S3.I6.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I6.ix1.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.ix1.p1.1.m1.1c">GS</annotation></semantics></math> for each team member equally.</p>
</div>
</li>
<li id="S3.I6.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I6.ix2.p1" class="ltx_para">
<p id="S3.I6.ix2.p1.1" class="ltx_p">5.2. For each song, <math id="S3.I6.ix2.p1.1.m1.1" class="ltx_Math" alttext="GS_{i}" display="inline"><semantics id="S3.I6.ix2.p1.1.m1.1a"><mrow id="S3.I6.ix2.p1.1.m1.1.1" xref="S3.I6.ix2.p1.1.m1.1.1.cmml"><mi id="S3.I6.ix2.p1.1.m1.1.1.2" xref="S3.I6.ix2.p1.1.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.I6.ix2.p1.1.m1.1.1.1" xref="S3.I6.ix2.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.I6.ix2.p1.1.m1.1.1.3" xref="S3.I6.ix2.p1.1.m1.1.1.3.cmml"><mi id="S3.I6.ix2.p1.1.m1.1.1.3.2" xref="S3.I6.ix2.p1.1.m1.1.1.3.2.cmml">S</mi><mi id="S3.I6.ix2.p1.1.m1.1.1.3.3" xref="S3.I6.ix2.p1.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.ix2.p1.1.m1.1b"><apply id="S3.I6.ix2.p1.1.m1.1.1.cmml" xref="S3.I6.ix2.p1.1.m1.1.1"><times id="S3.I6.ix2.p1.1.m1.1.1.1.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.1"></times><ci id="S3.I6.ix2.p1.1.m1.1.1.2.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.2">𝐺</ci><apply id="S3.I6.ix2.p1.1.m1.1.1.3.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I6.ix2.p1.1.m1.1.1.3.1.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I6.ix2.p1.1.m1.1.1.3.2.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.3.2">𝑆</ci><ci id="S3.I6.ix2.p1.1.m1.1.1.3.3.cmml" xref="S3.I6.ix2.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.ix2.p1.1.m1.1c">GS_{i}</annotation></semantics></math>, validation and annotation are performed according to Russell’s quadrants.</p>
<ul id="S3.I6.ix2.I1" class="ltx_itemize">
<li id="S3.I6.ix2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I6.ix2.I1.ix1.p1" class="ltx_para">
<p id="S3.I6.ix2.I1.ix1.p1.1" class="ltx_p">5.2.1. Verify that the song is valid (e.g., does not contain clapping, noise, or silence) and that the emotion present in the song is not ambiguous.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.5" class="ltx_p ltx_figure_panel">6. Retrieve the lyrics dataset, <math id="alg1.4.m1.1" class="ltx_Math" alttext="LS" display="inline"><semantics id="alg1.4.m1.1a"><mrow id="alg1.4.m1.1.1" xref="alg1.4.m1.1.1.cmml"><mi id="alg1.4.m1.1.1.2" xref="alg1.4.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="alg1.4.m1.1.1.1" xref="alg1.4.m1.1.1.1.cmml">​</mo><mi id="alg1.4.m1.1.1.3" xref="alg1.4.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.4.m1.1b"><apply id="alg1.4.m1.1.1.cmml" xref="alg1.4.m1.1.1"><times id="alg1.4.m1.1.1.1.cmml" xref="alg1.4.m1.1.1.1"></times><ci id="alg1.4.m1.1.1.2.cmml" xref="alg1.4.m1.1.1.2">𝐿</ci><ci id="alg1.4.m1.1.1.3.cmml" xref="alg1.4.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.4.m1.1c">LS</annotation></semantics></math>, corresponding to the validated audio clips, <math id="alg1.5.m2.1" class="ltx_Math" alttext="AVS" display="inline"><semantics id="alg1.5.m2.1a"><mrow id="alg1.5.m2.1.1" xref="alg1.5.m2.1.1.cmml"><mi id="alg1.5.m2.1.1.2" xref="alg1.5.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="alg1.5.m2.1.1.1" xref="alg1.5.m2.1.1.1.cmml">​</mo><mi id="alg1.5.m2.1.1.3" xref="alg1.5.m2.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="alg1.5.m2.1.1.1a" xref="alg1.5.m2.1.1.1.cmml">​</mo><mi id="alg1.5.m2.1.1.4" xref="alg1.5.m2.1.1.4.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.5.m2.1b"><apply id="alg1.5.m2.1.1.cmml" xref="alg1.5.m2.1.1"><times id="alg1.5.m2.1.1.1.cmml" xref="alg1.5.m2.1.1.1"></times><ci id="alg1.5.m2.1.1.2.cmml" xref="alg1.5.m2.1.1.2">𝐴</ci><ci id="alg1.5.m2.1.1.3.cmml" xref="alg1.5.m2.1.1.3">𝑉</ci><ci id="alg1.5.m2.1.1.4.cmml" xref="alg1.5.m2.1.1.4">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.5.m2.1c">AVS</annotation></semantics></math>, from the following platforms: lyrics.com, ChartLyrics, MaxiLyrics, and MusixMatch, leading to the lyrics dataset (instrumental songs (without lyrics) will not be part of the lyrics dataset).</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.6" class="ltx_p ltx_figure_panel">7. Obtain the manually validated lyrics dataset, <math id="alg1.6.m1.1" class="ltx_Math" alttext="LSV" display="inline"><semantics id="alg1.6.m1.1a"><mrow id="alg1.6.m1.1.1" xref="alg1.6.m1.1.1.cmml"><mi id="alg1.6.m1.1.1.2" xref="alg1.6.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="alg1.6.m1.1.1.1" xref="alg1.6.m1.1.1.1.cmml">​</mo><mi id="alg1.6.m1.1.1.3" xref="alg1.6.m1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="alg1.6.m1.1.1.1a" xref="alg1.6.m1.1.1.1.cmml">​</mo><mi id="alg1.6.m1.1.1.4" xref="alg1.6.m1.1.1.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.6.m1.1b"><apply id="alg1.6.m1.1.1.cmml" xref="alg1.6.m1.1.1"><times id="alg1.6.m1.1.1.1.cmml" xref="alg1.6.m1.1.1.1"></times><ci id="alg1.6.m1.1.1.2.cmml" xref="alg1.6.m1.1.1.2">𝐿</ci><ci id="alg1.6.m1.1.1.3.cmml" xref="alg1.6.m1.1.1.3">𝑆</ci><ci id="alg1.6.m1.1.1.4.cmml" xref="alg1.6.m1.1.1.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.6.m1.1c">LSV</annotation></semantics></math>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I7" class="ltx_itemize ltx_figure_panel">
<li id="S3.I7.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I7.ix1.p1" class="ltx_para">
<p id="S3.I7.ix1.p1.1" class="ltx_p">7.1. Distribute all the songs in the <math id="S3.I7.ix1.p1.1.m1.1" class="ltx_Math" alttext="LS" display="inline"><semantics id="S3.I7.ix1.p1.1.m1.1a"><mrow id="S3.I7.ix1.p1.1.m1.1.1" xref="S3.I7.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I7.ix1.p1.1.m1.1.1.2" xref="S3.I7.ix1.p1.1.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.I7.ix1.p1.1.m1.1.1.1" xref="S3.I7.ix1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I7.ix1.p1.1.m1.1.1.3" xref="S3.I7.ix1.p1.1.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I7.ix1.p1.1.m1.1b"><apply id="S3.I7.ix1.p1.1.m1.1.1.cmml" xref="S3.I7.ix1.p1.1.m1.1.1"><times id="S3.I7.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I7.ix1.p1.1.m1.1.1.1"></times><ci id="S3.I7.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I7.ix1.p1.1.m1.1.1.2">𝐿</ci><ci id="S3.I7.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I7.ix1.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.ix1.p1.1.m1.1c">LS</annotation></semantics></math> set for each team member equally.</p>
</div>
</li>
<li id="S3.I7.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I7.ix2.p1" class="ltx_para">
<p id="S3.I7.ix2.p1.1" class="ltx_p">7.2. For each song, <math id="S3.I7.ix2.p1.1.m1.1" class="ltx_Math" alttext="LS_{i}" display="inline"><semantics id="S3.I7.ix2.p1.1.m1.1a"><mrow id="S3.I7.ix2.p1.1.m1.1.1" xref="S3.I7.ix2.p1.1.m1.1.1.cmml"><mi id="S3.I7.ix2.p1.1.m1.1.1.2" xref="S3.I7.ix2.p1.1.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.I7.ix2.p1.1.m1.1.1.1" xref="S3.I7.ix2.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.I7.ix2.p1.1.m1.1.1.3" xref="S3.I7.ix2.p1.1.m1.1.1.3.cmml"><mi id="S3.I7.ix2.p1.1.m1.1.1.3.2" xref="S3.I7.ix2.p1.1.m1.1.1.3.2.cmml">S</mi><mi id="S3.I7.ix2.p1.1.m1.1.1.3.3" xref="S3.I7.ix2.p1.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I7.ix2.p1.1.m1.1b"><apply id="S3.I7.ix2.p1.1.m1.1.1.cmml" xref="S3.I7.ix2.p1.1.m1.1.1"><times id="S3.I7.ix2.p1.1.m1.1.1.1.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.1"></times><ci id="S3.I7.ix2.p1.1.m1.1.1.2.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.2">𝐿</ci><apply id="S3.I7.ix2.p1.1.m1.1.1.3.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I7.ix2.p1.1.m1.1.1.3.1.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I7.ix2.p1.1.m1.1.1.3.2.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.3.2">𝑆</ci><ci id="S3.I7.ix2.p1.1.m1.1.1.3.3.cmml" xref="S3.I7.ix2.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.ix2.p1.1.m1.1c">LS_{i}</annotation></semantics></math>, perform validation and annotation of the song according to Russell’s quadrants.</p>
<ul id="S3.I7.ix2.I1" class="ltx_itemize">
<li id="S3.I7.ix2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I7.ix2.I1.ix1.p1" class="ltx_para">
<p id="S3.I7.ix2.I1.ix1.p1.1" class="ltx_p">7.2.1. Verify that the lyrics file is well structured, belongs to the correct audio clip, and that the emotion in the file is not ambiguous.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.7" class="ltx_p ltx_figure_panel">8. Define the bimodal dataset, <math id="alg1.7.m1.1" class="ltx_Math" alttext="Bm" display="inline"><semantics id="alg1.7.m1.1a"><mrow id="alg1.7.m1.1.1" xref="alg1.7.m1.1.1.cmml"><mi id="alg1.7.m1.1.1.2" xref="alg1.7.m1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="alg1.7.m1.1.1.1" xref="alg1.7.m1.1.1.1.cmml">​</mo><mi id="alg1.7.m1.1.1.3" xref="alg1.7.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.7.m1.1b"><apply id="alg1.7.m1.1.1.cmml" xref="alg1.7.m1.1.1"><times id="alg1.7.m1.1.1.1.cmml" xref="alg1.7.m1.1.1.1"></times><ci id="alg1.7.m1.1.1.2.cmml" xref="alg1.7.m1.1.1.2">𝐵</ci><ci id="alg1.7.m1.1.1.3.cmml" xref="alg1.7.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.7.m1.1c">Bm</annotation></semantics></math>, by keeping only the songs where audio and lyrics annotations match.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I8" class="ltx_itemize ltx_figure_panel">
<li id="S3.I8.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I8.ix1.p1" class="ltx_para">
<p id="S3.I8.ix1.p1.2" class="ltx_p">8.1. For each song, <math id="S3.I8.ix1.p1.1.m1.1" class="ltx_Math" alttext="ASV_{i}" display="inline"><semantics id="S3.I8.ix1.p1.1.m1.1a"><mrow id="S3.I8.ix1.p1.1.m1.1.1" xref="S3.I8.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I8.ix1.p1.1.m1.1.1.2" xref="S3.I8.ix1.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.I8.ix1.p1.1.m1.1.1.1" xref="S3.I8.ix1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I8.ix1.p1.1.m1.1.1.3" xref="S3.I8.ix1.p1.1.m1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I8.ix1.p1.1.m1.1.1.1a" xref="S3.I8.ix1.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.I8.ix1.p1.1.m1.1.1.4" xref="S3.I8.ix1.p1.1.m1.1.1.4.cmml"><mi id="S3.I8.ix1.p1.1.m1.1.1.4.2" xref="S3.I8.ix1.p1.1.m1.1.1.4.2.cmml">V</mi><mi id="S3.I8.ix1.p1.1.m1.1.1.4.3" xref="S3.I8.ix1.p1.1.m1.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I8.ix1.p1.1.m1.1b"><apply id="S3.I8.ix1.p1.1.m1.1.1.cmml" xref="S3.I8.ix1.p1.1.m1.1.1"><times id="S3.I8.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.1"></times><ci id="S3.I8.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.I8.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.3">𝑆</ci><apply id="S3.I8.ix1.p1.1.m1.1.1.4.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.I8.ix1.p1.1.m1.1.1.4.1.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.4">subscript</csymbol><ci id="S3.I8.ix1.p1.1.m1.1.1.4.2.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.4.2">𝑉</ci><ci id="S3.I8.ix1.p1.1.m1.1.1.4.3.cmml" xref="S3.I8.ix1.p1.1.m1.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I8.ix1.p1.1.m1.1c">ASV_{i}</annotation></semantics></math> and <math id="S3.I8.ix1.p1.2.m2.1" class="ltx_Math" alttext="LSV_{i}" display="inline"><semantics id="S3.I8.ix1.p1.2.m2.1a"><mrow id="S3.I8.ix1.p1.2.m2.1.1" xref="S3.I8.ix1.p1.2.m2.1.1.cmml"><mi id="S3.I8.ix1.p1.2.m2.1.1.2" xref="S3.I8.ix1.p1.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.I8.ix1.p1.2.m2.1.1.1" xref="S3.I8.ix1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I8.ix1.p1.2.m2.1.1.3" xref="S3.I8.ix1.p1.2.m2.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I8.ix1.p1.2.m2.1.1.1a" xref="S3.I8.ix1.p1.2.m2.1.1.1.cmml">​</mo><msub id="S3.I8.ix1.p1.2.m2.1.1.4" xref="S3.I8.ix1.p1.2.m2.1.1.4.cmml"><mi id="S3.I8.ix1.p1.2.m2.1.1.4.2" xref="S3.I8.ix1.p1.2.m2.1.1.4.2.cmml">V</mi><mi id="S3.I8.ix1.p1.2.m2.1.1.4.3" xref="S3.I8.ix1.p1.2.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I8.ix1.p1.2.m2.1b"><apply id="S3.I8.ix1.p1.2.m2.1.1.cmml" xref="S3.I8.ix1.p1.2.m2.1.1"><times id="S3.I8.ix1.p1.2.m2.1.1.1.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.1"></times><ci id="S3.I8.ix1.p1.2.m2.1.1.2.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.2">𝐿</ci><ci id="S3.I8.ix1.p1.2.m2.1.1.3.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.3">𝑆</ci><apply id="S3.I8.ix1.p1.2.m2.1.1.4.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.I8.ix1.p1.2.m2.1.1.4.1.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.I8.ix1.p1.2.m2.1.1.4.2.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.4.2">𝑉</ci><ci id="S3.I8.ix1.p1.2.m2.1.1.4.3.cmml" xref="S3.I8.ix1.p1.2.m2.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I8.ix1.p1.2.m2.1c">LSV_{i}</annotation></semantics></math>, if the annotated audio and lyrics quadrants match, the song is added to the bimodal dataset; otherwise, the song will be absent from the bimodal dataset (but present in the audio subset with a given quadrant and in the lyrics subset with a different quadrant).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.12" class="ltx_p ltx_figure_panel">9. Create the final complete and balanced audio, lyrics, and bimodal datasets.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I9" class="ltx_itemize ltx_figure_panel">
<li id="S3.I9.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I9.ix1.p1" class="ltx_para">
<p id="S3.I9.ix1.p1.3" class="ltx_p">9.1. The above <math id="S3.I9.ix1.p1.1.m1.1" class="ltx_Math" alttext="ASV" display="inline"><semantics id="S3.I9.ix1.p1.1.m1.1a"><mrow id="S3.I9.ix1.p1.1.m1.1.1" xref="S3.I9.ix1.p1.1.m1.1.1.cmml"><mi id="S3.I9.ix1.p1.1.m1.1.1.2" xref="S3.I9.ix1.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix1.p1.1.m1.1.1.1" xref="S3.I9.ix1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I9.ix1.p1.1.m1.1.1.3" xref="S3.I9.ix1.p1.1.m1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix1.p1.1.m1.1.1.1a" xref="S3.I9.ix1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I9.ix1.p1.1.m1.1.1.4" xref="S3.I9.ix1.p1.1.m1.1.1.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix1.p1.1.m1.1b"><apply id="S3.I9.ix1.p1.1.m1.1.1.cmml" xref="S3.I9.ix1.p1.1.m1.1.1"><times id="S3.I9.ix1.p1.1.m1.1.1.1.cmml" xref="S3.I9.ix1.p1.1.m1.1.1.1"></times><ci id="S3.I9.ix1.p1.1.m1.1.1.2.cmml" xref="S3.I9.ix1.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.I9.ix1.p1.1.m1.1.1.3.cmml" xref="S3.I9.ix1.p1.1.m1.1.1.3">𝑆</ci><ci id="S3.I9.ix1.p1.1.m1.1.1.4.cmml" xref="S3.I9.ix1.p1.1.m1.1.1.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix1.p1.1.m1.1c">ASV</annotation></semantics></math>, <math id="S3.I9.ix1.p1.2.m2.1" class="ltx_Math" alttext="LSV" display="inline"><semantics id="S3.I9.ix1.p1.2.m2.1a"><mrow id="S3.I9.ix1.p1.2.m2.1.1" xref="S3.I9.ix1.p1.2.m2.1.1.cmml"><mi id="S3.I9.ix1.p1.2.m2.1.1.2" xref="S3.I9.ix1.p1.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix1.p1.2.m2.1.1.1" xref="S3.I9.ix1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I9.ix1.p1.2.m2.1.1.3" xref="S3.I9.ix1.p1.2.m2.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix1.p1.2.m2.1.1.1a" xref="S3.I9.ix1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I9.ix1.p1.2.m2.1.1.4" xref="S3.I9.ix1.p1.2.m2.1.1.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix1.p1.2.m2.1b"><apply id="S3.I9.ix1.p1.2.m2.1.1.cmml" xref="S3.I9.ix1.p1.2.m2.1.1"><times id="S3.I9.ix1.p1.2.m2.1.1.1.cmml" xref="S3.I9.ix1.p1.2.m2.1.1.1"></times><ci id="S3.I9.ix1.p1.2.m2.1.1.2.cmml" xref="S3.I9.ix1.p1.2.m2.1.1.2">𝐿</ci><ci id="S3.I9.ix1.p1.2.m2.1.1.3.cmml" xref="S3.I9.ix1.p1.2.m2.1.1.3">𝑆</ci><ci id="S3.I9.ix1.p1.2.m2.1.1.4.cmml" xref="S3.I9.ix1.p1.2.m2.1.1.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix1.p1.2.m2.1c">LSV</annotation></semantics></math>, and <math id="S3.I9.ix1.p1.3.m3.1" class="ltx_Math" alttext="Bm" display="inline"><semantics id="S3.I9.ix1.p1.3.m3.1a"><mrow id="S3.I9.ix1.p1.3.m3.1.1" xref="S3.I9.ix1.p1.3.m3.1.1.cmml"><mi id="S3.I9.ix1.p1.3.m3.1.1.2" xref="S3.I9.ix1.p1.3.m3.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix1.p1.3.m3.1.1.1" xref="S3.I9.ix1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.I9.ix1.p1.3.m3.1.1.3" xref="S3.I9.ix1.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix1.p1.3.m3.1b"><apply id="S3.I9.ix1.p1.3.m3.1.1.cmml" xref="S3.I9.ix1.p1.3.m3.1.1"><times id="S3.I9.ix1.p1.3.m3.1.1.1.cmml" xref="S3.I9.ix1.p1.3.m3.1.1.1"></times><ci id="S3.I9.ix1.p1.3.m3.1.1.2.cmml" xref="S3.I9.ix1.p1.3.m3.1.1.2">𝐵</ci><ci id="S3.I9.ix1.p1.3.m3.1.1.3.cmml" xref="S3.I9.ix1.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix1.p1.3.m3.1c">Bm</annotation></semantics></math> datasets form the complete sets.</p>
</div>
</li>
<li id="S3.I9.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S3.I9.ix2.p1" class="ltx_para">
<p id="S3.I9.ix2.p1.3" class="ltx_p">9.2 From the datasets in 9.1, obtain balanced datasets, <math id="S3.I9.ix2.p1.1.m1.1" class="ltx_Math" alttext="ASV_{b}" display="inline"><semantics id="S3.I9.ix2.p1.1.m1.1a"><mrow id="S3.I9.ix2.p1.1.m1.1.1" xref="S3.I9.ix2.p1.1.m1.1.1.cmml"><mi id="S3.I9.ix2.p1.1.m1.1.1.2" xref="S3.I9.ix2.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix2.p1.1.m1.1.1.1" xref="S3.I9.ix2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.I9.ix2.p1.1.m1.1.1.3" xref="S3.I9.ix2.p1.1.m1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix2.p1.1.m1.1.1.1a" xref="S3.I9.ix2.p1.1.m1.1.1.1.cmml">​</mo><msub id="S3.I9.ix2.p1.1.m1.1.1.4" xref="S3.I9.ix2.p1.1.m1.1.1.4.cmml"><mi id="S3.I9.ix2.p1.1.m1.1.1.4.2" xref="S3.I9.ix2.p1.1.m1.1.1.4.2.cmml">V</mi><mi id="S3.I9.ix2.p1.1.m1.1.1.4.3" xref="S3.I9.ix2.p1.1.m1.1.1.4.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix2.p1.1.m1.1b"><apply id="S3.I9.ix2.p1.1.m1.1.1.cmml" xref="S3.I9.ix2.p1.1.m1.1.1"><times id="S3.I9.ix2.p1.1.m1.1.1.1.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.1"></times><ci id="S3.I9.ix2.p1.1.m1.1.1.2.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.I9.ix2.p1.1.m1.1.1.3.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.3">𝑆</ci><apply id="S3.I9.ix2.p1.1.m1.1.1.4.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.I9.ix2.p1.1.m1.1.1.4.1.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.4">subscript</csymbol><ci id="S3.I9.ix2.p1.1.m1.1.1.4.2.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.4.2">𝑉</ci><ci id="S3.I9.ix2.p1.1.m1.1.1.4.3.cmml" xref="S3.I9.ix2.p1.1.m1.1.1.4.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix2.p1.1.m1.1c">ASV_{b}</annotation></semantics></math>, <math id="S3.I9.ix2.p1.2.m2.1" class="ltx_Math" alttext="LSV_{b}" display="inline"><semantics id="S3.I9.ix2.p1.2.m2.1a"><mrow id="S3.I9.ix2.p1.2.m2.1.1" xref="S3.I9.ix2.p1.2.m2.1.1.cmml"><mi id="S3.I9.ix2.p1.2.m2.1.1.2" xref="S3.I9.ix2.p1.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix2.p1.2.m2.1.1.1" xref="S3.I9.ix2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.I9.ix2.p1.2.m2.1.1.3" xref="S3.I9.ix2.p1.2.m2.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix2.p1.2.m2.1.1.1a" xref="S3.I9.ix2.p1.2.m2.1.1.1.cmml">​</mo><msub id="S3.I9.ix2.p1.2.m2.1.1.4" xref="S3.I9.ix2.p1.2.m2.1.1.4.cmml"><mi id="S3.I9.ix2.p1.2.m2.1.1.4.2" xref="S3.I9.ix2.p1.2.m2.1.1.4.2.cmml">V</mi><mi id="S3.I9.ix2.p1.2.m2.1.1.4.3" xref="S3.I9.ix2.p1.2.m2.1.1.4.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix2.p1.2.m2.1b"><apply id="S3.I9.ix2.p1.2.m2.1.1.cmml" xref="S3.I9.ix2.p1.2.m2.1.1"><times id="S3.I9.ix2.p1.2.m2.1.1.1.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.1"></times><ci id="S3.I9.ix2.p1.2.m2.1.1.2.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.2">𝐿</ci><ci id="S3.I9.ix2.p1.2.m2.1.1.3.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.3">𝑆</ci><apply id="S3.I9.ix2.p1.2.m2.1.1.4.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.I9.ix2.p1.2.m2.1.1.4.1.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.I9.ix2.p1.2.m2.1.1.4.2.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.4.2">𝑉</ci><ci id="S3.I9.ix2.p1.2.m2.1.1.4.3.cmml" xref="S3.I9.ix2.p1.2.m2.1.1.4.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix2.p1.2.m2.1c">LSV_{b}</annotation></semantics></math>, and <math id="S3.I9.ix2.p1.3.m3.1" class="ltx_Math" alttext="Bm_{b}" display="inline"><semantics id="S3.I9.ix2.p1.3.m3.1a"><mrow id="S3.I9.ix2.p1.3.m3.1.1" xref="S3.I9.ix2.p1.3.m3.1.1.cmml"><mi id="S3.I9.ix2.p1.3.m3.1.1.2" xref="S3.I9.ix2.p1.3.m3.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.I9.ix2.p1.3.m3.1.1.1" xref="S3.I9.ix2.p1.3.m3.1.1.1.cmml">​</mo><msub id="S3.I9.ix2.p1.3.m3.1.1.3" xref="S3.I9.ix2.p1.3.m3.1.1.3.cmml"><mi id="S3.I9.ix2.p1.3.m3.1.1.3.2" xref="S3.I9.ix2.p1.3.m3.1.1.3.2.cmml">m</mi><mi id="S3.I9.ix2.p1.3.m3.1.1.3.3" xref="S3.I9.ix2.p1.3.m3.1.1.3.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I9.ix2.p1.3.m3.1b"><apply id="S3.I9.ix2.p1.3.m3.1.1.cmml" xref="S3.I9.ix2.p1.3.m3.1.1"><times id="S3.I9.ix2.p1.3.m3.1.1.1.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.1"></times><ci id="S3.I9.ix2.p1.3.m3.1.1.2.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.2">𝐵</ci><apply id="S3.I9.ix2.p1.3.m3.1.1.3.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.I9.ix2.p1.3.m3.1.1.3.1.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.I9.ix2.p1.3.m3.1.1.3.2.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.3.2">𝑚</ci><ci id="S3.I9.ix2.p1.3.m3.1.1.3.3.cmml" xref="S3.I9.ix2.p1.3.m3.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I9.ix2.p1.3.m3.1c">Bm_{b}</annotation></semantics></math>, respectively, by discarding samples from the more represented quadrants, respecting genre balancing.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
</div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Dataset Description</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The resulting datasets are hereafter termed <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">MERGE Audio</span>, <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">MERGE Lyrics</span>, and <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">MERGE Bimodal</span> and are summarized in Table <a href="#S3.T1" title="TABLE I ‣ III-C Dataset Description ‣ III Proposed Datasets ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Datasets used for evaluation with respective sample distribution.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Q1</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Q2</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Q3</span></th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Q4</span></th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.1.1" class="ltx_text">MERGE Audio Complete</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">875</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">915</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">808</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">956</td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">3554</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.1.1" class="ltx_text">MERGE Audio Balanced</span></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center">808</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">808</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">808</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">808</td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_center">3232</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.4.3.1.1" class="ltx_text">MERGE Lyrics Complete</span></td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center">710</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">621</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center">637</td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center">2568</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.5.4.1.1" class="ltx_text">MERGE Lyrics Balanced</span></td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.5.4.6" class="ltx_td ltx_align_center">2400</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.6.5.1.1" class="ltx_text">MERGE Bimodal Complete</span></td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center">525</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center">673</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">500</td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center">518</td>
<td id="S3.T1.1.6.5.6" class="ltx_td ltx_align_center">2216</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.7.6.1.1" class="ltx_text">MERGE Bimodal Balanced</span></td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">500</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">500</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb">500</td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb">500</td>
<td id="S3.T1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_bb">2000</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In short, the complete audio dataset contains 3554 samples, while its balanced version comprises 3232 (808 per quadrant). For lyrics, the complete set includes 2568 samples, while the balanced subset has 600 samples per quadrant, for a total of 2400 samples. Finally, the complete bimodal dataset comprises 2216 samples, whereas its balanced subset contains exactly 2000 samples (500 samples per quadrant). As can be observed, the audio sets are larger since, as mentioned previously, retrieving lyrics from the corresponding songs was not always possible. In addition, the bimodal dataset is smaller, as the annotated audio audio and lyrics quadrants do not always match.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Besides audio clips and lyrics, each dataset provides individual metadata and train-validation-test (TVT) splits.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">A metadata file contains the following attributes: song identifier, title, artist, year, genre tags, emotion tags, and annotated quadrant. By providing this additional data, we enable our datasets to be used in related tasks, such as genre or era recognition.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Regarding the TVT splits provided for each dataset, two configurations can be found for training, validation, and testing: 70-15-15 and 40-30-30. These splits were obtained following the described procedure to maximize quadrant balancing and genre distribution over each set. In addition to experiments with k-fold cross-validation, we encourage researchers to use the proposed TVT splits instead of performing their own splits to ensure reproducibility.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Baseline Methodologies and Evaluation Strategy</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conducted several experiments to establish a baseline for benchmarking and provide a comprehensive evaluation of the proposed datasets. Feature engineering, classical machine learning, and deep learning approaches were applied to each modality. This section presents the overall evaluation strategy employed in this study, followed by descriptions of the baseline methodologies developed. After discussing the optimization strategy followed for each method, a table summarizing the optimal hyperparameters obtained is also provided.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Evaluation Strategy</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Two methods were used to evaluate the performance of each methodology. First, a repeated stratified k-fold cross-validation strategy was employed, with ten folds and ten repetitions. The hyperparameters of the different models were optimized for each fold (e.g., for DL models, the best optimizer and optimal learning rate, batch size, and number of epochs to run the model). The final reported results are the average of the metrics obtained from all folds. The second method employed the two described TVT splits (70-15-15 and 40-30-30). Each model was also optimized for the hyperparameters, and the final results were obtained directly after the optimization step.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Statistical significance tests were performed to compare the classification results from the proposed models and modalities. Differences are statistically significant for p <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><lt id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&lt;</annotation></semantics></math> 0.05.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Audio Classical ML</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our approach in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> served as the basis for the conducted classical ML experiments. All songs are standardized to a 16-bit PCM signed WAV format at a 22.5 kHz sample rate and mono channel. Features related to the eight standard musical dimensions (melody, harmony, rhythm, dynamics, expressivity, texture, and form) are then extracted. Further details about features are available in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The ReliefF algorithm is then employed for feature ranking and selection. For classification, SVMs were used, and their optimal hyperparameters were obtained through a Bayesian search. This was used as a faster alternative that yielded equal or better results than grid search. To perform a Bayesian search, the boundaries of the search space and the step size between the experimented values must be set. As for the kernel, the radial basis function (RBF) was selected, as earlier experimentation led to better results. This kernel requires tuning two hyperparameters: cost (C) and gamma. Here, the interval for gamma was set to [1e-6, 100]. For the cost parameter, the interval was set to [1e-6, 1500]. A logarithmic uniform step size was defined in both, allowing a higher number of smaller values to be tested. The optimal hyperparameters can be found in Table <a href="#S4.SS2" title="IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Most of the implementations (SVMs and the Bayesian search algorithm used for model optimization) are provided by the scikit-learn Python library<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://scikit-learn.org/</span>.</span></span></span>. ReliefF is an exception, being provided by the <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">attrEval</span> function of the CORElearn R package<span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.rdocumentation.org/packages/CORElearn/</span>.</span></span></span>, for which no equivalent was found in Python.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Two approaches were followed to optimize the SVMs: repeated 10-fold cross-validation and the described TVT splits. In both methods, different numbers of features were tested to find the feature set that yielded the best results. This was performed independently for each of the four audio datasets mentioned in Section <a href="#S3.SS3" title="III-C Dataset Description ‣ III Proposed Datasets ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>.</p>
</div>
<figure id="S4.SS2.tab1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span id="S4.SS2.tab1.2.1" class="ltx_text ltx_font_bold">Audio Classical ML</span> Hyperparameters </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.SS2.tab1.3" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS2.tab1.3.1.1" class="ltx_tr">
<th id="S4.SS2.tab1.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t">
<span id="S4.SS2.tab1.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.1.1.1.1.1" class="ltx_p" style="width:56.9pt;">Dataset</span>
</span>
</th>
<td id="S4.SS2.tab1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Evaluation</td>
<td id="S4.SS2.tab1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Best Hyperparameters</td>
</tr>
<tr id="S4.SS2.tab1.3.2.2" class="ltx_tr">
<th id="S4.SS2.tab1.3.2.2.1" class="ltx_td ltx_align_top ltx_th ltx_th_row"></th>
<td id="S4.SS2.tab1.3.2.2.2" class="ltx_td ltx_align_center">Strategy</td>
<td id="S4.SS2.tab1.3.2.2.3" class="ltx_td ltx_align_center">Kernel</td>
<td id="S4.SS2.tab1.3.2.2.4" class="ltx_td ltx_align_center">C</td>
<td id="S4.SS2.tab1.3.2.2.5" class="ltx_td ltx_align_center">Gamma</td>
</tr>
<tr id="S4.SS2.tab1.3.3.3" class="ltx_tr">
<th id="S4.SS2.tab1.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt">
<span id="S4.SS2.tab1.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.3.3.1.1.1" class="ltx_p" style="width:56.9pt;">4QAED</span>
</span>
</th>
<td id="S4.SS2.tab1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS2.tab1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.SS2.tab1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">1.7323</td>
<td id="S4.SS2.tab1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">6.6683e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.4.4" class="ltx_tr">
<th id="S4.SS2.tab1.3.4.4.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.SS2.tab1.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.4.4.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.SS2.tab1.3.4.4.1.1.1.1" class="ltx_text">MERGE</span>
Audio
Complete</span>
</span>
</th>
<td id="S4.SS2.tab1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS2.tab1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.SS2.tab1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">124.3628</td>
<td id="S4.SS2.tab1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">1.0292e-3</td>
</tr>
<tr id="S4.SS2.tab1.3.5.5" class="ltx_tr">
<td id="S4.SS2.tab1.3.5.5.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS2.tab1.3.5.5.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t">4.3586</td>
<td id="S4.SS2.tab1.3.5.5.4" class="ltx_td ltx_align_center ltx_border_t">2.3548e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.6.6" class="ltx_tr">
<td id="S4.SS2.tab1.3.6.6.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS2.tab1.3.6.6.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t">5000</td>
<td id="S4.SS2.tab1.3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">9.8942e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.7.7" class="ltx_tr">
<th id="S4.SS2.tab1.3.7.7.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" rowspan="3">
<span id="S4.SS2.tab1.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.7.7.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.SS2.tab1.3.7.7.1.1.1.1" class="ltx_text">MERGE</span>
Audio
Balanced</span>
</span>
</th>
<td id="S4.SS2.tab1.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.SS2.tab1.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">2.5737</td>
<td id="S4.SS2.tab1.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t">4.2191e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.8.8" class="ltx_tr">
<td id="S4.SS2.tab1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS2.tab1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">2.7421</td>
<td id="S4.SS2.tab1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">2.7657e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.9.9" class="ltx_tr">
<td id="S4.SS2.tab1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS2.tab1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">3.0406</td>
<td id="S4.SS2.tab1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">3.0309e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.10.10" class="ltx_tr">
<th id="S4.SS2.tab1.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.SS2.tab1.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.10.10.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.SS2.tab1.3.10.10.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</th>
<td id="S4.SS2.tab1.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS2.tab1.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.SS2.tab1.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">3.7924</td>
<td id="S4.SS2.tab1.3.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">9.0179e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.11.11" class="ltx_tr">
<td id="S4.SS2.tab1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS2.tab1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">414.8008</td>
<td id="S4.SS2.tab1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">1.5974e-6</td>
</tr>
<tr id="S4.SS2.tab1.3.12.12" class="ltx_tr">
<td id="S4.SS2.tab1.3.12.12.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS2.tab1.3.12.12.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t">6.4457</td>
<td id="S4.SS2.tab1.3.12.12.4" class="ltx_td ltx_align_center ltx_border_t">1.0137e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.13.13" class="ltx_tr">
<th id="S4.SS2.tab1.3.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.SS2.tab1.3.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS2.tab1.3.13.13.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.SS2.tab1.3.13.13.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</th>
<td id="S4.SS2.tab1.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.SS2.tab1.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">4.4393</td>
<td id="S4.SS2.tab1.3.13.13.5" class="ltx_td ltx_align_center ltx_border_t">1.1031e-3</td>
</tr>
<tr id="S4.SS2.tab1.3.14.14" class="ltx_tr">
<td id="S4.SS2.tab1.3.14.14.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS2.tab1.3.14.14.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.14.14.3" class="ltx_td ltx_align_center ltx_border_t">4.2639</td>
<td id="S4.SS2.tab1.3.14.14.4" class="ltx_td ltx_align_center ltx_border_t">4.7517e-4</td>
</tr>
<tr id="S4.SS2.tab1.3.15.15" class="ltx_tr">
<td id="S4.SS2.tab1.3.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.SS2.tab1.3.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">RBF</td>
<td id="S4.SS2.tab1.3.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1303.7637</td>
<td id="S4.SS2.tab1.3.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">3.7781e-4</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<section id="S4.SS3" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Audio Deep Learning</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Regarding DL models, the classifier is adapted from the CNN-based model proposed by Choi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for music multi-label tag classification. The feature extraction part of the architecture includes four convolutional blocks. Each block comprises a sequence of convolutional, pooling, batch normalization, and dropout layers (to reduce feature redundancy). After flattening, the output of the blocks is sent to a classifier section (consisting of a dropout and two dense layers).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The network inputs are Mel spectrogram representations of each sample’s raw audio signal, ultimately outputting one of the four quadrants of Russell’s Circumplex Model. The architecture is depicted in Figure <a href="#S4.F2" title="Figure 2 ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We opted not to adapt the CRNN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (described in Section <a href="#S2.SS4" title="II-D MER Systems ‣ II Background and Related Work ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a>) as it has previously shown to be unstable with the amount of data available on our datasets through experimentation.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2407.06060/assets/figs/DL_Baseline_Journal.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="473" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture for audio DL experimentation as seen in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The feature learning portion comprises four 2D convolutional blocks. The output is processed by the classifier portion, done sequentially by a dropout layer and two dense layers, outputting one of four quadrants.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The Mel spectrogram representations for the audio samples are generated using librosa’s <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">melspectrogram</span> implementation.
We employ a 16kHz sample rate and default hop and window length values. The implementation generates a power spectrogram that is transformed into a magnitude spectrogram, meaning that the y-axis changes from frequency to decibel.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">The sample rate is lower when compared to the classical experiments to reduce the complexity of the model. It has also been stated that such reduction does not impact the model’s performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, as confirmed experimentally.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">A Bayesian optimization strategy akin to the one used for the classical approach is applied using the KerasTunner library<span id="footnote18" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://keras.io/api/keras_tuner/</span>.</span></span></span>. Hyperparameters tuned using this strategy include optimizer (Stochastic Gradient Descent (SGD) and Adam), learning rate, and batch size.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">As for the learning rate, the search space is set between [1e-5, 1e-2] with a logarithmic step size of 10, meaning that the actual values tested are 1e-5, 1e-4, 1e-3, and 1e-2. A learning rate higher than 1e-2 is inefficient for learning, while a smaller one may benefit Adam due to its more aggressive optimization approach. The batch size search space is in the range [32, 256], and the step size between values is also logarithmic and set to 2, meaning each consecutive value is doubled compared to its predecessor. Values higher than 256 would make the training phase of the model very resource-intensive with little benefit, as found experimentally. In contrast, lower values can benefit from certain optimizer and learning rate combinations.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">We implemented an early stopping strategy for the classifier methodologies to prevent the models from overfitting on the training data. The training phase of the model was halted when accuracy reached values above a threshold of 90%, a value found to be optimal in previously conducted experiments by our team on this architecture, with a hard limit set to 200 epochs. The optimal hyperparameters for training a network depending on the dataset at hand can be found in Table <a href="#S4.SS3" title="IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.</p>
</div>
<figure id="S4.SS3.tab1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span><span id="S4.SS3.tab1.2.1" class="ltx_text ltx_font_bold">Audio DL</span> Hyperparameters </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.SS3.tab1.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS3.tab1.3.1.1" class="ltx_tr">
<td id="S4.SS3.tab1.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.SS3.tab1.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.1.1.1.1.1" class="ltx_p" style="width:39.8pt;">Dataset</span>
</span>
</td>
<td id="S4.SS3.tab1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Evaluation</td>
<td id="S4.SS3.tab1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Best Hyperparameters</td>
</tr>
<tr id="S4.SS3.tab1.3.2.2" class="ltx_tr">
<td id="S4.SS3.tab1.3.2.2.1" class="ltx_td ltx_align_top"></td>
<td id="S4.SS3.tab1.3.2.2.2" class="ltx_td ltx_align_center">Strategy</td>
<td id="S4.SS3.tab1.3.2.2.3" class="ltx_td ltx_align_center">Batch Size</td>
<td id="S4.SS3.tab1.3.2.2.4" class="ltx_td ltx_align_center">Optimizer</td>
<td id="S4.SS3.tab1.3.2.2.5" class="ltx_td ltx_align_center">Learning Rate</td>
</tr>
<tr id="S4.SS3.tab1.3.3.3" class="ltx_tr">
<td id="S4.SS3.tab1.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.SS3.tab1.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.3.3.1.1.1" class="ltx_p" style="width:39.8pt;">4QAED</span>
</span>
</td>
<td id="S4.SS3.tab1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS3.tab1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">150</td>
<td id="S4.SS3.tab1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">SGD</td>
<td id="S4.SS3.tab1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.4.4" class="ltx_tr">
<td id="S4.SS3.tab1.3.4.4.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" rowspan="3">
<span id="S4.SS3.tab1.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.4.4.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.SS3.tab1.3.4.4.1.1.1.1" class="ltx_text">MERGE</span>
Audio
Complete</span>
</span>
</td>
<td id="S4.SS3.tab1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS3.tab1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">150</td>
<td id="S4.SS3.tab1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">SGD</td>
<td id="S4.SS3.tab1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.5.5" class="ltx_tr">
<td id="S4.SS3.tab1.3.5.5.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS3.tab1.3.5.5.2" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S4.SS3.tab1.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t">Adam</td>
<td id="S4.SS3.tab1.3.5.5.4" class="ltx_td ltx_align_center ltx_border_t">1e-3</td>
</tr>
<tr id="S4.SS3.tab1.3.6.6" class="ltx_tr">
<td id="S4.SS3.tab1.3.6.6.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS3.tab1.3.6.6.2" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S4.SS3.tab1.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.7.7" class="ltx_tr">
<td id="S4.SS3.tab1.3.7.7.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S4.SS3.tab1.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.7.7.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.SS3.tab1.3.7.7.1.1.1.1" class="ltx_text">MERGE</span>
Audio
Balanced</span>
</span>
</td>
<td id="S4.SS3.tab1.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.SS3.tab1.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.8.8" class="ltx_tr">
<td id="S4.SS3.tab1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS3.tab1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.9.9" class="ltx_tr">
<td id="S4.SS3.tab1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS3.tab1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S4.SS3.tab1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">Adam</td>
<td id="S4.SS3.tab1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">1e-3</td>
</tr>
<tr id="S4.SS3.tab1.3.10.10" class="ltx_tr">
<td id="S4.SS3.tab1.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="3">
<span id="S4.SS3.tab1.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.10.10.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.SS3.tab1.3.10.10.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</td>
<td id="S4.SS3.tab1.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.SS3.tab1.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">150</td>
<td id="S4.SS3.tab1.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">SGD</td>
<td id="S4.SS3.tab1.3.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.11.11" class="ltx_tr">
<td id="S4.SS3.tab1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS3.tab1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.12.12" class="ltx_tr">
<td id="S4.SS3.tab1.3.12.12.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.SS3.tab1.3.12.12.2" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S4.SS3.tab1.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.12.12.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.13.13" class="ltx_tr">
<td id="S4.SS3.tab1.3.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.SS3.tab1.3.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS3.tab1.3.13.13.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.SS3.tab1.3.13.13.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</td>
<td id="S4.SS3.tab1.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.SS3.tab1.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.13.13.5" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.14.14" class="ltx_tr">
<td id="S4.SS3.tab1.3.14.14.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.SS3.tab1.3.14.14.2" class="ltx_td ltx_align_center ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.14.14.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.14.14.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.SS3.tab1.3.15.15" class="ltx_tr">
<td id="S4.SS3.tab1.3.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.SS3.tab1.3.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">150</td>
<td id="S4.SS3.tab1.3.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">SGD</td>
<td id="S4.SS3.tab1.3.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1e-2</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<section id="S4.SS4" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Lyrics Classical ML</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The basis for the following machine learning experiments, which includes data pre-processing, feature selection, and the creation of classification and models, is described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The lyrics are standardized through a series of operations. These include correcting spelling errors, eliminating lyrics that are not in English, removing lyrics with less than 100 characters, getting rid of text that is unrelated to the lyrics, such as the names of artists, composers, and instruments, and eliminating common patterns in lyrics such as [Chorus x2], [Verse1 x2], among others. Additionally, the lyrics are complemented according to the corresponding audio. This means that repetitions of the chorus in the audio are added to the lyrics. Similarly, metadata defined in the lyrics (e.g., [Chorus x2]) implies adding one more instance of the chorus to the lyrics. After making these additions, the lyrics are then checked for any remaining cases of these patterns and eliminated. This process is described in greater detail in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">As for feature extraction, we use the features proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which are briefly divided into content-based (e.g., bags-of-words), stylistic (e.g., number of occurrences of nouns, adjectives, adverbs, slang words, etc.), song-structure (e.g., number of repetitions of the chorus and song title, etc.) and semantic features (e.g., features extracted from frameworks such as Synesketch<span id="footnote19" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/parthenocissus/synesketch_v2.1/</span>.</span></span></span>, ConceptNet<span id="footnote20" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://conceptnet.io/</span>.</span></span></span>, LIWC<span id="footnote21" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.liwc.app/</span>.</span></span></span> and General Inquirer<span id="footnote22" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://inquirer.sites.fas.harvard.edu/</span>.</span></span></span>, as well as features based on word dictionaries (gazetteers) related to each of Russell’s emotion quadrants).</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">As in audio, SVMs are used to create classification models, which are parameterized with an RBF kernel and tuned using Bayesian parameter search. The optimal hyperparameters are available in Table <a href="#S4.T4" title="TABLE IV ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We employ the ReliefF algorithm for feature selection and ranking.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">The optimization strategy presented in Section <a href="#S4.SS2" title="IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a> (repeated 10-fold cross-validation and TVT) was also applied to the lyrics counterpart.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span><span id="S4.T4.2.1" class="ltx_text ltx_font_bold">Lyrics Classical ML</span> Hyperparameters </figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t">
<span id="S4.T4.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.1.1.1.1.1" class="ltx_p" style="width:42.7pt;">Dataset</span>
</span>
</th>
<td id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Evaluation</td>
<td id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Best Hyperparameters</td>
</tr>
<tr id="S4.T4.3.2.2" class="ltx_tr">
<th id="S4.T4.3.2.2.1" class="ltx_td ltx_align_top ltx_th ltx_th_row"></th>
<td id="S4.T4.3.2.2.2" class="ltx_td ltx_align_center">Strategy</td>
<td id="S4.T4.3.2.2.3" class="ltx_td ltx_align_center">Kernel</td>
<td id="S4.T4.3.2.2.4" class="ltx_td ltx_align_center">C</td>
<td id="S4.T4.3.2.2.5" class="ltx_td ltx_align_center">Gamma</td>
</tr>
<tr id="S4.T4.3.3.3" class="ltx_tr">
<th id="S4.T4.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt">
<span id="S4.T4.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.3.3.1.1.1" class="ltx_p" style="width:42.7pt;">LED</span>
</span>
</th>
<td id="S4.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T4.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">2.2341</td>
<td id="S4.T4.3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">1.8399e-3</td>
</tr>
<tr id="S4.T4.3.4.4" class="ltx_tr">
<th id="S4.T4.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.T4.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.4.4.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T4.3.4.4.1.1.1.1" class="ltx_text">MERGE Lyrics Complete</span></span>
</span>
</th>
<td id="S4.T4.3.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T4.3.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T4.3.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">8.0471e-1</td>
<td id="S4.T4.3.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">5.5370e-5</td>
</tr>
<tr id="S4.T4.3.5.5" class="ltx_tr">
<td id="S4.T4.3.5.5.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T4.3.5.5.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t">8.3447e-1</td>
<td id="S4.T4.3.5.5.4" class="ltx_td ltx_align_center ltx_border_t">4.7805e-3</td>
</tr>
<tr id="S4.T4.3.6.6" class="ltx_tr">
<td id="S4.T4.3.6.6.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T4.3.6.6.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t">6.3741e-1</td>
<td id="S4.T4.3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">3.1121e-3</td>
</tr>
<tr id="S4.T4.3.7.7" class="ltx_tr">
<th id="S4.T4.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" rowspan="3">
<span id="S4.T4.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.7.7.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T4.3.7.7.1.1.1.1" class="ltx_text">MERGE Lyrics Balanced</span></span>
</span>
</th>
<td id="S4.T4.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T4.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">1.6424</td>
<td id="S4.T4.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t">1.7503e-3</td>
</tr>
<tr id="S4.T4.3.8.8" class="ltx_tr">
<td id="S4.T4.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T4.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">6.7008e-1</td>
<td id="S4.T4.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">1.1359e-2</td>
</tr>
<tr id="S4.T4.3.9.9" class="ltx_tr">
<td id="S4.T4.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T4.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">1026.2858</td>
<td id="S4.T4.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">1.8928e-6</td>
</tr>
<tr id="S4.T4.3.10.10" class="ltx_tr">
<th id="S4.T4.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.T4.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.10.10.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T4.3.10.10.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</th>
<td id="S4.T4.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T4.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T4.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">7.7807e-1</td>
<td id="S4.T4.3.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">1.9050e-3</td>
</tr>
<tr id="S4.T4.3.11.11" class="ltx_tr">
<td id="S4.T4.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T4.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">6.3023e-1</td>
<td id="S4.T4.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">8.4513e-3</td>
</tr>
<tr id="S4.T4.3.12.12" class="ltx_tr">
<td id="S4.T4.3.12.12.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T4.3.12.12.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t">1.2022</td>
<td id="S4.T4.3.12.12.4" class="ltx_td ltx_align_center ltx_border_t">7.8160e-3</td>
</tr>
<tr id="S4.T4.3.13.13" class="ltx_tr">
<th id="S4.T4.3.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.T4.3.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.13.13.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T4.3.13.13.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</th>
<td id="S4.T4.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T4.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">6.6272e-1</td>
<td id="S4.T4.3.13.13.5" class="ltx_td ltx_align_center ltx_border_t">2.4404e-3</td>
</tr>
<tr id="S4.T4.3.14.14" class="ltx_tr">
<td id="S4.T4.3.14.14.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T4.3.14.14.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T4.3.14.14.3" class="ltx_td ltx_align_center ltx_border_t">1.1839</td>
<td id="S4.T4.3.14.14.4" class="ltx_td ltx_align_center ltx_border_t">6.8337e-4</td>
</tr>
<tr id="S4.T4.3.15.15" class="ltx_tr">
<td id="S4.T4.3.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.T4.3.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">RBF</td>
<td id="S4.T4.3.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1.3006</td>
<td id="S4.T4.3.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1.7698e-3</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Lyrics Deep Learning</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We propose an approach focused on exploiting a combination of word embeddings with SVMs<span id="footnote23" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>Other classical ML approaches were evaluated (e.g., K-Nearest Neighbours and Random Forest), but SVMs achieved the best results.</span></span></span> or CNNs. Regarding the latter, we implemented a CNN architecture, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. We evaluated various CNN-based architecture configurations using the 180 subset of the LED dataset. The final architecture receives previously embedded lyrics as its input and processes them through 4 consecutive one-dimensional convolutional blocks. The features learned in the convolutional blocks are fed to a set of layers in the following sequence: dense-dropout-dense-dropout-dense. The final dense layer outputs the predicted label.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">After experimenting with several word embedding approaches, we obtained the embedded vectors through the Robustly Optimized BERT Pre-Training Approach (RoBERTa) pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The lyrics of each song are fed to the model after converting to lowercase and replacing explicit newline symbols with blank spaces to indicate new verses.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">After experimentation, we found that encoding the full lyrics performed better than encoding individual verses. However, a caveat of obtaining RoBERTa’s embeddings from the available HuggingFace implementation<span id="footnote24" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span>Available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/sentence-transformers/all-roberta-large-v1</span>.</span></span></span> limits the input to 512 characters, meaning that lyrics had to be truncated. The architecture is depicted in Figure <a href="#S4.F3" title="Figure 3 ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2407.06060/assets/figs/Lyrics_CNN-Model.jpg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="475" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Architecture for lyrics DL experimentation. Like its audio counterpart, the feature learning portion comprises four 1D convolutional blocks, as the inputs are word embedding vectors. The classifier portion includes an additional dropout and dense layer, with a higher number of units to process the information volume properly.</figcaption>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Similarly to the audio modality, optimization is conducted using scikit-learn’s Bayesian search for SVM, where different kernels were experimented with. Also, we again employ the KerasTuner library used for the CNN-based methodology. We define the search range of each parameter as the values close to the default scikit-learn parameters.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">As for the CNN hyperparameters, the same optimizers and learning rate ranges experimented with for the audio modality were kept. We increased the batch size range to [32, 1024], as it is possible for the model to process a larger batch of lyrics at a time when compared with the audio counterpart. Table <a href="#S4.T5" title="TABLE V ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the best hyperparameters found for the SVM classifier.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span><span id="S4.T5.2.1" class="ltx_text ltx_font_bold">Lyrics DL</span> Hyperparameters </figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t">
<span id="S4.T5.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.1.1.1" class="ltx_p" style="width:37.0pt;">Dataset</span>
</span>
</th>
<td id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Evaluation</td>
<td id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Best Hyperparameters</td>
</tr>
<tr id="S4.T5.3.2.2" class="ltx_tr">
<th id="S4.T5.3.2.2.1" class="ltx_td ltx_align_top ltx_th ltx_th_row"></th>
<td id="S4.T5.3.2.2.2" class="ltx_td ltx_align_center">Strategy</td>
<td id="S4.T5.3.2.2.3" class="ltx_td ltx_align_center">Kernel</td>
<td id="S4.T5.3.2.2.4" class="ltx_td ltx_align_center">C</td>
<td id="S4.T5.3.2.2.5" class="ltx_td ltx_align_center">Gamma</td>
<td id="S4.T5.3.2.2.6" class="ltx_td ltx_align_center">Degree</td>
</tr>
<tr id="S4.T5.3.3.3" class="ltx_tr">
<th id="S4.T5.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt">
<span id="S4.T5.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.3.1.1.1" class="ltx_p" style="width:37.0pt;">LED</span>
</span>
</th>
<td id="S4.T5.3.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T5.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">7.6095e-1</td>
<td id="S4.T5.3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">1.0088</td>
<td id="S4.T5.3.3.3.6" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.T5.3.4.4" class="ltx_tr">
<th id="S4.T5.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.T5.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.4.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T5.3.4.4.1.1.1.1" class="ltx_text">MERGE Lyrics Complete</span></span>
</span>
</th>
<td id="S4.T5.3.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T5.3.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">Linear</td>
<td id="S4.T5.3.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">1.1056</td>
<td id="S4.T5.3.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td id="S4.T5.3.4.4.6" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.T5.3.5.5" class="ltx_tr">
<td id="S4.T5.3.5.5.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T5.3.5.5.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T5.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t">1500</td>
<td id="S4.T5.3.5.5.4" class="ltx_td ltx_align_center ltx_border_t">2.5975</td>
<td id="S4.T5.3.5.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.6.6" class="ltx_tr">
<td id="S4.T5.3.6.6.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T5.3.6.6.2" class="ltx_td ltx_align_center ltx_border_t">Linear</td>
<td id="S4.T5.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t">4.8493e-1</td>
<td id="S4.T5.3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.3.6.6.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.7.7" class="ltx_tr">
<th id="S4.T5.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" rowspan="3">
<span id="S4.T5.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.7.7.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T5.3.7.7.1.1.1.1" class="ltx_text">MERGE Lyrics Balanced</span></span>
</span>
</th>
<td id="S4.T5.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T5.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">Poly</td>
<td id="S4.T5.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">1e-6</td>
<td id="S4.T5.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t">95.1005</td>
<td id="S4.T5.3.7.7.6" class="ltx_td ltx_align_center ltx_border_t">2</td>
</tr>
<tr id="S4.T5.3.8.8" class="ltx_tr">
<td id="S4.T5.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T5.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T5.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">1500</td>
<td id="S4.T5.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">1.7766e-4</td>
<td id="S4.T5.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.9.9" class="ltx_tr">
<td id="S4.T5.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T5.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">Linear</td>
<td id="S4.T5.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">3.5212e-1</td>
<td id="S4.T5.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.10.10" class="ltx_tr">
<th id="S4.T5.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.T5.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.10.10.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T5.3.10.10.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</th>
<td id="S4.T5.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T5.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T5.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">397.6961</td>
<td id="S4.T5.3.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">2.8527</td>
<td id="S4.T5.3.10.10.6" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.T5.3.11.11" class="ltx_tr">
<td id="S4.T5.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T5.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">Linear</td>
<td id="S4.T5.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">2.0852</td>
<td id="S4.T5.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.12.12" class="ltx_tr">
<td id="S4.T5.3.12.12.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T5.3.12.12.2" class="ltx_td ltx_align_center ltx_border_t">Linear</td>
<td id="S4.T5.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t">4.7367e-1</td>
<td id="S4.T5.3.12.12.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.3.12.12.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.13.13" class="ltx_tr">
<th id="S4.T5.3.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.T5.3.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.13.13.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T5.3.13.13.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</th>
<td id="S4.T5.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T5.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T5.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">1500</td>
<td id="S4.T5.3.13.13.5" class="ltx_td ltx_align_center ltx_border_t">3.3525</td>
<td id="S4.T5.3.13.13.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.14.14" class="ltx_tr">
<td id="S4.T5.3.14.14.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T5.3.14.14.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T5.3.14.14.3" class="ltx_td ltx_align_center ltx_border_t">1500</td>
<td id="S4.T5.3.14.14.4" class="ltx_td ltx_align_center ltx_border_t">1.7077e-2</td>
<td id="S4.T5.3.14.14.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.3.15.15" class="ltx_tr">
<td id="S4.T5.3.15.15.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.T5.3.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Linear</td>
<td id="S4.T5.3.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">3.6914e-1</td>
<td id="S4.T5.3.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
<td id="S4.T5.3.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">-</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.4.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.5.2" class="ltx_text ltx_font_italic">Bimodal Classical ML</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">We perform feature-level fusion to combine audio and lyric modalities in classical machine learning. The combined audio and lyrics features are fed to the ReliefF feature selection algorithm altogether, with the rest of the pipeline remaining unchanged.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">One caveat that required special attention was the thousands of content-based features extracted from lyrics, which initially led to worse results when combined with the audio features. This was possibly due to the inability of the feature selection algorithm to deal with such a high dimensionality. As such, the developed bimodal classical machine learning approach includes all audio and lyrics features, except for the content-based ones.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">The best hyperparameters for the bimodal classical approach can be found in Table <a href="#S4.T6" title="TABLE VI ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span><span id="S4.T6.2.1" class="ltx_text ltx_font_bold">Bimodal Classical ML</span> Hyperparameters </figcaption>
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.3.1.1" class="ltx_tr">
<th id="S4.T6.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T6.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.1.1.1.1.1" class="ltx_p" style="width:42.7pt;">Dataset</span>
</span>
</th>
<th id="S4.T6.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Evaluation</th>
<th id="S4.T6.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Best Hyperparameters</th>
</tr>
<tr id="S4.T6.3.2.2" class="ltx_tr">
<th id="S4.T6.3.2.2.1" class="ltx_td ltx_align_top ltx_th ltx_th_row"></th>
<th id="S4.T6.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Strategy</th>
<th id="S4.T6.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Kernel</th>
<th id="S4.T6.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">C</th>
<th id="S4.T6.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Gamma</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.3.3.1" class="ltx_tr">
<th id="S4.T6.3.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_tt" rowspan="3">
<span id="S4.T6.3.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.3.1.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T6.3.3.1.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</th>
<td id="S4.T6.3.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">Cross-Val</td>
<td id="S4.T6.3.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">RBF</td>
<td id="S4.T6.3.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">49.7161</td>
<td id="S4.T6.3.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">7.0038e-4</td>
</tr>
<tr id="S4.T6.3.4.2" class="ltx_tr">
<td id="S4.T6.3.4.2.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T6.3.4.2.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T6.3.4.2.3" class="ltx_td ltx_align_center ltx_border_t">1268.6758</td>
<td id="S4.T6.3.4.2.4" class="ltx_td ltx_align_center ltx_border_t">4.3292e-4</td>
</tr>
<tr id="S4.T6.3.5.3" class="ltx_tr">
<td id="S4.T6.3.5.3.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T6.3.5.3.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T6.3.5.3.3" class="ltx_td ltx_align_center ltx_border_t">70.2010</td>
<td id="S4.T6.3.5.3.4" class="ltx_td ltx_align_center ltx_border_t">1.3115e-5</td>
</tr>
<tr id="S4.T6.3.6.4" class="ltx_tr">
<th id="S4.T6.3.6.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.T6.3.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.6.4.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T6.3.6.4.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</th>
<td id="S4.T6.3.6.4.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T6.3.6.4.3" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T6.3.6.4.4" class="ltx_td ltx_align_center ltx_border_t">3.7576</td>
<td id="S4.T6.3.6.4.5" class="ltx_td ltx_align_center ltx_border_t">4.6117e-4</td>
</tr>
<tr id="S4.T6.3.7.5" class="ltx_tr">
<td id="S4.T6.3.7.5.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T6.3.7.5.2" class="ltx_td ltx_align_center ltx_border_t">RBF</td>
<td id="S4.T6.3.7.5.3" class="ltx_td ltx_align_center ltx_border_t">402.6879</td>
<td id="S4.T6.3.7.5.4" class="ltx_td ltx_align_center ltx_border_t">3.4055e-4</td>
</tr>
<tr id="S4.T6.3.8.6" class="ltx_tr">
<td id="S4.T6.3.8.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.T6.3.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">RBF</td>
<td id="S4.T6.3.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">18.2484</td>
<td id="S4.T6.3.8.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">4.5943e-5</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS7.4.1.1" class="ltx_text">IV-G</span> </span><span id="S4.SS7.5.2" class="ltx_text ltx_font_italic">Bimodal Deep Learning</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">After reviewing the state of the art regarding bimodal approaches in Section <a href="#S2.SS4" title="II-D MER Systems ‣ II Background and Related Work ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a>, it was apparent that the best results were obtained using a mix of learned features from spectral representations of audio and lyrics embeddings. With this in mind, we propose a system similar to the late-fusion approach of Delbouys et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, with each branch corresponding to the unimodal approaches presented before.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">As shown in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the architecture comprises audio and lyrics branches receiving the song’s Mel spectrogram and word embeddings obtained with RoBERTa, respectively. Since the output of each branch is not equal, with the lyrics branch outputting a vector with more than 16000 features in total, a dense layer downsamples the lyrics output to the same size as the audio output feature vector. This ensures that both modalities contribute equally to the final classification. Next, the learned features are joined by a concatenation layer, followed by two sets of dropout and dense layers.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.06060/assets/figs/AudioLyrics.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="468" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Architecture for bimodal DL experimentation. The feature learning portions of the previous models are kept as is, with the addition of a dense layer to balance the amount of features coming from the lyrics branch. The classifier portion is adapted from Delbouys et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</figcaption>
</figure>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p">The search spaces were mostly kept, except for the batch size, whose range was modified to [16, 128]. As observed in the lyrics DL experiments, the optimal batch size is relatively small compared to audio, so it made sense to adjust the corresponding range. Table <a href="#S4.T7" title="TABLE VII ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> contains the optimal hyperparameters for this methodology.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VII: </span><span id="S4.T7.2.1" class="ltx_text ltx_font_bold">Bimodal DL</span> Methodologies Hyperparameters </figcaption>
<table id="S4.T7.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.3.1.1" class="ltx_tr">
<th id="S4.T7.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T7.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.3.1.1.1.1.1" class="ltx_p" style="width:39.8pt;">Dataset</span>
</span>
</th>
<th id="S4.T7.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Evaluation</th>
<th id="S4.T7.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Best Hyperparameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.3.2.1" class="ltx_tr">
<td id="S4.T7.3.2.1.1" class="ltx_td ltx_align_top"></td>
<th id="S4.T7.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Strategy</th>
<th id="S4.T7.3.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Batch Size</th>
<th id="S4.T7.3.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Optimizer</th>
<th id="S4.T7.3.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Learning Rate</th>
</tr>
<tr id="S4.T7.3.3.2" class="ltx_tr">
<td id="S4.T7.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S4.T7.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.3.3.2.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.T7.3.3.2.1.1.1.1" class="ltx_text">MERGE Bimodal Complete</span></span>
</span>
</td>
<td id="S4.T7.3.3.2.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T7.3.3.2.3" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T7.3.3.2.4" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.T7.3.3.2.5" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.T7.3.4.3" class="ltx_tr">
<td id="S4.T7.3.4.3.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T7.3.4.3.2" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T7.3.4.3.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.T7.3.4.3.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.T7.3.5.4" class="ltx_tr">
<td id="S4.T7.3.5.4.1" class="ltx_td ltx_align_center ltx_border_t">40-30-30</td>
<td id="S4.T7.3.5.4.2" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T7.3.5.4.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.T7.3.5.4.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.T7.3.6.5" class="ltx_tr">
<td id="S4.T7.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" rowspan="3">
<span id="S4.T7.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.3.6.5.1.1.1" class="ltx_p" style="width:39.8pt;"><span id="S4.T7.3.6.5.1.1.1.1" class="ltx_text">MERGE Bimodal Balanced</span></span>
</span>
</td>
<td id="S4.T7.3.6.5.2" class="ltx_td ltx_align_center ltx_border_t">Cross-Val</td>
<td id="S4.T7.3.6.5.3" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T7.3.6.5.4" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.T7.3.6.5.5" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.T7.3.7.6" class="ltx_tr">
<td id="S4.T7.3.7.6.1" class="ltx_td ltx_align_center ltx_border_t">75-15-15</td>
<td id="S4.T7.3.7.6.2" class="ltx_td ltx_align_center ltx_border_t">64</td>
<td id="S4.T7.3.7.6.3" class="ltx_td ltx_align_center ltx_border_t">SGD</td>
<td id="S4.T7.3.7.6.4" class="ltx_td ltx_align_center ltx_border_t">1e-2</td>
</tr>
<tr id="S4.T7.3.8.7" class="ltx_tr">
<td id="S4.T7.3.8.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">40-30-30</td>
<td id="S4.T7.3.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">16</td>
<td id="S4.T7.3.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">SGD</td>
<td id="S4.T7.3.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1e-2</td>
</tr>
</tbody>
</table>
</figure>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section presents and discusses the results obtained for the audio, lyrics, and bimodal datasets after training models using the described baseline methodologies and evaluation strategies. As mentioned, we performed statistical significance tests in all comparisons, with a threshold set to p <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><lt id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">&lt;</annotation></semantics></math> 0.05. In addition, we employ 4QAED and LED as baseline datasets for comparison purposes with the new datasets.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">MERGE Audio</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T8" title="TABLE VIII ‣ V-A MERGE Audio ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a> shows the overall results for the audio modality. There (and in the following tables), CV stands for 10x10-fold Cross Validation, TVT for Train-Validate-Test (using 70-15-15 and 40-30-30 splits), CML for Classical ML, HF for Handcrafted Features, MS for Mel Spectrogram, and WE for Word Embeddings, respectively. Regarding TVT, we only present F1-scores for compactness since we obtained similar recall and precision metrics values.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Starting with results for 10x10-fold CV, it is clear that classical ML approaches, relying on handcrafted audio features, significantly outperform the CNN-based methodologies (a maximum F1-score of 74.14% in the former against 63.63% in the latter). Despite the increase in the dataset size, its dimension still seems insufficient to fully exploit the feature learning capabilities of CNNs, which demand significant amounts of data.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Comparing baseline and new datasets, for the classical approach, the results in the new outperform the baseline ones (from 71.71% in 4QAED<span id="footnote25" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span>Even though our implementation is the same as the original, we could not attain the original 76.4% score. This is a consequence of updates to the underlying feature extraction frameworks, leading to different values for some extracted features. For the sake of fair comparison between 4QAED and the novel datasets, it is essential to report the results obtained under the same conditions. We will address this issue in future work.</span></span></span> to a maximum of 74.14% in the bimodal complete dataset). This suggests that the proposed classical approach takes advantage of the increased dataset size. Another possibility is that the novel audio datasets are less complex than 4QAED (maybe due to reduced ambiguity). As for DL methodologies, the performance increase on the new datasets is significant compared to the baseline dataset (from 60.62% in 4QAED to a maximum of 63.63% in the bimodal complete dataset). As mentioned, we may attribute this to the increased dataset size and possible reduced ambiguity.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Regarding the influence of the size and imbalance of the new datasets, these factors showed little impact since the results attained for the four datasets (audio complete and balanced, bimodal complete and balanced) are similar.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">As for the standard deviation of the F1-scores for 10x10-fold CV, we can observe that they are low (from 1.97% to 5.07%), which denotes low sensitivity to the defined folds.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">When compared with CV, TVT attains, in general, slightly lower but comparable results (for example, a top result of 74.14% in CV against 71.79% in TVT 70-15-15, in the classical approach). This indicates the robustness of the proposed TVT splits and their feasibility for benchmarking, leading to more straightforward and faster model training compared to CV. Comparing the two proposed splits, 70-15-15 outperforms 40-30-30 (a top F1-score of 71.79% in the former against 69.63% in the latter). This might result from the more extensive training set in the 70-15-15 split.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">Finally, the confusion matrix for the best-performing audio model with CV (HF + SVM, on the bimodal complete dataset, with an F1-score of 74.14%) is presented in Table <a href="#S5.T9" title="TABLE IX ‣ V-A MERGE Audio ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>. As can be observed, the model can accurately predict Q2, followed by Q1. However, despite our efforts to foster decreased ambiguity in the datasets, there is some confusion between Q3 and Q4, which leads to a lower score in these quadrants. This aligns with other studies in the literature that show the difficulty of distinguishing valence in low-arousal quadrants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span><span id="S5.T8.32.1" class="ltx_text ltx_font_bold">Audio</span> Best Results </figcaption>
<table id="S5.T8.30" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.30.31.1" class="ltx_tr">
<th id="S5.T8.30.31.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T8.30.31.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S5.T8.30.31.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.31.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.31.1.2.1.1" class="ltx_p">Methodology</span>
</span>
</th>
<th id="S5.T8.30.31.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T8.30.31.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.31.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.31.1.4.1.1" class="ltx_p">Cross Val</span>
</span>
</th>
<th id="S5.T8.30.31.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T8.30.31.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.31.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.31.1.6.1.1" class="ltx_p">TVT 75-15-15</span>
</span>
</th>
<th id="S5.T8.30.31.1.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.31.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.31.1.7.1.1" class="ltx_p">TVT 40-30-30</span>
</span>
</th>
</tr>
<tr id="S5.T8.30.32.2" class="ltx_tr">
<th id="S5.T8.30.32.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column"></th>
<th id="S5.T8.30.32.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.32.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.32.2.2.1.1" class="ltx_p">Precision</span>
</span>
</th>
<th id="S5.T8.30.32.2.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.32.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.32.2.3.1.1" class="ltx_p">Recall</span>
</span>
</th>
<th id="S5.T8.30.32.2.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.32.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.32.2.4.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T8.30.32.2.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.32.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.32.2.5.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T8.30.32.2.6" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T8.30.32.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.32.2.6.1.1" class="ltx_p">F1-score</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.3.3" class="ltx_tr">
<th id="S5.T8.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S5.T8.3.3.4.1" class="ltx_text">4QAED</span></th>
<td id="S5.T8.3.3.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.3.3.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T8.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.1.1.1.1.1" class="ltx_p">72.39% <math id="S5.T8.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.1.1.1.1.1.m1.1a"><mo id="S5.T8.1.1.1.1.1.m1.1.1" xref="S5.T8.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.1.1.1.1.1.m1.1.1.cmml" xref="S5.T8.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 4.64</span>
</span>
</td>
<td id="S5.T8.2.2.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.2.2.2.1.1" class="ltx_p">71.91% <math id="S5.T8.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.2.2.2.1.1.m1.1a"><mo id="S5.T8.2.2.2.1.1.m1.1.1" xref="S5.T8.2.2.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.2.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.2.2.2.1.1.m1.1.1.cmml" xref="S5.T8.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.2.2.1.1.m1.1c">\pm</annotation></semantics></math> 4.43</span>
</span>
</td>
<td id="S5.T8.3.3.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.3.3.3.1.1" class="ltx_p"><span id="S5.T8.3.3.3.1.1.1" class="ltx_text ltx_font_bold">71.71%</span> <math id="S5.T8.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.3.3.3.1.1.m1.1a"><mo id="S5.T8.3.3.3.1.1.m1.1.1" xref="S5.T8.3.3.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.3.3.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.3.3.3.1.1.m1.1.1.cmml" xref="S5.T8.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.3.3.3.1.1.m1.1c">\pm</annotation></semantics></math> 4.50</span>
</span>
</td>
<td id="S5.T8.3.3.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.3.3.6.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T8.3.3.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T8.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.3.3.7.1.1" class="ltx_p">-</span>
</span>
</td>
</tr>
<tr id="S5.T8.6.6" class="ltx_tr">
<td id="S5.T8.6.6.4" class="ltx_td ltx_align_justify">
<span id="S5.T8.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.6.6.4.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T8.4.4.1" class="ltx_td ltx_align_justify">
<span id="S5.T8.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.4.4.1.1.1" class="ltx_p">62.76% <math id="S5.T8.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.4.4.1.1.1.m1.1a"><mo id="S5.T8.4.4.1.1.1.m1.1.1" xref="S5.T8.4.4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.4.4.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.4.4.1.1.1.m1.1.1.cmml" xref="S5.T8.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.4.4.1.1.1.m1.1c">\pm</annotation></semantics></math> 5.39</span>
</span>
</td>
<td id="S5.T8.5.5.2" class="ltx_td ltx_align_justify">
<span id="S5.T8.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.5.5.2.1.1" class="ltx_p">61.69% <math id="S5.T8.5.5.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.5.5.2.1.1.m1.1a"><mo id="S5.T8.5.5.2.1.1.m1.1.1" xref="S5.T8.5.5.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.5.5.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.5.5.2.1.1.m1.1.1.cmml" xref="S5.T8.5.5.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.5.5.2.1.1.m1.1c">\pm</annotation></semantics></math> 4.74</span>
</span>
</td>
<td id="S5.T8.6.6.3" class="ltx_td ltx_align_justify">
<span id="S5.T8.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.6.6.3.1.1" class="ltx_p"><span id="S5.T8.6.6.3.1.1.1" class="ltx_text ltx_font_bold">60.62%</span> <math id="S5.T8.6.6.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.6.6.3.1.1.m1.1a"><mo id="S5.T8.6.6.3.1.1.m1.1.1" xref="S5.T8.6.6.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.6.6.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.6.6.3.1.1.m1.1.1.cmml" xref="S5.T8.6.6.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.6.6.3.1.1.m1.1c">\pm</annotation></semantics></math> 5.07</span>
</span>
</td>
<td id="S5.T8.6.6.5" class="ltx_td ltx_align_justify">
<span id="S5.T8.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.6.6.5.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T8.6.6.6" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T8.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.6.6.6.1.1" class="ltx_p">-</span>
</span>
</td>
</tr>
<tr id="S5.T8.9.9" class="ltx_tr">
<th id="S5.T8.9.9.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">MERGE</th>
<td id="S5.T8.9.9.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.9.9.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T8.7.7.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.7.7.1.1.1" class="ltx_p">73.74% <math id="S5.T8.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.7.7.1.1.1.m1.1a"><mo id="S5.T8.7.7.1.1.1.m1.1.1" xref="S5.T8.7.7.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.7.7.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.7.7.1.1.1.m1.1.1.cmml" xref="S5.T8.7.7.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.7.7.1.1.1.m1.1c">\pm</annotation></semantics></math> 1.99</span>
</span>
</td>
<td id="S5.T8.8.8.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.8.8.2.1.1" class="ltx_p">72.72% <math id="S5.T8.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.8.8.2.1.1.m1.1a"><mo id="S5.T8.8.8.2.1.1.m1.1.1" xref="S5.T8.8.8.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.8.8.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.8.8.2.1.1.m1.1.1.cmml" xref="S5.T8.8.8.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.8.8.2.1.1.m1.1c">\pm</annotation></semantics></math> 1.94</span>
</span>
</td>
<td id="S5.T8.9.9.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.9.9.3.1.1" class="ltx_p">73.60% <math id="S5.T8.9.9.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.9.9.3.1.1.m1.1a"><mo id="S5.T8.9.9.3.1.1.m1.1.1" xref="S5.T8.9.9.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.9.9.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.9.9.3.1.1.m1.1.1.cmml" xref="S5.T8.9.9.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.9.9.3.1.1.m1.1c">\pm</annotation></semantics></math> 1.97</span>
</span>
</td>
<td id="S5.T8.9.9.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.9.9.6.1.1" class="ltx_p"><span id="S5.T8.9.9.6.1.1.1" class="ltx_text ltx_font_bold">71.79%</span></span>
</span>
</td>
<td id="S5.T8.9.9.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T8.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.9.9.7.1.1" class="ltx_p">66.38%</span>
</span>
</td>
</tr>
<tr id="S5.T8.12.12" class="ltx_tr">
<th id="S5.T8.12.12.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Audio Complete</th>
<td id="S5.T8.12.12.5" class="ltx_td ltx_align_justify">
<span id="S5.T8.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.12.12.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T8.10.10.1" class="ltx_td ltx_align_justify">
<span id="S5.T8.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.10.10.1.1.1" class="ltx_p">65.35% <math id="S5.T8.10.10.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.10.10.1.1.1.m1.1a"><mo id="S5.T8.10.10.1.1.1.m1.1.1" xref="S5.T8.10.10.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.10.10.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.10.10.1.1.1.m1.1.1.cmml" xref="S5.T8.10.10.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.10.10.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.55</span>
</span>
</td>
<td id="S5.T8.11.11.2" class="ltx_td ltx_align_justify">
<span id="S5.T8.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.11.11.2.1.1" class="ltx_p">65.02% <math id="S5.T8.11.11.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.11.11.2.1.1.m1.1a"><mo id="S5.T8.11.11.2.1.1.m1.1.1" xref="S5.T8.11.11.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.11.11.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.11.11.2.1.1.m1.1.1.cmml" xref="S5.T8.11.11.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.11.11.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.68</span>
</span>
</td>
<td id="S5.T8.12.12.3" class="ltx_td ltx_align_justify">
<span id="S5.T8.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.12.12.3.1.1" class="ltx_p">63.53% <math id="S5.T8.12.12.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.12.12.3.1.1.m1.1a"><mo id="S5.T8.12.12.3.1.1.m1.1.1" xref="S5.T8.12.12.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.12.12.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.12.12.3.1.1.m1.1.1.cmml" xref="S5.T8.12.12.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.12.12.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.34</span>
</span>
</td>
<td id="S5.T8.12.12.6" class="ltx_td ltx_align_justify">
<span id="S5.T8.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.12.12.6.1.1" class="ltx_p">59.93%</span>
</span>
</td>
<td id="S5.T8.12.12.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T8.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.12.12.7.1.1" class="ltx_p">57.50%</span>
</span>
</td>
</tr>
<tr id="S5.T8.15.15" class="ltx_tr">
<th id="S5.T8.15.15.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">MERGE</th>
<td id="S5.T8.15.15.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.15.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.15.15.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T8.13.13.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.13.13.1.1.1" class="ltx_p">72.78% <math id="S5.T8.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.13.13.1.1.1.m1.1a"><mo id="S5.T8.13.13.1.1.1.m1.1.1" xref="S5.T8.13.13.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.13.13.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.13.13.1.1.1.m1.1.1.cmml" xref="S5.T8.13.13.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.13.13.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.32</span>
</span>
</td>
<td id="S5.T8.14.14.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.14.14.2.1.1" class="ltx_p">72.87% <math id="S5.T8.14.14.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.14.14.2.1.1.m1.1a"><mo id="S5.T8.14.14.2.1.1.m1.1.1" xref="S5.T8.14.14.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.14.14.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.14.14.2.1.1.m1.1.1.cmml" xref="S5.T8.14.14.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.14.14.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.29</span>
</span>
</td>
<td id="S5.T8.15.15.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.15.15.3.1.1" class="ltx_p">72.69% <math id="S5.T8.15.15.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.15.15.3.1.1.m1.1a"><mo id="S5.T8.15.15.3.1.1.m1.1.1" xref="S5.T8.15.15.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.15.15.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.15.15.3.1.1.m1.1.1.cmml" xref="S5.T8.15.15.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.15.15.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.31</span>
</span>
</td>
<td id="S5.T8.15.15.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.15.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.15.15.6.1.1" class="ltx_p">70.40%</span>
</span>
</td>
<td id="S5.T8.15.15.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S5.T8.15.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.15.15.7.1.1" class="ltx_p">69.58%</span>
</span>
</td>
</tr>
<tr id="S5.T8.18.18" class="ltx_tr">
<th id="S5.T8.18.18.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Audio Balanced</th>
<td id="S5.T8.18.18.5" class="ltx_td ltx_align_justify">
<span id="S5.T8.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.18.18.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T8.16.16.1" class="ltx_td ltx_align_justify">
<span id="S5.T8.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.16.16.1.1.1" class="ltx_p">64.97% <math id="S5.T8.16.16.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.16.16.1.1.1.m1.1a"><mo id="S5.T8.16.16.1.1.1.m1.1.1" xref="S5.T8.16.16.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.16.16.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.16.16.1.1.1.m1.1.1.cmml" xref="S5.T8.16.16.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.16.16.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.93</span>
</span>
</td>
<td id="S5.T8.17.17.2" class="ltx_td ltx_align_justify">
<span id="S5.T8.17.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.17.17.2.1.1" class="ltx_p">64.62% <math id="S5.T8.17.17.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.17.17.2.1.1.m1.1a"><mo id="S5.T8.17.17.2.1.1.m1.1.1" xref="S5.T8.17.17.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.17.17.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.17.17.2.1.1.m1.1.1.cmml" xref="S5.T8.17.17.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.17.17.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.85</span>
</span>
</td>
<td id="S5.T8.18.18.3" class="ltx_td ltx_align_justify">
<span id="S5.T8.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.18.18.3.1.1" class="ltx_p">63.37% <math id="S5.T8.18.18.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.18.18.3.1.1.m1.1a"><mo id="S5.T8.18.18.3.1.1.m1.1.1" xref="S5.T8.18.18.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.18.18.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.18.18.3.1.1.m1.1.1.cmml" xref="S5.T8.18.18.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.18.18.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.28</span>
</span>
</td>
<td id="S5.T8.18.18.6" class="ltx_td ltx_align_justify">
<span id="S5.T8.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.18.18.6.1.1" class="ltx_p"><span id="S5.T8.18.18.6.1.1.1" class="ltx_text ltx_font_bold">66.38%</span></span>
</span>
</td>
<td id="S5.T8.18.18.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T8.18.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.18.18.7.1.1" class="ltx_p">60.43%</span>
</span>
</td>
</tr>
<tr id="S5.T8.21.21" class="ltx_tr">
<th id="S5.T8.21.21.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">MERGE</th>
<td id="S5.T8.21.21.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.21.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.21.21.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T8.19.19.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.19.19.1.1.1" class="ltx_p">74.28% <math id="S5.T8.19.19.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.19.19.1.1.1.m1.1a"><mo id="S5.T8.19.19.1.1.1.m1.1.1" xref="S5.T8.19.19.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.19.19.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.19.19.1.1.1.m1.1.1.cmml" xref="S5.T8.19.19.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.19.19.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.93</span>
</span>
</td>
<td id="S5.T8.20.20.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.20.20.2.1.1" class="ltx_p">72.28% <math id="S5.T8.20.20.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.20.20.2.1.1.m1.1a"><mo id="S5.T8.20.20.2.1.1.m1.1.1" xref="S5.T8.20.20.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.20.20.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.20.20.2.1.1.m1.1.1.cmml" xref="S5.T8.20.20.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.20.20.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.90</span>
</span>
</td>
<td id="S5.T8.21.21.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.21.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.21.21.3.1.1" class="ltx_p"><span id="S5.T8.21.21.3.1.1.1" class="ltx_text ltx_font_bold">74.14%</span> <math id="S5.T8.21.21.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.21.21.3.1.1.m1.1a"><mo id="S5.T8.21.21.3.1.1.m1.1.1" xref="S5.T8.21.21.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.21.21.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.21.21.3.1.1.m1.1.1.cmml" xref="S5.T8.21.21.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.21.21.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.92</span>
</span>
</td>
<td id="S5.T8.21.21.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T8.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.21.21.6.1.1" class="ltx_p">71.43%</span>
</span>
</td>
<td id="S5.T8.21.21.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T8.21.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.21.21.7.1.1" class="ltx_p"><span id="S5.T8.21.21.7.1.1.1" class="ltx_text ltx_font_bold">69.63%</span></span>
</span>
</td>
</tr>
<tr id="S5.T8.24.24" class="ltx_tr">
<th id="S5.T8.24.24.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Bimodal Complete</th>
<td id="S5.T8.24.24.5" class="ltx_td ltx_align_justify">
<span id="S5.T8.24.24.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.24.24.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T8.22.22.1" class="ltx_td ltx_align_justify">
<span id="S5.T8.22.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.22.22.1.1.1" class="ltx_p">64.99% <math id="S5.T8.22.22.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.22.22.1.1.1.m1.1a"><mo id="S5.T8.22.22.1.1.1.m1.1.1" xref="S5.T8.22.22.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.22.22.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.22.22.1.1.1.m1.1.1.cmml" xref="S5.T8.22.22.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.22.22.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.45</span>
</span>
</td>
<td id="S5.T8.23.23.2" class="ltx_td ltx_align_justify">
<span id="S5.T8.23.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.23.23.2.1.1" class="ltx_p">65.03% <math id="S5.T8.23.23.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.23.23.2.1.1.m1.1a"><mo id="S5.T8.23.23.2.1.1.m1.1.1" xref="S5.T8.23.23.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.23.23.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.23.23.2.1.1.m1.1.1.cmml" xref="S5.T8.23.23.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.23.23.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.10</span>
</span>
</td>
<td id="S5.T8.24.24.3" class="ltx_td ltx_align_justify">
<span id="S5.T8.24.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.24.24.3.1.1" class="ltx_p"><span id="S5.T8.24.24.3.1.1.1" class="ltx_text ltx_font_bold">63.63%</span> <math id="S5.T8.24.24.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.24.24.3.1.1.m1.1a"><mo id="S5.T8.24.24.3.1.1.m1.1.1" xref="S5.T8.24.24.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.24.24.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.24.24.3.1.1.m1.1.1.cmml" xref="S5.T8.24.24.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.24.24.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.63</span>
</span>
</td>
<td id="S5.T8.24.24.6" class="ltx_td ltx_align_justify">
<span id="S5.T8.24.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.24.24.6.1.1" class="ltx_p">62.10%</span>
</span>
</td>
<td id="S5.T8.24.24.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T8.24.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.24.24.7.1.1" class="ltx_p"><span id="S5.T8.24.24.7.1.1.1" class="ltx_text ltx_font_bold">61.67%</span></span>
</span>
</td>
</tr>
<tr id="S5.T8.27.27" class="ltx_tr">
<th id="S5.T8.27.27.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">MERGE</th>
<td id="S5.T8.27.27.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.27.27.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.27.27.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T8.25.25.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.25.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.25.25.1.1.1" class="ltx_p">72.29% <math id="S5.T8.25.25.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.25.25.1.1.1.m1.1a"><mo id="S5.T8.25.25.1.1.1.m1.1.1" xref="S5.T8.25.25.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.25.25.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.25.25.1.1.1.m1.1.1.cmml" xref="S5.T8.25.25.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.25.25.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</span>
</span>
</td>
<td id="S5.T8.26.26.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.26.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.26.26.2.1.1" class="ltx_p">72.25% <math id="S5.T8.26.26.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.26.26.2.1.1.m1.1a"><mo id="S5.T8.26.26.2.1.1.m1.1.1" xref="S5.T8.26.26.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.26.26.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.26.26.2.1.1.m1.1.1.cmml" xref="S5.T8.26.26.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.26.26.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</span>
</span>
</td>
<td id="S5.T8.27.27.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.27.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.27.27.3.1.1" class="ltx_p">72.11% <math id="S5.T8.27.27.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.27.27.3.1.1.m1.1a"><mo id="S5.T8.27.27.3.1.1.m1.1.1" xref="S5.T8.27.27.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.27.27.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.27.27.3.1.1.m1.1.1.cmml" xref="S5.T8.27.27.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.27.27.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</span>
</span>
</td>
<td id="S5.T8.27.27.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T8.27.27.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.27.27.6.1.1" class="ltx_p">69.39%</span>
</span>
</td>
<td id="S5.T8.27.27.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S5.T8.27.27.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.27.27.7.1.1" class="ltx_p">67.47%</span>
</span>
</td>
</tr>
<tr id="S5.T8.30.30" class="ltx_tr">
<th id="S5.T8.30.30.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">Bimodal Balanced</th>
<td id="S5.T8.30.30.5" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T8.30.30.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.30.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T8.28.28.1" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T8.28.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.28.28.1.1.1" class="ltx_p">63.48% <math id="S5.T8.28.28.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.28.28.1.1.1.m1.1a"><mo id="S5.T8.28.28.1.1.1.m1.1.1" xref="S5.T8.28.28.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.28.28.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.28.28.1.1.1.m1.1.1.cmml" xref="S5.T8.28.28.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.28.28.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.71</span>
</span>
</td>
<td id="S5.T8.29.29.2" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T8.29.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.29.29.2.1.1" class="ltx_p">63.58% <math id="S5.T8.29.29.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.29.29.2.1.1.m1.1a"><mo id="S5.T8.29.29.2.1.1.m1.1.1" xref="S5.T8.29.29.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.29.29.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.29.29.2.1.1.m1.1.1.cmml" xref="S5.T8.29.29.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.29.29.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.46</span>
</span>
</td>
<td id="S5.T8.30.30.3" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T8.30.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.30.3.1.1" class="ltx_p">62.59% <math id="S5.T8.30.30.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T8.30.30.3.1.1.m1.1a"><mo id="S5.T8.30.30.3.1.1.m1.1.1" xref="S5.T8.30.30.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T8.30.30.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T8.30.30.3.1.1.m1.1.1.cmml" xref="S5.T8.30.30.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.30.30.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.78</span>
</span>
</td>
<td id="S5.T8.30.30.6" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T8.30.30.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.30.6.1.1" class="ltx_p">63.95%</span>
</span>
</td>
<td id="S5.T8.30.30.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_b">
<span id="S5.T8.30.30.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T8.30.30.7.1.1" class="ltx_p">60.86%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IX: </span><span id="S5.T9.18.1" class="ltx_text ltx_font_bold">Audio</span> Confusion Matrix Best Results (CV) </figcaption>
<table id="S5.T9.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T9.16.17.1" class="ltx_tr">
<th id="S5.T9.16.17.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T9.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Predicted</th>
</tr>
<tr id="S5.T9.16.18.2" class="ltx_tr">
<th id="S5.T9.16.18.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T9.16.18.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q1</th>
<th id="S5.T9.16.18.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q2</th>
<th id="S5.T9.16.18.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q3</th>
<th id="S5.T9.16.18.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T9.16.19.1" class="ltx_tr">
<th id="S5.T9.16.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="8">
<span id="S5.T9.16.19.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S5.T9.16.19.1.1.1.1" class="ltx_p">
<span id="S5.T9.16.19.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:29.2pt;vertical-align:-11.1pt;"><span class="ltx_transformed_inner" style="width:29.2pt;transform:translate(-11.11pt,0pt) rotate(-90deg) ;">
<span id="S5.T9.16.19.1.1.1.1.1.1" class="ltx_p">Actual</span>
</span></span></span>
</span>
</th>
<th id="S5.T9.16.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T9.16.19.1.2.1" class="ltx_text">Q1</span></th>
<td id="S5.T9.16.19.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.16.19.1.3.1" class="ltx_text ltx_font_bold">77.9%</span></td>
<td id="S5.T9.16.19.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.9%</td>
<td id="S5.T9.16.19.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.5%</td>
<td id="S5.T9.16.19.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.7%</td>
</tr>
<tr id="S5.T9.4.4" class="ltx_tr">
<td id="S5.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.1.1.1.m1.1a"><mo id="S5.T9.1.1.1.m1.1.1" xref="S5.T9.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T9.1.1.1.m1.1.1.cmml" xref="S5.T9.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.1.1.1.m1.1c">\pm</annotation></semantics></math>5.4</td>
<td id="S5.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.2.2.2.m1.1a"><mo id="S5.T9.2.2.2.m1.1.1" xref="S5.T9.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T9.2.2.2.m1.1.1.cmml" xref="S5.T9.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.2.2.2.m1.1c">\pm</annotation></semantics></math>2.6</td>
<td id="S5.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.3.3.3.m1.1a"><mo id="S5.T9.3.3.3.m1.1.1" xref="S5.T9.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T9.3.3.3.m1.1.1.cmml" xref="S5.T9.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.3.3.3.m1.1c">\pm</annotation></semantics></math>3.7</td>
<td id="S5.T9.4.4.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.4.4.4.m1.1a"><mo id="S5.T9.4.4.4.m1.1.1" xref="S5.T9.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.4.4.4.m1.1b"><csymbol cd="latexml" id="S5.T9.4.4.4.m1.1.1.cmml" xref="S5.T9.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.4.4.4.m1.1c">\pm</annotation></semantics></math>4.8</td>
</tr>
<tr id="S5.T9.16.20.2" class="ltx_tr">
<th id="S5.T9.16.20.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T9.16.20.2.1.1" class="ltx_text">Q2</span></th>
<td id="S5.T9.16.20.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.1%</td>
<td id="S5.T9.16.20.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.16.20.2.3.1" class="ltx_text ltx_font_bold">91.6%</span></td>
<td id="S5.T9.16.20.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.6%</td>
<td id="S5.T9.16.20.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6%</td>
</tr>
<tr id="S5.T9.8.8" class="ltx_tr">
<td id="S5.T9.5.5.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.5.5.1.m1.1a"><mo id="S5.T9.5.5.1.m1.1.1" xref="S5.T9.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T9.5.5.1.m1.1.1.cmml" xref="S5.T9.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.5.5.1.m1.1c">\pm</annotation></semantics></math>3.7</td>
<td id="S5.T9.6.6.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.6.6.2.m1.1a"><mo id="S5.T9.6.6.2.m1.1.1" xref="S5.T9.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T9.6.6.2.m1.1.1.cmml" xref="S5.T9.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.6.6.2.m1.1c">\pm</annotation></semantics></math>3.1</td>
<td id="S5.T9.7.7.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.7.7.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.7.7.3.m1.1a"><mo id="S5.T9.7.7.3.m1.1.1" xref="S5.T9.7.7.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.7.7.3.m1.1b"><csymbol cd="latexml" id="S5.T9.7.7.3.m1.1.1.cmml" xref="S5.T9.7.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.7.7.3.m1.1c">\pm</annotation></semantics></math>2.1</td>
<td id="S5.T9.8.8.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.8.8.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.8.8.4.m1.1a"><mo id="S5.T9.8.8.4.m1.1.1" xref="S5.T9.8.8.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.8.8.4.m1.1b"><csymbol cd="latexml" id="S5.T9.8.8.4.m1.1.1.cmml" xref="S5.T9.8.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.8.8.4.m1.1c">\pm</annotation></semantics></math>1.0</td>
</tr>
<tr id="S5.T9.16.21.3" class="ltx_tr">
<th id="S5.T9.16.21.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T9.16.21.3.1.1" class="ltx_text">Q3</span></th>
<td id="S5.T9.16.21.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.0%</td>
<td id="S5.T9.16.21.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.4%</td>
<td id="S5.T9.16.21.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.16.21.3.4.1" class="ltx_text ltx_font_bold">60.9%</span></td>
<td id="S5.T9.16.21.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.6%</td>
</tr>
<tr id="S5.T9.12.12" class="ltx_tr">
<td id="S5.T9.9.9.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.9.9.1.m1.1a"><mo id="S5.T9.9.9.1.m1.1.1" xref="S5.T9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.9.9.1.m1.1b"><csymbol cd="latexml" id="S5.T9.9.9.1.m1.1.1.cmml" xref="S5.T9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.9.9.1.m1.1c">\pm</annotation></semantics></math>3.0</td>
<td id="S5.T9.10.10.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.10.10.2.m1.1a"><mo id="S5.T9.10.10.2.m1.1.1" xref="S5.T9.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.10.10.2.m1.1b"><csymbol cd="latexml" id="S5.T9.10.10.2.m1.1.1.cmml" xref="S5.T9.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.10.10.2.m1.1c">\pm</annotation></semantics></math>1.3</td>
<td id="S5.T9.11.11.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.11.11.3.m1.1a"><mo id="S5.T9.11.11.3.m1.1.1" xref="S5.T9.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.11.11.3.m1.1b"><csymbol cd="latexml" id="S5.T9.11.11.3.m1.1.1.cmml" xref="S5.T9.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.11.11.3.m1.1c">\pm</annotation></semantics></math>7.5</td>
<td id="S5.T9.12.12.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T9.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.12.12.4.m1.1a"><mo id="S5.T9.12.12.4.m1.1.1" xref="S5.T9.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.12.12.4.m1.1b"><csymbol cd="latexml" id="S5.T9.12.12.4.m1.1.1.cmml" xref="S5.T9.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.12.12.4.m1.1c">\pm</annotation></semantics></math>6.3</td>
</tr>
<tr id="S5.T9.16.22.4" class="ltx_tr">
<th id="S5.T9.16.22.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T9.16.22.4.1.1" class="ltx_text">Q4</span></th>
<td id="S5.T9.16.22.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.0%</td>
<td id="S5.T9.16.22.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.2%</td>
<td id="S5.T9.16.22.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.0%</td>
<td id="S5.T9.16.22.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T9.16.22.4.5.1" class="ltx_text ltx_font_bold">61.1%</span></td>
</tr>
<tr id="S5.T9.16.16" class="ltx_tr">
<td id="S5.T9.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T9.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.13.13.1.m1.1a"><mo id="S5.T9.13.13.1.m1.1.1" xref="S5.T9.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T9.13.13.1.m1.1.1.cmml" xref="S5.T9.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.13.13.1.m1.1c">\pm</annotation></semantics></math>3.7</td>
<td id="S5.T9.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T9.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.14.14.2.m1.1a"><mo id="S5.T9.14.14.2.m1.1.1" xref="S5.T9.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T9.14.14.2.m1.1.1.cmml" xref="S5.T9.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.14.14.2.m1.1c">\pm</annotation></semantics></math>0.5</td>
<td id="S5.T9.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T9.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.15.15.3.m1.1a"><mo id="S5.T9.15.15.3.m1.1.1" xref="S5.T9.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T9.15.15.3.m1.1.1.cmml" xref="S5.T9.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.15.15.3.m1.1c">\pm</annotation></semantics></math>5.9</td>
<td id="S5.T9.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T9.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T9.16.16.4.m1.1a"><mo id="S5.T9.16.16.4.m1.1.1" xref="S5.T9.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T9.16.16.4.m1.1b"><csymbol cd="latexml" id="S5.T9.16.16.4.m1.1.1.cmml" xref="S5.T9.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.16.16.4.m1.1c">\pm</annotation></semantics></math>6.3</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">MERGE Lyrics</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Regarding the experiments using only lyrics, we observe the opposite of audio regarding classical versus DL approaches. Here, most of the experiments using word embeddings as input outperformed the ones employing handcrafted features, as illustrated in Table <a href="#S5.T10" title="TABLE X ‣ V-B MERGE Lyrics ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a>. When using the new datasets, the classical approach topped at 69.31%, while the DL method attained a maximum F1-score of 74.16% (both in the bimodal complete dataset). The same trend occurs in the baseline LED dataset, where the classical and DL approaches reached a maximum of 72.94%<span id="footnote26" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span>It is worth noting that the 73.6% F1-score reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> was obtained on the 771-lyrics subset; here, we performed 10x10-fold CV on the entire 942-lyrics set, hence, the slight differences.</span></span></span> and 76.91%, respectively. This suggests that the employed word embeddings can capture the emotional content of the lyrics more accurately than the handcrafted features. This is unsurprising since these embeddings were trained with large amounts of text data.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Nevertheless, word embeddings used as inputs to an SVM outperformed the CNN model trained with the same word embeddings. As in the audio counterpart, despite the richness of the employed input word embeddings, the CNN-based methodology does not reach its full capabilities with the current dataset sizes.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">When comparing baseline and new datasets, contrary to audio, the results in the new datasets using the classical approach underperform the ones attained in the baseline dataset (from 72.94% in LED to a maximum of 69.31% in the bimodal complete dataset). Despite the increased size, this suggests that the complexity of the novel lyrics datasets increased compared to LED. We can make the same observation regarding the DL methodologies (76.91% in LED against 74.16% in the bimodal complete dataset), further reinforcing the previous argument.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">As for audio versus lyrics, results in the novel datasets for the classical approach show that the best audio method significantly outperforms the best lyrics model (74.14% against 69.31%, respectively, both in the bimodal complete dataset). However, comparing the results attained with baseline datasets (4QAED and LED), they are similar (71.71% for audio and 72.94% for lyrics). Once again, this suggests an increased complexity in the novel lyrics datasets. As for DL approaches, the reverse happens: the best lyrics methodology significantly outperforms the best audio model (74.16% against 63.63%, respectively, once again with the bimodal complete dataset). This occurs for both the new and baseline datasets, confirming the impact of employing word embeddings trained in large text corpora against learning audio features from a (still) small dataset.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">As before, the new datasets’ size and imbalance had little impact. Once again, the results attained for the four datasets (lyrics complete and balanced, bimodal complete and balanced) are similar.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">Regarding the standard deviation of the F1-scores for 10x10-fold CV, we can again observe a reasonably low sensitivity to the data folds (from 2.49% to 4.65%).</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p">When compared with CV, TVT attains again slightly lower but comparable results (for example, a top result of 74.16% in CV against 71.55% in TVT 70-15-15, using word embeddings and SVMs, in the bimodal complete dataset), indicating its robustness. Once again, the 70-15-15 split outperforms the 40-30-30 split, although in a less notorious way (a top F1-score of 73.81% in the former against 73.7% in the latter).</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.1" class="ltx_p">Finally, the confusion matrix for the best-performing lyrics model with CV (WE + SVM, on the bimodal complete dataset, with an F1-score of 74.16%) is presented in Table <a href="#S5.T11" title="TABLE XI ‣ V-B MERGE Lyrics ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XI</span></a>. As in the audio counterpart, the model can accurately predict Q2, followed by Q1. Again, the results for Q3 and Q4 are lower than the ones for Q1 and Q2. However, compared to audio, scores are higher for both Q3 and Q4 (60.9% and 61.4% for audio, and 66.4% and 66% for lyrics). As previously discussed, lyrics convey important valence information, particularly relevant to distinguishing low arousal quadrants. Conversely, we can observe some confusion between Q1 and Q4, which stems from the difficulty of lyrics to capture arousal accurately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<figure id="S5.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE X: </span><span id="S5.T10.32.1" class="ltx_text ltx_font_bold">Lyrics</span> CV Best Results </figcaption>
<table id="S5.T10.30" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T10.30.31.1" class="ltx_tr">
<th id="S5.T10.30.31.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T10.30.31.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S5.T10.30.31.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.31.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.31.1.2.1.1" class="ltx_p">Methodology</span>
</span>
</th>
<th id="S5.T10.30.31.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T10.30.31.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.31.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.31.1.4.1.1" class="ltx_p">Cross Val</span>
</span>
</th>
<th id="S5.T10.30.31.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T10.30.31.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.31.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.31.1.6.1.1" class="ltx_p">TVT 75-15-15</span>
</span>
</th>
<th id="S5.T10.30.31.1.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.31.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.31.1.7.1.1" class="ltx_p">TVT 40-30-30</span>
</span>
</th>
</tr>
<tr id="S5.T10.30.32.2" class="ltx_tr">
<th id="S5.T10.30.32.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column"></th>
<th id="S5.T10.30.32.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.32.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.32.2.2.1.1" class="ltx_p">Precision</span>
</span>
</th>
<th id="S5.T10.30.32.2.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.32.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.32.2.3.1.1" class="ltx_p">Recall</span>
</span>
</th>
<th id="S5.T10.30.32.2.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.32.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.32.2.4.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T10.30.32.2.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.32.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.32.2.5.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T10.30.32.2.6" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T10.30.32.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.32.2.6.1.1" class="ltx_p">F1-score</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T10.3.3" class="ltx_tr">
<th id="S5.T10.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S5.T10.3.3.4.1" class="ltx_text">LED</span></th>
<td id="S5.T10.3.3.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.3.3.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T10.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.1.1.1.1" class="ltx_p">73.66% <math id="S5.T10.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.1.1.1.1.1.m1.1a"><mo id="S5.T10.1.1.1.1.1.m1.1.1" xref="S5.T10.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.1.1.1.1.1.m1.1.1.cmml" xref="S5.T10.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 4.33</span>
</span>
</td>
<td id="S5.T10.2.2.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.2.2.2.1.1" class="ltx_p">73.03% <math id="S5.T10.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.2.2.2.1.1.m1.1a"><mo id="S5.T10.2.2.2.1.1.m1.1.1" xref="S5.T10.2.2.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.2.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.2.2.2.1.1.m1.1.1.cmml" xref="S5.T10.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.2.2.2.1.1.m1.1c">\pm</annotation></semantics></math> 4.42</span>
</span>
</td>
<td id="S5.T10.3.3.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.3.3.3.1.1" class="ltx_p"><span id="S5.T10.3.3.3.1.1.1" class="ltx_text ltx_font_bold">72.94%</span> <math id="S5.T10.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.3.3.3.1.1.m1.1a"><mo id="S5.T10.3.3.3.1.1.m1.1.1" xref="S5.T10.3.3.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.3.3.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.3.3.3.1.1.m1.1.1.cmml" xref="S5.T10.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.3.3.3.1.1.m1.1c">\pm</annotation></semantics></math> 4.42</span>
</span>
</td>
<td id="S5.T10.3.3.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.3.3.6.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T10.3.3.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T10.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.3.3.7.1.1" class="ltx_p">-</span>
</span>
</td>
</tr>
<tr id="S5.T10.6.6" class="ltx_tr">
<td id="S5.T10.6.6.4" class="ltx_td ltx_align_justify">
<span id="S5.T10.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.6.6.4.1.1" class="ltx_p">WE + SVM (DL)</span>
</span>
</td>
<td id="S5.T10.4.4.1" class="ltx_td ltx_align_justify">
<span id="S5.T10.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.4.4.1.1.1" class="ltx_p">77.39% <math id="S5.T10.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.4.4.1.1.1.m1.1a"><mo id="S5.T10.4.4.1.1.1.m1.1.1" xref="S5.T10.4.4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.4.4.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.4.4.1.1.1.m1.1.1.cmml" xref="S5.T10.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.4.4.1.1.1.m1.1c">\pm</annotation></semantics></math> 4.72</span>
</span>
</td>
<td id="S5.T10.5.5.2" class="ltx_td ltx_align_justify">
<span id="S5.T10.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.5.5.2.1.1" class="ltx_p">77.01% <math id="S5.T10.5.5.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.5.5.2.1.1.m1.1a"><mo id="S5.T10.5.5.2.1.1.m1.1.1" xref="S5.T10.5.5.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.5.5.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.5.5.2.1.1.m1.1.1.cmml" xref="S5.T10.5.5.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.5.5.2.1.1.m1.1c">\pm</annotation></semantics></math> 4.60</span>
</span>
</td>
<td id="S5.T10.6.6.3" class="ltx_td ltx_align_justify">
<span id="S5.T10.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.6.6.3.1.1" class="ltx_p"><span id="S5.T10.6.6.3.1.1.1" class="ltx_text ltx_font_bold">76.91%</span> <math id="S5.T10.6.6.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.6.6.3.1.1.m1.1a"><mo id="S5.T10.6.6.3.1.1.m1.1.1" xref="S5.T10.6.6.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.6.6.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.6.6.3.1.1.m1.1.1.cmml" xref="S5.T10.6.6.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.6.6.3.1.1.m1.1c">\pm</annotation></semantics></math> 4.65</span>
</span>
</td>
<td id="S5.T10.6.6.5" class="ltx_td ltx_align_justify">
<span id="S5.T10.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.6.6.5.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T10.6.6.6" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T10.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.6.6.6.1.1" class="ltx_p">-</span>
</span>
</td>
</tr>
<tr id="S5.T10.9.9" class="ltx_tr">
<th id="S5.T10.9.9.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">MERGE</th>
<td id="S5.T10.9.9.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.9.9.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T10.7.7.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.7.7.1.1.1" class="ltx_p">67.67% <math id="S5.T10.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.7.7.1.1.1.m1.1a"><mo id="S5.T10.7.7.1.1.1.m1.1.1" xref="S5.T10.7.7.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.7.7.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.7.7.1.1.1.m1.1.1.cmml" xref="S5.T10.7.7.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.7.7.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.84</span>
</span>
</td>
<td id="S5.T10.8.8.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.8.8.2.1.1" class="ltx_p">67.56% <math id="S5.T10.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.8.8.2.1.1.m1.1a"><mo id="S5.T10.8.8.2.1.1.m1.1.1" xref="S5.T10.8.8.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.8.8.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.8.8.2.1.1.m1.1.1.cmml" xref="S5.T10.8.8.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.8.8.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.89</span>
</span>
</td>
<td id="S5.T10.9.9.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.9.9.3.1.1" class="ltx_p">67.46% <math id="S5.T10.9.9.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.9.9.3.1.1.m1.1a"><mo id="S5.T10.9.9.3.1.1.m1.1.1" xref="S5.T10.9.9.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.9.9.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.9.9.3.1.1.m1.1.1.cmml" xref="S5.T10.9.9.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.9.9.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.87</span>
</span>
</td>
<td id="S5.T10.9.9.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.9.9.6.1.1" class="ltx_p">70.98%</span>
</span>
</td>
<td id="S5.T10.9.9.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T10.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.9.9.7.1.1" class="ltx_p">64.95%</span>
</span>
</td>
</tr>
<tr id="S5.T10.12.12" class="ltx_tr">
<th id="S5.T10.12.12.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Lyrics Complete</th>
<td id="S5.T10.12.12.5" class="ltx_td ltx_align_justify">
<span id="S5.T10.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.12.12.5.1.1" class="ltx_p">WE + SVM (DL)</span>
</span>
</td>
<td id="S5.T10.10.10.1" class="ltx_td ltx_align_justify">
<span id="S5.T10.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.10.10.1.1.1" class="ltx_p">73.12% <math id="S5.T10.10.10.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.10.10.1.1.1.m1.1a"><mo id="S5.T10.10.10.1.1.1.m1.1.1" xref="S5.T10.10.10.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.10.10.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.10.10.1.1.1.m1.1.1.cmml" xref="S5.T10.10.10.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.10.10.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.84</span>
</span>
</td>
<td id="S5.T10.11.11.2" class="ltx_td ltx_align_justify">
<span id="S5.T10.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.11.11.2.1.1" class="ltx_p">73.12% <math id="S5.T10.11.11.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.11.11.2.1.1.m1.1a"><mo id="S5.T10.11.11.2.1.1.m1.1.1" xref="S5.T10.11.11.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.11.11.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.11.11.2.1.1.m1.1.1.cmml" xref="S5.T10.11.11.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.11.11.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.79</span>
</span>
</td>
<td id="S5.T10.12.12.3" class="ltx_td ltx_align_justify">
<span id="S5.T10.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.12.12.3.1.1" class="ltx_p">72.95% <math id="S5.T10.12.12.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.12.12.3.1.1.m1.1a"><mo id="S5.T10.12.12.3.1.1.m1.1.1" xref="S5.T10.12.12.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.12.12.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.12.12.3.1.1.m1.1.1.cmml" xref="S5.T10.12.12.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.12.12.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.80</span>
</span>
</td>
<td id="S5.T10.12.12.6" class="ltx_td ltx_align_justify">
<span id="S5.T10.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.12.12.6.1.1" class="ltx_p">73.37%</span>
</span>
</td>
<td id="S5.T10.12.12.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T10.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.12.12.7.1.1" class="ltx_p">71.92%</span>
</span>
</td>
</tr>
<tr id="S5.T10.15.15" class="ltx_tr">
<th id="S5.T10.15.15.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">MERGE</th>
<td id="S5.T10.15.15.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.15.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.15.15.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T10.13.13.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.13.13.1.1.1" class="ltx_p">67.80% <math id="S5.T10.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.13.13.1.1.1.m1.1a"><mo id="S5.T10.13.13.1.1.1.m1.1.1" xref="S5.T10.13.13.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.13.13.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.13.13.1.1.1.m1.1.1.cmml" xref="S5.T10.13.13.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.13.13.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</span>
</span>
</td>
<td id="S5.T10.14.14.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.14.14.2.1.1" class="ltx_p">67.54% <math id="S5.T10.14.14.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.14.14.2.1.1.m1.1a"><mo id="S5.T10.14.14.2.1.1.m1.1.1" xref="S5.T10.14.14.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.14.14.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.14.14.2.1.1.m1.1.1.cmml" xref="S5.T10.14.14.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.14.14.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.47</span>
</span>
</td>
<td id="S5.T10.15.15.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.15.15.3.1.1" class="ltx_p">67.48% <math id="S5.T10.15.15.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.15.15.3.1.1.m1.1a"><mo id="S5.T10.15.15.3.1.1.m1.1.1" xref="S5.T10.15.15.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.15.15.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.15.15.3.1.1.m1.1.1.cmml" xref="S5.T10.15.15.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.15.15.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.49</span>
</span>
</td>
<td id="S5.T10.15.15.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.15.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.15.15.6.1.1" class="ltx_p">69.25%</span>
</span>
</td>
<td id="S5.T10.15.15.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S5.T10.15.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.15.15.7.1.1" class="ltx_p">65.99%</span>
</span>
</td>
</tr>
<tr id="S5.T10.18.18" class="ltx_tr">
<th id="S5.T10.18.18.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Lyrics Balanced</th>
<td id="S5.T10.18.18.5" class="ltx_td ltx_align_justify">
<span id="S5.T10.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.18.18.5.1.1" class="ltx_p">WE + SVM (DL)</span>
</span>
</td>
<td id="S5.T10.16.16.1" class="ltx_td ltx_align_justify">
<span id="S5.T10.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.16.16.1.1.1" class="ltx_p">73.56% <math id="S5.T10.16.16.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.16.16.1.1.1.m1.1a"><mo id="S5.T10.16.16.1.1.1.m1.1.1" xref="S5.T10.16.16.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.16.16.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.16.16.1.1.1.m1.1.1.cmml" xref="S5.T10.16.16.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.16.16.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.59</span>
</span>
</td>
<td id="S5.T10.17.17.2" class="ltx_td ltx_align_justify">
<span id="S5.T10.17.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.17.17.2.1.1" class="ltx_p">75.51% <math id="S5.T10.17.17.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.17.17.2.1.1.m1.1a"><mo id="S5.T10.17.17.2.1.1.m1.1.1" xref="S5.T10.17.17.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.17.17.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.17.17.2.1.1.m1.1.1.cmml" xref="S5.T10.17.17.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.17.17.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</span>
</span>
</td>
<td id="S5.T10.18.18.3" class="ltx_td ltx_align_justify">
<span id="S5.T10.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.18.18.3.1.1" class="ltx_p">73.40% <math id="S5.T10.18.18.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.18.18.3.1.1.m1.1a"><mo id="S5.T10.18.18.3.1.1.m1.1.1" xref="S5.T10.18.18.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.18.18.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.18.18.3.1.1.m1.1.1.cmml" xref="S5.T10.18.18.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.18.18.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.54</span>
</span>
</td>
<td id="S5.T10.18.18.6" class="ltx_td ltx_align_justify">
<span id="S5.T10.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.18.18.6.1.1" class="ltx_p"><span id="S5.T10.18.18.6.1.1.1" class="ltx_text ltx_font_bold">73.81%</span></span>
</span>
</td>
<td id="S5.T10.18.18.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T10.18.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.18.18.7.1.1" class="ltx_p"><span id="S5.T10.18.18.7.1.1.1" class="ltx_text ltx_font_bold">73.70%</span></span>
</span>
</td>
</tr>
<tr id="S5.T10.21.21" class="ltx_tr">
<th id="S5.T10.21.21.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">MERGE</th>
<td id="S5.T10.21.21.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.21.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.21.21.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T10.19.19.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.19.19.1.1.1" class="ltx_p">69.54% <math id="S5.T10.19.19.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.19.19.1.1.1.m1.1a"><mo id="S5.T10.19.19.1.1.1.m1.1.1" xref="S5.T10.19.19.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.19.19.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.19.19.1.1.1.m1.1.1.cmml" xref="S5.T10.19.19.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.19.19.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.26</span>
</span>
</td>
<td id="S5.T10.20.20.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.20.20.2.1.1" class="ltx_p">69.52% <math id="S5.T10.20.20.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.20.20.2.1.1.m1.1a"><mo id="S5.T10.20.20.2.1.1.m1.1.1" xref="S5.T10.20.20.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.20.20.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.20.20.2.1.1.m1.1.1.cmml" xref="S5.T10.20.20.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.20.20.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.25</span>
</span>
</td>
<td id="S5.T10.21.21.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.21.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.21.21.3.1.1" class="ltx_p"><span id="S5.T10.21.21.3.1.1.1" class="ltx_text ltx_font_bold">69.31%</span> <math id="S5.T10.21.21.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.21.21.3.1.1.m1.1a"><mo id="S5.T10.21.21.3.1.1.m1.1.1" xref="S5.T10.21.21.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.21.21.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.21.21.3.1.1.m1.1.1.cmml" xref="S5.T10.21.21.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.21.21.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.27</span>
</span>
</td>
<td id="S5.T10.21.21.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T10.21.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.21.21.6.1.1" class="ltx_p"><span id="S5.T10.21.21.6.1.1.1" class="ltx_text ltx_font_bold">71.31%</span></span>
</span>
</td>
<td id="S5.T10.21.21.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T10.21.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.21.21.7.1.1" class="ltx_p"><span id="S5.T10.21.21.7.1.1.1" class="ltx_text ltx_font_bold">69.57%</span></span>
</span>
</td>
</tr>
<tr id="S5.T10.24.24" class="ltx_tr">
<th id="S5.T10.24.24.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Bimodal Complete</th>
<td id="S5.T10.24.24.5" class="ltx_td ltx_align_justify">
<span id="S5.T10.24.24.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.24.24.5.1.1" class="ltx_p">WE + SVM (DL)</span>
</span>
</td>
<td id="S5.T10.22.22.1" class="ltx_td ltx_align_justify">
<span id="S5.T10.22.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.22.22.1.1.1" class="ltx_p">74.34% <math id="S5.T10.22.22.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.22.22.1.1.1.m1.1a"><mo id="S5.T10.22.22.1.1.1.m1.1.1" xref="S5.T10.22.22.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.22.22.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.22.22.1.1.1.m1.1.1.cmml" xref="S5.T10.22.22.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.22.22.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.80</span>
</span>
</td>
<td id="S5.T10.23.23.2" class="ltx_td ltx_align_justify">
<span id="S5.T10.23.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.23.23.2.1.1" class="ltx_p">74.36% <math id="S5.T10.23.23.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.23.23.2.1.1.m1.1a"><mo id="S5.T10.23.23.2.1.1.m1.1.1" xref="S5.T10.23.23.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.23.23.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.23.23.2.1.1.m1.1.1.cmml" xref="S5.T10.23.23.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.23.23.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.70</span>
</span>
</td>
<td id="S5.T10.24.24.3" class="ltx_td ltx_align_justify">
<span id="S5.T10.24.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.24.24.3.1.1" class="ltx_p"><span id="S5.T10.24.24.3.1.1.1" class="ltx_text ltx_font_bold">74.16%</span> <math id="S5.T10.24.24.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.24.24.3.1.1.m1.1a"><mo id="S5.T10.24.24.3.1.1.m1.1.1" xref="S5.T10.24.24.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.24.24.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.24.24.3.1.1.m1.1.1.cmml" xref="S5.T10.24.24.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.24.24.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.75</span>
</span>
</td>
<td id="S5.T10.24.24.6" class="ltx_td ltx_align_justify">
<span id="S5.T10.24.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.24.24.6.1.1" class="ltx_p">72.33%</span>
</span>
</td>
<td id="S5.T10.24.24.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T10.24.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.24.24.7.1.1" class="ltx_p">71.55%</span>
</span>
</td>
</tr>
<tr id="S5.T10.27.27" class="ltx_tr">
<th id="S5.T10.27.27.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">MERGE</th>
<td id="S5.T10.27.27.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.27.27.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.27.27.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T10.25.25.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.25.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.25.25.1.1.1" class="ltx_p">67.35% <math id="S5.T10.25.25.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.25.25.1.1.1.m1.1a"><mo id="S5.T10.25.25.1.1.1.m1.1.1" xref="S5.T10.25.25.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.25.25.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.25.25.1.1.1.m1.1.1.cmml" xref="S5.T10.25.25.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.25.25.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.42</span>
</span>
</td>
<td id="S5.T10.26.26.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.26.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.26.26.2.1.1" class="ltx_p">67.05% <math id="S5.T10.26.26.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.26.26.2.1.1.m1.1a"><mo id="S5.T10.26.26.2.1.1.m1.1.1" xref="S5.T10.26.26.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.26.26.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.26.26.2.1.1.m1.1.1.cmml" xref="S5.T10.26.26.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.26.26.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.33</span>
</span>
</td>
<td id="S5.T10.27.27.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.27.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.27.27.3.1.1" class="ltx_p">66.96% <math id="S5.T10.27.27.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.27.27.3.1.1.m1.1a"><mo id="S5.T10.27.27.3.1.1.m1.1.1" xref="S5.T10.27.27.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.27.27.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.27.27.3.1.1.m1.1.1.cmml" xref="S5.T10.27.27.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.27.27.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.35</span>
</span>
</td>
<td id="S5.T10.27.27.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T10.27.27.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.27.27.6.1.1" class="ltx_p">69.50%</span>
</span>
</td>
<td id="S5.T10.27.27.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S5.T10.27.27.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.27.27.7.1.1" class="ltx_p">66.74%</span>
</span>
</td>
</tr>
<tr id="S5.T10.30.30" class="ltx_tr">
<th id="S5.T10.30.30.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">Bimodal Balanced</th>
<td id="S5.T10.30.30.5" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T10.30.30.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.30.5.1.1" class="ltx_p">WE + SVM (DL)</span>
</span>
</td>
<td id="S5.T10.28.28.1" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T10.28.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.28.28.1.1.1" class="ltx_p">73.34% <math id="S5.T10.28.28.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.28.28.1.1.1.m1.1a"><mo id="S5.T10.28.28.1.1.1.m1.1.1" xref="S5.T10.28.28.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.28.28.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.28.28.1.1.1.m1.1.1.cmml" xref="S5.T10.28.28.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.28.28.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.10</span>
</span>
</td>
<td id="S5.T10.29.29.2" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T10.29.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.29.29.2.1.1" class="ltx_p">73.14% <math id="S5.T10.29.29.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.29.29.2.1.1.m1.1a"><mo id="S5.T10.29.29.2.1.1.m1.1.1" xref="S5.T10.29.29.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.29.29.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.29.29.2.1.1.m1.1.1.cmml" xref="S5.T10.29.29.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.29.29.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.05</span>
</span>
</td>
<td id="S5.T10.30.30.3" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T10.30.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.30.3.1.1" class="ltx_p">73.06% <math id="S5.T10.30.30.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T10.30.30.3.1.1.m1.1a"><mo id="S5.T10.30.30.3.1.1.m1.1.1" xref="S5.T10.30.30.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T10.30.30.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T10.30.30.3.1.1.m1.1.1.cmml" xref="S5.T10.30.30.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T10.30.30.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.09</span>
</span>
</td>
<td id="S5.T10.30.30.6" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T10.30.30.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.30.6.1.1" class="ltx_p">71.82%</span>
</span>
</td>
<td id="S5.T10.30.30.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_b">
<span id="S5.T10.30.30.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.30.30.7.1.1" class="ltx_p">71.57%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XI: </span><span id="S5.T11.18.1" class="ltx_text ltx_font_bold">Lyrics</span> Confusion Matrix Best Results (CV) </figcaption>
<table id="S5.T11.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T11.16.17.1" class="ltx_tr">
<th id="S5.T11.16.17.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T11.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Predicted</th>
</tr>
<tr id="S5.T11.16.18.2" class="ltx_tr">
<th id="S5.T11.16.18.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T11.16.18.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q1</th>
<th id="S5.T11.16.18.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q2</th>
<th id="S5.T11.16.18.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q3</th>
<th id="S5.T11.16.18.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T11.16.19.1" class="ltx_tr">
<th id="S5.T11.16.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="8">
<span id="S5.T11.16.19.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S5.T11.16.19.1.1.1.1" class="ltx_p">
<span id="S5.T11.16.19.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:29.2pt;vertical-align:-11.1pt;"><span class="ltx_transformed_inner" style="width:29.2pt;transform:translate(-11.11pt,0pt) rotate(-90deg) ;">
<span id="S5.T11.16.19.1.1.1.1.1.1" class="ltx_p">Actual</span>
</span></span></span>
</span>
</th>
<th id="S5.T11.16.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T11.16.19.1.2.1" class="ltx_text">Q1</span></th>
<td id="S5.T11.16.19.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T11.16.19.1.3.1" class="ltx_text ltx_font_bold">71.6%</span></td>
<td id="S5.T11.16.19.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8%</td>
<td id="S5.T11.16.19.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.8%</td>
<td id="S5.T11.16.19.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.4%</td>
</tr>
<tr id="S5.T11.4.4" class="ltx_tr">
<td id="S5.T11.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.1.1.1.m1.1a"><mo id="S5.T11.1.1.1.m1.1.1" xref="S5.T11.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T11.1.1.1.m1.1.1.cmml" xref="S5.T11.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.1.1.1.m1.1c">\pm</annotation></semantics></math>6.6</td>
<td id="S5.T11.2.2.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.2.2.2.m1.1a"><mo id="S5.T11.2.2.2.m1.1.1" xref="S5.T11.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T11.2.2.2.m1.1.1.cmml" xref="S5.T11.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.2.2.2.m1.1c">\pm</annotation></semantics></math>2.4</td>
<td id="S5.T11.3.3.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.3.3.3.m1.1a"><mo id="S5.T11.3.3.3.m1.1.1" xref="S5.T11.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T11.3.3.3.m1.1.1.cmml" xref="S5.T11.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.3.3.3.m1.1c">\pm</annotation></semantics></math>3.4</td>
<td id="S5.T11.4.4.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.4.4.4.m1.1a"><mo id="S5.T11.4.4.4.m1.1.1" xref="S5.T11.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.4.4.4.m1.1b"><csymbol cd="latexml" id="S5.T11.4.4.4.m1.1.1.cmml" xref="S5.T11.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.4.4.4.m1.1c">\pm</annotation></semantics></math>5.5</td>
</tr>
<tr id="S5.T11.16.20.2" class="ltx_tr">
<th id="S5.T11.16.20.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T11.16.20.2.1.1" class="ltx_text">Q2</span></th>
<td id="S5.T11.16.20.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.4%</td>
<td id="S5.T11.16.20.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T11.16.20.2.3.1" class="ltx_text ltx_font_bold">88.8%</span></td>
<td id="S5.T11.16.20.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.7%</td>
<td id="S5.T11.16.20.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.3%</td>
</tr>
<tr id="S5.T11.8.8" class="ltx_tr">
<td id="S5.T11.5.5.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.5.5.1.m1.1a"><mo id="S5.T11.5.5.1.m1.1.1" xref="S5.T11.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T11.5.5.1.m1.1.1.cmml" xref="S5.T11.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.5.5.1.m1.1c">\pm</annotation></semantics></math>3.3</td>
<td id="S5.T11.6.6.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.6.6.2.m1.1a"><mo id="S5.T11.6.6.2.m1.1.1" xref="S5.T11.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T11.6.6.2.m1.1.1.cmml" xref="S5.T11.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.6.6.2.m1.1c">\pm</annotation></semantics></math>4.3</td>
<td id="S5.T11.7.7.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.7.7.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.7.7.3.m1.1a"><mo id="S5.T11.7.7.3.m1.1.1" xref="S5.T11.7.7.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.7.7.3.m1.1b"><csymbol cd="latexml" id="S5.T11.7.7.3.m1.1.1.cmml" xref="S5.T11.7.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.7.7.3.m1.1c">\pm</annotation></semantics></math>4.7</td>
<td id="S5.T11.8.8.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.8.8.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.8.8.4.m1.1a"><mo id="S5.T11.8.8.4.m1.1.1" xref="S5.T11.8.8.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.8.8.4.m1.1b"><csymbol cd="latexml" id="S5.T11.8.8.4.m1.1.1.cmml" xref="S5.T11.8.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.8.8.4.m1.1c">\pm</annotation></semantics></math>3.0</td>
</tr>
<tr id="S5.T11.16.21.3" class="ltx_tr">
<th id="S5.T11.16.21.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T11.16.21.3.1.1" class="ltx_text">Q3</span></th>
<td id="S5.T11.16.21.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.2%</td>
<td id="S5.T11.16.21.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.4%</td>
<td id="S5.T11.16.21.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T11.16.21.3.4.1" class="ltx_text ltx_font_bold">66.4%</span></td>
<td id="S5.T11.16.21.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.63%</td>
</tr>
<tr id="S5.T11.12.12" class="ltx_tr">
<td id="S5.T11.9.9.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.9.9.1.m1.1a"><mo id="S5.T11.9.9.1.m1.1.1" xref="S5.T11.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.9.9.1.m1.1b"><csymbol cd="latexml" id="S5.T11.9.9.1.m1.1.1.cmml" xref="S5.T11.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.9.9.1.m1.1c">\pm</annotation></semantics></math>3.2</td>
<td id="S5.T11.10.10.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.10.10.2.m1.1a"><mo id="S5.T11.10.10.2.m1.1.1" xref="S5.T11.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.10.10.2.m1.1b"><csymbol cd="latexml" id="S5.T11.10.10.2.m1.1.1.cmml" xref="S5.T11.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.10.10.2.m1.1c">\pm</annotation></semantics></math>3.0</td>
<td id="S5.T11.11.11.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.11.11.3.m1.1a"><mo id="S5.T11.11.11.3.m1.1.1" xref="S5.T11.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.11.11.3.m1.1b"><csymbol cd="latexml" id="S5.T11.11.11.3.m1.1.1.cmml" xref="S5.T11.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.11.11.3.m1.1c">\pm</annotation></semantics></math>6.1</td>
<td id="S5.T11.12.12.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T11.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.12.12.4.m1.1a"><mo id="S5.T11.12.12.4.m1.1.1" xref="S5.T11.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.12.12.4.m1.1b"><csymbol cd="latexml" id="S5.T11.12.12.4.m1.1.1.cmml" xref="S5.T11.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.12.12.4.m1.1c">\pm</annotation></semantics></math>4.8</td>
</tr>
<tr id="S5.T11.16.22.4" class="ltx_tr">
<th id="S5.T11.16.22.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T11.16.22.4.1.1" class="ltx_text">Q4</span></th>
<td id="S5.T11.16.22.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.8%</td>
<td id="S5.T11.16.22.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.0%</td>
<td id="S5.T11.16.22.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.1%</td>
<td id="S5.T11.16.22.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T11.16.22.4.5.1" class="ltx_text ltx_font_bold">66.0%</span></td>
</tr>
<tr id="S5.T11.16.16" class="ltx_tr">
<td id="S5.T11.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T11.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.13.13.1.m1.1a"><mo id="S5.T11.13.13.1.m1.1.1" xref="S5.T11.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T11.13.13.1.m1.1.1.cmml" xref="S5.T11.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.13.13.1.m1.1c">\pm</annotation></semantics></math>4.8</td>
<td id="S5.T11.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T11.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.14.14.2.m1.1a"><mo id="S5.T11.14.14.2.m1.1.1" xref="S5.T11.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T11.14.14.2.m1.1.1.cmml" xref="S5.T11.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.14.14.2.m1.1c">\pm</annotation></semantics></math>1.1</td>
<td id="S5.T11.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T11.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.15.15.3.m1.1a"><mo id="S5.T11.15.15.3.m1.1.1" xref="S5.T11.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T11.15.15.3.m1.1.1.cmml" xref="S5.T11.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.15.15.3.m1.1c">\pm</annotation></semantics></math>4.8</td>
<td id="S5.T11.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T11.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T11.16.16.4.m1.1a"><mo id="S5.T11.16.16.4.m1.1.1" xref="S5.T11.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T11.16.16.4.m1.1b"><csymbol cd="latexml" id="S5.T11.16.16.4.m1.1.1.cmml" xref="S5.T11.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.16.16.4.m1.1c">\pm</annotation></semantics></math>6.3</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">MERGE Bimodal</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Regarding the experiments using the bimodal datasets, Table <a href="#S5.T12" title="TABLE XII ‣ V-C MERGE Bimodal ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XII</span></a> shows that the classical approach with a feature-level fusion of audio and lyrics features outperforms a DL approach based on a late fusion of audio and lyrics CNN branches (a maximum F1-score of 78.58% in the former, against 74.5% in the latter, both in the bimodal complete dataset). Again, this might be a consequence of the fact that the potential of CNN-based architectures cannot yet be fully exploited due to the dataset size.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">When comparing the bimodal, audio-only, and lyrics-only approaches, results in the novel datasets for the classical approach show that the bimodal strategy significantly outperforms the best methods from the isolated modalities, as expected: the bimodal model attained a maximum F1-score of 78.58%, against 74.14% for audio and 69.31% for lyrics, all in the bimodal complete dataset. The same happens for DL approaches, where the bimodal methodology reached 74.5%, against 74.16% for lyrics-only and 63.63% for audio-only.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Once again, the new datasets’ size and imbalance had little impact as the results attained for the two datasets (bimodal complete and balanced) are similar. Nevertheless, we can observe that, in all experiments (audio-only, lyrics-only, and bimodal), the bimodal complete dataset slightly outperformed the other datasets.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Regarding the standard deviation of the F1-scores for 10x10-fold CV, we can again observe a low sensitivity to the data folds (from 2.41% to 4.21%).</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">When compared with CV, TVT attains again comparable results, although this time higher (for example, a top result of 74.5% in CV against 79.21% in TVT 70-15-15, using the DL approach). As before, the 70-15-15 split outperforms the 40-30-30 split. An F1-score of 79.21% was achieved with 70-15-15 (the top overall result achieved in all the experiments conducted in this study) against 75.9% in 40-30-30. It is worth noting that, despite the size of the dataset, the best overall result was obtained with a DL approach.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">Finally, the confusion matrix for the best performing bimodal model with CV (HF + SVM, on the bimodal complete dataset, with an F1-score of 78.58%) is presented in Table <a href="#S5.T13" title="TABLE XIII ‣ V-C MERGE Bimodal ‣ V Results and Discussion ‣ IV-G Bimodal Deep Learning ‣ IV-F Bimodal Classical ML ‣ IV-E Lyrics Deep Learning ‣ IV-D Lyrics Classical ML ‣ IV-C Audio Deep Learning ‣ IV-B Audio Classical ML ‣ IV Baseline Methodologies and Evaluation Strategy ‣ MERGE - A Bimodal Dataset For Static Music Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XIII</span></a>. As can be observed, compared to the audio-only solution, the score increased for all quadrants, particularly Q3 (from 60.9% to 69.5%) and Q4 (from 61.1% to 65.8%). This confirms the potential of bimodal approaches to reduce the confusion between low arousal quadrants. Yet, the attained results show that there is plenty of room for improvement and that the separation between Q3 and Q4 is far from being solved <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Compared to the lyrics-only model, the improvements observed in the prediction of Q3 are not so notorious (from 66.4% to 69.5%), while, for Q4, the results are nearly the same (66% against 65.8%). This reinforces the conclusion that most of the improvement in the classification of the lower arousal quadrants stems from the lyrics.</p>
</div>
<figure id="S5.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XII: </span><span id="S5.T12.14.1" class="ltx_text ltx_font_bold">Bimodal</span> CV Best Results </figcaption>
<table id="S5.T12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T12.12.13.1" class="ltx_tr">
<th id="S5.T12.12.13.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S5.T12.12.13.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S5.T12.12.13.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.13.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.13.1.2.1.1" class="ltx_p">Methodology</span>
</span>
</th>
<th id="S5.T12.12.13.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T12.12.13.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.13.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.13.1.4.1.1" class="ltx_p">Cross Val</span>
</span>
</th>
<th id="S5.T12.12.13.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t"></th>
<th id="S5.T12.12.13.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.13.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.13.1.6.1.1" class="ltx_p">TVT 75-15-15</span>
</span>
</th>
<th id="S5.T12.12.13.1.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.13.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.13.1.7.1.1" class="ltx_p">TVT 40-30-30</span>
</span>
</th>
</tr>
<tr id="S5.T12.12.14.2" class="ltx_tr">
<th id="S5.T12.12.14.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column"></th>
<th id="S5.T12.12.14.2.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.14.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.14.2.2.1.1" class="ltx_p">Precision</span>
</span>
</th>
<th id="S5.T12.12.14.2.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.14.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.14.2.3.1.1" class="ltx_p">Recall</span>
</span>
</th>
<th id="S5.T12.12.14.2.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.14.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.14.2.4.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T12.12.14.2.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.14.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.14.2.5.1.1" class="ltx_p">F1-score</span>
</span>
</th>
<th id="S5.T12.12.14.2.6" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S5.T12.12.14.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.14.2.6.1.1" class="ltx_p">F1-score</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T12.3.3" class="ltx_tr">
<th id="S5.T12.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">MERGE</th>
<td id="S5.T12.3.3.5" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T12.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.3.3.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T12.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T12.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.1.1.1.1.1" class="ltx_p">78.71% <math id="S5.T12.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.1.1.1.1.1.m1.1a"><mo id="S5.T12.1.1.1.1.1.m1.1.1" xref="S5.T12.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.1.1.1.1.1.m1.1.1.cmml" xref="S5.T12.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.52</span>
</span>
</td>
<td id="S5.T12.2.2.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T12.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.2.2.2.1.1" class="ltx_p">78.74% <math id="S5.T12.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.2.2.2.1.1.m1.1a"><mo id="S5.T12.2.2.2.1.1.m1.1.1" xref="S5.T12.2.2.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.2.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.2.2.2.1.1.m1.1.1.cmml" xref="S5.T12.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.2.2.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.44</span>
</span>
</td>
<td id="S5.T12.3.3.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T12.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.3.3.3.1.1" class="ltx_p"><span id="S5.T12.3.3.3.1.1.1" class="ltx_text ltx_font_bold">78.58%</span> <math id="S5.T12.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.3.3.3.1.1.m1.1a"><mo id="S5.T12.3.3.3.1.1.m1.1.1" xref="S5.T12.3.3.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.3.3.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.3.3.3.1.1.m1.1.1.cmml" xref="S5.T12.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.3.3.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.47</span>
</span>
</td>
<td id="S5.T12.3.3.6" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S5.T12.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.3.3.6.1.1" class="ltx_p"><span id="S5.T12.3.3.6.1.1.1" class="ltx_text ltx_font_bold">77.98%</span></span>
</span>
</td>
<td id="S5.T12.3.3.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S5.T12.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.3.3.7.1.1" class="ltx_p"><span id="S5.T12.3.3.7.1.1.1" class="ltx_text ltx_font_bold">75.90%</span></span>
</span>
</td>
</tr>
<tr id="S5.T12.6.6" class="ltx_tr">
<th id="S5.T12.6.6.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">Bimodal Complete</th>
<td id="S5.T12.6.6.5" class="ltx_td ltx_align_justify">
<span id="S5.T12.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.6.6.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T12.4.4.1" class="ltx_td ltx_align_justify">
<span id="S5.T12.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.4.4.1.1.1" class="ltx_p">76.31% <math id="S5.T12.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.4.4.1.1.1.m1.1a"><mo id="S5.T12.4.4.1.1.1.m1.1.1" xref="S5.T12.4.4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.4.4.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.4.4.1.1.1.m1.1.1.cmml" xref="S5.T12.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.4.4.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.69</span>
</span>
</td>
<td id="S5.T12.5.5.2" class="ltx_td ltx_align_justify">
<span id="S5.T12.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.5.5.2.1.1" class="ltx_p">74.81% <math id="S5.T12.5.5.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.5.5.2.1.1.m1.1a"><mo id="S5.T12.5.5.2.1.1.m1.1.1" xref="S5.T12.5.5.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.5.5.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.5.5.2.1.1.m1.1.1.cmml" xref="S5.T12.5.5.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.5.5.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.88</span>
</span>
</td>
<td id="S5.T12.6.6.3" class="ltx_td ltx_align_justify">
<span id="S5.T12.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.6.6.3.1.1" class="ltx_p"><span id="S5.T12.6.6.3.1.1.1" class="ltx_text ltx_font_bold">74.50%</span> <math id="S5.T12.6.6.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.6.6.3.1.1.m1.1a"><mo id="S5.T12.6.6.3.1.1.m1.1.1" xref="S5.T12.6.6.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.6.6.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.6.6.3.1.1.m1.1.1.cmml" xref="S5.T12.6.6.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.6.6.3.1.1.m1.1c">\pm</annotation></semantics></math> 4.21</span>
</span>
</td>
<td id="S5.T12.6.6.6" class="ltx_td ltx_align_justify">
<span id="S5.T12.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.6.6.6.1.1" class="ltx_p"><span id="S5.T12.6.6.6.1.1.1" class="ltx_text ltx_font_bold">79.21%</span></span>
</span>
</td>
<td id="S5.T12.6.6.7" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S5.T12.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.6.6.7.1.1" class="ltx_p"><span id="S5.T12.6.6.7.1.1.1" class="ltx_text ltx_font_bold">75.65%</span></span>
</span>
</td>
</tr>
<tr id="S5.T12.9.9" class="ltx_tr">
<th id="S5.T12.9.9.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">MERGE</th>
<td id="S5.T12.9.9.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T12.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.9.9.5.1.1" class="ltx_p">HF + SVM (CML)</span>
</span>
</td>
<td id="S5.T12.7.7.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T12.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.7.7.1.1.1" class="ltx_p">77.53% <math id="S5.T12.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.7.7.1.1.1.m1.1a"><mo id="S5.T12.7.7.1.1.1.m1.1.1" xref="S5.T12.7.7.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.7.7.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.7.7.1.1.1.m1.1.1.cmml" xref="S5.T12.7.7.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.7.7.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.41</span>
</span>
</td>
<td id="S5.T12.8.8.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T12.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.8.8.2.1.1" class="ltx_p">77.51% <math id="S5.T12.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.8.8.2.1.1.m1.1a"><mo id="S5.T12.8.8.2.1.1.m1.1.1" xref="S5.T12.8.8.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.8.8.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.8.8.2.1.1.m1.1.1.cmml" xref="S5.T12.8.8.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.8.8.2.1.1.m1.1c">\pm</annotation></semantics></math> 2.34</span>
</span>
</td>
<td id="S5.T12.9.9.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T12.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.9.9.3.1.1" class="ltx_p">77.34% <math id="S5.T12.9.9.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.9.9.3.1.1.m1.1a"><mo id="S5.T12.9.9.3.1.1.m1.1.1" xref="S5.T12.9.9.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.9.9.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.9.9.3.1.1.m1.1.1.cmml" xref="S5.T12.9.9.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.9.9.3.1.1.m1.1c">\pm</annotation></semantics></math> 2.41</span>
</span>
</td>
<td id="S5.T12.9.9.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S5.T12.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.9.9.6.1.1" class="ltx_p">76.45%</span>
</span>
</td>
<td id="S5.T12.9.9.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S5.T12.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.9.9.7.1.1" class="ltx_p">75.18%</span>
</span>
</td>
</tr>
<tr id="S5.T12.12.12" class="ltx_tr">
<th id="S5.T12.12.12.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">Bimodal Balanced</th>
<td id="S5.T12.12.12.5" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T12.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.12.5.1.1" class="ltx_p">MS + CNN (DL)</span>
</span>
</td>
<td id="S5.T12.10.10.1" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T12.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.10.10.1.1.1" class="ltx_p">75.34% <math id="S5.T12.10.10.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.10.10.1.1.1.m1.1a"><mo id="S5.T12.10.10.1.1.1.m1.1.1" xref="S5.T12.10.10.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.10.10.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.10.10.1.1.1.m1.1.1.cmml" xref="S5.T12.10.10.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.10.10.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.06</span>
</span>
</td>
<td id="S5.T12.11.11.2" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T12.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.11.11.2.1.1" class="ltx_p">73.56% <math id="S5.T12.11.11.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.11.11.2.1.1.m1.1a"><mo id="S5.T12.11.11.2.1.1.m1.1.1" xref="S5.T12.11.11.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.11.11.2.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.11.11.2.1.1.m1.1.1.cmml" xref="S5.T12.11.11.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.11.11.2.1.1.m1.1c">\pm</annotation></semantics></math> 3.32</span>
</span>
</td>
<td id="S5.T12.12.12.3" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T12.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.12.3.1.1" class="ltx_p">73.23% <math id="S5.T12.12.12.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T12.12.12.3.1.1.m1.1a"><mo id="S5.T12.12.12.3.1.1.m1.1.1" xref="S5.T12.12.12.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T12.12.12.3.1.1.m1.1b"><csymbol cd="latexml" id="S5.T12.12.12.3.1.1.m1.1.1.cmml" xref="S5.T12.12.12.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.12.12.3.1.1.m1.1c">\pm</annotation></semantics></math> 3.46</span>
</span>
</td>
<td id="S5.T12.12.12.6" class="ltx_td ltx_align_justify ltx_border_b">
<span id="S5.T12.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.12.6.1.1" class="ltx_p">78.41%</span>
</span>
</td>
<td id="S5.T12.12.12.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_b">
<span id="S5.T12.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T12.12.12.7.1.1" class="ltx_p">74.48%</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XIII: </span><span id="S5.T13.18.1" class="ltx_text ltx_font_bold">Bimodal</span> Confusion Matrix Best Results (CV) </figcaption>
<table id="S5.T13.16" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T13.16.17.1" class="ltx_tr">
<th id="S5.T13.16.17.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T13.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Predicted</th>
</tr>
<tr id="S5.T13.16.18.2" class="ltx_tr">
<th id="S5.T13.16.18.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" colspan="2"></th>
<th id="S5.T13.16.18.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q1</th>
<th id="S5.T13.16.18.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q2</th>
<th id="S5.T13.16.18.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q3</th>
<th id="S5.T13.16.18.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Q4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T13.16.19.1" class="ltx_tr">
<th id="S5.T13.16.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="8">
<span id="S5.T13.16.19.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S5.T13.16.19.1.1.1.1" class="ltx_p">
<span id="S5.T13.16.19.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:29.2pt;vertical-align:-11.1pt;"><span class="ltx_transformed_inner" style="width:29.2pt;transform:translate(-11.11pt,0pt) rotate(-90deg) ;">
<span id="S5.T13.16.19.1.1.1.1.1.1" class="ltx_p">Actual</span>
</span></span></span>
</span>
</th>
<th id="S5.T13.16.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T13.16.19.1.2.1" class="ltx_text">Q1</span></th>
<td id="S5.T13.16.19.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T13.16.19.1.3.1" class="ltx_text ltx_font_bold">81.1%</span></td>
<td id="S5.T13.16.19.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.9%</td>
<td id="S5.T13.16.19.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.1%</td>
<td id="S5.T13.16.19.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.2%</td>
</tr>
<tr id="S5.T13.4.4" class="ltx_tr">
<td id="S5.T13.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.1.1.1.m1.1a"><mo id="S5.T13.1.1.1.m1.1.1" xref="S5.T13.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T13.1.1.1.m1.1.1.cmml" xref="S5.T13.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.1.1.1.m1.1c">\pm</annotation></semantics></math>4.8</td>
<td id="S5.T13.2.2.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.2.2.2.m1.1a"><mo id="S5.T13.2.2.2.m1.1.1" xref="S5.T13.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T13.2.2.2.m1.1.1.cmml" xref="S5.T13.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.2.2.2.m1.1c">\pm</annotation></semantics></math>2.5</td>
<td id="S5.T13.3.3.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.3.3.3.m1.1a"><mo id="S5.T13.3.3.3.m1.1.1" xref="S5.T13.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T13.3.3.3.m1.1.1.cmml" xref="S5.T13.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.3.3.3.m1.1c">\pm</annotation></semantics></math>3.0</td>
<td id="S5.T13.4.4.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.4.4.4.m1.1a"><mo id="S5.T13.4.4.4.m1.1.1" xref="S5.T13.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.4.4.4.m1.1b"><csymbol cd="latexml" id="S5.T13.4.4.4.m1.1.1.cmml" xref="S5.T13.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.4.4.4.m1.1c">\pm</annotation></semantics></math>4.5</td>
</tr>
<tr id="S5.T13.16.20.2" class="ltx_tr">
<th id="S5.T13.16.20.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T13.16.20.2.1.1" class="ltx_text">Q2</span></th>
<td id="S5.T13.16.20.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.7%</td>
<td id="S5.T13.16.20.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T13.16.20.2.3.1" class="ltx_text ltx_font_bold">93.7%</span></td>
<td id="S5.T13.16.20.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.5%</td>
<td id="S5.T13.16.20.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5%</td>
</tr>
<tr id="S5.T13.8.8" class="ltx_tr">
<td id="S5.T13.5.5.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.5.5.1.m1.1a"><mo id="S5.T13.5.5.1.m1.1.1" xref="S5.T13.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T13.5.5.1.m1.1.1.cmml" xref="S5.T13.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.5.5.1.m1.1c">\pm</annotation></semantics></math>3.3</td>
<td id="S5.T13.6.6.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.6.6.2.m1.1a"><mo id="S5.T13.6.6.2.m1.1.1" xref="S5.T13.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T13.6.6.2.m1.1.1.cmml" xref="S5.T13.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.6.6.2.m1.1c">\pm</annotation></semantics></math>2.7</td>
<td id="S5.T13.7.7.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.7.7.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.7.7.3.m1.1a"><mo id="S5.T13.7.7.3.m1.1.1" xref="S5.T13.7.7.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.7.7.3.m1.1b"><csymbol cd="latexml" id="S5.T13.7.7.3.m1.1.1.cmml" xref="S5.T13.7.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.7.7.3.m1.1c">\pm</annotation></semantics></math>2.6</td>
<td id="S5.T13.8.8.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.8.8.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.8.8.4.m1.1a"><mo id="S5.T13.8.8.4.m1.1.1" xref="S5.T13.8.8.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.8.8.4.m1.1b"><csymbol cd="latexml" id="S5.T13.8.8.4.m1.1.1.cmml" xref="S5.T13.8.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.8.8.4.m1.1c">\pm</annotation></semantics></math>1.1</td>
</tr>
<tr id="S5.T13.16.21.3" class="ltx_tr">
<th id="S5.T13.16.21.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T13.16.21.3.1.1" class="ltx_text">Q3</span></th>
<td id="S5.T13.16.21.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.6%</td>
<td id="S5.T13.16.21.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.2%</td>
<td id="S5.T13.16.21.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T13.16.21.3.4.1" class="ltx_text ltx_font_bold">69.5%</span></td>
<td id="S5.T13.16.21.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.4%</td>
</tr>
<tr id="S5.T13.12.12" class="ltx_tr">
<td id="S5.T13.9.9.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.9.9.1.m1.1a"><mo id="S5.T13.9.9.1.m1.1.1" xref="S5.T13.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.9.9.1.m1.1b"><csymbol cd="latexml" id="S5.T13.9.9.1.m1.1.1.cmml" xref="S5.T13.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.9.9.1.m1.1c">\pm</annotation></semantics></math>2.5</td>
<td id="S5.T13.10.10.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.10.10.2.m1.1a"><mo id="S5.T13.10.10.2.m1.1.1" xref="S5.T13.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.10.10.2.m1.1b"><csymbol cd="latexml" id="S5.T13.10.10.2.m1.1.1.cmml" xref="S5.T13.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.10.10.2.m1.1c">\pm</annotation></semantics></math>1.4</td>
<td id="S5.T13.11.11.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.11.11.3.m1.1a"><mo id="S5.T13.11.11.3.m1.1.1" xref="S5.T13.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.11.11.3.m1.1b"><csymbol cd="latexml" id="S5.T13.11.11.3.m1.1.1.cmml" xref="S5.T13.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.11.11.3.m1.1c">\pm</annotation></semantics></math>6.4</td>
<td id="S5.T13.12.12.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T13.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.12.12.4.m1.1a"><mo id="S5.T13.12.12.4.m1.1.1" xref="S5.T13.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.12.12.4.m1.1b"><csymbol cd="latexml" id="S5.T13.12.12.4.m1.1.1.cmml" xref="S5.T13.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.12.12.4.m1.1c">\pm</annotation></semantics></math>5.0</td>
</tr>
<tr id="S5.T13.16.22.4" class="ltx_tr">
<th id="S5.T13.16.22.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T13.16.22.4.1.1" class="ltx_text">Q4</span></th>
<td id="S5.T13.16.22.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.7%</td>
<td id="S5.T13.16.22.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.2%</td>
<td id="S5.T13.16.22.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.9%</td>
<td id="S5.T13.16.22.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T13.16.22.4.5.1" class="ltx_text ltx_font_bold">65.8%</span></td>
</tr>
<tr id="S5.T13.16.16" class="ltx_tr">
<td id="S5.T13.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.13.13.1.m1.1a"><mo id="S5.T13.13.13.1.m1.1.1" xref="S5.T13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T13.13.13.1.m1.1.1.cmml" xref="S5.T13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.13.13.1.m1.1c">\pm</annotation></semantics></math>4.0</td>
<td id="S5.T13.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T13.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.14.14.2.m1.1a"><mo id="S5.T13.14.14.2.m1.1.1" xref="S5.T13.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T13.14.14.2.m1.1.1.cmml" xref="S5.T13.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.14.14.2.m1.1c">\pm</annotation></semantics></math>0.5</td>
<td id="S5.T13.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T13.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.15.15.3.m1.1a"><mo id="S5.T13.15.15.3.m1.1.1" xref="S5.T13.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T13.15.15.3.m1.1.1.cmml" xref="S5.T13.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.15.15.3.m1.1c">\pm</annotation></semantics></math>5.4</td>
<td id="S5.T13.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T13.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T13.16.16.4.m1.1a"><mo id="S5.T13.16.16.4.m1.1.1" xref="S5.T13.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T13.16.16.4.m1.1b"><csymbol cd="latexml" id="S5.T13.16.16.4.m1.1.1.cmml" xref="S5.T13.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T13.16.16.4.m1.1c">\pm</annotation></semantics></math>5.9</td>
</tr>
</tbody>
</table>
</figure>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This article proposed three new datasets focused on audio, lyrics, and bimodal audio-lyrics MER. For each, both a complete and balanced variation are available. Two TVT splits were created and released alongside these datasets to enable fast experimentation and guarantee uniformity for all research works that employ them.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">To validate the proposed datasets and data splits, we performed experiments using state-of-the-art classical approaches (based on handcrafted features and standard ML algorithms) and DL methodologies (either learning relevant features or extracting alternative representations for classification).</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">From the obtained results, we conclude that the proposed datasets (and the related semi-automatic creation protocol) and TVT data splits are viable for MER benchmarking. In addition, the methods employed provide a solid baseline for comparison with future works using the MERGE datasets.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">This responds to a critical need of this research area, in particular, the bimodal dataset, which is the main contribution of this study. To the best of our knowledge, this is the largest publicly available MER bimodal dataset. In this respect, the approaches employing the bimodal dataset outperformed audio-only and lyrics-only strategies, further confirming the importance of leveraging audio and lyrics information to resolve ambiguity.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Moreover, the proposed data splits, especially the 70-15-15 strategy, are well suited for optimizing and quickly validating MER systems.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Additionally, the proposed datasets are designed for various research purposes. In addition to emotion quadrant annotations, the datasets also include metadata like genre, artist, album, year, and complete emotion tags. These features could benefit a wide range of MIR research and advanced MER tasks, including multi-label emotion classification.</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p">Due to the current dataset sizes, the CNN-based methods used in this work have not yet fully utilized the potential of deep learning. Although current DL methodologies in the literature open up many exciting research paths, the need for extensive data is still an issue that needs to be addressed. In this respect, a positive sign is the fact that the best overall result was obtained with a DL approach (using the 70-15-15 TVT split on the bimodal dataset). Moreover, a preliminary study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> shows the promise of hybrid approaches. The combination of handcrafted features with deep neural networks outperformed traditional feature engineering and machine learning methods. Therefore, despite its (still) limited size, the MERGE dataset is a step toward unlocking the potential of deep learning solutions for MER.</p>
</div>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">The methods employed in this work aimed to establish a baseline for benchmarking future work. As such, there is plenty of room for improvement, e.g., exploiting the potential of hybrid feature engineering and deep learning approaches, advancing research on new emotionally relevant features (particularly for musical expressivity, texture, and form <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>), or novel deep learning architectures.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is funded by FCT - Foundation for Science and Technology, I.P., within the scope of the projects: MERGE - DOI: 10.54499/PTDC/CCI-COM/3171/2021 financed with national funds (PIDDAC) via the Portuguese State Budget; and project CISUC - UID/CEC/00326/2020 with funds from the European Social Fund, through the Regional Operational Program Centro 2020. Renato Panda was supported by Ci2 - FCT UIDP/05567/2020.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Feng, Y. Zhuang, and Y. Pan, “Music information retrieval by detecting mood
via computational media aesthetics,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings IEEE/WIC
International Conference on Web Intelligence (WI 2003)</em>.   Halifax, NS, Canada: IEEE Computer Society, Oct.
2003, pp. 235–241.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. Orjesek, R. Jarina, and M. Chmulik, “End-to-end music emotion variation
detection using iteratively reconstructed deep features,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Multimedia
Tools and Applications</em>, vol. 81, no. 4, pp. 5017–5031, Feb. 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. Yang and W. Lee, “Disambiguating Music Emotion Using Software Agents,”
in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th Int. Society for Music Information
Retrieval Conf. (ISMIR)</em>, Barcelona, Spain, 2003.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Laurier, “Automatic Classification of Musical Mood by Content-Based
Analysis,” PhD Thesis, Universitat Pompeu Fabra, 2011. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://mtg.upf.edu/node/2385</span>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Convolutional Recurrent Neural
Networks for Music Classification,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>.   New
Orleans, LA, USA: arXiv, 2017, pp. 2392–2396.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. Pyrovolakis, P. Tzouveli, and G. Stamou, “Multi-Modal Song Mood
Detection with Deep Learning,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 3, p.
1065, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ò. Celma, “Bridging the Music Semantic Gap,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Workshop on
Mastering the Gap: From Information Extraction to Semantic
Representation</em>, 2006.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Panda, R. Malheiro, and R. P. Paiva, “Novel Audio Features for Music
Emotion Recognition,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>,
vol. 11, no. 4, pp. 614–626, Oct. 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X. Hu, J. S. Downie, C. Laurier, M. Bay, and A. F. Ehmann, “The 2007 Mirex
Audio Mood Classification Task: Lessons Learned,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 9th International Society for Music Information Retrieval
Conference</em>, Drexel University, Philadelphia, Pennsylvania, USA, 2008, pp.
462–467.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Malheiro, R. Panda, P. Gomes, and R. P. Paiva, “Emotionally-Relevant
Features for Classification and Regression of Music Lyrics,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 9, no. 2, pp. 240–254,
Apr. 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Aljanaki, Y.-H. Yang, and M. Soleymani, “Developing a benchmark for
emotional analysis of music,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">PLOS ONE</em>, vol. 12, no. 3, p. e0173392,
Mar. 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Hu and J. Downie, “Exploring mood metadata: Relationships with genre,
artist and usage metadata.” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International
Conference on Music Information Retrieval, ISMIR 2007</em>, 01 2007, pp. 67–72.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. N. Juslin, “From everyday emotions to aesthetic emotions: Towards a
unified theory of musical emotions,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Physics of Life Reviews</em>,
vol. 10, no. 3, pp. 235–266, Sep. 2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. A. Russell, “A circumplex model of affect.” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Journal of Personality
and Social Psychology</em>, vol. 39, no. 6, pp. 1161–1178, Dec. 1980.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Hevner, “Experimental Studies of the Elements of Expression in
Music,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">The American Journal of Psychology</em>, vol. 48, no. 2, p.
246, Apr. 1936.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P. L. Louro, H. Redinho, R. Malheiro, R. P. Paiva, and R. Panda, “A
Comparison Study of Deep Learning Methodologies for Music Emotion
Recognition,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 24, no. 7, p. 2201, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, P. Richardson, J. Scott,
J. A. Speck, and D. Turnbull, “Music Emotion Recognition: A State of
the Art Review,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International
Society for Music Information Retrieval Conference</em>, Porto, Portugal,
2010, pp. 225–266.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. Krippendorff, <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Content Analysis: An Introduction to its Methodology</em>,
2nd ed.   Sage, Thousand Oaks, CA, 2004.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Hu, “Improving Music Mood Classification Using Lyrics, Audio and
Social Tags,” Ph.D. dissertation, University of Illinois, Urbana,
Illinois, 2010.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Bertin-Mahieux, D. P. W. Ellis, B. Whitman, and P. Lamere, “The Million
Song Dataset,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Society
for Music Information Retrieval Conference</em>, Miami, Florida, USA, 2011,
pp. 591–596.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Zentner, D. Grandjean, and K. R. Scherer, “Emotions evoked by the sound of
music: Characterization, classification, and measurement.”
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Emotion</em>, vol. 8, no. 4, pp. 494–521, 2008.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y.-H. Yang, Y.-C. Lin, Y.-F. Su, and H. H. Chen, “A Regression Approach to
Music Emotion Recognition,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech,
and Language Processing</em>, vol. 16, no. 2, pp. 448–457, Feb. 2008.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, “Evaluation of
Algorithms Using Games: The Case of Music Tagging,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Society for Music
Information Retrieval Conference</em>, Kobe, Japan, 2009, pp. 387–392.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Choi, G. Fazekas, and M. Sandler, “Automatic Tagging Using Deep
Convolutional Neural Networks,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Int.
Society for Music Information Retrieval Conf. (ISMIR)</em>, New York
City, United States, 2016, pp. 805–811.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Kim, K. Choi, M. Modrzejewski, and C. C. S. Liem, “The biased journey of
MSD_AUDIO.ZIP,” 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Çano and M. Morisio, “MoodyLyrics: A Sentiment Annotated Lyrics
Dataset,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 International Conference on
Intelligent Systems, Metaheuristics &amp; Swarm Intelligence</em>.   Hong Kong Hong Kong: ACM, Mar. 2017, pp.
118–124.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Chen, J. Xu, and T. Joachims, “Multi-space probabilistic sequence
modeling,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining</em>, 2013, p. 865–873.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet, “Semantic
Annotation and Retrieval of Music and Sound Effects,”
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 16,
no. 2, pp. 467–476, 2008.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
R. Delbouys, R. Hennequin, F. Piccoli, J. Royo-Letelier, and M. Moussallam,
“Music Mood Detection Based On Audio And Lyrics With Deep Neural Net,”
in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th International Society for Music
Information Retrieval Conference</em>, Paris, France, 2018, pp. 370–375.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Lee, J. Park, K. Kim, and J. Nam, “SampleCNN: End-to-End Deep
Convolutional Neural Networks Using Very Small Filters for Music
Classification,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 8, no. 1, p. 150, Jan.
2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Pons, O. Nieto, M. Prockup, E. Schmidt, A. Ehmann, and X. Serra,
“End-to-end learning for music audio tagging at scale,” in
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th International Society for Music
Information Retrieval Conference</em>, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Won, S. Chun, O. Nieto, and X. Serrc, “Data-Driven Harmonic Filters for
Audio Representation Learning,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>, Barcelona, Spain, May 2020, pp. 536–540.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Abdillah, I. Asror, and Y. Wibowo, “Emotion Classification of Song
Lyrics using Bidirectional LSTM Method with GloVe Word Representation
Weighting,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Jurnal RESTI (Rekayasa Sistem dan Teknologi Informasi)</em>,
vol. 4, pp. 723–729, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Agrawal, R. G. R. Shanker, and V. Alluri, “Transformer-based approach
towards music emotion recognition from lyrics,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of
Advances in Information Retrieval - 43rd European Conference on
IR Research</em>, vol. 2, 2021, pp. 167–175.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
P. M. F. Vale, “The Role of Artist and Genre on Music Emotion
Recognition,” Master’s thesis, Universidade Nova de Lisboa, Lisbon,
Portugal, Jul. 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. B. Warriner, V. Kuperman, and M. Brysbaert, “Norms of valence, arousal, and
dominance for 13,915 English lemmas,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Behavior Research Methods</em>,
vol. 45, no. 4, pp. 1191–1207, Dec. 2013.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A Robustly Optimized BERT
Pretraining Approach,” 2019.

</span>
</li>
</ul>
</section>
<figure id="Sx1.1" class="ltx_float biography">
<table id="Sx1.1.1" class="ltx_tabular">
<tr id="Sx1.1.1.1" class="ltx_tr">
<td id="Sx1.1.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/pedrolouro.jpg" id="Sx1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="Sx1.1.1.1.2" class="ltx_td">
<span id="Sx1.1.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.1.1.1.2.1.1" class="ltx_p"><span id="Sx1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Pedro Louro</span>  is a PhD Research Student at the University of Coimbra, where he also concluded is Masters degree, specializing in Intelligent Systems. He is a member of the Cognitive and Media Systems (CMS) research group at the Centre for Informatics and Systems of the University of Coimbra (CISUC). His main research interests include Music Information Retrieval (MIR), Music Emotion Recognition (MER), and Deep Learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.2" class="ltx_float biography">
<table id="Sx1.2.1" class="ltx_tabular">
<tr id="Sx1.2.1.1" class="ltx_tr">
<td id="Sx1.2.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/hugoredinho.jpg" id="Sx1.2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="Sx1.2.1.1.2" class="ltx_td">
<span id="Sx1.2.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.2.1.1.2.1.1" class="ltx_p"><span id="Sx1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Hugo Redinho</span>  is an MSc from the University of Coimbra, specializing in Intelligent Systems, where he also concluded his Bachelor’s degree in Informatics Engineering. He is a member of the Music Information Retrieval (MIR) research team at the Center for Informatics and Systems of the University of Coimbra (CISUC). His main research interests are related to Music Emotion Recognition (MER) and MIR.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.3" class="ltx_float biography">
<table id="Sx1.3.1" class="ltx_tabular">
<tr id="Sx1.3.1.1" class="ltx_tr">
<td id="Sx1.3.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/ricardosantos.jpg" id="Sx1.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="Sx1.3.1.1.2" class="ltx_td">
<span id="Sx1.3.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.3.1.1.2.1.1" class="ltx_p"><span id="Sx1.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Ricardo Santos</span>  is a PhD Research Student at the Centre for Informatics and Systems of the University of Coimbra (CISUC). His research interests include Music Emotion Recognition, Deep Learning, and Large Language Models. Santos received his Master’s in Computer Engineering from IPT/USP, Brazil.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.4" class="ltx_float biography">
<table id="Sx1.4.1" class="ltx_tabular">
<tr id="Sx1.4.1.1" class="ltx_tr">
<td id="Sx1.4.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/ricardomalheiro.jpg" id="Sx1.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="116" alt="[Uncaptioned image]"></td>
<td id="Sx1.4.1.1.2" class="ltx_td">
<span id="Sx1.4.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.4.1.1.2.1.1" class="ltx_p"><span id="Sx1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Ricardo Malheiro</span>  is a PhD from the University of Coimbra, where he also concluded his Master and Bachelor (Licenciatura - 5 years) degrees, respectively in Informatics Engineering and Mathematics. He is a Professor at the Polytechnic Institute of Leiria - School of Technology and Management. He is also a member of the Cognitive and Media Systems (CMS) research group at CISUC. His main research interests are Natural Language Processing, Detection of Emotions in Music Lyrics and Text, and Text/Data Mining.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.5" class="ltx_float biography">
<table id="Sx1.5.1" class="ltx_tabular">
<tr id="Sx1.5.1.1" class="ltx_tr">
<td id="Sx1.5.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/renatopanda.jpg" id="Sx1.5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="120" alt="[Uncaptioned image]"></td>
<td id="Sx1.5.1.1.2" class="ltx_td">
<span id="Sx1.5.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.5.1.1.2.1.1" class="ltx_p"><span id="Sx1.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Renato Panda</span>  is an Assistant Researcher at Ci2, Polytechnic Institute of Tomar, Portugal. His main research interests are Music Emotion Recognition (MER) and Music Information Retrieval (MIR), as well as Applied Machine Learning and Software Engineering. He earned his PhD in Informatics Engineering from the University of Coimbra in 2019. Since then, he has been a member of the Cognitive and Media Systems group at the Centre for Informatics and Systems of the University of Coimbra (CISUC), where he remains actively involved.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="Sx1.6" class="ltx_float biography">
<table id="Sx1.6.1" class="ltx_tabular">
<tr id="Sx1.6.1.1" class="ltx_tr">
<td id="Sx1.6.1.1.1" class="ltx_td"><img src="/html/2407.06060/assets/figs/authors/ruipedropaiva.jpg" id="Sx1.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="113" alt="[Uncaptioned image]"></td>
<td id="Sx1.6.1.1.2" class="ltx_td">
<span id="Sx1.6.1.1.2.1" class="ltx_inline-block">
<span id="Sx1.6.1.1.2.1.1" class="ltx_p"><span id="Sx1.6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Rui Pedro Paiva</span>  is a Professor at the Department of Informatics Engineering of the University of Coimbra, where he concluded his Doctoral, Master and Bachelor degrees in 2007, 1999 and 1996, respectively. He is also a member of the CMS group at CISUC. His main research interests are in the areas of MIR and Health Informatics. The common research hat is the study of feature engineering, machine learning, and signal processing to analyze musical and bio signals.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.06059" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.06060" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.06060">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.06060" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.06062" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 13:01:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
