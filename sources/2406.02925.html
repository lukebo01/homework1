<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.02925] Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition</title><meta property="og:description" content="Synthetic data is widely used in speech recognition due to the availability of text-to-speech models, which facilitate adapting models to previously unseen text domains. However, existing methods suffer in performance â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.02925">

<!--Generated on Sat Jul  6 00:32:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Task Arithmetic can Mitigate Synthetic-to-Real Gap in 
<br class="ltx_break">Automatic Speech Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hsuan Su<sup id="id8.8.id1" class="ltx_sup">â™¡</sup> â€ƒâ€ƒHua Farn<sup id="id9.9.id2" class="ltx_sup">â™¡</sup> â€ƒâ€ƒFan-Yun Sun<sup id="id10.10.id3" class="ltx_sup">â™¢</sup> â€ƒâ€ƒShang-Tse Chen<sup id="id11.11.id4" class="ltx_sup">â™¡</sup> â€ƒâ€ƒHung-yi Lee<sup id="id12.12.id5" class="ltx_sup">â™¡</sup>
<br class="ltx_break">
<br class="ltx_break"><sup id="id13.13.id6" class="ltx_sup">â™¡</sup>National Taiwan University â€ƒâ€ƒ<sup id="id14.14.id7" class="ltx_sup">â™¢</sup>Stanford University
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Synthetic data is widely used in speech recognition due to the availability of text-to-speech models, which facilitate adapting models to previously unseen text domains. However, existing methods suffer in performance when they fine-tune an automatic speech recognition (ASR) model on synthetic data as they suffer from the distributional shift commonly referred to as the synthetic-to-real gap. In this paper, we find that task arithmetic is effective at mitigating this gap. Our proposed method, <span id="id15.id1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector, shows an average improvement of 10.03% improvement in word error rate over baselines on the SLURP dataset. Additionally, we show that an average of <span id="id15.id1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors, when we have real speeches from multiple different domains, can further adapt the original ASR model to perform better on the target text domain.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.7" class="ltx_block ltx_align_bottom">
<p id="p1.7.8" class="ltx_p"><span id="p1.7.8.1" class="ltx_text ltx_font_bold">Task Arithmetic can Mitigate Synthetic-to-Real Gap in 
<br class="ltx_break">Automatic Speech Recognition</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.7.7" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.7.7.7" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.7.7.7.7" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.5.5.5.5.5" class="ltx_tr">
<span id="p1.5.5.5.5.5.5" class="ltx_td ltx_align_center"><span id="p1.5.5.5.5.5.5.5" class="ltx_text ltx_font_bold">
Hsuan Su<sup id="p1.5.5.5.5.5.5.5.1" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.1.1" class="ltx_text ltx_font_medium">â™¡</span></sup> â€ƒâ€ƒHua Farn<sup id="p1.5.5.5.5.5.5.5.2" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.2.1" class="ltx_text ltx_font_medium">â™¡</span></sup> â€ƒâ€ƒFan-Yun Sun<sup id="p1.5.5.5.5.5.5.5.3" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.3.1" class="ltx_text ltx_font_medium">â™¢</span></sup> â€ƒâ€ƒShang-Tse Chen<sup id="p1.5.5.5.5.5.5.5.4" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.4.1" class="ltx_text ltx_font_medium">â™¡</span></sup> â€ƒâ€ƒHung-yi Lee<sup id="p1.5.5.5.5.5.5.5.5" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.5.1" class="ltx_text ltx_font_medium">â™¡</span></sup></span></span></span>
<span id="p1.7.7.7.7.7" class="ltx_tr">
<span id="p1.7.7.7.7.7.2" class="ltx_td ltx_align_center"><sup id="p1.7.7.7.7.7.2.1" class="ltx_sup">â™¡</sup>National Taiwan University â€ƒâ€ƒ<sup id="p1.7.7.7.7.7.2.2" class="ltx_sup">â™¢</sup>Stanford University</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Existing automatic speech recognition (ASR) models have been found to lack generalizability towards domains unseen during training <cite class="ltx_cite ltx_citemacro_cite">Bartelds etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Radford etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Sundar etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. Existing works, when adapting an ASR model to a previously unseen domain, often rely on synthetic speech data <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2024</a>); Bataev etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>); Joshi and Singh (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>); Zheng etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>); Yuen etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>); Yang etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> due to its ease of generation and availability. However, this approach often leads to performance degradation due to acoustic mismatches such as intonations, background noise, speaker accents, and environmental sound differences between synthetic and real speechÂ <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2024</a>)</cite>. This distributional shift is often referred to as the synthetic-to-real gap. This paper tackles this problem, particularly when adapting an ASR model from a source domain with text and real speech data to a new target domain with only text data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Our idea is that paired synthetic speech and real speech data within a single domain can guide the adaptation of models trained on synthetic data to perform better on real-world data in a new domain. Inspired by the new paradigm of editing pre-trained neural networks by manipulating their weights<cite class="ltx_cite ltx_citemacro_cite">Sung etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>); Tam etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2024</a>)</cite>, we propose to bridge the synthetic-to-real gap using task vectorsÂ <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>); Bhardwaj etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>. A task vector is a representation that encodes the difference between two tasks, allowing models to perform arithmetic operations to transition from one task to another. In this paper, we show that we can apply simple arithmetic operations to bridge the synthetic-to-real gap.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div id="S1.F1.1" class="ltx_inline-block ltx_transformed_outer" style="width:1170.8pt;height:774.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S1.F1.1.1" class="ltx_p"><span id="S1.F1.1.1.1" class="ltx_text"><img src="/html/2406.02925/assets/pics/motivation.drawio.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="1620" height="1070" alt="Refer to caption"></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.5.1" class="ltx_text ltx_font_bold">Overview of the <span id="S1.F1.5.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> Task Vector Approach.</span> The pre-trained model is fine-tuned on source domain synthetic and real speech data, separately. The difference between their parameters forms the <span id="S1.F1.6.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. The <span id="S1.F1.7.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector is then added to a model fine-tuned on target synthetic data to overcome the synthetic-to-real gap.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Taking inspiration fromÂ <cite class="ltx_cite ltx_citemacro_citet">Ilharco etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector for synthetic-to-real adaptation in ASR. FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of the <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector approach. The top row illustrates the process of fine-tuning models on synthetic and real speech data separately and then deriving the <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector from the differences in their parameters. The bottom row demonstrates the application of this vector to a model fine-tuned on synthetic target domain data, resulting in an adapted model with improved performance by incorporating the acoustic characteristics of real speech.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Empirically, we conduct comprehensive experiments and ablation studies to demonstrate the effectiveness of our approach. Applying the SYN2REAL task vector results in an average improvement of 10.03% in word error rate (WER) for unseen target domains compared to the model before applying our method. Cosine similarity analysis of task vectors generated by different TTS systems confirms that SYN2REAL vectors effectively capture domain-specific acoustic information.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ASR Text-only Domain Adaptation</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">In the context of automatic speech recognition (ASR), "text-only" domain adaptation typically refers to scenarios where the target domain only provides text data for training or fine-tuning the models. Previous work has explored internal language models adaptation that finetune language models in ene-to-end ASR models with CTC loss to improve the generalizability <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); Sato etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>); Vuong etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">The other direction is to adapt ASR models with synthetic speech. <cite class="ltx_cite ltx_citemacro_citet">Zheng etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> develop a method that provides synthetic audio for out-of-vocabulary (OOV) words to boost recognition accuracy. <cite class="ltx_cite ltx_citemacro_citet">Yang etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> works on personalize ASR with synthetic speech. <cite class="ltx_cite ltx_citemacro_citet">Bataev etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> focuses on developing a mel-spectrogram generator to improve ASR models.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task Arithmetic</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The concept of task vector is introduced inÂ <cite class="ltx_cite ltx_citemacro_citet">Ilharco etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. Task vectors are created by subtracting the weights of a fine-tuned model from those of its corresponding pre-trained model.
Different task vectors derived from the same pre-trained models can then be adjusted and combined through these simple arithmetic operations such as addition and subtraction to achieve multi-task learningÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> and task forgettingÂ <cite class="ltx_cite ltx_citemacro_cite">Daheim etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Recently, task vectors have shown promise in natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>); Daheim etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>); Bhardwaj etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>); Zhang etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Daheim etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2023</a></cite> used a task vector from a negatively fine-tuned model to mitigate hallucinations. <cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> proposed combining parameter-efficient fine-tuning (PEFT) modules <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Liu etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> arithmetically. <cite class="ltx_cite ltx_citemacro_citet">Huang etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite> obtained the Chat Vector by subtracting the chat version of Llama 2 <cite class="ltx_cite ltx_citemacro_cite">Touvron etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> from its pre-trained version, enhancing dialogue capabilities and safety. <cite class="ltx_cite ltx_citemacro_citet">Bhardwaj etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite> introduced RESTA, adding a safety vector to re-align safety for models fine-tuned on downstream tasks. The application of task vectors is relatively underexplored in ASR.<cite class="ltx_cite ltx_citemacro_citet">Ramesh etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite> applied task arithmetic to ASR models and introduced a "task analogy" formulation, improving performance on low-resource tasks using models trained on high-resource tasks. UnlikeÂ <cite class="ltx_cite ltx_citemacro_citet">Ramesh etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>, we focus on using task vector to mitigate the distributional shift between real and synthetic data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we aim to adapt ASR models from source domains with real speech and text data to a new domain with only text data. More specifically, we adapt ASR models using data synthesized from off-the-shelf text-to-speech (TTS) models.
However, ASR models often perform poorly due to acoustic differences between synthetic data generated by TTS and real speech data. To overcome this limitation, we introduce the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector, a novel approach that bridges the gap between the acoustic characteristics of synthetic and real speech data.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">We assume a problem setting in which we have two domains: a source domain <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ·</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">D_{s}</annotation></semantics></math> and a target domain <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="D_{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ·</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">D_{t}</annotation></semantics></math>.
The source domain <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ·</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">D_{s}</annotation></semantics></math> consists of paired text and speech samples, denoted as <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="T_{s}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘‡</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">T_{s}</annotation></semantics></math> and <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="S_{s}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ‘†</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">S_{s}</annotation></semantics></math>, respectively. The target domain <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="D_{t}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ·</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">D_{t}</annotation></semantics></math> contains only text data, denoted as <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="T_{t}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘‡</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">T_{t}</annotation></semantics></math>. This problem setting is common as it is easy to generate synthetic text data, whereas collecting paired real speech data is labor-intensive.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> Task Vector</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To adapt ASR models to a previously unseen domain, we employ a common methodology <cite class="ltx_cite ltx_citemacro_cite">Joshi and Singh (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> that utilizes synthetic data generated from the target text <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="T_{t}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">T_{t}</annotation></semantics></math> for model adaptation.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.1" class="ltx_inline-block ltx_transformed_outer" style="width:513.1pt;height:316.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.F2.1.1" class="ltx_p"><span id="S3.F2.1.1.1" class="ltx_text"><img src="/html/2406.02925/assets/x1.png" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="547" height="336" alt="Refer to caption"></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.4.1" class="ltx_text ltx_font_bold">Framework for <span id="S3.F2.4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector in Domain Adaptation for ASR.</span> The framework illustrates the process of creating the <span id="S3.F2.5.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector by subtracting the parameter differences between a model fine-tuned on synthetic speech (Source Synthetic) and a model fine-tuned on real speech (Source Real) from pretrained ASR (PASR). This task vector is then applied to the target synthetic domain (Target Synthetic) to improve ASR performance by bridging the gap between synthetic and real speech data.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">Previous work in task arithmetic has demonstrated that vectors can encode distinct capabilities, such as language or domain-specific features. We hypothesize that the differences in acoustic properties between real and synthetic speech are also learnable and can be isolated through parameter arithmetic. Specifically, we assume that we have models fine-tuned on real and synthetic data from the source domain, denoted as <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\theta_{real}^{S}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msubsup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.3.2" xref="S3.SS2.p2.1.m1.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.2.3.1" xref="S3.SS2.p2.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.1.m1.1.1.2.3.3" xref="S3.SS2.p2.1.m1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.2.3.1a" xref="S3.SS2.p2.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.1.m1.1.1.2.3.4" xref="S3.SS2.p2.1.m1.1.1.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.2.3.1b" xref="S3.SS2.p2.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.1.m1.1.1.2.3.5" xref="S3.SS2.p2.1.m1.1.1.2.3.5.cmml">l</mi></mrow><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">S</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3"><times id="S3.SS2.p2.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3.3">ğ‘’</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3.4">ğ‘</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.5.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3.5">ğ‘™</ci></apply></apply><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\theta_{real}^{S}</annotation></semantics></math> and <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\theta_{syn}^{S}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msubsup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.3.2" xref="S3.SS2.p2.2.m2.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.1" xref="S3.SS2.p2.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.3" xref="S3.SS2.p2.2.m2.1.1.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.3.1a" xref="S3.SS2.p2.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3.4" xref="S3.SS2.p2.2.m2.1.1.2.3.4.cmml">n</mi></mrow><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">S</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3"><times id="S3.SS2.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.3">ğ‘¦</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.4.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\theta_{syn}^{S}</annotation></semantics></math> respectively.
The acoustic disparity between real and synthetic speech is quantified by subtracting the parameter sets of these models:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\tau=\theta_{real}^{S}-\theta_{syn}^{S}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">Ï„</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msubsup id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml">Î¸</mi><mrow id="S3.E1.m1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.3.2.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.2.3.2" xref="S3.E1.m1.1.1.3.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.2.3.3" xref="S3.E1.m1.1.1.3.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1a" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.2.3.4" xref="S3.E1.m1.1.1.3.2.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1b" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.2.3.5" xref="S3.E1.m1.1.1.3.2.2.3.5.cmml">l</mi></mrow><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">S</mi></msubsup><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">âˆ’</mo><msubsup id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">Î¸</mi><mrow id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2.3.2" xref="S3.E1.m1.1.1.3.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.2.3.1" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.2.3.3" xref="S3.E1.m1.1.1.3.3.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.2.3.1a" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.3.2.3.4" xref="S3.E1.m1.1.1.3.3.2.3.4.cmml">n</mi></mrow><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">S</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ğœ</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">superscript</csymbol><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">ğœƒ</ci><apply id="S3.E1.m1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3"><times id="S3.E1.m1.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2">ğ‘Ÿ</ci><ci id="S3.E1.m1.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3.3">ğ‘’</ci><ci id="S3.E1.m1.1.1.3.2.2.3.4.cmml" xref="S3.E1.m1.1.1.3.2.2.3.4">ğ‘</ci><ci id="S3.E1.m1.1.1.3.2.2.3.5.cmml" xref="S3.E1.m1.1.1.3.2.2.3.5">ğ‘™</ci></apply></apply><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">ğ‘†</ci></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">superscript</csymbol><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">ğœƒ</ci><apply id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3"><times id="S3.E1.m1.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2">ğ‘ </ci><ci id="S3.E1.m1.1.1.3.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.3">ğ‘¦</ci><ci id="S3.E1.m1.1.1.3.3.2.3.4.cmml" xref="S3.E1.m1.1.1.3.3.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\tau=\theta_{real}^{S}-\theta_{syn}^{S}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Once the <span id="S3.SS2.p3.2.1" class="ltx_text ltx_font_italic">SYN2REAL</span> vector <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\tau</annotation></semantics></math> is computed, we apply it to the model parameters fine-tuned on the synthetic data in the target domain<math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\theta_{syn}^{T}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msubsup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2.2" xref="S3.SS2.p3.2.m2.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.p3.2.m2.1.1.2.3" xref="S3.SS2.p3.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2.3.2" xref="S3.SS2.p3.2.m2.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.2.3.1" xref="S3.SS2.p3.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p3.2.m2.1.1.2.3.3" xref="S3.SS2.p3.2.m2.1.1.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.1.1.2.3.1a" xref="S3.SS2.p3.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p3.2.m2.1.1.2.3.4" xref="S3.SS2.p3.2.m2.1.1.2.3.4.cmml">n</mi></mrow><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.p3.2.m2.1.1.2.3.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3"><times id="S3.SS2.p3.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3.1"></times><ci id="S3.SS2.p3.2.m2.1.1.2.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.p3.2.m2.1.1.2.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3.3">ğ‘¦</ci><ci id="S3.SS2.p3.2.m2.1.1.2.3.4.cmml" xref="S3.SS2.p3.2.m2.1.1.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\theta_{syn}^{T}</annotation></semantics></math>, thereby enhancing its adaptation to the target domain:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\theta_{syn\_new}=\theta_{syn}^{T}+\lambda\tau" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">Î¸</mi><mrow id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.2.3.2" xref="S3.E2.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.2.3.3" xref="S3.E2.m1.1.1.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1a" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.2.3.4" xref="S3.E2.m1.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1b" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.E2.m1.1.1.2.3.5" xref="S3.E2.m1.1.1.2.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1c" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.2.3.6" xref="S3.E2.m1.1.1.2.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1d" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.2.3.7" xref="S3.E2.m1.1.1.2.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.2.3.1e" xref="S3.E2.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.2.3.8" xref="S3.E2.m1.1.1.2.3.8.cmml">w</mi></mrow></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><msubsup id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.cmml">Î¸</mi><mrow id="S3.E2.m1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.3.2.2.3.cmml"><mi id="S3.E2.m1.1.1.3.2.2.3.2" xref="S3.E2.m1.1.1.3.2.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.3.1" xref="S3.E2.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.3.2.2.3.3" xref="S3.E2.m1.1.1.3.2.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.2.3.1a" xref="S3.E2.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.3.2.2.3.4" xref="S3.E2.m1.1.1.3.2.2.3.4.cmml">n</mi></mrow><mi id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml">T</mi></msubsup><mo id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">Ï„</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ğœƒ</ci><apply id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"><times id="S3.E2.m1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.2.3.1"></times><ci id="S3.E2.m1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.2.3.2">ğ‘ </ci><ci id="S3.E2.m1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.2.3.3">ğ‘¦</ci><ci id="S3.E2.m1.1.1.2.3.4.cmml" xref="S3.E2.m1.1.1.2.3.4">ğ‘›</ci><ci id="S3.E2.m1.1.1.2.3.5.cmml" xref="S3.E2.m1.1.1.2.3.5">_</ci><ci id="S3.E2.m1.1.1.2.3.6.cmml" xref="S3.E2.m1.1.1.2.3.6">ğ‘›</ci><ci id="S3.E2.m1.1.1.2.3.7.cmml" xref="S3.E2.m1.1.1.2.3.7">ğ‘’</ci><ci id="S3.E2.m1.1.1.2.3.8.cmml" xref="S3.E2.m1.1.1.2.3.8">ğ‘¤</ci></apply></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><plus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2">superscript</csymbol><apply id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2">ğœƒ</ci><apply id="S3.E2.m1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.3"><times id="S3.E2.m1.1.1.3.2.2.3.1.cmml" xref="S3.E2.m1.1.1.3.2.2.3.1"></times><ci id="S3.E2.m1.1.1.3.2.2.3.2.cmml" xref="S3.E2.m1.1.1.3.2.2.3.2">ğ‘ </ci><ci id="S3.E2.m1.1.1.3.2.2.3.3.cmml" xref="S3.E2.m1.1.1.3.2.2.3.3">ğ‘¦</ci><ci id="S3.E2.m1.1.1.3.2.2.3.4.cmml" xref="S3.E2.m1.1.1.3.2.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3">ğ‘‡</ci></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">ğœ†</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">ğœ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\theta_{syn\_new}=\theta_{syn}^{T}+\lambda\tau</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.3" class="ltx_p">Where <math id="S3.SS2.p3.3.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p3.3.m1.1a"><mi id="S3.SS2.p3.3.m1.1.1" xref="S3.SS2.p3.3.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m1.1b"><ci id="S3.SS2.p3.3.m1.1.1.cmml" xref="S3.SS2.p3.3.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m1.1c">\lambda</annotation></semantics></math> is the scaling factor of <span id="S3.SS2.p3.3.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">This adjusted model, <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\theta_{syn\_new}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">Î¸</mi><mrow id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1a" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.4" xref="S3.SS2.p4.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1b" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p4.1.m1.1.1.3.5" xref="S3.SS2.p4.1.m1.1.1.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1c" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.6" xref="S3.SS2.p4.1.m1.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1d" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.7" xref="S3.SS2.p4.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.1.m1.1.1.3.1e" xref="S3.SS2.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.1.m1.1.1.3.8" xref="S3.SS2.p4.1.m1.1.1.3.8.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğœƒ</ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><times id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3.1"></times><ci id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2">ğ‘ </ci><ci id="S3.SS2.p4.1.m1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3">ğ‘¦</ci><ci id="S3.SS2.p4.1.m1.1.1.3.4.cmml" xref="S3.SS2.p4.1.m1.1.1.3.4">ğ‘›</ci><ci id="S3.SS2.p4.1.m1.1.1.3.5.cmml" xref="S3.SS2.p4.1.m1.1.1.3.5">_</ci><ci id="S3.SS2.p4.1.m1.1.1.3.6.cmml" xref="S3.SS2.p4.1.m1.1.1.3.6">ğ‘›</ci><ci id="S3.SS2.p4.1.m1.1.1.3.7.cmml" xref="S3.SS2.p4.1.m1.1.1.3.7">ğ‘’</ci><ci id="S3.SS2.p4.1.m1.1.1.3.8.cmml" xref="S3.SS2.p4.1.m1.1.1.3.8">ğ‘¤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\theta_{syn\_new}</annotation></semantics></math>, is expected to perform more robustly in the target domain as it incorporates the acoustic characteristics of real speech, making it better suited for practical ASR tasks where real speech is present.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">
<span id="S3.SS2.SSS0.Px1.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> </h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">In the previous discussion, we assume no access to domain labels such as â€™emailâ€™ or â€™musicâ€™ from the source domain to emulate real-world situations better. All real speech data falls in the source domain.
In scenarios where we have access to data of multiple domains, another approach is to create <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors for each domain separately and then combine these vectors. This method involves fine-tuning separate ASR models on each individual source domain to obtain domain-specific <span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors, which are then averaged to form a comprehensive <span id="S3.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.2" class="ltx_p">For each domain <math id="S3.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.1c">i</annotation></semantics></math> in source domain <math id="S3.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.2.m2.1a"><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.2.m2.1c">S</annotation></semantics></math>. The task vector can be defined as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\tau_{i}=\theta_{real}^{S_{i}}-\theta_{syn}^{S_{i}}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">Ï„</mi><mi id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msubsup id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.cmml">Î¸</mi><mrow id="S3.E3.m1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.3.2.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.2.3.2" xref="S3.E3.m1.1.1.3.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.2.3.1" xref="S3.E3.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.3.2.2.3.3" xref="S3.E3.m1.1.1.3.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.2.3.1a" xref="S3.E3.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.3.2.2.3.4" xref="S3.E3.m1.1.1.3.2.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.2.3.1b" xref="S3.E3.m1.1.1.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.3.2.2.3.5" xref="S3.E3.m1.1.1.3.2.2.3.5.cmml">l</mi></mrow><msub id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml">S</mi><mi id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml">i</mi></msub></msubsup><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">âˆ’</mo><msubsup id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml">Î¸</mi><mrow id="S3.E3.m1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.3.2" xref="S3.E3.m1.1.1.3.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.3.1" xref="S3.E3.m1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.3.3.2.3.3" xref="S3.E3.m1.1.1.3.3.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.3.1a" xref="S3.E3.m1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.3.3.2.3.4" xref="S3.E3.m1.1.1.3.3.2.3.4.cmml">n</mi></mrow><msub id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">S</mi><mi id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3.cmml">i</mi></msub></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">ğœ</ci><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><minus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></minus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">superscript</csymbol><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2">ğœƒ</ci><apply id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3"><times id="S3.E3.m1.1.1.3.2.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.2.3.1"></times><ci id="S3.E3.m1.1.1.3.2.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.2.3.2">ğ‘Ÿ</ci><ci id="S3.E3.m1.1.1.3.2.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3.3">ğ‘’</ci><ci id="S3.E3.m1.1.1.3.2.2.3.4.cmml" xref="S3.E3.m1.1.1.3.2.2.3.4">ğ‘</ci><ci id="S3.E3.m1.1.1.3.2.2.3.5.cmml" xref="S3.E3.m1.1.1.3.2.2.3.5">ğ‘™</ci></apply></apply><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">ğ‘†</ci><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">superscript</csymbol><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">ğœƒ</ci><apply id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3"><times id="S3.E3.m1.1.1.3.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2">ğ‘ </ci><ci id="S3.E3.m1.1.1.3.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3.3">ğ‘¦</ci><ci id="S3.E3.m1.1.1.3.3.2.3.4.cmml" xref="S3.E3.m1.1.1.3.3.2.3.4">ğ‘›</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">ğ‘†</ci><ci id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\tau_{i}=\theta_{real}^{S_{i}}-\theta_{syn}^{S_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px1.p2.7" class="ltx_p">Where <math id="S3.SS2.SSS0.Px1.p2.3.m1.1" class="ltx_Math" alttext="\theta_{real}^{S_{i}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.3.m1.1a"><msubsup id="S3.SS2.SSS0.Px1.p2.3.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.3" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1a" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.4" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1b" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.5" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.5.cmml">l</mi></mrow><msub id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.2.cmml">S</mi><mi id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.3.cmml">i</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.3.m1.1b"><apply id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3"><times id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.1"></times><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.3">ğ‘’</ci><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.4.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.4">ğ‘</ci><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.5.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.2.3.5">ğ‘™</ci></apply></apply><apply id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.2">ğ‘†</ci><ci id="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.3.m1.1c">\theta_{real}^{S_{i}}</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px1.p2.4.m2.1" class="ltx_Math" alttext="\theta_{syn}^{S_{i}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.4.m2.1a"><msubsup id="S3.SS2.SSS0.Px1.p2.4.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.3" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1a" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.4" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.4.cmml">n</mi></mrow><msub id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.2.cmml">S</mi><mi id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.3.cmml">i</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.4.m2.1b"><apply id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3"><times id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.1"></times><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.3">ğ‘¦</ci><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.4.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.2.3.4">ğ‘›</ci></apply></apply><apply id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.2">ğ‘†</ci><ci id="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m2.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.4.m2.1c">\theta_{syn}^{S_{i}}</annotation></semantics></math> represent the model parameters fine-tuned on real and synthetic data for domain <math id="S3.SS2.SSS0.Px1.p2.5.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.5.m3.1a"><mi id="S3.SS2.SSS0.Px1.p2.5.m3.1.1" xref="S3.SS2.SSS0.Px1.p2.5.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.5.m3.1b"><ci id="S3.SS2.SSS0.Px1.p2.5.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.5.m3.1c">i</annotation></semantics></math> respectively.
Once the vector <math id="S3.SS2.SSS0.Px1.p2.6.m4.1" class="ltx_Math" alttext="\tau_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.6.m4.1a"><msub id="S3.SS2.SSS0.Px1.p2.6.m4.1.1" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1.2.cmml">Ï„</mi><mi id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.6.m4.1b"><apply id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1.2">ğœ</ci><ci id="S3.SS2.SSS0.Px1.p2.6.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.6.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.6.m4.1c">\tau_{i}</annotation></semantics></math> is computed, we apply it to the model parameters fine-tuned on synthetic target domain data <math id="S3.SS2.SSS0.Px1.p2.7.m5.1" class="ltx_Math" alttext="\theta_{syn}^{T}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.7.m5.1a"><msubsup id="S3.SS2.SSS0.Px1.p2.7.m5.1.1" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.2" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.2.cmml">Î¸</mi><mrow id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.3" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1a" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.4" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.4.cmml">n</mi></mrow><mi id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.7.m5.1b"><apply id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.2">ğœƒ</ci><apply id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3"><times id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.1"></times><ci id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.2">ğ‘ </ci><ci id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.3">ğ‘¦</ci><ci id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.4.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.SS2.SSS0.Px1.p2.7.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m5.1.1.3">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.7.m5.1c">\theta_{syn}^{T}</annotation></semantics></math>, thereby enhancing its adaptation to the target domain:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\theta_{syn\_new}=\theta_{syn}^{T}+\frac{\lambda}{|S|}\sum_{i=0}^{|S|}\tau_{i}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.3" xref="S3.E4.m1.2.3.cmml"><msub id="S3.E4.m1.2.3.2" xref="S3.E4.m1.2.3.2.cmml"><mi id="S3.E4.m1.2.3.2.2" xref="S3.E4.m1.2.3.2.2.cmml">Î¸</mi><mrow id="S3.E4.m1.2.3.2.3" xref="S3.E4.m1.2.3.2.3.cmml"><mi id="S3.E4.m1.2.3.2.3.2" xref="S3.E4.m1.2.3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3.3" xref="S3.E4.m1.2.3.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1a" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3.4" xref="S3.E4.m1.2.3.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1b" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.E4.m1.2.3.2.3.5" xref="S3.E4.m1.2.3.2.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1c" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3.6" xref="S3.E4.m1.2.3.2.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1d" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3.7" xref="S3.E4.m1.2.3.2.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.2.3.1e" xref="S3.E4.m1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.2.3.8" xref="S3.E4.m1.2.3.2.3.8.cmml">w</mi></mrow></msub><mo id="S3.E4.m1.2.3.1" xref="S3.E4.m1.2.3.1.cmml">=</mo><mrow id="S3.E4.m1.2.3.3" xref="S3.E4.m1.2.3.3.cmml"><msubsup id="S3.E4.m1.2.3.3.2" xref="S3.E4.m1.2.3.3.2.cmml"><mi id="S3.E4.m1.2.3.3.2.2.2" xref="S3.E4.m1.2.3.3.2.2.2.cmml">Î¸</mi><mrow id="S3.E4.m1.2.3.3.2.2.3" xref="S3.E4.m1.2.3.3.2.2.3.cmml"><mi id="S3.E4.m1.2.3.3.2.2.3.2" xref="S3.E4.m1.2.3.3.2.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.3.2.2.3.1" xref="S3.E4.m1.2.3.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.3.2.2.3.3" xref="S3.E4.m1.2.3.3.2.2.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.3.2.2.3.1a" xref="S3.E4.m1.2.3.3.2.2.3.1.cmml">â€‹</mo><mi id="S3.E4.m1.2.3.3.2.2.3.4" xref="S3.E4.m1.2.3.3.2.2.3.4.cmml">n</mi></mrow><mi id="S3.E4.m1.2.3.3.2.3" xref="S3.E4.m1.2.3.3.2.3.cmml">T</mi></msubsup><mo id="S3.E4.m1.2.3.3.1" xref="S3.E4.m1.2.3.3.1.cmml">+</mo><mrow id="S3.E4.m1.2.3.3.3" xref="S3.E4.m1.2.3.3.3.cmml"><mfrac id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">Î»</mi><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.2.1.cmml">|</mo><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S3.E4.m1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.3.3.3.1" xref="S3.E4.m1.2.3.3.3.1.cmml">â€‹</mo><mrow id="S3.E4.m1.2.3.3.3.2" xref="S3.E4.m1.2.3.3.3.2.cmml"><munderover id="S3.E4.m1.2.3.3.3.2.1" xref="S3.E4.m1.2.3.3.3.2.1.cmml"><mo movablelimits="false" id="S3.E4.m1.2.3.3.3.2.1.2.2" xref="S3.E4.m1.2.3.3.3.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.2.3.3.3.2.1.2.3" xref="S3.E4.m1.2.3.3.3.2.1.2.3.cmml"><mi id="S3.E4.m1.2.3.3.3.2.1.2.3.2" xref="S3.E4.m1.2.3.3.3.2.1.2.3.2.cmml">i</mi><mo id="S3.E4.m1.2.3.3.3.2.1.2.3.1" xref="S3.E4.m1.2.3.3.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.2.3.3.3.2.1.2.3.3" xref="S3.E4.m1.2.3.3.3.2.1.2.3.3.cmml">0</mn></mrow><mrow id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.3.1" xref="S3.E4.m1.2.2.1.2.1.cmml">|</mo><mi id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml">S</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.3.2" xref="S3.E4.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><msub id="S3.E4.m1.2.3.3.3.2.2" xref="S3.E4.m1.2.3.3.3.2.2.cmml"><mi id="S3.E4.m1.2.3.3.3.2.2.2" xref="S3.E4.m1.2.3.3.3.2.2.2.cmml">Ï„</mi><mi id="S3.E4.m1.2.3.3.3.2.2.3" xref="S3.E4.m1.2.3.3.3.2.2.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.3.cmml" xref="S3.E4.m1.2.3"><eq id="S3.E4.m1.2.3.1.cmml" xref="S3.E4.m1.2.3.1"></eq><apply id="S3.E4.m1.2.3.2.cmml" xref="S3.E4.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.2.1.cmml" xref="S3.E4.m1.2.3.2">subscript</csymbol><ci id="S3.E4.m1.2.3.2.2.cmml" xref="S3.E4.m1.2.3.2.2">ğœƒ</ci><apply id="S3.E4.m1.2.3.2.3.cmml" xref="S3.E4.m1.2.3.2.3"><times id="S3.E4.m1.2.3.2.3.1.cmml" xref="S3.E4.m1.2.3.2.3.1"></times><ci id="S3.E4.m1.2.3.2.3.2.cmml" xref="S3.E4.m1.2.3.2.3.2">ğ‘ </ci><ci id="S3.E4.m1.2.3.2.3.3.cmml" xref="S3.E4.m1.2.3.2.3.3">ğ‘¦</ci><ci id="S3.E4.m1.2.3.2.3.4.cmml" xref="S3.E4.m1.2.3.2.3.4">ğ‘›</ci><ci id="S3.E4.m1.2.3.2.3.5.cmml" xref="S3.E4.m1.2.3.2.3.5">_</ci><ci id="S3.E4.m1.2.3.2.3.6.cmml" xref="S3.E4.m1.2.3.2.3.6">ğ‘›</ci><ci id="S3.E4.m1.2.3.2.3.7.cmml" xref="S3.E4.m1.2.3.2.3.7">ğ‘’</ci><ci id="S3.E4.m1.2.3.2.3.8.cmml" xref="S3.E4.m1.2.3.2.3.8">ğ‘¤</ci></apply></apply><apply id="S3.E4.m1.2.3.3.cmml" xref="S3.E4.m1.2.3.3"><plus id="S3.E4.m1.2.3.3.1.cmml" xref="S3.E4.m1.2.3.3.1"></plus><apply id="S3.E4.m1.2.3.3.2.cmml" xref="S3.E4.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.3.2.1.cmml" xref="S3.E4.m1.2.3.3.2">superscript</csymbol><apply id="S3.E4.m1.2.3.3.2.2.cmml" xref="S3.E4.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.3.2.2.1.cmml" xref="S3.E4.m1.2.3.3.2">subscript</csymbol><ci id="S3.E4.m1.2.3.3.2.2.2.cmml" xref="S3.E4.m1.2.3.3.2.2.2">ğœƒ</ci><apply id="S3.E4.m1.2.3.3.2.2.3.cmml" xref="S3.E4.m1.2.3.3.2.2.3"><times id="S3.E4.m1.2.3.3.2.2.3.1.cmml" xref="S3.E4.m1.2.3.3.2.2.3.1"></times><ci id="S3.E4.m1.2.3.3.2.2.3.2.cmml" xref="S3.E4.m1.2.3.3.2.2.3.2">ğ‘ </ci><ci id="S3.E4.m1.2.3.3.2.2.3.3.cmml" xref="S3.E4.m1.2.3.3.2.2.3.3">ğ‘¦</ci><ci id="S3.E4.m1.2.3.3.2.2.3.4.cmml" xref="S3.E4.m1.2.3.3.2.2.3.4">ğ‘›</ci></apply></apply><ci id="S3.E4.m1.2.3.3.2.3.cmml" xref="S3.E4.m1.2.3.3.2.3">ğ‘‡</ci></apply><apply id="S3.E4.m1.2.3.3.3.cmml" xref="S3.E4.m1.2.3.3.3"><times id="S3.E4.m1.2.3.3.3.1.cmml" xref="S3.E4.m1.2.3.3.3.1"></times><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><divide id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1"></divide><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">ğœ†</ci><apply id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.3"><abs id="S3.E4.m1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.3.1"></abs><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ğ‘†</ci></apply></apply><apply id="S3.E4.m1.2.3.3.3.2.cmml" xref="S3.E4.m1.2.3.3.3.2"><apply id="S3.E4.m1.2.3.3.3.2.1.cmml" xref="S3.E4.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.3.3.2.1.1.cmml" xref="S3.E4.m1.2.3.3.3.2.1">superscript</csymbol><apply id="S3.E4.m1.2.3.3.3.2.1.2.cmml" xref="S3.E4.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.3.3.2.1.2.1.cmml" xref="S3.E4.m1.2.3.3.3.2.1">subscript</csymbol><sum id="S3.E4.m1.2.3.3.3.2.1.2.2.cmml" xref="S3.E4.m1.2.3.3.3.2.1.2.2"></sum><apply id="S3.E4.m1.2.3.3.3.2.1.2.3.cmml" xref="S3.E4.m1.2.3.3.3.2.1.2.3"><eq id="S3.E4.m1.2.3.3.3.2.1.2.3.1.cmml" xref="S3.E4.m1.2.3.3.3.2.1.2.3.1"></eq><ci id="S3.E4.m1.2.3.3.3.2.1.2.3.2.cmml" xref="S3.E4.m1.2.3.3.3.2.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E4.m1.2.3.3.3.2.1.2.3.3.cmml" xref="S3.E4.m1.2.3.3.3.2.1.2.3.3">0</cn></apply></apply><apply id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.3"><abs id="S3.E4.m1.2.2.1.2.1.cmml" xref="S3.E4.m1.2.2.1.3.1"></abs><ci id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1">ğ‘†</ci></apply></apply><apply id="S3.E4.m1.2.3.3.3.2.2.cmml" xref="S3.E4.m1.2.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.3.3.2.2.1.cmml" xref="S3.E4.m1.2.3.3.3.2.2">subscript</csymbol><ci id="S3.E4.m1.2.3.3.3.2.2.2.cmml" xref="S3.E4.m1.2.3.3.3.2.2.2">ğœ</ci><ci id="S3.E4.m1.2.3.3.3.2.2.3.cmml" xref="S3.E4.m1.2.3.3.3.2.2.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\theta_{syn\_new}=\theta_{syn}^{T}+\frac{\lambda}{|S|}\sum_{i=0}^{|S|}\tau_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px1.p2.9" class="ltx_p">Where <math id="S3.SS2.SSS0.Px1.p2.8.m1.1" class="ltx_Math" alttext="|S|" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.8.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p2.8.m1.1.2.2" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p2.8.m1.1.2.2.1" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.2.1.1.cmml">|</mo><mi id="S3.SS2.SSS0.Px1.p2.8.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.1.cmml">S</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p2.8.m1.1.2.2.2" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.8.m1.1b"><apply id="S3.SS2.SSS0.Px1.p2.8.m1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.2.2"><abs id="S3.SS2.SSS0.Px1.p2.8.m1.1.2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.2.2.1"></abs><ci id="S3.SS2.SSS0.Px1.p2.8.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.8.m1.1c">|S|</annotation></semantics></math> is the number of source domains, and <math id="S3.SS2.SSS0.Px1.p2.9.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.9.m2.1a"><mi id="S3.SS2.SSS0.Px1.p2.9.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.9.m2.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.9.m2.1b"><ci id="S3.SS2.SSS0.Px1.p2.9.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.9.m2.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.9.m2.1c">\lambda</annotation></semantics></math> is the scaling factor for the task vector.

</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setups</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We design our experiments to answer the following questions: <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Q1: </span>What is the efficacy of <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector?, <span id="S4.p1.1.3" class="ltx_text ltx_font_bold">Q2: </span>How does <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector perform across different model sizes? <span id="S4.p1.1.5" class="ltx_text ltx_font_bold">Q3: </span>Is <span id="S4.p1.1.6" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector effective on ASR models other than Whisper?, <span id="S4.p1.1.7" class="ltx_text ltx_font_bold">Q4: </span>Can we form <span id="S4.p1.1.8" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors from other TTS models?, <span id="S4.p1.1.9" class="ltx_text ltx_font_bold">Q5: </span>What is the impact of the scaling factor <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\lambda?" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">?</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">ğœ†</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">?</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\lambda?</annotation></semantics></math> , <span id="S4.p1.1.10" class="ltx_text ltx_font_bold">Q6: </span>Do <span id="S4.p1.1.11" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors obtained with the same TTS have similar direction?</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To answer the questions above and to mimic real-world use cases, we first form a source domain ASR model by compiling synthetic and real speech data from various domains. We then adapt this source domain ASR model to the target domain using data synthesized by TTS models.
<span id="S4.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector is constructed by subtracting the weights of an ASR model fine-tuned on synthetic data from the weights of the same ASR model fine-tuned on real data, both using the same pre-trained model as the starting point. Our goal is to improve the performance of an ASR model on the target domain without using any real speech from the target domain.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:1029.4pt;height:114.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T1.1.1" class="ltx_p"><span id="S4.T1.1.1.1" class="ltx_text">
<span id="S4.T1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T1.1.1.1.1.2.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">WER</span></span>
<span id="S4.T1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_18"><span id="S4.T1.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Target Domains</span></span>
<span id="S4.T1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S4.T1.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Average</span></span></span>
<span id="S4.T1.1.1.1.1.3.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Methods</span></span>
<span id="S4.T1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.2.1" class="ltx_text ltx_font_bold">Alarm</span></span>
<span id="S4.T1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.3.1" class="ltx_text ltx_font_bold">Audio</span></span>
<span id="S4.T1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.4.1" class="ltx_text ltx_font_bold">Calendar</span></span>
<span id="S4.T1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.5.1" class="ltx_text ltx_font_bold">Cooking</span></span>
<span id="S4.T1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.6.1" class="ltx_text ltx_font_bold">Datetime</span></span>
<span id="S4.T1.1.1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.7.1" class="ltx_text ltx_font_bold">Email</span></span>
<span id="S4.T1.1.1.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.8.1" class="ltx_text ltx_font_bold">General</span></span>
<span id="S4.T1.1.1.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.9.1" class="ltx_text ltx_font_bold">IOT</span></span>
<span id="S4.T1.1.1.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.10.1" class="ltx_text ltx_font_bold">Lists</span></span>
<span id="S4.T1.1.1.1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.11.1" class="ltx_text ltx_font_bold">Music</span></span>
<span id="S4.T1.1.1.1.1.3.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.12.1" class="ltx_text ltx_font_bold">News</span></span>
<span id="S4.T1.1.1.1.1.3.2.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.13.1" class="ltx_text ltx_font_bold">Play</span></span>
<span id="S4.T1.1.1.1.1.3.2.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.14.1" class="ltx_text ltx_font_bold">QA</span></span>
<span id="S4.T1.1.1.1.1.3.2.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.15.1" class="ltx_text ltx_font_bold">Recommendation</span></span>
<span id="S4.T1.1.1.1.1.3.2.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.16.1" class="ltx_text ltx_font_bold">Social</span></span>
<span id="S4.T1.1.1.1.1.3.2.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.17.1" class="ltx_text ltx_font_bold">Takeaway</span></span>
<span id="S4.T1.1.1.1.1.3.2.18" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.18.1" class="ltx_text ltx_font_bold">Transport</span></span>
<span id="S4.T1.1.1.1.1.3.2.19" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.3.2.19.1" class="ltx_text ltx_font_bold">Weather</span></span></span>
<span id="S4.T1.1.1.1.1.4.3" class="ltx_tr">
<span id="S4.T1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.1.1.1.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.1.4.3.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Target Synthetic ASR</span></span>
<span id="S4.T1.1.1.1.1.4.3.1.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.4.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Baseline)</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">16.13</span>
<span id="S4.T1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">14.69</span>
<span id="S4.T1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">22.88</span>
<span id="S4.T1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">14.26</span>
<span id="S4.T1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">47.16</span>
<span id="S4.T1.1.1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">16.23</span>
<span id="S4.T1.1.1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">27.16</span>
<span id="S4.T1.1.1.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">13.67</span>
<span id="S4.T1.1.1.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">15.49</span>
<span id="S4.T1.1.1.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t">23.51</span>
<span id="S4.T1.1.1.1.1.4.3.12" class="ltx_td ltx_align_center ltx_border_t">21.31</span>
<span id="S4.T1.1.1.1.1.4.3.13" class="ltx_td ltx_align_center ltx_border_t">21.61</span>
<span id="S4.T1.1.1.1.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t">24.04</span>
<span id="S4.T1.1.1.1.1.4.3.15" class="ltx_td ltx_align_center ltx_border_t">17.54</span>
<span id="S4.T1.1.1.1.1.4.3.16" class="ltx_td ltx_align_center ltx_border_t">29.57</span>
<span id="S4.T1.1.1.1.1.4.3.17" class="ltx_td ltx_align_center ltx_border_t">21.25</span>
<span id="S4.T1.1.1.1.1.4.3.18" class="ltx_td ltx_align_center ltx_border_t">18.91</span>
<span id="S4.T1.1.1.1.1.4.3.19" class="ltx_td ltx_align_center ltx_border_t">15.45</span>
<span id="S4.T1.1.1.1.1.4.3.20" class="ltx_td ltx_align_center ltx_border_t">21.16</span></span>
<span id="S4.T1.1.1.1.1.5.4" class="ltx_tr">
<span id="S4.T1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">+ <span id="S4.T1.1.1.1.1.5.4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span></span>
<span id="S4.T1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.2.1" class="ltx_text ltx_font_bold">15.65</span></span>
<span id="S4.T1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.3.1" class="ltx_text ltx_font_bold">13.68</span></span>
<span id="S4.T1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.4.1" class="ltx_text ltx_font_bold">22.64</span></span>
<span id="S4.T1.1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">14.36</span>
<span id="S4.T1.1.1.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.6.1" class="ltx_text ltx_font_bold">40.29</span></span>
<span id="S4.T1.1.1.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.7.1" class="ltx_text ltx_font_bold">16.15</span></span>
<span id="S4.T1.1.1.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.8.1" class="ltx_text ltx_font_bold">16.87</span></span>
<span id="S4.T1.1.1.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.9.1" class="ltx_text ltx_font_bold">12.49</span></span>
<span id="S4.T1.1.1.1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.10.1" class="ltx_text ltx_font_bold">15.22</span></span>
<span id="S4.T1.1.1.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.11.1" class="ltx_text ltx_font_bold">17.03</span></span>
<span id="S4.T1.1.1.1.1.5.4.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.12.1" class="ltx_text ltx_font_bold">21.25</span></span>
<span id="S4.T1.1.1.1.1.5.4.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.13.1" class="ltx_text ltx_font_bold">20.77</span></span>
<span id="S4.T1.1.1.1.1.5.4.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.14.1" class="ltx_text ltx_font_bold">23.88</span></span>
<span id="S4.T1.1.1.1.1.5.4.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.15.1" class="ltx_text ltx_font_bold">15.19</span></span>
<span id="S4.T1.1.1.1.1.5.4.16" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.16.1" class="ltx_text ltx_font_bold">21.87</span></span>
<span id="S4.T1.1.1.1.1.5.4.17" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.17.1" class="ltx_text ltx_font_bold">18.03</span></span>
<span id="S4.T1.1.1.1.1.5.4.18" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.18.1" class="ltx_text ltx_font_bold">16.90</span></span>
<span id="S4.T1.1.1.1.1.5.4.19" class="ltx_td ltx_align_center ltx_border_t">20.38</span>
<span id="S4.T1.1.1.1.1.5.4.20" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.5.4.20.1" class="ltx_text ltx_font_bold">19.04</span></span></span>
<span id="S4.T1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_bb">Relative WER (%)<math id="S4.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S4.T1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">2.95%</span></span>
<span id="S4.T1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">6.87 %</span></span>
<span id="S4.T1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">1.03 %</span></span>
<span id="S4.T1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb">-0.70 %</span>
<span id="S4.T1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">14.58 %</span></span>
<span id="S4.T1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">0.50 %</span></span>
<span id="S4.T1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">37.89 %</span></span>
<span id="S4.T1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">8.58 %</span></span>
<span id="S4.T1.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.10.1" class="ltx_text ltx_font_bold">1.74 %</span></span>
<span id="S4.T1.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.11.1" class="ltx_text ltx_font_bold">27.57 %</span></span>
<span id="S4.T1.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.12.1" class="ltx_text ltx_font_bold">0.28 %</span></span>
<span id="S4.T1.1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.13.1" class="ltx_text ltx_font_bold">3.88 %</span></span>
<span id="S4.T1.1.1.1.1.1.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.14.1" class="ltx_text ltx_font_bold">0.64 %</span></span>
<span id="S4.T1.1.1.1.1.1.15" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.15.1" class="ltx_text ltx_font_bold">13.42 %</span></span>
<span id="S4.T1.1.1.1.1.1.16" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.16.1" class="ltx_text ltx_font_bold">26.04 %</span></span>
<span id="S4.T1.1.1.1.1.1.17" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.17.1" class="ltx_text ltx_font_bold">15.14 %</span></span>
<span id="S4.T1.1.1.1.1.1.18" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.18.1" class="ltx_text ltx_font_bold">10.65 %</span></span>
<span id="S4.T1.1.1.1.1.1.19" class="ltx_td ltx_align_center ltx_border_bb">-31.91%</span>
<span id="S4.T1.1.1.1.1.1.20" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.1.1.1.20.1" class="ltx_text ltx_font_bold">10.03%</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.5.1" class="ltx_text ltx_font_bold">Word Error Rate (WER) Performance Across Various Target Domains.</span>
Comparison of the baseline Whisper model and the model enhanced with the <span id="S4.T1.6.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector generated by BARK. The <span id="S4.T1.7.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector shows an average WER reduction of 10.03% across various target domains. Target Synthetic ASR refers to the baseline that is finetuned on 17 domains (excluding the target domain) real+synthetic data followed by synthetic data from the target domains in the SLURP dataset.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">SLURPÂ <cite class="ltx_cite ltx_citemacro_cite">Bastianelli etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> is a spoken language understanding dataset containing 16521 utterances of human commands towards a virtual agent, based on 200 pre-defined prompts such as â€œHow would you ask for the time.â€ The utterances are categorized into 18 domains (e.g., email, cooking, etc.). In each of our experiments, we select one of these domains as the target domain and combine the remaining 17 domains to form the source domain.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Text-to-Speech (TTS) Models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In our experiments, we prepare synthetic speech using two off-the-shelf TTS models for text from the target domains.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">BARK</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">BARK<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/suno-ai/bark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/suno-ai/bark</a></span></span></span> is a transformer-based <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> autoregressive model, it is pretrained with similar architecture as AudioLM <cite class="ltx_cite ltx_citemacro_cite">Borsos etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> and Vall-E <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. The input of BARK contains prompts, transcription, and users. In our experiments, we do not specify the speaker for BARK.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Speech T5</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">Speech T5Â <cite class="ltx_cite ltx_citemacro_cite">Ao etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> is a unified model framework that employs encoder-decoder pre-training for self-supervised speech/text representation learning.
In our experiments, we randomly sample 5 speakers from 7931 speakers.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>ASR Models</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Whisper</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> is an encoder-decoder Transformer-based <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> model that supervised finetuned on 680,000 hours of labeled audio data.
All experiments are conducted using the Whisper small model, except for the ablation study, where we experiment with models of different sizes, including the base and tiny models, to validate our method.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Wav2Vec2-Conformer</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">Baevski etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> is a framework for self-supervised learning of speech representations that masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations.
Wav2Vec2-Conformer <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> (referred to as Wav2vec2 in the experiments) follows the same architecture as Wav2Vec2, but replaces the Attention-block with a Conformer-block <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> is the conformer <cite class="ltx_cite ltx_citemacro_cite">Gulati etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. We use the large checkpoint<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://huggingface.co/facebook/wav2vec2-conformer-rope-large-960h-ft" title="" class="ltx_ref ltx_href">facebook/wav2vec2-conformer-rope-large-960h-ft</a></span></span></span> with 618M parameters with rotary position embeddings, pretrained and fine-tuned on 960 hours of Librispeech <cite class="ltx_cite ltx_citemacro_cite">Panayotov etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite> on 16kHz sampled speech audio to conduct experiments</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results &amp; Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Here, we discuss our results in relation to the questions we set out to answer.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>What is the efficacy of <span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector?</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To answer <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Q1</span>, we apply our method by comparing the word error rate (WER) across various target domains. We select one of these domains as the target domain and combine the remaining 17 domains to form the source domain
TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experimental Setups â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the WER results for both the baseline ASR model fine-tuned on synthetic speech data and the model enhanced with the <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The baseline model, fine-tuned solely on synthetic data, exhibits varying WERs across different target domains, with an average WER of 20.15. This performance highlights the challenge of adapting ASR models to real-world data when trained on synthetic speech, primarily due to acoustic mismatches.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">By applying the <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector, we observe a significant reduction in WER across most target domains. The <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span>-enhanced model achieves an average WER of 19.04, representing an average relative WER reduction of 10.03%. This improvement demonstrates the effectiveness of the <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector in bridging the gap between synthetic and real speech data, thus enhancing the modelâ€™s adaptability to diverse real-world scenarios.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">The <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector shows particularly notable improvements in domains such as â€™Musicâ€™ (27.57% reduction), â€™Takeawayâ€™ (15.14% reduction), and â€™Socialâ€™ (26.04% reduction). These results suggest that the task vector effectively captures domain-specific acoustic variations, enabling the ASR model to generalize better to unseen target domains. In the following experiments we select the four domains includes two highest improved domains (â€™Musicâ€™ &amp; â€™Socialâ€™), and the two lowest improved domains (â€™Weatherâ€™ &amp; â€™Cookingâ€™) to conduct the experiments.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">However, it is important to note that some domains, such as â€™Cookingâ€™ and â€™Weather,â€™ exhibit marginal improvements or slight degradation in WER. These variations indicate that while the <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector generally enhances performance, further fine-tuning and domain-specific adjustments may be necessary to optimize results across all target domains.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">Overall, the results demonstrate that the <span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector is a promising approach for improving ASR domain adaptation. By addressing the acoustic mismatches between synthetic and real speech data, our method significantly enhances the performance of ASR models in real-world applications.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>How does <span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector perform across different model sizes?</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To answer <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Q2</span>, we analyze the effect of model size on the performance of ASR adaptation using the <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 How does SYN2REAL task vector perform across different model sizes? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the relative word error rate (WER) improvements across different model sizes (Tiny, Base, Small) and various target domains.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:321.1pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T2.1.1" class="ltx_p"><span id="S5.T2.1.1.1" class="ltx_text">
<span id="S5.T2.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T2.1.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Relative WER</span> <math id="S5.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S5.T2.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Cooking</span></span>
<span id="S5.T2.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Music</span></span>
<span id="S5.T2.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Social</span></span>
<span id="S5.T2.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Weather</span></span>
<span id="S5.T2.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Average</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T2.1.1.1.1.2.1" class="ltx_tr">
<span id="S5.T2.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Tiny</span>
<span id="S5.T2.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">41.11%</span></span>
<span id="S5.T2.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">-13.47%</span>
<span id="S5.T2.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">2.60%</span>
<span id="S5.T2.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">30.42%</span></span>
<span id="S5.T2.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">19.48%</span></span></span>
<span id="S5.T2.1.1.1.1.3.2" class="ltx_tr">
<span id="S5.T2.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">Base</span>
<span id="S5.T2.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">1.49%</span>
<span id="S5.T2.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.1.1.3.2.3.1" class="ltx_text ltx_font_bold">37.80%</span></span>
<span id="S5.T2.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">5.00%</span>
<span id="S5.T2.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">6.82%</span>
<span id="S5.T2.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">14.70%</span></span>
<span id="S5.T2.1.1.1.1.4.3" class="ltx_tr">
<span id="S5.T2.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Small</span>
<span id="S5.T2.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-0.70%</span>
<span id="S5.T2.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">27.56%</span>
<span id="S5.T2.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T2.1.1.1.1.4.3.4.1" class="ltx_text ltx_font_bold">26.04%</span></span>
<span id="S5.T2.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-31.91%</span>
<span id="S5.T2.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">12.43%</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S5.T2.3.1" class="ltx_text ltx_font_bold">Relative WER Improvement Across Different Model Sizes after applying <span id="S5.T2.3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</span> This table shows the relative WER improvement compared to the Target Synthetic ASR for Whisper models of various sizes (Tiny, Base, and Small).</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The results indicate that the Base model achieves the highest average relative WER improvement of 14.70% across all target domains. This model size shows substantial gains, particularly in the â€™Musicâ€™ (37.80%) and â€™Socialâ€™ (5.00%) domains, demonstrating its robustness in adapting to diverse acoustic characteristics using the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The Tiny model, while achieving a higher average improvement of 19.48%, shows considerable performance gains in the â€™Cookingâ€™ (41.11%) and â€™Weatherâ€™ (30.42%) domains. However, it experiences a performance degradation in the â€™Musicâ€™ domain (-13.47%). This suggests that while the Tiny model can benefit significantly from the <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector in certain domains, its overall adaptability might be limited compared to larger models due to its reduced model size.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Interestingly, the Small model exhibits an average relative WER improvement of 12.43%, with significant performance enhancement in the â€™Socialâ€™ (26.04%) and â€™Musicâ€™ (27.56%) domains. However, it shows a notable degradation in the â€™Weatherâ€™ domain (-31.91%), indicating potential overfitting or sensitivity to specific acoustic variations.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">These results highlight the importance of model size in ASR adaptation using the <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. The Base model consistently provides balanced performance across most domains, suggesting it strikes a good balance between model size and performance. In contrast, the Tiny and Small models show varying degrees of effectiveness.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">Overall, the analysis demonstrates that while the <span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector significantly improves ASR performance across different model sizes, the extent of improvement is influenced by the modelâ€™s capacity.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Is <span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector effective on ASR models other than Whisper?</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To validate the effectiveness of the <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector on other ASR models, we conduct additional experiments using the Wav2vec2-Conformer large model.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:348.5pt;height:95.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T3.1.1" class="ltx_p"><span id="S5.T3.1.1.1" class="ltx_text">
<span id="S5.T3.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T3.1.1.1.1.2.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Wav2Vec2-Conformer</span></span>
<span id="S5.T3.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Cooking</span></span>
<span id="S5.T3.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Music</span></span>
<span id="S5.T3.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Social</span></span>
<span id="S5.T3.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">Weather</span></span>
<span id="S5.T3.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">Average</span></span></span>
<span id="S5.T3.1.1.1.1.3.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S5.T3.1.1.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.3.2.1.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Target Synthetic ASR</span></span>
<span id="S5.T3.1.1.1.1.3.2.1.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Baseline)</span></span>
</span></span>
<span id="S5.T3.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">21.26</span>
<span id="S5.T3.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">17.41</span>
<span id="S5.T3.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">25.84</span>
<span id="S5.T3.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.74</span>
<span id="S5.T3.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">20.31</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T3.1.1.1.1.4.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">+ <span id="S5.T3.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span></span>
<span id="S5.T3.1.1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.4.1.2.1" class="ltx_text ltx_font_bold">18.88</span></span>
<span id="S5.T3.1.1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.4.1.3.1" class="ltx_text ltx_font_bold">14.33</span></span>
<span id="S5.T3.1.1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.4.1.4.1" class="ltx_text ltx_font_bold">21.48</span></span>
<span id="S5.T3.1.1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.4.1.5.1" class="ltx_text ltx_font_bold">13.36</span></span>
<span id="S5.T3.1.1.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.4.1.6.1" class="ltx_text ltx_font_bold">17.01</span></span></span>
<span id="S5.T3.1.1.1.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Relative WER <math id="S5.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S5.T3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">11.21%</span></span>
<span id="S5.T3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">17.66%</span></span>
<span id="S5.T3.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">16.87%</span></span>
<span id="S5.T3.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">20.22%</span></span>
<span id="S5.T3.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">16.25%</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.4.1" class="ltx_text ltx_font_bold">WER of <span id="S5.T3.4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector on Wav2Vec2-Conformer.</span> This table shows the WER and relative WER improvement across different target domains on Wav2Vec2-Conformer model before and after applying <span id="S5.T3.5.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. </figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Is SYN2REAL task vector effective on ASR models other than Whisper? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the WER results across various target domains, including â€™Cookingâ€™, â€™Musicâ€™, â€™Socialâ€™, and â€™Weatherâ€™, comparing the baseline model finetuned on synthetic speech with the model enhanced by the <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.
The TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Is SYN2REAL task vector effective on ASR models other than Whisper? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a significant reduction in WER when the <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector is applied. The average WER drops from 20.31 to 17.01, representing an overall relative improvement of 16.25%.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The most notable improvement is observed in the â€™socialâ€™ domain, with a relative WER reduction of 16.87%. The â€™Musicâ€™ domain also shows a substantial improvement of 17.66%, indicating that the task vector successfully captures and mitigates the acoustic variability associated with music-related speech.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">In the â€™Cookingâ€™ and â€™Weatherâ€™ domains, the WER reductions are 11.21% and 20.22%, respectively. While the improvement in the â€™Cookingâ€™ domain is more modest, it still indicates that the <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector enhances the modelâ€™s adaptability to domain-specific acoustic characteristics.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">Overall, the application of the <span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector significantly enhances the performance of the Wav2vec2-Conformer large model across all tested domains. These results validate the effectiveness of the <span id="S5.SS3.p5.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> approach in bridging the gap between synthetic and real speech data, ultimately improving the robustness and versatility of ASR systems in diverse real-world scenarios.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:338.5pt;height:118.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T4.1.1" class="ltx_p"><span id="S5.T4.1.1.1" class="ltx_text">
<span id="S5.T4.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T4.1.1.1.1.2.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S5.T4.1.1.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.1.2.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T4.1.1.1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Whisper Small</span></span></span>
<span id="S5.T4.1.1.1.1.2.1.1.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T4.1.1.1.1.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">+Speech T5</span></span></span>
</span></span>
<span id="S5.T4.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Cooking</span></span>
<span id="S5.T4.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Music</span></span>
<span id="S5.T4.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Social</span></span>
<span id="S5.T4.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">Weather</span></span>
<span id="S5.T4.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">Average</span></span></span>
<span id="S5.T4.1.1.1.1.3.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S5.T4.1.1.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.1.3.2.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Target Synthetic ASR</span></span>
<span id="S5.T4.1.1.1.1.3.2.1.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Baseline)</span></span>
</span></span>
<span id="S5.T4.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.94</span>
<span id="S5.T4.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.04</span>
<span id="S5.T4.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">53.34</span>
<span id="S5.T4.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.27</span>
<span id="S5.T4.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">25.65</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T4.1.1.1.1.4.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">+ <span id="S5.T4.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span></span>
<span id="S5.T4.1.1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.1.4.1.2.1" class="ltx_text ltx_font_bold">16.00</span></span>
<span id="S5.T4.1.1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.1.4.1.3.1" class="ltx_text ltx_font_bold">15.75</span></span>
<span id="S5.T4.1.1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.1.4.1.4.1" class="ltx_text ltx_font_bold">52.95</span></span>
<span id="S5.T4.1.1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.1.4.1.5.1" class="ltx_text ltx_font_bold">15.97</span></span>
<span id="S5.T4.1.1.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.1.4.1.6.1" class="ltx_text ltx_font_bold">25.17</span></span></span>
<span id="S5.T4.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Relative WER <math id="S5.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S5.T4.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">5.57%</span></span>
<span id="S5.T4.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">1.77%</span></span>
<span id="S5.T4.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">0.73%</span></span>
<span id="S5.T4.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">1.82%</span></span>
<span id="S5.T4.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">1.86%</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S5.T4.3.1" class="ltx_text ltx_font_bold">WER on Whisper small with <span id="S5.T4.3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector derived from Speech T5 TTS models.</span> This table shows the WER and relative WER improvement accross different target domains on Whisper small with synthetic data from Speech T5. </figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Can we form <span id="S5.SS4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector from other TTS models?</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To answer <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_bold">Q4</span>, we conducted experiments using the Whisper Small model with synthetic data generated by the Speech T5 model. TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.3 Is SYN2REAL task vector effective on ASR models other than Whisper? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the WER results across various target domains, including â€™Cookingâ€™, â€™Musicâ€™, â€™Socialâ€™, and â€™Weatherâ€™, comparing the baseline model finetuned on synthetic speech with the model enhanced by the <span id="S5.SS4.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">The results indicate that applying the <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector leads to a reduction in WER across all tested domains. The average WER drops from 25.65 to 25.17, representing an overall relative improvement of 1.86%.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">The â€™Cookingâ€™ domain shows the highest relative WER reduction of 5.57%, suggesting that the <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector effectively adapts the model to this specific domain. The â€™Musicâ€™ and â€™Weatherâ€™ domains also exhibit relative improvements of 1.77% and 1.25%, respectively, indicating that the <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector helps mitigate the acoustic variations in these domains.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">However, the improvement in the â€™socialâ€™ domain is relatively modest, with a relative WER reduction of only 0.73 %. This could be attributed to the high baseline WER in this domain, suggesting that the synthetic data from Speech T5 might have limitations.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p">Overall, the application of the <span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector to the Whisper Small model with Speech T5 synthetic data demonstrates consistent performance enhancements, albeit with varying degrees of improvement across different domains. These results validate the flexibility and effectiveness of our approach in improving ASR models trained with synthetic data from different TTS models.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div id="S5.F3.1" class="ltx_inline-block ltx_transformed_outer" style="width:470.5pt;height:307.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.F3.1.1" class="ltx_p"><span id="S5.F3.1.1.1" class="ltx_text"><img src="/html/2406.02925/assets/x2.png" id="S5.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="502" height="327" alt="Refer to caption"></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S5.F3.6.1" class="ltx_text ltx_font_bold">WER vs. Scaling Factor across Different ASR Models &amp; Different TTS Models </span> The plot shows the average WER on â€™Cookingâ€™, â€™Musicâ€™, â€™Socialâ€™, and â€™Weatherâ€™ target domains as a function of the scaling factor <math id="S5.F3.3.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.F3.3.m1.1b"><mi id="S5.F3.3.m1.1.1" xref="S5.F3.3.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.F3.3.m1.1c"><ci id="S5.F3.3.m1.1.1.cmml" xref="S5.F3.3.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.3.m1.1d">\lambda</annotation></semantics></math> for various ASR models (Whisper and W2V2-conformer) and the TTS models (BARK and Speech T5) to make <span id="S5.F3.7.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors. We denote it as â€™{ASR+TTS}â€™, such as â€™Whisper Tiny+BARKâ€™ in the figure.
The scaling factor adjusts the magnitude of the SYN2REAL task vector applied to each model.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>What is the impact of the scaling factor <math id="S5.SS5.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.1.m1.1b"><mi id="S5.SS5.1.m1.1.1" xref="S5.SS5.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.1.m1.1c"><ci id="S5.SS5.1.m1.1.1.cmml" xref="S5.SS5.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.1.m1.1d">\lambda</annotation></semantics></math>?</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">This section investigates the effect of scaling the <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector on the WER of different ASR models. FigureÂ <a href="#S5.F3" title="Figure 3 â€£ 5.4 Can we form SYN2REAL task vector from other TTS models? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the WER as a function of the scaling factor <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">\lambda</annotation></semantics></math> for various ASR models and synthetic data, including Whisper Tiny with BARK, Whisper Base with BARK, Whisper Small with BARK, Whisper Small with Speech T5, and W2V2-Conformer with BARK.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">The scaling factor <math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><mi id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><ci id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">\lambda</annotation></semantics></math> adjusts the magnitude of the <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector applied to the ASR models. We evaluated a range of scaling factors from 0.1 to 1.0 to determine the optimal balance that minimizes WER.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">The results show that different models respond variably to changes in the scaling factor. For Whisper Tiny+BARK, the curve is steeper, indicating that smaller models may be more sensitive to larger adjustments from the <span id="S5.SS5.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. In contrast, Whisper Base+BARK maintains relatively stable WER values across different scaling factors, suggesting a more robust performance.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.2" class="ltx_p">Notably, Whisper Small+BARK and Whisper SmallÂ +Â Speech T5 exhibit a U-shaped trend, where moderate scaling factors (around <math id="S5.SS5.p4.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.p4.1.m1.1a"><mi id="S5.SS5.p4.1.m1.1.1" xref="S5.SS5.p4.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p4.1.m1.1b"><ci id="S5.SS5.p4.1.m1.1.1.cmml" xref="S5.SS5.p4.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p4.1.m1.1c">\lambda</annotation></semantics></math> = 0.3 to 0.5) yield the lowest WER. This indicates that an optimal scaling factor exists for these models, which balances the incorporation of real speech characteristics without overwhelming the model with excessive parameter adjustments. The Wav2vec2-Conformer model consistently shows lower WER values across all scaling factors, with the best performance at <math id="S5.SS5.p4.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.p4.2.m2.1a"><mi id="S5.SS5.p4.2.m2.1.1" xref="S5.SS5.p4.2.m2.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p4.2.m2.1b"><ci id="S5.SS5.p4.2.m2.1.1.cmml" xref="S5.SS5.p4.2.m2.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p4.2.m2.1c">\lambda</annotation></semantics></math> = 0.5.</p>
</div>
<div id="S5.SS5.p5" class="ltx_para">
<p id="S5.SS5.p5.1" class="ltx_p">Overall, the analysis suggests that the optimal scaling factor <math id="S5.SS5.p5.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS5.p5.1.m1.1a"><mi id="S5.SS5.p5.1.m1.1.1" xref="S5.SS5.p5.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p5.1.m1.1b"><ci id="S5.SS5.p5.1.m1.1.1.cmml" xref="S5.SS5.p5.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p5.1.m1.1c">\lambda</annotation></semantics></math> varies depending on the ASR modelâ€™s architecture and size. While smaller models like Whisper Tiny+BARK may benefit from lower scaling factors, larger and more robust models like W2V2-Conformer+BARK can effectively leverage higher scaling factors. These findings highlight the importance of tuning the scaling factor to achieve the best domain adaptation performance for different ASR models.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Do <span id="S5.SS6.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors obtained with the same TTS have similar directions?</h3>

<figure id="S5.F4" class="ltx_figure">
<div id="S5.F4.1" class="ltx_inline-block ltx_transformed_outer" style="width:526.8pt;height:606.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.F4.1.1" class="ltx_p"><span id="S5.F4.1.1.1" class="ltx_text"><img src="/html/2406.02925/assets/x3.png" id="S5.F4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="562" height="647" alt="Refer to caption"></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.4.1" class="ltx_text ltx_font_bold">Cosine Similarity between task vectors derived from Different TTS Models.</span> This heatmap shows the cosine similarity between task vectors generated by BARK (B_) and Speech T5 (S_) models. Higher similarity values between vectors from similar domains indicate effective acoustic-specific information transfer by the <span id="S5.F4.5.2" class="ltx_text ltx_font_italic">SYN2REAL</span> method. </figcaption>
</figure>
<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">To further validate the <span id="S5.SS6.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> approach, we conducted a cosine similarity analysis between <span id="S5.SS6.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors derived by different text-to-speech (TTS) models: BARK (denoted as B_) and Speech T5 (denoted as S_). FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.6 Do SYN2REAL task vectors obtained with the same TTS have similar directions? â€£ 5 Results &amp; Discussion â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the cosine similarity heatmap between these <span id="S5.SS6.p1.1.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">The heat map reveals that <span id="S5.SS6.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors from similar TTS exhibit higher cosine similarity, indicating that the <span id="S5.SS6.p2.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector effectively captures the distributional shifts between different acoustic domains.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p">Moreover, the negative similarities between certain <span id="S5.SS6.p3.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors, such as â€™B_recommendationâ€™ and â€™S_musicâ€™ (-0.67), highlight the distinct acoustic features between these TTS synthetic data, further emphasizing the effectiveness of the <span id="S5.SS6.p3.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> approach in distinguishing and adapting to different acoustic environments.</p>
</div>
<div id="S5.SS6.p4" class="ltx_para">
<p id="S5.SS6.p4.1" class="ltx_p">The overall trend observed in the heatmap supports the hypothesis that the <span id="S5.SS6.p4.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors not only bridge the gap between synthetic and real data but also maintain consistency within similar acoustic environments. This consistency is crucial for enhancing ASR performance across diverse target domains, as it ensures that the task vectors can generalize well to new, unseen domains.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> Task Vector given Domain Labels</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we explore an alternative approach to generating <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors, assuming we have access to domain labels for the data in the source domains.
This approach, which we refer to as <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span>, involves generating separate <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectors for each source domain and then combining them to enhance the adaptation of the ASR model to the target domain.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:355.2pt;height:95.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T5.1.1" class="ltx_p"><span id="S6.T5.1.1.1" class="ltx_text">
<span id="S6.T5.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S6.T5.1.1.1.1.2.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">SYN2REAL Ensemble</span></span>
<span id="S6.T5.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Cooking</span></span>
<span id="S6.T5.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Music</span></span>
<span id="S6.T5.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Social</span></span>
<span id="S6.T5.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">Weather</span></span>
<span id="S6.T5.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">Average</span></span></span>
<span id="S6.T5.1.1.1.1.3.2" class="ltx_tr">
<span id="S6.T5.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S6.T5.1.1.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T5.1.1.1.1.3.2.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Target Synthetic ASR</span></span>
<span id="S6.T5.1.1.1.1.3.2.1.1.2" class="ltx_tr">
<span id="S6.T5.1.1.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(Baseline)</span></span>
</span></span>
<span id="S6.T5.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">14.26</span>
<span id="S6.T5.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">23.51</span>
<span id="S6.T5.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">29.57</span>
<span id="S6.T5.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">15.45</span>
<span id="S6.T5.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">20.70</span></span>
</span>
<span class="ltx_tbody">
<span id="S6.T5.1.1.1.1.4.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">+ <span id="S6.T5.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span></span>
<span id="S6.T5.1.1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">14.46</span>
<span id="S6.T5.1.1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.1.1.1.1.4.1.3.1" class="ltx_text ltx_font_bold">16.98</span></span>
<span id="S6.T5.1.1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.1.1.1.1.4.1.4.1" class="ltx_text ltx_font_bold">21.13</span></span>
<span id="S6.T5.1.1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.1.1.1.1.4.1.5.1" class="ltx_text ltx_font_bold">15.11</span></span>
<span id="S6.T5.1.1.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.1.1.1.1.4.1.6.1" class="ltx_text ltx_font_bold">16.92</span></span></span>
<span id="S6.T5.1.1.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Relative WER <math id="S6.T5.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T5.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T5.1.1.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S6.T5.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_bb">-1.40%</span>
<span id="S6.T5.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T5.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">27.78%</span></span>
<span id="S6.T5.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T5.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">28.54%</span></span>
<span id="S6.T5.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T5.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">2.20%</span></span>
<span id="S6.T5.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T5.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">18.25%</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S6.T5.4.1" class="ltx_text ltx_font_bold">WER Performance on Whisper Small Model with <span id="S6.T5.4.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vectors.</span> This table compares the word error rate (WER) of the baseline ASR model fine-tuned on synthetic speech data with the WER of the model enhanced with the <span id="S6.T5.5.2" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vectors across four target domains: â€™Cookingâ€™, â€™Musicâ€™, â€™Socialâ€™, and â€™Weatherâ€™. </figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Performance of <span id="S6.SS1.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vector</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">To evaluate the effectiveness of the <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vector, we conducted experiments using the Whisper Small model with synthetic speech generated by the BARK TTS model. The experiments were carried out on four target domains: â€™cooking,â€™ â€™music,â€™ â€™social,â€™ and â€™weather,â€™ using 17 source domains to create the <span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> vectors. The results are presented in Table <a href="#S6.T5" title="Table 5 â€£ 6 SYN2REAL Task Vector given Domain Labels â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
The results indicate that the <span id="S6.SS1.p1.1.3" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> approach provides significant improvements in WER for several target domains compared to the baseline method. The average WER is reduced from 20.70 to 16.92, representing an overall relative improvement of 18.25%. This highlights the effectiveness of using domain-specific task vectors to capture detailed acoustic characteristics, leading to enhanced model adaptation.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Comparing the <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> approach to the original <span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> method, we find that <span id="S6.SS1.p2.1.3" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> generally outperforms the original approach. The detailed domain-specific information captured by the distinct task vectors enhances model adaptation in most domains. However, in real-world scenarios, we often do not have access to labels finer-grained domain labels.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Impact of the number of domains on the performance of <span id="S6.SS2.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vector</h3>

<figure id="S6.F5" class="ltx_figure">
<div id="S6.F5.1" class="ltx_inline-block ltx_transformed_outer" style="width:460.4pt;height:305.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.F5.1.1" class="ltx_p"><span id="S6.F5.1.1.1" class="ltx_text"><img src="/html/2406.02925/assets/x4.png" id="S6.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="491" height="325" alt="Refer to caption"></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S6.F5.4.1" class="ltx_text ltx_font_bold">WER vs. Number of Source Domains for <span id="S6.F5.4.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vector.</span> This plot shows the word error rate (WER) of the Whisper small model and the number of source domains used to generate the <span id="S6.F5.5.2" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vector with BARK model. The x-axis represents the number of source domains, and the y-axis represents the WER on average of the four domains (â€™Cookingâ€™, â€™Musicâ€™, â€™Socialâ€™, â€™Weatherâ€™).</figcaption>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">FigureÂ <a href="#S6.F5" title="Figure 5 â€£ 6.2 Impact of the number of domains on the performance of SYN2REAL Ensemble task vector â€£ 6 SYN2REAL Task Vector given Domain Labels â€£ Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the average WER across four target domains (â€™cooking,â€™ â€™music,â€™ â€™social,â€™ and â€™weatherâ€™) when we use different numbers of source domain data to generate <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL Ensemble</span> task vectors.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">The results indicate that increasing the number of source domains generally improves ASR performance. The WER decreases from 17.8 to 16.8 as the number of source domains increases from 1 to 17. Notably, significant improvements are observed within the incorporation of the first 5 source domains. This trend suggests that incorporating more source domains helps capture diverse acoustic characteristics, leading to better domain adaptation.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper introduces <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector to address mismatches between synthetic and real speech data. Future work will refine this approach and extend its application to other types of tasks and data such as visual data, contributing to more reliable speech and vision recognition systems.
Experiments showed significant WER reductions, averaging 10.03% across 18 domains. We also test various models, including Whisper Small, Whisper Base,, Whisper Tiny and Wav2vec2-Conformer, with <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">SYN2REAL</span> showing robust performance.

</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>

<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Domain-Specific Performance Variations</h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p">While the <span id="S8.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector shows significant improvements in many target domains, certain domains, such as â€™Cookingâ€™ and â€™Weather,â€™ exhibit marginal improvements or slight degradation in word error rate (WER). This suggests that the task vectorâ€™s effectiveness may vary based on the specific characteristics of different domains, indicating a need for further domain-specific fine-tuning and adjustments.</p>
</div>
</section>
<section id="S8.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling Factor Sensitivity</h4>

<div id="S8.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px2.p1.1" class="ltx_p">The performance of the <span id="S8.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span>-enhanced models is sensitive to the scaling factor <math id="S8.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S8.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S8.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px2.p1.1.m1.1c">\lambda</annotation></semantics></math>. Finding the optimal scaling factor requires careful tuning, and the best value can vary between different ASR models and target domains. This adds a layer of complexity to the implementation and may limit the approachâ€™s generalizability without additional adaptive scaling strategies.</p>
</div>
</section>
<section id="S8.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Data Quality</h4>

<div id="S8.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px3.p1.1" class="ltx_p">The approach relies heavily on the quality of synthetic speech data generated by TTS systems. Variations in the quality and acoustic properties of synthetic data across different TTS systems can impact the effectiveness of the <span id="S8.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vector. Ensuring consistent quality in synthetic data is crucial for achieving robust domain adaptation.</p>
</div>
</section>
<section id="S8.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model-Specific Dependencies</h4>

<div id="S8.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px4.p1.1" class="ltx_p">The observed improvements are model-dependent, with larger models like Wav2Vec2-Conformer showing more substantial gains compared to smaller models like Whisper Tiny. This indicates that the <span id="S8.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_italic">SYN2REAL</span> task vectorâ€™s effectiveness might be influenced by the underlying model architecture and size, potentially limiting its applicability to a wider range of ASR models without further optimization.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgements</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">We specifically thank Ting-Yao Hu for all the insightful discussions and constructive suggestions for this work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, YuÂ Wu, Shujie Liu, Tom Ko, Qing Li, YuÂ Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.393" title="" class="ltx_ref ltx_href">SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 5723â€“5738, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2006.11477" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2006.11477.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartelds etÂ al. (2023)</span>
<span class="ltx_bibblock">
Martijn Bartelds, Nay San, Bradley McDonnell, Dan Jurafsky, and Martijn Wieling. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2305.10951" title="" class="ltx_ref ltx_href">Making more of little data: Improving low-resource automatic speech recognition using data augmentation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2305.10951.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bastianelli etÂ al. (2020)</span>
<span class="ltx_bibblock">
Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.588" title="" class="ltx_ref ltx_href">SLURP: A spoken language understanding resource package</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7252â€“7262, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bataev etÂ al. (2023)</span>
<span class="ltx_bibblock">
Vladimir Bataev, Roman Korostik, Evgeny Shabalin, Vitaly Lavrukhin, and Boris Ginsburg. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2302.14036" title="" class="ltx_ref ltx_href">Text-only domain adaptation for end-to-end asr using integrated text-to-mel-spectrogram generator</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2302.14036.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhardwaj etÂ al. (2024)</span>
<span class="ltx_bibblock">
Rishabh Bhardwaj, DoÂ Duc Anh, and Soujanya Poria. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2402.11746" title="" class="ltx_ref ltx_href">Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2402.11746.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borsos etÂ al. (2023)</span>
<span class="ltx_bibblock">
ZalÃ¡n Borsos, RaphaÃ«l Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2209.03143" title="" class="ltx_ref ltx_href">Audiolm: a language modeling approach to audio generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2209.03143.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chang Chen, Xun Gong, and Yanmin Qian. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389682" title="" class="ltx_ref ltx_href">Efficient text-only domain adaptation for ctc-based asr</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1â€“7.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daheim etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and EdoardoÂ M. Ponti. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.17574" title="" class="ltx_ref ltx_href">Elastic weight removal for faithful and abstractive dialogue generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2303.17574.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati etÂ al. (2020)</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, YuÂ Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-3015" title="" class="ltx_ref ltx_href">Conformer: Convolution-augmented Transformer for Speech Recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 5036â€“5040.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, Kuang-Ming Chen, YuÂ Tung Lin, Shih-Kai Hsiao, Richard Tzong-Han Tsai, and Hung yiÂ Lee. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2310.04799" title="" class="ltx_ref ltx_href">Chat vector: A simple approach to equip llms with instruction following and model alignment in new languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2310.04799.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gabriel Ilharco, MarcoÂ Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=6t0Kwf8-jrj" title="" class="ltx_ref ltx_href">Editing models with task arithmetic</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi and Singh (2022)</span>
<span class="ltx_bibblock">
Raviraj Joshi and Anupam Singh. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.ecnlp-1.28" title="" class="ltx_ref ltx_href">A simple baseline for domain adaptation in end to end ASR systems using synthetic data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5)</em>, pages 244â€“249, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rBCvMG-JsPd" title="" class="ltx_ref ltx_href">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov etÂ al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">Librispeech: An asr corpus based on public domain audio books</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 5206â€“5210.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2212.04356.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2024)</span>
<span class="ltx_bibblock">
Gowtham Ramesh, Kartik Audhkhasi, and Bhuvana Ramabhadran. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447848" title="" class="ltx_ref ltx_href">Task vector algebra for asr models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 12256â€“12260.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sato etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hiroaki Sato, Tomoyasu Komori, Takeshi Mishima, Yoshihiko Kawai, Takahiro Mochizuki, Shoei Sato, and Tetsuji Ogawa. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2022-10114" title="" class="ltx_ref ltx_href">Text-Only Domain Adaptation Based on Intermediate CTC</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, pages 2208â€“2212.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hsuan Su, Ting-Yao Hu, HemaÂ Swetha Koppula, Raviteja Vemulapalli, Jen-HaoÂ Rick Chang, Karren Yang, GautamÂ Varma Mantena, and Oncel Tuzel. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447240" title="" class="ltx_ref ltx_href">Corpus synthesis for zero-shot asr domain adaptation using large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 12326â€“12330.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundar etÂ al. (2023)</span>
<span class="ltx_bibblock">
AnirudhÂ S. Sundar, Chao-HanÂ Huck Yang, DavidÂ M. Chan, Shalini Ghosh, Venkatesh Ravichandran, and PhaniÂ Sankar Nidadavolu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:266520969" title="" class="ltx_ref ltx_href">Multimodal attention merging for improved speech recognition and audio event classification</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2312.14378.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sung etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.105" title="" class="ltx_ref ltx_href">An empirical study of multimodal model merging</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 1563â€“1575, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tam etÂ al. (2024)</span>
<span class="ltx_bibblock">
Derek Tam, Mohit Bansal, and Colin Raffel. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=qNGo6ghWFB" title="" class="ltx_ref ltx_href">Merging by matching models in task parameter subspaces</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2307.09288.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:1706.03762.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vuong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tyler Vuong, Karel Mundnich, Dhanush Bekal, Veera Elluru, Srikanth Ronanki, and Sravan Bodapati. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-industry.35" title="" class="ltx_ref ltx_href">AdaBERT-CTC: Leveraging BERT-CTC for text-only domain adaptation in ASR</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, pages 364â€“371, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.aacl-demo.6" title="" class="ltx_ref ltx_href">Fairseq S2T: Fast speech-to-text modeling with fairseq</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</em>, pages 33â€“39, Suzhou, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, and Juan Pino. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2010.05171" title="" class="ltx_ref ltx_href">fairseq s2t: Fast speech-to-text modeling with fairseq</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2010.05171.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chengyi Wang, Sanyuan Chen, YuÂ Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2301.02111" title="" class="ltx_ref ltx_href">Neural codec language models are zero-shot text to speech synthesizers</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2301.02111.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Karren Yang, Ting-Yao Hu, Jen-HaoÂ Rick Chang, Hema SwethaÂ Koppula, and Oncel Tuzel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP49357.2023.10096971" title="" class="ltx_ref ltx_href">Text is all you need: Personalizing asr models using controllable speech synthesis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1â€“5.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuen etÂ al. (2023)</span>
<span class="ltx_bibblock">
KwokÂ Chin Yuen, LiÂ Haoyang, and ChngÂ Eng Siong. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317116" title="" class="ltx_ref ltx_href">Asr model adaptation for rare words using synthetic data generated by multiple text-to-speech systems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</em>, pages 1771â€“1778.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=5r3e27I9Gy" title="" class="ltx_ref ltx_href">Composing parameter-efficient modules with arithmetic operation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xianrui Zheng, Yulan Liu, Deniz Gunceler, and Daniel Willett. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP39728.2021.9414778" title="" class="ltx_ref ltx_href">Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 5674â€“5678.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.02924" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.02925" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.02925">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.02925" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.02926" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:32:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
