<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.07336] PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores</title><meta property="og:description" content="Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks. However, the growth is not attributed solely to models and benchmar…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.07336">

<!--Generated on Sun May  5 20:01:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">(eccv)                Package eccv Warning: Package ‘hyperref’ is not loaded, but highly recommended for camera-ready version</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>The University of Texas at Dallas
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>AWS AI Labs
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>pramathu@amazon.com</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lucas Goncalves 
</span><span class="ltx_author_notes">Work done during internship at Amazon.11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prashant Mathur
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chandrashekhar Lavania
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Metehan Cekic
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcello Federico
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kyu J. Han
</span><span class="ltx_author_notes">22</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks. However, the growth is not attributed solely to models and benchmarks. Universally accepted evaluation metrics also play an important role in advancing the field. While there are many metrics available to evaluate audio and visual content separately, there is a lack of metrics that offer a quantitative and interpretable measure of audio-visual synchronization for videos ‘in the wild’. To address this gap, we first created a large scale human annotated dataset (100+ hrs) representing nine types of synchronization errors in audio-visual content and how human perceive them. We then developed a PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) score, a novel automatic metric with a 5-point scale that evaluates the quality of audio-visual synchronization. We validate PEAVS using a newly generated dataset, achieving a Pearson correlation of 0.79 at the set level and 0.54 at the clip level when compared to human labels. In our experiments, we observe a relative gain 50% over a natural extension of Fréchet based metrics for Audio-Visual synchrony, confirming PEAVS’ efficacy in objectively modeling subjective perceptions of audio-visual synchronization for videos ‘in the wild’.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Audio-visual (AV) generative modeling has made rapid progress in recent years due to advancements in deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and the availability of large-scale datasets like VGGSound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, evaluation of these models remains an open challenge.
Existing automatic metrics often focus on specific aspects, such as image quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> or audio fidelity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, lacking a satisfactory measure for assessing audio-visual synchronization of ‘in the wild’ videos.
In fact, lack of well-established metrics for AV synchrony issues can already be observed in literature. For instance, recent works in the realm of audio-visual generation, like MM-Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, had to solely rely on single-modality metrics to evaluate their outputs. The work on Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> had to train their own audio-visual synchrony classifier.
More advanced evaluation metrics are needed to provide a holistic assessment of audio-visual coherence. The development of such metrics is essential to accurately judge model performance, identify failure cases, and drive further progress in this evolving field.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent studies such as AV Synchrony Transformer (AVST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, SparseSync <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> have defined the synchronization problem between audio-visual modalities primarily as an issue of temporal offset determination. However, this perspective addresses only one aspect of synchronization challenges. In our work, we extend the scope to a broader range of synchronization issues, encompassing audio/visual speed variations, intermittent muting, fragment shuffling, AV flickering, and temporal shifts. We also delve into developing a perceptual, automatic metric that aligns with human judgment. To achieve this, we initiated a large scale annotation study, gathering human assessments on audio-visual synchrony across over 100 hours of both original and distorted content, featuring the aforementioned synchronization issues.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this endeavor, our objective is to create a metric that accomplishes the following: (a) attains a high correlation with human assessments of synchronization issues, (b) offers interpretable scoring for end users, and (c) operates in a reference-free manner, meaning it does not rely on ground truth for quality prediction. The key innovation in our approach lies in the development of an interpretable scale based on detailed perceptual guidelines provided to human raters. By optimizing for concordance with human-annotated scores during training, our metric produces scores directly aligned with predefined levels of audio-visual (a)synchrony. This emphasis on interpretability and alignment with human perception addresses the need for a more insightful evaluation of AV synchronization.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Our main contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present a new Audio-Visual Synchrony human perception (AVS) benchmark data set with over 120K annotations and over 100+ hrs of content. Each video in this benchmark is annotated by three different annotators and we see an agreement of 0.71 Krippendorff’s alpha <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a new audio-visual synchrony evaluation metric - PEAVS - that is reference-free and has interpretable scoring on a scale of 1 to 5.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">PEAVS shows high correlation of 0.79 with human judgements on the benchmark data, significantly outperforming a Fréchet distance based AV synchrony metric.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Unimodal Metrics:</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Several factors have fueled progress in single modality generation, including advancement in deep learning model architectures and training/optimization methodology, increased computational capabilities, a rich availability of multimodal data, and the development of consistent metrics. Established metrics like Inception Score (ISc) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Fréchet Audio Distance (FAD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Fréchet Video Distance (FVD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, inter alia, are frequently employed to gauge the quality of the content generated by unimodal generative models, providing a standard for comparison. However, these metrics are reliant only on one modality - either audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, or just applied to images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In the evaluation of generated multimodal content, ensuring the quality of individual modalities is only half the battle. It is crucial to evaluate the synchronization between audio and visual elements, especially in the context of audio-visual generation.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multimodal Metrics:</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">For AV synchrony evaluation, it is important to determine the misalignment between audio and visual inputs. In the past, researchers have explored handling synchrony by statistical approaches and specifically designed features to pinpoint where the sound comes from, like locating the source of a voice in a scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, with the rise of deep learning recent studies are harnessing distinct connections between audio and visuals, like those in human speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, audio-visual correspondence to check for matching content<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or musical performances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Some recent research studies, such as those by Chen <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Iashin <span id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, propose methods to determine audio-visual synchronization in a natural diverse video environment. However, their focus is solely on tackling the synchronization problem arising from delays in audio or video streams, specifically addressing differences in timing (offsets). In these existing approaches, the model is trained with the objective of either 1) detecting synchronization errors as a binary classification or 2) predicting the precise offset values through regression. In contrast, our aim is to have the model replicate the way humans perceive synchronization issues across a range of distortion types (see more in Sec <a href="#S3.SS1" title="3.1 Data Preparation and Distortions ‣ 3 AVS Benchmark Dataset ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Metrics for “in the wild” videos:</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The assessment of multimodal content has been extensively investigated within the computer vision community however most of the work has been tailored to lip-synchrony for instance, SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> quantifies lip-sync errors by comparing mouth movements to the corresponding speech; AlignNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> focuses on aligning video content with ground truth audio, and predicts frame-wise offsets.
In this work, we focus on evaluation of AV synchrony for “in the wild” videos which apart from Chen <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Iashin <span id="S2.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> has been explored in Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> where the authors train their own alignment classifier to detect synchrony. However, none of these works directly target modeling of human perception.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Perceptual Metrics:</h5>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Previous studies on audio metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> demonstrated that a learned metric trained on data collected via crowd-sourced human annotations, particularly on just noticeably different samples, correlates well with mean opinion scores.
For video modality, Wang et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> designed a perceptual metric — learnt over a large scale human annotated data — designed to distinguish user generated video content in different dimensions such as semantic content, technical quality, and compression level. Our work in contrast centers on the synchronization between audio and video than solely on video or audio quality.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T1.2.1.1.1.1" class="ltx_text ltx_font_bold">ID</span></th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt"><span id="S2.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">Distortion</span></th>
<th id="S2.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt"><span id="S2.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">Parameter</span></th>
<th id="S2.T1.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.2.1.1.4.1" class="ltx_text ltx_font_bold">Levels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<td id="S2.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.2.2.1.1.1" class="ltx_text">1</span></td>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Temporal Shift</td>
<td id="S2.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">shift length</td>
<td id="S2.T1.2.2.1.4" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S2.T1.2.2.1.4.1" class="ltx_text">-1, -.5, -.125, .045, .1, .125, .25, .5, 1, 2</span></td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<td id="S2.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Audio</td>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">(sec)</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.4.3.1.1" class="ltx_text" style="background-color:#E6E6E6;">2</span></td>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.4.3.2.1" class="ltx_text" style="background-color:#E6E6E6;">Audio Speed</span></td>
<td id="S2.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.4.3.3.1" class="ltx_text" style="background-color:#E6E6E6;">speed factor</span></td>
<td id="S2.T1.2.4.3.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.4.3.4.1" class="ltx_text" style="background-color:#E6E6E6;">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.5.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">2</span></td>
<td id="S2.T1.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.5.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">Change up</span></td>
<td id="S2.T1.2.5.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.5.4.3.1" class="ltx_text" style="background-color:#E6E6E6;">%</span></td>
<td id="S2.T1.2.5.4.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.5.4.4.1" class="ltx_text" style="background-color:#E6E6E6;">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.6.5" class="ltx_tr">
<td id="S2.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_border_r" rowspan="2"><span id="S2.T1.2.6.5.1.1" class="ltx_text">3</span></td>
<td id="S2.T1.2.6.5.2" class="ltx_td ltx_align_center ltx_border_r">Video Speed</td>
<td id="S2.T1.2.6.5.3" class="ltx_td ltx_align_center ltx_border_r">speed factor</td>
<td id="S2.T1.2.6.5.4" class="ltx_td ltx_align_left" rowspan="2"><span id="S2.T1.2.6.5.4.1" class="ltx_text">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.7.6" class="ltx_tr">
<td id="S2.T1.2.7.6.1" class="ltx_td ltx_align_center ltx_border_r">Change up</td>
<td id="S2.T1.2.7.6.2" class="ltx_td ltx_align_center ltx_border_r">%</td>
</tr>
<tr id="S2.T1.2.8.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.8.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.8.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">4</span></td>
<td id="S2.T1.2.8.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.8.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">Audio Speed</span></td>
<td id="S2.T1.2.8.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.8.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">speed factor</span></td>
<td id="S2.T1.2.8.7.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.8.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.9.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.9.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.9.8.1.1" class="ltx_text" style="background-color:#E6E6E6;">4</span></td>
<td id="S2.T1.2.9.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.9.8.2.1" class="ltx_text" style="background-color:#E6E6E6;">Change down</span></td>
<td id="S2.T1.2.9.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.9.8.3.1" class="ltx_text" style="background-color:#E6E6E6;">%</span></td>
<td id="S2.T1.2.9.8.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.9.8.4.1" class="ltx_text" style="background-color:#E6E6E6;">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.10.9" class="ltx_tr">
<td id="S2.T1.2.10.9.1" class="ltx_td ltx_align_left ltx_border_r" rowspan="2"><span id="S2.T1.2.10.9.1.1" class="ltx_text">5</span></td>
<td id="S2.T1.2.10.9.2" class="ltx_td ltx_align_center ltx_border_r">Video Speed</td>
<td id="S2.T1.2.10.9.3" class="ltx_td ltx_align_center ltx_border_r">speed factor</td>
<td id="S2.T1.2.10.9.4" class="ltx_td ltx_align_left" rowspan="2"><span id="S2.T1.2.10.9.4.1" class="ltx_text">.025, .05, .10, .15, .20, .25, .30, .4, .5, .75</span></td>
</tr>
<tr id="S2.T1.2.11.10" class="ltx_tr">
<td id="S2.T1.2.11.10.1" class="ltx_td ltx_align_center ltx_border_r">Change down</td>
<td id="S2.T1.2.11.10.2" class="ltx_td ltx_align_center ltx_border_r">%</td>
</tr>
<tr id="S2.T1.2.12.11" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.12.11.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.12.11.1.1" class="ltx_text" style="background-color:#E6E6E6;">6</span></td>
<td id="S2.T1.2.12.11.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.12.11.2.1" class="ltx_text" style="background-color:#E6E6E6;">Intermittent</span></td>
<td id="S2.T1.2.12.11.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.12.11.3.1" class="ltx_text" style="background-color:#E6E6E6;">duration (sec) of mute</span></td>
<td id="S2.T1.2.12.11.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.12.11.4.1" class="ltx_text" style="background-color:#E6E6E6;">.01, .025, .05, .1, .2, .3, .5, 1, 2.5, 4</span></td>
</tr>
<tr id="S2.T1.2.13.12" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.13.12.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.13.12.1.1" class="ltx_text" style="background-color:#E6E6E6;">6</span></td>
<td id="S2.T1.2.13.12.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.13.12.2.1" class="ltx_text" style="background-color:#E6E6E6;">Muting</span></td>
<td id="S2.T1.2.13.12.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.13.12.3.1" class="ltx_text" style="background-color:#E6E6E6;">for every 1 sec</span></td>
<td id="S2.T1.2.13.12.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.13.12.4.1" class="ltx_text" style="background-color:#E6E6E6;">.01, .025, .05, .1, .2, .3, .5, 1, 2.5, 4</span></td>
</tr>
<tr id="S2.T1.2.14.13" class="ltx_tr">
<td id="S2.T1.2.14.13.1" class="ltx_td ltx_align_left ltx_border_r" rowspan="2"><span id="S2.T1.2.14.13.1.1" class="ltx_text">7</span></td>
<td id="S2.T1.2.14.13.2" class="ltx_td ltx_align_center ltx_border_r">Randomly Sized</td>
<td id="S2.T1.2.14.13.3" class="ltx_td ltx_align_center ltx_border_r">gap duration (sec)</td>
<td id="S2.T1.2.14.13.4" class="ltx_td ltx_align_left" rowspan="2"><span id="S2.T1.2.14.13.4.1" class="ltx_text">.01, .025, .05, .1, .2, .3, .5, 1, 2.5, 4</span></td>
</tr>
<tr id="S2.T1.2.15.14" class="ltx_tr">
<td id="S2.T1.2.15.14.1" class="ltx_td ltx_align_center ltx_border_r">Gaps Video</td>
<td id="S2.T1.2.15.14.2" class="ltx_td ltx_align_center ltx_border_r">and prob. 40%</td>
</tr>
<tr id="S2.T1.2.16.15" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.16.15.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.16.15.1.1" class="ltx_text" style="background-color:#E6E6E6;">8</span></td>
<td id="S2.T1.2.16.15.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.16.15.2.1" class="ltx_text" style="background-color:#E6E6E6;">Fragment</span></td>
<td id="S2.T1.2.16.15.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.16.15.3.1" class="ltx_text" style="background-color:#E6E6E6;">duration (sec) of</span></td>
<td id="S2.T1.2.16.15.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.16.15.4.1" class="ltx_text" style="background-color:#E6E6E6;">.3, .4, .5, 1, 1.5, 2, 2.5, 3, 3.5, 4</span></td>
</tr>
<tr id="S2.T1.2.17.16" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S2.T1.2.17.16.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.2.17.16.1.1" class="ltx_text" style="background-color:#E6E6E6;">8</span></td>
<td id="S2.T1.2.17.16.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.17.16.2.1" class="ltx_text" style="background-color:#E6E6E6;">Shuffling</span></td>
<td id="S2.T1.2.17.16.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S2.T1.2.17.16.3.1" class="ltx_text" style="background-color:#E6E6E6;">segments to shuffle</span></td>
<td id="S2.T1.2.17.16.4" class="ltx_td ltx_align_left"><span id="S2.T1.2.17.16.4.1" class="ltx_text" style="background-color:#E6E6E6;">.3, .4, .5, 1, 1.5, 2, 2.5, 3, 3.5, 4</span></td>
</tr>
<tr id="S2.T1.2.18.17" class="ltx_tr">
<td id="S2.T1.2.18.17.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" rowspan="2"><span id="S2.T1.2.18.17.1.1" class="ltx_text">9</span></td>
<td id="S2.T1.2.18.17.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" rowspan="2"><span id="S2.T1.2.18.17.2.1" class="ltx_text">AV Flickering</span></td>
<td id="S2.T1.2.18.17.3" class="ltx_td ltx_align_center ltx_border_r">gap duration (sec)</td>
<td id="S2.T1.2.18.17.4" class="ltx_td ltx_align_left ltx_border_bb" rowspan="2"><span id="S2.T1.2.18.17.4.1" class="ltx_text">.01, .025, .05, .1, .2, .3, .5, 1, 2.5, 4</span></td>
</tr>
<tr id="S2.T1.2.19.18" class="ltx_tr">
<td id="S2.T1.2.19.18.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">and prob. 40%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Summary of Distortions and their Parameters for Audio-Visual Content.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>AVS Benchmark Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To develop an effective metric that aligns closely with human perception, it is important to have a dataset representative of human judgments. With this in mind, our study was designed to create a benchmark for our metric training and to assess its correlation with human perception, especially in the context of AV synchronization.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Preparation and Distortions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Audio-visual synchrony, the alignment of audio and visual elements, is important for an immersive multimedia experience. Even minor disruptions can notably degrade the user’s experience, as witnessed in scenarios like watching a video where a mere mismatch in drums playing (visuals) and beats (audio) can become distracting. Various real-world challenges, from unreliable network connection to encoding mishaps, can lead to such anomalies.
To produce realistic data representing such scenarios, we first collected 200 videos from the AudioSet corpus’ <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">evaluation</em> split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> ensuring that there is an aspect of synchrony in each of the samples. The samples selected are examples of “in the wild” videos, and does not include any samples with faces. We only include videos from actions such as: car driving by, dogs barking, instruments being played, manual labor being done, etc.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Appendix <a href="#Pt0.A3" title="Appendix 0.C Dataset Labels Information ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.C</span></a> contains a brief overview of top-30 label frequencies contained within the videos subset selected for our dataset.</span></span></span> Each video then underwent nine synchrony related distortions at ten varying levels as shown in Table  <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, totaling in 18,200 distorted videos.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we apply static and temporal noises in the AV content.
<span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Temporal noise</span> is where the distortions in either or both modalities cause de-synchronization in the AV content:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Temporal Misalignment:</span> The audio track was offset, either forwards or backwards relative to the video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, creating de-synchronizations spanning from -1 to 2 seconds.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Audio Speed Change:</span> Similar to FAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the audio playback speed was manipulated, without corresponding changes in the video, resulting in gradual de-synchronization in the AV content. Depending on the speed change (up/down), we either clip the audio or visual frames to keep the same duration as the original video. In this setting, we ensure the sound characteristics is preserved by keeping essential features such as pitch and timbre while altering speed to minimize changes on sound texture.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Video Speed Change:</span> Similar to FVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, the speed of the video is adjusted while leaving the audio track untouched. This would cause the visuals to slowly move out of sync with the audio. As in the audio speed change, we clip the content to keep the same duration as the original one.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Fragment Shuffling:</span> Similar to the global swap distortion in FVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, both audio and video tracks were segmented and rearranged, preserving segment-level synchrony but disrupting overall synchronization of the video.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Static noise</span> is where the distortions do not necessarily change the synchrony in the AV content but it can be perceived as synchronization errors by humans:</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Intermittent Muting:</span> Intermittent periods of silences with varying duration is introduced in the audio track. This distortion disrupts the continuity, which can pose a challenge for some synchronization metrics. This distortion is similar to Pops as described in Kilgour et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Randomly Sized Gaps:</span> We randomly pick a frame with a certain probability and add black out visuals with varying number of frames in the video track. This distortion disrupts the continuity of the video. This distortion is similar to black rectangle noise as introduced in FVD.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">AV Flickering:</span> Periods of silence and blackout were introduced in the audio-visual track. Although flickering by itself is not a synchronization issue, it can however be perceived as synchronization errors by humans and poses a challenging scenario for metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Annotation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our aim in this task is to <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">NOT</span> evaluate the standalone quality of either audio or video. Instead, our focus is on synchrony issues and how humans perceive them.
Since pairwise stimulus experiments have shown to be faster than single-stimuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and have been used in dubbing evaluation to foster relative measures in the rating <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we also present our participants with two videos side-by-side. They are asked to play both videos, compare them, and rate each one on a scale from 1-5 based on the provided guidelines.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Annotation guidelines are shown in Appendix <a href="#Pt0.A2.SS2" title="0.B.2 Annotation Criteria ‣ Appendix 0.B Human Annotation Guidelines ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.B.2</span></a>. Upon publication, we will release the data set and code for creating distorted samples.</span></span></span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">From 18.2K videos, we randomly sampled (with replacement) 20K video pairs for comparison while taking into account the different levels of distortion (c.f. Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Participants conducted pairwise comparisons on these videos and each pair was rated at least three times by different annotators. This resulted in a total of 60,000 pairwise ratings or 120,000 ratings for individual videos. Each video duration being at least 10 seconds, this amounts to 100+ hours of annotated videos with each video annotated at least 3 times.
When looking at the inter-annotator agreement, we divided our data based on the severity of disagreement between the annotators. If all annotators annotated different scores those samples were put in “disagreement” group which constitute nearly 10% of the total annotated data.
Within the disagreement group, we saw about 16% of cases where disagreements were equal or higher than 3, e.g., one video was rated as 1/4/5 by three annotators. These high-disagreement samples were then sent back to the annotators for QA, after which we received their revised annotations. We are unable to directly calculate inter-annotator agreement (like Cohen’s or Fleiss Kappa), as we had a large pool of annotators (80+) and not all annotators annotated all samples. However, we do observe a Krippendorff’s alpha value of 0.71 for the ratings assigned to all the samples, which corresponds to a moderate agreement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">After removing outliers and deduplication we are left with 15.2K videos out of 40K.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Since we randomly sampled with replacement, one video was annotated approx. 2.4 times by annotators, which led to duplication.</span></span></span> We divide this 15.2K into 10.6K for training, 2.3K for development and 2.3K for evaluation. While creating these splits, we ensured that no original files and their derived (i.e., distorted) files are present in more than one set.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">To gain insights into which distortion types posed the most perceptual challenges for humans in terms of synchronization, we examined videos that underwent direct comparisons and analyzed the variations in the annotated scores.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We leave out audio-shift distortion in this analysis as the levels are not linearly increasing or decreasing.</span></span></span> Discrepancies in these scores (absolute values) offered insights into how differently these distortion types were perceived. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Data Annotation ‣ 3 AVS Benchmark Dataset ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> highlights that intermittent muting (type 6) is consistently the most noticeable distortion amongst all types (all high peaks). This observation aligns with the score distribution we observed in AVS benchmark (see Appendix Section <a href="#Pt0.A4" title="Appendix 0.D Score Distribution in AVS Benchmark ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.D</span></a>) and our manual analysis where audio muting was the most disruptive distortion and humans could easily differentiate audio muting from other distortions.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.07336/assets/images/data_annotation_analysis/across_types_meandiffVSdistance.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">We compare absolute differences in annotation score across the distortion types. In this plot, x-axis shows distortion types that were compared, i.e., ‘2 4’ represents distortion type 2 v/s 4 and y-axis represents the difference in scores across annotation tasks. For ID to distortion type mapping see Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Preliminary Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Studies by Unterthiner <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> (Fréchet Video Distance) and Kilgour <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (Fréchet Audio Distance) have demonstrated the efficacy of embeddings from I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> models in capturing qualitative and temporal characteristics from audio and video inputs, respectively. In the absence of dedicated metrics for assessing AV synchrony of ‘in the wild’ content and the lack of baseline models, we formulated a new Fréchet audio-visual distance (FAVD) metric.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>SparseSync <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and AVST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are only able to detect audio shift but our benchmark contain many more synchrony issues which render these metrics unusable.</span></span></span> Additionally, we want to assess if the combination of embeddings from I3D and VGGish models can be useful for detecting synchrony issues.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Fréchet Distance metrics—FID, FAD, FVD, and now FAVD—compare the statistics of generated samples to those of the ground truth or a large collection of samples. In our preliminary experiment, we aimed to explore how FAD, FVD, and FAVD metrics respond to temporal discrepancies in the input data. We conducted our study on 18.2K videos, generating two sets of audio-visual embeddings: one from disrupted content and another representing aligned content (ground truth). For each type and level of distortion, we created a distinct "evaluation set" to analyze the impact of each specific distortion type and level on these metrics.
In our experiments, we deliberately chose not to use FID (Fréchet Inception Distance) due to its limitation in evaluating temporal aspects. FID is designed for static images and cannot adequately assess content with time-dependent elements, such as videos, where scene continuity and flow are essential for quality evaluation.
By focusing on metrics capable of capturing these temporal dynamics, we aim to provide a more accurate and relevant evaluation of the audio-visual content’s quality.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We use I3D and VGGish to extract features from audio-visual content within the same time span (of 0.96 seconds - a default time span from VGGish).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.2" class="ltx_p">These representations are concatenated time-wise and we calculate multivariate Gaussians for both evaluation set embeddings, <math id="S4.p4.1.m1.2" class="ltx_Math" alttext="N_{e}(\mu_{e},\Sigma_{e})" display="inline"><semantics id="S4.p4.1.m1.2a"><mrow id="S4.p4.1.m1.2.2" xref="S4.p4.1.m1.2.2.cmml"><msub id="S4.p4.1.m1.2.2.4" xref="S4.p4.1.m1.2.2.4.cmml"><mi id="S4.p4.1.m1.2.2.4.2" xref="S4.p4.1.m1.2.2.4.2.cmml">N</mi><mi id="S4.p4.1.m1.2.2.4.3" xref="S4.p4.1.m1.2.2.4.3.cmml">e</mi></msub><mo lspace="0em" rspace="0em" id="S4.p4.1.m1.2.2.3" xref="S4.p4.1.m1.2.2.3.cmml">​</mo><mrow id="S4.p4.1.m1.2.2.2.2" xref="S4.p4.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S4.p4.1.m1.2.2.2.2.3" xref="S4.p4.1.m1.2.2.2.3.cmml">(</mo><msub id="S4.p4.1.m1.1.1.1.1.1" xref="S4.p4.1.m1.1.1.1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.1.1.1.2" xref="S4.p4.1.m1.1.1.1.1.1.2.cmml">μ</mi><mi id="S4.p4.1.m1.1.1.1.1.1.3" xref="S4.p4.1.m1.1.1.1.1.1.3.cmml">e</mi></msub><mo id="S4.p4.1.m1.2.2.2.2.4" xref="S4.p4.1.m1.2.2.2.3.cmml">,</mo><msub id="S4.p4.1.m1.2.2.2.2.2" xref="S4.p4.1.m1.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.p4.1.m1.2.2.2.2.2.2" xref="S4.p4.1.m1.2.2.2.2.2.2.cmml">Σ</mi><mi id="S4.p4.1.m1.2.2.2.2.2.3" xref="S4.p4.1.m1.2.2.2.2.2.3.cmml">e</mi></msub><mo stretchy="false" id="S4.p4.1.m1.2.2.2.2.5" xref="S4.p4.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.2b"><apply id="S4.p4.1.m1.2.2.cmml" xref="S4.p4.1.m1.2.2"><times id="S4.p4.1.m1.2.2.3.cmml" xref="S4.p4.1.m1.2.2.3"></times><apply id="S4.p4.1.m1.2.2.4.cmml" xref="S4.p4.1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.p4.1.m1.2.2.4.1.cmml" xref="S4.p4.1.m1.2.2.4">subscript</csymbol><ci id="S4.p4.1.m1.2.2.4.2.cmml" xref="S4.p4.1.m1.2.2.4.2">𝑁</ci><ci id="S4.p4.1.m1.2.2.4.3.cmml" xref="S4.p4.1.m1.2.2.4.3">𝑒</ci></apply><interval closure="open" id="S4.p4.1.m1.2.2.2.3.cmml" xref="S4.p4.1.m1.2.2.2.2"><apply id="S4.p4.1.m1.1.1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.1.1.1.2">𝜇</ci><ci id="S4.p4.1.m1.1.1.1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.1.1.1.3">𝑒</ci></apply><apply id="S4.p4.1.m1.2.2.2.2.2.cmml" xref="S4.p4.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p4.1.m1.2.2.2.2.2.1.cmml" xref="S4.p4.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.p4.1.m1.2.2.2.2.2.2.cmml" xref="S4.p4.1.m1.2.2.2.2.2.2">Σ</ci><ci id="S4.p4.1.m1.2.2.2.2.2.3.cmml" xref="S4.p4.1.m1.2.2.2.2.2.3">𝑒</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.2c">N_{e}(\mu_{e},\Sigma_{e})</annotation></semantics></math>, and the ground truth AV collection embeddings, <math id="S4.p4.2.m2.2" class="ltx_Math" alttext="N_{av}(\mu_{av},\Sigma_{av})" display="inline"><semantics id="S4.p4.2.m2.2a"><mrow id="S4.p4.2.m2.2.2" xref="S4.p4.2.m2.2.2.cmml"><msub id="S4.p4.2.m2.2.2.4" xref="S4.p4.2.m2.2.2.4.cmml"><mi id="S4.p4.2.m2.2.2.4.2" xref="S4.p4.2.m2.2.2.4.2.cmml">N</mi><mrow id="S4.p4.2.m2.2.2.4.3" xref="S4.p4.2.m2.2.2.4.3.cmml"><mi id="S4.p4.2.m2.2.2.4.3.2" xref="S4.p4.2.m2.2.2.4.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.2.2.4.3.1" xref="S4.p4.2.m2.2.2.4.3.1.cmml">​</mo><mi id="S4.p4.2.m2.2.2.4.3.3" xref="S4.p4.2.m2.2.2.4.3.3.cmml">v</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.2.2.3" xref="S4.p4.2.m2.2.2.3.cmml">​</mo><mrow id="S4.p4.2.m2.2.2.2.2" xref="S4.p4.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S4.p4.2.m2.2.2.2.2.3" xref="S4.p4.2.m2.2.2.2.3.cmml">(</mo><msub id="S4.p4.2.m2.1.1.1.1.1" xref="S4.p4.2.m2.1.1.1.1.1.cmml"><mi id="S4.p4.2.m2.1.1.1.1.1.2" xref="S4.p4.2.m2.1.1.1.1.1.2.cmml">μ</mi><mrow id="S4.p4.2.m2.1.1.1.1.1.3" xref="S4.p4.2.m2.1.1.1.1.1.3.cmml"><mi id="S4.p4.2.m2.1.1.1.1.1.3.2" xref="S4.p4.2.m2.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.1.1.1.1.1.3.1" xref="S4.p4.2.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p4.2.m2.1.1.1.1.1.3.3" xref="S4.p4.2.m2.1.1.1.1.1.3.3.cmml">v</mi></mrow></msub><mo id="S4.p4.2.m2.2.2.2.2.4" xref="S4.p4.2.m2.2.2.2.3.cmml">,</mo><msub id="S4.p4.2.m2.2.2.2.2.2" xref="S4.p4.2.m2.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.p4.2.m2.2.2.2.2.2.2" xref="S4.p4.2.m2.2.2.2.2.2.2.cmml">Σ</mi><mrow id="S4.p4.2.m2.2.2.2.2.2.3" xref="S4.p4.2.m2.2.2.2.2.2.3.cmml"><mi id="S4.p4.2.m2.2.2.2.2.2.3.2" xref="S4.p4.2.m2.2.2.2.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p4.2.m2.2.2.2.2.2.3.1" xref="S4.p4.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S4.p4.2.m2.2.2.2.2.2.3.3" xref="S4.p4.2.m2.2.2.2.2.2.3.3.cmml">v</mi></mrow></msub><mo stretchy="false" id="S4.p4.2.m2.2.2.2.2.5" xref="S4.p4.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.2b"><apply id="S4.p4.2.m2.2.2.cmml" xref="S4.p4.2.m2.2.2"><times id="S4.p4.2.m2.2.2.3.cmml" xref="S4.p4.2.m2.2.2.3"></times><apply id="S4.p4.2.m2.2.2.4.cmml" xref="S4.p4.2.m2.2.2.4"><csymbol cd="ambiguous" id="S4.p4.2.m2.2.2.4.1.cmml" xref="S4.p4.2.m2.2.2.4">subscript</csymbol><ci id="S4.p4.2.m2.2.2.4.2.cmml" xref="S4.p4.2.m2.2.2.4.2">𝑁</ci><apply id="S4.p4.2.m2.2.2.4.3.cmml" xref="S4.p4.2.m2.2.2.4.3"><times id="S4.p4.2.m2.2.2.4.3.1.cmml" xref="S4.p4.2.m2.2.2.4.3.1"></times><ci id="S4.p4.2.m2.2.2.4.3.2.cmml" xref="S4.p4.2.m2.2.2.4.3.2">𝑎</ci><ci id="S4.p4.2.m2.2.2.4.3.3.cmml" xref="S4.p4.2.m2.2.2.4.3.3">𝑣</ci></apply></apply><interval closure="open" id="S4.p4.2.m2.2.2.2.3.cmml" xref="S4.p4.2.m2.2.2.2.2"><apply id="S4.p4.2.m2.1.1.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S4.p4.2.m2.1.1.1.1.1.2.cmml" xref="S4.p4.2.m2.1.1.1.1.1.2">𝜇</ci><apply id="S4.p4.2.m2.1.1.1.1.1.3.cmml" xref="S4.p4.2.m2.1.1.1.1.1.3"><times id="S4.p4.2.m2.1.1.1.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.1.1.1.3.1"></times><ci id="S4.p4.2.m2.1.1.1.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.1.1.1.3.2">𝑎</ci><ci id="S4.p4.2.m2.1.1.1.1.1.3.3.cmml" xref="S4.p4.2.m2.1.1.1.1.1.3.3">𝑣</ci></apply></apply><apply id="S4.p4.2.m2.2.2.2.2.2.cmml" xref="S4.p4.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p4.2.m2.2.2.2.2.2.1.cmml" xref="S4.p4.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S4.p4.2.m2.2.2.2.2.2.2.cmml" xref="S4.p4.2.m2.2.2.2.2.2.2">Σ</ci><apply id="S4.p4.2.m2.2.2.2.2.2.3.cmml" xref="S4.p4.2.m2.2.2.2.2.2.3"><times id="S4.p4.2.m2.2.2.2.2.2.3.1.cmml" xref="S4.p4.2.m2.2.2.2.2.2.3.1"></times><ci id="S4.p4.2.m2.2.2.2.2.2.3.2.cmml" xref="S4.p4.2.m2.2.2.2.2.2.3.2">𝑎</ci><ci id="S4.p4.2.m2.2.2.2.2.2.3.3.cmml" xref="S4.p4.2.m2.2.2.2.2.2.3.3">𝑣</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.2c">N_{av}(\mu_{av},\Sigma_{av})</annotation></semantics></math>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">We then compute the Fréchet distance between these two Gaussians, as described by Dowson <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>:</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.4" class="ltx_Math" alttext="F(N_{av},N_{e})=\left|\mu_{av}-\mu_{e}\right|^{2}+\text{\emph{tr}}(\Sigma_{av}+\Sigma_{e}-2(\Sigma_{av}\Sigma_{e})^{\frac{1}{2}})" display="block"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4" xref="S4.E1.m1.4.4.cmml"><mrow id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml"><mi id="S4.E1.m1.2.2.2.4" xref="S4.E1.m1.2.2.2.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.E1.m1.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.3.cmml">(</mo><msub id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml">N</mi><mrow id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.3.3.cmml">v</mi></mrow></msub><mo id="S4.E1.m1.2.2.2.2.2.4" xref="S4.E1.m1.2.2.2.2.3.cmml">,</mo><msub id="S4.E1.m1.2.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.2.cmml"><mi id="S4.E1.m1.2.2.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.2.2.cmml">N</mi><mi id="S4.E1.m1.2.2.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.2.2.3.cmml">e</mi></msub><mo stretchy="false" id="S4.E1.m1.2.2.2.2.2.5" xref="S4.E1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.5" xref="S4.E1.m1.4.4.5.cmml">=</mo><mrow id="S4.E1.m1.4.4.4" xref="S4.E1.m1.4.4.4.cmml"><msup id="S4.E1.m1.3.3.3.1" xref="S4.E1.m1.3.3.3.1.cmml"><mrow id="S4.E1.m1.3.3.3.1.1.1" xref="S4.E1.m1.3.3.3.1.1.2.cmml"><mo id="S4.E1.m1.3.3.3.1.1.1.2" xref="S4.E1.m1.3.3.3.1.1.2.1.cmml">|</mo><mrow id="S4.E1.m1.3.3.3.1.1.1.1" xref="S4.E1.m1.3.3.3.1.1.1.1.cmml"><msub id="S4.E1.m1.3.3.3.1.1.1.1.2" xref="S4.E1.m1.3.3.3.1.1.1.1.2.cmml"><mi id="S4.E1.m1.3.3.3.1.1.1.1.2.2" xref="S4.E1.m1.3.3.3.1.1.1.1.2.2.cmml">μ</mi><mrow id="S4.E1.m1.3.3.3.1.1.1.1.2.3" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.cmml"><mi id="S4.E1.m1.3.3.3.1.1.1.1.2.3.2" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.3.1.1.1.1.2.3.1" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.3.3.3.1.1.1.1.2.3.3" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.3.cmml">v</mi></mrow></msub><mo id="S4.E1.m1.3.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.3.1.1.1.1.1.cmml">−</mo><msub id="S4.E1.m1.3.3.3.1.1.1.1.3" xref="S4.E1.m1.3.3.3.1.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.3.1.1.1.1.3.2" xref="S4.E1.m1.3.3.3.1.1.1.1.3.2.cmml">μ</mi><mi id="S4.E1.m1.3.3.3.1.1.1.1.3.3" xref="S4.E1.m1.3.3.3.1.1.1.1.3.3.cmml">e</mi></msub></mrow><mo id="S4.E1.m1.3.3.3.1.1.1.3" xref="S4.E1.m1.3.3.3.1.1.2.1.cmml">|</mo></mrow><mn id="S4.E1.m1.3.3.3.1.3" xref="S4.E1.m1.3.3.3.1.3.cmml">2</mn></msup><mo id="S4.E1.m1.4.4.4.3" xref="S4.E1.m1.4.4.4.3.cmml">+</mo><mrow id="S4.E1.m1.4.4.4.2" xref="S4.E1.m1.4.4.4.2.cmml"><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.4.4.4.2.3" xref="S4.E1.m1.4.4.4.2.3b.cmml"><em id="S4.E1.m1.4.4.4.2.3.1nest" class="ltx_emph ltx_font_italic">tr</em></mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.4.2.2" xref="S4.E1.m1.4.4.4.2.2.cmml">​</mo><mrow id="S4.E1.m1.4.4.4.2.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.4.2.1.1.2" xref="S4.E1.m1.4.4.4.2.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.4.2.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.cmml"><mrow id="S4.E1.m1.4.4.4.2.1.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.3.cmml"><msub id="S4.E1.m1.4.4.4.2.1.1.1.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.4.4.4.2.1.1.1.3.2.2" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.2.cmml">Σ</mi><mrow id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.1" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.3.cmml">v</mi></mrow></msub><mo id="S4.E1.m1.4.4.4.2.1.1.1.3.1" xref="S4.E1.m1.4.4.4.2.1.1.1.3.1.cmml">+</mo><msub id="S4.E1.m1.4.4.4.2.1.1.1.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3.cmml"><mi mathvariant="normal" id="S4.E1.m1.4.4.4.2.1.1.1.3.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3.2.cmml">Σ</mi><mi id="S4.E1.m1.4.4.4.2.1.1.1.3.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3.3.cmml">e</mi></msub></mrow><mo id="S4.E1.m1.4.4.4.2.1.1.1.2" xref="S4.E1.m1.4.4.4.2.1.1.1.2.cmml">−</mo><mrow id="S4.E1.m1.4.4.4.2.1.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.cmml"><mn id="S4.E1.m1.4.4.4.2.1.1.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.4.2.1.1.1.1.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.2.cmml">​</mo><msup id="S4.E1.m1.4.4.4.2.1.1.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.2.cmml">Σ</mi><mrow id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.3.cmml">v</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.2.cmml">Σ</mi><mi id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.3.cmml">e</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mfrac id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.cmml"><mn id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.2" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.2.cmml">1</mn><mn id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.3" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.3.cmml">2</mn></mfrac></msup></mrow></mrow><mo stretchy="false" id="S4.E1.m1.4.4.4.2.1.1.3" xref="S4.E1.m1.4.4.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.cmml" xref="S4.E1.m1.4.4"><eq id="S4.E1.m1.4.4.5.cmml" xref="S4.E1.m1.4.4.5"></eq><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"><times id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.3"></times><ci id="S4.E1.m1.2.2.2.4.cmml" xref="S4.E1.m1.2.2.2.4">𝐹</ci><interval closure="open" id="S4.E1.m1.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2"><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">𝑁</ci><apply id="S4.E1.m1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3"><times id="S4.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.2">𝑎</ci><ci id="S4.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.3">𝑣</ci></apply></apply><apply id="S4.E1.m1.2.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.2">𝑁</ci><ci id="S4.E1.m1.2.2.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.3">𝑒</ci></apply></interval></apply><apply id="S4.E1.m1.4.4.4.cmml" xref="S4.E1.m1.4.4.4"><plus id="S4.E1.m1.4.4.4.3.cmml" xref="S4.E1.m1.4.4.4.3"></plus><apply id="S4.E1.m1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.1.2.cmml" xref="S4.E1.m1.3.3.3.1">superscript</csymbol><apply id="S4.E1.m1.3.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.3.1.1.1"><abs id="S4.E1.m1.3.3.3.1.1.2.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.2"></abs><apply id="S4.E1.m1.3.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1"><minus id="S4.E1.m1.3.3.3.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.1"></minus><apply id="S4.E1.m1.3.3.3.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.3.3.3.1.1.1.1.2.2.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2.2">𝜇</ci><apply id="S4.E1.m1.3.3.3.1.1.1.1.2.3.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3"><times id="S4.E1.m1.3.3.3.1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.1"></times><ci id="S4.E1.m1.3.3.3.1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.2">𝑎</ci><ci id="S4.E1.m1.3.3.3.1.1.1.1.2.3.3.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.2.3.3">𝑣</ci></apply></apply><apply id="S4.E1.m1.3.3.3.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.3.1.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.3.2">𝜇</ci><ci id="S4.E1.m1.3.3.3.1.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.3.1.1.1.1.3.3">𝑒</ci></apply></apply></apply><cn type="integer" id="S4.E1.m1.3.3.3.1.3.cmml" xref="S4.E1.m1.3.3.3.1.3">2</cn></apply><apply id="S4.E1.m1.4.4.4.2.cmml" xref="S4.E1.m1.4.4.4.2"><times id="S4.E1.m1.4.4.4.2.2.cmml" xref="S4.E1.m1.4.4.4.2.2"></times><ci id="S4.E1.m1.4.4.4.2.3b.cmml" xref="S4.E1.m1.4.4.4.2.3"><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.4.4.4.2.3.cmml" xref="S4.E1.m1.4.4.4.2.3"><em id="S4.E1.m1.4.4.4.2.3.1anest" class="ltx_emph ltx_font_italic">tr</em></mtext></ci><apply id="S4.E1.m1.4.4.4.2.1.1.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1"><minus id="S4.E1.m1.4.4.4.2.1.1.1.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.2"></minus><apply id="S4.E1.m1.4.4.4.2.1.1.1.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3"><plus id="S4.E1.m1.4.4.4.2.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.1"></plus><apply id="S4.E1.m1.4.4.4.2.1.1.1.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.2.1.1.1.3.2.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.4.4.4.2.1.1.1.3.2.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.2">Σ</ci><apply id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3"><times id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.2">𝑎</ci><ci id="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.2.3.3">𝑣</ci></apply></apply><apply id="S4.E1.m1.4.4.4.2.1.1.1.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.2.1.1.1.3.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.4.4.4.2.1.1.1.3.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3.2">Σ</ci><ci id="S4.E1.m1.4.4.4.2.1.1.1.3.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.3.3.3">𝑒</ci></apply></apply><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1"><times id="S4.E1.m1.4.4.4.2.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.2"></times><cn type="integer" id="S4.E1.m1.4.4.4.2.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.3">2</cn><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1">superscript</csymbol><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1"><times id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.1"></times><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.2">Σ</ci><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3"><times id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.2">𝑎</ci><ci id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.2.3.3">𝑣</ci></apply></apply><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.2">Σ</ci><ci id="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.3.3">𝑒</ci></apply></apply><apply id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3"><divide id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3"></divide><cn type="integer" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.2">1</cn><cn type="integer" id="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.4.4.4.2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">F(N_{av},N_{e})=\left|\mu_{av}-\mu_{e}\right|^{2}+\text{\emph{tr}}(\Sigma_{av}+\Sigma_{e}-2(\Sigma_{av}\Sigma_{e})^{\frac{1}{2}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.p6.1" class="ltx_p">where <em id="S4.p6.1.1" class="ltx_emph ltx_font_italic">tr</em> is the trace of the matrix. For FAD and FVD, we compute the embeddings from respective modalities.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Shifting.png" id="S4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F2.1.2.2" class="ltx_text" style="font-size:90%;">Audio Shift</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Speed_Up.png" id="S4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F2.2.2.2" class="ltx_text" style="font-size:90%;">Audio Speed Up</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F2.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Speed_Down.png" id="S4.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F2.3.2.2" class="ltx_text" style="font-size:90%;">Audio Speed Down</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.8.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.9.2" class="ltx_text" style="font-size:90%;">In these plots, we show the effect of distortions with varying levels on <span id="S4.F2.9.2.1" class="ltx_text ltx_font_bold" style="color:#FF8000;">FAD</span>, <span id="S4.F2.9.2.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">FVD</span>, and <span id="S4.F2.9.2.3" class="ltx_text ltx_font_bold" style="color:#00FF00;">FAVD</span> for three distortion types. Flat trend line implies that a metric is not able to capture the distortion type. Increasing or decreasing trends show that a metric is susceptible to varying levels in a distortion. Distortion levels are taken from Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Detailed plots for all distortion types are available in Appendix <a href="#Pt0.A5" title="Appendix 0.E FAD v/s FVD v/s FAVD ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.E</span></a>.</span></figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.2" class="ltx_p">Examining the plots in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Preliminary Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, it becomes evident that only FAVD (depicted in green) is responsive to various temporal distortions associated with AV synchrony across different levels. This is mainly due to use of covariance matrices (<math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\Sigma_{av}" display="inline"><semantics id="S4.p7.1.m1.1a"><msub id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">Σ</mi><mrow id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mi id="S4.p7.1.m1.1.1.3.2" xref="S4.p7.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.1" xref="S4.p7.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p7.1.m1.1.1.3.3" xref="S4.p7.1.m1.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1">subscript</csymbol><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">Σ</ci><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><times id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3.1"></times><ci id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3.2">𝑎</ci><ci id="S4.p7.1.m1.1.1.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\Sigma_{av}</annotation></semantics></math>, <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\Sigma_{e}" display="inline"><semantics id="S4.p7.2.m2.1a"><msub id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">Σ</mi><mi id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1">subscript</csymbol><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">Σ</ci><ci id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\Sigma_{e}</annotation></semantics></math>) from audio-visual features (shown in Eq <a href="#S4.E1" title="Equation 1 ‣ 4 Preliminary Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) which helps FAVD track the interaction of AV modality. Additionally, in cases like audio shift (a) where neither FAD nor FVD can detect synchronization issues (indicated by flat trend lines), and audio speed-up where FAD shows minimal movement, FAVD formulation exhibits a more pronounced reaction, highlighting that the fusion of features from I3D and VGGish models effectively captures temporality in an audio-visual context. Based on these observations, we chose to use I3D and VGGish features as inputs for our metric training, with FAVD serving as a baseline metric for the remainder of our work.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>PEAVS Score Formulation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section describes the architecture we have used to train our ‘PEAVS’ score metric and its training details.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Architecture</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The input to our proposed model are the video and audio streams, as seen in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Architecture ‣ 5 PEAVS Score Formulation ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For feature extraction, we leverage I3D and VGGish.
In addition to this, our design draws inspiration from the audio-visual framework presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Central to our architecture is the utilization of the cross-modal transformer formulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which excels in capturing temporal information and relationships between different modalities. To ensure temporal consistency across modalities, both audio and visual input features are extracted over matched time spans and are then relayed through cross-modal transformer layers.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.7" class="ltx_p">There are several key components in the our architecture. First, the audio and visual inputs are processed through our frozen backbone feature extractors. Both features are extracted over 0.96 second windows.
The I3D extracted inputs <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="x_{v}" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><msub id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">x</mi><mi id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">𝑥</ci><ci id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">x_{v}</annotation></semantics></math> have dimensionality <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="x_{v}\in\mathbb{R}^{N_{av}\times 1024}" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><msub id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2.2" xref="S5.SS1.p2.2.m2.1.1.2.2.cmml">x</mi><mi id="S5.SS1.p2.2.m2.1.1.2.3" xref="S5.SS1.p2.2.m2.1.1.2.3.cmml">v</mi></msub><mo id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml"><mi id="S5.SS1.p2.2.m2.1.1.3.2" xref="S5.SS1.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS1.p2.2.m2.1.1.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.cmml"><msub id="S5.SS1.p2.2.m2.1.1.3.3.2" xref="S5.SS1.p2.2.m2.1.1.3.3.2.cmml"><mi id="S5.SS1.p2.2.m2.1.1.3.3.2.2" xref="S5.SS1.p2.2.m2.1.1.3.3.2.2.cmml">N</mi><mrow id="S5.SS1.p2.2.m2.1.1.3.3.2.3" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.cmml"><mi id="S5.SS1.p2.2.m2.1.1.3.3.2.3.2" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.2.m2.1.1.3.3.2.3.1" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.1.cmml">​</mo><mi id="S5.SS1.p2.2.m2.1.1.3.3.2.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.3.cmml">v</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.2.m2.1.1.3.3.1" xref="S5.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S5.SS1.p2.2.m2.1.1.3.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.3.cmml">1024</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><in id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></in><apply id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.2.1.cmml" xref="S5.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2.2">𝑥</ci><ci id="S5.SS1.p2.2.m2.1.1.2.3.cmml" xref="S5.SS1.p2.2.m2.1.1.2.3">𝑣</ci></apply><apply id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.3.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S5.SS1.p2.2.m2.1.1.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3"><times id="S5.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.1"></times><apply id="S5.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.3.3.2.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.3.3.2.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2.2">𝑁</ci><apply id="S5.SS1.p2.2.m2.1.1.3.3.2.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3"><times id="S5.SS1.p2.2.m2.1.1.3.3.2.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.1"></times><ci id="S5.SS1.p2.2.m2.1.1.3.3.2.3.2.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.2">𝑎</ci><ci id="S5.SS1.p2.2.m2.1.1.3.3.2.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.2.3.3">𝑣</ci></apply></apply><cn type="integer" id="S5.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3.3">1024</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">x_{v}\in\mathbb{R}^{N_{av}\times 1024}</annotation></semantics></math>, where <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="N_{av}" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">N</mi><mrow id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml"><mi id="S5.SS1.p2.3.m3.1.1.3.2" xref="S5.SS1.p2.3.m3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.3.m3.1.1.3.1" xref="S5.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p2.3.m3.1.1.3.3" xref="S5.SS1.p2.3.m3.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝑁</ci><apply id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3"><times id="S5.SS1.p2.3.m3.1.1.3.1.cmml" xref="S5.SS1.p2.3.m3.1.1.3.1"></times><ci id="S5.SS1.p2.3.m3.1.1.3.2.cmml" xref="S5.SS1.p2.3.m3.1.1.3.2">𝑎</ci><ci id="S5.SS1.p2.3.m3.1.1.3.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">N_{av}</annotation></semantics></math> is the dimension of the audiovisual feature sequence, and 1,024 is the feature vector dimension. The acoustic feature vector <math id="S5.SS1.p2.4.m4.1" class="ltx_Math" alttext="x_{a}" display="inline"><semantics id="S5.SS1.p2.4.m4.1a"><msub id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml"><mi id="S5.SS1.p2.4.m4.1.1.2" xref="S5.SS1.p2.4.m4.1.1.2.cmml">x</mi><mi id="S5.SS1.p2.4.m4.1.1.3" xref="S5.SS1.p2.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><apply id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.1.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p2.4.m4.1.1.2.cmml" xref="S5.SS1.p2.4.m4.1.1.2">𝑥</ci><ci id="S5.SS1.p2.4.m4.1.1.3.cmml" xref="S5.SS1.p2.4.m4.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">x_{a}</annotation></semantics></math> extracted from the VGGish model is <math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="x_{a}\in\mathbb{R}^{N_{av}\times 128}" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><mrow id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml"><msub id="S5.SS1.p2.5.m5.1.1.2" xref="S5.SS1.p2.5.m5.1.1.2.cmml"><mi id="S5.SS1.p2.5.m5.1.1.2.2" xref="S5.SS1.p2.5.m5.1.1.2.2.cmml">x</mi><mi id="S5.SS1.p2.5.m5.1.1.2.3" xref="S5.SS1.p2.5.m5.1.1.2.3.cmml">a</mi></msub><mo id="S5.SS1.p2.5.m5.1.1.1" xref="S5.SS1.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S5.SS1.p2.5.m5.1.1.3" xref="S5.SS1.p2.5.m5.1.1.3.cmml"><mi id="S5.SS1.p2.5.m5.1.1.3.2" xref="S5.SS1.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS1.p2.5.m5.1.1.3.3" xref="S5.SS1.p2.5.m5.1.1.3.3.cmml"><msub id="S5.SS1.p2.5.m5.1.1.3.3.2" xref="S5.SS1.p2.5.m5.1.1.3.3.2.cmml"><mi id="S5.SS1.p2.5.m5.1.1.3.3.2.2" xref="S5.SS1.p2.5.m5.1.1.3.3.2.2.cmml">N</mi><mrow id="S5.SS1.p2.5.m5.1.1.3.3.2.3" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.cmml"><mi id="S5.SS1.p2.5.m5.1.1.3.3.2.3.2" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.5.m5.1.1.3.3.2.3.1" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.1.cmml">​</mo><mi id="S5.SS1.p2.5.m5.1.1.3.3.2.3.3" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.3.cmml">v</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.5.m5.1.1.3.3.1" xref="S5.SS1.p2.5.m5.1.1.3.3.1.cmml">×</mo><mn id="S5.SS1.p2.5.m5.1.1.3.3.3" xref="S5.SS1.p2.5.m5.1.1.3.3.3.cmml">128</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><apply id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1"><in id="S5.SS1.p2.5.m5.1.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1.1"></in><apply id="S5.SS1.p2.5.m5.1.1.2.cmml" xref="S5.SS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.2.1.cmml" xref="S5.SS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.2.2.cmml" xref="S5.SS1.p2.5.m5.1.1.2.2">𝑥</ci><ci id="S5.SS1.p2.5.m5.1.1.2.3.cmml" xref="S5.SS1.p2.5.m5.1.1.2.3">𝑎</ci></apply><apply id="S5.SS1.p2.5.m5.1.1.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.3.1.cmml" xref="S5.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.3.2.cmml" xref="S5.SS1.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S5.SS1.p2.5.m5.1.1.3.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3"><times id="S5.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.1"></times><apply id="S5.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.3.3.2.1.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.3.3.2.2.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2.2">𝑁</ci><apply id="S5.SS1.p2.5.m5.1.1.3.3.2.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3"><times id="S5.SS1.p2.5.m5.1.1.3.3.2.3.1.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.1"></times><ci id="S5.SS1.p2.5.m5.1.1.3.3.2.3.2.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.2">𝑎</ci><ci id="S5.SS1.p2.5.m5.1.1.3.3.2.3.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.2.3.3">𝑣</ci></apply></apply><cn type="integer" id="S5.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3.3.3">128</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">x_{a}\in\mathbb{R}^{N_{av}\times 128}</annotation></semantics></math>, where <math id="S5.SS1.p2.6.m6.1" class="ltx_Math" alttext="N_{av}" display="inline"><semantics id="S5.SS1.p2.6.m6.1a"><msub id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml"><mi id="S5.SS1.p2.6.m6.1.1.2" xref="S5.SS1.p2.6.m6.1.1.2.cmml">N</mi><mrow id="S5.SS1.p2.6.m6.1.1.3" xref="S5.SS1.p2.6.m6.1.1.3.cmml"><mi id="S5.SS1.p2.6.m6.1.1.3.2" xref="S5.SS1.p2.6.m6.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.6.m6.1.1.3.1" xref="S5.SS1.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p2.6.m6.1.1.3.3" xref="S5.SS1.p2.6.m6.1.1.3.3.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><apply id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.6.m6.1.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S5.SS1.p2.6.m6.1.1.2.cmml" xref="S5.SS1.p2.6.m6.1.1.2">𝑁</ci><apply id="S5.SS1.p2.6.m6.1.1.3.cmml" xref="S5.SS1.p2.6.m6.1.1.3"><times id="S5.SS1.p2.6.m6.1.1.3.1.cmml" xref="S5.SS1.p2.6.m6.1.1.3.1"></times><ci id="S5.SS1.p2.6.m6.1.1.3.2.cmml" xref="S5.SS1.p2.6.m6.1.1.3.2">𝑎</ci><ci id="S5.SS1.p2.6.m6.1.1.3.3.cmml" xref="S5.SS1.p2.6.m6.1.1.3.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">N_{av}</annotation></semantics></math> is the dimension of the acoustic feature sequence, and 128 is the feature vector dimension. We ensure that all inputs to the cross-modal layers have same dimensionality, by projecting the visual features from 1,024 to 128 through the use of a 1D convolutional layer to produce <math id="S5.SS1.p2.7.m7.1" class="ltx_Math" alttext="\bar{x}_{v}\in\mathbb{R}^{N_{v}\times 128}" display="inline"><semantics id="S5.SS1.p2.7.m7.1a"><mrow id="S5.SS1.p2.7.m7.1.1" xref="S5.SS1.p2.7.m7.1.1.cmml"><msub id="S5.SS1.p2.7.m7.1.1.2" xref="S5.SS1.p2.7.m7.1.1.2.cmml"><mover accent="true" id="S5.SS1.p2.7.m7.1.1.2.2" xref="S5.SS1.p2.7.m7.1.1.2.2.cmml"><mi id="S5.SS1.p2.7.m7.1.1.2.2.2" xref="S5.SS1.p2.7.m7.1.1.2.2.2.cmml">x</mi><mo id="S5.SS1.p2.7.m7.1.1.2.2.1" xref="S5.SS1.p2.7.m7.1.1.2.2.1.cmml">¯</mo></mover><mi id="S5.SS1.p2.7.m7.1.1.2.3" xref="S5.SS1.p2.7.m7.1.1.2.3.cmml">v</mi></msub><mo id="S5.SS1.p2.7.m7.1.1.1" xref="S5.SS1.p2.7.m7.1.1.1.cmml">∈</mo><msup id="S5.SS1.p2.7.m7.1.1.3" xref="S5.SS1.p2.7.m7.1.1.3.cmml"><mi id="S5.SS1.p2.7.m7.1.1.3.2" xref="S5.SS1.p2.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS1.p2.7.m7.1.1.3.3" xref="S5.SS1.p2.7.m7.1.1.3.3.cmml"><msub id="S5.SS1.p2.7.m7.1.1.3.3.2" xref="S5.SS1.p2.7.m7.1.1.3.3.2.cmml"><mi id="S5.SS1.p2.7.m7.1.1.3.3.2.2" xref="S5.SS1.p2.7.m7.1.1.3.3.2.2.cmml">N</mi><mi id="S5.SS1.p2.7.m7.1.1.3.3.2.3" xref="S5.SS1.p2.7.m7.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.7.m7.1.1.3.3.1" xref="S5.SS1.p2.7.m7.1.1.3.3.1.cmml">×</mo><mn id="S5.SS1.p2.7.m7.1.1.3.3.3" xref="S5.SS1.p2.7.m7.1.1.3.3.3.cmml">128</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.1b"><apply id="S5.SS1.p2.7.m7.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1"><in id="S5.SS1.p2.7.m7.1.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1.1"></in><apply id="S5.SS1.p2.7.m7.1.1.2.cmml" xref="S5.SS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.7.m7.1.1.2.1.cmml" xref="S5.SS1.p2.7.m7.1.1.2">subscript</csymbol><apply id="S5.SS1.p2.7.m7.1.1.2.2.cmml" xref="S5.SS1.p2.7.m7.1.1.2.2"><ci id="S5.SS1.p2.7.m7.1.1.2.2.1.cmml" xref="S5.SS1.p2.7.m7.1.1.2.2.1">¯</ci><ci id="S5.SS1.p2.7.m7.1.1.2.2.2.cmml" xref="S5.SS1.p2.7.m7.1.1.2.2.2">𝑥</ci></apply><ci id="S5.SS1.p2.7.m7.1.1.2.3.cmml" xref="S5.SS1.p2.7.m7.1.1.2.3">𝑣</ci></apply><apply id="S5.SS1.p2.7.m7.1.1.3.cmml" xref="S5.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.7.m7.1.1.3.1.cmml" xref="S5.SS1.p2.7.m7.1.1.3">superscript</csymbol><ci id="S5.SS1.p2.7.m7.1.1.3.2.cmml" xref="S5.SS1.p2.7.m7.1.1.3.2">ℝ</ci><apply id="S5.SS1.p2.7.m7.1.1.3.3.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3"><times id="S5.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.1"></times><apply id="S5.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.7.m7.1.1.3.3.2.1.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.2">subscript</csymbol><ci id="S5.SS1.p2.7.m7.1.1.3.3.2.2.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.2.2">𝑁</ci><ci id="S5.SS1.p2.7.m7.1.1.3.3.2.3.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.2.3">𝑣</ci></apply><cn type="integer" id="S5.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S5.SS1.p2.7.m7.1.1.3.3.3">128</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.1c">\bar{x}_{v}\in\mathbb{R}^{N_{v}\times 128}</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Next, we introduce positional embeddings to the feature vectors, following the step adopted in the original transformer paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
The resultant features are used by the model to compute Query, Key, and Value vectors for each modality, as illustrated in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Architecture ‣ 5 PEAVS Score Formulation ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In our model, the Query vector from one modality is paired in the transformer layer with the Key and Value vectors of the other, ensuring a cross-modal interaction.
This design choice allows the cross-modal transformer encoder layers for each modality branch to produce representations for a given modality that are influenced by the other modality. Following this, the output of the cross-modal layers are refined through the self-attention layers to enhance the cross-modal representations. The resultant features are then processed by averaging the outputs from the self-attention layers followed by concatenation of these averaged representations from both modality branches. Finally, a <em id="S5.SS1.p3.1.1" class="ltx_emph ltx_font_italic">multilayer perceptron</em> (MLP) with three fully-connected layers is integrated to produce a prediction score based on these concatenated features.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2404.07336/assets/x1.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="342" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Framework Overview</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As seen in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Architecture ‣ 5 PEAVS Score Formulation ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our training method consists of a two-stage strategy. In stage 1, we pre-train the model using a contrastive learning task, distinguishing between aligned and non-aligned pairs. Afterwards, in Stage Two, we fine-tune the model on our primary task, which involves predicting scores based on human annotators scores.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Stage 1</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">In stage 1, we pre-train our model using a contrastive learning objective. In this phase, we used AudioSet’s <em id="S5.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">Balanced train</em> split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which after downloading all videos possible and removing corrupted ones resulted in 17K videos for our contrastive training stage.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that this is different from AVS benchmark.</span></span></span> We then subjected each of these files to ten levels of temporal misalignment (audio shift), as specified in section <a href="#S3.SS1" title="3.1 Data Preparation and Distortions ‣ 3 AVS Benchmark Dataset ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. This pre-processing resulted in a total of <math id="S5.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mo id="S5.SS2.SSS1.p1.1.m1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">\sim</annotation></semantics></math>190K files for the contrastive learning phase. Similar to SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the training objective ensures that the outputs of both the audio and video branches are similar for aligned pairs and dissimilar for misaligned pairs. Specifically, we either minimize or maximize the Euclidean distance between the network outputs for these cases, respectively. Our objective consists of a contrastive loss (Equation <a href="#S5.E2" title="Equation 2 ‣ 5.2.1 Stage 1 ‣ 5.2 Training ‣ 5 PEAVS Score Formulation ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The primary intention behind this pre-training is to embed regularization into the audio and visual features extracted from our frozen backbone networks.
This ensures that the model prioritizes temporal attributes of the features over their qualitative characteristics.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.5" class="ltx_Math" alttext="\mathcal{L}_{c}=\frac{1}{2N}\sum_{n=1}^{N}(y_{n})D_{n}^{2}+(1-y_{n})(\max(0,m-D_{n}))^{2}" display="block"><semantics id="S5.E2.m1.5a"><mrow id="S5.E2.m1.5.5" xref="S5.E2.m1.5.5.cmml"><msub id="S5.E2.m1.5.5.5" xref="S5.E2.m1.5.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E2.m1.5.5.5.2" xref="S5.E2.m1.5.5.5.2.cmml">ℒ</mi><mi id="S5.E2.m1.5.5.5.3" xref="S5.E2.m1.5.5.5.3.cmml">c</mi></msub><mo id="S5.E2.m1.5.5.4" xref="S5.E2.m1.5.5.4.cmml">=</mo><mrow id="S5.E2.m1.5.5.3" xref="S5.E2.m1.5.5.3.cmml"><mrow id="S5.E2.m1.3.3.1.1" xref="S5.E2.m1.3.3.1.1.cmml"><mfrac id="S5.E2.m1.3.3.1.1.3" xref="S5.E2.m1.3.3.1.1.3.cmml"><mn id="S5.E2.m1.3.3.1.1.3.2" xref="S5.E2.m1.3.3.1.1.3.2.cmml">1</mn><mrow id="S5.E2.m1.3.3.1.1.3.3" xref="S5.E2.m1.3.3.1.1.3.3.cmml"><mn id="S5.E2.m1.3.3.1.1.3.3.2" xref="S5.E2.m1.3.3.1.1.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S5.E2.m1.3.3.1.1.3.3.1" xref="S5.E2.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S5.E2.m1.3.3.1.1.3.3.3" xref="S5.E2.m1.3.3.1.1.3.3.3.cmml">N</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S5.E2.m1.3.3.1.1.2" xref="S5.E2.m1.3.3.1.1.2.cmml">​</mo><mrow id="S5.E2.m1.3.3.1.1.1" xref="S5.E2.m1.3.3.1.1.1.cmml"><munderover id="S5.E2.m1.3.3.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S5.E2.m1.3.3.1.1.1.2.2.2" xref="S5.E2.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E2.m1.3.3.1.1.1.2.2.3" xref="S5.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.2.2.3.2" xref="S5.E2.m1.3.3.1.1.1.2.2.3.2.cmml">n</mi><mo id="S5.E2.m1.3.3.1.1.1.2.2.3.1" xref="S5.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E2.m1.3.3.1.1.1.2.2.3.3" xref="S5.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E2.m1.3.3.1.1.1.2.3" xref="S5.E2.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S5.E2.m1.3.3.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E2.m1.3.3.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E2.m1.3.3.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml">n</mi></msub><mo stretchy="false" id="S5.E2.m1.3.3.1.1.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.E2.m1.3.3.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.2.cmml">​</mo><msubsup id="S5.E2.m1.3.3.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.3.2.2" xref="S5.E2.m1.3.3.1.1.1.1.3.2.2.cmml">D</mi><mi id="S5.E2.m1.3.3.1.1.1.1.3.2.3" xref="S5.E2.m1.3.3.1.1.1.1.3.2.3.cmml">n</mi><mn id="S5.E2.m1.3.3.1.1.1.1.3.3" xref="S5.E2.m1.3.3.1.1.1.1.3.3.cmml">2</mn></msubsup></mrow></mrow></mrow><mo id="S5.E2.m1.5.5.3.4" xref="S5.E2.m1.5.5.3.4.cmml">+</mo><mrow id="S5.E2.m1.5.5.3.3" xref="S5.E2.m1.5.5.3.3.cmml"><mrow id="S5.E2.m1.4.4.2.2.1.1" xref="S5.E2.m1.4.4.2.2.1.1.1.cmml"><mo stretchy="false" id="S5.E2.m1.4.4.2.2.1.1.2" xref="S5.E2.m1.4.4.2.2.1.1.1.cmml">(</mo><mrow id="S5.E2.m1.4.4.2.2.1.1.1" xref="S5.E2.m1.4.4.2.2.1.1.1.cmml"><mn id="S5.E2.m1.4.4.2.2.1.1.1.2" xref="S5.E2.m1.4.4.2.2.1.1.1.2.cmml">1</mn><mo id="S5.E2.m1.4.4.2.2.1.1.1.1" xref="S5.E2.m1.4.4.2.2.1.1.1.1.cmml">−</mo><msub id="S5.E2.m1.4.4.2.2.1.1.1.3" xref="S5.E2.m1.4.4.2.2.1.1.1.3.cmml"><mi id="S5.E2.m1.4.4.2.2.1.1.1.3.2" xref="S5.E2.m1.4.4.2.2.1.1.1.3.2.cmml">y</mi><mi id="S5.E2.m1.4.4.2.2.1.1.1.3.3" xref="S5.E2.m1.4.4.2.2.1.1.1.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S5.E2.m1.4.4.2.2.1.1.3" xref="S5.E2.m1.4.4.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.E2.m1.5.5.3.3.3" xref="S5.E2.m1.5.5.3.3.3.cmml">​</mo><msup id="S5.E2.m1.5.5.3.3.2" xref="S5.E2.m1.5.5.3.3.2.cmml"><mrow id="S5.E2.m1.5.5.3.3.2.1.1" xref="S5.E2.m1.5.5.3.3.2.cmml"><mo stretchy="false" id="S5.E2.m1.5.5.3.3.2.1.1.2" xref="S5.E2.m1.5.5.3.3.2.cmml">(</mo><mrow id="S5.E2.m1.5.5.3.3.2.1.1.1.1" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml"><mi id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml">max</mi><mo id="S5.E2.m1.5.5.3.3.2.1.1.1.1a" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml">⁡</mo><mrow id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml"><mo stretchy="false" id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.2" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml">(</mo><mn id="S5.E2.m1.2.2" xref="S5.E2.m1.2.2.cmml">0</mn><mo id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.3" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml">,</mo><mrow id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.2" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.2.cmml">m</mi><mo id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.1" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.cmml"><mi id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.2" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.2.cmml">D</mi><mi id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.3" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.4" xref="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S5.E2.m1.5.5.3.3.2.1.1.3" xref="S5.E2.m1.5.5.3.3.2.cmml">)</mo></mrow><mn id="S5.E2.m1.5.5.3.3.2.3" xref="S5.E2.m1.5.5.3.3.2.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.5b"><apply id="S5.E2.m1.5.5.cmml" xref="S5.E2.m1.5.5"><eq id="S5.E2.m1.5.5.4.cmml" xref="S5.E2.m1.5.5.4"></eq><apply id="S5.E2.m1.5.5.5.cmml" xref="S5.E2.m1.5.5.5"><csymbol cd="ambiguous" id="S5.E2.m1.5.5.5.1.cmml" xref="S5.E2.m1.5.5.5">subscript</csymbol><ci id="S5.E2.m1.5.5.5.2.cmml" xref="S5.E2.m1.5.5.5.2">ℒ</ci><ci id="S5.E2.m1.5.5.5.3.cmml" xref="S5.E2.m1.5.5.5.3">𝑐</ci></apply><apply id="S5.E2.m1.5.5.3.cmml" xref="S5.E2.m1.5.5.3"><plus id="S5.E2.m1.5.5.3.4.cmml" xref="S5.E2.m1.5.5.3.4"></plus><apply id="S5.E2.m1.3.3.1.1.cmml" xref="S5.E2.m1.3.3.1.1"><times id="S5.E2.m1.3.3.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.2"></times><apply id="S5.E2.m1.3.3.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.3"><divide id="S5.E2.m1.3.3.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.3"></divide><cn type="integer" id="S5.E2.m1.3.3.1.1.3.2.cmml" xref="S5.E2.m1.3.3.1.1.3.2">1</cn><apply id="S5.E2.m1.3.3.1.1.3.3.cmml" xref="S5.E2.m1.3.3.1.1.3.3"><times id="S5.E2.m1.3.3.1.1.3.3.1.cmml" xref="S5.E2.m1.3.3.1.1.3.3.1"></times><cn type="integer" id="S5.E2.m1.3.3.1.1.3.3.2.cmml" xref="S5.E2.m1.3.3.1.1.3.3.2">2</cn><ci id="S5.E2.m1.3.3.1.1.3.3.3.cmml" xref="S5.E2.m1.3.3.1.1.3.3.3">𝑁</ci></apply></apply><apply id="S5.E2.m1.3.3.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1"><apply id="S5.E2.m1.3.3.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S5.E2.m1.3.3.1.1.1.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S5.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.2.2.2"></sum><apply id="S5.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.2.2.3"><eq id="S5.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S5.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.2.2.3.2">𝑛</ci><cn type="integer" id="S5.E2.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S5.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E2.m1.3.3.1.1.1.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1"><times id="S5.E2.m1.3.3.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.2"></times><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3">𝑛</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3">superscript</csymbol><apply id="S5.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3.2.2">𝐷</ci><ci id="S5.E2.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3.2.3">𝑛</ci></apply><cn type="integer" id="S5.E2.m1.3.3.1.1.1.1.3.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.3.3">2</cn></apply></apply></apply></apply><apply id="S5.E2.m1.5.5.3.3.cmml" xref="S5.E2.m1.5.5.3.3"><times id="S5.E2.m1.5.5.3.3.3.cmml" xref="S5.E2.m1.5.5.3.3.3"></times><apply id="S5.E2.m1.4.4.2.2.1.1.1.cmml" xref="S5.E2.m1.4.4.2.2.1.1"><minus id="S5.E2.m1.4.4.2.2.1.1.1.1.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.1"></minus><cn type="integer" id="S5.E2.m1.4.4.2.2.1.1.1.2.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.2">1</cn><apply id="S5.E2.m1.4.4.2.2.1.1.1.3.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.4.4.2.2.1.1.1.3.1.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.4.4.2.2.1.1.1.3.2.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.3.2">𝑦</ci><ci id="S5.E2.m1.4.4.2.2.1.1.1.3.3.cmml" xref="S5.E2.m1.4.4.2.2.1.1.1.3.3">𝑛</ci></apply></apply><apply id="S5.E2.m1.5.5.3.3.2.cmml" xref="S5.E2.m1.5.5.3.3.2"><csymbol cd="ambiguous" id="S5.E2.m1.5.5.3.3.2.2.cmml" xref="S5.E2.m1.5.5.3.3.2">superscript</csymbol><apply id="S5.E2.m1.5.5.3.3.2.1.1.1.2.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1"><max id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"></max><cn type="integer" id="S5.E2.m1.2.2.cmml" xref="S5.E2.m1.2.2">0</cn><apply id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1"><minus id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.1"></minus><ci id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.2">𝑚</ci><apply id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.2">𝐷</ci><ci id="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.5.5.3.3.2.1.1.1.1.1.1.3.3">𝑛</ci></apply></apply></apply><cn type="integer" id="S5.E2.m1.5.5.3.3.2.3.cmml" xref="S5.E2.m1.5.5.3.3.2.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.5c">\mathcal{L}_{c}=\frac{1}{2N}\sum_{n=1}^{N}(y_{n})D_{n}^{2}+(1-y_{n})(\max(0,m-D_{n}))^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS2.SSS1.p2.3" class="ltx_p">where <math id="S5.SS2.SSS1.p2.1.m1.2" class="ltx_Math" alttext="y\in\{0,1\}" display="inline"><semantics id="S5.SS2.SSS1.p2.1.m1.2a"><mrow id="S5.SS2.SSS1.p2.1.m1.2.3" xref="S5.SS2.SSS1.p2.1.m1.2.3.cmml"><mi id="S5.SS2.SSS1.p2.1.m1.2.3.2" xref="S5.SS2.SSS1.p2.1.m1.2.3.2.cmml">y</mi><mo id="S5.SS2.SSS1.p2.1.m1.2.3.1" xref="S5.SS2.SSS1.p2.1.m1.2.3.1.cmml">∈</mo><mrow id="S5.SS2.SSS1.p2.1.m1.2.3.3.2" xref="S5.SS2.SSS1.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S5.SS2.SSS1.p2.1.m1.2.3.3.2.1" xref="S5.SS2.SSS1.p2.1.m1.2.3.3.1.cmml">{</mo><mn id="S5.SS2.SSS1.p2.1.m1.1.1" xref="S5.SS2.SSS1.p2.1.m1.1.1.cmml">0</mn><mo id="S5.SS2.SSS1.p2.1.m1.2.3.3.2.2" xref="S5.SS2.SSS1.p2.1.m1.2.3.3.1.cmml">,</mo><mn id="S5.SS2.SSS1.p2.1.m1.2.2" xref="S5.SS2.SSS1.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S5.SS2.SSS1.p2.1.m1.2.3.3.2.3" xref="S5.SS2.SSS1.p2.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.1.m1.2b"><apply id="S5.SS2.SSS1.p2.1.m1.2.3.cmml" xref="S5.SS2.SSS1.p2.1.m1.2.3"><in id="S5.SS2.SSS1.p2.1.m1.2.3.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.2.3.1"></in><ci id="S5.SS2.SSS1.p2.1.m1.2.3.2.cmml" xref="S5.SS2.SSS1.p2.1.m1.2.3.2">𝑦</ci><set id="S5.SS2.SSS1.p2.1.m1.2.3.3.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.2.3.3.2"><cn type="integer" id="S5.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1">0</cn><cn type="integer" id="S5.SS2.SSS1.p2.1.m1.2.2.cmml" xref="S5.SS2.SSS1.p2.1.m1.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.1.m1.2c">y\in\{0,1\}</annotation></semantics></math> is the label (1 for similar pairs and 0 for dissimilar pairs), <math id="S5.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="D_{w}" display="inline"><semantics id="S5.SS2.SSS1.p2.2.m2.1a"><msub id="S5.SS2.SSS1.p2.2.m2.1.1" xref="S5.SS2.SSS1.p2.2.m2.1.1.cmml"><mi id="S5.SS2.SSS1.p2.2.m2.1.1.2" xref="S5.SS2.SSS1.p2.2.m2.1.1.2.cmml">D</mi><mi id="S5.SS2.SSS1.p2.2.m2.1.1.3" xref="S5.SS2.SSS1.p2.2.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.2.m2.1b"><apply id="S5.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.SSS1.p2.2.m2.1.1.2.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1.2">𝐷</ci><ci id="S5.SS2.SSS1.p2.2.m2.1.1.3.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.2.m2.1c">D_{w}</annotation></semantics></math> is the Euclidean distance between the two data points in the pair after passing them through the network,
and <math id="S5.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.SS2.SSS1.p2.3.m3.1a"><mi id="S5.SS2.SSS1.p2.3.m3.1.1" xref="S5.SS2.SSS1.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.3.m3.1b"><ci id="S5.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.3.m3.1c">m</annotation></semantics></math> is the margin.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Stage 2</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p1.6" class="ltx_p">In stage 2 the output modality heads of the pre-trained model are concatenated together to produce audio-visual representations. Three fully-connected layers are added on top of these concatenated heads.
We then fine tune the entire model for the downstream task of predicting human-alignment scores. Since the aggregated human annotation scores are non-negative real numbers, we approach model training as a regression task and utilize <em id="S5.SS2.SSS2.p1.6.1" class="ltx_emph ltx_font_italic">concordance correlation coefficient</em> (CCC) to measure the agreement between the true and predicted scores.
The CCC measurement is illustrated in Equation <a href="#S5.E3" title="Equation 3 ‣ 5.2.2 Stage 2 ‣ 5.2 Training ‣ 5 PEAVS Score Formulation ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.1" class="ltx_Math" alttext="\mathcal{L}_{CCC}=\frac{2\rho\sigma_{x}\sigma_{y}}{\sigma_{x}^{2}+\sigma_{y}^{2}+(\mu_{x}-\mu_{y})^{2}}" display="block"><semantics id="S5.E3.m1.1a"><mrow id="S5.E3.m1.1.2" xref="S5.E3.m1.1.2.cmml"><msub id="S5.E3.m1.1.2.2" xref="S5.E3.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E3.m1.1.2.2.2" xref="S5.E3.m1.1.2.2.2.cmml">ℒ</mi><mrow id="S5.E3.m1.1.2.2.3" xref="S5.E3.m1.1.2.2.3.cmml"><mi id="S5.E3.m1.1.2.2.3.2" xref="S5.E3.m1.1.2.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.2.2.3.1" xref="S5.E3.m1.1.2.2.3.1.cmml">​</mo><mi id="S5.E3.m1.1.2.2.3.3" xref="S5.E3.m1.1.2.2.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.2.2.3.1a" xref="S5.E3.m1.1.2.2.3.1.cmml">​</mo><mi id="S5.E3.m1.1.2.2.3.4" xref="S5.E3.m1.1.2.2.3.4.cmml">C</mi></mrow></msub><mo id="S5.E3.m1.1.2.1" xref="S5.E3.m1.1.2.1.cmml">=</mo><mfrac id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml"><mrow id="S5.E3.m1.1.1.3" xref="S5.E3.m1.1.1.3.cmml"><mn id="S5.E3.m1.1.1.3.2" xref="S5.E3.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.1" xref="S5.E3.m1.1.1.3.1.cmml">​</mo><mi id="S5.E3.m1.1.1.3.3" xref="S5.E3.m1.1.1.3.3.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.1a" xref="S5.E3.m1.1.1.3.1.cmml">​</mo><msub id="S5.E3.m1.1.1.3.4" xref="S5.E3.m1.1.1.3.4.cmml"><mi id="S5.E3.m1.1.1.3.4.2" xref="S5.E3.m1.1.1.3.4.2.cmml">σ</mi><mi id="S5.E3.m1.1.1.3.4.3" xref="S5.E3.m1.1.1.3.4.3.cmml">x</mi></msub><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.3.1b" xref="S5.E3.m1.1.1.3.1.cmml">​</mo><msub id="S5.E3.m1.1.1.3.5" xref="S5.E3.m1.1.1.3.5.cmml"><mi id="S5.E3.m1.1.1.3.5.2" xref="S5.E3.m1.1.1.3.5.2.cmml">σ</mi><mi id="S5.E3.m1.1.1.3.5.3" xref="S5.E3.m1.1.1.3.5.3.cmml">y</mi></msub></mrow><mrow id="S5.E3.m1.1.1.1" xref="S5.E3.m1.1.1.1.cmml"><msubsup id="S5.E3.m1.1.1.1.3" xref="S5.E3.m1.1.1.1.3.cmml"><mi id="S5.E3.m1.1.1.1.3.2.2" xref="S5.E3.m1.1.1.1.3.2.2.cmml">σ</mi><mi id="S5.E3.m1.1.1.1.3.2.3" xref="S5.E3.m1.1.1.1.3.2.3.cmml">x</mi><mn id="S5.E3.m1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.3.3.cmml">2</mn></msubsup><mo id="S5.E3.m1.1.1.1.2" xref="S5.E3.m1.1.1.1.2.cmml">+</mo><msubsup id="S5.E3.m1.1.1.1.4" xref="S5.E3.m1.1.1.1.4.cmml"><mi id="S5.E3.m1.1.1.1.4.2.2" xref="S5.E3.m1.1.1.1.4.2.2.cmml">σ</mi><mi id="S5.E3.m1.1.1.1.4.2.3" xref="S5.E3.m1.1.1.1.4.2.3.cmml">y</mi><mn id="S5.E3.m1.1.1.1.4.3" xref="S5.E3.m1.1.1.1.4.3.cmml">2</mn></msubsup><mo id="S5.E3.m1.1.1.1.2a" xref="S5.E3.m1.1.1.1.2.cmml">+</mo><msup id="S5.E3.m1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E3.m1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E3.m1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S5.E3.m1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.2.2" xref="S5.E3.m1.1.1.1.1.1.1.1.2.2.cmml">μ</mi><mi id="S5.E3.m1.1.1.1.1.1.1.1.2.3" xref="S5.E3.m1.1.1.1.1.1.1.1.2.3.cmml">x</mi></msub><mo id="S5.E3.m1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S5.E3.m1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E3.m1.1.1.1.1.1.1.1.3.2" xref="S5.E3.m1.1.1.1.1.1.1.1.3.2.cmml">μ</mi><mi id="S5.E3.m1.1.1.1.1.1.1.1.3.3" xref="S5.E3.m1.1.1.1.1.1.1.1.3.3.cmml">y</mi></msub></mrow><mo stretchy="false" id="S5.E3.m1.1.1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.E3.m1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.1b"><apply id="S5.E3.m1.1.2.cmml" xref="S5.E3.m1.1.2"><eq id="S5.E3.m1.1.2.1.cmml" xref="S5.E3.m1.1.2.1"></eq><apply id="S5.E3.m1.1.2.2.cmml" xref="S5.E3.m1.1.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.2.2.1.cmml" xref="S5.E3.m1.1.2.2">subscript</csymbol><ci id="S5.E3.m1.1.2.2.2.cmml" xref="S5.E3.m1.1.2.2.2">ℒ</ci><apply id="S5.E3.m1.1.2.2.3.cmml" xref="S5.E3.m1.1.2.2.3"><times id="S5.E3.m1.1.2.2.3.1.cmml" xref="S5.E3.m1.1.2.2.3.1"></times><ci id="S5.E3.m1.1.2.2.3.2.cmml" xref="S5.E3.m1.1.2.2.3.2">𝐶</ci><ci id="S5.E3.m1.1.2.2.3.3.cmml" xref="S5.E3.m1.1.2.2.3.3">𝐶</ci><ci id="S5.E3.m1.1.2.2.3.4.cmml" xref="S5.E3.m1.1.2.2.3.4">𝐶</ci></apply></apply><apply id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"><divide id="S5.E3.m1.1.1.2.cmml" xref="S5.E3.m1.1.1"></divide><apply id="S5.E3.m1.1.1.3.cmml" xref="S5.E3.m1.1.1.3"><times id="S5.E3.m1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.3.1"></times><cn type="integer" id="S5.E3.m1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.3.2">2</cn><ci id="S5.E3.m1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.3.3">𝜌</ci><apply id="S5.E3.m1.1.1.3.4.cmml" xref="S5.E3.m1.1.1.3.4"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.4.1.cmml" xref="S5.E3.m1.1.1.3.4">subscript</csymbol><ci id="S5.E3.m1.1.1.3.4.2.cmml" xref="S5.E3.m1.1.1.3.4.2">𝜎</ci><ci id="S5.E3.m1.1.1.3.4.3.cmml" xref="S5.E3.m1.1.1.3.4.3">𝑥</ci></apply><apply id="S5.E3.m1.1.1.3.5.cmml" xref="S5.E3.m1.1.1.3.5"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.3.5.1.cmml" xref="S5.E3.m1.1.1.3.5">subscript</csymbol><ci id="S5.E3.m1.1.1.3.5.2.cmml" xref="S5.E3.m1.1.1.3.5.2">𝜎</ci><ci id="S5.E3.m1.1.1.3.5.3.cmml" xref="S5.E3.m1.1.1.3.5.3">𝑦</ci></apply></apply><apply id="S5.E3.m1.1.1.1.cmml" xref="S5.E3.m1.1.1.1"><plus id="S5.E3.m1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.2"></plus><apply id="S5.E3.m1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.3">superscript</csymbol><apply id="S5.E3.m1.1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.3.2.1.cmml" xref="S5.E3.m1.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.1.1.1.3.2.2.cmml" xref="S5.E3.m1.1.1.1.3.2.2">𝜎</ci><ci id="S5.E3.m1.1.1.1.3.2.3.cmml" xref="S5.E3.m1.1.1.1.3.2.3">𝑥</ci></apply><cn type="integer" id="S5.E3.m1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.3.3">2</cn></apply><apply id="S5.E3.m1.1.1.1.4.cmml" xref="S5.E3.m1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.4.1.cmml" xref="S5.E3.m1.1.1.1.4">superscript</csymbol><apply id="S5.E3.m1.1.1.1.4.2.cmml" xref="S5.E3.m1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.4.2.1.cmml" xref="S5.E3.m1.1.1.1.4">subscript</csymbol><ci id="S5.E3.m1.1.1.1.4.2.2.cmml" xref="S5.E3.m1.1.1.1.4.2.2">𝜎</ci><ci id="S5.E3.m1.1.1.1.4.2.3.cmml" xref="S5.E3.m1.1.1.1.4.2.3">𝑦</ci></apply><cn type="integer" id="S5.E3.m1.1.1.1.4.3.cmml" xref="S5.E3.m1.1.1.1.4.3">2</cn></apply><apply id="S5.E3.m1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1">superscript</csymbol><apply id="S5.E3.m1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1"><minus id="S5.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.1"></minus><apply id="S5.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.2.2">𝜇</ci><ci id="S5.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S5.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.3.2">𝜇</ci><ci id="S5.E3.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.1.1.1.1.1.1.1.3.3">𝑦</ci></apply></apply><cn type="integer" id="S5.E3.m1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.1c">\mathcal{L}_{CCC}=\frac{2\rho\sigma_{x}\sigma_{y}}{\sigma_{x}^{2}+\sigma_{y}^{2}+(\mu_{x}-\mu_{y})^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S5.SS2.SSS2.p1.5" class="ltx_p">where <math id="S5.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mu_{x}" display="inline"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><msub id="S5.SS2.SSS2.p1.1.m1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS2.p1.1.m1.1.1.2" xref="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml">μ</mi><mi id="S5.SS2.SSS2.p1.1.m1.1.1.3" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.1.m1.1b"><apply id="S5.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.2">𝜇</ci><ci id="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">\mu_{x}</annotation></semantics></math>
and <math id="S5.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mu_{y}" display="inline"><semantics id="S5.SS2.SSS2.p1.2.m2.1a"><msub id="S5.SS2.SSS2.p1.2.m2.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.SSS2.p1.2.m2.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml">μ</mi><mi id="S5.SS2.SSS2.p1.2.m2.1.1.3" xref="S5.SS2.SSS2.p1.2.m2.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.2.m2.1b"><apply id="S5.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.2">𝜇</ci><ci id="S5.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.2.m2.1c">\mu_{y}</annotation></semantics></math> denote the means of the true and predicted scores of a batch, respectively;
<math id="S5.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\sigma_{x}" display="inline"><semantics id="S5.SS2.SSS2.p1.3.m3.1a"><msub id="S5.SS2.SSS2.p1.3.m3.1.1" xref="S5.SS2.SSS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.SSS2.p1.3.m3.1.1.2" xref="S5.SS2.SSS2.p1.3.m3.1.1.2.cmml">σ</mi><mi id="S5.SS2.SSS2.p1.3.m3.1.1.3" xref="S5.SS2.SSS2.p1.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.3.m3.1b"><apply id="S5.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1.2">𝜎</ci><ci id="S5.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.3.m3.1c">\sigma_{x}</annotation></semantics></math> and <math id="S5.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\sigma_{y}" display="inline"><semantics id="S5.SS2.SSS2.p1.4.m4.1a"><msub id="S5.SS2.SSS2.p1.4.m4.1.1" xref="S5.SS2.SSS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.SSS2.p1.4.m4.1.1.2" xref="S5.SS2.SSS2.p1.4.m4.1.1.2.cmml">σ</mi><mi id="S5.SS2.SSS2.p1.4.m4.1.1.3" xref="S5.SS2.SSS2.p1.4.m4.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.4.m4.1b"><apply id="S5.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1.2">𝜎</ci><ci id="S5.SS2.SSS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.4.m4.1c">\sigma_{y}</annotation></semantics></math> represent the standard deviations of the true and
predicted scores of the batch, respectively; and <math id="S5.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.SS2.SSS2.p1.5.m5.1a"><mi id="S5.SS2.SSS2.p1.5.m5.1.1" xref="S5.SS2.SSS2.p1.5.m5.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.5.m5.1b"><ci id="S5.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.5.m5.1c">\rho</annotation></semantics></math> is their Pearson’s correlation coefficient.
Our training objective is to maximize CCC, ensuring that the predicted scores
closely correlate with the true scores.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Training Details</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">In stage 1, we divided the video files obtained from AudioSet’s <em id="S5.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">Balanced train</em> split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> into an 80% training set and a 20% validation set. This 80/20 split is carried out on the 17K original videos and extrapolated to distorted samples. We pre-trained the model in stage 1 until the validation loss plateaued. In stage 2, we use the train/validation/testing sets provided in the AVS benchmark corpus. We fine-tuned the model on training data for 20 epochs, saving the best-performing model based on the validation set performance.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>For further details on the pre-training and fine-tuning settings, refer to appendix <a href="#Pt0.A1" title="Appendix 0.A Model Training Settings ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.A</span></a>.</span></span></span></p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we evaluate the performance of our trained model using human-evaluation scores. We compare the model’s scores with those provided by human evaluators on the <span id="S6.p1.1.1" class="ltx_text ltx_font_bold">test split</span> of AVS benchmark. To further assess the effectiveness of our cross-modal transformer architecture for metric training, we run two ablation experiments.
First, to assess the impact of pre-training in stage 1, we directly train our model from scratch as in stage 2. This variant is termed “Cross-Modal Base” as it incorporates the same audiovisual cross-modal framework but excludes the contrastive learning pre-training phase.
Secondly, to assess the importance of cross-modal framework, we test a version without cross-modal layers called “Basic Transformer”. This version employs only the self-attention branches, differently from the cross-modal approach of sharing query vector from one modality to another.</p>
</div>
<figure id="S6.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.07336/assets/x2.png" id="S4.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="273" height="81" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.07336/assets/x3.png" id="S6.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="229" height="81" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F4.7.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S6.F4.3.1" class="ltx_text" style="font-size:90%;">PEAVS results with Human Evaluation Scores compared side-by-side for each distortion type and at different levels. PEAVS scores are represented by <span id="S6.F4.3.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">blue</span> bars and Human Evaluation scores are represented by <span id="S6.F4.3.1.2" class="ltx_text ltx_font_bold" style="color:#FF8000;">orange</span> bars. The x-axis are marked by the parameter and its levels as shown in Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. <math id="S6.F4.3.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S6.F4.3.1.m1.1b"><mi id="S6.F4.3.1.m1.1.1" xref="S6.F4.3.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S6.F4.3.1.m1.1c"><ci id="S6.F4.3.1.m1.1.1.cmml" xref="S6.F4.3.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.3.1.m1.1d">\rho</annotation></semantics></math> is the correlation of PEAVS with human judgements.</span></figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Human Scores Correlation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For the above models, we compute the correlation of their outputs with the human judgements using pearson correlation coefficient (PCC). Since Fréchet based metrics require a set of videos to calculate distance between multivariate Gaussians we compute set level correlations. As PEAVS and its variants produce a score for each video separately, we average the scores across the test set to compute set level scores.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Each set is defined as a collection of videos showing same distortion type and level. In total we had 90 sets, i.e. 9 distortions x 10 levels.</span></span></span> We also provide correlations at a much fine-grained clip level. Examining the results presented in Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Human Scores Correlation ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, FAVD shows a relative improvement of 11% (from 0.475 to 0.527) and 64% (from 0.321 to 0.527) over FAD and FVD baselines setting a strong baseline for future work. This highlights FAVD’s effectiveness in capturing AV synchrony issues.
PEAVS model achieves a Pearson correlation of 0.794 at the Set Level, marking a substantial 51% improvement over the robust FAVD baseline. Notably, when we analyze the impact of pre-training on PEAVS (c.f. Cross-Modal Base), performance drops by 7% &amp; 11% at set and clip level, respectively, underscoring the significance of pre-training in PEAVS. Furthermore, removing the cross-modal layers (c.f. Base Transformer) results in a performance decrease of 9.4% &amp; 19% at set and clip level, respectively, emphasizing the critical role of cross-modality layers.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.2.1.1" class="ltx_tr">
<th id="S6.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Model Type</span></th>
<th id="S6.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">Set Level</span></th>
<th id="S6.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S6.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">Clip Level</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.2.2.1" class="ltx_tr">
<th id="S6.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</th>
<td id="S6.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.321</td>
<td id="S6.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T2.2.3.2" class="ltx_tr">
<th id="S6.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<td id="S6.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.475</td>
<td id="S6.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T2.2.4.3" class="ltx_tr">
<th id="S6.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FAVD (Ours)</th>
<td id="S6.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.527</td>
<td id="S6.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T2.2.5.4" class="ltx_tr">
<th id="S6.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Basic Transformer</th>
<td id="S6.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.720</td>
<td id="S6.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.434</td>
</tr>
<tr id="S6.T2.2.6.5" class="ltx_tr">
<th id="S6.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Cross-Modal Base</th>
<td id="S6.T2.2.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.736</td>
<td id="S6.T2.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.477</td>
</tr>
<tr id="S6.T2.2.7.6" class="ltx_tr">
<th id="S6.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">PEAVS (Ours)</th>
<td id="S6.T2.2.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T2.2.7.6.2.1" class="ltx_text ltx_font_bold">0.794</span></td>
<td id="S6.T2.2.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S6.T2.2.7.6.3.1" class="ltx_text ltx_font_bold">0.536</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.4.2" class="ltx_text" style="font-size:90%;">Correlation scores of different model types at Set and Clip levels.</span></figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Per Distortion Analysis</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In this section, we analyze the response of PEAVS to varying distortions at different levels. Figure <a href="#S6.F4" title="Figure 4 ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> factorizes the results in Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Human Scores Correlation ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, by each distortion type and level. PEAVS scores are shown side-by-side with the scores from human annotations on AVS benchmark test set. Generally, PEAVS metric aligns well with scores from human judgements across the board except in a few cases.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">In top-right corner of Figure <a href="#S6.F4" title="Figure 4 ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present the effects of audio shift. For ease of analysis, we display these results in terms of absolute shift values. We start with a shift of 0.045 based on a report from ITU which indicated that the threshold for audio-visual shift detection by human ranges from -125ms to +45ms.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. At this level, both the metric’s output and human scores peak (<math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mo id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><gt id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">&gt;</annotation></semantics></math>4), and they subsequently decrease as the shift increases.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">In contrast to audio-shift, PEAVS shows the highest correlation for intermittent muting distortion which implies it is easier for the metric to capture this distortion. This confirms the notion that we observed in Section <a href="#S3.SS2" title="3.2 Data Annotation ‣ 3 AVS Benchmark Dataset ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> where intermittent muting stood out as the most noticeable distortion and it was easier for humans to detect it as well.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">In case of the fragment shuffling (bottom-right corner), initial levels of perturbation — shuffled fragments of 0.3-0.4 seconds — is significantly more disruptive for humans than the metric, i.e. the metric fails to capture this insight.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<thead class="ltx_thead">
<tr id="S6.T3.2.1.1" class="ltx_tr">
<th id="S6.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S6.T3.2.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S6.T3.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Predicted</span></th>
</tr>
<tr id="S6.T3.2.2.2" class="ltx_tr">
<th id="S6.T3.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S6.T3.2.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T3.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Pos.</span></th>
<th id="S6.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S6.T3.2.2.2.4.1" class="ltx_text" style="font-size:90%;">Neg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.2.3.1" class="ltx_tr">
<th id="S6.T3.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S6.T3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S6.T3.2.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:26.3pt;vertical-align:-10.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10pt,0pt) rotate(-90deg) ;">
<span id="S6.T3.2.3.1.1.1.1.1" class="ltx_p">Actual</span>
</span></span></span></th>
<th id="S6.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.2.3.1.2.1" class="ltx_text" style="font-size:90%;">Pos</span></th>
<td id="S6.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.2.3.1.3.1" class="ltx_text" style="font-size:90%;">39</span></td>
<td id="S6.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.2.3.1.4.1" class="ltx_text" style="font-size:90%;">227</span></td>
</tr>
<tr id="S6.T3.2.4.2" class="ltx_tr">
<th id="S6.T3.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.2.4.2.1.1" class="ltx_text" style="font-size:90%;">Neg</span></th>
<td id="S6.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.2.4.2.2.1" class="ltx_text" style="font-size:90%;">4</span></td>
<td id="S6.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.2.4.2.3.1" class="ltx_text" style="font-size:90%;">130</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S6.T3.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<thead class="ltx_thead">
<tr id="S6.T3.3.1.1" class="ltx_tr">
<th id="S6.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S6.T3.3.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S6.T3.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Predicted</span></th>
</tr>
<tr id="S6.T3.3.2.2" class="ltx_tr">
<th id="S6.T3.3.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S6.T3.3.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S6.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T3.3.2.2.3.1" class="ltx_text" style="font-size:90%;">Pos.</span></th>
<th id="S6.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S6.T3.3.2.2.4.1" class="ltx_text" style="font-size:90%;">Neg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.3.3.1" class="ltx_tr">
<th id="S6.T3.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S6.T3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S6.T3.3.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:26.3pt;vertical-align:-10.0pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-10pt,0pt) rotate(-90deg) ;">
<span id="S6.T3.3.3.1.1.1.1.1" class="ltx_p">Actual</span>
</span></span></span></th>
<th id="S6.T3.3.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.3.1.2.1" class="ltx_text" style="font-size:90%;">Pos</span></th>
<td id="S6.T3.3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.3.1.3.1" class="ltx_text" style="font-size:90%;">92</span></td>
<td id="S6.T3.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.3.3.1.4.1" class="ltx_text" style="font-size:90%;">174</span></td>
</tr>
<tr id="S6.T3.3.4.2" class="ltx_tr">
<th id="S6.T3.3.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.4.2.1.1" class="ltx_text" style="font-size:90%;">Neg</span></th>
<td id="S6.T3.3.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.4.2.2.1" class="ltx_text" style="font-size:90%;">24</span></td>
<td id="S6.T3.3.4.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.3.4.2.3.1" class="ltx_text" style="font-size:90%;">110</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Confusion matrix for SparseSync (left) and PEAVS (right). Accuracy of SparseSync = 42.3% v/s PEAVS = 50.5%</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>PEAVS vs. SparseSync</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In this section, we report an experiment comparing PEAVS with SparseSync, which was introduced by Iashin <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for detecting audio-visual shifts inside ‘in the wild’ videos. While SparseSync’s goal is to objectively quantify specific synchronization issues, by estimating an audio-visual offset shift value, PEAVS evaluates synchronization under multiple facets and according to a perceptual scale. Due to their diverging objectives, a direct comparison is challenging. Moreover, SparseSync is trained on 21 classes representing distinct audio-shift levels from -2.0 sec (left) to 2.0 sec (right) with increments of 0.2, with 0.0 representing no audio shift; while PEAVS is trained to predict float values between 1 and 5.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.3" class="ltx_p">In this experiment, we assess both model’s performance in accurately classifying 400 videos, including 200 ground truth videos and 200 randomly audio-shifted versions of them. <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>More details about this evaluation set are in Appendix Section <a href="#Pt0.A6" title="Appendix 0.F PEAVS v/s SparseSync Benchmark ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">0.F</span></a>.</span></span></span>
We consider distortions of 0.045, 0.1 and <math id="S6.SS3.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><mo id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><csymbol cd="latexml" id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">\pm</annotation></semantics></math>0.125 as positive cases (i.e. ground truth) as ITU recommend an acceptability threshold for AV shift up to 185 ms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This also aligns well with SparseSync buckets where first distortion starts at <math id="S6.SS3.p2.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.SS3.p2.2.m2.1a"><mo id="S6.SS3.p2.2.m2.1.1" xref="S6.SS3.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S6.SS3.p2.2.m2.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">\pm</annotation></semantics></math>0.2 seconds (i.e. SparseSync’s step-size).
To ensure a fair assessment, we divided the PEAVS scale into 21 bins, matching the output structure of SparseSync.<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We use increments of 0.238 (=<math id="footnote10.m1.1" class="ltx_Math" alttext="\frac{5}{21}" display="inline"><semantics id="footnote10.m1.1b"><mfrac id="footnote10.m1.1.1" xref="footnote10.m1.1.1.cmml"><mn id="footnote10.m1.1.1.2" xref="footnote10.m1.1.1.2.cmml">5</mn><mn id="footnote10.m1.1.1.3" xref="footnote10.m1.1.1.3.cmml">21</mn></mfrac><annotation-xml encoding="MathML-Content" id="footnote10.m1.1c"><apply id="footnote10.m1.1.1.cmml" xref="footnote10.m1.1.1"><divide id="footnote10.m1.1.1.1.cmml" xref="footnote10.m1.1.1"></divide><cn type="integer" id="footnote10.m1.1.1.2.cmml" xref="footnote10.m1.1.1.2">5</cn><cn type="integer" id="footnote10.m1.1.1.3.cmml" xref="footnote10.m1.1.1.3">21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote10.m1.1d">\frac{5}{21}</annotation></semantics></math>).</span></span></span> For ground truth videos, we anticipate that SparseSync will classify them as having zero shifts (0.0 class), while PEAVS is expected to assign them to the highest scoring bin (i.e. <math id="S6.SS3.p2.3.m3.2" class="ltx_Math" alttext="(4.76,5]" display="inline"><semantics id="S6.SS3.p2.3.m3.2a"><mrow id="S6.SS3.p2.3.m3.2.3.2" xref="S6.SS3.p2.3.m3.2.3.1.cmml"><mo stretchy="false" id="S6.SS3.p2.3.m3.2.3.2.1" xref="S6.SS3.p2.3.m3.2.3.1.cmml">(</mo><mn id="S6.SS3.p2.3.m3.1.1" xref="S6.SS3.p2.3.m3.1.1.cmml">4.76</mn><mo id="S6.SS3.p2.3.m3.2.3.2.2" xref="S6.SS3.p2.3.m3.2.3.1.cmml">,</mo><mn id="S6.SS3.p2.3.m3.2.2" xref="S6.SS3.p2.3.m3.2.2.cmml">5</mn><mo stretchy="false" id="S6.SS3.p2.3.m3.2.3.2.3" xref="S6.SS3.p2.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.3.m3.2b"><interval closure="open-closed" id="S6.SS3.p2.3.m3.2.3.1.cmml" xref="S6.SS3.p2.3.m3.2.3.2"><cn type="float" id="S6.SS3.p2.3.m3.1.1.cmml" xref="S6.SS3.p2.3.m3.1.1">4.76</cn><cn type="integer" id="S6.SS3.p2.3.m3.2.2.cmml" xref="S6.SS3.p2.3.m3.2.2">5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.3.m3.2c">(4.76,5]</annotation></semantics></math>).</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">In Table <a href="#S6.T3" title="Table 3 ‣ 6.2 Per Distortion Analysis ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show the confusion matrices for SparseSync (left) and PEAVS (right). In terms of accuracy, PEAVS outperforms SparseSync by 19%, i.e. 50.5% vs. 42.3%.
In particular, SparseSync shows an 18% edge in detecting distortions while PEAVS shows a 136% higher rate in detecting clean content.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Like any research, our work is not without its limitations, and we have identified a couple of them here:</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Dataset</span>: Due to budget constraints (30K USD), we could only source 200 videos and conduct 60K pair-wise annotations. In selecting these 200 videos, we ensured diversity. Our dataset comprises 215 labels out of the 512 present in AudioSet, representing only 40% of labels. Expanding this dataset for broader video coverage is left for future work.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Metric</span>: The limited size of the AVS benchmark highlights the necessity of pre-training in model-based metrics (in our case, PEAVS). This is further evidenced by the results in Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Human Scores Correlation ‣ 6 Experiments ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where pre-training improves correlation from 0.736 (Cross-Modal Base) to 0.794 (PEAVS). In this study, we only pre-trained PEAVS on audio-shift noise, leaving pre-training on other noise types for future work.</p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">“in the wild” videos</span>: Due to proprietary issues, we could not experiment with datasets featuring talking faces like VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or Lip Reading Sentences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Our study is confined to addressing synchrony issues for “in the wild” videos.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We introduced PEAVS, a novel metric designed for measuring synchrony in audio-visual content, an important advancement in the field of evaluation of audio-visual content. While the domain of audio-visual generative modeling is ever-evolving, assessing synchronization remains crucial for enhancing user experience. Although numerous metrics cater to either audio or visual content separately, a noticeable gap exists in evaluating their synchronization with a focus on viewers opinions. Our study addresses this gap with PEAVS, a metric based on a vast human-annotated dataset. The strong correlation between PEAVS scores and human evaluations confirms its efficacy in capturing perceptions surrounding audio-visual synchronization. As the scope of audio-visual content generation expands, tools like PEAVS are essential in maintaining a balanced interplay between sight and sound, guiding both creators and researchers.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Furthermore, we introduced the Audio-Visual Synchrony human perception benchmark, which provides a new dataset detailing human-perceptual scores on various audio-visual synchrony challenges for real-world videos. By isolating specific challenges in a controlled audio-visual environment, we aim to simplify the task for researchers to propose, assess, and analyze potential solutions while maintaining agreement with human perception.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Afouras, T., Chung, J.S., Zisserman, A.: Lrs3-ted: a large-scale dataset for visual speech recognition (2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Agarwal, M., Agrawal, S., Anastasopoulos, A., Bentivogli, L., Bojar, O., Borg, C., Carpuat, M., Cattoni, R., Cettolo, M., Chen, M., Chen, W., Choukri, K., Chronopoulou, A., Currey, A., Declerck, T., Dong, Q., Duh, K., Estève, Y., Federico, M., Gahbiche, S., Haddow, B., Hsu, B., Mon Htut, P., Inaguma, H., Javorský, D., Judge, J., Kano, Y., Ko, T., Kumar, R., Li, P., Ma, X., Mathur, P., Matusov, E., McNamee, P., P. McCrae, J., Murray, K., Nadejde, M., Nakamura, S., Negri, M., Nguyen, H., Niehues, J., Niu, X., Kr. Ojha, A., E. Ortega, J., Pal, P., Pino, J., van der Plas, L., Polák, P., Rippeth, E., Salesky, E., Shi, J., Sperber, M., Stüker, S., Sudoh, K., Tang, Y., Thompson, B., Tran, K., Turchi, M., Waibel, A., Wang, M., Watanabe, S., Zevallos, R.: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN. In: Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). pp. 1–61. Association for Computational Linguistics, Toronto, Canada (in-person and online)
(Jul 2023). https://doi.org/10.18653/v1/2023.iwslt-1.1, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2023.iwslt-1.1</span>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arandjelović, R., Zisserman, A.: Look, listen and learn. 2017 IEEE International Conference on Computer Vision (ICCV) pp. 609–617 (2017), <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://api.semanticscholar.org/CorpusID:10769575</span>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Arandjelović, R., Zisserman, A.: Objects that sound. In: Computer Vision – ECCV 2018. pp. 451–466. Springer International Publishing, Cham (2018)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Binkowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying MMD gans. In: 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net (2018), <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=r1lUOzWCW</span>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
BT.1359, R.I.R.: BT.1359 - Relative timing of sound and vision for broadcasting (1998)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4724–4733. IEEE Computer Society, Los Alamitos, CA, USA (jul 2017). https://doi.org/10.1109/CVPR.2017.502, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.502</span>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chen, H., Xie, W., Afouras, T., Nagrani, A., Vedaldi, A., Zisserman, A.: Audio-visual synchronisation in the wild (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chen, H., Xie, W., Vedaldi, A., Zisserman, A.: Vggsound: A large-scale audio-visual dataset (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chinen, M., Lim, F.S.C., Skoglund, J., Gureev, N., O’Gorman, F., Hines, A.: Visqol v3: An open source production ready objective speech and audio metric. In: 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX). pp. 1–6 (2020). https://doi.org/10.1109/QoMEX48832.2020.9123150

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chung, J.S., Zisserman, A.: Out of time: automated lip sync in the wild. In: Workshop on Multi-view Lip-reading, ACCV (2016)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Clark, A.P., Howard, K.L., Woods, A.T., Penton-Voak, I.S., Neumann, C.: Why rate when you could compare? Using the “EloChoice” package to assess pairwise comparisons of perceived physical strength. PLOS ONE <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">13</span>(1), e0190393 (Jan 2018). https://doi.org/10.1371/journal.pone.0190393, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dx.plos.org/10.1371/journal.pone.0190393</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Committee, A.T.S.: Atsc implementation subcommittee finding: Relative timing of sound and vision for broadcast operations. Tech. rep., Advanced Television Systems Committee, 1750 K Street, N.W., Suite 1200, Washington, D.C. 20006 (6 2003), doc. IS-191

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Dowson, D., Landau, B.: The fréchet distance between multivariate normal distributions. Journal of Multivariate Analysis <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">12</span>(3), 450–455 (1982). https://doi.org/https://doi.org/10.1016/0047-259X(82)90077-X, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/0047259X8290077X</span>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 776–780 (2017). https://doi.org/10.1109/ICASSP.2017.7952261

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Goncalves, L., Busso, C.: Auxformer: Robust approach to audiovisual emotion recognition. In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 7357–7361 (2022). https://doi.org/10.1109/ICASSP43922.2022.9747157

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hershey, J., Movellan, J.: Audio vision: Using audio-visual synchrony to locate sounds. In: Solla, S., Leen, T., Müller, K. (eds.) Advances in Neural Information Processing Systems. vol. 12. MIT Press (1999), <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Paper.pdf</span>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Hershey, S., Chaudhuri, S., Ellis, D.P.W., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., Slaney, M., Weiss, R.J., Wilson, K.: Cnn architectures for large-scale audio classification (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: CLIPScore: a reference-free evaluation metric for image captioning. In: EMNLP (2021)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Neural Information Processing Systems (2017)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Iashin, V., Xie, W., Rahtu, E., Zisserman, A.: Sparse in space and time: Audio-visual synchronisation with trainable selectors. In: British Machine Vision Conference (BMVC) (2022)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
ITU-T RECOMMENDATION, P.: Subjective video quality assessment methods for multimedia applications (1999)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kilgour, K., Zuluaga, M., Roblek, D., Sharifi, M.: Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms. In: Interspeech (2019), <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://api.semanticscholar.org/CorpusID:202725406</span>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Korhonen, J., You, J.: Korhonen. In: 2012 Fourth International Workshop on Quality of Multimedia Experience. pp. 37–38 (2012). https://doi.org/10.1109/QoMEX.2012.6263880

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Krippendorff, K.: Content Analysis: An Introduction to Its Methodology (second edition). Sage Publications (2004)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Luo, S., Yan, C., Hu, C., Zhao, H.: Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models (2023)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Luo, Y., Mesgarani, N.: Tasnet: Time-domain audio separation network for real-time, single-channel speech separation. In: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 696–700 (2018). https://doi.org/10.1109/ICASSP.2018.8462116

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Manocha, P., Finkelstein, A., Zhang, R., Bryan, N.J., Mysore, G.J., Jin, Z.: A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences. In: Interspeech 2020. pp. 2852–2856. ISCA (Oct 2020). https://doi.org/10.21437/Interspeech.2020-1191, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.isca-speech.org/archive/interspeech_2020/manocha20_interspeech.html</span>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Manocha, P., Kumar, A., Xu, B., Menon, A., Gebru, I.D., Ithapu, V.K., Calamia, P.: Dplm: A deep perceptual spatial-audio localization metric. In: 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). pp. 6–10 (2021). https://doi.org/10.1109/WASPAA52581.2021.9632781

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Nagrani, A., Chung, J.S., Zisserman, A.: Voxceleb: a large-scale speaker identification dataset. In: INTERSPEECH (2017)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Rix, A., Beerends, J., Hollier, M., Hekstra, A.: Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In: 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221). vol. 2, pp. 749–752 vol.2 (2001). https://doi.org/10.1109/ICASSP.2001.941023

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Roux, J.L., Wisdom, S., Erdogan, H., Hershey, J.R.: Sdr – half-baked or well done? In: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 626–630 (2019). https://doi.org/10.1109/ICASSP.2019.8683855

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N., Jin, Q., Guo, B.: Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10219–10228. IEEE Computer Society, Los Alamitos, CA, USA (jun 2023). https://doi.org/10.1109/CVPR52729.2023.00985, <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00985</span>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training gans. In: Proceedings of the 30th International Conference on Neural Information Processing Systems. p. 2234–2242. NIPS’16, Curran Associates Inc., Red Hook, NY, USA (2016)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Taal, C.H., Hendriks, R.C., Heusdens, R., Jensen, J.: An algorithm for intelligibility prediction of time–frequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">19</span>(7), 2125–2136 (2011). https://doi.org/10.1109/TASL.2011.2114881

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov, R.: Multimodal transformer for unaligned multimodal language sequences. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Florence, Italy (7 2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Towards accurate generative models of video: A new metric &amp; challenges (2019)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017), <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</span>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Vincent, E., Gribonval, R., Fevotte, C.: Performance measurement in blind audio source separation. IEEE Transactions on Audio, Speech, and Language Processing <span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">14</span>(4), 1462–1469 (2006). https://doi.org/10.1109/TSA.2005.858005

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Wang, J., Fang, Z., Zhao, H.: Alignnet: A unifying approach to audio-visual alignment. In: WACV (2020)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Wang, Y., Ke, J., Talebi, H., Yim, J.G., Birkbeck, N., Adsumilli, B., Milanfar, P., Yang, F.: Rich features for perceptual quality assessment of ugc videos. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 13430–13439 (2021). https://doi.org/10.1109/CVPR46437.2021.01323

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wang, Z., Simoncelli, E., Bovik, A.: Multiscale structural similarity for image quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers, 2003. vol. 2, pp. 1398–1402 Vol.2 (2003). https://doi.org/10.1109/ACSSC.2003.1292216

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Wang, Z., Bovik, A.: A universal image quality index. IEEE Signal Processing Letters <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">9</span>(3), 81–84 (2002). https://doi.org/10.1109/97.995823

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 586–595 (2018). https://doi.org/10.1109/CVPR.2018.00068

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Pt0.A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Model Training Settings</h2>

<figure id="Pt0.A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A1.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="Pt0.A1.T4.3.2" class="ltx_text" style="font-size:90%;">Training Settings</span></figcaption>
<table id="Pt0.A1.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Pt0.A1.T4.4.1.1" class="ltx_tr">
<th id="Pt0.A1.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Pt0.A1.T4.4.1.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></th>
<td id="Pt0.A1.T4.4.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="Pt0.A1.T4.4.1.1.2.1" class="ltx_text ltx_font_bold">Stage 1</span></td>
<td id="Pt0.A1.T4.4.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="Pt0.A1.T4.4.1.1.3.1" class="ltx_text ltx_font_bold">Stage 2</span></td>
</tr>
<tr id="Pt0.A1.T4.4.2.2" class="ltx_tr">
<th id="Pt0.A1.T4.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">LR</th>
<td id="Pt0.A1.T4.4.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.001</td>
<td id="Pt0.A1.T4.4.2.2.3" class="ltx_td ltx_align_left ltx_border_t">0.0001</td>
</tr>
<tr id="Pt0.A1.T4.4.3.3" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Pt0.A1.T4.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="Pt0.A1.T4.4.3.3.1.1" class="ltx_text" style="background-color:#E6E6E6;">Optimizer</span></th>
<td id="Pt0.A1.T4.4.3.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T4.4.3.3.2.1" class="ltx_text" style="background-color:#E6E6E6;">Adam</span></td>
<td id="Pt0.A1.T4.4.3.3.3" class="ltx_td ltx_align_left"><span id="Pt0.A1.T4.4.3.3.3.1" class="ltx_text" style="background-color:#E6E6E6;">Adam</span></td>
</tr>
<tr id="Pt0.A1.T4.4.4.4" class="ltx_tr">
<th id="Pt0.A1.T4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Loss</th>
<td id="Pt0.A1.T4.4.4.4.2" class="ltx_td ltx_align_left ltx_border_r">Contrastive</td>
<td id="Pt0.A1.T4.4.4.4.3" class="ltx_td ltx_align_left">CCC</td>
</tr>
<tr id="Pt0.A1.T4.4.5.5" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Pt0.A1.T4.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="Pt0.A1.T4.4.5.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">Batch Size</span></th>
<td id="Pt0.A1.T4.4.5.5.2" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T4.4.5.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">128</span></td>
<td id="Pt0.A1.T4.4.5.5.3" class="ltx_td ltx_align_left"><span id="Pt0.A1.T4.4.5.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">64</span></td>
</tr>
<tr id="Pt0.A1.T4.4.6.6" class="ltx_tr">
<th id="Pt0.A1.T4.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Epochs</th>
<td id="Pt0.A1.T4.4.6.6.2" class="ltx_td ltx_align_left ltx_border_r">60</td>
<td id="Pt0.A1.T4.4.6.6.3" class="ltx_td ltx_align_left">20</td>
</tr>
<tr id="Pt0.A1.T4.4.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Pt0.A1.T4.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="Pt0.A1.T4.4.7.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">LR Scheduler</span></th>
<td id="Pt0.A1.T4.4.7.7.2" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T4.4.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">.1 (Patience 10)</span></td>
<td id="Pt0.A1.T4.4.7.7.3" class="ltx_td ltx_align_left"><span id="Pt0.A1.T4.4.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">.1 (Patience 3)</span></td>
</tr>
<tr id="Pt0.A1.T4.4.8.8" class="ltx_tr">
<th id="Pt0.A1.T4.4.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Margin</th>
<td id="Pt0.A1.T4.4.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">1.0</td>
<td id="Pt0.A1.T4.4.8.8.3" class="ltx_td ltx_align_left ltx_border_b">-</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Pt0.A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A1.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="Pt0.A1.T5.3.2" class="ltx_text" style="font-size:90%;">Transformer Layers Settings</span></figcaption>
<table id="Pt0.A1.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Pt0.A1.T5.4.1.1" class="ltx_tr">
<th id="Pt0.A1.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Pt0.A1.T5.4.1.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></th>
<th id="Pt0.A1.T5.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="Pt0.A1.T5.4.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Pt0.A1.T5.4.2.1" class="ltx_tr">
<td id="Pt0.A1.T5.4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Heads</td>
<td id="Pt0.A1.T5.4.2.1.2" class="ltx_td ltx_align_left ltx_border_t">8</td>
</tr>
<tr id="Pt0.A1.T5.4.3.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="Pt0.A1.T5.4.3.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T5.4.3.2.1.1" class="ltx_text" style="background-color:#E6E6E6;">Layers</span></td>
<td id="Pt0.A1.T5.4.3.2.2" class="ltx_td ltx_align_left"><span id="Pt0.A1.T5.4.3.2.2.1" class="ltx_text" style="background-color:#E6E6E6;">3</span></td>
</tr>
<tr id="Pt0.A1.T5.4.4.3" class="ltx_tr">
<td id="Pt0.A1.T5.4.4.3.1" class="ltx_td ltx_align_left ltx_border_r">Embed Dim</td>
<td id="Pt0.A1.T5.4.4.3.2" class="ltx_td ltx_align_left">128</td>
</tr>
<tr id="Pt0.A1.T5.4.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="Pt0.A1.T5.4.5.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T5.4.5.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">Attention Dropout</span></td>
<td id="Pt0.A1.T5.4.5.4.2" class="ltx_td ltx_align_left"><span id="Pt0.A1.T5.4.5.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">0.1</span></td>
</tr>
<tr id="Pt0.A1.T5.4.6.5" class="ltx_tr">
<td id="Pt0.A1.T5.4.6.5.1" class="ltx_td ltx_align_left ltx_border_r">Relu Dropout</td>
<td id="Pt0.A1.T5.4.6.5.2" class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr id="Pt0.A1.T5.4.7.6" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="Pt0.A1.T5.4.7.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="Pt0.A1.T5.4.7.6.1.1" class="ltx_text" style="background-color:#E6E6E6;">Embed. Dropout</span></td>
<td id="Pt0.A1.T5.4.7.6.2" class="ltx_td ltx_align_left"><span id="Pt0.A1.T5.4.7.6.2.1" class="ltx_text" style="background-color:#E6E6E6;">0.25</span></td>
</tr>
<tr id="Pt0.A1.T5.4.8.7" class="ltx_tr">
<td id="Pt0.A1.T5.4.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Residual Block Dropout</td>
<td id="Pt0.A1.T5.4.8.7.2" class="ltx_td ltx_align_left ltx_border_b">0.1</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="Pt0.A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Human Annotation Guidelines</h2>

<section id="Pt0.A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.B.1 </span>Background</h3>

<div id="Pt0.A2.SS1.p1" class="ltx_para">
<p id="Pt0.A2.SS1.p1.1" class="ltx_p">In this annotation task, we want to evaluate the synchronization quality between audio and visual modalities in the video, and get an understanding of how the two videos presented to you compare to each other in terms of quality of synchronization. Please note that:</p>
</div>
<div id="Pt0.A2.SS1.p2" class="ltx_para">
<ul id="Pt0.A2.I1" class="ltx_itemize">
<li id="Pt0.A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Pt0.A2.I1.i1.p1" class="ltx_para">
<p id="Pt0.A2.I1.i1.p1.1" class="ltx_p">We are NOT looking at the perceived quality of audio or video independently, but only how well they are aligned.</p>
</div>
</li>
<li id="Pt0.A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Pt0.A2.I1.i2.p1" class="ltx_para">
<p id="Pt0.A2.I1.i2.p1.1" class="ltx_p">We added certain types of distortions in these videos that disrupt the AV synchrony like speeding up the audio or speeding up the video, intermittently muting the audio, blacking out parts of video, flickering, etc.</p>
</div>
</li>
<li id="Pt0.A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Pt0.A2.I1.i3.p1" class="ltx_para">
<p id="Pt0.A2.I1.i3.p1.1" class="ltx_p">Remember that we are also interested to see how the synchronization quality in these videos compares to each other, i.e. your relative judgment.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Pt0.A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.B.2 </span>Annotation Criteria</h3>

<div id="Pt0.A2.SS2.p1" class="ltx_para">
<p id="Pt0.A2.SS2.p1.1" class="ltx_p">In each comparison view, you will see two videos side by side. Play both videos and rate them on a scale of 1-5 in terms of disruption caused by these distortions based on the following likert scale:</p>
</div>
<div id="Pt0.A2.SS2.p2" class="ltx_para">
<ol id="Pt0.A2.I2" class="ltx_enumerate">
<li id="Pt0.A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Pt0.A2.I2.i1.p1" class="ltx_para">
<p id="Pt0.A2.I2.i1.p1.1" class="ltx_p">Score 1 when there is complete misalignment between audio and video OR the video/audio is totally incomprehensible due to disruptions.</p>
</div>
</li>
<li id="Pt0.A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Pt0.A2.I2.i2.p1" class="ltx_para">
<p id="Pt0.A2.I2.i2.p1.1" class="ltx_p">Score 2 when only a few parts of audio/video are in alignment OR large portion of video/audio is incomprehensible due to disruptions.</p>
</div>
</li>
<li id="Pt0.A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Pt0.A2.I2.i3.p1" class="ltx_para">
<p id="Pt0.A2.I2.i3.p1.1" class="ltx_p">Score 3 when there is moderate mis-alignment between audio/video OR some portion of video/audio is comprehensible but there are visible disruptions.</p>
</div>
</li>
<li id="Pt0.A2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="Pt0.A2.I2.i4.p1" class="ltx_para">
<p id="Pt0.A2.I2.i4.p1.1" class="ltx_p">Score 4 when there is almost perfect alignment with minor mis-alignments in some parts of video OR most of the video and audio is comprehensible with minor disruptions.</p>
</div>
</li>
<li id="Pt0.A2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="Pt0.A2.I2.i5.p1" class="ltx_para">
<p id="Pt0.A2.I2.i5.p1.1" class="ltx_p">Score 5 when there is perfect alignment and audio/video are flawlessly in sync, AND/OR have no disruptions at all.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="Pt0.A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>Dataset Labels Information</h2>

<div id="Pt0.A3.p1" class="ltx_para">
<p id="Pt0.A3.p1.1" class="ltx_p">To generate our dataset, we selected 200 videos from AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> ensuring that each sample contained well-aligned audio-visual content. The chosen samples are representative of videos "in the wild"; we excluded any samples that featured talking faces. Instead, our selection primarily contains videos depicting actions such as: cars driving by, dogs barking, instruments being played, and manual labor, among others. Based on the labels provided in AudioSet, our dataset comprises 215 labels. The number of labels exceeds the number of files because some files may be associated with multiple classes, resulting in co-occurrence of labels in many files within AudioSet. To provide an overview of the labels in our selected dataset, Figure <a href="#Pt0.A3.F5" title="Figure 5 ‣ Appendix 0.C Dataset Labels Information ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents a histogram of the top 30 label frequencies from our AudioSet subset.</p>
</div>
<figure id="Pt0.A3.F5" class="ltx_figure"><img src="/html/2404.07336/assets/images/data_annotation_analysis/top30_distribution_plot.png" id="Pt0.A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="Pt0.A3.F5.3.2" class="ltx_text" style="font-size:90%;">Histogram of the top 30 label frequencies from the AVS benchmark sub-sampled from the AudioSet corpus.</span></figcaption>
</figure>
<figure id="Pt0.A3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Pt0.A3.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.07336/assets/images/data_annotation_analysis/heatmap_before_filtering.png" id="Pt0.A3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="628" height="419" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Pt0.A3.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.07336/assets/images/data_annotation_analysis/heatmap_after_filtering.png" id="Pt0.A3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="628" height="419" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A3.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="Pt0.A3.F6.5.2" class="ltx_text" style="font-size:90%;">Heatmap of score distribution on AVS benchmark, across distortion types and levels before (left) and after (right) filtering of data. The lighter the color the higher the scores and vice-versa.</span></figcaption>
</figure>
</section>
<section id="Pt0.A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.D </span>Score Distribution in AVS Benchmark</h2>

<div id="Pt0.A4.p1" class="ltx_para">
<p id="Pt0.A4.p1.1" class="ltx_p">In this section, we will analyse the score distribution on the AVS benchmark data and the effect of filtering.
We wanted to do a followup analysis from the main paper, especially by looking at the distribution of human annotation scores in the AVS benchmark data. Figure <a href="#Pt0.A3.F6" title="Figure 6 ‣ Appendix 0.C Dataset Labels Information ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides more insights in our benchmark. Here, we observe a similar trend of intermittent muting (distortion type 6) being highly disruptive perceptually (colored with dark cells in the figure) compared to other distortion types especially in the lower levels of distortion - where the distorted videos are more jittery.</p>
</div>
<div id="Pt0.A4.p2" class="ltx_para">
<p id="Pt0.A4.p2.1" class="ltx_p">To remove the outliers in the benchmark, we filtered the dataset on the following criteria:</p>
<ol id="Pt0.A4.I1" class="ltx_enumerate">
<li id="Pt0.A4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Pt0.A4.I1.i1.p1" class="ltx_para">
<p id="Pt0.A4.I1.i1.p1.1" class="ltx_p">We filter out all samples where ground truth videos are annotated with an average score (of 3 raters) of 3.5 or below.</p>
</div>
</li>
<li id="Pt0.A4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Pt0.A4.I1.i2.p1" class="ltx_para">
<p id="Pt0.A4.I1.i2.p1.1" class="ltx_p">We filter out all samples with extreme distortion levels rated as 5.</p>
</div>
</li>
</ol>
<p id="Pt0.A4.p2.2" class="ltx_p">Filtering the data increases the variance within each distortion type and a clear trend across levels emerges. After filtering, less distorted videos have higher scores as compared to those without filtering. See distortion type IDs 7,8,9 (in Figure <a href="#Pt0.A3.F6" title="Figure 6 ‣ Appendix 0.C Dataset Labels Information ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) where values in initial level shows an increasing trend in the right plot (filtered) as opposed to the left one (unfiltered).</p>
</div>
<div id="Pt0.A4.p3" class="ltx_para">
<p id="Pt0.A4.p3.1" class="ltx_p">Note that in Figure <a href="#Pt0.A3.F6" title="Figure 6 ‣ Appendix 0.C Dataset Labels Information ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, distortion type IDs are the same as in Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Levels in the audio-shift distortion is sorted in increasing order (from left shift of -1 to right shift of +2).</p>
</div>
</section>
<section id="Pt0.A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.E </span>FAD v/s FVD v/s FAVD</h2>

<figure id="Pt0.A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Shifting.png" id="Pt0.A5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="Pt0.A5.F7.1.2.2" class="ltx_text" style="font-size:90%;">Audio Shift</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Speed_Up.png" id="Pt0.A5.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="Pt0.A5.F7.2.2.2" class="ltx_text" style="font-size:90%;">Audio Speed Up</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Audio_Speed_Down.png" id="Pt0.A5.F7.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="Pt0.A5.F7.3.2.2" class="ltx_text" style="font-size:90%;">Audio Speed Down</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Video_Speed_Up.png" id="Pt0.A5.F7.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.4.1.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="Pt0.A5.F7.4.2.2" class="ltx_text" style="font-size:90%;">Video Speed Up</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Video_Speed_Down.png" id="Pt0.A5.F7.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.5.1.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="Pt0.A5.F7.5.2.2" class="ltx_text" style="font-size:90%;">Video Speed Down</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Randomly_Sized_Gaps_Video.png" id="Pt0.A5.F7.6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.6.1.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="Pt0.A5.F7.6.2.2" class="ltx_text" style="font-size:90%;">Random Gaps</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.7" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Fragment_Shuffling.png" id="Pt0.A5.F7.7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.7.1.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="Pt0.A5.F7.7.2.2" class="ltx_text" style="font-size:90%;">Fragment Shuffle</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.8" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/Itermittent_Muting.png" id="Pt0.A5.F7.8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.8.1.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="Pt0.A5.F7.8.2.2" class="ltx_text" style="font-size:90%;">Intermittent Mute</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Pt0.A5.F7.9" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.8pt;"><img src="/html/2404.07336/assets/images/frechet_dist_line_plots/AV_Flickering.png" id="Pt0.A5.F7.9.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.9.1.1.1" class="ltx_text" style="font-size:90%;">(i)</span> </span><span id="Pt0.A5.F7.9.2.2" class="ltx_text" style="font-size:90%;">AV Flickering</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A5.F7.14.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="Pt0.A5.F7.15.2" class="ltx_text" style="font-size:90%;">In these plots, we show the effect of distortions with varying levels on <span id="Pt0.A5.F7.15.2.1" class="ltx_text ltx_font_bold" style="color:#FF8000;">FAD</span>, <span id="Pt0.A5.F7.15.2.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">FVD</span>, and <span id="Pt0.A5.F7.15.2.3" class="ltx_text ltx_font_bold" style="color:#00FF00;">FAVD</span>. Flat trend line implies that a metric is not able to capture the distortion type. Increasing or decreasing trends show that a metric is susceptible to varying levels in a distortion. Distortion levels are taken from Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></figcaption>
</figure>
</section>
<section id="Pt0.A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.F </span>PEAVS v/s SparseSync Benchmark</h2>

<div id="Pt0.A6.p1" class="ltx_para">
<p id="Pt0.A6.p1.1" class="ltx_p">In our comparison for PEAVS v/s SparseSync, we created a held out evaluation set specifically for the head to head comparison. In addition to the 37 videos (annotated from AVS benchmark), we selected 163 more diverse set of videos from AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, ensuring that they do not contain talking faces but showcase various real-world scenarios, such as a car driving by, an instrument being played, a dog barking, and many more. We manually looked into each video to ensure if they are well aligned. We then changed the offset of audio stream of these 200 videos by randomly sampling from the different levels of audio-shift distortion (as described in Table <a href="#S2.T1" title="Table 1 ‣ Perceptual Metrics: ‣ 2 Related Works ‣ PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers’ Opinion Scores" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This resulted in a total of 400 videos with an equal mix of ground truth and distorted videos. This portion of evaluation set will also be released as a part of AVS benchmark.</p>
</div>
<section id="Pt0.A6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Levels of Distortion</h5>

<div id="Pt0.A6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Pt0.A6.SS0.SSS0.Px1.p1.2" class="ltx_p">ITU recommends an acceptability thresholds for AV shift up to 185 ms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, this is why we consider distortions of 0.045, 0.1 and <math id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A6.SS0.SSS0.Px1.p1.1.m1.1c">\pm</annotation></semantics></math>0.125 as positive cases (i.e. ground truth). This also aligns well with SparseSync buckets where the first distortion starts at <math id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1a"><mo id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1.1" xref="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1b"><csymbol cd="latexml" id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A6.SS0.SSS0.Px1.p1.2.m2.1c">\pm</annotation></semantics></math>0.2 seconds (i.e. SparseSync’s step-size).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.07335" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.07336" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.07336">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.07336" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.07337" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:01:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
