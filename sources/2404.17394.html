<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.17394] Child Speech Recognition in Human-Robot Interaction: Problem Solved?</title><meta property="og:description" content="Automated Speech Recognition shows superhuman performance for adult English speech on a range of benchmarks, but disappoints when fed children’s speech. This has long sat in the way of child-robot interaction. Recent e…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Child Speech Recognition in Human-Robot Interaction: Problem Solved?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Child Speech Recognition in Human-Robot Interaction: Problem Solved?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.17394">

<!--Generated on Sun May  5 17:29:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Child-Robot Interaction; Automatic Speech Recognition;
Verbal Interaction; Interaction Design Recommendations">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Child Speech Recognition in Human-Robot Interaction: Problem Solved?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Janssens
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ruben.janssens@ugent.be">ruben.janssens@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eva Verhelst
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:eva.verhelst@ugent.be">eva.verhelst@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giulio Antonio Abbo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:giulioantonio.abbo@ugent.be">giulioantonio.abbo@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiaoqiao Ren
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:qiaoqiao.ren@ugent.be">qiaoqiao.ren@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Jose Pinto Bernal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mariajose.pintobernal@ugent.be">mariajose.pintobernal@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tony Belpaeme
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tony.belpaeme@ugent.be">tony.belpaeme@ugent.be</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">IDLab-AIRO, Ghent University - imec</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country">Ghent, Belgium</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id13.id1" class="ltx_p">Automated Speech Recognition shows superhuman performance for adult English speech on a range of benchmarks, but disappoints when fed children’s speech. This has long sat in the way of child-robot interaction. Recent evolutions in data-driven speech recognition, including the availability of Transformer architectures and unprecedented volumes of training data, might mean a breakthrough for child speech recognition and social robot applications aimed at children. We revisit a study on child speech recognition from 2017 and show that indeed performance has increased, with newcomer OpenAI Whisper doing markedly better than leading commercial cloud services. While transcription is not perfect yet, the
best model recognises 60.3% of sentences correctly barring small grammatical differences, with sub-second transcription time running on a local GPU, showing potential for usable autonomous child-robot speech interactions.</p>
</div>
<div class="ltx_keywords">Child-Robot Interaction; Automatic Speech Recognition;
Verbal Interaction; Interaction Design Recommendations
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Technological Advances in Human-Robot Interaction 2024; March 09–10,
2024; Boulder, CO</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization Robotics</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Speech recognition</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Children</span></span></span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Cite as: Ruben Janssens, Eva Verhelst, Giulio Antonio Abbo, Qiaoqiao Ren, Maria Jose Pinto Bernal and Tony Belpaeme. 2024. Child Speech Recognition in Human-Robot Interaction: Problem Solved?. In <em id="footnote1.1" class="ltx_emph ltx_font_italic">2024 International Symposium on
Technological Advances in Human-Robot Interaction</em>, March 8-9, Boulder, CO, USA.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction and Background</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Spoken language interaction is for many the holy grail in HCI and HRI. It is built upon a collection of technologies, such as Automated Speech Recognition, Dialogue Management, or Text-to-Speech, that are chained together to create a system which allows the user to interact or converse with an artificial system using the most natural interface known to man. While this processing chain is brittle, the point of entry is Automated Speech Recognition (ASR). The ability to automatically transcribe speech utterances –converting continuous acoustic signals into discrete symbolic representations, typically text– has been studied extensively in academic and industrial research. In recent decades, ASR performance has come along in leaps and bounds, with companies claiming “super-human performance” on conversational ASR benchmarks in 2017 <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. On certain benchmarks and for resource-rich languages, both in terms of training data availability and priorities imposed by economic returns on investment, speech recognition performance is on par or even better than mean human transcription performance. The popular metric for ASR performance is Word Error Rate (WER), calculated as the total number of errors —substitutions, insertions, and deletions— divided by the total number of words in the text. WER was typically reported to be below 5%. These systems relied on neural networks such as CNNs and LSTMs to extract features from audio signals and convert time series to text. Combined with large, annotated training sets and unsupervised learning, these systems improved over earlier model-based learning. However, while impressive, these systems’ performance degraded catastrophically on speech for which it was not optimised, including atypical voices such as the speech of elderly or young children. This has repercussions for HRI and specifically for applications in which autonomous social robots are expected to interact with non-typical users, such as robots for elder care or robots for education <cite class="ltx_cite ltx_citemacro_citep">(Belpaeme et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In 2017, Kennedy <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> published a widely cited study showing that then state-of-the-art ASR could not reliably transcribe the speech of native 5-year-old English speakers. They recorded speech from 11 children in a primary school in the U.K. The speech ranged from constrained utterances –such as counting from 1 to 10– to unconstrained telling of a story from a picture book. Recordings were made using three different microphones, to evaluate whether the quality and hardware integration of the microphone into a robot had an impact on ASR. The ASR performance was evaluated for four different engines, three commercial ASR solutions —Nuance VoCon 4.7, Microsoft Speech API (2016), Google Speech API (2016)— and CMU PocketSphinx, the leading open-source solution at the time. The results were nothing but disappointing. While WER for adult speech was below 5%, most engines could not correctly transcribe a single child utterance. Only Google’s ASR did marginally better, recognising 11.8% of constrained child speech and about 6% of spontaneous child speech. Still, only correctly being able to transcribe 1 utterance out of 10 is a recipe for interaction disaster, and the authors of the study at the time recommended against relying on ASR for child-robot interaction.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Forward 6 years. Artificial intelligence has been revolutionised by the Transformer architecture for sequence-to-sequence tasks, not only resulting in a sea of change in the performance of generative language models but also in the performance of ASR <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. In September 2022, OpenAI released Whisper, an ASR engine built using an encoder-decoder Transformer architecture trained on an unprecedented 680,000 hours of labelled audio data <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. While the specifics of Whisper’s training regimen and its training data are proprietary to OpenAI, the inference model is released as public open-source software. Whisper’s performance on average is better than competing solutions, but was found to still be subpar to solutions that have been specifically trained or fine-tuned on specific datasets, such as LibriSpeech <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Next to the publicly available Whisper models, which still require one to install and run the ASR on own hardware, there are several cloud-based solutions. In this area large players —Amazon, Google, Microsoft and Tencent— compete with smaller, sometimes specialised vendors, but all offer convenient online services that are easily integrated within code.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Given the availability of new architectures trained on larger and more diverse corpora, the time is opportune to revisit the results from Kennedy <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> and evaluate whether state-of-the-art ASR can now handle child speech. We decided to compare OpenAI’s Whisper, as it is open-source and exemplifies the new direction in data-driven ASR, and two commercial cloud-based solutions, opting for Microsoft Azure Speech to Text, due to its popularity and the fact that we integrate it into our robot systems at Ghent University, and Google Cloud Speech-to-text. We are first and foremost interested in transcription accuracy, but for our aim of integrating child speech recognition into an interactive HRI scenario, we also wish to explore how responsive different systems are and to which extent they would support real-time spoken interaction.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To evaluate the ASR engines, we use the data from <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> which contains audio recordings (44KHz lossless WAV files) of 11 young children (age M=4.9 years old; 5 females, 6 males) recorded at an English primary school. The recordings consist of spontaneous speech (retelling a picture book, ‘Frog, Where Are You?’ by Mercer Mayer) and speech in which children count from 1 to 10 or repeat short sentences spoken by an adult (such as “the horse is in the stable”). Each sample is recorded from 3 sources: a studio-grade microphone (Rode NT1-A), a portable microphone (Zoom H1) and the two front microphones of the Aldebaran NAO robot. All recordings have been manually transcribed and this is used as ground truth.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">We evaluated three ASR engines: Microsoft Azure Speech to text, Google Cloud Speech-to-text, and OpenAI’s Whisper. The Azure and Google models were used through a cloud API.
Whisper exists in different model sizes: tiny (39M parameters), base, small, medium, and large (1550M parameters), with three versions of the large model.
All seven of these models are compared in this study: we expect the smaller models to run faster but have lower accuracy. We used the <span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter">faster-whisper<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1a.1.1.1" class="ltx_text ltx_font_serif">1</span></span><span id="footnote1a.5" class="ltx_text ltx_font_serif">github.com/SYSTRAN/faster-whisper</span></span></span></span></span> reimplementation of the Whisper models, which claims a transcription time of up to four times faster than OpenAI’s original Whisper implementation.
The Whisper models were run locally on an NVIDIA GeForce GTX 1080 Ti with 11 GB of VRAM. We also ran them on only a CPU, to assess the necessity of a dedicated GPU for these models.
We configured the models to expect English language speech, as preliminary testing revealed that without this option Whisper Large-v3 correctly detected English in only 84% of spontaneous speech samples. All transcriptions were performed in January 2024.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The performance of the models is compared using three different metrics, as in <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>.
Primarily, we use the Levenshtein distance at the letter level, which represents the minimum amount of insertions, deletions and substitutions required to change one sequence into the other. Using this metric, small errors are penalised less than they would be when using a metric like the Word Error Rate. For example, when the word “robots” is recognized instead of the word “robot”, the Levenshtein distance would be 1 (as only one edit is needed to change the recognised word into the original word). We then normalise this metric by the amount of letters in the ground truth sequence. A score of 0 means perfect recognition, a score of 1 could reflect a recognised sequence of the same length but with no single letter in the right position.
Furthermore, we also report the recognition percentage, which represents the amount of utterances that are completely correctly recognised. Finally, also a relaxed accuracy is reported: this measure counts how many utterances are correctly recognised, also counting as accurate those with small grammatical differences that do not impact the meaning of the utterance, following the same rules as in <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">To estimate the possibility of real-time interactions, we explore the responsiveness of the different systems by reporting their transcription time. For all Whisper models, the transcription time is the time it takes for the model to return a result, which varies due to the model size as well as the hardware on which it runs. As the Azure and Google systems are cloud-based, their transcription time also includes the transmission time of the audio file and the result.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.2" class="ltx_p">In all analyses, unless otherwise stated, we use only the studio microphone recordings, and only the recordings of the sentences that the children repeat from the adult (<math id="S2.p5.1.m1.1" class="ltx_Math" alttext="n=50" display="inline"><semantics id="S2.p5.1.m1.1a"><mrow id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml"><mi id="S2.p5.1.m1.1.1.2" xref="S2.p5.1.m1.1.1.2.cmml">n</mi><mo id="S2.p5.1.m1.1.1.1" xref="S2.p5.1.m1.1.1.1.cmml">=</mo><mn id="S2.p5.1.m1.1.1.3" xref="S2.p5.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><apply id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1"><eq id="S2.p5.1.m1.1.1.1.cmml" xref="S2.p5.1.m1.1.1.1"></eq><ci id="S2.p5.1.m1.1.1.2.cmml" xref="S2.p5.1.m1.1.1.2">𝑛</ci><cn type="integer" id="S2.p5.1.m1.1.1.3.cmml" xref="S2.p5.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">n=50</annotation></semantics></math>) and of the spontaneous utterances (split into sentences, <math id="S2.p5.2.m2.1" class="ltx_Math" alttext="n=222" display="inline"><semantics id="S2.p5.2.m2.1a"><mrow id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml"><mi id="S2.p5.2.m2.1.1.2" xref="S2.p5.2.m2.1.1.2.cmml">n</mi><mo id="S2.p5.2.m2.1.1.1" xref="S2.p5.2.m2.1.1.1.cmml">=</mo><mn id="S2.p5.2.m2.1.1.3" xref="S2.p5.2.m2.1.1.3.cmml">222</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><apply id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1"><eq id="S2.p5.2.m2.1.1.1.cmml" xref="S2.p5.2.m2.1.1.1"></eq><ci id="S2.p5.2.m2.1.1.2.cmml" xref="S2.p5.2.m2.1.1.2">𝑛</ci><cn type="integer" id="S2.p5.2.m2.1.1.3.cmml" xref="S2.p5.2.m2.1.1.3">222</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">n=222</annotation></semantics></math>), because preliminary analysis showed that utterances consisting of a single number are often too short for the engines to detect any speech.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We will first compare the models’ transcription accuracy, followed by the responsiveness, the impact of the microphone used, and finally a reflection on the power consumption of the models.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Transcription accuracy</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">First of all, we compare the performance of Google, Azure and the best Whisper model (large-v3) with the four engines reported in the 2017 paper. These results are shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1. Transcription accuracy ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. They show that the Google speech recognition did not improve compared to 2017 (Levenshtein distance <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="LD=0.38" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.2.1" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3.cmml">D</mi></mrow><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">0.38</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><eq id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></eq><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><times id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.2.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">𝐿</ci><ci id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">𝐷</ci></apply><cn type="float" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">0.38</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">LD=0.38</annotation></semantics></math> in 2017 and in 2024), but the performance of both the Azure model (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="LD=0.23" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.2.1" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml">D</mi></mrow><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">0.23</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><eq id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></eq><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><times id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">𝐿</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">𝐷</ci></apply><cn type="float" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">0.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">LD=0.23</annotation></semantics></math>), and the Whisper model (<math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="LD=0.14" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.2.1" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">D</mi></mrow><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">0.14</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><times id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝐿</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">𝐷</ci></apply><cn type="float" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">0.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">LD=0.14</annotation></semantics></math>) are better than all models tested in the 2017 paper, with Whisper performing best of all.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This is also reflected in the recognition percentage: in 2017, Google was able to recognise 7.5% of utterances correctly, in 2024, this became 9.6%, Azure recognises 23.5%, and Whisper 36.8%.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The relaxed accuracy score gives an impression of the usability of the models: in 2017, Google recognised 20.3% of the utterances correctly using relaxed criteria. This was only 14.7% in 2024, with Azure achieving 43.0% and Whisper 60.3%.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">While this is not yet an ideal performance level, this shows that Whisper is already rather usable, as small mistakes that do not count as accurate for the relaxed accuracy criteria, could still be handled by dialogue management software. Table <a href="#S3.T1" title="Table 1 ‣ 3.1. Transcription accuracy ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows some examples of small mistakes still made by Google, Azure and the best-performing Whisper model.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1. Transcription accuracy ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a more detailed comparison between the different model sizes of Whisper and the Azure and Google services. As expected, the large Whisper models perform best, with Whisper large v3 performing best of all.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.3" class="ltx_p">The Kruskal-Wallis test reveals significant differences between the Levenshtein distance for the tested models (<math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><lt id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></lt><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">𝑝</ci><cn type="float" id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">p&lt;0.001</annotation></semantics></math>), and post-hoc Dunn tests with Bonferroni correction do not show significant differences between Whisper large v3 and Whisper small, but do show a significant difference between, among others, all large Whisper models and Azure (<math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="p&lt;0.005" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><lt id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1"></lt><ci id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">𝑝</ci><cn type="float" id="S3.SS1.p6.2.m2.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">p&lt;0.005</annotation></semantics></math>), and between Azure and Google (<math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mrow id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><mi id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml">p</mi><mo id="S3.SS1.p6.3.m3.1.1.1" xref="S3.SS1.p6.3.m3.1.1.1.cmml">&lt;</mo><mn id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><lt id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1.1"></lt><ci id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2">𝑝</ci><cn type="float" id="S3.SS1.p6.3.m3.1.1.3.cmml" xref="S3.SS1.p6.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">p&lt;0.001</annotation></semantics></math>).</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.17394/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Performance of ASR engines in 2017 and 2024, calculated as mean normalised Levenshtein distance between ground truth and transcription (lower is better).</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Examples of Small Transcription Mistakes</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Ground truth</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">the dog is in front of the horse</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">Whisper</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">the dog is the front of the horse</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.3.2.1.1" class="ltx_text ltx_font_bold">Azure</span></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left">the dog is the front of the horse</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">Google</span></td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left">the song in the front of the horse</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.17394/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Performance of all current ASR engines (Whisper model versions in green, Google and Azure in red) calculated as Levenshtein distance between ground truth and transcription (lower is better).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Responsiveness</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2. Responsiveness ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the average transcription times for short sentences (spontaneous speech and repeat sentences) are shown for Google, Azure, all of the Whisper models on GPU and the tiny, base and small Whisper models on CPU. The transcription time for the Whisper medium and large models on CPU are respectively 17.5s and 30.5s, and were left out of the graph for readability. We visually mark the 1000ms line on the figure because, even though the mean response time in human conversation is 200ms, for spoken dialogue systems a delay of between 700 and 1000ms is deemed acceptable <cite class="ltx_cite ltx_citemacro_citep">(Skantze, <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>. From this data, it can be concluded that using a local model run on a GPU, instead of CPU or using an API, can greatly improve the responsiveness, until an acceptable level for spoken dialogue.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The Kruskal-Wallis test shows significant differences between the transcription time of the tested models (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><lt id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></lt><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑝</ci><cn type="float" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">p&lt;0.001</annotation></semantics></math>), and post-hoc Dunn tests with Bonferroni correction show significant differences between all pairs of models, except for between Whisper tiny, Whisper base and Whisper small, between Whisper large v3 and Whisper medium, and between Whisper large, Whisper large v2 and Azure.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2. Responsiveness ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the relation between the models’ average transcription time with their accuracy using the Levenshtein distance. To choose which model to use, both responsiveness and performance should be taken into account. Lower results are preferred for both, so models in the lower left corner of the scatter plot are ideal. As apparent in the figure, there is a trade-off between transcription time and transcription performance, so the choice should be made based on the specific application.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.17394/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="457" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Average transcription time for a sentence. Whisper models (in green) were run on GPU unless CPU is specified. Dashed line shows maximum acceptable delay of 1000ms.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.17394/assets/figures/scatter.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="421" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Transcription time vs. accuracy (lower is better). Whisper models were run on GPU unless CPU is specified. Ideal ASR systems would be in the lower left corner. </figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Microphone</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3. Microphone ‣ 3. Results ‣ Child Speech Recognition in Human-Robot Interaction: Problem Solved?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the Levenshtein distance when using audio recorded by the three different microphones. Here, the transcriptions by Google, Azure and Whisper large v3 were used. When comparing the results of the internal Nao microphone with the portable and studio microphone, the Kruskal-Wallis test shows a significant difference between the groups, and Dunn’s test with Bonferroni corrections as post-hoc analysis shows a significant difference between the Nao and portable microphone (<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">p</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><lt id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></lt><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑝</ci><cn type="float" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">p&lt;0.001</annotation></semantics></math>) and the Nao and studio microphone (<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><lt id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></lt><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑝</ci><cn type="float" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">p&lt;0.01</annotation></semantics></math>). There is no significant difference between the studio and portable microphone (<math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="p=0.399" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">p</mi><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">0.399</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><eq id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></eq><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝑝</ci><cn type="float" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">0.399</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">p=0.399</annotation></semantics></math>). In conclusion, the worst results are obtained when the microphone in the Nao robot is used, as there is a lot of added noise due to the closeness to the robot’s motor and ventilation, but no difference is found between both external microphones.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2404.17394/assets/x4.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Performance of Azure, Google and best Whisper model when using different microphone types, calculated as Levenshtein distance (lower is better). Best results are obtained when using a microphone external from the robot.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Energy consumption</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">As concerns have been raised over the energy consumption and consequently carbon emissions of state-of-the-art machine learning <cite class="ltx_cite ltx_citemacro_citep">(Lacoste et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, we think it is valuable to consider these for ASR systems. We only have been able to measure Whisper’s consumption. Per hour of transcribed data, the largest and best-performing model (large-v3) consumes <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="32.3Wh" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">32.3</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.1a" xref="S3.SS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.1.1.4" xref="S3.SS4.p1.1.m1.1.1.4.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><cn type="float" id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">32.3</cn><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑊</ci><ci id="S3.SS4.p1.1.m1.1.1.4.cmml" xref="S3.SS4.p1.1.m1.1.1.4">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">32.3Wh</annotation></semantics></math> and produces <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="7.7gCO_{2}eq" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mn id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">7.7</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.1a" xref="S3.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.4" xref="S3.SS4.p1.2.m2.1.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.1b" xref="S3.SS4.p1.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS4.p1.2.m2.1.1.5" xref="S3.SS4.p1.2.m2.1.1.5.cmml"><mi id="S3.SS4.p1.2.m2.1.1.5.2" xref="S3.SS4.p1.2.m2.1.1.5.2.cmml">O</mi><mn id="S3.SS4.p1.2.m2.1.1.5.3" xref="S3.SS4.p1.2.m2.1.1.5.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.1c" xref="S3.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.6" xref="S3.SS4.p1.2.m2.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.1d" xref="S3.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.7" xref="S3.SS4.p1.2.m2.1.1.7.cmml">q</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><times id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></times><cn type="float" id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">7.7</cn><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑔</ci><ci id="S3.SS4.p1.2.m2.1.1.4.cmml" xref="S3.SS4.p1.2.m2.1.1.4">𝐶</ci><apply id="S3.SS4.p1.2.m2.1.1.5.cmml" xref="S3.SS4.p1.2.m2.1.1.5"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.5.1.cmml" xref="S3.SS4.p1.2.m2.1.1.5">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.5.2.cmml" xref="S3.SS4.p1.2.m2.1.1.5.2">𝑂</ci><cn type="integer" id="S3.SS4.p1.2.m2.1.1.5.3.cmml" xref="S3.SS4.p1.2.m2.1.1.5.3">2</cn></apply><ci id="S3.SS4.p1.2.m2.1.1.6.cmml" xref="S3.SS4.p1.2.m2.1.1.6">𝑒</ci><ci id="S3.SS4.p1.2.m2.1.1.7.cmml" xref="S3.SS4.p1.2.m2.1.1.7">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">7.7gCO_{2}eq</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Based on our evaluation, we can make the following recommendations, updating or overriding those made in <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>:</p>
</div>
<div id="S4.p2" class="ltx_para">
<dl id="S4.I1" class="ltx_description">
<dt id="S4.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Recognition performance.: </span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">The recognition performance has improved dramatically for state-of-the-art ASR, with the best models of 2024 showing over 60% fewer transcription errors than in 2017. Still, adult-like recognition is not available yet, but the semantic content of children’s speech is now sufficiently transcribed to offer potential for robust spoken interaction, especially if other components within the dialogue management –such as large language models– can cover for suboptimal or even failing ASR.</p>
</div>
</dd>
<dt id="S4.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Responsiveness.: </span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">The responsiveness of locally hosted models (in our case OpenAI’s Whisper) is significantly better than that of cloud-based solutions, with sub-second results for some models. The network overhead and shared services of using cloud-based solutions are not optimal for real-time spoken interaction, and local models even outperform the cloud-based solutions in accuracy.</p>
</div>
</dd>
<dt id="S4.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Impact of microphone.: </span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix3.p1" class="ltx_para">
<p id="S4.I1.ix3.p1.1" class="ltx_p">Using an external microphone, as opposed to a microphone embedded in the robot, leads to a significantly improved recognition performance. Performance improves regardless of the quality of the microphone, as the robot’s noise has a stronger effect on the speech recognition than the choice of microphone.</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research received funding from imec (Smart Education), the Flemish Government (AI Research Program) and the Horizon Europe VALAWAI project (grant agreement number 101070930). We are indebted to the authors of <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Kennedy et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> for making the recordings and transcriptions available.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belpaeme et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tony Belpaeme, James Kennedy, Aditi Ramachandran, Brian Scassellati, and Fumihide Tanaka. 2018.

</span>
<span class="ltx_bibblock">Social robots for education: A review.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Science robotics</em> 3, 21 (2018), eaat5954.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kennedy et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
James Kennedy, Séverin Lemaignan, Caroline Montassier, Pauline Lavalade, Bahar Irfan, Fotios Papadopoulos, Emmanuel Senft, and Tony Belpaeme. 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Children speech recording (English, spontaneous speech + pre-defined sentences)</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.5281/zenodo.200495" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.200495</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kennedy et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
James Kennedy, Séverin Lemaignan, Caroline Montassier, Pauline Lavalade, Bahar Irfan, Fotios Papadopoulos, Emmanuel Senft, and Tony Belpaeme. 2017.

</span>
<span class="ltx_bibblock">Child speech recognition in human-robot interaction: evaluations and recommendations. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction</em>. 82–90.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lacoste et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019.

</span>
<span class="ltx_bibblock">Quantifying the carbon emissions of machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.09700</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, and Junaid Qadir. 2023.

</span>
<span class="ltx_bibblock">Transformers in speech processing: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11607</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 28492–28518.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skantze (2021)</span>
<span class="ltx_bibblock">
Gabriel Skantze. 2021.

</span>
<span class="ltx_bibblock">Turn-taking in conversational systems and human-robot interaction: a review.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em> 67 (2021), 101178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xuedong Huang, and Andreas Stolcke. 2018.

</span>
<span class="ltx_bibblock">The Microsoft 2017 conversational speech recognition system. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>. IEEE, 5934–5938.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.17393" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.17394" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.17394">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.17394" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.17395" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:29:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
