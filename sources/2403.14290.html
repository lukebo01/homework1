<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.14290] Exploring Green AI for Audio Deepfake Detection</title><meta property="og:description" content="The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly dueâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploring Green AI for Audio Deepfake Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploring Green AI for Audio Deepfake Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.14290">

<!--Generated on Fri Apr  5 16:44:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
ASVspoof,  Anti-spoofing,  Audio deepfake detection,  Green AI,  Low-carbon footprint,  Self-supervised learning.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Exploring Green AI for Audio Deepfake Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subhajit Saha<sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1,3</span></sup>, Md Sahidullah<sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">1,3</span></sup>, Swagatam Das<sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id10.10.id1" class="ltx_sup">1</sup>Institute for Advancing Intelligence, TCG Centres for Research and Education in Science and Technology
<br class="ltx_break"><sup id="id11.11.id2" class="ltx_sup">2</sup>Indian Statistical Institute, Kolkata
<br class="ltx_break"><sup id="id12.12.id3" class="ltx_sup">3</sup>Academy of Scientific and Innovative Research (AcSIR), Ghaziabad- 201002, India
<br class="ltx_break">e-mail: subhajit.saha.131@tcgcrest.org, md.sahidullah@tcgcrest.org, swagatam.das@tcgcrest.org
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly due to the use of high-performance computing with accelerators and high training time. Studies show that average deep NLP model produces around 626k lbs of CO<sub id="id13.id1.1" class="ltx_sub">2</sub> which is equivalent to five times of average US car emission at its lifetime. This is certainly a massive threat to the environment. To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources. Our proposed framework utilizes off-the-shelve self-supervised learning (SSL) based models which are pre-trained and available in public repositories. In contrast to existing methods that fine-tune SSL models and employ additional deep neural networks for downstream tasks, we exploit classical machine learning algorithms such as logistic regression and shallow neural networks using the SSL embeddings extracted using the pre-trained model. Our approach shows competitive results compared to the commonly used high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA dataset, we achieve a 0.90% equal error rate (EER) with less than 1k trainable model parameters. To encourage further research in this direction and support reproducible results, the Python code will be made publicly accessible following acceptance<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/sahasubhajit/Speech-Spoofing-" title="" class="ltx_ref ltx_href">GitHub link</a></span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
ASVspoof, Anti-spoofing, Audio deepfake detection, Green AI, Low-carbon footprint, Self-supervised learning.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Advancements in deep learning algorithms have revolutionized various fields of machine learning by showcasing impressive performance across different tasks in computer vision (CV), natural language processing (NLP), and speech technology, to name a fewÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Arguably, the majority of the improvement stems from the utilization of high-performance computing to process large amounts of image, video, text, and speech dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The organization of different machine learning challenges also boosted the collaborative efforts within the research community and facilitated systematic benchmarking of algorithms and methodologies. In spite of all these encouraging progress, the carbon footprint and energy consumption are becoming an important concernÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This study highlights environmental concerns of modern deep learning systems and introduces a framework for crafting an eco-friendly, computationally and memory-efficient machine learning system with high performance. It specifically targets its application in detecting audio deepfakes.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Modern deep learning algorithms, especially in the areas of CV, speech, and NLP are predominantly built upon foundation models comprising billions of parameters. Training these algorithms requires massive amounts of training data and often hundreds of GPUs, taking several days to complete. All of these necessitate significant energy consumption and expenditure which are often overlooked as obtaining â€œstate-of-the-artâ€ performance is typically the primary focusÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Leaderboards in machine learning challenges often prioritize showcasing the efficacy of models without duly considering their associated costs or efficiency. The cost for conducting a machine learning experiment to compute the result (<math id="S1.p2.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">R</annotation></semantics></math>), as defined byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>,</p>
</div>
<div id="S1.p3" class="ltx_para">
<table id="S1.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E1.m1.2" class="ltx_Math" alttext="\mathrm{Cost}(R)\propto E\times D\times H," display="block"><semantics id="S1.E1.m1.2a"><mrow id="S1.E1.m1.2.2.1" xref="S1.E1.m1.2.2.1.1.cmml"><mrow id="S1.E1.m1.2.2.1.1" xref="S1.E1.m1.2.2.1.1.cmml"><mrow id="S1.E1.m1.2.2.1.1.2" xref="S1.E1.m1.2.2.1.1.2.cmml"><mi id="S1.E1.m1.2.2.1.1.2.2" xref="S1.E1.m1.2.2.1.1.2.2.cmml">Cost</mi><mo lspace="0em" rspace="0em" id="S1.E1.m1.2.2.1.1.2.1" xref="S1.E1.m1.2.2.1.1.2.1.cmml">â€‹</mo><mrow id="S1.E1.m1.2.2.1.1.2.3.2" xref="S1.E1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S1.E1.m1.2.2.1.1.2.3.2.1" xref="S1.E1.m1.2.2.1.1.2.cmml">(</mo><mi id="S1.E1.m1.1.1" xref="S1.E1.m1.1.1.cmml">R</mi><mo stretchy="false" id="S1.E1.m1.2.2.1.1.2.3.2.2" xref="S1.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S1.E1.m1.2.2.1.1.1" xref="S1.E1.m1.2.2.1.1.1.cmml">âˆ</mo><mrow id="S1.E1.m1.2.2.1.1.3" xref="S1.E1.m1.2.2.1.1.3.cmml"><mi id="S1.E1.m1.2.2.1.1.3.2" xref="S1.E1.m1.2.2.1.1.3.2.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S1.E1.m1.2.2.1.1.3.1" xref="S1.E1.m1.2.2.1.1.3.1.cmml">Ã—</mo><mi id="S1.E1.m1.2.2.1.1.3.3" xref="S1.E1.m1.2.2.1.1.3.3.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S1.E1.m1.2.2.1.1.3.1a" xref="S1.E1.m1.2.2.1.1.3.1.cmml">Ã—</mo><mi id="S1.E1.m1.2.2.1.1.3.4" xref="S1.E1.m1.2.2.1.1.3.4.cmml">H</mi></mrow></mrow><mo id="S1.E1.m1.2.2.1.2" xref="S1.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.E1.m1.2b"><apply id="S1.E1.m1.2.2.1.1.cmml" xref="S1.E1.m1.2.2.1"><csymbol cd="latexml" id="S1.E1.m1.2.2.1.1.1.cmml" xref="S1.E1.m1.2.2.1.1.1">proportional-to</csymbol><apply id="S1.E1.m1.2.2.1.1.2.cmml" xref="S1.E1.m1.2.2.1.1.2"><times id="S1.E1.m1.2.2.1.1.2.1.cmml" xref="S1.E1.m1.2.2.1.1.2.1"></times><ci id="S1.E1.m1.2.2.1.1.2.2.cmml" xref="S1.E1.m1.2.2.1.1.2.2">Cost</ci><ci id="S1.E1.m1.1.1.cmml" xref="S1.E1.m1.1.1">ğ‘…</ci></apply><apply id="S1.E1.m1.2.2.1.1.3.cmml" xref="S1.E1.m1.2.2.1.1.3"><times id="S1.E1.m1.2.2.1.1.3.1.cmml" xref="S1.E1.m1.2.2.1.1.3.1"></times><ci id="S1.E1.m1.2.2.1.1.3.2.cmml" xref="S1.E1.m1.2.2.1.1.3.2">ğ¸</ci><ci id="S1.E1.m1.2.2.1.1.3.3.cmml" xref="S1.E1.m1.2.2.1.1.3.3">ğ·</ci><ci id="S1.E1.m1.2.2.1.1.3.4.cmml" xref="S1.E1.m1.2.2.1.1.3.4">ğ»</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E1.m1.2c">\mathrm{Cost}(R)\propto E\times D\times H,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">where the training cost <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">E</annotation></semantics></math> of executing the model on a single example, dataset size <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">D</annotation></semantics></math>, and the number of hyper-parameters <math id="S1.p4.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S1.p4.3.m3.1a"><mi id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><ci id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">H</annotation></semantics></math>. The participants with better computing resources often outperform others and the associated cost as given in Eq.Â <a href="#S1.E1" title="In I Introduction â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> remained overlooked. Even with alternative options like fine-tuning the pre-trained foundation model, these activities still demands high-performance computing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Recently, thereâ€™s been a push for <span id="S1.p5.1.1" class="ltx_text ltx_font_bold" style="color:#00FF00;">Green AI</span>, which is environmentally friendly, as opposed to the traditional <span id="S1.p5.1.2" class="ltx_text ltx_font_bold" style="color:#FF0000;">Red AI</span> that relies heavily on massive data and computationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Green AI refers to AI research that achieves new results while considering the computational costs and resources involved. Despite the significant scientific contributions of Red AI, thereâ€™s been less exploration of the Green AI option, which not only benefits the environment but is also suitable for medium-scale industries and academic institutions with limited budgets for computational resources. This study presents a new Green AI framework for detecting audio deepfakes.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Rapid advancements in neural generation of synthetic speech using <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">voice conversion</em> (VC) and <em id="S1.p6.1.2" class="ltx_emph ltx_font_italic">text-to-speech</em> (TTS) techniques, commonly referred to as audio deepfake technology, have presented a significant threat of identity theft and fraudulent impersonationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. A significant amount of research has been conducted in recent years, largely due to the availability of publicly accessible large datasets from initiatives like the ASVspoof challenge series<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.asvspoof.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.asvspoof.org/</a></span></span></span>, ADD challenges<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://addchallenge.cn/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://addchallenge.cn/</a></span></span></span>, and othersÂ <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://deepfake-demo.aisec.fraunhofer.de/in_the_wild" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://deepfake-demo.aisec.fraunhofer.de/in_the_wild</a></span></span></span>. Algorithms developed to detect audio deepfakes primarily utilize speech spectrum or other features as the front-end, with the back-end consisting of deep neural networks (DNNs) based on convolutional neural networks (CNNs) or their variants such as ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The key idea is to learn the artifact in synthesized speech and model them using neural networks. A light <em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">convolutional neural network</em> (LCNN) with <em id="S1.p6.1.4" class="ltx_emph ltx_font_italic">linear-frequency cepstral coefficients</em> (LFCCs) has been shown state-of-the-art results on several ASVspoof datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Recently, the <em id="S1.p6.1.5" class="ltx_emph ltx_font_italic">audio anti-spoofing using integrated spectro-temporal graph attention networks</em> (AASIST) approach has shown promising results, utilizing raw waveform as inputÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The AASIST framework is further improved by integrating with the wav2vec 2.0 based speech foundation model as the front-end, which undergoes joint fine-tuning with the AASIST classifierÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In recent times, a good amount of research has been conducted in this direction either by exploring other speech foundation models or by combining various levels of information using fusion techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. All these efforts help in improving the deepfake detection performance; however, they belong to the Red AI approach and drastically increase the computational overhead and latency.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our work builds on insights from ongoing Red AI research. It is widely known that speech foundation models are efficient at learning speech patterns to distinguish real from fake speech. We revisit classical machine learning algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> with those learned representations. We advocate for using the publicly available, widely used speech foundation model for feature extraction without further fine-tuning itÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The proposed approach falls under the category of Green AI as it does not necessarily require high-performance computing for training and inference of the deepfake detector. Moreover, we explore layer-wise representations and the potential use of earlier layers of the speech foundation model, which significantly reduces parameters and computational time. To the best of our knowledge, this is the first study to investigate the use of classical machine learning techniques alongside speech foundation models for audio deepfake detection.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The main contributions of this work are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We employ speech foundation models for speech feature extraction without further fine-tuning.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">In stead of using deep models which is widely studied, we thoroughly evaluate the effectiveness of up to six different classical ML algorithms.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our comprehensive experiments on the LA subset of the ASVspoof 2019 dataset demonstrate that state-of-the-art results can be achieved by considering the initial layer representations from the speech foundation model.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We achieve competitive results with a very few model parameters, and both training and inference computations can be efficiently performed with just single CPU.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Overview of the Proposed Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this study, we aim to develop a low-carbon footprint methodology for speech anti-spoofing, leveraging the synergies between self-supervised learning (SSL)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and traditional supervised machine learning algorithms. While state-of-the-art deep speech foundation models have shown effectiveness in representing raw audio for various tasks, integrating them into downstream tasks requires additional deep learning architectures, and fine-tuning the SSL model for task-specific needs adds complexity by increasing trainable parameters. Our proposed methodology exploits learned representations from various layers of the SSL model, recognizing that each layer contains distinct abstractions of speech. We also propose using aggregated learned representations with a low-cost classification algorithm commonly employed for pattern recognition tasks. As illustrated in Fig.Â <a href="#S2.F1" title="Figure 1 â€£ II Overview of the Proposed Methodology â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our proposed framework consists of two key components: (i)Â a <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">deep feature extractor</em> based on publicly available pre-trained SSL model, and (ii)Â a <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">downstream classifier</em> requiring fewer training parameters compared to commonly used DNNs.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.11" class="ltx_p">Let us denote the two key models, the model trained for pretext task and the downstream task, in the proposed framework as <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\Theta_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">Î˜</mi><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">Î˜</ci><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\Theta_{\mathrm{ptext}}</annotation></semantics></math> and <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="\theta_{\mathrm{dstream}}" display="inline"><semantics id="S2.p2.2.m2.1a"><msub id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">Î¸</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">dstream</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">ğœƒ</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">dstream</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\theta_{\mathrm{dstream}}</annotation></semantics></math>, respectively. Additionally, consider that those two models are trained with <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">ğ’Ÿ</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\mathcal{D}_{\mathrm{ptext}}</annotation></semantics></math> (large unlabelled data) and <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{dstreamt}}" display="inline"><semantics id="S2.p2.4.m4.1a"><msub id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">dstreamt</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">ğ’Ÿ</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">dstreamt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">\mathcal{D}_{\mathrm{dstreamt}}</annotation></semantics></math> (limited unlabelled data), respectively. Usually, the duration of <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.5.m5.1a"><msub id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">ğ’Ÿ</ci><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">\mathcal{D}_{\mathrm{ptext}}</annotation></semantics></math> in hours significantly higher than that of <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{dstreamt}}" display="inline"><semantics id="S2.p2.6.m6.1a"><msub id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">dstreamt</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">ğ’Ÿ</ci><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">dstreamt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">\mathcal{D}_{\mathrm{dstreamt}}</annotation></semantics></math>, aligning with the relative differences in parameter counts between <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="\Theta_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.7.m7.1a"><msub id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mi mathvariant="normal" id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">Î˜</mi><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1">subscript</csymbol><ci id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">Î˜</ci><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">\Theta_{\mathrm{ptext}}</annotation></semantics></math> and <math id="S2.p2.8.m8.1" class="ltx_Math" alttext="\theta_{\mathrm{dstream}}" display="inline"><semantics id="S2.p2.8.m8.1a"><msub id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><mi id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml">Î¸</mi><mi id="S2.p2.8.m8.1.1.3" xref="S2.p2.8.m8.1.1.3.cmml">dstream</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1">subscript</csymbol><ci id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2">ğœƒ</ci><ci id="S2.p2.8.m8.1.1.3.cmml" xref="S2.p2.8.m8.1.1.3">dstream</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">\theta_{\mathrm{dstream}}</annotation></semantics></math>. Now we intended to use a <em id="S2.p2.11.1" class="ltx_emph ltx_font_italic">slice</em> of the pre-trained SSL model which can be denoted by <math id="S2.p2.9.m9.1" class="ltx_Math" alttext="\theta_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.9.m9.1a"><msub id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml"><mi id="S2.p2.9.m9.1.1.2" xref="S2.p2.9.m9.1.1.2.cmml">Î¸</mi><mi id="S2.p2.9.m9.1.1.3" xref="S2.p2.9.m9.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.1b"><apply id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p2.9.m9.1.1.1.cmml" xref="S2.p2.9.m9.1.1">subscript</csymbol><ci id="S2.p2.9.m9.1.1.2.cmml" xref="S2.p2.9.m9.1.1.2">ğœƒ</ci><ci id="S2.p2.9.m9.1.1.3.cmml" xref="S2.p2.9.m9.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.1c">\theta_{\mathrm{ptext}}</annotation></semantics></math>. Therefore, if we only consider first a few layers of the pre-text SSL model, the number of parameters of the <math id="S2.p2.10.m10.1" class="ltx_Math" alttext="\theta_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.10.m10.1a"><msub id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml"><mi id="S2.p2.10.m10.1.1.2" xref="S2.p2.10.m10.1.1.2.cmml">Î¸</mi><mi id="S2.p2.10.m10.1.1.3" xref="S2.p2.10.m10.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.1b"><apply id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p2.10.m10.1.1.1.cmml" xref="S2.p2.10.m10.1.1">subscript</csymbol><ci id="S2.p2.10.m10.1.1.2.cmml" xref="S2.p2.10.m10.1.1.2">ğœƒ</ci><ci id="S2.p2.10.m10.1.1.3.cmml" xref="S2.p2.10.m10.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.1c">\theta_{\mathrm{ptext}}</annotation></semantics></math> can be substantially lower than the number of parameters in the entire <math id="S2.p2.11.m11.1" class="ltx_Math" alttext="\Theta_{\mathrm{ptext}}" display="inline"><semantics id="S2.p2.11.m11.1a"><msub id="S2.p2.11.m11.1.1" xref="S2.p2.11.m11.1.1.cmml"><mi mathvariant="normal" id="S2.p2.11.m11.1.1.2" xref="S2.p2.11.m11.1.1.2.cmml">Î˜</mi><mi id="S2.p2.11.m11.1.1.3" xref="S2.p2.11.m11.1.1.3.cmml">ptext</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.1b"><apply id="S2.p2.11.m11.1.1.cmml" xref="S2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S2.p2.11.m11.1.1.1.cmml" xref="S2.p2.11.m11.1.1">subscript</csymbol><ci id="S2.p2.11.m11.1.1.2.cmml" xref="S2.p2.11.m11.1.1.2">Î˜</ci><ci id="S2.p2.11.m11.1.1.3.cmml" xref="S2.p2.11.m11.1.1.3">ptext</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.1c">\Theta_{\mathrm{ptext}}</annotation></semantics></math>. This aspect not only reduces the number of parameters but also makes it possible to utilize conventional CPUs for faster inference. The algorithm of the proposed framework can now be summarized in<span id="S2.p2.11.2" class="ltx_text ltx_font_bold"> AlgorithmÂ <a href="#alg1" title="In II Overview of the Proposed Methodology â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_algorithm">
<div id="alg1.2" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="alg1.2.1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">1</span></span>

<div id="alg1.2.1.2" class="ltx_listing ltx_listing">
</div>
</div>
<div id="alg1.2.2" class="ltx_listingline">
</div>
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Pre-training:</span><span id="alg1.l1.3" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text" style="font-size:80%;">Train the model on </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{ptext}}" display="inline"><semantics id="alg1.l2.m1.1a"><msub id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">ğ’Ÿ</mi><mtext mathsize="80%" id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3a.cmml">ptext</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><csymbol cd="ambiguous" id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1">subscript</csymbol><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">ğ’Ÿ</ci><ci id="alg1.l2.m1.1.1.3a.cmml" xref="alg1.l2.m1.1.1.3"><mtext mathsize="56%" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">ptext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">\mathcal{D}_{\text{ptext}}</annotation></semantics></math><span id="alg1.l2.3" class="ltx_text" style="font-size:80%;"> and estimate </span><math id="alg1.l2.m2.1" class="ltx_Math" alttext="\Theta_{\text{ptext}}" display="inline"><semantics id="alg1.l2.m2.1a"><msub id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="alg1.l2.m2.1.1.2" xref="alg1.l2.m2.1.1.2.cmml">Î˜</mi><mtext mathsize="80%" id="alg1.l2.m2.1.1.3" xref="alg1.l2.m2.1.1.3a.cmml">ptext</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><apply id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1"><csymbol cd="ambiguous" id="alg1.l2.m2.1.1.1.cmml" xref="alg1.l2.m2.1.1">subscript</csymbol><ci id="alg1.l2.m2.1.1.2.cmml" xref="alg1.l2.m2.1.1.2">Î˜</ci><ci id="alg1.l2.m2.1.1.3a.cmml" xref="alg1.l2.m2.1.1.3"><mtext mathsize="56%" id="alg1.l2.m2.1.1.3.cmml" xref="alg1.l2.m2.1.1.3">ptext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">\Theta_{\text{ptext}}</annotation></semantics></math><span id="alg1.l2.4" class="ltx_text" style="font-size:80%;">.
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text ltx_font_bold" style="font-size:80%;">End Pre-training.</span><span id="alg1.l3.3" class="ltx_text" style="font-size:80%;">

</span>
<span id="alg1.l3.4" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="alg1.l4" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Downstream feature extraction and modeling:</span><span id="alg1.l4.3" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span id="alg1.l5" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.2" class="ltx_text" style="font-size:80%;">Compute the features using </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="\theta_{\text{ptext}}(\subset\Theta_{\text{ptext}})" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><msub id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"><mi mathsize="80%" id="alg1.l5.m1.1.1.3.2" xref="alg1.l5.m1.1.1.3.2.cmml">Î¸</mi><mtext mathsize="80%" id="alg1.l5.m1.1.1.3.3" xref="alg1.l5.m1.1.1.3.3a.cmml">ptext</mtext></msub><mspace width="0.3888888888888889em" id="alg1.l5.m1.1.1a" xref="alg1.l5.m1.1.1.cmml"></mspace><mrow id="alg1.l5.m1.1.1.1.1" xref="alg1.l5.m1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="alg1.l5.m1.1.1.1.1.2" xref="alg1.l5.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l5.m1.1.1.1.1.1" xref="alg1.l5.m1.1.1.1.1.1.cmml"><mi id="alg1.l5.m1.1.1.1.1.1.2" xref="alg1.l5.m1.1.1.1.1.1.2.cmml"></mi><mo mathsize="80%" id="alg1.l5.m1.1.1.1.1.1.1" xref="alg1.l5.m1.1.1.1.1.1.1.cmml">âŠ‚</mo><msub id="alg1.l5.m1.1.1.1.1.1.3" xref="alg1.l5.m1.1.1.1.1.1.3.cmml"><mi mathsize="80%" mathvariant="normal" id="alg1.l5.m1.1.1.1.1.1.3.2" xref="alg1.l5.m1.1.1.1.1.1.3.2.cmml">Î˜</mi><mtext mathsize="80%" id="alg1.l5.m1.1.1.1.1.1.3.3" xref="alg1.l5.m1.1.1.1.1.1.3.3a.cmml">ptext</mtext></msub></mrow><mo maxsize="80%" minsize="80%" id="alg1.l5.m1.1.1.1.1.3" xref="alg1.l5.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><csymbol cd="latexml" id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1">annotated</csymbol><apply id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.3">subscript</csymbol><ci id="alg1.l5.m1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.3.2">ğœƒ</ci><ci id="alg1.l5.m1.1.1.3.3a.cmml" xref="alg1.l5.m1.1.1.3.3"><mtext mathsize="56%" id="alg1.l5.m1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.3.3">ptext</mtext></ci></apply><apply id="alg1.l5.m1.1.1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1.1"><subset id="alg1.l5.m1.1.1.1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1.1.1.1"></subset><csymbol cd="latexml" id="alg1.l5.m1.1.1.1.1.1.2.cmml" xref="alg1.l5.m1.1.1.1.1.1.2">absent</csymbol><apply id="alg1.l5.m1.1.1.1.1.1.3.cmml" xref="alg1.l5.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.l5.m1.1.1.1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.1.1.1.3.2">Î˜</ci><ci id="alg1.l5.m1.1.1.1.1.1.3.3a.cmml" xref="alg1.l5.m1.1.1.1.1.1.3.3"><mtext mathsize="56%" id="alg1.l5.m1.1.1.1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.1.1.1.3.3">ptext</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">\theta_{\text{ptext}}(\subset\Theta_{\text{ptext}})</annotation></semantics></math><span id="alg1.l5.3" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span id="alg1.l6" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:80%;">Average pooling over frames.
</span>
</span>
<span id="alg1.l7" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:80%;">Train on </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{dstreamt}}" display="inline"><semantics id="alg1.l7.m1.1a"><msub id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">ğ’Ÿ</mi><mtext mathsize="80%" id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3a.cmml">dstreamt</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">ğ’Ÿ</ci><ci id="alg1.l7.m1.1.1.3a.cmml" xref="alg1.l7.m1.1.1.3"><mtext mathsize="56%" id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">dstreamt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\mathcal{D}_{\text{dstreamt}}</annotation></semantics></math><span id="alg1.l7.3" class="ltx_text" style="font-size:80%;"> to get </span><math id="alg1.l7.m2.1" class="ltx_Math" alttext="\theta_{\text{ptext}}" display="inline"><semantics id="alg1.l7.m2.1a"><msub id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml"><mi mathsize="80%" id="alg1.l7.m2.1.1.2" xref="alg1.l7.m2.1.1.2.cmml">Î¸</mi><mtext mathsize="80%" id="alg1.l7.m2.1.1.3" xref="alg1.l7.m2.1.1.3a.cmml">ptext</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><apply id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.1.cmml" xref="alg1.l7.m2.1.1">subscript</csymbol><ci id="alg1.l7.m2.1.1.2.cmml" xref="alg1.l7.m2.1.1.2">ğœƒ</ci><ci id="alg1.l7.m2.1.1.3a.cmml" xref="alg1.l7.m2.1.1.3"><mtext mathsize="56%" id="alg1.l7.m2.1.1.3.cmml" xref="alg1.l7.m2.1.1.3">ptext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">\theta_{\text{ptext}}</annotation></semantics></math><span id="alg1.l7.4" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span id="alg1.l8" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">2</span></span><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.2.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.3" class="ltx_text ltx_font_bold" style="font-size:80%;">End Downstream Classification.</span><span id="alg1.l8.4" class="ltx_text" style="font-size:80%;">

</span>
</span>
<span id="alg1.l3.4.1" class="ltx_listingline">
</span>
<span id="alg1.l9" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Evaluation:</span><span id="alg1.l9.3" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span id="alg1.l10" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:80%;">Compute the features using </span><math id="alg1.l10.m1.1" class="ltx_Math" alttext="\theta_{\text{ptext}}" display="inline"><semantics id="alg1.l10.m1.1a"><msub id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi mathsize="80%" id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">Î¸</mi><mtext mathsize="80%" id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3a.cmml">ptext</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><csymbol cd="ambiguous" id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1">subscript</csymbol><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">ğœƒ</ci><ci id="alg1.l10.m1.1.1.3a.cmml" xref="alg1.l10.m1.1.1.3"><mtext mathsize="56%" id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">ptext</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">\theta_{\text{ptext}}</annotation></semantics></math><span id="alg1.l10.3" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span id="alg1.l11" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg1.l11.2" class="ltx_text" style="font-size:80%;">Average pooling over frames.
</span>
</span>
<span id="alg1.l12" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg1.l12.2" class="ltx_text" style="font-size:80%;">Predict the decision using </span><math id="alg1.l12.m1.1" class="ltx_Math" alttext="\mathcal{\theta}_{\text{dstreamt}}" display="inline"><semantics id="alg1.l12.m1.1a"><msub id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi mathsize="80%" id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">Î¸</mi><mtext mathsize="80%" id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3a.cmml">dstreamt</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1">subscript</csymbol><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">ğœƒ</ci><ci id="alg1.l12.m1.1.1.3a.cmml" xref="alg1.l12.m1.1.1.3"><mtext mathsize="56%" id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3">dstreamt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">\mathcal{\theta}_{\text{dstreamt}}</annotation></semantics></math><span id="alg1.l12.3" class="ltx_text" style="font-size:80%;">.
</span>
</span>
<span id="alg1.l13" class="ltx_listingline"><span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg1.l13.2" class="ltx_text ltx_font_bold" style="font-size:80%;">End Evaluation.</span><span id="alg1.l13.3" class="ltx_text" style="font-size:80%;">
</span>
</span>
</span><span id="alg1.l3.5" class="ltx_text" style="font-size:80%;"> </span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.3.1.1" class="ltx_text ltx_font_bold">AlgorithmÂ 1</span> </span>Summary of the proposed algorithm.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The remaining part of the section contains the description of the adopted SSL and downstream model.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.14290/assets/ssl_fig.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic diagram of the proposed framework illustrating SSL training (in <span id="S2.F1.3.1" class="ltx_text" style="color:#FF0000;">red</span>) and downstream training (in <span id="S2.F1.4.2" class="ltx_text" style="color:#00FF00;">green</span>).</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Description of the SSL model</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this study, we have considered the wav2vec 2.0 model, which is recognized as one of the most commonly used SSL frameworks for speech tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. This consists of a feature encoder, followed by several transformer layers and a quantization module. The feature encoder takes raw waveform as input and consists of a temporal convolution followed by layer normalization and a GELU activation function. The features are processed at the contextual level with several transformer modules. A convolutional layer is used for relative positional embedding. The quantization module discretizes the feature encoder output using product quantization.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.5" class="ltx_p">We have considered the wav2vec 2.0 <span id="S2.SS1.p2.5.1" class="ltx_text ltx_font_typewriter">BASE</span> model<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec</a></span></span></span> with Hugging Face interface<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://huggingface.co/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/</a></span></span></span> for implementation. The model consists of 12 transformer layers following a CNN with positional encoding layer, which is lightweight compared to the wav2vec 2.0 <span id="S2.SS1.p2.5.2" class="ltx_text ltx_font_typewriter">LARGE</span> model, and requires several days to train even with 128 V100 GPUs. It is also important to note that for processing a speech signal of approximately <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="3.5" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">3.5</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><cn type="float" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">3.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">3.5</annotation></semantics></math>Â s (the average size of the training data used in this study), wav2vec 2.0 <span id="S2.SS1.p2.5.3" class="ltx_text ltx_font_typewriter">BASE</span> requires a <em id="S2.SS1.p2.5.4" class="ltx_emph ltx_font_italic">multiply-accumulate count</em> (MAC) of <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="23.04" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">23.04</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><cn type="float" id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">23.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">23.04</annotation></semantics></math>Â GMACs, while wav2vec 2.0 <span id="S2.SS1.p2.5.5" class="ltx_text ltx_font_typewriter">LARGE</span> requires <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="60.22" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mn id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">60.22</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><cn type="float" id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">60.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">60.22</annotation></semantics></math>Â GMACs. We have extracted the embeddings from each of the transformer layers as well as the feature encoder layer. The dimension of extracted embeddings is <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="768\times N" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mn id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">768</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><times id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></times><cn type="integer" id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">768</cn><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">768\times N</annotation></semantics></math>, where 768 is due to the model size of the wav2vec 2.0 <span id="S2.SS1.p2.5.6" class="ltx_text ltx_font_typewriter">BASE</span> model and <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">N</annotation></semantics></math> is a variable that depends on the number of frames. We compute frame-level average embeddings, i.e., 768-dimensional features, and use them as input to the downstream classifier.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">FigureÂ <a href="#S2.F2" title="Figure 2 â€£ II-A Description of the SSL model â€£ II Overview of the Proposed Methodology â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the t-SNE visualization of speech embeddings from two classes computed from various layers of the wav2vec 2.0. Interestingly, the embeddings from the feature encoder layer are visually more distinguishable than the other, which motivates us to explore embeddings from other layers.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.14290/assets/tsne_fig.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="354" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>t-SNE plots showing embeddings from both the feature encoder output (left) and the 12th layer (last transformer) (right), based on the training set of ASVspoof 2019 LA subset consisting of both bonafide and spoof classes.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Description of downstream classifiers</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We assess the performance of six traditional classification algorithms listed below. They are capable of being trained efficiently, even on low-end CPUs with limited memory.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.5.1.1" class="ltx_text">II-B</span>1 </span>K-nearest neighbors (KNN)</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Our first classification approach employs the k-nearest neighbors algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This is a non-parametric approach and predicts the class labels of test data by considering the majority class among its k nearest neighbors in the feature space.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.5.1.1" class="ltx_text">II-B</span>2 </span>Logistic regression</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Logistic regression is a parametric statistical method used for predicting the probability of an observation belonging to a particular classÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. It accomplishes this by estimating model parameters that linearly combine input features, resulting in probabilities bounded between 0 and 1. These probabilities are determined through the logistic function. The model parameters are optimized by maximizing the log likelihood function based on the training dataset <math id="S2.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{train}}" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><msub id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS2.p1.1.m1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.SS2.SSS2.p1.1.m1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><apply id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2">ğ’Ÿ</ci><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">\mathcal{D}_{\mathrm{train}}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.5.1.1" class="ltx_text">II-B</span>3 </span>Support vector machine</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The support vector machine (SVM) finds an optimal decision boundary by maximizing the margin between two classesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In this work, SVM employs a kernel trick, using a radial basis function (RBF) kernel, to map the input data into a higher-dimensional feature space where a linear decision boundary can be more easily determined.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS4.5.1.1" class="ltx_text">II-B</span>4 </span>Naive Bayes</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">A simple classic ML technique which is used for classification task. It uses Bayes theorem and estimates the posterior probability value of the test sample for each class (also assuming conditional independence between all input features) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. To deal with continuous feature we have used Gaussian Naive Bayes algorithm.</p>
</div>
</section>
<section id="S2.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS5.5.1.1" class="ltx_text">II-B</span>5 </span>Decision tree</h4>

<div id="S2.SS2.SSS5.p1" class="ltx_para">
<p id="S2.SS2.SSS5.p1.1" class="ltx_p">Decision trees are a popular machine learning algorithm known for their simplicity and interpretabilityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. They recursively split the data based on some conditions to make predictions or classify instances.</p>
</div>
</section>
<section id="S2.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS6.5.1.1" class="ltx_text">II-B</span>6 </span>Multi-layer perceptron</h4>

<div id="S2.SS2.SSS6.p1" class="ltx_para">
<p id="S2.SS2.SSS6.p1.1" class="ltx_p">The multi-layer perceptron (MLP) consists of only one hidden layerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. We have not increased the depth to maintain a lower model complexity.</p>
</div>
<div id="S2.SS2.SSS6.p2" class="ltx_para">
<p id="S2.SS2.SSS6.p2.1" class="ltx_p">We have implemented this with the help of the scikit-learn library for PythonÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> utilizing its default parameter settings unless specified otherwise.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We utilized the logical access (LA) subset of the ASVSpoof 2019 dataset for our spoofing detection experiment, as detailed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Even though there are several other datasets, this dataset is widely used for spoofing detection research mainly due to its attack diversity and highly controlled spoofing generation and evaluation protocolÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The LA subset comprises 19 TTS and VC attacks distributed across training, development, and evaluation subsets. Different statistical and neural methods have been used to generate high quality spoofed data. The training and development sets share the same set of six attacks, whereas the evaluation set comprises 13 unseen attacks. This setup facilitates the evaluation of the generalization ability of the proposed algorithm.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The pre-trained SSL model we employed for this work was trained with 960 hours of audio from the LibriSpeech datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The audio data comprises a read speech corpus in English, similar to the VCTK corpora upon which the ASVspoof 2019 dataset is developedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Note that although both LibriSpeech and VCTK contain read speech data in English, the latter is recorded in a higher quality studio environment, introducing mismatch in the data conditions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Hyperparameter settings for downstream classification task</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For downstream classification models, we have conducted a simple brute-force grid search for finding the appropriate hyperparameter setting. We select the values corresponds to the best F1 score on the development set. The TableÂ <a href="#S3.T1" title="TABLE I â€£ III-B Hyperparameter settings for downstream classification task â€£ III Experimental Setup â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> describes the key hyperparameter settings of each classification algorithm along with the search spaces. Note that the final chosen hyperparameter is highlighted in boldface for each case.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Hyperparameters related to grid search for classifiers.</figcaption>
<table id="S3.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.8.9.1" class="ltx_tr">
<th id="S3.T1.8.9.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.8.9.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.9.1.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.8.9.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></span>
</span>
</th>
<th id="S3.T1.8.9.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.8.9.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.9.1.2.1.1" class="ltx_p" style="width:176.4pt;"><span id="S3.T1.8.9.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Hyperparameter settings</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">k-NN</span></span>
</span>
</td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1" class="ltx_p" style="width:176.4pt;"><math id="S3.T1.1.1.1.1.1.m1.3" class="ltx_Math" alttext="\mathtt{k=\{3,5,6\}}" display="inline"><semantics id="S3.T1.1.1.1.1.1.m1.3a"><mrow id="S3.T1.1.1.1.1.1.m1.3.4" xref="S3.T1.1.1.1.1.1.m1.3.4.cmml"><mi mathsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.2" xref="S3.T1.1.1.1.1.1.m1.3.4.2.cmml">ğš”</mi><mo mathsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.1" xref="S3.T1.1.1.1.1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.T1.1.1.1.1.1.m1.3.4.3.2" xref="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.3.2.1" xref="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml">{</mo><mn mathsize="70%" id="S3.T1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.m1.1.1.cmml">ğŸ¹</mn><mo mathsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.3.2.2" xref="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml">,</mo><mn mathsize="70%" id="S3.T1.1.1.1.1.1.m1.2.2" xref="S3.T1.1.1.1.1.1.m1.2.2.cmml">ğŸ»</mn><mo mathsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.3.2.3" xref="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml">,</mo><mn mathsize="70%" id="S3.T1.1.1.1.1.1.m1.3.3" xref="S3.T1.1.1.1.1.1.m1.3.3.cmml">ğŸ¼</mn><mo maxsize="70%" minsize="70%" id="S3.T1.1.1.1.1.1.m1.3.4.3.2.4" xref="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.m1.3b"><apply id="S3.T1.1.1.1.1.1.m1.3.4.cmml" xref="S3.T1.1.1.1.1.1.m1.3.4"><eq id="S3.T1.1.1.1.1.1.m1.3.4.1.cmml" xref="S3.T1.1.1.1.1.1.m1.3.4.1"></eq><ci id="S3.T1.1.1.1.1.1.m1.3.4.2.cmml" xref="S3.T1.1.1.1.1.1.m1.3.4.2">ğš”</ci><set id="S3.T1.1.1.1.1.1.m1.3.4.3.1.cmml" xref="S3.T1.1.1.1.1.1.m1.3.4.3.2"><cn type="integer" id="S3.T1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.m1.1.1">3</cn><cn type="integer" id="S3.T1.1.1.1.1.1.m1.2.2.cmml" xref="S3.T1.1.1.1.1.1.m1.2.2">5</cn><cn type="integer" id="S3.T1.1.1.1.1.1.m1.3.3.cmml" xref="S3.T1.1.1.1.1.1.m1.3.3">6</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.m1.3c">\mathtt{k=\{3,5,6\}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">Logistic regression</span></span>
</span>
</td>
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.1.1.1" class="ltx_p" style="width:176.4pt;"><math id="S3.T1.2.2.1.1.1.m1.3" class="ltx_Math" alttext="\mathtt{\text{regularization~{}factor}=\{0.2,0.1,10\}}" display="inline"><semantics id="S3.T1.2.2.1.1.1.m1.3a"><mrow id="S3.T1.2.2.1.1.1.m1.3.4" xref="S3.T1.2.2.1.1.1.m1.3.4.cmml"><mtext mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.2" xref="S3.T1.2.2.1.1.1.m1.3.4.2a.cmml">regularizationÂ factor</mtext><mo mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.1" xref="S3.T1.2.2.1.1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.T1.2.2.1.1.1.m1.3.4.3.2" xref="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.3.2.1" xref="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml">{</mo><mn class="ltx_mathvariant_monospace" mathsize="70%" mathvariant="monospace" id="S3.T1.2.2.1.1.1.m1.1.1" xref="S3.T1.2.2.1.1.1.m1.1.1.cmml">0.2</mn><mo mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.3.2.2" xref="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml">,</mo><mn class="ltx_mathvariant_monospace" mathsize="70%" mathvariant="monospace" id="S3.T1.2.2.1.1.1.m1.2.2" xref="S3.T1.2.2.1.1.1.m1.2.2.cmml">0.1</mn><mo mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.3.2.3" xref="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml">,</mo><mn mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.3" xref="S3.T1.2.2.1.1.1.m1.3.3.cmml">ğŸ·ğŸ¶</mn><mo maxsize="70%" minsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.3.2.4" xref="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.1.1.m1.3b"><apply id="S3.T1.2.2.1.1.1.m1.3.4.cmml" xref="S3.T1.2.2.1.1.1.m1.3.4"><eq id="S3.T1.2.2.1.1.1.m1.3.4.1.cmml" xref="S3.T1.2.2.1.1.1.m1.3.4.1"></eq><ci id="S3.T1.2.2.1.1.1.m1.3.4.2a.cmml" xref="S3.T1.2.2.1.1.1.m1.3.4.2"><mtext mathsize="70%" id="S3.T1.2.2.1.1.1.m1.3.4.2.cmml" xref="S3.T1.2.2.1.1.1.m1.3.4.2">regularizationÂ factor</mtext></ci><set id="S3.T1.2.2.1.1.1.m1.3.4.3.1.cmml" xref="S3.T1.2.2.1.1.1.m1.3.4.3.2"><cn type="float" id="S3.T1.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.2.2.1.1.1.m1.1.1">0.2</cn><cn type="float" id="S3.T1.2.2.1.1.1.m1.2.2.cmml" xref="S3.T1.2.2.1.1.1.m1.2.2">0.1</cn><cn type="integer" id="S3.T1.2.2.1.1.1.m1.3.3.cmml" xref="S3.T1.2.2.1.1.1.m1.3.3">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.1.1.m1.3c">\mathtt{\text{regularization~{}factor}=\{0.2,0.1,10\}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.3.3.2.1.1.1" class="ltx_text" style="font-size:70%;">SVM</span></span>
</span>
</td>
<td id="S3.T1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.1.1.1" class="ltx_p" style="width:176.4pt;"><math id="S3.T1.3.3.1.1.1.m1.6" class="ltx_Math" alttext="\mathtt{\text{regularization~{}factor}=\{0.2,0.1,1\},kernel=\{rbf\}}" display="inline"><semantics id="S3.T1.3.3.1.1.1.m1.6a"><mrow id="S3.T1.3.3.1.1.1.m1.6.6.2" xref="S3.T1.3.3.1.1.1.m1.6.6.3.cmml"><mrow id="S3.T1.3.3.1.1.1.m1.5.5.1.1" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.cmml"><mtext mathsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.2" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.2a.cmml">regularizationÂ factor</mtext><mo mathsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.1" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2.1" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml">{</mo><mn class="ltx_mathvariant_monospace" mathsize="70%" mathvariant="monospace" id="S3.T1.3.3.1.1.1.m1.1.1" xref="S3.T1.3.3.1.1.1.m1.1.1.cmml">0.2</mn><mo mathsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2.2" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml">,</mo><mn class="ltx_mathvariant_monospace" mathsize="70%" mathvariant="monospace" id="S3.T1.3.3.1.1.1.m1.2.2" xref="S3.T1.3.3.1.1.1.m1.2.2.cmml">0.1</mn><mo mathsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2.3" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml">,</mo><mn mathsize="70%" id="S3.T1.3.3.1.1.1.m1.3.3" xref="S3.T1.3.3.1.1.1.m1.3.3.cmml">ğŸ·</mn><mo maxsize="70%" minsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2.4" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml">}</mo></mrow></mrow><mo mathsize="70%" id="S3.T1.3.3.1.1.1.m1.6.6.2.3" xref="S3.T1.3.3.1.1.1.m1.6.6.3a.cmml">,</mo><mrow id="S3.T1.3.3.1.1.1.m1.6.6.2.2" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.cmml"><mi mathsize="70%" id="S3.T1.3.3.1.1.1.m1.6.6.2.2.2" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.2.cmml">ğš”ğšğš›ğš—ğšğš•</mi><mo mathsize="70%" id="S3.T1.3.3.1.1.1.m1.6.6.2.2.1" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.1.cmml">=</mo><mrow id="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.2" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.2.1" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.1.cmml">{</mo><mi mathsize="70%" id="S3.T1.3.3.1.1.1.m1.4.4" xref="S3.T1.3.3.1.1.1.m1.4.4.cmml">ğš›ğš‹ğš</mi><mo maxsize="70%" minsize="70%" id="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.2.2" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.1.1.m1.6b"><apply id="S3.T1.3.3.1.1.1.m1.6.6.3.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2"><csymbol cd="ambiguous" id="S3.T1.3.3.1.1.1.m1.6.6.3a.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2.3">formulae-sequence</csymbol><apply id="S3.T1.3.3.1.1.1.m1.5.5.1.1.cmml" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1"><eq id="S3.T1.3.3.1.1.1.m1.5.5.1.1.1.cmml" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.1"></eq><ci id="S3.T1.3.3.1.1.1.m1.5.5.1.1.2a.cmml" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.2"><mtext mathsize="70%" id="S3.T1.3.3.1.1.1.m1.5.5.1.1.2.cmml" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.2">regularizationÂ factor</mtext></ci><set id="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.1.cmml" xref="S3.T1.3.3.1.1.1.m1.5.5.1.1.3.2"><cn type="float" id="S3.T1.3.3.1.1.1.m1.1.1.cmml" xref="S3.T1.3.3.1.1.1.m1.1.1">0.2</cn><cn type="float" id="S3.T1.3.3.1.1.1.m1.2.2.cmml" xref="S3.T1.3.3.1.1.1.m1.2.2">0.1</cn><cn type="integer" id="S3.T1.3.3.1.1.1.m1.3.3.cmml" xref="S3.T1.3.3.1.1.1.m1.3.3">1</cn></set></apply><apply id="S3.T1.3.3.1.1.1.m1.6.6.2.2.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2"><eq id="S3.T1.3.3.1.1.1.m1.6.6.2.2.1.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.1"></eq><ci id="S3.T1.3.3.1.1.1.m1.6.6.2.2.2.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.2">ğš”ğšğš›ğš—ğšğš•</ci><set id="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.1.cmml" xref="S3.T1.3.3.1.1.1.m1.6.6.2.2.3.2"><ci id="S3.T1.3.3.1.1.1.m1.4.4.cmml" xref="S3.T1.3.3.1.1.1.m1.4.4">ğš›ğš‹ğš</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.1.1.m1.6c">\mathtt{\text{regularization~{}factor}=\{0.2,0.1,1\},kernel=\{rbf\}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.4.4.2.1.1.1" class="ltx_text" style="font-size:70%;">Decision tree</span></span>
</span>
</td>
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.1.1.1" class="ltx_p" style="width:176.4pt;"><span id="S3.T1.4.4.1.1.1.1" class="ltx_text ltx_markedasmath" style="font-size:70%;">split. criteria = {gini, entropy}, max. depth = {50, 100, 150}</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5" class="ltx_tr">
<td id="S3.T1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.5.5.2.1.1.1" class="ltx_text" style="font-size:70%;">Naive Bayes</span></span>
</span>
</td>
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.1.1.1" class="ltx_p" style="width:176.4pt;"><math id="S3.T1.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\mathtt{\text{variance smoothing factor}=1e^{-9}}" display="inline"><semantics id="S3.T1.5.5.1.1.1.m1.1a"><mrow id="S3.T1.5.5.1.1.1.m1.1.1" xref="S3.T1.5.5.1.1.1.m1.1.1.cmml"><mtext mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.2" xref="S3.T1.5.5.1.1.1.m1.1.1.2a.cmml">variance smoothing factor</mtext><mo mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.1" xref="S3.T1.5.5.1.1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.T1.5.5.1.1.1.m1.1.1.3" xref="S3.T1.5.5.1.1.1.m1.1.1.3.cmml"><mn mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.3.2" xref="S3.T1.5.5.1.1.1.m1.1.1.3.2.cmml">ğŸ·</mn><mo lspace="0em" rspace="0em" id="S3.T1.5.5.1.1.1.m1.1.1.3.1" xref="S3.T1.5.5.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><msup id="S3.T1.5.5.1.1.1.m1.1.1.3.3" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.cmml"><mi mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.3.3.2" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.2.cmml">ğš</mi><mrow id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.cmml"><mo mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3a" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.cmml">âˆ’</mo><mn mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.2" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.2.cmml">ğŸ¿</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.1.1.m1.1b"><apply id="S3.T1.5.5.1.1.1.m1.1.1.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1"><eq id="S3.T1.5.5.1.1.1.m1.1.1.1.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.1"></eq><ci id="S3.T1.5.5.1.1.1.m1.1.1.2a.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.2"><mtext mathsize="70%" id="S3.T1.5.5.1.1.1.m1.1.1.2.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.2">variance smoothing factor</mtext></ci><apply id="S3.T1.5.5.1.1.1.m1.1.1.3.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3"><times id="S3.T1.5.5.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.T1.5.5.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.2">1</cn><apply id="S3.T1.5.5.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.5.5.1.1.1.m1.1.1.3.3.1.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3">superscript</csymbol><ci id="S3.T1.5.5.1.1.1.m1.1.1.3.3.2.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.2">ğš</ci><apply id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3"><minus id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.1.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3"></minus><cn type="integer" id="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.2.cmml" xref="S3.T1.5.5.1.1.1.m1.1.1.3.3.3.2">9</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.1.1.m1.1c">\mathtt{\text{variance smoothing factor}=1e^{-9}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<td id="S3.T1.6.6.2" class="ltx_td ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;"></td>
<td id="S3.T1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.1.1.1" class="ltx_p" style="width:176.4pt;"><span id="S3.T1.6.6.1.1.1.1" class="ltx_text ltx_markedasmath" style="font-size:70%;">hidden layers = {(50), (100)}, activation function = {relu}</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.7.7" class="ltx_tr">
<td id="S3.T1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S3.T1.7.7.2.1.1.1" class="ltx_text" style="font-size:70%;">MLP</span></span>
</span>
</td>
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.1.1.1" class="ltx_p" style="width:176.4pt;"><span id="S3.T1.7.7.1.1.1.1" class="ltx_text ltx_markedasmath" style="font-size:70%;">batch size = {32, 64}, learning rate = {constant, invscaling}</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.8.8" class="ltx_tr">
<td id="S3.T1.8.8.2" class="ltx_td ltx_align_top ltx_border_b ltx_border_l ltx_border_r" style="padding-bottom:3.01389pt;padding-top:1.05pt;padding-bottom:1.05pt;"></td>
<td id="S3.T1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" style="padding-bottom:3.01389pt;padding-top:1.05pt;padding-bottom:1.05pt;">
<span id="S3.T1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.8.1.1.1" class="ltx_p" style="width:176.4pt;"><span id="S3.T1.8.8.1.1.1.1" class="ltx_text ltx_markedasmath" style="font-size:70%;">regularization factor = {0.0001}</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Evaluation metrics</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We have used <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">equal error rate</em> (EER) and F1 score metrics for performance evaluation on the test set. This EER is calculated based on a decision threshold where false positive rate and false negative rate are equal. The F1 score is also important to report as the evaluation dataset is imbalanced. We have annotated the bonafide class as positive class. A lower EER and higher F1 score indicate better performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Comparison of backends</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In our initial experiment, we compare all six downstream models on the LA subset of ASVspoof 2019. The results are depicted in Fig.Â <a href="#S4.F3" title="Figure 3 â€£ IV-A Comparison of backends â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where we report performance by separately assessing spoofing detection across all 13 layers (the output of one feature encoder layer plus 12 transformer layers). The figure indicates that logistic regression, SVM, and MLP generally outperform the others. We have achieved average EER of 4.25 % using SVM for downstream task modeling, slightly better than MLP (4.37 %) and logistic regression (4.87 %). The best performance with SVM, as separately shown in TableÂ <a href="#S4.T2" title="TABLE II â€£ IV-C Best configuration â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, is competitive to the state-of-the-art AASIST model which gives EER of 0.83% on the same test conditionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Note that these reports are based on the optimal hyperparameter settings for each model and embedding, with each training instance being independent and not utilizing further fusion of models or ensemble techniques.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.14290/assets/group_by_mode.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Boxplots of EERs (left) and F1 scores (right) for the six methods. Each boxplot summarizes the EERs computed across 13 different layers.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Effect of choice of layers</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the next experiment, we compare the spoofing detection performance using embeddings from each layer separately. In Fig.Â <a href="#S4.F4" title="Figure 4 â€£ IV-B Effect of choice of layers â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare the performance by showing a box-plot of results from all the six classifiers. The results indicate that the second transformer layer gives lowest EER and highest F1 score on average. FiguresÂ <a href="#S4.F3" title="Figure 3 â€£ IV-A Comparison of backends â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> andÂ <a href="#S4.F4" title="Figure 4 â€£ IV-B Effect of choice of layers â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> suggest that embedding at the third layer, combined with either logistic regression, SVM, or MLP, yields better results. This finding contradicts the common practice where the last layer is usually used to extract the speech embeddings from wav2vec 2.0 model for further processing with deep model such as AASISTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.14290/assets/group_by_layer.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Boxplots of EERs and F1 scores for each layer. Each boxplot summarizes the EERs computed across six downsteam classifiers.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Best configuration</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">Based on the observations made so far, we select the the second transformer layer as the front-end, along with an SVM-backend, considering this as the best system. TableÂ <a href="#S4.T2" title="TABLE II â€£ IV-C Best configuration â€£ IV Results â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> compares the proposed methods (i.e., wav2vec 2.0 with MLP, Log. reg., and SVM) with recent state-of-the-art techniques. Our proposed best system (wav2vec 2.0) achieves a <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.90\%" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">0.90</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">0.90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.90\%</annotation></semantics></math> EER and F1 score of <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="0.95" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">0.95</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="float" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">0.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">0.95</annotation></semantics></math> with fewer than 1K trainable model parameters. Our method has additional parameters from the pre-trained SSL model, but these are not trainable.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Summary of the different audio deepfake detection methods. #params indicates the no of trainable parameters only which are associated with energy consumption during training.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.1.1.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">System</span></span>
</span>
</th>
<td id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">#params</span></td>
<td id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">GPU</span></td>
<td id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Front-end</span></td>
<td id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:1.50694pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EER</span></td>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<th id="S4.T2.3.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.2.2.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">RawGAT-ST </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.2.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S4.T2.3.2.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.2.2.2.1" class="ltx_text" style="font-size:70%;">437K</span></td>
<td id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.2.2.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.2.2.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.2.2.5.1" class="ltx_text" style="font-size:70%;">1.06</span></td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.3.3.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.3.3.1.1.1.1" class="ltx_text" style="font-size:70%;">SENet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.3.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.T2.3.3.3.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.3.3.2.1" class="ltx_text" style="font-size:70%;">1,100K</span></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.3.3.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.3.3.4.1" class="ltx_text" style="font-size:70%;">FFT</span></td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.3.3.5.1" class="ltx_text" style="font-size:70%;">1.14</span></td>
</tr>
<tr id="S4.T2.3.4.4" class="ltx_tr">
<th id="S4.T2.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.4.4.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.4.4.1.1.1.1" class="ltx_text" style="font-size:70%;">Res-TSSDNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.4.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S4.T2.3.4.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.4.4.2.1" class="ltx_text" style="font-size:70%;">350K</span></td>
<td id="S4.T2.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.4.4.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.4.4.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.4.4.5.1" class="ltx_text" style="font-size:70%;">1.64</span></td>
</tr>
<tr id="S4.T2.3.5.5" class="ltx_tr">
<th id="S4.T2.3.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.5.5.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.5.5.1.1.1.1" class="ltx_text" style="font-size:70%;">Raw PC-DARTS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.5.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S4.T2.3.5.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.5.5.2.1" class="ltx_text" style="font-size:70%;">24,480K</span></td>
<td id="S4.T2.3.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.5.5.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.5.5.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.5.5.5.1" class="ltx_text" style="font-size:70%;">1.77</span></td>
</tr>
<tr id="S4.T2.3.6.6" class="ltx_tr">
<th id="S4.T2.3.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.6.6.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.6.6.1.1.1.1" class="ltx_text" style="font-size:70%;">LCNN-LSTM-sum </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.6.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T2.3.6.6.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.6.6.2.1" class="ltx_text" style="font-size:70%;">276K</span></td>
<td id="S4.T2.3.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.6.6.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.6.6.4.1" class="ltx_text" style="font-size:70%;">LFCC</span></td>
<td id="S4.T2.3.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.6.6.5.1" class="ltx_text" style="font-size:70%;">1.92</span></td>
</tr>
<tr id="S4.T2.3.7.7" class="ltx_tr">
<th id="S4.T2.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.7.7.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.7.7.1.1.1.1" class="ltx_text" style="font-size:70%;">AASIST-L </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.7.7.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.3.7.7.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.7.7.2.1" class="ltx_text" style="font-size:70%;">85K</span></td>
<td id="S4.T2.3.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.7.7.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.7.7.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.7.7.5.1" class="ltx_text" style="font-size:70%;">0.99</span></td>
</tr>
<tr id="S4.T2.3.8.8" class="ltx_tr">
<th id="S4.T2.3.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.8.8.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.8.8.1.1.1.1" class="ltx_text" style="font-size:70%;">AASIST </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.8.8.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.3.8.8.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.8.8.2.1" class="ltx_text" style="font-size:70%;">297K</span></td>
<td id="S4.T2.3.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.8.8.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.8.8.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.8.8.5.1" class="ltx_text" style="font-size:70%;">0.83</span></td>
</tr>
<tr id="S4.T2.3.9.9" class="ltx_tr">
<th id="S4.T2.3.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.9.9.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.9.9.1.1.1.1" class="ltx_text" style="font-size:70%;">wav2vec 2.0-light-DARTS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.9.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S4.T2.3.9.9.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</th>
<td id="S4.T2.3.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.9.9.2.1" class="ltx_text" style="font-size:70%;">NA</span></td>
<td id="S4.T2.3.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.9.9.3.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
<td id="S4.T2.3.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.9.9.4.1" class="ltx_text" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.9.9.5.1" class="ltx_text" style="font-size:70%;">1.09</span></td>
</tr>
<tr id="S4.T2.3.10.10" class="ltx_tr">
<th id="S4.T2.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.10.10.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.10.10.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Wave2Vec 2.0 - MLPÂ (ours)</span></span>
</span>
</th>
<td id="S4.T2.3.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.10.10.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">77K</span></td>
<td id="S4.T2.3.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.10.10.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">No</span></td>
<td id="S4.T2.3.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.10.10.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.10.10.5.1" class="ltx_text" style="font-size:70%;">1.11</span></td>
</tr>
<tr id="S4.T2.3.11.11" class="ltx_tr">
<th id="S4.T2.3.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.11.11.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.11.11.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Wave2Vec 2.0 - LogÂ (ours)</span></span>
</span>
</th>
<td id="S4.T2.3.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.11.11.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">769</span></td>
<td id="S4.T2.3.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.11.11.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">No</span></td>
<td id="S4.T2.3.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.11.11.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.11.11.5.1" class="ltx_text" style="font-size:70%;">1.25</span></td>
</tr>
<tr id="S4.T2.3.12.12" class="ltx_tr">
<th id="S4.T2.3.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;">
<span id="S4.T2.3.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.3.12.12.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S4.T2.3.12.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Wave2Vec 2.0 - SVMÂ (ours)</span></span>
</span>
</th>
<td id="S4.T2.3.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.12.12.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">808</span></td>
<td id="S4.T2.3.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.12.12.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">No</span></td>
<td id="S4.T2.3.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.12.12.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Raw audio</span></td>
<td id="S4.T2.3.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-bottom:0.03012pt;padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S4.T2.3.12.12.5.1" class="ltx_text" style="font-size:70%;">0.90</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Towards Green AI solution</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">We achieve optimal spoofing detection performance by utilizing embeddings from the output of the second transformer layer. Consequently, we confine our use of model parameters to this layer and those preceding it. This drastically reduces computation (MAC count by <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\approx\text{12 GMAC }(52\%)" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml"></mi><mo id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">â‰ˆ</mo><mrow id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml"><mtext id="S4.SS4.p1.1.m1.1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.1.3a.cmml">12 GMACÂ </mtext><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS4.p1.1.m1.1.1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p1.1.m1.1.1.1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.1.m1.1.1.1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.1.1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.2.cmml">52</mn><mo id="S4.SS4.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.SS4.p1.1.m1.1.1.1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><approx id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2"></approx><csymbol cd="latexml" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">absent</csymbol><apply id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.1.2"></times><ci id="S4.SS4.p1.1.m1.1.1.1.3a.cmml" xref="S4.SS4.p1.1.m1.1.1.1.3"><mtext id="S4.SS4.p1.1.m1.1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.1.3">12 GMACÂ </mtext></ci><apply id="S4.SS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS4.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.1.1.1.1.2">52</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\approx\text{12 GMAC }(52\%)</annotation></semantics></math> on average size training data. Moreover, we need to store and utilize <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="\approx 19\text{M}(80\%\text{ less})" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml"></mi><mo id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">â‰ˆ</mo><mrow id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1.1.3" xref="S4.SS4.p1.2.m2.1.1.1.3.cmml">19</mn><mo lspace="0em" rspace="0em" id="S4.SS4.p1.2.m2.1.1.1.2" xref="S4.SS4.p1.2.m2.1.1.1.2.cmml">â€‹</mo><mtext id="S4.SS4.p1.2.m2.1.1.1.4" xref="S4.SS4.p1.2.m2.1.1.1.4a.cmml">M</mtext><mo lspace="0em" rspace="0em" id="S4.SS4.p1.2.m2.1.1.1.2a" xref="S4.SS4.p1.2.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS4.p1.2.m2.1.1.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p1.2.m2.1.1.1.1.1.2" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.2.m2.1.1.1.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.cmml"><mrow id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.cmml"><mn id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.2" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.2.cmml">80</mn><mo id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.1" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S4.SS4.p1.2.m2.1.1.1.1.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mtext id="S4.SS4.p1.2.m2.1.1.1.1.1.1.3" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.3a.cmml">Â less</mtext></mrow><mo stretchy="false" id="S4.SS4.p1.2.m2.1.1.1.1.1.3" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><approx id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2"></approx><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">absent</csymbol><apply id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1"><times id="S4.SS4.p1.2.m2.1.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.1.2"></times><cn type="integer" id="S4.SS4.p1.2.m2.1.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.1.3">19</cn><ci id="S4.SS4.p1.2.m2.1.1.1.4a.cmml" xref="S4.SS4.p1.2.m2.1.1.1.4"><mtext id="S4.SS4.p1.2.m2.1.1.1.4.cmml" xref="S4.SS4.p1.2.m2.1.1.1.4">M</mtext></ci><apply id="S4.SS4.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1"><times id="S4.SS4.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.1"></times><apply id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.1">percent</csymbol><cn type="integer" id="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.2.2">80</cn></apply><ci id="S4.SS4.p1.2.m2.1.1.1.1.1.1.3a.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.3"><mtext id="S4.SS4.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.1.1.1.1.3">Â less</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\approx 19\text{M}(80\%\text{ less})</annotation></semantics></math> parameters instead of <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="\approx 95M" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml"></mi><mo id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml"><mn id="S4.SS4.p1.3.m3.1.1.3.2" xref="S4.SS4.p1.3.m3.1.1.3.2.cmml">95</mn><mo lspace="0em" rspace="0em" id="S4.SS4.p1.3.m3.1.1.3.1" xref="S4.SS4.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS4.p1.3.m3.1.1.3.3" xref="S4.SS4.p1.3.m3.1.1.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><approx id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></approx><csymbol cd="latexml" id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">absent</csymbol><apply id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3"><times id="S4.SS4.p1.3.m3.1.1.3.1.cmml" xref="S4.SS4.p1.3.m3.1.1.3.1"></times><cn type="integer" id="S4.SS4.p1.3.m3.1.1.3.2.cmml" xref="S4.SS4.p1.3.m3.1.1.3.2">95</cn><ci id="S4.SS4.p1.3.m3.1.1.3.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3.3">ğ‘€</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">\approx 95M</annotation></semantics></math> in the original wav2vec 2.0 <span id="S4.SS4.p1.4.1" class="ltx_text ltx_font_typewriter">BASE</span> model. This significantly reduces overall training cost (i.e., <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mi id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><ci id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">E</annotation></semantics></math> in Eq. <a href="#S1.E1" title="In I Introduction â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) by minimizing cost at downstream feature extraction step (from algorithm <a href="#alg1" title="In II Overview of the Proposed Methodology â€£ Exploring Green AI for Audio Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We conduct all experiments using both Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz instead of any GPU-powered computer. Some studies have reported even lower EER; however, they eventually fall under the category of Red AI, as achieving better performance requires larger modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, model fine-tuning and/or joint trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and fusionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which also necessitate GPUs.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we advocate for the optimal use of pre-trained speech foundation models for spoofing detection within a CPU-only architecture, aligning with environmentally-friendly as well as cost-effective AI initiatives. Our initial findings with the wav2vec 2.0 model and the ASVspoof 2019 LA dataset demonstrate the potential of classical ML algorithms. By considering speech embedding from an earlier intermediate layer, we have achieved the best spoofing detection results with an SVM classifier using an RBF kernel, which are competitive with the performance obtained using the state-of-the-art graph neural network-based AASIST model. The key advantage of the proposed framework is that developers can utilize publicly available models without the need for GPU resources for training or fine-tuning. The current study can be extended by exploring other large publicly available speech foundation models as extensively evaluated in SUPERB bench-marking for other tasks. Exploring the generalization ability of the proposed approach on different datasets could be another interesting future direction.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y.Â LeCun, Y.Â Bengio, and G.Â Hinton, â€œDeep learning,â€ <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Nature</span>, vol.Â 521, no.Â 7553, pp.Â 436â€“444, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S.Â J. Prince, <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Understanding Deep Learning</span>.

</span>
<span class="ltx_bibblock">MIT Press, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E.Â Strubell, A.Â Ganesh, and A.Â Mccallum, â€œEnergy and policy considerations for deep learning in NLP,â€ in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. ACL</span>, pp.Â 3645â€“3650, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R.Â Schwartz, J.Â Dodge, N.Â A. Smith, and O.Â Etzioni, â€œGreen AI,â€ <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, vol.Â 63, no.Â 12, pp.Â 54â€“63, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
E.Â Strubell, A.Â Ganesh, and A.Â McCallum, â€œEnergy and policy considerations for modern deep learning research,â€ <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol.Â 34, pp.Â 13693â€“13696, Apr. 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R.Â Verdecchia, J.Â Sallou, and L.Â Cruz, â€œA systematic review of Green AI,â€ <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</span>, vol.Â 13, no.Â 4, p.Â e1507, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M.Â R. Kamble, H.Â B. Sailor, H.Â A. Patil, and H.Â Li, â€œAdvances in anti-spoofing: from the perspective of ASVspoof challenges,â€ <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">APSIPA Transactions on Signal and Information Processing</span>, vol.Â 9, p.Â e2, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X.Â Liu <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œASVspoof 2021: Towards spoofed and deepfake speech detection in the wild,â€ <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X.Â Wang and J.Â Yamagishi, â€œA comparative study on recent neural spoofing countermeasures for synthetic speech detection,â€ in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, pp.Â 4259â€“4263, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.-w. Jung <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œAASIST: Audio anti-spoofing using integrated spectro-temporal graph attention networks,â€ in <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H.Â Tak <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œAutomatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation,â€ in <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Proc. ODYSSEY</span>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S.Â Sinha, S.Â Dey, and G.Â Saha, â€œImproving self-supervised learning model for audio spoofing detection with layer-conditioned embedding fusion,â€ <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Computer Speech &amp; Language</span>, vol.Â 86, p.Â 101599, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T.Â Kang <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œExperimental study: Enhancing voice spoofing detection models with wav2vec 2.0,â€ <span id="bib.bib13.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.17127</span>, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S.Â BorzÃ¬ <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œIs synthetic voice detection research going into the right direction?,â€ in <span id="bib.bib14.2.2" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.Â wen Yang <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œSUPERB: Speech processing universal performance benchmark,â€ in <span id="bib.bib15.2.2" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Mohamed <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œSelf-supervised speech representation learning: A review,â€ <span id="bib.bib16.2.2" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, vol.Â 16, no.Â 6, pp.Â 1179â€“1210, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Baevski <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œwav2vec 2.0: A framework for self-supervised learning of speech representations,â€ in <span id="bib.bib17.2.2" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R.Â O. Duda, P.Â E. Hart, and D.Â G. Stork, <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Pattern Classification</span>.

</span>
<span class="ltx_bibblock">John Wiley &amp; Sons, 2006.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G.Â James, D.Â Witten, T.Â Hastie, and R.Â Tibshirani, <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Introduction to Statistical Learning: with Applications in R</span>.

</span>
<span class="ltx_bibblock">Springer, 2013.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C.Â J. Burges, â€œA tutorial on support vector machines for pattern recognition,â€ <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Data Mining and Knowledge Discovery</span>, vol.Â 2, no.Â 2, pp.Â 121â€“167, 1998.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G.Â I. Webb, <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">NaÃ¯ve Bayes</span>, pp.Â 713â€“714.

</span>
<span class="ltx_bibblock">Boston, MA: Springer US, 2010.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â R. Quinlan, â€œInduction of decision trees,â€ <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Machine learning</span>, vol.Â 1, pp.Â 81â€“106, 1986.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.Â Haykin, <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Neural Networks and Learning Machines</span>.

</span>
<span class="ltx_bibblock">Pearson Education, 3rdÂ ed., 2009.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
F.Â Pedregosa <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œScikit-learn: Machine learning in Python,â€ <span id="bib.bib24.2.2" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, vol.Â 12, pp.Â 2825â€“2830, 2011.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
X.Â Wang <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech,â€ <span id="bib.bib25.2.2" class="ltx_text ltx_font_italic">Computer Speech &amp; Language</span>, vol.Â 64, p.Â 101114, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.Â Yamagishi <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œASVspoof 2019: Automatic speaker verification spoofing and countermeasures challenge evaluation plan.â€ <a target="_blank" href="https://www.asvspoof.org/asvspoof2019/asvspoof2019_evaluation_plan.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.asvspoof.org/asvspoof2019/asvspoof2019_evaluation_plan.pdf</a>, 2019.

</span>
<span class="ltx_bibblock">Online; accessed 29 January 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
V.Â Panayotov <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œLibriSpeech: an ASR corpus based on public domain audio books,â€ in <span id="bib.bib27.2.2" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H.Â Tak <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œEnd-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection,â€ in <span id="bib.bib28.2.2" class="ltx_text ltx_font_italic">Proc. ASVspoof</span>, pp.Â 1â€“8, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y.Â Zhang <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œThe effect of silence and dual-band fusion in anti-spoofing system,â€ in <span id="bib.bib29.2.2" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G.Â Hua, A.Â B.Â J. Teoh, and H.Â Zhang, â€œTowards end-to-end synthetic speech detection,â€ <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Letters</span>, vol.Â 28, pp.Â 1265â€“1269, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
W.Â Ge <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œRaw differentiable architecture search for speech deepfake and spoofing detection,â€ in <span id="bib.bib31.2.2" class="ltx_text ltx_font_italic">Proc. ASVspoof</span>, pp.Â 22â€“28, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X.Â Wang and J.Â Yamagishi, â€œA comparative study on recent neural spoofing countermeasures for synthetic speech detection,â€ in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C.Â Wang <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œFully automated end-to-end fake audio detection,â€ in <span id="bib.bib33.2.2" class="ltx_text ltx_font_italic">Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia</span>, p.Â 27â€“33, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
X.Â Wang and J.Â Yamagishi, â€œInvestigating self-supervised front ends for speech spoofing countermeasures,â€ in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proc. ODYSSEY</span>, pp.Â 100â€“106, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.14289" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.14290" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.14290">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.14290" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.14291" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:44:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
