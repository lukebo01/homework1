<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.21174] AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning</title><meta property="og:description" content="Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversariâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.21174">

<!--Generated on Mon Aug  5 18:26:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Adversarial attack,  Adversarial training,  Vision Transformer,  GPT,  Multimodal machine learning">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maisha Binte Rashid
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Baylor University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">One Bear Place</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Waco</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">Texas</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:maisha_rashid1@baylor.edu">maisha_rashid1@baylor.edu</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo Rivas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Baylor University</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_streetaddress">One Bear Place</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_city">Waco</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_country">Texas</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pablo_rivas@baylor.edu">pablo_rivas@baylor.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Method (FGSM) to generate adversarial examples and incorporating adversarial training techniques, we demonstrate improved model robustness on two benchmark datasets: Flickr8k and COCO. Our findings indicate that selectively training only the text decoder of the multimodal architecture shows performance comparable to full adversarial training while offering increased computational efficiency. This targeted approach suggests a balance between robustness and training costs, facilitating the ethical deployment of multimodal AI systems across various domains.</p>
</div>
<div class="ltx_keywords">Adversarial attack, Adversarial training, Vision Transformer, GPT, Multimodal machine learning
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>ACM KDD 2024; August 24â€“29, 2024; Barcelona, Spain</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Image representations</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Language resources</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Security and privacyÂ Adversarial learning</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Neural networks</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Cross-validation</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multimodal machine learning is an advanced area of artificial intelligence that integrates and processes multiple types of data inputs, such as text, images, and audio, to perform tasks that mimic human cognitive abilitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>. This integration allows the model to leverage the strengths of each modality, leading to richer interpretations and more accurate predictions that could be achieved by models processing the data in isolation. One of the most compelling applications of multimodal learning involves the combination of visual and textual data, commonly referred to as image-text pairs. This pairing is particularly significant as it mirrors the way humans often receive and interpret information, making the study of these models not only interesting but also aligned with natural human communication patterns.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The increasing deployment of multimodal models in critical applications also raises significant safety and security concerns. These models, like all machine learning systems, are susceptible to adversarial attacks which means intentional inputs designed to confuse the model and provoke incorrect outputsÂ <cite class="ltx_cite ltx_citemacro_citep">(Ozdag, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Evtimov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>. The robustness of multimodal models, therefore, becomes a critical area of focus. Ensuring these models can withstand adversarial attacks is not just a technical necessity but a safety imperative, particularly when these models are employed in sensitive contexts such as autonomous driving, healthcare, and content moderation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Making multimodal machine learning models more robust against adversarial attacks is a key challengeÂ <cite class="ltx_cite ltx_citemacro_citep">(Schlarmann and Hein, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. These models, which combine different types of data like text and images, can be especially vulnerable because of the way these data types interact. Traditional methods that focus on making each type of data robust on its own can be very demanding and might not address all the issues.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we look at a simpler and more efficient approach. Instead of trying to make the entire model robust, we focus on improving just one part of it. We apply adversarial training techniques specifically to the text decoder part of our image captioning model. Our experiments with the Flickr8k and COCO datasets show that this method works well. Training only the text decoder, while keeping the image encoder fixed, gives us results almost as good as training the whole model. However, when we fix the text decoder and train only the image encoder, the performance drops significantly. This shows that the text decoder plays a crucial role in making the model robust against attacks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper helps improve ethical AI by showing a focused way to make multimodal models stronger. This method not only improves the technical side of AI but also helps build public trust in AI systems used in different areas.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper is organized as follows: Section 2 reviews related work on adversarial attacks and defenses in multimodal machine learning. Section 3 describes our model architecture and the adversarial training method. Section 4 presents the experimental setup and results on the Flickr8k and COCO datasets. Section 5 discusses what our findings mean for developing robust and ethical AI systems. Finally, Section 6 concludes the paper and suggests future research directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">InÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>, the authors investigated the vulnerability of multimodal deep learning models to adversarial attacks, finding that even if only one input modality is attacked, the overall performance degrades significantly. The authors highlight the need to explore ways to obtain robust features from multimodal data to achieve useful information from each modality and improve adversarial defense mechanism. To develop adversarial defense mechanisms we explored different adversarial attacks in machine learning models.
One of the first and best-known adversarial attack techniques is the Fast Gradient Sign Method (FGSM), which involves applying perturbations to the input data in the direction of the gradient of the loss function with respect to the input data to produce adversarial examplesÂ <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite>. Another adversarial attack similar to FGSM, which creates the attack iteratively, is Projected Gradient Descent (PGD)Â <cite class="ltx_cite ltx_citemacro_citep">(Madry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. PGD generates adversarial examples by iteratively taking small steps toward the direction of maximum loss function and projecting the outcome back onto the allowed data range at each step. Other well-known adversarial attack strategies are Jacobian Saliency Map Adversary (JSMA) and C&amp;W Attack.
JSMA is an iterative adversarial attack technique that computes a saliency map based on the Jacobian matrix to identify the most significant pixels to perturb in order to mislead neural network classifiersÂ <cite class="ltx_cite ltx_citemacro_citep">(Papernot etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>. The C&amp;W attack, proposed by Carlini and Wagner, is an powerful and efficient optimization-based adversarial attack that finds quasi-imperceptible perturbations to input examples that reliably cause misclassification in neural networksÂ <cite class="ltx_cite ltx_citemacro_citep">(Carlini and Wagner, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The authors inÂ <cite class="ltx_cite ltx_citemacro_citep">(Noever and Noever, <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> discussed about adversarial attack in multimodal machine learning model like CLIP in their paper. They presented new types of adversarial attacks against the multi-modal neural network model CLIP, which integrates the detection of visual objects with text reading. Basic typographical manipulations like misspellings and font changes are included in the attacks, along with conceptual ones that use contradicting text and image inputs to trick the model. Adversarial training is one of the most common defense mechanism for adversarial attack in machine learning models. Adversarial training is using adversarial examples as samples in the training phase of the model. The authors inÂ <cite class="ltx_cite ltx_citemacro_citep">(Shafahi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> proposed a fast adversarial training algorithm that produces robust models at a low computational cost by recycling the gradient information computed during training. The method is able to train robust image classifiers for the ImageNet dataset in a short time on a modest hardware setup.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Model Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This work uses an architecture for the image captioning task that combines GPT-2 and Vision Transformer (ViT) modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, with the GPT-2Â <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite> acting as a decoder and the ViT as an encoder. The ViT model divides an input image of 224 x 224 pixels into 16 x 16 pixel patches. Then, these patches are flattened and projected linearly into a space with dimensions of 768. The transformer encoder receives the image patch sequence and a [CLS] token. The encoder consists of multiple layers of position-wise feedforward and multi-headed self-attention networks. The <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">[CLS]</span> token, which compiles data from all patches, represents the whole image. This tokenâ€™s final hidden state at the ViT encoderâ€™s output captures the contextualized global aspects of the image, which are essential for generating a caption.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The decoder obtains the encoded image features from the ViTâ€™s [CLS] token, which uses them as the starting point for caption generation. Through many layers of masked multi-headed self-attention, the GPT-2 model processes this input, allowing each point in the output sequence to attend only to earlier positions in the sequence. This arrangement preserves the autoregressive characteristic by ensuring that each word in the caption is generated solely based on the words that came before it. The GPT-2 model produces a sequence of tokens that comprise the imageâ€™s caption. Every token is created one after the other, and the model predicts the subsequent token by analyzing the preceding tokens and the contextual data extracted from the image.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Adversarial Attack</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In our work, we used the adversarial example technique to create adversarial attacks. Specifically, we employed the Fast Gradient Sign Method (FGSM) to generate these examples. FGSM is a method for crafting adversarial examples, which are inputs to machine learning models that have been intentionally modified to cause the model to make incorrect predictionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.7" class="ltx_p">Given an input <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathbf{x}</annotation></semantics></math> and its true label <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">y</annotation></semantics></math>, we define the loss function <math id="S3.SS2.p2.3.m3.3" class="ltx_Math" alttext="J(\theta,\mathbf{x},y)" display="inline"><semantics id="S3.SS2.p2.3.m3.3a"><mrow id="S3.SS2.p2.3.m3.3.4" xref="S3.SS2.p2.3.m3.3.4.cmml"><mi id="S3.SS2.p2.3.m3.3.4.2" xref="S3.SS2.p2.3.m3.3.4.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.3.4.1" xref="S3.SS2.p2.3.m3.3.4.1.cmml">â€‹</mo><mrow id="S3.SS2.p2.3.m3.3.4.3.2" xref="S3.SS2.p2.3.m3.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.3.4.3.2.1" xref="S3.SS2.p2.3.m3.3.4.3.1.cmml">(</mo><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">Î¸</mi><mo id="S3.SS2.p2.3.m3.3.4.3.2.2" xref="S3.SS2.p2.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p2.3.m3.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml">ğ±</mi><mo id="S3.SS2.p2.3.m3.3.4.3.2.3" xref="S3.SS2.p2.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p2.3.m3.3.3" xref="S3.SS2.p2.3.m3.3.3.cmml">y</mi><mo stretchy="false" id="S3.SS2.p2.3.m3.3.4.3.2.4" xref="S3.SS2.p2.3.m3.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.3b"><apply id="S3.SS2.p2.3.m3.3.4.cmml" xref="S3.SS2.p2.3.m3.3.4"><times id="S3.SS2.p2.3.m3.3.4.1.cmml" xref="S3.SS2.p2.3.m3.3.4.1"></times><ci id="S3.SS2.p2.3.m3.3.4.2.cmml" xref="S3.SS2.p2.3.m3.3.4.2">ğ½</ci><vector id="S3.SS2.p2.3.m3.3.4.3.1.cmml" xref="S3.SS2.p2.3.m3.3.4.3.2"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğœƒ</ci><ci id="S3.SS2.p2.3.m3.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2">ğ±</ci><ci id="S3.SS2.p2.3.m3.3.3.cmml" xref="S3.SS2.p2.3.m3.3.3">ğ‘¦</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.3c">J(\theta,\mathbf{x},y)</annotation></semantics></math>, where <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\theta</annotation></semantics></math> represents the parameters of the model. The goal is to find a small perturbation <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{\eta}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathbf{\eta}</annotation></semantics></math> such that the perturbed input <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{\eta}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><msup id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">ğ±</mi><mo id="S3.SS2.p2.6.m6.1.1.2.3" xref="S3.SS2.p2.6.m6.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">ğ±</mi><mo id="S3.SS2.p2.6.m6.1.1.3.1" xref="S3.SS2.p2.6.m6.1.1.3.1.cmml">+</mo><mi id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml">Î·</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><eq id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></eq><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">ğ±</ci><ci id="S3.SS2.p2.6.m6.1.1.2.3.cmml" xref="S3.SS2.p2.6.m6.1.1.2.3">â€²</ci></apply><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><plus id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3.1"></plus><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">ğ±</ci><ci id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3">ğœ‚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{\eta}</annotation></semantics></math> leads to a misclassification. FGSM achieves this by using the gradient of the loss function with respect to the input <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbf{x}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">First, we compute the gradient of the loss function:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\mathbf{g}=\nabla_{\mathbf{x}}J(\theta,\mathbf{x},y)." display="block"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mi id="S3.Ex1.m1.4.4.1.1.2" xref="S3.Ex1.m1.4.4.1.1.2.cmml">ğ </mi><mo id="S3.Ex1.m1.4.4.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.4.4.1.1.3" xref="S3.Ex1.m1.4.4.1.1.3.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.cmml"><msub id="S3.Ex1.m1.4.4.1.1.3.2.1" xref="S3.Ex1.m1.4.4.1.1.3.2.1.cmml"><mo rspace="0.167em" id="S3.Ex1.m1.4.4.1.1.3.2.1.2" xref="S3.Ex1.m1.4.4.1.1.3.2.1.2.cmml">âˆ‡</mo><mi id="S3.Ex1.m1.4.4.1.1.3.2.1.3" xref="S3.Ex1.m1.4.4.1.1.3.2.1.3.cmml">ğ±</mi></msub><mi id="S3.Ex1.m1.4.4.1.1.3.2.2" xref="S3.Ex1.m1.4.4.1.1.3.2.2.cmml">J</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.3.1" xref="S3.Ex1.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S3.Ex1.m1.4.4.1.1.3.3.2" xref="S3.Ex1.m1.4.4.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.3.3.2.1" xref="S3.Ex1.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">Î¸</mi><mo id="S3.Ex1.m1.4.4.1.1.3.3.2.2" xref="S3.Ex1.m1.4.4.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">ğ±</mi><mo id="S3.Ex1.m1.4.4.1.1.3.3.2.3" xref="S3.Ex1.m1.4.4.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">y</mi><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.3.3.2.4" xref="S3.Ex1.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex1.m1.4.4.1.2" xref="S3.Ex1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.1.1.cmml" xref="S3.Ex1.m1.4.4.1"><eq id="S3.Ex1.m1.4.4.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1"></eq><ci id="S3.Ex1.m1.4.4.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.2">ğ </ci><apply id="S3.Ex1.m1.4.4.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3"><times id="S3.Ex1.m1.4.4.1.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.1"></times><apply id="S3.Ex1.m1.4.4.1.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2"><apply id="S3.Ex1.m1.4.4.1.1.3.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.3.2.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.1">subscript</csymbol><ci id="S3.Ex1.m1.4.4.1.1.3.2.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.1.2">âˆ‡</ci><ci id="S3.Ex1.m1.4.4.1.1.3.2.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.1.3">ğ±</ci></apply><ci id="S3.Ex1.m1.4.4.1.1.3.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2">ğ½</ci></apply><vector id="S3.Ex1.m1.4.4.1.1.3.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.2"><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">ğœƒ</ci><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">ğ±</ci><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">ğ‘¦</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\mathbf{g}=\nabla_{\mathbf{x}}J(\theta,\mathbf{x},y).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Then, the adversarial example <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{x}^{\prime}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msup id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">ğ±</mi><mo id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ±</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathbf{x}^{\prime}</annotation></semantics></math> is generated by adding a perturbation in the direction of the sign of this gradient:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="\mathbf{x}^{\prime}=\mathbf{x}+\epsilon\cdot\text{sign}(\mathbf{g})." display="block"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mrow id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><msup id="S3.Ex2.m1.2.2.1.1.2" xref="S3.Ex2.m1.2.2.1.1.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.cmml">ğ±</mi><mo id="S3.Ex2.m1.2.2.1.1.2.3" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.Ex2.m1.2.2.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.1.1.3" xref="S3.Ex2.m1.2.2.1.1.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.3.2" xref="S3.Ex2.m1.2.2.1.1.3.2.cmml">ğ±</mi><mo id="S3.Ex2.m1.2.2.1.1.3.1" xref="S3.Ex2.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S3.Ex2.m1.2.2.1.1.3.3" xref="S3.Ex2.m1.2.2.1.1.3.3.cmml"><mrow id="S3.Ex2.m1.2.2.1.1.3.3.2" xref="S3.Ex2.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.3.3.2.2" xref="S3.Ex2.m1.2.2.1.1.3.3.2.2.cmml">Ïµ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex2.m1.2.2.1.1.3.3.2.1" xref="S3.Ex2.m1.2.2.1.1.3.3.2.1.cmml">â‹…</mo><mtext id="S3.Ex2.m1.2.2.1.1.3.3.2.3" xref="S3.Ex2.m1.2.2.1.1.3.3.2.3a.cmml">sign</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.2.2.1.1.3.3.1" xref="S3.Ex2.m1.2.2.1.1.3.3.1.cmml">â€‹</mo><mrow id="S3.Ex2.m1.2.2.1.1.3.3.3.2" xref="S3.Ex2.m1.2.2.1.1.3.3.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.3.3.3.2.1" xref="S3.Ex2.m1.2.2.1.1.3.3.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">ğ </mi><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.3.3.3.2.2" xref="S3.Ex2.m1.2.2.1.1.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex2.m1.2.2.1.2" xref="S3.Ex2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1"><eq id="S3.Ex2.m1.2.2.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1"></eq><apply id="S3.Ex2.m1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2">superscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2">ğ±</ci><ci id="S3.Ex2.m1.2.2.1.1.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.3">â€²</ci></apply><apply id="S3.Ex2.m1.2.2.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3"><plus id="S3.Ex2.m1.2.2.1.1.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.3.1"></plus><ci id="S3.Ex2.m1.2.2.1.1.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.3.2">ğ±</ci><apply id="S3.Ex2.m1.2.2.1.1.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3"><times id="S3.Ex2.m1.2.2.1.1.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.1"></times><apply id="S3.Ex2.m1.2.2.1.1.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.2"><ci id="S3.Ex2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.2.1">â‹…</ci><ci id="S3.Ex2.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.2.2">italic-Ïµ</ci><ci id="S3.Ex2.m1.2.2.1.1.3.3.2.3a.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.2.3"><mtext id="S3.Ex2.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.2.3">sign</mtext></ci></apply><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">ğ </ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\mathbf{x}^{\prime}=\mathbf{x}+\epsilon\cdot\text{sign}(\mathbf{g}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.3" class="ltx_p">Here, <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\epsilon</annotation></semantics></math> is a small constant that controls the magnitude of the perturbation. The sign function, <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\text{sign}(\mathbf{g})" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.2" xref="S3.SS2.p5.2.m2.1.2.cmml"><mtext id="S3.SS2.p5.2.m2.1.2.2" xref="S3.SS2.p5.2.m2.1.2.2a.cmml">sign</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.2.1" xref="S3.SS2.p5.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p5.2.m2.1.2.3.2" xref="S3.SS2.p5.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p5.2.m2.1.2.3.2.1" xref="S3.SS2.p5.2.m2.1.2.cmml">(</mo><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">ğ </mi><mo stretchy="false" id="S3.SS2.p5.2.m2.1.2.3.2.2" xref="S3.SS2.p5.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.2.cmml" xref="S3.SS2.p5.2.m2.1.2"><times id="S3.SS2.p5.2.m2.1.2.1.cmml" xref="S3.SS2.p5.2.m2.1.2.1"></times><ci id="S3.SS2.p5.2.m2.1.2.2a.cmml" xref="S3.SS2.p5.2.m2.1.2.2"><mtext id="S3.SS2.p5.2.m2.1.2.2.cmml" xref="S3.SS2.p5.2.m2.1.2.2">sign</mtext></ci><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">ğ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\text{sign}(\mathbf{g})</annotation></semantics></math>, is applied element-wise to the gradient, ensuring that each element of the perturbation has the same magnitude, <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\epsilon</annotation></semantics></math>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Examples of adversarial perturbations from the Flickr dataset (top three rows) and COCO (bottom two rows).</figcaption>
<table id="S3.T1.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.15.16.1" class="ltx_tr">
<th id="S3.T1.15.16.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.15.16.1.1.1" class="ltx_text ltx_font_bold">Original</span></th>
<th id="S3.T1.15.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.15.16.1.2.1" class="ltx_text ltx_font_bold">Perturbed Example</span></th>
<th id="S3.T1.15.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.15.16.1.3.1" class="ltx_text ltx_font_bold">Difference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/clean_image_new1.png" id="S3.T1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/perturbed_image_new1.png" id="S3.T1.2.2.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/diff_image_new1.png" id="S3.T1.3.3.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/clean_image_new2.png" id="S3.T1.4.4.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/perturbed_image_new2.png" id="S3.T1.5.5.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/diff_image_new2.png" id="S3.T1.6.6.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S3.T1.9.9" class="ltx_tr">
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/clean_image_new3.png" id="S3.T1.7.7.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/perturbed_image_new3.png" id="S3.T1.8.8.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/diff_image_new3.png" id="S3.T1.9.9.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S3.T1.12.12" class="ltx_tr">
<td id="S3.T1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/clean_image_coco1.png" id="S3.T1.10.10.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/perturbed_image_coco1.png" id="S3.T1.11.11.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/diff_image_coco1.png" id="S3.T1.12.12.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S3.T1.15.15" class="ltx_tr">
<td id="S3.T1.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/clean_image_coco.png" id="S3.T1.13.13.1.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/perturbed_image_coco.png" id="S3.T1.14.14.2.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
<td id="S3.T1.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><img src="/html/2407.21174/assets/saved_images/diff_image_coco.png" id="S3.T1.15.15.3.g1" class="ltx_graphics ltx_img_square" width="132" height="132" alt="[Uncaptioned image]"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2. Adversarial Attack â€£ 3. Methodology â€£ AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows examples of the adversarial perturbations generated using FGSM on the Flickr8k and COCO datasets. Despite its simplicity, FGSM is effective in generating adversarial examples that are close to the original inputs but cause the model to make incorrect predictions. This method is computationally efficient and serves as a useful tool for testing and improving the robustness of machine learning models. This table shows how adversarial attacks are imperceptible to human eyes, as shown in the second column. The third column demonstrates the perturbations by showing the difference between the original and perturbed images. In the â€™Differenceâ€™ column, we can see square or patch-like patterns. This happens because of ViTâ€™s architecture, which processes input images by dividing them into patches. FGSM leverages the gradients computed for each patch, generating perturbations that exploit the vulnerabilities within these regions. Table <a href="#S3.T1" title="Table 1 â€£ 3.2. Adversarial Attack â€£ 3. Methodology â€£ AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> underscores the necessity for robust models capable of defying such attacks. By incorporating adversarial examples into the training process, we can enhance the modelâ€™s ability to defend against such attacks and improve its overall robustness.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this study, we leverage two widely-used and benchmark datasets for the image captioning task: Flickr8k and COCO (Common Objects in Context).</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Flickr8k Dataset</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">The Flickr8k dataset comprises 8,000 images, each accompanied by five human-annotated captions. The images are sourced from Flickr, a popular online photo-sharing platform, and are categorized into 20 distinct classes.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>COCO Dataset</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The COCO dataset is a larger and more challenging resource for image captioning, containing 123,287 images with five captions per image. Unlike the Flickr8k dataset, the images in COCO are sourced from various online platforms, including Flickr, and are categorized into 80 classes.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Evaluation Metric</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To evaluate the performance for image captioning task, we used Bilingual Evaluation Understudy (BLEU) scoreÂ <cite class="ltx_cite ltx_citemacro_citep">(Papineni etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2002</a>)</cite>. Comparing the generated captions to the reference captions in the dataset generates a similarity score known as the BLEU score. Higher scores correspond to a closer match. The BLEU score computes the n-gram overlap between the modelâ€™s output and the ground truth. There is a perfect match between the generated and reference captions when the BLEU score is 1, from 0 to 1.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Experiment</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We used a multi-phased approach incorporating adversarial training to increase the robustness of our multimodal image captioning model, which combines a Vision Transformer (ViT) encoder and a GPT-2 decoder. We conducted our experiment in five phases as follows:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">We trained the ViT-GPT-2 architecture on the Flickr8k dataset without considering adversarial factors, creating a baseline model that served as a benchmark for assessing the following experiments.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">We applied the FGSM to create adversarial examples for the training and test sets. We used these samples to train the model and evaluated the performance of the model.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">To improve robustness, we performed adversarial training on the model using both the original data and adversarial examples.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">We investigated a more focused strategy to further improve the modelâ€™s robustness. Initially, we used only the GPT-2 decoder for adversarial training while freezing the ViT encoder.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">We reversed the process by freezing the GPT-2 decoder and using the ViT encoder alone for adversarial training.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">We conducted steps four and five three times each and calculated the average of these trials.
Focusing on a single modality at a time allowed us to implement more effective adversarial defense mechanisms without having to scale them across the entire model. We repeated these five steps of experiment with COCO dataset as well to access the robustness of our model. For both the datasets we have used the same perturbation magnitude, <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\epsilon</annotation></semantics></math> = 0.1.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">After conducting experiments on both the Flickr8k and COCO datasets, we evaluated the performance of our multimodal machine learning models using the BLEU score metric. The goal of these experiments was to find an efficient way to increase the robustness of these models against adversarial attacks. As shown in Table <a href="#S4.T2" title="Table 2 â€£ 4. Results â€£ AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our baseline model, trained on the original data, initially achieved the best performance. However, when we generated adversarial examples using the FGSM and trained the model with these samples, its performance decreased significantly. This suggests that our multimodal model is not robust against adversarial attacks, as generating a single adversarial example for each image in the dataset was sufficient to degrade its performance.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To enhance the modelâ€™s robustness, we trained it with both the original data and the adversarial examples. This adversarial training approach improved the modelâ€™s performance, indicating that incorporating adversarial examples during training can help mitigate the impact of such attacks.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Aiming to find an efficient way to increase the robustness of our multimodal model, we investigated whether training only one of its components (either the image model or the text model) would be sufficient. First, we froze the image model (ViT) and trained the text model with both the original data and the adversarial examples. As shown in Table <a href="#S4.T2" title="Table 2 â€£ 4. Results â€£ AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the modelâ€™s performance in this case was lower than with full adversarial training, although the difference was not significant. Alternatively, we froze the text model (GPT) and trained the image model; in this case, we can see the performance is much lower than the full adversarial trainingâ€™s performance.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Model performance on training and testing datasets using Flickr8k dataset.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Train Set</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Test Set</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline BLEU Score</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.285</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">0.232</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Example</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_right">0.164</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_right">0.143</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Training</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_right">0.217</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_right">0.215</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Training by freezing ViT</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_right">0.200</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_right">0.181</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Adversarial Training by freezing GPT</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_right ltx_border_b">0.060</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_right ltx_border_b">0.050</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We conducted similar experiments using the COCO dataset, and the results are presented in Table <a href="#S4.T3" title="Table 3 â€£ 4. Results â€£ AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The baseline performance on the COCO dataset was higher than on Flickr8k. However, when the model was trained solely on adversarial examples generated from the COCO dataset, the performance decreased significantly, as expected. In the third phase, where we trained the model on both the original data and the adversarial samples, the result was comparable to the baseline modelâ€™s performance.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">When we froze the image model (ViT) and trained only the text model, the difference in BLEU score compared to the full adversarial training approach was relatively small. This observation aligns with our findings from the Flickr8k dataset.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Model performance on training and testing datasets using COCO dataset.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Train Set</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Test Set</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline BLEU Score</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.3000</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">0.2812</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Example</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_right">0.1464</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_right">0.1430</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Training</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_right">0.2910</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_right">0.2890</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Adversarial Training by freezing ViT</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_right">0.2601</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_right">0.2500</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Adversarial Training by freezing GPT</th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_right ltx_border_b">0.0900</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_right ltx_border_b">0.0920</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Through experiments on both the Flickr8k and COCO datasets, we can conclude that while the adversarial training technique improves the robustness of our multimodal models against adversarial attacks, their performance does not fully match the baseline levels achieved with clean data. However, the adversarially trained models showed performance close to the baseline, suggesting that this approach can effectively mitigate the impact of adversarial attacks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our findings suggest that performing adversarial training only in the text model can achieve comparable performance to full adversarial training. However, freezing the text model or the GPT-2 model during training significantly degrades performance. When the ViT encoder is frozen and adversarial training is performed solely on the GPT-2 decoder, the model still benefits from the ViT encoderâ€™s robust feature extraction capabilities. The ViT encoder continues to deliver consistent and reliable image features, enabling the GPT-2 decoder to concentrate on learning to generate text robustly. In contrast, when the GPT-2 decoder is frozen and adversarial training is applied solely to the ViT encoder, the modelâ€™s capacity to generate coherent and contextually relevant text is hampered. Since the parameters of the GPT-2 decoder remain fixed, it cannot adjust to the alterations in the image features produced by the ViT encoder during adversarial training. Consequently, this lack of adaptability in the text generation process results in a decline in performance. In conclusion, this finding presents an opportunity to increase computational efficiency by focusing the training efforts on a text modality, with a modest trade-off in terms of performance compared to training the entire multimodal model.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our research has shown important steps forward in making multimodal machine learning models more robust, especially for tasks like image captioning. We combined Vision Transformer (ViT) and GPT-2 architectures and used adversarial training methods to focus on specific parts of the model. This focused approach helps protect against adversarial attacks without needing to train the entire model, which saves time and resources. Through our experiments with the Flickr8k and COCO datasets, we found that applying adversarial training to just the text decoder can make the models much more robust. While the performance of these models with adversarial training is slightly lower than those trained only on clean data, the trade-off is minimal. This means we can improve model safety without losing much accuracy. Freezing the image encoder and training the text decoder helps balance performance and robustness efficiently. On the other hand, training the image encoder alone does not provide the same benefits, highlighting the text decoderâ€™s key role in maintaining robustness.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Future work will explore changes in activation functions for additional gains in adversarial robustnessÂ <cite class="ltx_cite ltx_citemacro_citep">(Sooksatra etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. Current and future work supports the goals of Ethical AI by making AI systems safer and reliable. By focusing on specific parts of the model, we can make the adversarial defense process more efficient. This targeted approach can lead to the development of AI systems that are both resource-efficient and secure for public use. Enhancing the robustness of AI models against adversarial attacks builds public trust and confidence in AI systems, especially in critical applications like healthcare, autonomous driving, and content moderation.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgment</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This research work was part of a project funded by the National Science Foundation under grants CNS-2210091, CHE-1905043, and CNS-2136961.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini and Wagner (2017)</span>
<span class="ltx_bibblock">
Nicholas Carlini and David Wagner. 2017.

</span>
<span class="ltx_bibblock">Towards Evaluating the Robustness of Neural Networks. In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</em>. 39â€“57.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/SP.2017.49" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/SP.2017.49</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, etÂ al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evtimov etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, and CristianÂ Canton Ferrer. 2020.

</span>
<span class="ltx_bibblock">Adversarial evaluation of multimodal models under realistic gray box assumption.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.12902</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jing Gao, Peng Li, Zhikui Chen, and Jianing Zhang. 2020.

</span>
<span class="ltx_bibblock">A Survey on Deep Learning for Multimodal Data Fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Neural Computation</em> 32, 5 (05 2020), 829â€“864.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1162/neco_a_01273" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1162/neco_a_01273</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
IanÂ J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014.

</span>
<span class="ltx_bibblock">Explaining and harnessing adversarial examples.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6572</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madry etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017.

</span>
<span class="ltx_bibblock">Towards deep learning models resistant to adversarial attacks.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.06083</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noever and Noever (2021)</span>
<span class="ltx_bibblock">
DavidÂ A Noever and Samantha EÂ Miller Noever. 2021.

</span>
<span class="ltx_bibblock">Reading Isnâ€™t Believing: Adversarial Attacks On Multi-Modal Neurons.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.10480</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozdag (2018)</span>
<span class="ltx_bibblock">
Mesut Ozdag. 2018.

</span>
<span class="ltx_bibblock">Adversarial Attacks and Defenses Against Deep Neural Networks: A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Procedia Computer Science</em> 140 (2018), 152â€“161.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.procs.2018.10.315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.procs.2018.10.315</a>

</span>
<span class="ltx_bibblock">Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papernot etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z.Â Berkay Celik, and Ananthram Swami. 2016.

</span>
<span class="ltx_bibblock">The Limitations of Deep Learning in Adversarial Settings. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">2016 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</em>. 372â€“387.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/EuroSP.2016.36" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/EuroSP.2016.36</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">BLEU: a method for automatic evaluation of machine translation. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em> (Philadelphia, Pennsylvania) <em id="bib.bib11.4.2" class="ltx_emph ltx_font_italic">(ACL â€™02)</em>. Association for Computational Linguistics, USA, 311â€“318.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3115/1073083.1073135</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, etÂ al<span id="bib.bib12.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.4.1" class="ltx_emph ltx_font_italic">OpenAI blog</em> 1, 8 (2019), 9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlarmann and Hein (2023)</span>
<span class="ltx_bibblock">
Christian Schlarmann and Matthias Hein. 2023.

</span>
<span class="ltx_bibblock">On the Adversarial Robustness of Multi-Modal Foundation Models. In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</em>. 3679â€“3687.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICCVW60793.2023.00395" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCVW60793.2023.00395</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shafahi etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ali Shafahi, Mahyar Najibi, MohammadÂ Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, LarryÂ S Davis, Gavin Taylor, and Tom Goldstein. 2019.

</span>
<span class="ltx_bibblock">Adversarial training for free!

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arxiv.1904.12843" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arxiv.1904.12843</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sooksatra etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Korn Sooksatra, Greg Hamerly, and Pablo Rivas. 2023.

</span>
<span class="ltx_bibblock">Is ReLU Adversarially Robust?. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">LXAI Workshop @ International Conference on Machine Learning (ICML 2023)</em>. 1â€“10.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/doi.org/10.52591/lxai202307232" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/doi.org/10.52591/lxai202307232</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Youngjoon Yu, HongÂ Joo Lee, ByeongÂ Cheon Kim, JungÂ Uk Kim, and YongÂ Man Ro. 2020.

</span>
<span class="ltx_bibblock">Investigating vulnerability to adversarial examples on multimodal data fusion in deep learning.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arxiv.2005.10987" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arxiv.2005.10987</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.21173" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.21174" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.21174">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.21174" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.21175" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:26:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
