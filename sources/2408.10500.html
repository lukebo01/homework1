<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.10500] SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition</title><meta property="og:description" content="This paper presents our winning approach for the MER-NOISE and MER-OV tracks of the MER2024 Challenge on multimodal emotion recognition. Our system leverages the advanced emotional understanding capabilities of Emotion…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.10500">

<!--Generated on Thu Sep  5 13:51:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="MER2024,  Noise Robustness,  Open-Vocabulary Recognition">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zebang Cheng<sup id="id1.1.id1" class="ltx_sup">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_affiliation_institution">ShenZhen Technology University</span><span id="id3.3.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id4.4.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zebang.cheng@gmail.com">zebang.cheng@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuyuan Tu<sup id="id5.1.id1" class="ltx_sup">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text ltx_affiliation_institution">Carnegie Mellon University</span><span id="id7.3.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id8.4.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:francisshuyuan@gmail.com">francisshuyuan@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dawei Huang<sup id="id9.1.id1" class="ltx_sup">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.2.id1" class="ltx_text ltx_affiliation_institution">ShenZhen Technology University</span><span id="id11.3.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id12.4.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:huangdawei2023@email.szu.edu.cn">huangdawei2023@email.szu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minghan Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Carnegie Mellon University</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:lperlpm@gmail.com">lperlpm@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaojiang Peng<sup id="id16.1.id1" class="ltx_sup">†</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id17.2.id1" class="ltx_text ltx_affiliation_institution">ShenZhen Technology University</span><span id="id18.3.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id19.4.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pengxiaojiang@sztu.edu.cn">pengxiaojiang@sztu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhi-Qi Cheng<sup id="id20.1.id1" class="ltx_sup">†</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id21.2.id1" class="ltx_text ltx_affiliation_institution">Carnegie Mellon University</span><span id="id22.3.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id23.4.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhiqic@cs.cmu.edu">zhiqic@cs.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander G. Hauptmann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id24.1.id1" class="ltx_text ltx_affiliation_institution">Carnegie Mellon University</span><span id="id25.2.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id26.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:Alex@cs.cmu.edu">Alex@cs.cmu.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id27.id1" class="ltx_p">This paper presents our winning approach for the <span id="id27.id1.1" class="ltx_text ltx_font_italic">MER-NOISE</span> and <span id="id27.id1.2" class="ltx_text ltx_font_italic">MER-OV</span> tracks of the <span id="id27.id1.3" class="ltx_text ltx_font_italic">MER2024 Challenge</span> on multimodal emotion recognition. Our system leverages the advanced emotional understanding capabilities of <span id="id27.id1.4" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> to generate high-quality annotations for unlabeled samples, addressing the challenge of limited labeled data. To enhance multimodal fusion while mitigating modality-specific noise, we introduce <span id="id27.id1.5" class="ltx_text ltx_font_italic">Conv-Attention</span>, a lightweight and efficient hybrid framework. Extensive experimentation validates the effectiveness of our approach. In the <span id="id27.id1.6" class="ltx_text ltx_font_italic">MER-NOISE</span> track, our system achieves a state-of-the-art weighted average F-score of 85.30%, surpassing the second and third-place teams by 1.47% and 1.65%, respectively. For the <span id="id27.id1.7" class="ltx_text ltx_font_italic">MER-OV</span> track, our utilization of <span id="id27.id1.8" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> for open-vocabulary annotation yields an 8.52% improvement in average accuracy and recall compared to <span id="id27.id1.9" class="ltx_text ltx_font_italic">GPT-4V</span>, securing the highest score among all participating large multimodal models. The code and model for <span id="id27.id1.10" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> are available at <a target="_blank" href="https://github.com/ZebangCheng/Emotion-LLaMA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ZebangCheng/Emotion-LLaMA</a>.</p>
</div>
<div class="ltx_keywords">MER2024, Noise Robustness, Open-Vocabulary Recognition
</div>
<div class="ltx_acknowledgements">Z. Cheng (Emotion-LLaMA), S. Tu (Conv-Attention), D. Huang (feature engineering) contrib. equally. M. Li (replicated last year’s champ solution).
</div>
<div class="ltx_acknowledgements">X. Peng &amp; Z-Q. Cheng (corresponding authors, guided research/system, organized and rewrote the paper). A. Hauptmann (valuable insights &amp; suggestions).
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing; October 28-November 1 2024; Melbourne, VIC, Australia</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing (MRAC ’24), October 28-November 1 2024, Melbourne, VIC, Australia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3689092.3689404</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-1203-6/24/10</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer vision</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Humancentered computing</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>HCI design and evaluation methods.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.10500/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of the Emotion-LLaMA architecture, which integrates audio, visual, and text inputs for advanced multimodal emotion recognition and reasoning. The model aligns and fuses audio and visual features into a shared semantic space, thereby enhancing the contextual understanding of textual inputs. Emotion-LLaMA leverages multiple visual encoders to capture global, local, and temporal visual aspects, which are then combined with audio and text features to generate detailed emotion descriptions. For further details, refer to the original Emotion-LLaMA paper <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite>.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Multimodal Emotion Recognition (MER)</span> aims to integrate information from various modalities—such as text, speech, and visual cues—to accurately identify and understand human emotions. This field has shown great promise in applications ranging from human-computer interaction to mental health care and education. However, achieving robust performance in real-world scenarios remains a significant challenge. The <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">MER2024 Challenge</span> addresses these challenges through two specialized tracks: <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">MER-NOISE</span> and <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">MER-OV</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">MER-NOISE</span> track focuses on enhancing noise robustness in emotion recognition systems. In practical settings, noise is pervasive, making it difficult to ensure that audio streams are free of distortions and video frames maintain high resolution. This track targets the two most prevalent types of noise: audio additive noise and image blur. Participants are encouraged to employ data augmentation techniques <cite class="ltx_cite ltx_citemacro_citep">(Pise and Kulkarni, <a href="#bib.bib49" title="" class="ltx_ref">2008</a>)</cite> and other innovative methods <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> to improve the resilience of emotion recognition systems against these noise factors.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">MER-OV</span> track introduces the concept of open-vocabulary emotion recognition, addressing the inherent subjectivity and ambiguity in emotion labeling. Traditional datasets often constrain label spaces to a few discrete categories, relying on multiple annotators and majority voting to determine the most likely label. This approach can overlook correct but non-candidate or minority labels, leading to potential inaccuracies. The <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">MER-OV</span> track challenges participants to generate any number of labels across diverse categories, striving for a more nuanced and precise description of emotional states <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024c</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address these challenges, we propose a robust system that integrates the advanced capabilities of <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> for generating high-quality labels with a <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">Conv-Attention</span> model designed for efficient multimodal feature fusion. Our approach is detailed in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2. Large Models in Emotion Understanding ‣ 2. Related Work ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where we outline the workflow and demonstrate how each component contributes to overcoming the limitations of existing methods.
A key limitation of previous approaches <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> lies in their reliance on models to generate pseudo-labels for unlabeled data, which are then used to augment training datasets. The effectiveness of this strategy depends heavily on the initial model’s quality—if the model lacks robustness, it can produce low-quality pseudo-labels, which may introduce errors in subsequent training phases. To mitigate this issue, we introduce <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite>, a model specifically designed to generate high-quality pseudo-labels for the unlabeled samples in the <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">MER2024</span> dataset. As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> processes inputs from multiple modalities, utilizing visual and auditory features as contextual information to enhance the interpretation of text-based emotions. This approach ensures robust multimodal emotion understanding, even in the presence of loss or noise of modality.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Prompts for generating emotion-related descriptions using Large Multimodal Models (LMMs). Prompts marked with <sup id="S1.T1.9.1" class="ltx_sup"><span id="S1.T1.9.1.1" class="ltx_text ltx_font_italic">†</span></sup> output only emotion categories, while those with <sup id="S1.T1.10.2" class="ltx_sup"><span id="S1.T1.10.2.1" class="ltx_text ltx_font_italic">‡</span></sup> provide complete emotion descriptions. These prompts direct the models to focus on key aspects of the input, ensuring high-quality, contextually rich outputs.</figcaption>
<table id="S1.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.6.3.1" class="ltx_tr">
<th id="S1.T1.6.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.6.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.3.1.1.1.1" class="ltx_p" style="width:99.6pt;">Models</span>
</span>
</th>
<th id="S1.T1.6.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.6.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.3.1.2.1.1" class="ltx_p" style="width:56.9pt;">Language</span>
</span>
</th>
<th id="S1.T1.6.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.6.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.3.1.3.1.1" class="ltx_p" style="width:284.5pt;">Prompt</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.5.1" class="ltx_tr">
<td id="S1.T1.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.5.1.1.1.1.1" class="ltx_text">Emotion-LLaMA<sup id="S1.T1.5.1.1.1.1.1.1" class="ltx_sup"><span id="S1.T1.5.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup></span></span>
</span>
</td>
<td id="S1.T1.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.5.1.2.1.1.1" class="ltx_text">English</span></span>
</span>
</td>
<td id="S1.T1.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.3.1.1" class="ltx_p" style="width:284.5pt;">Please determine which <span id="S1.T1.5.1.3.1.1.1" class="ltx_text" style="color:#ED0078;">emotion label</span> in the video represents: happy, sad, neutral, angry, worried, surprise.</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.2" class="ltx_tr">
<td id="S1.T1.6.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.2.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.2.1.1.1.1" class="ltx_text">Emotion-LLaMA<sup id="S1.T1.6.2.1.1.1.1.1" class="ltx_sup"><span id="S1.T1.6.2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">‡</span></sup></span></span>
</span>
</td>
<td id="S1.T1.6.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.2.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.2.2.1.1.1" class="ltx_text">English</span></span>
</span>
</td>
<td id="S1.T1.6.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.6.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.2.3.1.1" class="ltx_p" style="width:284.5pt;">Please analyze all the <span id="S1.T1.6.2.3.1.1.1" class="ltx_text" style="color:#ED0078;">clues</span> in the video and <span id="S1.T1.6.2.3.1.1.2" class="ltx_text" style="color:#ED0078;">reason out</span> the emotional label of the person in the video.</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.4.1" class="ltx_tr">
<td id="S1.T1.6.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.4.1.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.4.1.1.1.1.1" class="ltx_text">LLaMA-3</span></span>
</span>
</td>
<td id="S1.T1.6.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.4.1.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.4.1.2.1.1.1" class="ltx_text">English</span></span>
</span>
</td>
<td id="S1.T1.6.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.6.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.4.1.3.1.1" class="ltx_p" style="width:284.5pt;">You are an emotion analysis expert. Please analyze the input multimodal emotion description and <span id="S1.T1.6.4.1.3.1.1.1" class="ltx_text" style="color:#ED0078;">output keywords</span> related to the emotion description. 
<br class="ltx_break">Input: [Multimodal Emotion Description] 
<br class="ltx_break">Output:</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.5.2" class="ltx_tr">
<td id="S1.T1.6.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.5.2.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.5.2.1.1.1.1" class="ltx_text">Qwen1.5-32B</span></span>
</span>
</td>
<td id="S1.T1.6.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.5.2.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.5.2.2.1.1.1" class="ltx_text">Chinese</span></span>
</span>
</td>
<td id="S1.T1.6.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.6.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.5.2.3.1.1" class="ltx_p" style="width:284.5pt;">Please <span id="S1.T1.6.5.2.3.1.1.1" class="ltx_text" style="color:#ED0078;">analyze</span> the provided text content and <span id="S1.T1.6.5.2.3.1.1.2" class="ltx_text" style="color:#ED0078;">classify emotions</span> into six categories: [neutral, angry, happy, sad, worried, surprise], and <span id="S1.T1.6.5.2.3.1.1.3" class="ltx_text" style="color:#ED0078;">explain</span> the specific reasons: &lt;Text&gt;</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.6.3" class="ltx_tr">
<td id="S1.T1.6.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.6.3.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.6.3.1.1.1.1" class="ltx_text">Baichuan-13B (prompt 1)</span></span>
</span>
</td>
<td id="S1.T1.6.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.6.3.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.6.3.2.1.1.1" class="ltx_text">Chinese</span></span>
</span>
</td>
<td id="S1.T1.6.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.6.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.6.3.3.1.1" class="ltx_p" style="width:284.5pt;">Please <span id="S1.T1.6.6.3.3.1.1.1" class="ltx_text" style="color:#ED0078;">analyze</span> the provided text content and <span id="S1.T1.6.6.3.3.1.1.2" class="ltx_text" style="color:#ED0078;">classify emotions</span> into six categories: [neutral, angry, happy, sad, worried, surprise], and <span id="S1.T1.6.6.3.3.1.1.3" class="ltx_text" style="color:#ED0078;">explain</span> the specific reasons: &lt;Text&gt;</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.7.4" class="ltx_tr">
<td id="S1.T1.6.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.7.4.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.7.4.1.1.1.1" class="ltx_text">Baichuan-13B (prompt 2)</span></span>
</span>
</td>
<td id="S1.T1.6.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.6.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.7.4.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.7.4.2.1.1.1" class="ltx_text">Chinese</span></span>
</span>
</td>
<td id="S1.T1.6.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.6.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.7.4.3.1.1" class="ltx_p" style="width:284.5pt;">Please <span id="S1.T1.6.7.4.3.1.1.1" class="ltx_text" style="color:#ED0078;">analyze</span> the provided text content and <span id="S1.T1.6.7.4.3.1.1.2" class="ltx_text" style="color:#ED0078;">classify emotions</span> into six categories: [neutral, angry, happy, sad, worried, surprise]: &lt;Text&gt;</span>
</span>
</td>
</tr>
<tr id="S1.T1.6.8.5" class="ltx_tr">
<td id="S1.T1.6.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.6.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.8.5.1.1.1" class="ltx_p" style="width:99.6pt;"><span id="S1.T1.6.8.5.1.1.1.1" class="ltx_text">Baichuan-13B (prompt 3)</span></span>
</span>
</td>
<td id="S1.T1.6.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.6.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.8.5.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S1.T1.6.8.5.2.1.1.1" class="ltx_text">Chinese</span></span>
</span>
</td>
<td id="S1.T1.6.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.6.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.6.8.5.3.1.1" class="ltx_p" style="width:284.5pt;">Please <span id="S1.T1.6.8.5.3.1.1.1" class="ltx_text" style="color:#ED0078;">analyze</span> the provided text content: &lt;Text&gt;</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the feature extraction stage, we leverage high-performing unimodal models referenced in the official baseline papers <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024c</a>, <a href="#bib.bib35" title="" class="ltx_ref">a</a>)</cite>, such as <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">HuBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> and <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">CLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>. Our experiments revealed that visual modality models are particularly vulnerable to noise, prompting us to pre-train <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">MAE</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> and <span id="S1.p5.1.4" class="ltx_text ltx_font_italic">VideoMAE</span> <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite> on the unlabeled samples from the <span id="S1.p5.1.5" class="ltx_text ltx_font_italic">MER2024</span> dataset. This pre-training effectively captures both static and dynamic visual expression features. Additionally, to enhance the accuracy of textual feature extraction, we employed prompt-based strategies for models like <span id="S1.p5.1.6" class="ltx_text ltx_font_italic">Qwen</span> <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>; Team, <a href="#bib.bib56" title="" class="ltx_ref">2024</a>)</cite> and <span id="S1.p5.1.7" class="ltx_text ltx_font_italic">Baichuan</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>, which were carefully evaluated for their effectiveness in capturing emotion-related information.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Despite the strong performance of <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> in <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">MER</span>, its substantial computational overhead and slow iteration cycle present challenges. To address these, we propose <span id="S1.p6.1.3" class="ltx_text ltx_font_italic">Conv-Attention</span>, a lightweight and efficient hybrid framework that combines convolutional and global attention mechanisms for feature fusion. <span id="S1.p6.1.4" class="ltx_text ltx_font_italic">Conv-Attention</span> leverages the inductive biases inherent in convolutional operations, allowing the model to perform effectively even with limited data. By integrating a simple attention mechanism with multiple convolutional blocks, the model can prioritize critical features while minimizing the impact of noise. The attention mechanism excels in querying features from a global perspective, while the convolutional operation focuses on capturing fine-grained semantic details within a limited receptive field. This combined approach mitigates the disadvantages of each individual method, enhancing overall model performance.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In summary, our team makes the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix1.1.1.m1.1b"><mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c"><ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">In the <span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">MER-OV</span> track, we utilize <span id="S1.I1.ix1.p1.1.2" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> to extract multimodal emotion descriptions and employ <span id="S1.I1.ix1.p1.1.3" class="ltx_text ltx_font_italic">LLaMA-3</span> for open-vocabulary annotation. The results from <span id="S1.I1.ix1.p1.1.4" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> serve as high-quality annotations for the unlabeled samples in the <span id="S1.I1.ix1.p1.1.5" class="ltx_text ltx_font_italic">MER2024</span> dataset, addressing the need for extensive and accurate training data.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix2.1.1.m1.1b"><mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c"><ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">In the <span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">MER-NOISE</span> track, we introduce <span id="S1.I1.ix2.p1.1.2" class="ltx_text ltx_font_italic">Conv-Attention</span>, a lightweight and effective hybrid framework for feature fusion that combines convolution and global attention mechanisms. Our model achieves comprehensive feature fusion by integrating the limited receptive fields of convolutions with the global querying capabilities of attention.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix3.1.1.m1.1b"><mo id="S1.I1.ix3.1.1.m1.1.1" xref="S1.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix3.1.1.m1.1c"><ci id="S1.I1.ix3.1.1.m1.1.1.cmml" xref="S1.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">Our approach achieves a state-of-the-art weighted average F-score of 85.30% in the <span id="S1.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">MER-NOISE</span> track, outperforming the second and third-place entries by 1.47% and 1.65%, respectively. In the <span id="S1.I1.ix3.p1.1.2" class="ltx_text ltx_font_italic">MER-OV</span> track, the application of <span id="S1.I1.ix3.p1.1.3" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> for open-vocabulary annotation resulted in an 8.52% improvement in average accuracy and recall compared to <span id="S1.I1.ix3.p1.1.4" class="ltx_text ltx_font_italic">GPT-4V</span>, significantly enhancing our competitive position.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Multimodal Emotion Recognition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Multimodal Emotion Recognition (MER)</span> has become a focal point in multimedia research, driven by the limitations of single-modal approaches in handling noise and ensuring robustness. The rise of multimedia sensors has shifted research towards multimodal datasets from real-world scenarios, emphasizing the importance of feature alignment and fusion <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Aguilera et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>; Poria et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2018</a>)</cite>. Early <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">MER</span> approaches utilized separate models for feature extraction across different modalities, such as ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2016</a>)</cite>, MAE <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, VideoMAE <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, and HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, followed by basic linear fusion layers <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>; Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2019</a>)</cite>. However, these simplistic models struggled to capture the complexity of multimodal data, prompting the development of more sophisticated cross-attention-based fusion models <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. Despite their advancements, these fusion techniques <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Nagrani et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2024a</a>)</cite> often lead to competition between modalities, where dominant modalities disproportionately influence the results.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To address these challenges, <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">PMR</span> <cite class="ltx_cite ltx_citemacro_citep">(Lv et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> introduced a common message hub to better capture cross-modal dynamics. Subsequent research has focused on pre-fusion alignment, as seen in <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">EmotionCLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2023b</a>)</cite>, which aligns temporal visual and textual data, and <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_italic">VAT</span> <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023b</a>)</cite>, which aligns visual with audio features. However, these models still face challenges, including the need for large datasets and the difficulty of capturing fine-grained emotional features due to a reliance on global attention mechanisms.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Our work overcomes these limitations by leveraging the advanced capabilities of <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> for generating high-quality pseudo-labels and introducing a <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">Conv-Attention</span> model for efficient multimodal feature fusion, significantly improving the robustness and accuracy of emotion recognition in noisy environments.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Large Models in Emotion Understanding</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The advent of large multimodal models (<span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">LMMs</span>) has revolutionized emotion understanding, providing unprecedented inferential capabilities <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2023</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2024</a>)</cite>. Instruction-tuning techniques, pioneered by models such as <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">InstructionGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>, <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">FLAN</span> <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, and <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">OPT-IML</span> <cite class="ltx_cite ltx_citemacro_citep">(Iyer et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, have further expanded the practical applications of these models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib60" title="" class="ltx_ref">b</a>)</cite>. In the context of emotion recognition, <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">InstructERC</span> <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> has advanced conversation-based emotion recognition by introducing additional emotion alignment tasks. <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">DFER-CLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhao and Patras, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>, built on the <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_italic">CLIP</span> model <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>, has shown promise in dynamic facial expression recognition. <span id="S2.SS2.p1.1.8" class="ltx_text ltx_font_italic">MER-MCE</span> <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2024b</a>)</cite> has pushed the boundaries by inferring the causes of emotional triggers in conversations through multimodal inputs. Notably, <span id="S2.SS2.p1.1.9" class="ltx_text ltx_font_italic">GPT-4V</span> <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024b</a>)</cite> has demonstrated strong capabilities in generalized emotion recognition tasks.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Despite these advances, most approaches rely on single emotion labels, often neglecting non-candidate or minority yet correct labels. Addressing the need for more nuanced emotion descriptions in real-world contexts, <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">AffectGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2023b</a>)</cite> proposes a multimodal, explainable, open-vocabulary emotion recognition approach, using <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">GPT-4V</span> to generate visual and acoustic signals and extract reliable labels. <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">EmoVIT</span> <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2024</a>)</cite> further contributes by generating visual emotion instruction data using paired annotations.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Building on these developments, our approach utilizes <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite> to generate detailed multimodal emotional descriptions, resulting in comprehensive open-vocabulary labels. <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">Emotion-LLaMA’s</span> capability to align multimodal features within a semantic space allows it to maintain robust emotion understanding even in the presence of modality loss or noise, significantly enhancing the accuracy and robustness of emotion recognition systems in real-world applications.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.10500/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of our framework for MER2024. In the feature extraction phase, frozen encoders extract features from text, video, and audio, which are pooled to integrate multimodal information. In the feature fusion stage, our Conv-Attention mechanism is applied, as detailed in part (b) of the figure. The pre-trained Emotion-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite> model generates pseudo-labels, which are combined with original labeled data, enhancing the dataset through augmentation. Finally, the augmented dataset is used to train the Conv-Attention model, boosting the performance and robustness of our emotion recognition system.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section presents our method that secured the highest performance in Track 2: MER-NOISE at the MER 2024 contest. First, we detail our feature extraction process (Sec. <a href="#S3.SS1" title="3.1. Multimodal Feature Engineering ‣ 3. Methodology ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Next, we describe how <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> generates multimodal emotional descriptions and derives high-quality emotion labels (Sec. <a href="#S3.SS2" title="3.2. Emotion-LLaMA Pseudo-Labeling ‣ 3. Methodology ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Finally, we explain the Conv-Attention model used for feature fusion (Sec. <a href="#S3.SS3" title="3.3. Multimodal Feature Fusion ‣ 3. Methodology ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Multimodal Feature Engineering</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We employed domain-specific models to extract unimodal features from auditory, visual, and textual data, with each model leveraging prior knowledge tailored to its respective domain.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Auditory Modality</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The MER2024 dataset contains exclusively Mandarin speech dialogues, prompting the selection of audio encoders that support the Chinese language. We prioritized Chinese-Hubert <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> and emotion2vec <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, with Chinese-Hubert being a variant of Hubert pre-trained on Chinese datasets. This model excels in processing Mandarin, producing high-quality embeddings suitable for complex emotion detection, making it ideal for the challenges presented in MER2024.
To enhance the robustness and accuracy of our auditory emotion recognition pipeline, we also incorporated multilingual models such as Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, VGGish <cite class="ltx_cite ltx_citemacro_citep">(Hershey et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, and eGeMAPS <cite class="ltx_cite ltx_citemacro_citep">(Eyben et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>. Whisper, a significant advancement in Automatic Speech Recognition (ASR), combined with VGGish and eGeMAPS, provided a comprehensive audio processing framework.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Visual Modality</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Building on the experience and results from the MER2023 competition, we selected a series of high-performing visual encoders for comparative analysis, including CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>, MAE <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, VideoMAE <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, and MANet <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>.
CLIP, pre-trained on large-scale image-text pairs, excels at associating images with textual descriptions, making it particularly effective for affective computing tasks. MAE, designed for the visual domain, reconstructs obscured segments of input images, compelling the model to learn both global and local features, which is advantageous for facial recognition and emotion analysis.
VideoMAE extends MAE’s concept to video by applying random masking to video frames and training the model to reconstruct missing parts, effectively leveraging spatiotemporal characteristics for tasks such as video classification. Both MAE and VideoMAE were further fine-tuned to enhance their performance in noisy environments.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Textual Modality</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.4" class="ltx_p">Given that the subtitles extracted from audio are primarily in Chinese, we focused on models with strong Chinese language proficiency, including ChatGLM2 <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, Qwen <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, and Baichuan <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>. These models, pre-trained on extensive Chinese corpora, are particularly effective for handling Chinese text inputs.
We also utilized multilingual models such as RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, MacBERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>, and BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Workshop et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>. Inspired by In-Context Learning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> and Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite> techniques, we employed a prompt strategy to enhance feature extraction. This approach involved generating emotion-associated descriptions by appending a designed prompt before the text input, as formalized in the following equation:</p>
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle T^{\prime}=\text{prompt}\oplus T" display="inline"><semantics id="S3.E1X.2.1.1.m1.1a"><mrow id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml"><msup id="S3.E1X.2.1.1.m1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.2.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.2.2" xref="S3.E1X.2.1.1.m1.1.1.2.2.cmml">T</mi><mo id="S3.E1X.2.1.1.m1.1.1.2.3" xref="S3.E1X.2.1.1.m1.1.1.2.3.cmml">′</mo></msup><mo id="S3.E1X.2.1.1.m1.1.1.1" xref="S3.E1X.2.1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.1.1.3" xref="S3.E1X.2.1.1.m1.1.1.3.cmml"><mtext id="S3.E1X.2.1.1.m1.1.1.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2a.cmml">prompt</mtext><mo id="S3.E1X.2.1.1.m1.1.1.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.1.cmml">⊕</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.1b"><apply id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1"><eq id="S3.E1X.2.1.1.m1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1"></eq><apply id="S3.E1X.2.1.1.m1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.2">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.2">𝑇</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3">′</ci></apply><apply id="S3.E1X.2.1.1.m1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.E1X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.1">direct-sum</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.3.2a.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2"><mtext id="S3.E1X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2">prompt</mtext></ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.1c">\displaystyle T^{\prime}=\text{prompt}\oplus T</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle T_{\text{embedding}}=M(T^{\prime})" display="inline"><semantics id="S3.E1Xa.2.1.1.m1.1a"><mrow id="S3.E1Xa.2.1.1.m1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><msub id="S3.E1Xa.2.1.1.m1.1.1.3" xref="S3.E1Xa.2.1.1.m1.1.1.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.3.2" xref="S3.E1Xa.2.1.1.m1.1.1.3.2.cmml">T</mi><mtext id="S3.E1Xa.2.1.1.m1.1.1.3.3" xref="S3.E1Xa.2.1.1.m1.1.1.3.3a.cmml">embedding</mtext></msub><mo id="S3.E1Xa.2.1.1.m1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1Xa.2.1.1.m1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.1b"><apply id="S3.E1Xa.2.1.1.m1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1"><eq id="S3.E1Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2"></eq><apply id="S3.E1Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.1.1.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.1.1.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.3.2">𝑇</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.3.3a.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E1Xa.2.1.1.m1.1.1.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.3.3">embedding</mtext></ci></apply><apply id="S3.E1Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1"><times id="S3.E1Xa.2.1.1.m1.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2"></times><ci id="S3.E1Xa.2.1.1.m1.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.3">𝑀</ci><apply id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.1c">\displaystyle T_{\text{embedding}}=M(T^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.SSS3.p1.3" class="ltx_p">Here, <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="T^{\prime}" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><msup id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.1.1.2" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml">T</mi><mo id="S3.SS1.SSS3.p1.1.m1.1.1.3" xref="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><apply id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">T^{\prime}</annotation></semantics></math> represents the augmented text input, <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mo id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">\oplus</annotation></semantics></math> denotes the concatenation operation, and <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><mi id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><ci id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">M</annotation></semantics></math> is the language model used for feature extraction. This prompt-based method guides the model to capture emotional cues more effectively, resulting in richer and more accurate emotional descriptions.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Emotion-LLaMA Pseudo-Labeling</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite>, developed in our previous work, is a multimodal emotion recognition model that supports inputs across text, audio, and visual domains. By aligning audio and visual features within a shared semantic space as contextual information for the text modality, Emotion-LLaMA excels in multimodal emotion recognition and reasoning tasks. We leveraged Emotion-LLaMA’s capabilities for the MER2024 competition.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To address the challenge of limited labeled data, especially for the MER-NOISE track, we used Emotion-LLaMA to generate pseudo-labels. By performing multimodal emotion recognition on 20,000 unlabeled samples, we significantly augmented the training set with pseudo-labeled data. This approach not only increased the volume of training data but also introduced greater diversity, thereby enhancing the model’s generalization capabilities.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Prompt Design and Data Processing</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We designed specific prompts for Emotion-LLaMA and LLaMA-3 to extract detailed emotion-related descriptions and labels. These prompts guide the models to focus on relevant aspects of the input data, improving the quality and relevance of the generated labels. Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides examples of these prompts, illustrating their effectiveness in eliciting precise and informative responses.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.7" class="ltx_p">We processed the data by feeding it into Emotion-LLaMA with a simple instruction prompt to obtain emotion-related descriptions and category labels, as formalized below:</p>
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2X.2.1.1.m1.4" class="ltx_Math" alttext="\displaystyle\hat{\mathcal{T}}_{description}=E(\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u};\mathcal{P^{\ddagger}})" display="inline"><semantics id="S3.E2X.2.1.1.m1.4a"><mrow id="S3.E2X.2.1.1.m1.4.4" xref="S3.E2X.2.1.1.m1.4.4.cmml"><msub id="S3.E2X.2.1.1.m1.4.4.6" xref="S3.E2X.2.1.1.m1.4.4.6.cmml"><mover accent="true" id="S3.E2X.2.1.1.m1.4.4.6.2" xref="S3.E2X.2.1.1.m1.4.4.6.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.6.2.2" xref="S3.E2X.2.1.1.m1.4.4.6.2.2.cmml">𝒯</mi><mo id="S3.E2X.2.1.1.m1.4.4.6.2.1" xref="S3.E2X.2.1.1.m1.4.4.6.2.1.cmml">^</mo></mover><mrow id="S3.E2X.2.1.1.m1.4.4.6.3" xref="S3.E2X.2.1.1.m1.4.4.6.3.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.6.3.2" xref="S3.E2X.2.1.1.m1.4.4.6.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.3" xref="S3.E2X.2.1.1.m1.4.4.6.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1a" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.4" xref="S3.E2X.2.1.1.m1.4.4.6.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1b" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.5" xref="S3.E2X.2.1.1.m1.4.4.6.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1c" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.6" xref="S3.E2X.2.1.1.m1.4.4.6.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1d" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.7" xref="S3.E2X.2.1.1.m1.4.4.6.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1e" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.8" xref="S3.E2X.2.1.1.m1.4.4.6.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1f" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.9" xref="S3.E2X.2.1.1.m1.4.4.6.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1g" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.10" xref="S3.E2X.2.1.1.m1.4.4.6.3.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1h" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.11" xref="S3.E2X.2.1.1.m1.4.4.6.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.6.3.1i" xref="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml">​</mo><mi id="S3.E2X.2.1.1.m1.4.4.6.3.12" xref="S3.E2X.2.1.1.m1.4.4.6.3.12.cmml">n</mi></mrow></msub><mo id="S3.E2X.2.1.1.m1.4.4.5" xref="S3.E2X.2.1.1.m1.4.4.5.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.4.4.4" xref="S3.E2X.2.1.1.m1.4.4.4.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.4.6" xref="S3.E2X.2.1.1.m1.4.4.4.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.4.4.4.5" xref="S3.E2X.2.1.1.m1.4.4.4.5.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.4.4.4.4.4" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.4.4.4.4.4.5" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml">(</mo><msubsup id="S3.E2X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">𝒜</mi><mi id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.3.cmml">u</mi><mo id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S3.E2X.2.1.1.m1.4.4.4.4.4.6" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2X.2.1.1.m1.2.2.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.2.cmml">𝒱</mi><mi id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.3" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.3.cmml">u</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.3" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.3.cmml">′</mo></msubsup><mo id="S3.E2X.2.1.1.m1.4.4.4.4.4.7" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2X.2.1.1.m1.3.3.3.3.3.3" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.2" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.2.cmml">𝒯</mi><mi id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.3" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.3.cmml">u</mi><mo id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.3" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.3.cmml">′</mo></msubsup><mo id="S3.E2X.2.1.1.m1.4.4.4.4.4.8" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml">;</mo><msup id="S3.E2X.2.1.1.m1.4.4.4.4.4.4" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.2" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4.2.cmml">𝒫</mi><mo id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.3" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4.3.cmml">‡</mo></msup><mo stretchy="false" id="S3.E2X.2.1.1.m1.4.4.4.4.4.9" xref="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.4b"><apply id="S3.E2X.2.1.1.m1.4.4.cmml" xref="S3.E2X.2.1.1.m1.4.4"><eq id="S3.E2X.2.1.1.m1.4.4.5.cmml" xref="S3.E2X.2.1.1.m1.4.4.5"></eq><apply id="S3.E2X.2.1.1.m1.4.4.6.cmml" xref="S3.E2X.2.1.1.m1.4.4.6"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.4.4.6.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.6">subscript</csymbol><apply id="S3.E2X.2.1.1.m1.4.4.6.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.2"><ci id="S3.E2X.2.1.1.m1.4.4.6.2.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.2.1">^</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.2.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.2.2">𝒯</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.6.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3"><times id="S3.E2X.2.1.1.m1.4.4.6.3.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.1"></times><ci id="S3.E2X.2.1.1.m1.4.4.6.3.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.2">𝑑</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.3">𝑒</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.4.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.4">𝑠</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.5.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.5">𝑐</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.6.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.6">𝑟</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.7.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.7">𝑖</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.8.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.8">𝑝</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.9.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.9">𝑡</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.10.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.10">𝑖</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.11.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.11">𝑜</ci><ci id="S3.E2X.2.1.1.m1.4.4.6.3.12.cmml" xref="S3.E2X.2.1.1.m1.4.4.6.3.12">𝑛</ci></apply></apply><apply id="S3.E2X.2.1.1.m1.4.4.4.cmml" xref="S3.E2X.2.1.1.m1.4.4.4"><times id="S3.E2X.2.1.1.m1.4.4.4.5.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.5"></times><ci id="S3.E2X.2.1.1.m1.4.4.4.6.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.6">𝐸</ci><vector id="S3.E2X.2.1.1.m1.4.4.4.4.5.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4.4"><apply id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.2">𝒜</ci><ci id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.2.3">′</ci></apply><ci id="S3.E2X.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.1.1.1.1.1.1.3">𝑢</ci></apply><apply id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.2">𝒱</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.2.3">′</ci></apply><ci id="S3.E2X.2.1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.2.2.3">𝑢</ci></apply><apply id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.2">𝒯</ci><ci id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.2.3">′</ci></apply><ci id="S3.E2X.2.1.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3.3.3.3.3.3">𝑢</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4.2">𝒫</ci><ci id="S3.E2X.2.1.1.m1.4.4.4.4.4.4.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.4.4.4.4.3">‡</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.4c">\displaystyle\hat{\mathcal{T}}_{description}=E(\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u};\mathcal{P^{\ddagger}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E2Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2Xa.2.1.1.m1.4" class="ltx_Math" alttext="\displaystyle\hat{\mathcal{L}}=E(\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u};\mathcal{P^{\dagger}})" display="inline"><semantics id="S3.E2Xa.2.1.1.m1.4a"><mrow id="S3.E2Xa.2.1.1.m1.4.4" xref="S3.E2Xa.2.1.1.m1.4.4.cmml"><mover accent="true" id="S3.E2Xa.2.1.1.m1.4.4.6" xref="S3.E2Xa.2.1.1.m1.4.4.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.4.4.6.2" xref="S3.E2Xa.2.1.1.m1.4.4.6.2.cmml">ℒ</mi><mo id="S3.E2Xa.2.1.1.m1.4.4.6.1" xref="S3.E2Xa.2.1.1.m1.4.4.6.1.cmml">^</mo></mover><mo id="S3.E2Xa.2.1.1.m1.4.4.5" xref="S3.E2Xa.2.1.1.m1.4.4.5.cmml">=</mo><mrow id="S3.E2Xa.2.1.1.m1.4.4.4" xref="S3.E2Xa.2.1.1.m1.4.4.4.cmml"><mi id="S3.E2Xa.2.1.1.m1.4.4.4.6" xref="S3.E2Xa.2.1.1.m1.4.4.4.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2Xa.2.1.1.m1.4.4.4.5" xref="S3.E2Xa.2.1.1.m1.4.4.4.5.cmml">​</mo><mrow id="S3.E2Xa.2.1.1.m1.4.4.4.4.4" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml"><mo stretchy="false" id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.5" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml">(</mo><msubsup id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">𝒜</mi><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml">u</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.6" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.2" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.2.cmml">𝒱</mi><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.3" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.3.cmml">u</mi><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.3" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.3.cmml">′</mo></msubsup><mo id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.7" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml">,</mo><msubsup id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.2" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.2.cmml">𝒯</mi><mi id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.3" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.3.cmml">u</mi><mo id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.3" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.3.cmml">′</mo></msubsup><mo id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.8" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml">;</mo><msup id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.2" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.2.cmml">𝒫</mi><mo id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.3" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.3.cmml">†</mo></msup><mo stretchy="false" id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.9" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xa.2.1.1.m1.4b"><apply id="S3.E2Xa.2.1.1.m1.4.4.cmml" xref="S3.E2Xa.2.1.1.m1.4.4"><eq id="S3.E2Xa.2.1.1.m1.4.4.5.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.5"></eq><apply id="S3.E2Xa.2.1.1.m1.4.4.6.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.6"><ci id="S3.E2Xa.2.1.1.m1.4.4.6.1.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.6.1">^</ci><ci id="S3.E2Xa.2.1.1.m1.4.4.6.2.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.6.2">ℒ</ci></apply><apply id="S3.E2Xa.2.1.1.m1.4.4.4.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4"><times id="S3.E2Xa.2.1.1.m1.4.4.4.5.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.5"></times><ci id="S3.E2Xa.2.1.1.m1.4.4.4.6.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.6">𝐸</ci><vector id="S3.E2Xa.2.1.1.m1.4.4.4.4.5.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4"><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2">𝒜</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3">′</ci></apply><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.3">𝑢</ci></apply><apply id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.2">𝒱</ci><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.2.3">′</ci></apply><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.2.2.3">𝑢</ci></apply><apply id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.2">𝒯</ci><ci id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.2.3">′</ci></apply><ci id="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.3.3.3.3.3.3.3">𝑢</ci></apply><apply id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.1.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4">superscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.2.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.2">𝒫</ci><ci id="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.3.cmml" xref="S3.E2Xa.2.1.1.m1.4.4.4.4.4.4.3">†</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.2.1.1.m1.4c">\displaystyle\hat{\mathcal{L}}=E(\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u};\mathcal{P^{\dagger}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.SSS1.p2.6" class="ltx_p">where <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">E</annotation></semantics></math> represents Emotion-LLaMA, <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\hat{\mathcal{T}}_{description}" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><msub id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.SSS1.p2.2.m2.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.2.m2.1.1.2.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.2.cmml">𝒯</mi><mo id="S3.SS2.SSS1.p2.2.m2.1.1.2.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS2.SSS1.p2.2.m2.1.1.3" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1a" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.4" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1b" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.5" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1c" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.6" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1d" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.7" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1e" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.8" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1f" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.9" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1g" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.10" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1h" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.11" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.2.m2.1.1.3.1i" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3.12" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.12.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><apply id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.2"><ci id="S3.SS2.SSS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.2">𝒯</ci></apply><apply id="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3"><times id="S3.SS2.SSS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.2">𝑑</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.4.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.4">𝑠</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.5.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.5">𝑐</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.6.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.6">𝑟</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.7.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.7">𝑖</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.8.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.8">𝑝</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.9.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.9">𝑡</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.10.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.10">𝑖</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.11.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.11">𝑜</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.12.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.12">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">\hat{\mathcal{T}}_{description}</annotation></semantics></math> is the emotion-related description generated for the MER-OV track, <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\hat{\mathcal{L}}" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mover accent="true" id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.3.m3.1.1.2" xref="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml">ℒ</mi><mo id="S3.SS2.SSS1.p2.3.m3.1.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><apply id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.1">^</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.2">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">\hat{\mathcal{L}}</annotation></semantics></math> denotes the pseudo-label set, and <math id="S3.SS2.SSS1.p2.4.m4.3" class="ltx_Math" alttext="\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u}" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.3a"><mrow id="S3.SS2.SSS1.p2.4.m4.3.3.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.4.cmml"><msubsup id="S3.SS2.SSS1.p2.4.m4.1.1.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.2" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.2.cmml">𝒜</mi><mi id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.3" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.3.cmml">u</mi><mo id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.3" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S3.SS2.SSS1.p2.4.m4.3.3.3.4" xref="S3.SS2.SSS1.p2.4.m4.3.3.4.cmml">,</mo><msubsup id="S3.SS2.SSS1.p2.4.m4.2.2.2.2" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.2" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.2.cmml">𝒱</mi><mi id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.3" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.3.cmml">u</mi><mo id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.3" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.3.cmml">′</mo></msubsup><mo id="S3.SS2.SSS1.p2.4.m4.3.3.3.5" xref="S3.SS2.SSS1.p2.4.m4.3.3.4.cmml">,</mo><msubsup id="S3.SS2.SSS1.p2.4.m4.3.3.3.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.2" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.2.cmml">𝒯</mi><mi id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.3.cmml">u</mi><mo id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.3.cmml">′</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.3b"><list id="S3.SS2.SSS1.p2.4.m4.3.3.4.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3"><apply id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.2">𝒜</ci><ci id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.2.3">′</ci></apply><ci id="S3.SS2.SSS1.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1.1.1.3">𝑢</ci></apply><apply id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2">subscript</csymbol><apply id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2">superscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.2">𝒱</ci><ci id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.2.3">′</ci></apply><ci id="S3.SS2.SSS1.p2.4.m4.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.2.2.3">𝑢</ci></apply><apply id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3">subscript</csymbol><apply id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3">superscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.2">𝒯</ci><ci id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.2.3">′</ci></apply><ci id="S3.SS2.SSS1.p2.4.m4.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.3.3">𝑢</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.3c">\mathcal{A}^{\prime}_{u},\mathcal{V}^{\prime}_{u},\mathcal{T}^{\prime}_{u}</annotation></semantics></math> are the data in the audio, visual, and textual modalities, respectively. The prompts <math id="S3.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{P}^{\dagger}" display="inline"><semantics id="S3.SS2.SSS1.p2.5.m5.1a"><msup id="S3.SS2.SSS1.p2.5.m5.1.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.5.m5.1.1.2" xref="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml">𝒫</mi><mo id="S3.SS2.SSS1.p2.5.m5.1.1.3" xref="S3.SS2.SSS1.p2.5.m5.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.5.m5.1b"><apply id="S3.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.2">𝒫</ci><ci id="S3.SS2.SSS1.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.5.m5.1c">\mathcal{P}^{\dagger}</annotation></semantics></math> and <math id="S3.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{P}^{\ddagger}" display="inline"><semantics id="S3.SS2.SSS1.p2.6.m6.1a"><msup id="S3.SS2.SSS1.p2.6.m6.1.1" xref="S3.SS2.SSS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.6.m6.1.1.2" xref="S3.SS2.SSS1.p2.6.m6.1.1.2.cmml">𝒫</mi><mo id="S3.SS2.SSS1.p2.6.m6.1.1.3" xref="S3.SS2.SSS1.p2.6.m6.1.1.3.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.6.m6.1b"><apply id="S3.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1.2">𝒫</ci><ci id="S3.SS2.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1.3">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.6.m6.1c">\mathcal{P}^{\ddagger}</annotation></semantics></math> are used for multimodal emotion classification and reasoning, respectively.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Keyword Extraction and Dataset Augmentation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.5" class="ltx_p">We employed LLaMA-3 as a keyword extractor to convert emotional descriptions into labels, which were then used as the final prediction results for MER-OV. These pseudo-labeled samples were combined with the Train&amp;Val dataset from MER2024 to create the training set for the multimodal fusion model:</p>
<table id="S3.E3" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E3X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{OV}=LLaMA(\hat{\mathcal{T}}_{description})" display="inline"><semantics id="S3.E3X.2.1.1.m1.1a"><mrow id="S3.E3X.2.1.1.m1.1.1" xref="S3.E3X.2.1.1.m1.1.1.cmml"><msub id="S3.E3X.2.1.1.m1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.1.1.3.2" xref="S3.E3X.2.1.1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E3X.2.1.1.m1.1.1.3.3" xref="S3.E3X.2.1.1.m1.1.1.3.3.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.3.3.2" xref="S3.E3X.2.1.1.m1.1.1.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.3.3.1" xref="S3.E3X.2.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.3.3.3" xref="S3.E3X.2.1.1.m1.1.1.3.3.3.cmml">V</mi></mrow></msub><mo id="S3.E3X.2.1.1.m1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E3X.2.1.1.m1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.4" xref="S3.E3X.2.1.1.m1.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.2a" xref="S3.E3X.2.1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.5" xref="S3.E3X.2.1.1.m1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.2b" xref="S3.E3X.2.1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.6" xref="S3.E3X.2.1.1.m1.1.1.1.6.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.2c" xref="S3.E3X.2.1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.7" xref="S3.E3X.2.1.1.m1.1.1.1.7.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.2d" xref="S3.E3X.2.1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3X.2.1.1.m1.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3X.2.1.1.m1.1.1.1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">𝒯</mi><mo id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.2" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.3" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1a" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.4" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1b" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.5" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1c" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.6" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1d" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.7" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1e" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.8" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1f" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.9" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1g" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.10" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1h" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.11" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1i" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.12" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.12.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.E3X.2.1.1.m1.1.1.1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.1b"><apply id="S3.E3X.2.1.1.m1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1"><eq id="S3.E3X.2.1.1.m1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.2"></eq><apply id="S3.E3X.2.1.1.m1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.2">ℒ</ci><apply id="S3.E3X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.3"><times id="S3.E3X.2.1.1.m1.1.1.3.3.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.3.1"></times><ci id="S3.E3X.2.1.1.m1.1.1.3.3.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.3.2">𝑂</ci><ci id="S3.E3X.2.1.1.m1.1.1.3.3.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.3.3">𝑉</ci></apply></apply><apply id="S3.E3X.2.1.1.m1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1"><times id="S3.E3X.2.1.1.m1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.2"></times><ci id="S3.E3X.2.1.1.m1.1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.3">𝐿</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.4.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.4">𝐿</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.5.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.5">𝑎</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.6.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.6">𝑀</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.7.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.7">𝐴</ci><apply id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2"><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.2.2">𝒯</ci></apply><apply id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3"><times id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.2">𝑑</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.3">𝑒</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.4.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.4">𝑠</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.5.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.5">𝑐</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.6.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.6">𝑟</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.7.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.7">𝑖</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.8.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.8">𝑝</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.9.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.9">𝑡</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.10.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.10">𝑖</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.11.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.11">𝑜</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.12.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1.1.1.3.12">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{OV}=LLaMA(\hat{\mathcal{T}}_{description})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E3Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3Xa.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{D}_{aug}=\mathcal{D}_{l}\cup(\mathcal{D}^{\prime}_{u},\hat{\mathcal{L}})" display="inline"><semantics id="S3.E3Xa.2.1.1.m1.2a"><mrow id="S3.E3Xa.2.1.1.m1.2.2" xref="S3.E3Xa.2.1.1.m1.2.2.cmml"><msub id="S3.E3Xa.2.1.1.m1.2.2.3" xref="S3.E3Xa.2.1.1.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3Xa.2.1.1.m1.2.2.3.2" xref="S3.E3Xa.2.1.1.m1.2.2.3.2.cmml">𝒟</mi><mrow id="S3.E3Xa.2.1.1.m1.2.2.3.3" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.cmml"><mi id="S3.E3Xa.2.1.1.m1.2.2.3.3.2" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.2.3.3.1" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.2.3.3.3" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.2.3.3.1a" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.2.3.3.4" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.4.cmml">g</mi></mrow></msub><mo id="S3.E3Xa.2.1.1.m1.2.2.2" xref="S3.E3Xa.2.1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.E3Xa.2.1.1.m1.2.2.1" xref="S3.E3Xa.2.1.1.m1.2.2.1.cmml"><msub id="S3.E3Xa.2.1.1.m1.2.2.1.3" xref="S3.E3Xa.2.1.1.m1.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3Xa.2.1.1.m1.2.2.1.3.2" xref="S3.E3Xa.2.1.1.m1.2.2.1.3.2.cmml">𝒟</mi><mi id="S3.E3Xa.2.1.1.m1.2.2.1.3.3" xref="S3.E3Xa.2.1.1.m1.2.2.1.3.3.cmml">l</mi></msub><mo id="S3.E3Xa.2.1.1.m1.2.2.1.2" xref="S3.E3Xa.2.1.1.m1.2.2.1.2.cmml">∪</mo><mrow id="S3.E3Xa.2.1.1.m1.2.2.1.1.1" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.2" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.2.cmml">(</mo><msubsup id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.2" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.2.cmml">𝒟</mi><mi id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.3" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.3.cmml">u</mi><mo id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.3" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.3" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.2.cmml">,</mo><mover accent="true" id="S3.E3Xa.2.1.1.m1.1.1" xref="S3.E3Xa.2.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3Xa.2.1.1.m1.1.1.2" xref="S3.E3Xa.2.1.1.m1.1.1.2.cmml">ℒ</mi><mo id="S3.E3Xa.2.1.1.m1.1.1.1" xref="S3.E3Xa.2.1.1.m1.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.4" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3Xa.2.1.1.m1.2b"><apply id="S3.E3Xa.2.1.1.m1.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2"><eq id="S3.E3Xa.2.1.1.m1.2.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.2"></eq><apply id="S3.E3Xa.2.1.1.m1.2.2.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.2.3.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3">subscript</csymbol><ci id="S3.E3Xa.2.1.1.m1.2.2.3.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.2">𝒟</ci><apply id="S3.E3Xa.2.1.1.m1.2.2.3.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.3"><times id="S3.E3Xa.2.1.1.m1.2.2.3.3.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.1"></times><ci id="S3.E3Xa.2.1.1.m1.2.2.3.3.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.2">𝑎</ci><ci id="S3.E3Xa.2.1.1.m1.2.2.3.3.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.3">𝑢</ci><ci id="S3.E3Xa.2.1.1.m1.2.2.3.3.4.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.3.3.4">𝑔</ci></apply></apply><apply id="S3.E3Xa.2.1.1.m1.2.2.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1"><union id="S3.E3Xa.2.1.1.m1.2.2.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.2"></union><apply id="S3.E3Xa.2.1.1.m1.2.2.1.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.2.1.3.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.3">subscript</csymbol><ci id="S3.E3Xa.2.1.1.m1.2.2.1.3.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.3.2">𝒟</ci><ci id="S3.E3Xa.2.1.1.m1.2.2.1.3.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.3.3">𝑙</ci></apply><interval closure="open" id="S3.E3Xa.2.1.1.m1.2.2.1.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1"><apply id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1">superscript</csymbol><ci id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.2">𝒟</ci><ci id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.2.3">′</ci></apply><ci id="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.1.1.1.1.3">𝑢</ci></apply><apply id="S3.E3Xa.2.1.1.m1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1"><ci id="S3.E3Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.1">^</ci><ci id="S3.E3Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.2">ℒ</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.2.1.1.m1.2c">\displaystyle\mathcal{D}_{aug}=\mathcal{D}_{l}\cup(\mathcal{D}^{\prime}_{u},\hat{\mathcal{L}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.SSS2.p1.4" class="ltx_p">where <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{OV}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><msub id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml">V</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">ℒ</ci><apply id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2">𝑂</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\mathcal{L}_{OV}</annotation></semantics></math> represents the open vocabulary labels, <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{l}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><msub id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">𝒟</mi><mi id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝒟</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\mathcal{D}_{l}</annotation></semantics></math> is the labeled dataset from MER2024, <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}^{\prime}_{u}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><msubsup id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.2.cmml">𝒟</mi><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml">u</mi><mo id="S3.SS2.SSS2.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.2">𝒟</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.3">′</ci></apply><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">\mathcal{D}^{\prime}_{u}</annotation></semantics></math> is the unlabeled dataset, and <math id="S3.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{aug}" display="inline"><semantics id="S3.SS2.SSS2.p1.4.m4.1a"><msub id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.4.m4.1.1.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.2.cmml">𝒟</mi><mrow id="S3.SS2.SSS2.p1.4.m4.1.1.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.4.m4.1.1.3.1a" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.4.m4.1.1.3.4" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.1b"><apply id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.2">𝒟</ci><apply id="S3.SS2.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3"><times id="S3.SS2.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.2">𝑎</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.3">𝑢</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.3.4.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.1c">\mathcal{D}_{aug}</annotation></semantics></math> is the augmented dataset. By leveraging the capabilities of Emotion-LLaMA, our methodology significantly enhances the volume and quality of data available for training, effectively addressing the issue of sample scarcity.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Multimodal Feature Fusion</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To address the limitations of pure attention mechanisms, we employed the Conv-Attention structure, as illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2. Large Models in Emotion Understanding ‣ 2. Related Work ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b). This structure integrates convolutional blocks with attention mechanisms to introduce inductive biases, improving performance on limited-scale data. The convolutional branch captures semantic details due to its limited receptive fields, while the attention branch focuses on global, emotionally salient features.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">We began by using a multilayer perceptron (MLP) to standardize the channel depth of features from different modalities (audio, visual, text) to a common dimension. Each feature is represented as <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="(\text{batch},\text{depth})" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.2.3.2.1" xref="S3.SS3.p2.1.m1.2.3.1.cmml">(</mo><mtext id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1a.cmml">batch</mtext><mo id="S3.SS3.p2.1.m1.2.3.2.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml">,</mo><mtext id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2a.cmml">depth</mtext><mo stretchy="false" id="S3.SS3.p2.1.m1.2.3.2.3" xref="S3.SS3.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><interval closure="open" id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.2"><ci id="S3.SS3.p2.1.m1.1.1a.cmml" xref="S3.SS3.p2.1.m1.1.1"><mtext id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">batch</mtext></ci><ci id="S3.SS3.p2.1.m1.2.2a.cmml" xref="S3.SS3.p2.1.m1.2.2"><mtext id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2">depth</mtext></ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">(\text{batch},\text{depth})</annotation></semantics></math>. These features were then concatenated along their embedding depths and sequence lengths to obtain hybrid features <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\bm{F}_{d}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑭</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\bm{F}_{d}</annotation></semantics></math> and <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\bm{F}_{s}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝑭</ci><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\bm{F}_{s}</annotation></semantics></math>:</p>
<table id="S3.E4" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E4X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4X.2.1.1.m1.5" class="ltx_Math" alttext="\displaystyle\bm{F}_{d}=\mathtt{Concat}(\mathtt{MLP}(\bm{A}),\mathtt{MLP}(\bm{V}),\mathtt{MLP}(\bm{T}),\text{dim}=1)," display="inline"><semantics id="S3.E4X.2.1.1.m1.5a"><mrow id="S3.E4X.2.1.1.m1.5.5.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml"><msub id="S3.E4X.2.1.1.m1.5.5.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.2.cmml">𝑭</mi><mi id="S3.E4X.2.1.1.m1.5.5.1.1.3.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.cmml">d</mi></msub><mo id="S3.E4X.2.1.1.m1.5.5.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.3.cmml">𝙲𝚘𝚗𝚌𝚊𝚝</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.4.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.1.1" xref="S3.E4X.2.1.1.m1.1.1.cmml">𝑨</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.4" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.4.cmml">,</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.2.2" xref="S3.E4X.2.1.1.m1.2.2.cmml">𝑽</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.3.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.5" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.4.cmml">,</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.3.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.3.3" xref="S3.E4X.2.1.1.m1.3.3.cmml">𝑻</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.3.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.6" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.4.cmml">,</mo><mtext id="S3.E4X.2.1.1.m1.4.4" xref="S3.E4X.2.1.1.m1.4.4a.cmml">dim</mtext></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.4" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.4.cmml">=</mo><mn id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.5" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.5.cmml">1</mn></mrow><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4X.2.1.1.m1.5b"><apply id="S3.E4X.2.1.1.m1.5.5.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1"><eq id="S3.E4X.2.1.1.m1.5.5.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.2"></eq><apply id="S3.E4X.2.1.1.m1.5.5.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.5.5.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.5.5.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.2">𝑭</ci><ci id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3">𝑑</ci></apply><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.2"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.3">𝙲𝚘𝚗𝚌𝚊𝚝</ci><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1"><eq id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.4.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.4"></eq><list id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.4.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3"><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.2">𝙼𝙻𝙿</ci><ci id="S3.E4X.2.1.1.m1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1">𝑨</ci></apply><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.1"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.2.2.2">𝙼𝙻𝙿</ci><ci id="S3.E4X.2.1.1.m1.2.2.cmml" xref="S3.E4X.2.1.1.m1.2.2">𝑽</ci></apply><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.1"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.3.3.3.2">𝙼𝙻𝙿</ci><ci id="S3.E4X.2.1.1.m1.3.3.cmml" xref="S3.E4X.2.1.1.m1.3.3">𝑻</ci></apply><ci id="S3.E4X.2.1.1.m1.4.4a.cmml" xref="S3.E4X.2.1.1.m1.4.4"><mtext id="S3.E4X.2.1.1.m1.4.4.cmml" xref="S3.E4X.2.1.1.m1.4.4">dim</mtext></ci></list><cn type="integer" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.5.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.5">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4X.2.1.1.m1.5c">\displaystyle\bm{F}_{d}=\mathtt{Concat}(\mathtt{MLP}(\bm{A}),\mathtt{MLP}(\bm{V}),\mathtt{MLP}(\bm{T}),\text{dim}=1),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E4Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4Xa.2.1.1.m1.5" class="ltx_Math" alttext="\displaystyle\bm{F}_{s}=\mathtt{Stack}(\mathtt{MLP}(\bm{A}),\mathtt{MLP}(\bm{V}),\mathtt{MLP}(\bm{T}),\text{dim}=2)" display="inline"><semantics id="S3.E4Xa.2.1.1.m1.5a"><mrow id="S3.E4Xa.2.1.1.m1.5.5" xref="S3.E4Xa.2.1.1.m1.5.5.cmml"><msub id="S3.E4Xa.2.1.1.m1.5.5.3" xref="S3.E4Xa.2.1.1.m1.5.5.3.cmml"><mi id="S3.E4Xa.2.1.1.m1.5.5.3.2" xref="S3.E4Xa.2.1.1.m1.5.5.3.2.cmml">𝑭</mi><mi id="S3.E4Xa.2.1.1.m1.5.5.3.3" xref="S3.E4Xa.2.1.1.m1.5.5.3.3.cmml">s</mi></msub><mo id="S3.E4Xa.2.1.1.m1.5.5.2" xref="S3.E4Xa.2.1.1.m1.5.5.2.cmml">=</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.cmml"><mi id="S3.E4Xa.2.1.1.m1.5.5.1.3" xref="S3.E4Xa.2.1.1.m1.5.5.1.3.cmml">𝚂𝚝𝚊𝚌𝚔</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.5.5.1.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.cmml"><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.4.cmml"><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4Xa.2.1.1.m1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.cmml">𝑨</mi><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.4" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.4.cmml">,</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.cmml"><mi id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.1.cmml">​</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.3.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.cmml"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.3.2.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.cmml">(</mo><mi id="S3.E4Xa.2.1.1.m1.2.2" xref="S3.E4Xa.2.1.1.m1.2.2.cmml">𝑽</mi><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.3.2.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.5" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.4.cmml">,</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.cmml"><mi id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.2.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.1.cmml">​</mo><mrow id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.3.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.cmml"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.3.2.1" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.cmml">(</mo><mi id="S3.E4Xa.2.1.1.m1.3.3" xref="S3.E4Xa.2.1.1.m1.3.3.cmml">𝑻</mi><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.3.2.2" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.6" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.4.cmml">,</mo><mtext id="S3.E4Xa.2.1.1.m1.4.4" xref="S3.E4Xa.2.1.1.m1.4.4a.cmml">dim</mtext></mrow><mo id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.4" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.4.cmml">=</mo><mn id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.5" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.5.cmml">2</mn></mrow><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.3" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4Xa.2.1.1.m1.5b"><apply id="S3.E4Xa.2.1.1.m1.5.5.cmml" xref="S3.E4Xa.2.1.1.m1.5.5"><eq id="S3.E4Xa.2.1.1.m1.5.5.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.2"></eq><apply id="S3.E4Xa.2.1.1.m1.5.5.3.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E4Xa.2.1.1.m1.5.5.3.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.3">subscript</csymbol><ci id="S3.E4Xa.2.1.1.m1.5.5.3.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.3.2">𝑭</ci><ci id="S3.E4Xa.2.1.1.m1.5.5.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.3.3">𝑠</ci></apply><apply id="S3.E4Xa.2.1.1.m1.5.5.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1"><times id="S3.E4Xa.2.1.1.m1.5.5.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.2"></times><ci id="S3.E4Xa.2.1.1.m1.5.5.1.3.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.3">𝚂𝚝𝚊𝚌𝚔</ci><apply id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1"><eq id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.4.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.4"></eq><list id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.4.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3"><apply id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1"><times id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.1"></times><ci id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.1.1.1.2">𝙼𝙻𝙿</ci><ci id="S3.E4Xa.2.1.1.m1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1">𝑨</ci></apply><apply id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2"><times id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.1"></times><ci id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.2.2.2.2">𝙼𝙻𝙿</ci><ci id="S3.E4Xa.2.1.1.m1.2.2.cmml" xref="S3.E4Xa.2.1.1.m1.2.2">𝑽</ci></apply><apply id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3"><times id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.1.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.1"></times><ci id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.2.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.3.3.3.2">𝙼𝙻𝙿</ci><ci id="S3.E4Xa.2.1.1.m1.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.3.3">𝑻</ci></apply><ci id="S3.E4Xa.2.1.1.m1.4.4a.cmml" xref="S3.E4Xa.2.1.1.m1.4.4"><mtext id="S3.E4Xa.2.1.1.m1.4.4.cmml" xref="S3.E4Xa.2.1.1.m1.4.4">dim</mtext></ci></list><cn type="integer" id="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.5.cmml" xref="S3.E4Xa.2.1.1.m1.5.5.1.1.1.1.5">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4Xa.2.1.1.m1.5c">\displaystyle\bm{F}_{s}=\mathtt{Stack}(\mathtt{MLP}(\bm{A}),\mathtt{MLP}(\bm{V}),\mathtt{MLP}(\bm{T}),\text{dim}=2)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p2.6" class="ltx_p">where <math id="S3.SS3.p2.4.m1.1" class="ltx_Math" alttext="\bm{A}=\left\{\bm{A}_{hubert}\right\}" display="inline"><semantics id="S3.SS3.p2.4.m1.1a"><mrow id="S3.SS3.p2.4.m1.1.1" xref="S3.SS3.p2.4.m1.1.1.cmml"><mi id="S3.SS3.p2.4.m1.1.1.3" xref="S3.SS3.p2.4.m1.1.1.3.cmml">𝑨</mi><mo id="S3.SS3.p2.4.m1.1.1.2" xref="S3.SS3.p2.4.m1.1.1.2.cmml">=</mo><mrow id="S3.SS3.p2.4.m1.1.1.1.1" xref="S3.SS3.p2.4.m1.1.1.1.2.cmml"><mo id="S3.SS3.p2.4.m1.1.1.1.1.2" xref="S3.SS3.p2.4.m1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p2.4.m1.1.1.1.1.1" xref="S3.SS3.p2.4.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.2" xref="S3.SS3.p2.4.m1.1.1.1.1.1.2.cmml">𝑨</mi><mrow id="S3.SS3.p2.4.m1.1.1.1.1.1.3" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.2" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.3" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1a" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.4" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1b" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.5" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1c" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.6" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1d" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m1.1.1.1.1.1.3.7" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS3.p2.4.m1.1.1.1.1.3" xref="S3.SS3.p2.4.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m1.1b"><apply id="S3.SS3.p2.4.m1.1.1.cmml" xref="S3.SS3.p2.4.m1.1.1"><eq id="S3.SS3.p2.4.m1.1.1.2.cmml" xref="S3.SS3.p2.4.m1.1.1.2"></eq><ci id="S3.SS3.p2.4.m1.1.1.3.cmml" xref="S3.SS3.p2.4.m1.1.1.3">𝑨</ci><set id="S3.SS3.p2.4.m1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1"><apply id="S3.SS3.p2.4.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.2">𝑨</ci><apply id="S3.SS3.p2.4.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3"><times id="S3.SS3.p2.4.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.2">ℎ</ci><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.3">𝑢</ci><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.4.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.4">𝑏</ci><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.5.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.5">𝑒</ci><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.6.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.6">𝑟</ci><ci id="S3.SS3.p2.4.m1.1.1.1.1.1.3.7.cmml" xref="S3.SS3.p2.4.m1.1.1.1.1.1.3.7">𝑡</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m1.1c">\bm{A}=\left\{\bm{A}_{hubert}\right\}</annotation></semantics></math>, <math id="S3.SS3.p2.5.m2.4" class="ltx_Math" alttext="\bm{V}=\left\{\bm{V}_{clip},\bm{V}_{videomae},\bm{V}_{mae},\bm{V}_{manet}\right\}" display="inline"><semantics id="S3.SS3.p2.5.m2.4a"><mrow id="S3.SS3.p2.5.m2.4.4" xref="S3.SS3.p2.5.m2.4.4.cmml"><mi id="S3.SS3.p2.5.m2.4.4.6" xref="S3.SS3.p2.5.m2.4.4.6.cmml">𝑽</mi><mo id="S3.SS3.p2.5.m2.4.4.5" xref="S3.SS3.p2.5.m2.4.4.5.cmml">=</mo><mrow id="S3.SS3.p2.5.m2.4.4.4.4" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml"><mo id="S3.SS3.p2.5.m2.4.4.4.4.5" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml">{</mo><msub id="S3.SS3.p2.5.m2.1.1.1.1.1" xref="S3.SS3.p2.5.m2.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.5.m2.1.1.1.1.1.2" xref="S3.SS3.p2.5.m2.1.1.1.1.1.2.cmml">𝑽</mi><mrow id="S3.SS3.p2.5.m2.1.1.1.1.1.3" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p2.5.m2.1.1.1.1.1.3.2" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.1.1.1.1.1.3.1" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.1.1.1.1.1.3.3" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.1.1.1.1.1.3.1a" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.1.1.1.1.1.3.4" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.1.1.1.1.1.3.1b" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.1.1.1.1.1.3.5" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.5.cmml">p</mi></mrow></msub><mo id="S3.SS3.p2.5.m2.4.4.4.4.6" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS3.p2.5.m2.2.2.2.2.2" xref="S3.SS3.p2.5.m2.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.2" xref="S3.SS3.p2.5.m2.2.2.2.2.2.2.cmml">𝑽</mi><mrow id="S3.SS3.p2.5.m2.2.2.2.2.2.3" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.2" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.3" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1a" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.4" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1b" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.5" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1c" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.6" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1d" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.7" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1e" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.8" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1f" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.2.2.2.2.2.3.9" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.9.cmml">e</mi></mrow></msub><mo id="S3.SS3.p2.5.m2.4.4.4.4.7" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS3.p2.5.m2.3.3.3.3.3" xref="S3.SS3.p2.5.m2.3.3.3.3.3.cmml"><mi id="S3.SS3.p2.5.m2.3.3.3.3.3.2" xref="S3.SS3.p2.5.m2.3.3.3.3.3.2.cmml">𝑽</mi><mrow id="S3.SS3.p2.5.m2.3.3.3.3.3.3" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.cmml"><mi id="S3.SS3.p2.5.m2.3.3.3.3.3.3.2" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.3.3.3.3.3.3.1" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.3.3.3.3.3.3.3" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.3.3.3.3.3.3.1a" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.3.3.3.3.3.3.4" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.4.cmml">e</mi></mrow></msub><mo id="S3.SS3.p2.5.m2.4.4.4.4.8" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS3.p2.5.m2.4.4.4.4.4" xref="S3.SS3.p2.5.m2.4.4.4.4.4.cmml"><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.2" xref="S3.SS3.p2.5.m2.4.4.4.4.4.2.cmml">𝑽</mi><mrow id="S3.SS3.p2.5.m2.4.4.4.4.4.3" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.cmml"><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.3.2" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.4.4.4.4.4.3.1" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.3.3" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.4.4.4.4.4.3.1a" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.3.4" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.4.4.4.4.4.3.1b" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.3.5" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.5.m2.4.4.4.4.4.3.1c" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.1.cmml">​</mo><mi id="S3.SS3.p2.5.m2.4.4.4.4.4.3.6" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.6.cmml">t</mi></mrow></msub><mo id="S3.SS3.p2.5.m2.4.4.4.4.9" xref="S3.SS3.p2.5.m2.4.4.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m2.4b"><apply id="S3.SS3.p2.5.m2.4.4.cmml" xref="S3.SS3.p2.5.m2.4.4"><eq id="S3.SS3.p2.5.m2.4.4.5.cmml" xref="S3.SS3.p2.5.m2.4.4.5"></eq><ci id="S3.SS3.p2.5.m2.4.4.6.cmml" xref="S3.SS3.p2.5.m2.4.4.6">𝑽</ci><set id="S3.SS3.p2.5.m2.4.4.4.5.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4"><apply id="S3.SS3.p2.5.m2.1.1.1.1.1.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.2">𝑽</ci><apply id="S3.SS3.p2.5.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3"><times id="S3.SS3.p2.5.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p2.5.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.SS3.p2.5.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.3">𝑙</ci><ci id="S3.SS3.p2.5.m2.1.1.1.1.1.3.4.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.4">𝑖</ci><ci id="S3.SS3.p2.5.m2.1.1.1.1.1.3.5.cmml" xref="S3.SS3.p2.5.m2.1.1.1.1.1.3.5">𝑝</ci></apply></apply><apply id="S3.SS3.p2.5.m2.2.2.2.2.2.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.2">𝑽</ci><apply id="S3.SS3.p2.5.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3"><times id="S3.SS3.p2.5.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.1"></times><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.2">𝑣</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.3">𝑖</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.4.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.4">𝑑</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.5.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.5">𝑒</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.6.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.6">𝑜</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.7.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.7">𝑚</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.8.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.8">𝑎</ci><ci id="S3.SS3.p2.5.m2.2.2.2.2.2.3.9.cmml" xref="S3.SS3.p2.5.m2.2.2.2.2.2.3.9">𝑒</ci></apply></apply><apply id="S3.SS3.p2.5.m2.3.3.3.3.3.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m2.3.3.3.3.3.1.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.5.m2.3.3.3.3.3.2.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.2">𝑽</ci><apply id="S3.SS3.p2.5.m2.3.3.3.3.3.3.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3"><times id="S3.SS3.p2.5.m2.3.3.3.3.3.3.1.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.1"></times><ci id="S3.SS3.p2.5.m2.3.3.3.3.3.3.2.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.2">𝑚</ci><ci id="S3.SS3.p2.5.m2.3.3.3.3.3.3.3.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.3">𝑎</ci><ci id="S3.SS3.p2.5.m2.3.3.3.3.3.3.4.cmml" xref="S3.SS3.p2.5.m2.3.3.3.3.3.3.4">𝑒</ci></apply></apply><apply id="S3.SS3.p2.5.m2.4.4.4.4.4.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m2.4.4.4.4.4.1.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4">subscript</csymbol><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.2.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.2">𝑽</ci><apply id="S3.SS3.p2.5.m2.4.4.4.4.4.3.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3"><times id="S3.SS3.p2.5.m2.4.4.4.4.4.3.1.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.1"></times><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.3.2.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.2">𝑚</ci><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.3.3.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.3">𝑎</ci><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.3.4.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.4">𝑛</ci><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.3.5.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.5">𝑒</ci><ci id="S3.SS3.p2.5.m2.4.4.4.4.4.3.6.cmml" xref="S3.SS3.p2.5.m2.4.4.4.4.4.3.6">𝑡</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m2.4c">\bm{V}=\left\{\bm{V}_{clip},\bm{V}_{videomae},\bm{V}_{mae},\bm{V}_{manet}\right\}</annotation></semantics></math>, and <math id="S3.SS3.p2.6.m3.2" class="ltx_Math" alttext="\bm{T}=\left\{\bm{T}_{qwen},\bm{T}_{baichuan}\right\}" display="inline"><semantics id="S3.SS3.p2.6.m3.2a"><mrow id="S3.SS3.p2.6.m3.2.2" xref="S3.SS3.p2.6.m3.2.2.cmml"><mi id="S3.SS3.p2.6.m3.2.2.4" xref="S3.SS3.p2.6.m3.2.2.4.cmml">𝑻</mi><mo id="S3.SS3.p2.6.m3.2.2.3" xref="S3.SS3.p2.6.m3.2.2.3.cmml">=</mo><mrow id="S3.SS3.p2.6.m3.2.2.2.2" xref="S3.SS3.p2.6.m3.2.2.2.3.cmml"><mo id="S3.SS3.p2.6.m3.2.2.2.2.3" xref="S3.SS3.p2.6.m3.2.2.2.3.cmml">{</mo><msub id="S3.SS3.p2.6.m3.1.1.1.1.1" xref="S3.SS3.p2.6.m3.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.6.m3.1.1.1.1.1.2" xref="S3.SS3.p2.6.m3.1.1.1.1.1.2.cmml">𝑻</mi><mrow id="S3.SS3.p2.6.m3.1.1.1.1.1.3" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p2.6.m3.1.1.1.1.1.3.2" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.1.1.1.1.1.3.1" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.1.1.1.1.1.3.3" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.3.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.1.1.1.1.1.3.1a" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.1.1.1.1.1.3.4" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.1.1.1.1.1.3.1b" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.1.1.1.1.1.3.5" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.5.cmml">n</mi></mrow></msub><mo id="S3.SS3.p2.6.m3.2.2.2.2.4" xref="S3.SS3.p2.6.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.6.m3.2.2.2.2.2" xref="S3.SS3.p2.6.m3.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.2" xref="S3.SS3.p2.6.m3.2.2.2.2.2.2.cmml">𝑻</mi><mrow id="S3.SS3.p2.6.m3.2.2.2.2.2.3" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.cmml"><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.2" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.3" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1a" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.4" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1b" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.5" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1c" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.6" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1d" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.7" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1e" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.8" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1f" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.6.m3.2.2.2.2.2.3.9" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.9.cmml">n</mi></mrow></msub><mo id="S3.SS3.p2.6.m3.2.2.2.2.5" xref="S3.SS3.p2.6.m3.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m3.2b"><apply id="S3.SS3.p2.6.m3.2.2.cmml" xref="S3.SS3.p2.6.m3.2.2"><eq id="S3.SS3.p2.6.m3.2.2.3.cmml" xref="S3.SS3.p2.6.m3.2.2.3"></eq><ci id="S3.SS3.p2.6.m3.2.2.4.cmml" xref="S3.SS3.p2.6.m3.2.2.4">𝑻</ci><set id="S3.SS3.p2.6.m3.2.2.2.3.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2"><apply id="S3.SS3.p2.6.m3.1.1.1.1.1.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.2">𝑻</ci><apply id="S3.SS3.p2.6.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3"><times id="S3.SS3.p2.6.m3.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p2.6.m3.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.2">𝑞</ci><ci id="S3.SS3.p2.6.m3.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.3">𝑤</ci><ci id="S3.SS3.p2.6.m3.1.1.1.1.1.3.4.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.4">𝑒</ci><ci id="S3.SS3.p2.6.m3.1.1.1.1.1.3.5.cmml" xref="S3.SS3.p2.6.m3.1.1.1.1.1.3.5">𝑛</ci></apply></apply><apply id="S3.SS3.p2.6.m3.2.2.2.2.2.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.2">𝑻</ci><apply id="S3.SS3.p2.6.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3"><times id="S3.SS3.p2.6.m3.2.2.2.2.2.3.1.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.1"></times><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.2.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.2">𝑏</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.3.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.4.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.4">𝑖</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.5.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.5">𝑐</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.6.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.6">ℎ</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.7.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.7">𝑢</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.8.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.8">𝑎</ci><ci id="S3.SS3.p2.6.m3.2.2.2.2.2.3.9.cmml" xref="S3.SS3.p2.6.m3.2.2.2.2.2.3.9">𝑛</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m3.2c">\bm{T}=\left\{\bm{T}_{qwen},\bm{T}_{baichuan}\right\}</annotation></semantics></math> refer to audio, visual, and text features, respectively.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.6" class="ltx_p">In the attention branch, we applied <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathtt{Attn\_MLP}(\cdot)" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.2" xref="S3.SS3.p3.1.m1.1.2.cmml"><mi id="S3.SS3.p3.1.m1.1.2.2" xref="S3.SS3.p3.1.m1.1.2.2.cmml">𝙰𝚝𝚝𝚗</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.2.1" xref="S3.SS3.p3.1.m1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS3.p3.1.m1.1.2.3" xref="S3.SS3.p3.1.m1.1.2.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.2.1a" xref="S3.SS3.p3.1.m1.1.2.1.cmml">​</mo><mi id="S3.SS3.p3.1.m1.1.2.4" xref="S3.SS3.p3.1.m1.1.2.4.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.2.1b" xref="S3.SS3.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS3.p3.1.m1.1.2.5.2" xref="S3.SS3.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p3.1.m1.1.2.5.2.1" xref="S3.SS3.p3.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p3.1.m1.1.2.5.2.2" xref="S3.SS3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.2"><times id="S3.SS3.p3.1.m1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.2.1"></times><ci id="S3.SS3.p3.1.m1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.2.2">𝙰𝚝𝚝𝚗</ci><ci id="S3.SS3.p3.1.m1.1.2.3.cmml" xref="S3.SS3.p3.1.m1.1.2.3">_</ci><ci id="S3.SS3.p3.1.m1.1.2.4.cmml" xref="S3.SS3.p3.1.m1.1.2.4">𝙼𝙻𝙿</ci><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathtt{Attn\_MLP}(\cdot)</annotation></semantics></math> to <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\bm{F}_{d}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑭</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\bm{F}_{d}</annotation></semantics></math>, downsampling its embedding depth to align with the sequence length of <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\bm{F}_{s}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑭</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\bm{F}_{s}</annotation></semantics></math>. We then performed a matrix product between the downsampled <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="\bm{F}_{d}" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><msub id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">𝑭</ci><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\bm{F}_{d}</annotation></semantics></math> and <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="\bm{F}_{s}" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><msub id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">𝑭</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">𝑭</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">\bm{F}_{s}</annotation></semantics></math> to obtain the attention-based fusion feature <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\bm{F}_{attn}" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><msub id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">𝑭</mi><mrow id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml"><mi id="S3.SS3.p3.6.m6.1.1.3.2" xref="S3.SS3.p3.6.m6.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.6.m6.1.1.3.1" xref="S3.SS3.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.6.m6.1.1.3.3" xref="S3.SS3.p3.6.m6.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.6.m6.1.1.3.1a" xref="S3.SS3.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.6.m6.1.1.3.4" xref="S3.SS3.p3.6.m6.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.6.m6.1.1.3.1b" xref="S3.SS3.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.6.m6.1.1.3.5" xref="S3.SS3.p3.6.m6.1.1.3.5.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">𝑭</ci><apply id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3"><times id="S3.SS3.p3.6.m6.1.1.3.1.cmml" xref="S3.SS3.p3.6.m6.1.1.3.1"></times><ci id="S3.SS3.p3.6.m6.1.1.3.2.cmml" xref="S3.SS3.p3.6.m6.1.1.3.2">𝑎</ci><ci id="S3.SS3.p3.6.m6.1.1.3.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3.3">𝑡</ci><ci id="S3.SS3.p3.6.m6.1.1.3.4.cmml" xref="S3.SS3.p3.6.m6.1.1.3.4">𝑡</ci><ci id="S3.SS3.p3.6.m6.1.1.3.5.cmml" xref="S3.SS3.p3.6.m6.1.1.3.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\bm{F}_{attn}</annotation></semantics></math>, which enhances the model’s ability to identify emotionally salient components across different modalities:</p>
<table id="S3.E5" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E5X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5X.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\bm{F}_{attn}=\mathtt{Unsqueeze}(\mathtt{Attn\_MLP}(\bm{F}_{d}),\text{dim}=-1)\times\bm{F}_{s}" display="inline"><semantics id="S3.E5X.2.1.1.m1.2a"><mrow id="S3.E5X.2.1.1.m1.2.2" xref="S3.E5X.2.1.1.m1.2.2.cmml"><msub id="S3.E5X.2.1.1.m1.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.3.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.3.2" xref="S3.E5X.2.1.1.m1.2.2.3.2.cmml">𝑭</mi><mrow id="S3.E5X.2.1.1.m1.2.2.3.3" xref="S3.E5X.2.1.1.m1.2.2.3.3.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.3.3.2" xref="S3.E5X.2.1.1.m1.2.2.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.3.3.1" xref="S3.E5X.2.1.1.m1.2.2.3.3.1.cmml">​</mo><mi id="S3.E5X.2.1.1.m1.2.2.3.3.3" xref="S3.E5X.2.1.1.m1.2.2.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.3.3.1a" xref="S3.E5X.2.1.1.m1.2.2.3.3.1.cmml">​</mo><mi id="S3.E5X.2.1.1.m1.2.2.3.3.4" xref="S3.E5X.2.1.1.m1.2.2.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.3.3.1b" xref="S3.E5X.2.1.1.m1.2.2.3.3.1.cmml">​</mo><mi id="S3.E5X.2.1.1.m1.2.2.3.3.5" xref="S3.E5X.2.1.1.m1.2.2.3.3.5.cmml">n</mi></mrow></msub><mo id="S3.E5X.2.1.1.m1.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.2.2.1" xref="S3.E5X.2.1.1.m1.2.2.1.cmml"><mrow id="S3.E5X.2.1.1.m1.2.2.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.3.cmml">𝚄𝚗𝚜𝚚𝚞𝚎𝚎𝚣𝚎</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">𝙰𝚝𝚝𝚗</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi mathvariant="normal" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2a" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.5" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.5.cmml">𝙼𝙻𝙿</mi><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2b" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝑭</mi><mi id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">d</mi></msub><mo stretchy="false" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">,</mo><mtext id="S3.E5X.2.1.1.m1.1.1" xref="S3.E5X.2.1.1.m1.1.1a.cmml">dim</mtext></mrow><mo id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.cmml"><mo id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3a" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.cmml">−</mo><mn id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.2" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.2.cmml">1</mn></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E5X.2.1.1.m1.2.2.1.2" xref="S3.E5X.2.1.1.m1.2.2.1.2.cmml">×</mo><msub id="S3.E5X.2.1.1.m1.2.2.1.3" xref="S3.E5X.2.1.1.m1.2.2.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.1.3.2" xref="S3.E5X.2.1.1.m1.2.2.1.3.2.cmml">𝑭</mi><mi id="S3.E5X.2.1.1.m1.2.2.1.3.3" xref="S3.E5X.2.1.1.m1.2.2.1.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5X.2.1.1.m1.2b"><apply id="S3.E5X.2.1.1.m1.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2"><eq id="S3.E5X.2.1.1.m1.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2"></eq><apply id="S3.E5X.2.1.1.m1.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.3.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.2.2.3.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.2">𝑭</ci><apply id="S3.E5X.2.1.1.m1.2.2.3.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3"><times id="S3.E5X.2.1.1.m1.2.2.3.3.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3.1"></times><ci id="S3.E5X.2.1.1.m1.2.2.3.3.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3.2">𝑎</ci><ci id="S3.E5X.2.1.1.m1.2.2.3.3.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3.3">𝑡</ci><ci id="S3.E5X.2.1.1.m1.2.2.3.3.4.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3.4">𝑡</ci><ci id="S3.E5X.2.1.1.m1.2.2.3.3.5.cmml" xref="S3.E5X.2.1.1.m1.2.2.3.3.5">𝑛</ci></apply></apply><apply id="S3.E5X.2.1.1.m1.2.2.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1"><times id="S3.E5X.2.1.1.m1.2.2.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.2"></times><apply id="S3.E5X.2.1.1.m1.2.2.1.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1"><times id="S3.E5X.2.1.1.m1.2.2.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.2"></times><ci id="S3.E5X.2.1.1.m1.2.2.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.3">𝚄𝚗𝚜𝚚𝚞𝚎𝚎𝚣𝚎</ci><apply id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1"><eq id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.2"></eq><list id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1"><apply id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1"><times id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.3">𝙰𝚝𝚝𝚗</ci><ci id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.4">_</ci><ci id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.5">𝙼𝙻𝙿</ci><apply id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑭</ci><ci id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">𝑑</ci></apply></apply><ci id="S3.E5X.2.1.1.m1.1.1a.cmml" xref="S3.E5X.2.1.1.m1.1.1"><mtext id="S3.E5X.2.1.1.m1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1">dim</mtext></ci></list><apply id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3"><minus id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3"></minus><cn type="integer" id="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.1.1.1.1.3.2">1</cn></apply></apply></apply><apply id="S3.E5X.2.1.1.m1.2.2.1.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.1.3.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.2.2.1.3.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.3.2">𝑭</ci><ci id="S3.E5X.2.1.1.m1.2.2.1.3.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.1.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5X.2.1.1.m1.2c">\displaystyle\bm{F}_{attn}=\mathtt{Unsqueeze}(\mathtt{Attn\_MLP}(\bm{F}_{d}),\text{dim}=-1)\times\bm{F}_{s}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p3.8" class="ltx_p">where <math id="S3.SS3.p3.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p3.7.m1.1a"><mo id="S3.SS3.p3.7.m1.1.1" xref="S3.SS3.p3.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m1.1b"><times id="S3.SS3.p3.7.m1.1.1.cmml" xref="S3.SS3.p3.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m1.1c">\times</annotation></semantics></math> denotes the matrix product, and <math id="S3.SS3.p3.8.m2.1" class="ltx_Math" alttext="\mathtt{Unsqueeze}(\cdot)" display="inline"><semantics id="S3.SS3.p3.8.m2.1a"><mrow id="S3.SS3.p3.8.m2.1.2" xref="S3.SS3.p3.8.m2.1.2.cmml"><mi id="S3.SS3.p3.8.m2.1.2.2" xref="S3.SS3.p3.8.m2.1.2.2.cmml">𝚄𝚗𝚜𝚚𝚞𝚎𝚎𝚣𝚎</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.8.m2.1.2.1" xref="S3.SS3.p3.8.m2.1.2.1.cmml">​</mo><mrow id="S3.SS3.p3.8.m2.1.2.3.2" xref="S3.SS3.p3.8.m2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p3.8.m2.1.2.3.2.1" xref="S3.SS3.p3.8.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p3.8.m2.1.1" xref="S3.SS3.p3.8.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p3.8.m2.1.2.3.2.2" xref="S3.SS3.p3.8.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m2.1b"><apply id="S3.SS3.p3.8.m2.1.2.cmml" xref="S3.SS3.p3.8.m2.1.2"><times id="S3.SS3.p3.8.m2.1.2.1.cmml" xref="S3.SS3.p3.8.m2.1.2.1"></times><ci id="S3.SS3.p3.8.m2.1.2.2.cmml" xref="S3.SS3.p3.8.m2.1.2.2">𝚄𝚗𝚜𝚚𝚞𝚎𝚎𝚣𝚎</ci><ci id="S3.SS3.p3.8.m2.1.1.cmml" xref="S3.SS3.p3.8.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m2.1c">\mathtt{Unsqueeze}(\cdot)</annotation></semantics></math> denotes the unsqueeze operation.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.5" class="ltx_p">In the convolutional branch, we designed a lightweight convolution block consisting of a convolution layer <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\mathtt{Conv1d}(\cdot)" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><mi id="S3.SS3.p4.1.m1.1.2.2" xref="S3.SS3.p4.1.m1.1.2.2.cmml">𝙲𝚘𝚗𝚟𝟷𝚍</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.2.1" xref="S3.SS3.p4.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS3.p4.1.m1.1.2.3.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.1.2.3.2.1" xref="S3.SS3.p4.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p4.1.m1.1.2.3.2.2" xref="S3.SS3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.2"><times id="S3.SS3.p4.1.m1.1.2.1.cmml" xref="S3.SS3.p4.1.m1.1.2.1"></times><ci id="S3.SS3.p4.1.m1.1.2.2.cmml" xref="S3.SS3.p4.1.m1.1.2.2">𝙲𝚘𝚗𝚟𝟷𝚍</ci><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\mathtt{Conv1d}(\cdot)</annotation></semantics></math>, batch normalization <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="\mathtt{BN}(\cdot)" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.2" xref="S3.SS3.p4.2.m2.1.2.cmml"><mi id="S3.SS3.p4.2.m2.1.2.2" xref="S3.SS3.p4.2.m2.1.2.2.cmml">𝙱𝙽</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.2.m2.1.2.1" xref="S3.SS3.p4.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS3.p4.2.m2.1.2.3.2" xref="S3.SS3.p4.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.2.m2.1.2.3.2.1" xref="S3.SS3.p4.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p4.2.m2.1.2.3.2.2" xref="S3.SS3.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.2.cmml" xref="S3.SS3.p4.2.m2.1.2"><times id="S3.SS3.p4.2.m2.1.2.1.cmml" xref="S3.SS3.p4.2.m2.1.2.1"></times><ci id="S3.SS3.p4.2.m2.1.2.2.cmml" xref="S3.SS3.p4.2.m2.1.2.2">𝙱𝙽</ci><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\mathtt{BN}(\cdot)</annotation></semantics></math>, and an activation function <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="\mathtt{Swish}(\cdot)" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mrow id="S3.SS3.p4.3.m3.1.2" xref="S3.SS3.p4.3.m3.1.2.cmml"><mi id="S3.SS3.p4.3.m3.1.2.2" xref="S3.SS3.p4.3.m3.1.2.2.cmml">𝚂𝚠𝚒𝚜𝚑</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m3.1.2.1" xref="S3.SS3.p4.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p4.3.m3.1.2.3.2" xref="S3.SS3.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.3.m3.1.2.3.2.1" xref="S3.SS3.p4.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p4.3.m3.1.2.3.2.2" xref="S3.SS3.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.2.cmml" xref="S3.SS3.p4.3.m3.1.2"><times id="S3.SS3.p4.3.m3.1.2.1.cmml" xref="S3.SS3.p4.3.m3.1.2.1"></times><ci id="S3.SS3.p4.3.m3.1.2.2.cmml" xref="S3.SS3.p4.3.m3.1.2.2">𝚂𝚠𝚒𝚜𝚑</ci><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\mathtt{Swish}(\cdot)</annotation></semantics></math>. The convolutional branch includes <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="\bm{N}" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">𝑵</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">𝑵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">\bm{N}</annotation></semantics></math> convolution blocks and an adaptive average pooling layer <math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="\mathtt{Pool}(\cdot)" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mrow id="S3.SS3.p4.5.m5.1.2" xref="S3.SS3.p4.5.m5.1.2.cmml"><mi id="S3.SS3.p4.5.m5.1.2.2" xref="S3.SS3.p4.5.m5.1.2.2.cmml">𝙿𝚘𝚘𝚕</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.5.m5.1.2.1" xref="S3.SS3.p4.5.m5.1.2.1.cmml">​</mo><mrow id="S3.SS3.p4.5.m5.1.2.3.2" xref="S3.SS3.p4.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.5.m5.1.2.3.2.1" xref="S3.SS3.p4.5.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p4.5.m5.1.2.3.2.2" xref="S3.SS3.p4.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><apply id="S3.SS3.p4.5.m5.1.2.cmml" xref="S3.SS3.p4.5.m5.1.2"><times id="S3.SS3.p4.5.m5.1.2.1.cmml" xref="S3.SS3.p4.5.m5.1.2.1"></times><ci id="S3.SS3.p4.5.m5.1.2.2.cmml" xref="S3.SS3.p4.5.m5.1.2.2">𝙿𝚘𝚘𝚕</ci><ci id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">\mathtt{Pool}(\cdot)</annotation></semantics></math>, which reshapes the ultimate fusion features. The convolutional operators’ inductive bias and limited receptive fields encourage the model to focus on fine-grained details, enhancing robustness, particularly when trained on limited-scale datasets:</p>
<table id="S3.E6" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E6X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\bm{F}_{conv}^{k}=\mathtt{Swish}(\mathtt{BN}(\mathtt{Conv1d}(\bm{F}_{conv}^{k-1})))," display="inline"><semantics id="S3.E6X.2.1.1.m1.1a"><mrow id="S3.E6X.2.1.1.m1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S3.E6X.2.1.1.m1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml"><msubsup id="S3.E6X.2.1.1.m1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.2.cmml">𝑭</mi><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1a" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.4" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1b" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.5" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.5.cmml">v</mi></mrow><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.cmml">k</mi></msubsup><mo id="S3.E6X.2.1.1.m1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.3.cmml">𝚂𝚠𝚒𝚜𝚑</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml">𝙱𝙽</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">𝙲𝚘𝚗𝚟𝟷𝚍</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝑭</mi><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1a" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1b" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml">v</mi></mrow><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">k</mi><mo id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6X.2.1.1.m1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6X.2.1.1.m1.1b"><apply id="S3.E6X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1"><eq id="S3.E6X.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.2"></eq><apply id="S3.E6X.2.1.1.m1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.2">𝑭</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3"><times id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.2">𝑐</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.3">𝑜</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.4.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.4">𝑛</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.5.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.3.5">𝑣</ci></apply></apply><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3">𝑘</ci></apply><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.3">𝚂𝚠𝚒𝚜𝚑</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.3">𝙱𝙽</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝙲𝚘𝚗𝚟𝟷𝚍</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑭</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑐</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑜</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4">𝑛</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5">𝑣</ci></apply></apply><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6X.2.1.1.m1.1c">\displaystyle\bm{F}_{conv}^{k}=\mathtt{Swish}(\mathtt{BN}(\mathtt{Conv1d}(\bm{F}_{conv}^{k-1}))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E6Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\bm{F}_{conv}=\mathtt{Pool}(\bm{F}_{conv}^{N})" display="inline"><semantics id="S3.E6Xa.2.1.1.m1.1a"><mrow id="S3.E6Xa.2.1.1.m1.1.1" xref="S3.E6Xa.2.1.1.m1.1.1.cmml"><msub id="S3.E6Xa.2.1.1.m1.1.1.3" xref="S3.E6Xa.2.1.1.m1.1.1.3.cmml"><mi id="S3.E6Xa.2.1.1.m1.1.1.3.2" xref="S3.E6Xa.2.1.1.m1.1.1.3.2.cmml">𝑭</mi><mrow id="S3.E6Xa.2.1.1.m1.1.1.3.3" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.cmml"><mi id="S3.E6Xa.2.1.1.m1.1.1.3.3.2" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.3.3.1" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.3.3.3" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.3.3.1a" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.3.3.4" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.3.3.1b" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.3.3.5" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.5.cmml">v</mi></mrow></msub><mo id="S3.E6Xa.2.1.1.m1.1.1.2" xref="S3.E6Xa.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E6Xa.2.1.1.m1.1.1.1" xref="S3.E6Xa.2.1.1.m1.1.1.1.cmml"><mi id="S3.E6Xa.2.1.1.m1.1.1.1.3" xref="S3.E6Xa.2.1.1.m1.1.1.1.3.cmml">𝙿𝚘𝚘𝚕</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.1.2" xref="S3.E6Xa.2.1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E6Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.2" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">𝑭</mi><mrow id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.4" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.5" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.5.cmml">v</mi></mrow><mi id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml">N</mi></msubsup><mo stretchy="false" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.3" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6Xa.2.1.1.m1.1b"><apply id="S3.E6Xa.2.1.1.m1.1.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1"><eq id="S3.E6Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.2"></eq><apply id="S3.E6Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E6Xa.2.1.1.m1.1.1.3.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.E6Xa.2.1.1.m1.1.1.3.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.2">𝑭</ci><apply id="S3.E6Xa.2.1.1.m1.1.1.3.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3"><times id="S3.E6Xa.2.1.1.m1.1.1.3.3.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.1"></times><ci id="S3.E6Xa.2.1.1.m1.1.1.3.3.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.2">𝑐</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.3.3.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.3">𝑜</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.3.3.4.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.4">𝑛</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.3.3.5.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.3.3.5">𝑣</ci></apply></apply><apply id="S3.E6Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1"><times id="S3.E6Xa.2.1.1.m1.1.1.1.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.2"></times><ci id="S3.E6Xa.2.1.1.m1.1.1.1.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.3">𝙿𝚘𝚘𝚕</ci><apply id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.2">𝑭</ci><apply id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3"><times id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2">𝑐</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3">𝑜</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.4">𝑛</ci><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.2.3.5">𝑣</ci></apply></apply><ci id="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6Xa.2.1.1.m1.1.1.1.1.1.1.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6Xa.2.1.1.m1.1c">\displaystyle\bm{F}_{conv}=\mathtt{Pool}(\bm{F}_{conv}^{N})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p4.8" class="ltx_p">where <math id="S3.SS3.p4.6.m1.5" class="ltx_Math" alttext="k(k=1,2,...,N)" display="inline"><semantics id="S3.SS3.p4.6.m1.5a"><mrow id="S3.SS3.p4.6.m1.5.5" xref="S3.SS3.p4.6.m1.5.5.cmml"><mi id="S3.SS3.p4.6.m1.5.5.3" xref="S3.SS3.p4.6.m1.5.5.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.6.m1.5.5.2" xref="S3.SS3.p4.6.m1.5.5.2.cmml">​</mo><mrow id="S3.SS3.p4.6.m1.5.5.1.1" xref="S3.SS3.p4.6.m1.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.6.m1.5.5.1.1.2" xref="S3.SS3.p4.6.m1.5.5.1.1.1.cmml">(</mo><mrow id="S3.SS3.p4.6.m1.5.5.1.1.1" xref="S3.SS3.p4.6.m1.5.5.1.1.1.cmml"><mi id="S3.SS3.p4.6.m1.5.5.1.1.1.2" xref="S3.SS3.p4.6.m1.5.5.1.1.1.2.cmml">k</mi><mo id="S3.SS3.p4.6.m1.5.5.1.1.1.1" xref="S3.SS3.p4.6.m1.5.5.1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p4.6.m1.5.5.1.1.1.3.2" xref="S3.SS3.p4.6.m1.5.5.1.1.1.3.1.cmml"><mn id="S3.SS3.p4.6.m1.1.1" xref="S3.SS3.p4.6.m1.1.1.cmml">1</mn><mo id="S3.SS3.p4.6.m1.5.5.1.1.1.3.2.1" xref="S3.SS3.p4.6.m1.5.5.1.1.1.3.1.cmml">,</mo><mn id="S3.SS3.p4.6.m1.2.2" xref="S3.SS3.p4.6.m1.2.2.cmml">2</mn><mo id="S3.SS3.p4.6.m1.5.5.1.1.1.3.2.2" xref="S3.SS3.p4.6.m1.5.5.1.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p4.6.m1.3.3" xref="S3.SS3.p4.6.m1.3.3.cmml">…</mi><mo id="S3.SS3.p4.6.m1.5.5.1.1.1.3.2.3" xref="S3.SS3.p4.6.m1.5.5.1.1.1.3.1.cmml">,</mo><mi id="S3.SS3.p4.6.m1.4.4" xref="S3.SS3.p4.6.m1.4.4.cmml">N</mi></mrow></mrow><mo stretchy="false" id="S3.SS3.p4.6.m1.5.5.1.1.3" xref="S3.SS3.p4.6.m1.5.5.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m1.5b"><apply id="S3.SS3.p4.6.m1.5.5.cmml" xref="S3.SS3.p4.6.m1.5.5"><times id="S3.SS3.p4.6.m1.5.5.2.cmml" xref="S3.SS3.p4.6.m1.5.5.2"></times><ci id="S3.SS3.p4.6.m1.5.5.3.cmml" xref="S3.SS3.p4.6.m1.5.5.3">𝑘</ci><apply id="S3.SS3.p4.6.m1.5.5.1.1.1.cmml" xref="S3.SS3.p4.6.m1.5.5.1.1"><eq id="S3.SS3.p4.6.m1.5.5.1.1.1.1.cmml" xref="S3.SS3.p4.6.m1.5.5.1.1.1.1"></eq><ci id="S3.SS3.p4.6.m1.5.5.1.1.1.2.cmml" xref="S3.SS3.p4.6.m1.5.5.1.1.1.2">𝑘</ci><list id="S3.SS3.p4.6.m1.5.5.1.1.1.3.1.cmml" xref="S3.SS3.p4.6.m1.5.5.1.1.1.3.2"><cn type="integer" id="S3.SS3.p4.6.m1.1.1.cmml" xref="S3.SS3.p4.6.m1.1.1">1</cn><cn type="integer" id="S3.SS3.p4.6.m1.2.2.cmml" xref="S3.SS3.p4.6.m1.2.2">2</cn><ci id="S3.SS3.p4.6.m1.3.3.cmml" xref="S3.SS3.p4.6.m1.3.3">…</ci><ci id="S3.SS3.p4.6.m1.4.4.cmml" xref="S3.SS3.p4.6.m1.4.4">𝑁</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m1.5c">k(k=1,2,...,N)</annotation></semantics></math> refers to the index of the convolution block, and <math id="S3.SS3.p4.7.m2.1" class="ltx_Math" alttext="\bm{F}_{conv}" display="inline"><semantics id="S3.SS3.p4.7.m2.1a"><msub id="S3.SS3.p4.7.m2.1.1" xref="S3.SS3.p4.7.m2.1.1.cmml"><mi id="S3.SS3.p4.7.m2.1.1.2" xref="S3.SS3.p4.7.m2.1.1.2.cmml">𝑭</mi><mrow id="S3.SS3.p4.7.m2.1.1.3" xref="S3.SS3.p4.7.m2.1.1.3.cmml"><mi id="S3.SS3.p4.7.m2.1.1.3.2" xref="S3.SS3.p4.7.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m2.1.1.3.1" xref="S3.SS3.p4.7.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.7.m2.1.1.3.3" xref="S3.SS3.p4.7.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m2.1.1.3.1a" xref="S3.SS3.p4.7.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.7.m2.1.1.3.4" xref="S3.SS3.p4.7.m2.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m2.1.1.3.1b" xref="S3.SS3.p4.7.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.7.m2.1.1.3.5" xref="S3.SS3.p4.7.m2.1.1.3.5.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m2.1b"><apply id="S3.SS3.p4.7.m2.1.1.cmml" xref="S3.SS3.p4.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m2.1.1.1.cmml" xref="S3.SS3.p4.7.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m2.1.1.2.cmml" xref="S3.SS3.p4.7.m2.1.1.2">𝑭</ci><apply id="S3.SS3.p4.7.m2.1.1.3.cmml" xref="S3.SS3.p4.7.m2.1.1.3"><times id="S3.SS3.p4.7.m2.1.1.3.1.cmml" xref="S3.SS3.p4.7.m2.1.1.3.1"></times><ci id="S3.SS3.p4.7.m2.1.1.3.2.cmml" xref="S3.SS3.p4.7.m2.1.1.3.2">𝑐</ci><ci id="S3.SS3.p4.7.m2.1.1.3.3.cmml" xref="S3.SS3.p4.7.m2.1.1.3.3">𝑜</ci><ci id="S3.SS3.p4.7.m2.1.1.3.4.cmml" xref="S3.SS3.p4.7.m2.1.1.3.4">𝑛</ci><ci id="S3.SS3.p4.7.m2.1.1.3.5.cmml" xref="S3.SS3.p4.7.m2.1.1.3.5">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m2.1c">\bm{F}_{conv}</annotation></semantics></math> indicates the final convolution-based fusion feature. Note that <math id="S3.SS3.p4.8.m3.1" class="ltx_Math" alttext="\bm{F}_{conv}^{0}=\bm{F}_{s}" display="inline"><semantics id="S3.SS3.p4.8.m3.1a"><mrow id="S3.SS3.p4.8.m3.1.1" xref="S3.SS3.p4.8.m3.1.1.cmml"><msubsup id="S3.SS3.p4.8.m3.1.1.2" xref="S3.SS3.p4.8.m3.1.1.2.cmml"><mi id="S3.SS3.p4.8.m3.1.1.2.2.2" xref="S3.SS3.p4.8.m3.1.1.2.2.2.cmml">𝑭</mi><mrow id="S3.SS3.p4.8.m3.1.1.2.2.3" xref="S3.SS3.p4.8.m3.1.1.2.2.3.cmml"><mi id="S3.SS3.p4.8.m3.1.1.2.2.3.2" xref="S3.SS3.p4.8.m3.1.1.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.8.m3.1.1.2.2.3.1" xref="S3.SS3.p4.8.m3.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p4.8.m3.1.1.2.2.3.3" xref="S3.SS3.p4.8.m3.1.1.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.8.m3.1.1.2.2.3.1a" xref="S3.SS3.p4.8.m3.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p4.8.m3.1.1.2.2.3.4" xref="S3.SS3.p4.8.m3.1.1.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.8.m3.1.1.2.2.3.1b" xref="S3.SS3.p4.8.m3.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p4.8.m3.1.1.2.2.3.5" xref="S3.SS3.p4.8.m3.1.1.2.2.3.5.cmml">v</mi></mrow><mn id="S3.SS3.p4.8.m3.1.1.2.3" xref="S3.SS3.p4.8.m3.1.1.2.3.cmml">0</mn></msubsup><mo id="S3.SS3.p4.8.m3.1.1.1" xref="S3.SS3.p4.8.m3.1.1.1.cmml">=</mo><msub id="S3.SS3.p4.8.m3.1.1.3" xref="S3.SS3.p4.8.m3.1.1.3.cmml"><mi id="S3.SS3.p4.8.m3.1.1.3.2" xref="S3.SS3.p4.8.m3.1.1.3.2.cmml">𝑭</mi><mi id="S3.SS3.p4.8.m3.1.1.3.3" xref="S3.SS3.p4.8.m3.1.1.3.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m3.1b"><apply id="S3.SS3.p4.8.m3.1.1.cmml" xref="S3.SS3.p4.8.m3.1.1"><eq id="S3.SS3.p4.8.m3.1.1.1.cmml" xref="S3.SS3.p4.8.m3.1.1.1"></eq><apply id="S3.SS3.p4.8.m3.1.1.2.cmml" xref="S3.SS3.p4.8.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m3.1.1.2.1.cmml" xref="S3.SS3.p4.8.m3.1.1.2">superscript</csymbol><apply id="S3.SS3.p4.8.m3.1.1.2.2.cmml" xref="S3.SS3.p4.8.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m3.1.1.2.2.1.cmml" xref="S3.SS3.p4.8.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p4.8.m3.1.1.2.2.2.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.2">𝑭</ci><apply id="S3.SS3.p4.8.m3.1.1.2.2.3.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3"><times id="S3.SS3.p4.8.m3.1.1.2.2.3.1.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3.1"></times><ci id="S3.SS3.p4.8.m3.1.1.2.2.3.2.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3.2">𝑐</ci><ci id="S3.SS3.p4.8.m3.1.1.2.2.3.3.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3.3">𝑜</ci><ci id="S3.SS3.p4.8.m3.1.1.2.2.3.4.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3.4">𝑛</ci><ci id="S3.SS3.p4.8.m3.1.1.2.2.3.5.cmml" xref="S3.SS3.p4.8.m3.1.1.2.2.3.5">𝑣</ci></apply></apply><cn type="integer" id="S3.SS3.p4.8.m3.1.1.2.3.cmml" xref="S3.SS3.p4.8.m3.1.1.2.3">0</cn></apply><apply id="S3.SS3.p4.8.m3.1.1.3.cmml" xref="S3.SS3.p4.8.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m3.1.1.3.1.cmml" xref="S3.SS3.p4.8.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p4.8.m3.1.1.3.2.cmml" xref="S3.SS3.p4.8.m3.1.1.3.2">𝑭</ci><ci id="S3.SS3.p4.8.m3.1.1.3.3.cmml" xref="S3.SS3.p4.8.m3.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m3.1c">\bm{F}_{conv}^{0}=\bm{F}_{s}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.4" class="ltx_p">Finally, we employed a residual connection to combine <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\bm{F}_{conv}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><msub id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.2.cmml">𝑭</mi><mrow id="S3.SS3.p5.1.m1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.3.cmml"><mi id="S3.SS3.p5.1.m1.1.1.3.2" xref="S3.SS3.p5.1.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.3.1" xref="S3.SS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.3.3" xref="S3.SS3.p5.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.3.1a" xref="S3.SS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.3.4" xref="S3.SS3.p5.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.3.1b" xref="S3.SS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.3.5" xref="S3.SS3.p5.1.m1.1.1.3.5.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.2">𝑭</ci><apply id="S3.SS3.p5.1.m1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3"><times id="S3.SS3.p5.1.m1.1.1.3.1.cmml" xref="S3.SS3.p5.1.m1.1.1.3.1"></times><ci id="S3.SS3.p5.1.m1.1.1.3.2.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS3.p5.1.m1.1.1.3.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS3.p5.1.m1.1.1.3.4.cmml" xref="S3.SS3.p5.1.m1.1.1.3.4">𝑛</ci><ci id="S3.SS3.p5.1.m1.1.1.3.5.cmml" xref="S3.SS3.p5.1.m1.1.1.3.5">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\bm{F}_{conv}</annotation></semantics></math> and <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\bm{F}_{attn}" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><msub id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">𝑭</mi><mrow id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3.cmml"><mi id="S3.SS3.p5.2.m2.1.1.3.2" xref="S3.SS3.p5.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.2.m2.1.1.3.1" xref="S3.SS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.2.m2.1.1.3.3" xref="S3.SS3.p5.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.2.m2.1.1.3.1a" xref="S3.SS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.2.m2.1.1.3.4" xref="S3.SS3.p5.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.2.m2.1.1.3.1b" xref="S3.SS3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.2.m2.1.1.3.5" xref="S3.SS3.p5.2.m2.1.1.3.5.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">𝑭</ci><apply id="S3.SS3.p5.2.m2.1.1.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3"><times id="S3.SS3.p5.2.m2.1.1.3.1.cmml" xref="S3.SS3.p5.2.m2.1.1.3.1"></times><ci id="S3.SS3.p5.2.m2.1.1.3.2.cmml" xref="S3.SS3.p5.2.m2.1.1.3.2">𝑎</ci><ci id="S3.SS3.p5.2.m2.1.1.3.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3.3">𝑡</ci><ci id="S3.SS3.p5.2.m2.1.1.3.4.cmml" xref="S3.SS3.p5.2.m2.1.1.3.4">𝑡</ci><ci id="S3.SS3.p5.2.m2.1.1.3.5.cmml" xref="S3.SS3.p5.2.m2.1.1.3.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\bm{F}_{attn}</annotation></semantics></math> into the final fusion feature <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="\bm{F}_{fusion}" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><msub id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.2" xref="S3.SS3.p5.3.m3.1.1.2.cmml">𝑭</mi><mrow id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3.cmml"><mi id="S3.SS3.p5.3.m3.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1.3.1" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.3.m3.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1.3.1a" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.3.m3.1.1.3.4" xref="S3.SS3.p5.3.m3.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1.3.1b" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.3.m3.1.1.3.5" xref="S3.SS3.p5.3.m3.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1.3.1c" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.3.m3.1.1.3.6" xref="S3.SS3.p5.3.m3.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1.3.1d" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.3.m3.1.1.3.7" xref="S3.SS3.p5.3.m3.1.1.3.7.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">𝑭</ci><apply id="S3.SS3.p5.3.m3.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3"><times id="S3.SS3.p5.3.m3.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3.1"></times><ci id="S3.SS3.p5.3.m3.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2">𝑓</ci><ci id="S3.SS3.p5.3.m3.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3">𝑢</ci><ci id="S3.SS3.p5.3.m3.1.1.3.4.cmml" xref="S3.SS3.p5.3.m3.1.1.3.4">𝑠</ci><ci id="S3.SS3.p5.3.m3.1.1.3.5.cmml" xref="S3.SS3.p5.3.m3.1.1.3.5">𝑖</ci><ci id="S3.SS3.p5.3.m3.1.1.3.6.cmml" xref="S3.SS3.p5.3.m3.1.1.3.6">𝑜</ci><ci id="S3.SS3.p5.3.m3.1.1.3.7.cmml" xref="S3.SS3.p5.3.m3.1.1.3.7">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\bm{F}_{fusion}</annotation></semantics></math>, which is then fed into a linear classification head <math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="\mathtt{FC}_{out}(\cdot)" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><mrow id="S3.SS3.p5.4.m4.1.2" xref="S3.SS3.p5.4.m4.1.2.cmml"><msub id="S3.SS3.p5.4.m4.1.2.2" xref="S3.SS3.p5.4.m4.1.2.2.cmml"><mi id="S3.SS3.p5.4.m4.1.2.2.2" xref="S3.SS3.p5.4.m4.1.2.2.2.cmml">𝙵𝙲</mi><mrow id="S3.SS3.p5.4.m4.1.2.2.3" xref="S3.SS3.p5.4.m4.1.2.2.3.cmml"><mi id="S3.SS3.p5.4.m4.1.2.2.3.2" xref="S3.SS3.p5.4.m4.1.2.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.4.m4.1.2.2.3.1" xref="S3.SS3.p5.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p5.4.m4.1.2.2.3.3" xref="S3.SS3.p5.4.m4.1.2.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.4.m4.1.2.2.3.1a" xref="S3.SS3.p5.4.m4.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p5.4.m4.1.2.2.3.4" xref="S3.SS3.p5.4.m4.1.2.2.3.4.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p5.4.m4.1.2.1" xref="S3.SS3.p5.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS3.p5.4.m4.1.2.3.2" xref="S3.SS3.p5.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS3.p5.4.m4.1.2.3.2.1" xref="S3.SS3.p5.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p5.4.m4.1.2.3.2.2" xref="S3.SS3.p5.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><apply id="S3.SS3.p5.4.m4.1.2.cmml" xref="S3.SS3.p5.4.m4.1.2"><times id="S3.SS3.p5.4.m4.1.2.1.cmml" xref="S3.SS3.p5.4.m4.1.2.1"></times><apply id="S3.SS3.p5.4.m4.1.2.2.cmml" xref="S3.SS3.p5.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.4.m4.1.2.2.1.cmml" xref="S3.SS3.p5.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS3.p5.4.m4.1.2.2.2.cmml" xref="S3.SS3.p5.4.m4.1.2.2.2">𝙵𝙲</ci><apply id="S3.SS3.p5.4.m4.1.2.2.3.cmml" xref="S3.SS3.p5.4.m4.1.2.2.3"><times id="S3.SS3.p5.4.m4.1.2.2.3.1.cmml" xref="S3.SS3.p5.4.m4.1.2.2.3.1"></times><ci id="S3.SS3.p5.4.m4.1.2.2.3.2.cmml" xref="S3.SS3.p5.4.m4.1.2.2.3.2">𝑜</ci><ci id="S3.SS3.p5.4.m4.1.2.2.3.3.cmml" xref="S3.SS3.p5.4.m4.1.2.2.3.3">𝑢</ci><ci id="S3.SS3.p5.4.m4.1.2.2.3.4.cmml" xref="S3.SS3.p5.4.m4.1.2.2.3.4">𝑡</ci></apply></apply><ci id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\mathtt{FC}_{out}(\cdot)</annotation></semantics></math> for emotion prediction:</p>
<table id="S3.E7" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E7X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E7X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\bm{emotion}_{pred}=\mathtt{FC}_{out}(\bm{F}_{conv}+\bm{F}_{attn})" display="inline"><semantics id="S3.E7X.2.1.1.m1.1a"><mrow id="S3.E7X.2.1.1.m1.1.1" xref="S3.E7X.2.1.1.m1.1.1.cmml"><mrow id="S3.E7X.2.1.1.m1.1.1.3" xref="S3.E7X.2.1.1.m1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.3.2" xref="S3.E7X.2.1.1.m1.1.1.3.2.cmml">𝒆</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.3" xref="S3.E7X.2.1.1.m1.1.1.3.3.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1a" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.4" xref="S3.E7X.2.1.1.m1.1.1.3.4.cmml">𝒐</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1b" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.5" xref="S3.E7X.2.1.1.m1.1.1.3.5.cmml">𝒕</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1c" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.6" xref="S3.E7X.2.1.1.m1.1.1.3.6.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1d" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.7" xref="S3.E7X.2.1.1.m1.1.1.3.7.cmml">𝒐</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.1e" xref="S3.E7X.2.1.1.m1.1.1.3.1.cmml">​</mo><msub id="S3.E7X.2.1.1.m1.1.1.3.8" xref="S3.E7X.2.1.1.m1.1.1.3.8.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.3.8.2" xref="S3.E7X.2.1.1.m1.1.1.3.8.2.cmml">𝒏</mi><mrow id="S3.E7X.2.1.1.m1.1.1.3.8.3" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.3.8.3.2" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.8.3.1" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.8.3.3" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.8.3.1a" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.8.3.4" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.3.8.3.1b" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.3.8.3.5" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.5.cmml">d</mi></mrow></msub></mrow><mo id="S3.E7X.2.1.1.m1.1.1.2" xref="S3.E7X.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E7X.2.1.1.m1.1.1.1" xref="S3.E7X.2.1.1.m1.1.1.1.cmml"><msub id="S3.E7X.2.1.1.m1.1.1.1.3" xref="S3.E7X.2.1.1.m1.1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.3.2" xref="S3.E7X.2.1.1.m1.1.1.1.3.2.cmml">𝙵𝙲</mi><mrow id="S3.E7X.2.1.1.m1.1.1.1.3.3" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.3.3.2" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.3.3.1" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.3.3.3" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.3.3.1a" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.3.3.4" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.4.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.2" xref="S3.E7X.2.1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E7X.2.1.1.m1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7X.2.1.1.m1.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">𝑭</mi><mrow id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.4" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.5" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.5.cmml">v</mi></mrow></msub><mo id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.2.cmml">𝑭</mi><mrow id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.2" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1a" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.4" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1b" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.5" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.5.cmml">n</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E7X.2.1.1.m1.1.1.1.1.1.3" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7X.2.1.1.m1.1b"><apply id="S3.E7X.2.1.1.m1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1"><eq id="S3.E7X.2.1.1.m1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.2"></eq><apply id="S3.E7X.2.1.1.m1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3"><times id="S3.E7X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.2">𝒆</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.3">𝒎</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.4">𝒐</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.5.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.5">𝒕</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.6.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.6">𝒊</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.7.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.7">𝒐</ci><apply id="S3.E7X.2.1.1.m1.1.1.3.8.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.1.1.3.8.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.1.1.3.8.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.2">𝒏</ci><apply id="S3.E7X.2.1.1.m1.1.1.3.8.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3"><times id="S3.E7X.2.1.1.m1.1.1.3.8.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.3.8.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.2">𝑝</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.8.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.3">𝑟</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.8.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.4">𝑒</ci><ci id="S3.E7X.2.1.1.m1.1.1.3.8.3.5.cmml" xref="S3.E7X.2.1.1.m1.1.1.3.8.3.5">𝑑</ci></apply></apply></apply><apply id="S3.E7X.2.1.1.m1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1"><times id="S3.E7X.2.1.1.m1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.2"></times><apply id="S3.E7X.2.1.1.m1.1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.2">𝙵𝙲</ci><apply id="S3.E7X.2.1.1.m1.1.1.1.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.3"><times id="S3.E7X.2.1.1.m1.1.1.1.3.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.1.3.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.2">𝑜</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.3.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.3">𝑢</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.3.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.3.3.4">𝑡</ci></apply></apply><apply id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1"><plus id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.2">𝑭</ci><apply id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3"><times id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.2">𝑐</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.3">𝑜</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.4">𝑛</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.2.3.5">𝑣</ci></apply></apply><apply id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.2">𝑭</ci><apply id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3"><times id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.2">𝑎</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.3">𝑡</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.4">𝑡</ci><ci id="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1.1.1.3.3.5">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7X.2.1.1.m1.1c">\displaystyle\bm{emotion}_{pred}=\mathtt{FC}_{out}(\bm{F}_{conv}+\bm{F}_{attn})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS3.p5.5" class="ltx_p">The pseudo-labels generated by Emotion-LLaMA were integrated with the 5030 Train&amp;Val datasets from MER2024 to form the training set for our Conv-Attention model, as depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2. Large Models in Emotion Understanding ‣ 2. Related Work ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b). This integration through data augmentation significantly improves the performance and robustness of our emotion recognition system.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section presents a comprehensive evaluation of our approach on Track 2 (MER-NOISE) and Track 3 (MER-OV) of the MER2024 competition. We begin by analyzing the performance of single-modal models, followed by multimodal fusion experiments, and conclude with ablation studies. Our goal is to provide detailed insights that can inform future research and practical applications in multimodal emotion recognition, particularly in noisy and open-vocabulary environments.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Single-Modal Performance on Track 2: MER-NOISE</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We assessed several single-modal models on the MER-NOISE track to analyze the contributions of auditory, visual, and textual modalities to emotion recognition performance. The results are summarized in Table <a href="#S4.T2" title="Table 2 ‣ 4.1. Single-Modal Performance on Track 2: MER-NOISE ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Performance (%) of Single-Modal Models on Track 2: MER-NOISE. <sup id="S4.T2.9.1" class="ltx_sup">∗</sup>: Using prompts to extract features as input for the large language model.</figcaption>
<table id="S4.T2.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.7.6.1" class="ltx_tr">
<th id="S4.T2.7.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.7.6.1.1.1" class="ltx_text">Features</span></th>
<td id="S4.T2.7.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Train&amp;Val</td>
<td id="S4.T2.7.6.1.3" class="ltx_td ltx_align_center ltx_border_tt">MER-NOISE</td>
</tr>
<tr id="S4.T2.5.3" class="ltx_tr">
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center">WAF <math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mrow id="S4.T2.3.1.1.m1.1.2.2"><mo stretchy="false" id="S4.T2.3.1.1.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T2.3.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><ci id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">(\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">ACC <math id="S4.T2.4.2.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T2.4.2.2.m1.1a"><mrow id="S4.T2.4.2.2.m1.1.2.2"><mo stretchy="false" id="S4.T2.4.2.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T2.4.2.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><ci id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">(\uparrow)</annotation></semantics></math>
</td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_center">WAF <math id="S4.T2.5.3.3.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T2.5.3.3.m1.1a"><mrow id="S4.T2.5.3.3.m1.1.2.2"><mo stretchy="false" id="S4.T2.5.3.3.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T2.5.3.3.m1.1.1" xref="S4.T2.5.3.3.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T2.5.3.3.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.3.m1.1b"><ci id="S4.T2.5.3.3.m1.1.1.cmml" xref="S4.T2.5.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.3.m1.1c">(\uparrow)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.7.7.2" class="ltx_tr">
<th id="S4.T2.7.7.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4">Audio Modality</th>
</tr>
<tr id="S4.T2.7.8.3" class="ltx_tr">
<th id="S4.T2.7.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">eGeMAPS <cite class="ltx_cite ltx_citemacro_citep">(Eyben et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S4.T2.7.8.3.2" class="ltx_td ltx_align_center ltx_border_t">39.68</td>
<td id="S4.T2.7.8.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.88</td>
<td id="S4.T2.7.8.3.4" class="ltx_td ltx_align_center ltx_border_t">28.92</td>
</tr>
<tr id="S4.T2.7.9.4" class="ltx_tr">
<th id="S4.T2.7.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VGGish <cite class="ltx_cite ltx_citemacro_citep">(Hershey et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S4.T2.7.9.4.2" class="ltx_td ltx_align_center">48.60</td>
<td id="S4.T2.7.9.4.3" class="ltx_td ltx_align_center ltx_border_r">50.20</td>
<td id="S4.T2.7.9.4.4" class="ltx_td ltx_align_center">40.70</td>
</tr>
<tr id="S4.T2.7.10.5" class="ltx_tr">
<th id="S4.T2.7.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Whisper-base <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.7.10.5.2" class="ltx_td ltx_align_center">56.65</td>
<td id="S4.T2.7.10.5.3" class="ltx_td ltx_align_center ltx_border_r">57.08</td>
<td id="S4.T2.7.10.5.4" class="ltx_td ltx_align_center">41.26</td>
</tr>
<tr id="S4.T2.7.11.6" class="ltx_tr">
<th id="S4.T2.7.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">emotion2vec <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T2.7.11.6.2" class="ltx_td ltx_align_center">56.08</td>
<td id="S4.T2.7.11.6.3" class="ltx_td ltx_align_center ltx_border_r">56.48</td>
<td id="S4.T2.7.11.6.4" class="ltx_td ltx_align_center">45.66</td>
</tr>
<tr id="S4.T2.7.12.7" class="ltx_tr">
<th id="S4.T2.7.12.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HuBERT-large <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.7.12.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.7.12.7.2.1" class="ltx_text ltx_font_bold">72.77</span></td>
<td id="S4.T2.7.12.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.7.12.7.3.1" class="ltx_text ltx_font_bold">72.96</span></td>
<td id="S4.T2.7.12.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.7.12.7.4.1" class="ltx_text ltx_font_bold">72.67</span></td>
</tr>
<tr id="S4.T2.7.13.8" class="ltx_tr">
<th id="S4.T2.7.13.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4">Visual Modality</th>
</tr>
<tr id="S4.T2.7.14.9" class="ltx_tr">
<th id="S4.T2.7.14.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MANet-RAFDB <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.7.14.9.2" class="ltx_td ltx_align_center ltx_border_t">60.31</td>
<td id="S4.T2.7.14.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.33</td>
<td id="S4.T2.7.14.9.4" class="ltx_td ltx_align_center ltx_border_t">54.62</td>
</tr>
<tr id="S4.T2.7.15.10" class="ltx_tr">
<th id="S4.T2.7.15.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MAE-MER2024 <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.7.15.10.2" class="ltx_td ltx_align_center">61.48</td>
<td id="S4.T2.7.15.10.3" class="ltx_td ltx_align_center ltx_border_r">62.40</td>
<td id="S4.T2.7.15.10.4" class="ltx_td ltx_align_center">51.11</td>
</tr>
<tr id="S4.T2.7.16.11" class="ltx_tr">
<th id="S4.T2.7.16.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VideoMAE <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.7.16.11.2" class="ltx_td ltx_align_center">57.40</td>
<td id="S4.T2.7.16.11.3" class="ltx_td ltx_align_center ltx_border_r">58.10</td>
<td id="S4.T2.7.16.11.4" class="ltx_td ltx_align_center">49.18</td>
</tr>
<tr id="S4.T2.7.17.12" class="ltx_tr">
<th id="S4.T2.7.17.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VideoMAE-MER2024 <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.7.17.12.2" class="ltx_td ltx_align_center">64.87</td>
<td id="S4.T2.7.17.12.3" class="ltx_td ltx_align_center ltx_border_r">65.46</td>
<td id="S4.T2.7.17.12.4" class="ltx_td ltx_align_center">57.87</td>
</tr>
<tr id="S4.T2.7.18.13" class="ltx_tr">
<th id="S4.T2.7.18.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIP-large <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.7.18.13.2" class="ltx_td ltx_align_center"><span id="S4.T2.7.18.13.2.1" class="ltx_text ltx_font_bold">66.73</span></td>
<td id="S4.T2.7.18.13.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.7.18.13.3.1" class="ltx_text ltx_font_bold">67.28</span></td>
<td id="S4.T2.7.18.13.4" class="ltx_td ltx_align_center"><span id="S4.T2.7.18.13.4.1" class="ltx_text ltx_font_bold">58.80</span></td>
</tr>
<tr id="S4.T2.7.19.14" class="ltx_tr">
<th id="S4.T2.7.19.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4">Text Modality</th>
</tr>
<tr id="S4.T2.7.20.15" class="ltx_tr">
<th id="S4.T2.7.20.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RoBERTa-large <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.7.20.15.2" class="ltx_td ltx_align_center ltx_border_t">52.66</td>
<td id="S4.T2.7.20.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.92</td>
<td id="S4.T2.7.20.15.4" class="ltx_td ltx_align_center ltx_border_t">49.06</td>
</tr>
<tr id="S4.T2.7.21.16" class="ltx_tr">
<th id="S4.T2.7.21.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ChatGLM2-6B <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.7.21.16.2" class="ltx_td ltx_align_center">53.04</td>
<td id="S4.T2.7.21.16.3" class="ltx_td ltx_align_center ltx_border_r">53.28</td>
<td id="S4.T2.7.21.16.4" class="ltx_td ltx_align_center">50.39</td>
</tr>
<tr id="S4.T2.7.22.17" class="ltx_tr">
<th id="S4.T2.7.22.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MacBERT-large <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S4.T2.7.22.17.2" class="ltx_td ltx_align_center">52.19</td>
<td id="S4.T2.7.22.17.3" class="ltx_td ltx_align_center ltx_border_r">52.47</td>
<td id="S4.T2.7.22.17.4" class="ltx_td ltx_align_center">50.24</td>
</tr>
<tr id="S4.T2.7.23.18" class="ltx_tr">
<th id="S4.T2.7.23.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BLOOM-7B <cite class="ltx_cite ltx_citemacro_citep">(Workshop et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.7.23.18.2" class="ltx_td ltx_align_center">53.13</td>
<td id="S4.T2.7.23.18.3" class="ltx_td ltx_align_center ltx_border_r">53.30</td>
<td id="S4.T2.7.23.18.4" class="ltx_td ltx_align_center">50.38</td>
</tr>
<tr id="S4.T2.7.24.19" class="ltx_tr">
<th id="S4.T2.7.24.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Qwen1.5-32B <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib56" title="" class="ltx_ref">2024</a>)</cite>
</th>
<td id="S4.T2.7.24.19.2" class="ltx_td ltx_align_center">54.47</td>
<td id="S4.T2.7.24.19.3" class="ltx_td ltx_align_center ltx_border_r">54.82</td>
<td id="S4.T2.7.24.19.4" class="ltx_td ltx_align_center">50.12</td>
</tr>
<tr id="S4.T2.6.4" class="ltx_tr">
<th id="S4.T2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Qwen1.5-32B <cite class="ltx_cite ltx_citemacro_citep">(Team, <a href="#bib.bib56" title="" class="ltx_ref">2024</a>)</cite> <sup id="S4.T2.6.4.1.1" class="ltx_sup">∗</sup>
</th>
<td id="S4.T2.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.4.2.1" class="ltx_text ltx_font_bold">55.41</span></td>
<td id="S4.T2.6.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.6.4.3.1" class="ltx_text ltx_font_bold">55.89</span></td>
<td id="S4.T2.6.4.4" class="ltx_td ltx_align_center">58.88</td>
</tr>
<tr id="S4.T2.7.25.20" class="ltx_tr">
<th id="S4.T2.7.25.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Baichuan-13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T2.7.25.20.2" class="ltx_td ltx_align_center">55.15</td>
<td id="S4.T2.7.25.20.3" class="ltx_td ltx_align_center ltx_border_r">55.40</td>
<td id="S4.T2.7.25.20.4" class="ltx_td ltx_align_center">57.94</td>
</tr>
<tr id="S4.T2.7.5" class="ltx_tr">
<th id="S4.T2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Baichuan-13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite> <sup id="S4.T2.7.5.1.1" class="ltx_sup">∗</sup>
</th>
<td id="S4.T2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">54.29</td>
<td id="S4.T2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">54.48</td>
<td id="S4.T2.7.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.7.5.4.1" class="ltx_text ltx_font_bold">59.32</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Auditory Modality</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Given the limited availability of audio extraction models tailored for the Chinese language, we evaluated five models: eGeMAPS <cite class="ltx_cite ltx_citemacro_citep">(Eyben et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>, VGGish <cite class="ltx_cite ltx_citemacro_citep">(Hershey et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, emotion2vec <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, and Chinese-Hubert <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. The Chinese-Hubert model emerged as the top performer with a Weighted Average F-score (WAF) of 72.67%. This superior performance can be attributed to its pre-training on Chinese datasets, which enables it to capture contextual representations more effectively than other models.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Visual Modality</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">In the visual modality, we evaluated four models: MANet <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>, MAE <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, VideoMAE <cite class="ltx_cite ltx_citemacro_citep">(Tong et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, and CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>, along with versions fine-tuned for MER2024. CLIP achieved the highest WAF of 58.80%, likely due to its extensive pre-training and multimodal learning capabilities. VideoMAE was further enhanced by domain-specific fine-tuning for emotion recognition tasks.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Textual Modality</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">For the textual modality, we focused on models with strong Chinese language proficiency. Baichuan-13B, when used with carefully designed prompts, attained the highest WAF of 59.32%, closely followed by Qwen1.5-32B with a WAF of 58.88%. The strong performance of these models can be attributed to their large-scale pre-training on Chinese corpora and the effective use of prompts, which significantly enhanced their ability to recognize and classify emotions in textual data. Further analysis of prompt designs, as shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1.3. Textual Modality ‣ 4.1. Single-Modal Performance on Track 2: MER-NOISE ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, revealed that Prompt 1 provided the best performance across both single-modal and multimodal fusion scenarios.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Performance (%) of different prompts on Track 2: MER-NOISE. The contents of the three prompts are detailed in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We select acoustic features from HuBERT (HB), visual features from CLIP (CL), and textual features from Baichuan (BC).</figcaption>
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:238pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(51.8pt,-29.0pt) scale(1.32244319679239,1.32244319679239) ;">
<table id="S4.T3.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.3.4.1" class="ltx_tr">
<th id="S4.T3.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" colspan="3">Features</th>
<th id="S4.T3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Train&amp;Val</th>
<th id="S4.T3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MER-NOISE</th>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">A</th>
<th id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">V</th>
<th id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">T</th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1.2.2"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">ACC <math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.2.m1.1.2.2"><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mrow id="S4.T3.3.3.3.3.m1.1.2.2"><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3.5.1" class="ltx_tr">
<th id="S4.T3.3.3.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<th id="S4.T3.3.3.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<th id="S4.T3.3.3.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BC(w/o)</th>
<td id="S4.T3.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">55.15</td>
<td id="S4.T3.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.40</td>
<td id="S4.T3.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">57.94</td>
</tr>
<tr id="S4.T3.3.3.6.2" class="ltx_tr">
<th id="S4.T3.3.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.6.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/ prompt 1)</th>
<td id="S4.T3.3.3.6.2.4" class="ltx_td ltx_align_center">54.29</td>
<td id="S4.T3.3.3.6.2.5" class="ltx_td ltx_align_center ltx_border_r">54.48</td>
<td id="S4.T3.3.3.6.2.6" class="ltx_td ltx_align_center">59.32</td>
</tr>
<tr id="S4.T3.3.3.7.3" class="ltx_tr">
<th id="S4.T3.3.3.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.7.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.7.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/ prompt 2)</th>
<td id="S4.T3.3.3.7.3.4" class="ltx_td ltx_align_center">57.87</td>
<td id="S4.T3.3.3.7.3.5" class="ltx_td ltx_align_center ltx_border_r">61.15</td>
<td id="S4.T3.3.3.7.3.6" class="ltx_td ltx_align_center">58.66</td>
</tr>
<tr id="S4.T3.3.3.8.4" class="ltx_tr">
<th id="S4.T3.3.3.8.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.8.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T3.3.3.8.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/ prompt 3)</th>
<td id="S4.T3.3.3.8.4.4" class="ltx_td ltx_align_center">56.53</td>
<td id="S4.T3.3.3.8.4.5" class="ltx_td ltx_align_center ltx_border_r">59.00</td>
<td id="S4.T3.3.3.8.4.6" class="ltx_td ltx_align_center">57.77</td>
</tr>
<tr id="S4.T3.3.3.9.5" class="ltx_tr">
<th id="S4.T3.3.3.9.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">HB</th>
<th id="S4.T3.3.3.9.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL</th>
<th id="S4.T3.3.3.9.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/o)</th>
<td id="S4.T3.3.3.9.5.4" class="ltx_td ltx_align_center">78.58</td>
<td id="S4.T3.3.3.9.5.5" class="ltx_td ltx_align_center ltx_border_r">79.96</td>
<td id="S4.T3.3.3.9.5.6" class="ltx_td ltx_align_center">78.73</td>
</tr>
<tr id="S4.T3.3.3.10.6" class="ltx_tr">
<th id="S4.T3.3.3.10.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">HB</th>
<th id="S4.T3.3.3.10.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL</th>
<th id="S4.T3.3.3.10.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/ prompt 1)</th>
<td id="S4.T3.3.3.10.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.10.6.4.1" class="ltx_text ltx_font_bold">80.91</span></td>
<td id="S4.T3.3.3.10.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.3.10.6.5.1" class="ltx_text ltx_font_bold">81.01</span></td>
<td id="S4.T3.3.3.10.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.10.6.6.1" class="ltx_text ltx_font_bold">79.73</span></td>
</tr>
<tr id="S4.T3.3.3.11.7" class="ltx_tr">
<th id="S4.T3.3.3.11.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">HB</th>
<th id="S4.T3.3.3.11.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL</th>
<th id="S4.T3.3.3.11.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(w/ prompt 2)</th>
<td id="S4.T3.3.3.11.7.4" class="ltx_td ltx_align_center">78.96</td>
<td id="S4.T3.3.3.11.7.5" class="ltx_td ltx_align_center ltx_border_r">79.33</td>
<td id="S4.T3.3.3.11.7.6" class="ltx_td ltx_align_center">78.71</td>
</tr>
<tr id="S4.T3.3.3.12.8" class="ltx_tr">
<th id="S4.T3.3.3.12.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">HB</th>
<th id="S4.T3.3.3.12.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">CL</th>
<th id="S4.T3.3.3.12.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">BC(w/ prompt 3)</th>
<td id="S4.T3.3.3.12.8.4" class="ltx_td ltx_align_center ltx_border_bb">79.11</td>
<td id="S4.T3.3.3.12.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">79.50</td>
<td id="S4.T3.3.3.12.8.6" class="ltx_td ltx_align_center ltx_border_bb">77.61</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Multimodal Fusion on Track 2: MER-NOISE</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Leveraging the findings from the single-modal evaluations, we conducted multimodal fusion experiments by integrating features from the best-performing models in each modality. The results, as presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.2. Multimodal Fusion on Track 2: MER-NOISE ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, demonstrate the effectiveness of our proposed Conv-Attention model. The configuration combining HuBERT, CLIP, VideoMAE, Qwen, and Baichuan features yielded the highest WAF and ACC scores of 81.59% and 81.71% on the Train&amp;Val dataset. On the MER-NOISE track, the optimal setup, which also included additional visual models (MAE and MANet), achieved a WAF of 80.10%. These results underscore the effectiveness of multimodal fusion, particularly when employing our Conv-Attention model, which consistently outperformed other fusion strategies across all metrics (Table <a href="#S4.T5" title="Table 5 ‣ 4.2. Multimodal Fusion on Track 2: MER-NOISE ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) when trained on the augmented dataset.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Performance (%) of Multimodal Fusion Methods on MER-NOISE Track. <sup id="S4.T4.23.1" class="ltx_sup">∗</sup> Denotes methods using prompt-extracted features as input for large language models.</figcaption>
<table id="S4.T4.21" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.21.20.1" class="ltx_tr">
<th id="S4.T4.21.20.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="7">Features</th>
<th id="S4.T4.21.20.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Train&amp;Val</th>
<th id="S4.T4.21.20.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MER-NOISE</th>
</tr>
<tr id="S4.T4.5.3" class="ltx_tr">
<th id="S4.T4.5.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">A</th>
<th id="S4.T4.5.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">V</th>
<th id="S4.T4.5.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">V</th>
<th id="S4.T4.5.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">V</th>
<th id="S4.T4.5.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">V</th>
<th id="S4.T4.5.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">T</th>
<th id="S4.T4.5.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">T</th>
<th id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T4.3.1.1.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T4.3.1.1.m1.1a"><mrow id="S4.T4.3.1.1.m1.1.2.2"><mo stretchy="false" id="S4.T4.3.1.1.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T4.3.1.1.m1.1.1" xref="S4.T4.3.1.1.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T4.3.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.m1.1b"><ci id="S4.T4.3.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T4.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">ACC <math id="S4.T4.4.2.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T4.4.2.2.m1.1a"><mrow id="S4.T4.4.2.2.m1.1.2.2"><mo stretchy="false" id="S4.T4.4.2.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T4.4.2.2.m1.1.1" xref="S4.T4.4.2.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T4.4.2.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.m1.1b"><ci id="S4.T4.4.2.2.m1.1.1.cmml" xref="S4.T4.4.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T4.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T4.5.3.3.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T4.5.3.3.m1.1a"><mrow id="S4.T4.5.3.3.m1.1.2.2"><mo stretchy="false" id="S4.T4.5.3.3.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T4.5.3.3.m1.1.1" xref="S4.T4.5.3.3.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T4.5.3.3.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.3.m1.1b"><ci id="S4.T4.5.3.3.m1.1.1.cmml" xref="S4.T4.5.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.3.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.6.4" class="ltx_tr">
<td id="S4.T4.6.4.2" class="ltx_td ltx_align_center ltx_border_t">HuBERT</td>
<td id="S4.T4.6.4.3" class="ltx_td ltx_align_center ltx_border_t">CLIP</td>
<td id="S4.T4.6.4.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.6.4.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.6.4.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.6.4.1" class="ltx_td ltx_align_center ltx_border_t">Qwen <sup id="S4.T4.6.4.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T4.6.4.8" class="ltx_td ltx_align_center ltx_border_t">78.43</td>
<td id="S4.T4.6.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.67</td>
<td id="S4.T4.6.4.10" class="ltx_td ltx_align_center ltx_border_t">77.44</td>
</tr>
<tr id="S4.T4.7.5" class="ltx_tr">
<td id="S4.T4.7.5.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.7.5.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.7.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.7.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.7.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.7.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.7.5.1" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.7.5.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.7.5.8" class="ltx_td ltx_align_center">80.91</td>
<td id="S4.T4.7.5.9" class="ltx_td ltx_align_center ltx_border_r">81.01</td>
<td id="S4.T4.7.5.10" class="ltx_td ltx_align_center">79.73</td>
</tr>
<tr id="S4.T4.9.7" class="ltx_tr">
<td id="S4.T4.9.7.3" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.9.7.4" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.9.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.9.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.9.7.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.8.6.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.8.6.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.9.7.2" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.9.7.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.9.7.8" class="ltx_td ltx_align_center">80.50</td>
<td id="S4.T4.9.7.9" class="ltx_td ltx_align_center ltx_border_r">80.63</td>
<td id="S4.T4.9.7.10" class="ltx_td ltx_align_center">79.79</td>
</tr>
<tr id="S4.T4.10.8" class="ltx_tr">
<td id="S4.T4.10.8.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.10.8.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.10.8.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.10.8.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.10.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.10.8.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.10.8.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.10.8.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.10.8.8" class="ltx_td ltx_align_center">79.22</td>
<td id="S4.T4.10.8.9" class="ltx_td ltx_align_center ltx_border_r">79.23</td>
<td id="S4.T4.10.8.10" class="ltx_td ltx_align_center">77.10</td>
</tr>
<tr id="S4.T4.11.9" class="ltx_tr">
<td id="S4.T4.11.9.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.11.9.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.11.9.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.11.9.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.11.9.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.11.9.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.11.9.1" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.11.9.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.11.9.8" class="ltx_td ltx_align_center">80.44</td>
<td id="S4.T4.11.9.9" class="ltx_td ltx_align_center ltx_border_r">80.48</td>
<td id="S4.T4.11.9.10" class="ltx_td ltx_align_center">78.49</td>
</tr>
<tr id="S4.T4.13.11" class="ltx_tr">
<td id="S4.T4.13.11.3" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.13.11.4" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.13.11.5" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.13.11.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.13.11.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.12.10.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.12.10.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.13.11.2" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.13.11.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.13.11.8" class="ltx_td ltx_align_center"><span id="S4.T4.13.11.8.1" class="ltx_text ltx_font_bold">81.59</span></td>
<td id="S4.T4.13.11.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.13.11.9.1" class="ltx_text ltx_font_bold">81.71</span></td>
<td id="S4.T4.13.11.10" class="ltx_td ltx_align_center">79.63</td>
</tr>
<tr id="S4.T4.14.12" class="ltx_tr">
<td id="S4.T4.14.12.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.14.12.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.14.12.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.14.12.5" class="ltx_td ltx_align_center">MAE</td>
<td id="S4.T4.14.12.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.14.12.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.14.12.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.14.12.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.14.12.8" class="ltx_td ltx_align_center">78.98</td>
<td id="S4.T4.14.12.9" class="ltx_td ltx_align_center ltx_border_r">78.93</td>
<td id="S4.T4.14.12.10" class="ltx_td ltx_align_center">76.80</td>
</tr>
<tr id="S4.T4.15.13" class="ltx_tr">
<td id="S4.T4.15.13.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.15.13.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.15.13.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.15.13.5" class="ltx_td ltx_align_center">MAE</td>
<td id="S4.T4.15.13.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.15.13.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.15.13.1" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.15.13.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.15.13.8" class="ltx_td ltx_align_center">81.52</td>
<td id="S4.T4.15.13.9" class="ltx_td ltx_align_center ltx_border_r">81.59</td>
<td id="S4.T4.15.13.10" class="ltx_td ltx_align_center">79.03</td>
</tr>
<tr id="S4.T4.17.15" class="ltx_tr">
<td id="S4.T4.17.15.3" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.17.15.4" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.17.15.5" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.17.15.6" class="ltx_td ltx_align_center">MAE</td>
<td id="S4.T4.17.15.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.16.14.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.16.14.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.17.15.2" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.17.15.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.17.15.8" class="ltx_td ltx_align_center">81.49</td>
<td id="S4.T4.17.15.9" class="ltx_td ltx_align_center ltx_border_r">81.55</td>
<td id="S4.T4.17.15.10" class="ltx_td ltx_align_center">79.93</td>
</tr>
<tr id="S4.T4.18.16" class="ltx_tr">
<td id="S4.T4.18.16.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.18.16.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.18.16.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.18.16.5" class="ltx_td ltx_align_center">MAE</td>
<td id="S4.T4.18.16.6" class="ltx_td ltx_align_center">MANet</td>
<td id="S4.T4.18.16.1" class="ltx_td ltx_align_center">Qwen <sup id="S4.T4.18.16.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.18.16.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.18.16.8" class="ltx_td ltx_align_center">79.35</td>
<td id="S4.T4.18.16.9" class="ltx_td ltx_align_center ltx_border_r">79.48</td>
<td id="S4.T4.18.16.10" class="ltx_td ltx_align_center">77.30</td>
</tr>
<tr id="S4.T4.19.17" class="ltx_tr">
<td id="S4.T4.19.17.2" class="ltx_td ltx_align_center">HuBERT</td>
<td id="S4.T4.19.17.3" class="ltx_td ltx_align_center">CLIP</td>
<td id="S4.T4.19.17.4" class="ltx_td ltx_align_center">VideoMAE</td>
<td id="S4.T4.19.17.5" class="ltx_td ltx_align_center">MAE</td>
<td id="S4.T4.19.17.6" class="ltx_td ltx_align_center">MANet</td>
<td id="S4.T4.19.17.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.19.17.1" class="ltx_td ltx_align_center ltx_border_r">Baichuan <sup id="S4.T4.19.17.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.19.17.8" class="ltx_td ltx_align_center">81.45</td>
<td id="S4.T4.19.17.9" class="ltx_td ltx_align_center ltx_border_r">81.57</td>
<td id="S4.T4.19.17.10" class="ltx_td ltx_align_center">79.93</td>
</tr>
<tr id="S4.T4.21.19" class="ltx_tr">
<td id="S4.T4.21.19.3" class="ltx_td ltx_align_center ltx_border_bb">HuBERT</td>
<td id="S4.T4.21.19.4" class="ltx_td ltx_align_center ltx_border_bb">CLIP</td>
<td id="S4.T4.21.19.5" class="ltx_td ltx_align_center ltx_border_bb">VideoMAE</td>
<td id="S4.T4.21.19.6" class="ltx_td ltx_align_center ltx_border_bb">MAE</td>
<td id="S4.T4.21.19.7" class="ltx_td ltx_align_center ltx_border_bb">MANet</td>
<td id="S4.T4.20.18.1" class="ltx_td ltx_align_center ltx_border_bb">Qwen <sup id="S4.T4.20.18.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.21.19.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Baichuan <sup id="S4.T4.21.19.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T4.21.19.8" class="ltx_td ltx_align_center ltx_border_bb">81.40</td>
<td id="S4.T4.21.19.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">81.53</td>
<td id="S4.T4.21.19.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.21.19.10.1" class="ltx_text ltx_font_bold">80.10</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance (%) Comparison of Multimodal Fusion Models on Track 2: MER-NOISE.</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.0pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.7pt,6.3pt) scale(0.9,0.9) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Model</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Train WAF</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Train ACC</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Noise WAF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MLP <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite>
</th>
<td id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.44</td>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">65.08</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">50.02</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Attention <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024c</a>)</cite>
</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.29</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.59</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">83.48</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">FBP <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.98</td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.17</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">83.21</td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<th id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Convolution</th>
<td id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.90</td>
<td id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.35</td>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">84.50</td>
</tr>
<tr id="S4.T5.1.1.6.5" class="ltx_tr">
<th id="S4.T5.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Transformer</th>
<td id="S4.T5.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.81</td>
<td id="S4.T5.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.23</td>
<td id="S4.T5.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">84.55</td>
</tr>
<tr id="S4.T5.1.1.7.6" class="ltx_tr">
<th id="S4.T5.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">Conv-Attention (ours)</th>
<td id="S4.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T5.1.1.7.6.2.1" class="ltx_text ltx_font_bold">83.59</span></td>
<td id="S4.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T5.1.1.7.6.3.1" class="ltx_text ltx_font_bold">83.83</span></td>
<td id="S4.T5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T5.1.1.7.6.4.1" class="ltx_text ltx_font_bold">85.30</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Performance on Track 3: MER-OV</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">For Track 3 (MER-OV), which addresses open-vocabulary emotion recognition, we evaluated various large language models using <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mbox{Accuracy}_{s}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mtext id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2a.cmml">Accuracy</mtext><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><mtext id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">Accuracy</mtext></ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\mbox{Accuracy}_{s}</annotation></semantics></math>, <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mbox{Recall}_{s}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mtext id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2a.cmml">Recall</mtext><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2a.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><mtext id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">Recall</mtext></ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\mbox{Recall}_{s}</annotation></semantics></math>, and their average (Avg). The results are detailed in Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Performance on Track 3: MER-OV ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Emotion-LLaMA outperformed other models in terms of average performance. The version that outputs only emotion categories achieved an accuracy of 83.43%, while the version that generates complete emotion descriptions attained the highest recall of 62.59% and an average score of 66.10%. The success of Emotion-LLaMA can be attributed to three key factors: (1) emotion-specific pre-training on corpora rich in emotional content, enabling the model to capture subtle emotional nuances; (2) multi-task learning, which allows the model to excel in both emotion classification and description generation; and (3) an open-vocabulary design, which is well-suited for handling diverse and complex emotion descriptions. The trade-off observed between accuracy and recall suggests that while more detailed emotion descriptions enhance recall, they may also introduce a higher risk of misclassification.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>Performance (%) of Single-Modal Models on Track 3: MER-OV. The “avg” column represents the average of “<math id="S4.T6.5.m1.1" class="ltx_Math" alttext="\mbox{Accuracy}_{\mbox{s}}" display="inline"><semantics id="S4.T6.5.m1.1b"><msub id="S4.T6.5.m1.1.1" xref="S4.T6.5.m1.1.1.cmml"><mtext id="S4.T6.5.m1.1.1.2" xref="S4.T6.5.m1.1.1.2a.cmml">Accuracy</mtext><mtext id="S4.T6.5.m1.1.1.3" xref="S4.T6.5.m1.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.5.m1.1c"><apply id="S4.T6.5.m1.1.1.cmml" xref="S4.T6.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.5.m1.1.1.1.cmml" xref="S4.T6.5.m1.1.1">subscript</csymbol><ci id="S4.T6.5.m1.1.1.2a.cmml" xref="S4.T6.5.m1.1.1.2"><mtext id="S4.T6.5.m1.1.1.2.cmml" xref="S4.T6.5.m1.1.1.2">Accuracy</mtext></ci><ci id="S4.T6.5.m1.1.1.3a.cmml" xref="S4.T6.5.m1.1.1.3"><mtext mathsize="70%" id="S4.T6.5.m1.1.1.3.cmml" xref="S4.T6.5.m1.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.m1.1d">\mbox{Accuracy}_{\mbox{s}}</annotation></semantics></math>” and “<math id="S4.T6.6.m2.1" class="ltx_Math" alttext="\mbox{Recall}_{\mbox{s}}" display="inline"><semantics id="S4.T6.6.m2.1b"><msub id="S4.T6.6.m2.1.1" xref="S4.T6.6.m2.1.1.cmml"><mtext id="S4.T6.6.m2.1.1.2" xref="S4.T6.6.m2.1.1.2a.cmml">Recall</mtext><mtext id="S4.T6.6.m2.1.1.3" xref="S4.T6.6.m2.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.6.m2.1c"><apply id="S4.T6.6.m2.1.1.cmml" xref="S4.T6.6.m2.1.1"><csymbol cd="ambiguous" id="S4.T6.6.m2.1.1.1.cmml" xref="S4.T6.6.m2.1.1">subscript</csymbol><ci id="S4.T6.6.m2.1.1.2a.cmml" xref="S4.T6.6.m2.1.1.2"><mtext id="S4.T6.6.m2.1.1.2.cmml" xref="S4.T6.6.m2.1.1.2">Recall</mtext></ci><ci id="S4.T6.6.m2.1.1.3a.cmml" xref="S4.T6.6.m2.1.1.3"><mtext mathsize="70%" id="S4.T6.6.m2.1.1.3.cmml" xref="S4.T6.6.m2.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.m2.1d">\mbox{Recall}_{\mbox{s}}</annotation></semantics></math>”. <sup id="S4.T6.15.1" class="ltx_sup"><span id="S4.T6.15.1.1" class="ltx_text ltx_font_italic">†</span></sup>: Only outputs emotion categories; <sup id="S4.T6.16.2" class="ltx_sup"><span id="S4.T6.16.2.1" class="ltx_text ltx_font_italic">‡</span></sup>: Outputs complete emotion descriptions.</figcaption>
<table id="S4.T6.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.10.2" class="ltx_tr">
<th id="S4.T6.10.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S4.T6.9.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T6.9.1.1.m1.1" class="ltx_Math" alttext="\mbox{Accuracy}_{\mbox{s}}" display="inline"><semantics id="S4.T6.9.1.1.m1.1a"><msub id="S4.T6.9.1.1.m1.1.1" xref="S4.T6.9.1.1.m1.1.1.cmml"><mtext id="S4.T6.9.1.1.m1.1.1.2" xref="S4.T6.9.1.1.m1.1.1.2a.cmml">Accuracy</mtext><mtext id="S4.T6.9.1.1.m1.1.1.3" xref="S4.T6.9.1.1.m1.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.9.1.1.m1.1b"><apply id="S4.T6.9.1.1.m1.1.1.cmml" xref="S4.T6.9.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.9.1.1.m1.1.1.1.cmml" xref="S4.T6.9.1.1.m1.1.1">subscript</csymbol><ci id="S4.T6.9.1.1.m1.1.1.2a.cmml" xref="S4.T6.9.1.1.m1.1.1.2"><mtext id="S4.T6.9.1.1.m1.1.1.2.cmml" xref="S4.T6.9.1.1.m1.1.1.2">Accuracy</mtext></ci><ci id="S4.T6.9.1.1.m1.1.1.3a.cmml" xref="S4.T6.9.1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T6.9.1.1.m1.1.1.3.cmml" xref="S4.T6.9.1.1.m1.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.1.1.m1.1c">\mbox{Accuracy}_{\mbox{s}}</annotation></semantics></math></th>
<th id="S4.T6.10.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T6.10.2.2.m1.1" class="ltx_Math" alttext="\mbox{Recall}_{\mbox{s}}" display="inline"><semantics id="S4.T6.10.2.2.m1.1a"><msub id="S4.T6.10.2.2.m1.1.1" xref="S4.T6.10.2.2.m1.1.1.cmml"><mtext id="S4.T6.10.2.2.m1.1.1.2" xref="S4.T6.10.2.2.m1.1.1.2a.cmml">Recall</mtext><mtext id="S4.T6.10.2.2.m1.1.1.3" xref="S4.T6.10.2.2.m1.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.10.2.2.m1.1b"><apply id="S4.T6.10.2.2.m1.1.1.cmml" xref="S4.T6.10.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.10.2.2.m1.1.1.1.cmml" xref="S4.T6.10.2.2.m1.1.1">subscript</csymbol><ci id="S4.T6.10.2.2.m1.1.1.2a.cmml" xref="S4.T6.10.2.2.m1.1.1.2"><mtext id="S4.T6.10.2.2.m1.1.1.2.cmml" xref="S4.T6.10.2.2.m1.1.1.2">Recall</mtext></ci><ci id="S4.T6.10.2.2.m1.1.1.3a.cmml" xref="S4.T6.10.2.2.m1.1.1.3"><mtext mathsize="70%" id="S4.T6.10.2.2.m1.1.1.3.cmml" xref="S4.T6.10.2.2.m1.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.2.2.m1.1c">\mbox{Recall}_{\mbox{s}}</annotation></semantics></math></th>
<th id="S4.T6.10.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.12.5.1" class="ltx_tr">
<th id="S4.T6.12.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Empty</th>
<td id="S4.T6.12.5.1.2" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T6.12.5.1.3" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T6.12.5.1.4" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
</tr>
<tr id="S4.T6.12.6.2" class="ltx_tr">
<th id="S4.T6.12.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Random</th>
<td id="S4.T6.12.6.2.2" class="ltx_td ltx_align_center">13.42</td>
<td id="S4.T6.12.6.2.3" class="ltx_td ltx_align_center">24.85</td>
<td id="S4.T6.12.6.2.4" class="ltx_td ltx_align_center">19.13</td>
</tr>
<tr id="S4.T6.12.7.3" class="ltx_tr">
<th id="S4.T6.12.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ground Truth</th>
<td id="S4.T6.12.7.3.2" class="ltx_td ltx_align_center">93.37</td>
<td id="S4.T6.12.7.3.3" class="ltx_td ltx_align_center">52.51</td>
<td id="S4.T6.12.7.3.4" class="ltx_td ltx_align_center">72.94</td>
</tr>
<tr id="S4.T6.12.8.4" class="ltx_tr">
<th id="S4.T6.12.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Valley <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.8.4.2" class="ltx_td ltx_align_center ltx_border_t">20.16</td>
<td id="S4.T6.12.8.4.3" class="ltx_td ltx_align_center ltx_border_t">13.26</td>
<td id="S4.T6.12.8.4.4" class="ltx_td ltx_align_center ltx_border_t">16.71</td>
</tr>
<tr id="S4.T6.12.9.5" class="ltx_tr">
<th id="S4.T6.12.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Otter <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>)</cite>
</th>
<td id="S4.T6.12.9.5.2" class="ltx_td ltx_align_center">29.64</td>
<td id="S4.T6.12.9.5.3" class="ltx_td ltx_align_center">23.04</td>
<td id="S4.T6.12.9.5.4" class="ltx_td ltx_align_center">26.34</td>
</tr>
<tr id="S4.T6.12.10.6" class="ltx_tr">
<th id="S4.T6.12.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PandaGPT <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.10.6.2" class="ltx_td ltx_align_center">35.75</td>
<td id="S4.T6.12.10.6.3" class="ltx_td ltx_align_center">31.57</td>
<td id="S4.T6.12.10.6.4" class="ltx_td ltx_align_center">33.66</td>
</tr>
<tr id="S4.T6.12.11.7" class="ltx_tr">
<th id="S4.T6.12.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Video-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2023a</a>)</cite>
</th>
<td id="S4.T6.12.11.7.2" class="ltx_td ltx_align_center">31.08</td>
<td id="S4.T6.12.11.7.3" class="ltx_td ltx_align_center">32.26</td>
<td id="S4.T6.12.11.7.4" class="ltx_td ltx_align_center">31.67</td>
</tr>
<tr id="S4.T6.12.12.8" class="ltx_tr">
<th id="S4.T6.12.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VideoChat <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023a</a>)</cite>
</th>
<td id="S4.T6.12.12.8.2" class="ltx_td ltx_align_center">43.17</td>
<td id="S4.T6.12.12.8.3" class="ltx_td ltx_align_center">44.92</td>
<td id="S4.T6.12.12.8.4" class="ltx_td ltx_align_center">44.05</td>
</tr>
<tr id="S4.T6.12.13.9" class="ltx_tr">
<th id="S4.T6.12.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VideoChat2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2024b</a>)</cite>
</th>
<td id="S4.T6.12.13.9.2" class="ltx_td ltx_align_center">46.91</td>
<td id="S4.T6.12.13.9.3" class="ltx_td ltx_align_center">34.78</td>
<td id="S4.T6.12.13.9.4" class="ltx_td ltx_align_center">40.85</td>
</tr>
<tr id="S4.T6.12.14.10" class="ltx_tr">
<th id="S4.T6.12.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Video-ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Maaz et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.14.10.2" class="ltx_td ltx_align_center">46.20</td>
<td id="S4.T6.12.14.10.3" class="ltx_td ltx_align_center">39.33</td>
<td id="S4.T6.12.14.10.4" class="ltx_td ltx_align_center">42.77</td>
</tr>
<tr id="S4.T6.12.15.11" class="ltx_tr">
<th id="S4.T6.12.15.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SALMONN <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.15.11.2" class="ltx_td ltx_align_center">42.20</td>
<td id="S4.T6.12.15.11.3" class="ltx_td ltx_align_center">44.75</td>
<td id="S4.T6.12.15.11.4" class="ltx_td ltx_align_center">43.47</td>
</tr>
<tr id="S4.T6.12.16.12" class="ltx_tr">
<th id="S4.T6.12.16.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen-Audio <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.16.12.2" class="ltx_td ltx_align_center">55.12</td>
<td id="S4.T6.12.16.12.3" class="ltx_td ltx_align_center">32.91</td>
<td id="S4.T6.12.16.12.4" class="ltx_td ltx_align_center">44.02</td>
</tr>
<tr id="S4.T6.12.17.13" class="ltx_tr">
<th id="S4.T6.12.17.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.17.13.2" class="ltx_td ltx_align_center">44.80</td>
<td id="S4.T6.12.17.13.3" class="ltx_td ltx_align_center">46.54</td>
<td id="S4.T6.12.17.13.4" class="ltx_td ltx_align_center">45.67</td>
</tr>
<tr id="S4.T6.12.18.14" class="ltx_tr">
<th id="S4.T6.12.18.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AffectGPT <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2024d</a>)</cite>
</th>
<td id="S4.T6.12.18.14.2" class="ltx_td ltx_align_center">66.14</td>
<td id="S4.T6.12.18.14.3" class="ltx_td ltx_align_center">46.56</td>
<td id="S4.T6.12.18.14.4" class="ltx_td ltx_align_center">56.35</td>
</tr>
<tr id="S4.T6.12.19.15" class="ltx_tr">
<th id="S4.T6.12.19.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4V <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T6.12.19.15.2" class="ltx_td ltx_align_center">56.19</td>
<td id="S4.T6.12.19.15.3" class="ltx_td ltx_align_center">58.97</td>
<td id="S4.T6.12.19.15.4" class="ltx_td ltx_align_center">57.58</td>
</tr>
<tr id="S4.T6.11.3" class="ltx_tr">
<th id="S4.T6.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Emotion-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite> <sup id="S4.T6.11.3.1.1" class="ltx_sup"><span id="S4.T6.11.3.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</th>
<td id="S4.T6.11.3.2" class="ltx_td ltx_align_center"><span id="S4.T6.11.3.2.1" class="ltx_text ltx_font_bold">83.43</span></td>
<td id="S4.T6.11.3.3" class="ltx_td ltx_align_center">47.49</td>
<td id="S4.T6.11.3.4" class="ltx_td ltx_align_center">65.46</td>
</tr>
<tr id="S4.T6.12.4" class="ltx_tr">
<th id="S4.T6.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Emotion-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024a</a>)</cite> <sup id="S4.T6.12.4.1.1" class="ltx_sup"><span id="S4.T6.12.4.1.1.1" class="ltx_text ltx_font_italic">‡</span></sup>
</th>
<td id="S4.T6.12.4.2" class="ltx_td ltx_align_center ltx_border_bb">69.61</td>
<td id="S4.T6.12.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.12.4.3.1" class="ltx_text ltx_font_bold">62.59</span></td>
<td id="S4.T6.12.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.12.4.4.1" class="ltx_text ltx_font_bold">66.10</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To gain deeper insights into the effectiveness of our proposed Conv-Attention model, we conducted a series of ablation studies to evaluate the impact of various components and hyperparameters.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Component Ablation</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Table <a href="#S4.T7" title="Table 7 ‣ 4.4.1. Component Ablation ‣ 4.4. Ablation Studies ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents the results of our component ablation study. The findings highlight several important observations: (1) The ReLU activation function is crucial for introducing non-linearity, which significantly improves the model’s ability to learn complex patterns. Removing ReLU resulted in a noticeable performance drop. (2) The use of multiple convolutional blocks enhances the model’s capability to capture hierarchical features, which is beneficial for recognizing multi-scale patterns in multimodal data. The best performance was observed with two convolutional blocks.(3) Batch normalization plays a vital role in stabilizing the learning process and improving generalization. The inclusion of batch normalization layers led to a performance boost, likely due to their effect in reducing the internal covariate shift.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7. </span>Performance Impact (%) of Conv-Attention.</figcaption>
<div id="S4.T7.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:285.2pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.8pt,7.2pt) scale(0.9,0.9) ;">
<table id="S4.T7.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.4.4.4.5.1" class="ltx_tr">
<th id="S4.T7.4.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Model</th>
<th id="S4.T7.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Train WAF</th>
<th id="S4.T7.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Train ACC</th>
<th id="S4.T7.4.4.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">Noise WAF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.4.4.4.6.1" class="ltx_tr">
<th id="S4.T7.4.4.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Relu</th>
<td id="S4.T7.4.4.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.52</td>
<td id="S4.T7.4.4.4.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.69</td>
<td id="S4.T7.4.4.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.70</td>
</tr>
<tr id="S4.T7.1.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Attention(Conv-Block<math id="S4.T7.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.1.1.1.1.1.m1.1a"><mo id="S4.T7.1.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.m1.1b"><times id="S4.T7.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.m1.1c">\times</annotation></semantics></math>0)</th>
<td id="S4.T7.1.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.20</td>
<td id="S4.T7.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.43</td>
<td id="S4.T7.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.48</td>
</tr>
<tr id="S4.T7.2.2.2.2" class="ltx_tr">
<th id="S4.T7.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Conv-Block<math id="S4.T7.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.2.2.2.2.1.m1.1a"><mo id="S4.T7.2.2.2.2.1.m1.1.1" xref="S4.T7.2.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.1.m1.1b"><times id="S4.T7.2.2.2.2.1.m1.1.1.cmml" xref="S4.T7.2.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.1.m1.1c">\times</annotation></semantics></math>1</th>
<td id="S4.T7.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.88</td>
<td id="S4.T7.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.05</td>
<td id="S4.T7.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.74</td>
</tr>
<tr id="S4.T7.3.3.3.3" class="ltx_tr">
<th id="S4.T7.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Conv-Block<math id="S4.T7.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.3.3.3.3.1.m1.1a"><mo id="S4.T7.3.3.3.3.1.m1.1.1" xref="S4.T7.3.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.1.m1.1b"><times id="S4.T7.3.3.3.3.1.m1.1.1.cmml" xref="S4.T7.3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.1.m1.1c">\times</annotation></semantics></math>2</th>
<td id="S4.T7.3.3.3.3.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.00</td>
<td id="S4.T7.3.3.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.09</td>
<td id="S4.T7.3.3.3.3.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.09</td>
</tr>
<tr id="S4.T7.4.4.4.4" class="ltx_tr">
<th id="S4.T7.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">Conv-Block<math id="S4.T7.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.4.4.4.4.1.m1.1a"><mo id="S4.T7.4.4.4.4.1.m1.1.1" xref="S4.T7.4.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.1.m1.1b"><times id="S4.T7.4.4.4.4.1.m1.1.1.cmml" xref="S4.T7.4.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.1.m1.1c">\times</annotation></semantics></math>3</th>
<td id="S4.T7.4.4.4.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.88</td>
<td id="S4.T7.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.85</td>
<td id="S4.T7.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.29</td>
</tr>
<tr id="S4.T7.4.4.4.7.2" class="ltx_tr">
<th id="S4.T7.4.4.4.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S4.T7.4.4.4.7.2.1.1" class="ltx_text ltx_font_italic">w/o</span> Batch-Normalization</th>
<td id="S4.T7.4.4.4.7.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.96</td>
<td id="S4.T7.4.4.4.7.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.13</td>
<td id="S4.T7.4.4.4.7.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.64</td>
</tr>
<tr id="S4.T7.4.4.4.8.3" class="ltx_tr">
<th id="S4.T7.4.4.4.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Conv-Attention</th>
<td id="S4.T7.4.4.4.8.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T7.4.4.4.8.3.2.1" class="ltx_text ltx_font_bold">81.37</span></td>
<td id="S4.T7.4.4.4.8.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T7.4.4.4.8.3.3.1" class="ltx_text ltx_font_bold">81.45</span></td>
<td id="S4.T7.4.4.4.8.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T7.4.4.4.8.3.4.1" class="ltx_text ltx_font_bold">82.68</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Impact of Data Ratio and Learning Rate</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We also explored the effects of varying data ratios and learning rates on model performance. Tables <a href="#S4.T8" title="Table 8 ‣ 4.4.2. Impact of Data Ratio and Learning Rate ‣ 4.4. Ablation Studies ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#S4.T9" title="Table 9 ‣ 4.4.2. Impact of Data Ratio and Learning Rate ‣ 4.4. Ablation Studies ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> summarize our findings. Increasing the data ratio consistently improved performance, with 100% data usage yielding the highest scores across all metrics. This suggests that pseudo-labeling effectively augments the training data, enabling the model to learn from a larger and more diverse dataset. However, it is important to consider that while using 100% of pseudo-labeled data can enhance performance, it may also introduce some noise. The optimal ratio may depend on the quality of the pseudo-labels.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8. </span>Impact of Data Ratio on Performance(%).</figcaption>
<table id="S4.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.1.1.1" class="ltx_tr">
<th id="S4.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Data Ratio</span></th>
<th id="S4.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Train WAF</span></th>
<th id="S4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Train ACC</span></th>
<th id="S4.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">Noise WAF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.2.1" class="ltx_tr">
<th id="S4.T8.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">20%</th>
<td id="S4.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">79.18</td>
<td id="S4.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">79.72</td>
<td id="S4.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">79.88</td>
</tr>
<tr id="S4.T8.1.3.2" class="ltx_tr">
<th id="S4.T8.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">40%</th>
<td id="S4.T8.1.3.2.2" class="ltx_td ltx_align_center">80.06</td>
<td id="S4.T8.1.3.2.3" class="ltx_td ltx_align_center">80.36</td>
<td id="S4.T8.1.3.2.4" class="ltx_td ltx_align_center">81.29</td>
</tr>
<tr id="S4.T8.1.4.3" class="ltx_tr">
<th id="S4.T8.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">60%</th>
<td id="S4.T8.1.4.3.2" class="ltx_td ltx_align_center">81.29</td>
<td id="S4.T8.1.4.3.3" class="ltx_td ltx_align_center">81.54</td>
<td id="S4.T8.1.4.3.4" class="ltx_td ltx_align_center">82.2</td>
</tr>
<tr id="S4.T8.1.5.4" class="ltx_tr">
<th id="S4.T8.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">80%</th>
<td id="S4.T8.1.5.4.2" class="ltx_td ltx_align_center">81.98</td>
<td id="S4.T8.1.5.4.3" class="ltx_td ltx_align_center">82.19</td>
<td id="S4.T8.1.5.4.4" class="ltx_td ltx_align_center">82.5</td>
</tr>
<tr id="S4.T8.1.6.5" class="ltx_tr">
<th id="S4.T8.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">100%</th>
<td id="S4.T8.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T8.1.6.5.2.1" class="ltx_text ltx_font_bold">82.05</span></td>
<td id="S4.T8.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T8.1.6.5.3.1" class="ltx_text ltx_font_bold">82.3</span></td>
<td id="S4.T8.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T8.1.6.5.4.1" class="ltx_text ltx_font_bold">83.87</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9. </span>Impact of Learning Rate on Performance(%).</figcaption>
<div id="S4.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:266.2pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T9.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.1.1.1.1" class="ltx_tr">
<th id="S4.T9.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Learning Rate</th>
<th id="S4.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Train WAF</th>
<th id="S4.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Train ACC</th>
<th id="S4.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Noise WAF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.1.1.2.1" class="ltx_tr">
<th id="S4.T9.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1e-4</th>
<td id="S4.T9.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.1.1.2.1.2.1" class="ltx_text ltx_font_bold">83.29</span></td>
<td id="S4.T9.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.1.1.2.1.3.1" class="ltx_text ltx_font_bold">83.38</span></td>
<td id="S4.T9.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">83.10</td>
</tr>
<tr id="S4.T9.1.1.3.2" class="ltx_tr">
<th id="S4.T9.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5e-4</th>
<td id="S4.T9.1.1.3.2.2" class="ltx_td ltx_align_center">82.95</td>
<td id="S4.T9.1.1.3.2.3" class="ltx_td ltx_align_center">83.20</td>
<td id="S4.T9.1.1.3.2.4" class="ltx_td ltx_align_center">83.72</td>
</tr>
<tr id="S4.T9.1.1.4.3" class="ltx_tr">
<th id="S4.T9.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1e-3</th>
<td id="S4.T9.1.1.4.3.2" class="ltx_td ltx_align_center">82.05</td>
<td id="S4.T9.1.1.4.3.3" class="ltx_td ltx_align_center">82.30</td>
<td id="S4.T9.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T9.1.1.4.3.4.1" class="ltx_text ltx_font_bold">83.87</span></td>
</tr>
<tr id="S4.T9.1.1.5.4" class="ltx_tr">
<th id="S4.T9.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5e-3</th>
<td id="S4.T9.1.1.5.4.2" class="ltx_td ltx_align_center">79.87</td>
<td id="S4.T9.1.1.5.4.3" class="ltx_td ltx_align_center">80.29</td>
<td id="S4.T9.1.1.5.4.4" class="ltx_td ltx_align_center">83.32</td>
</tr>
<tr id="S4.T9.1.1.6.5" class="ltx_tr">
<th id="S4.T9.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1e-2</th>
<td id="S4.T9.1.1.6.5.2" class="ltx_td ltx_align_center">78.58</td>
<td id="S4.T9.1.1.6.5.3" class="ltx_td ltx_align_center">79.45</td>
<td id="S4.T9.1.1.6.5.4" class="ltx_td ltx_align_center">82.09</td>
</tr>
<tr id="S4.T9.1.1.7.6" class="ltx_tr">
<th id="S4.T9.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">5e-2</th>
<td id="S4.T9.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">72.82</td>
<td id="S4.T9.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">76.13</td>
<td id="S4.T9.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb">73.31</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">Regarding learning rates, a rate of 1e-3 provided the best balance between convergence speed and model accuracy, achieving the highest Noise WAF. Lower learning rates (e.g., 1e-4) resulted in slower convergence, while higher rates (e.g., 5e-3 and above) caused unstable training and poor generalization, especially in noisy environments. These findings highlight the critical importance of proper hyperparameter tuning in achieving optimal performance, particularly in challenging multimodal and noisy settings.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Modality Alignment</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Building on previous work <cite class="ltx_cite ltx_citemacro_citep">(Zong et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>, we examined the impact of modality alignment on MER, with results presented in Table <a href="#S4.T10" title="Table 10 ‣ 4.4.3. Modality Alignment ‣ 4.4. Ablation Studies ‣ 4. Experiments ‣ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Interesting phenomena emerged from these experiments: (1) Post-alignment, the scores of the previously weaker visual and textual modalities improved. However, this came at the cost of a performance decline in the best-performing audio modality, suggesting that alignment may sometimes dilute the complementary strengths of individual modalities. (2) The performance of the multimodal fusion dropped significantly after alignment. This indicates that while alignment may homogenize features across modalities, it can reduce the benefits derived from the diversity of information carried by different modalities.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10. </span>Modality Alignment Impact on Performance (%). Features: HuBERT (HB) for acoustic, CLIP (CL) for visual, Baichuan (BC) for textual.</figcaption>
<div id="S4.T10.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:215.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(33.9pt,-17.7pt) scale(1.19720104128243,1.19720104128243) ;">
<table id="S4.T10.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T10.3.3.4.1" class="ltx_tr">
<th id="S4.T10.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" colspan="3">Features</th>
<th id="S4.T10.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Train&amp;Val</th>
<th id="S4.T10.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MER-NOISE</th>
</tr>
<tr id="S4.T10.3.3.3" class="ltx_tr">
<th id="S4.T10.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">A</th>
<th id="S4.T10.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">V</th>
<th id="S4.T10.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">T</th>
<th id="S4.T10.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T10.1.1.1.1.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T10.1.1.1.1.m1.1a"><mrow id="S4.T10.1.1.1.1.m1.1.2.2"><mo stretchy="false" id="S4.T10.1.1.1.1.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T10.1.1.1.1.m1.1.1" xref="S4.T10.1.1.1.1.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T10.1.1.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T10.1.1.1.1.m1.1b"><ci id="S4.T10.1.1.1.1.m1.1.1.cmml" xref="S4.T10.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.1.1.1.1.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T10.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">ACC <math id="S4.T10.2.2.2.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T10.2.2.2.2.m1.1a"><mrow id="S4.T10.2.2.2.2.m1.1.2.2"><mo stretchy="false" id="S4.T10.2.2.2.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T10.2.2.2.2.m1.1.1" xref="S4.T10.2.2.2.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T10.2.2.2.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T10.2.2.2.2.m1.1b"><ci id="S4.T10.2.2.2.2.m1.1.1.cmml" xref="S4.T10.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.2.2.2.2.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
<th id="S4.T10.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">WAF <math id="S4.T10.3.3.3.3.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S4.T10.3.3.3.3.m1.1a"><mrow id="S4.T10.3.3.3.3.m1.1.2.2"><mo stretchy="false" id="S4.T10.3.3.3.3.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S4.T10.3.3.3.3.m1.1.1" xref="S4.T10.3.3.3.3.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S4.T10.3.3.3.3.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T10.3.3.3.3.m1.1b"><ci id="S4.T10.3.3.3.3.m1.1.1.cmml" xref="S4.T10.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.3.3.3.3.m1.1c">(\uparrow)</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T10.3.3.5.1" class="ltx_tr">
<th id="S4.T10.3.3.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">HB</th>
<th id="S4.T10.3.3.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<th id="S4.T10.3.3.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="S4.T10.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">72.77</td>
<td id="S4.T10.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.96</td>
<td id="S4.T10.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">72.67</td>
</tr>
<tr id="S4.T10.3.3.6.2" class="ltx_tr">
<th id="S4.T10.3.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">HB(align)</th>
<th id="S4.T10.3.3.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.6.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S4.T10.3.3.6.2.4" class="ltx_td ltx_align_center">66.18</td>
<td id="S4.T10.3.3.6.2.5" class="ltx_td ltx_align_center ltx_border_r">66.63</td>
<td id="S4.T10.3.3.6.2.6" class="ltx_td ltx_align_center">66.04</td>
</tr>
<tr id="S4.T10.3.3.7.3" class="ltx_tr">
<th id="S4.T10.3.3.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.7.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL</th>
<th id="S4.T10.3.3.7.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S4.T10.3.3.7.3.4" class="ltx_td ltx_align_center">66.73</td>
<td id="S4.T10.3.3.7.3.5" class="ltx_td ltx_align_center ltx_border_r">67.28</td>
<td id="S4.T10.3.3.7.3.6" class="ltx_td ltx_align_center">58.80</td>
</tr>
<tr id="S4.T10.3.3.8.4" class="ltx_tr">
<th id="S4.T10.3.3.8.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.8.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL(align)</th>
<th id="S4.T10.3.3.8.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S4.T10.3.3.8.4.4" class="ltx_td ltx_align_center">69.07</td>
<td id="S4.T10.3.3.8.4.5" class="ltx_td ltx_align_center ltx_border_r">69.50</td>
<td id="S4.T10.3.3.8.4.6" class="ltx_td ltx_align_center">65.93</td>
</tr>
<tr id="S4.T10.3.3.9.5" class="ltx_tr">
<th id="S4.T10.3.3.9.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.9.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.9.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC</th>
<td id="S4.T10.3.3.9.5.4" class="ltx_td ltx_align_center">54.29</td>
<td id="S4.T10.3.3.9.5.5" class="ltx_td ltx_align_center ltx_border_r">54.48</td>
<td id="S4.T10.3.3.9.5.6" class="ltx_td ltx_align_center">59.32</td>
</tr>
<tr id="S4.T10.3.3.10.6" class="ltx_tr">
<th id="S4.T10.3.3.10.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.10.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<th id="S4.T10.3.3.10.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC(align)</th>
<td id="S4.T10.3.3.10.6.4" class="ltx_td ltx_align_center">59.87</td>
<td id="S4.T10.3.3.10.6.5" class="ltx_td ltx_align_center ltx_border_r">57.78</td>
<td id="S4.T10.3.3.10.6.6" class="ltx_td ltx_align_center">48.66</td>
</tr>
<tr id="S4.T10.3.3.11.7" class="ltx_tr">
<th id="S4.T10.3.3.11.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">HB</th>
<th id="S4.T10.3.3.11.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">CL</th>
<th id="S4.T10.3.3.11.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BC</th>
<td id="S4.T10.3.3.11.7.4" class="ltx_td ltx_align_center">80.91</td>
<td id="S4.T10.3.3.11.7.5" class="ltx_td ltx_align_center ltx_border_r">81.01</td>
<td id="S4.T10.3.3.11.7.6" class="ltx_td ltx_align_center">79.73</td>
</tr>
<tr id="S4.T10.3.3.12.8" class="ltx_tr">
<th id="S4.T10.3.3.12.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">HB(align)</th>
<th id="S4.T10.3.3.12.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">CL(align)</th>
<th id="S4.T10.3.3.12.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">BC(align)</th>
<td id="S4.T10.3.3.12.8.4" class="ltx_td ltx_align_center ltx_border_bb">73.01</td>
<td id="S4.T10.3.3.12.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">73.29</td>
<td id="S4.T10.3.3.12.8.6" class="ltx_td ltx_align_center ltx_border_bb">70.67</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Limitations &amp; Future Work</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Despite the effectiveness of our methods, several limitations remain that warrant further investigation: (1) Our research is limited to Chinese language data, highlighting the need for validation across other languages and cross-lingual scenarios. (2) The models are not optimized for real-time emotion recognition, indicating a need for improvements in computational efficiency. (3) While our ablation studies provide valuable insights, enhancing the interpretability of multimodal fusion models is crucial, particularly in understanding the contributions of each modality. (4) Our emphasis on the <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">MER-NOISE</span> track underscores the importance of exploring model robustness across different noise types and levels.
Addressing these limitations is crucial for the continued advancement of <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">multimodal emotion recognition</span> and the development of more flexible and effective systems for practical applications.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we presented our winning approach for enhancing <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">multimodal emotion recognition</span> in the <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">MER2024 Challenge</span>, specifically targeting the <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">MER-NOISE</span> and <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">MER-OV</span> tracks. By leveraging the advanced capabilities of <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> to generate high-quality pseudo-labels and introducing a <span id="S5.p1.1.6" class="ltx_text ltx_font_italic">Conv-Attention</span> mechanism for efficient feature fusion, we significantly improved the robustness and accuracy of emotion recognition. Our method delivered state-of-the-art performance in the <span id="S5.p1.1.7" class="ltx_text ltx_font_italic">MER-NOISE</span> track with a weighted average F-score of 85.30% and achieved top results in the <span id="S5.p1.1.8" class="ltx_text ltx_font_italic">MER-OV</span> track, enhancing average accuracy and recall by 8.52% compared to GPT-4V. The integration of <span id="S5.p1.1.9" class="ltx_text ltx_font_italic">Emotion-LLaMA</span> was pivotal in achieving these results, underscoring its potential to advance the field of multimodal emotion recognition.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The SZTU team acknowledges support from the National Natural Science Foundation of China (62176165), the Stable Support Projects for Shenzhen Higher Education Institutions (20220718110918001), and the Natural Science Foundation of Top Talent at SZTU (GDRC202131). The CMU team thanks the Student Travel Support for the ACM International Conference on Multimedia (ACM MM) and acknowledges the support of NSF CISE under grant number 1937998.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aguilera et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ana Aguilera, Diego Mellado, and Felipe Rojas. 2023.

</span>
<span class="ltx_bibblock">An assessment of in-the-wild datasets for multimodal emotion recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 23, 11 (2023), 5184.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 35 (2022), 23716–23736.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.16609 [cs.CL]

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.14165 [cs.CL]

<a target="_blank" href="https://arxiv.org/abs/2005.14165" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2005.14165</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. 2021.

</span>
<span class="ltx_bibblock">Crossvit: Cross-attention multi-scale vision transformer for image classification. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>. 357–366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023.

</span>
<span class="ltx_bibblock">Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15195</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, and Alexander Hauptmann. 2024a.

</span>
<span class="ltx_bibblock">Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.11161</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zebang Cheng, Yuxiang Lin, Zhaoru Chen, Xiang Li, Shuyi Mao, Fan Zhang, Daijun Ding, Bowen Zhang, and Xiaojiang Peng. 2023.

</span>
<span class="ltx_bibblock">Semi-supervised multimodal emotion recognition with expression mae. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 9436–9440.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang, and Xiaojiang Peng. 2024b.

</span>
<span class="ltx_bibblock">Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.00511</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07919</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al<span id="bib.bib12.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020.

</span>
<span class="ltx_bibblock">Revisiting Pre-Trained Models for Chinese Natural Language Processing. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2020</em>. 657–668.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1810.04805 [cs.CL]

<a target="_blank" href="https://arxiv.org/abs/1810.04805" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1810.04805</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chaoyue Ding, Jiakui Li, Daoming Zong, Baoxiang Li, TianHao Zhang, and Qunyan Zhou. 2023a.

</span>
<span class="ltx_bibblock">Stable Speech Emotion Recognition with Head-k-Pooling Loss. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Chaoyue Ding, Daoming Zong, Baoxiang Li, Ken Zheng, Dinghao Zhou, Jiakui Li, and Qunyan Zhou. 2023b.

</span>
<span class="ltx_bibblock">Learning aligned audiovisual representations for multimodal sentiment analysis. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing</em>. 21–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Kevin Ding, Martin Zong, Jiakui Li, and Baoxiang Li. 2022.

</span>
<span class="ltx_bibblock">LETR: A lightweight and efficient transformer for keyword spotting. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 7987–7991.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock">GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 320–335.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyben et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Florian Eyben, Klaus R Scherer, Björn W Schuller, Johan Sundberg, Elisabeth André, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S Narayanan, et al<span id="bib.bib19.3.1" class="ltx_text">.</span> 2016.

</span>
<span class="ltx_bibblock">The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.4.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 7, 2 (2016), 190–202.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021.

</span>
<span class="ltx_bibblock">Masked Autoencoders Are Scalable Vision Learners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2111.06377 [cs.CV]

<a target="_blank" href="https://arxiv.org/abs/2111.06377" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2111.06377</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 770–778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hershey et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al<span id="bib.bib22.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">CNN architectures for large-scale audio classification. In <em id="bib.bib22.4.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 131–135.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 29 (2021), 3451–3460.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Dawei Huang, Chuan Yan, Qing Li, and Xiaojiang Peng. 2024.

</span>
<span class="ltx_bibblock">From Large Language Models to Large Multimodal Models: A Literature Review.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 14, 12 (2024).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/app14125068" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/app14125068</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al<span id="bib.bib25.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.12017</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, and Jiateng Liu. 2020.

</span>
<span class="ltx_bibblock">Dfew: A large-scale database for recognizing dynamic facial expressions in the wild. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on multimedia</em>. 2881–2889.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2023.

</span>
<span class="ltx_bibblock">Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.11911</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023b.

</span>
<span class="ltx_bibblock">Otter: A Multi-Modal Model with In-Context Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.03726</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023a.

</span>
<span class="ltx_bibblock">Videochat: Chat-centric video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06355</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. 2024b.

</span>
<span class="ltx_bibblock">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and Min Zhang. 2024a.

</span>
<span class="ltx_bibblock">Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.11273</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming Zhao, et al<span id="bib.bib32.3.1" class="ltx_text">.</span> 2023a.

</span>
<span class="ltx_bibblock">Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning. In <em id="bib.bib32.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 9610–9614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2024c)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, et al<span id="bib.bib33.3.1" class="ltx_text">.</span> 2024c.

</span>
<span class="ltx_bibblock">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.17113</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2024d)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Jiangyan Yi, Bin Liu, and Jianhua Tao. 2024d.

</span>
<span class="ltx_bibblock">AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.07653</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and Jianhua Tao. 2024a.

</span>
<span class="ltx_bibblock">Merbench: A unified evaluation benchmark for multimodal emotion recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.03429</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, and Jianhua Tao. 2024b.

</span>
<span class="ltx_bibblock">GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Information Fusion</em> (2024), 102367.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zheng Lian, Licai Sun, Mingyu Xu, Haiyang Sun, Ke Xu, Zhuofan Wen, Shun Chen, Bin Liu, and Jianhua Tao. 2023b.

</span>
<span class="ltx_bibblock">Explainable multimodal emotion reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15401</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. 2021.

</span>
<span class="ltx_bibblock">Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2108.05302 [cs.CV]

<a target="_blank" href="https://arxiv.org/abs/2108.05302" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2108.05302</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and Xiaohu Qie. 2022.

</span>
<span class="ltx_bibblock">Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3042–3051.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining Approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. 2023.

</span>
<span class="ltx_bibblock">Valley: Video assistant with large language model enhanced ability.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.07207</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Fengmao Lv, Xiang Chen, Yanyong Huang, Lixin Duan, and Guosheng Lin. 2021.

</span>
<span class="ltx_bibblock">Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2554–2562.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen. 2023.

</span>
<span class="ltx_bibblock">emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15185</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023.

</span>
<span class="ltx_bibblock">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05424</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagrani et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. 2021.

</span>
<span class="ltx_bibblock">Attention bottlenecks for multimodal fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 34 (2021), 14200–14213.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4V(ision) system card.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openai.com/research/gpt-4v-system-card" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/gpt-4v-system-card</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al<span id="bib.bib47.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.4.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding Multimodal Large Language Models to the World.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.14824</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pise and Kulkarni (2008)</span>
<span class="ltx_bibblock">
Nitin Namdeo Pise and Parag Kulkarni. 2008.

</span>
<span class="ltx_bibblock">A survey of semi-supervised learning methods. In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">2008 International conference on computational intelligence and security</em>, Vol. 2. IEEE, 30–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018.

</span>
<span class="ltx_bibblock">Meld: A multimodal multi-party dataset for emotion recognition in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.02508</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span id="bib.bib51.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em id="bib.bib51.4.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock">Robust Speech Recognition via Large-Scale Weak Supervision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.04356 [eess.AS]

<a target="_blank" href="https://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2212.04356</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023.

</span>
<span class="ltx_bibblock">PandaGPT: One Model To Instruction-Follow Them All. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants</em>. 11–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. 2023.

</span>
<span class="ltx_bibblock">Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 15, 1 (2023), 309–325.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock">SALMONN: Towards Generic Hearing Abilities for Large Language Models. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2024)</span>
<span class="ltx_bibblock">
Qwen Team. 2024.

</span>
<span class="ltx_bibblock">Introducing Qwen1.5.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://qwenlm.github.io/blog/qwen1.5/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://qwenlm.github.io/blog/qwen1.5/</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 2022.

</span>
<span class="ltx_bibblock">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Advances in Neural Information Processing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al<span id="bib.bib58.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11175</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Model with Self Generated Instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10560</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al<span id="bib.bib60.3.1" class="ltx_text">.</span> 2022b.

</span>
<span class="ltx_bibblock">Benchmarking generalization via in-context instructions on 1,600+ language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.07705</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11903 [cs.CL]

<a target="_blank" href="https://arxiv.org/abs/2201.11903" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.11903</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al<span id="bib.bib62.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, and Wen-Huang Cheng. 2024.

</span>
<span class="ltx_bibblock">EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.16670</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al<span id="bib.bib64.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open large-scale language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023.

</span>
<span class="ltx_bibblock">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.14178 [cs.CL]

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023a.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02858</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Sitao Zhang, Yimu Pan, and James Z Wang. 2023b.

</span>
<span class="ltx_bibblock">Learning emotion representations from verbal and nonverbal communication. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18993–19004.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Patras (2023)</span>
<span class="ltx_bibblock">
Zengqun Zhao and Ioannis Patras. 2023.

</span>
<span class="ltx_bibblock">Prompting visual-language models for dynamic facial expression recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.13382</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Hengshun Zhou, Debin Meng, Yuanyuan Zhang, Xiaojiang Peng, Jun Du, Kai Wang, and Yu Qiao. 2019.

</span>
<span class="ltx_bibblock">Exploring emotion features and fusion strategies for audio-video emotion recognition. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">2019 International conference on multimodal interaction</em>. 562–566.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zong et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Daoming Zong, Chaoyue Ding, Baoxiang Li, Jiakui Li, Ken Zheng, and Qunyan Zhou. 2023.

</span>
<span class="ltx_bibblock">AcFormer: An Aligned and Compact Transformer for Multimodal Sentiment Analysis. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 833–842.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.10499" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.10500" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.10500">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.10500" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.10501" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:51:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
