<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.13004] Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition</title><meta property="og:description" content="Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.13004">

<!--Generated on Tue Mar  5 18:15:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>visual speech recognition, comparative study, decoding speech, cross-language analysis</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\newcites</span>
<p id="p1.2" class="ltx_p">languageresourceLanguage Resources









</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">Pattern Recognition and Human Language Technologies Research Center</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Universitat Politècnica de València, Camino de Vera, s/n, 46022, València, Spain</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.4.4.1.1" class="ltx_text ltx_font_typewriter">dagigo1@dsic.upv.es, cmartine@dsic.upv.es</span></td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Inspired by different studies that have shown the relevance of visual cues during our speech perception process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">McGurk and MacDonald, 1976</a>, <a href="#bib.bibx7" title="" class="ltx_ref">Besle et al., 2004</a>]</cite>, several authors explored the task of Automatic Speech Recognition (ASR) from an audio-visual perspective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Potamianos et al., 2003</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>, <a href="#bib.bibx37" title="" class="ltx_ref">Ma et al., 2021b</a>]</cite>. These works supported that auditory and visual cues complement each other, leading to more robust systems, especially in adverse scenarios such as a noisy environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Juang, 1991</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Nonetheless, in the last few decades, there has been an increasing interest in Visual Speech Recognition (VSR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>]</cite>, a challenging task that aims to interpret speech solely by reading the speaker’s lips. Recognizing speech without the need for the auditory sense can offer a wide range of applications, e.g., silent visual passwords <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Ezz et al., 2020</a>]</cite>, active speaker detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Kim et al., 2021</a>, <a href="#bib.bibx56" title="" class="ltx_ref">Tao et al., 2021</a>]</cite>, visual keyword spotting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Stafylakis and Tzimiropoulos, 2018</a>, <a href="#bib.bibx48" title="" class="ltx_ref">Prajwal et al., 2021</a>]</cite>, or the development of silent speech interfaces that would be able to improve the lives of people who experience difficulties in producing speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Denby et al., 2010</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Gonzalez-Lopez et al., 2020</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Although unprecedented results have recently been achieved in the field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Prajwal et al., 2022</a>]</cite>, VSR remains an open research problem, where different factors must be considered, e.g., visual ambiguities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Bear et al., 2014b</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Fernández-López and Sukno, 2017</a>]</cite>, the complex modeling of silence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">Thangthai, 2018</a>]</cite>, the inter-personal variability among speakers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Cox et al., 2008</a>]</cite>, and different lighting conditions, as well as more technical aspects such as frame rate and image resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Bear and Harvey, 2016</a>, <a href="#bib.bibx5" title="" class="ltx_ref">Bear et al., 2014a</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Dungan et al., 2018</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Motivated by these challenges, one of the main research purposes in the field of VSR has been exploring diverse approaches to extract powerful visual speech representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>]</cite>. Traditional techniques, either based on Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), or Active Shape Models (AAM), were the object of study for decades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Bregler and Konig, 1994</a>, <a href="#bib.bibx40" title="" class="ltx_ref">Matthews et al., 2002</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Potamianos et al., 2003</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2021</a>]</cite>. Nowadays, the most common approach is the design of end-to-end architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Prajwal et al., 2022</a>]</cite>, models capable of automatically learning these visual speech representations in a data-driven manner. However, to the best of our knowledge, there is no systematic analysis of how robust these data-driven features are to domain- and language-mismatch scenarios.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Regarding visual speech decoders, different approaches have been explored in the literature. From the systems based on Hidden Markov Models combined with Gaussian Mixture Models (GMM-HMM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Juang and Rabiner, 1991</a>, <a href="#bib.bibx18" title="" class="ltx_ref">Gales and Young, 2008</a>]</cite>, the use of Deep Neural Networks (DNNs) to model emission probabilities provided the so-called hybrid DNN-HMM model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Hinton et al., 2012</a>]</cite>. However, these conventional systems present limitations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx62" title="" class="ltx_ref">Watanabe et al., 2017</a>]</cite> because of the need for forced alignments, a pre-defined lexicon, independent module optimizations, and the use of pre-defined, non-adaptive visual speech representations. Hence, research shifted towards end-to-end architectures based on Deep Learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx61" title="" class="ltx_ref">Wang et al., 2019</a>]</cite>. Early works, either based on Recurrent Neural Networks (RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Chan et al., 2016</a>]</cite> or on the Connectionist Temporal Classification (CTC) paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Graves et al., 2006</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Graves and Jaitly, 2014</a>]</cite> demonstrated what these novel architectures were capable of. Thereafter, based on advances in neural machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite>, remarkable results were obtained thanks to the use of powerful attention-based mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Dong et al., 2018</a>]</cite>. Nowadays, the hybrid CTC/Attention decoder, which combines the properties of both paradigms, is considered the current state of the art in speech processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx62" title="" class="ltx_ref">Watanabe et al., 2017</a>, <a href="#bib.bibx37" title="" class="ltx_ref">Ma et al., 2021b</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">Most comparative studies on decoding paradigms were conducted for auditory-based ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Lüscher et al., 2019</a>, <a href="#bib.bibx31" title="" class="ltx_ref">Karita et al., 2019</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>]</cite>. However, albeit different speech decoders were explored in VSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx57" title="" class="ltx_ref">Thangthai and Harvey, 2017</a>, <a href="#bib.bibx54" title="" class="ltx_ref">Son Chung et al., 2017</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>, <a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>]</cite>, it is not easy to compare all these approaches due to the use of different databases or the design of different experimental setups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">These were the main reasons that motivated our research, where our key contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i1.p1.1" class="ltx_p">A comprehensive comparison of conventional hybrid DNN-HMM decoders and their state-of-the-art CTC/Attention counterpart for the continuous VSR task.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i2.p1.1" class="ltx_p">We systematically studied how these different decoding paradigms behave based on the amount of data available for their estimation, showing that conventional HMM-based systems significantly outperformed state-of-the-art architectures in data-scarcity scenarios.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">We discussed different deployment aspects, such as training time, number of parameters, or real-time factor, supporting a more appropriate model selection where not only performance is considered.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i4.p1.1" class="ltx_p">We analyzed to what extent our pre-trained data-driven visual speech features were able to adapt to scenarios for which they were not explicitly trained by considering databases covering a different domain or language.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Speech Features.</span> Unlike auditory-based ASR, there was no consensus on the most suitable visual speech representation for decades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>]</cite>. In the past, diverse traditional techniques were widely explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Bregler and Konig, 1994</a>, <a href="#bib.bibx40" title="" class="ltx_ref">Matthews et al., 2002</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Potamianos et al., 2003</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2021</a>]</cite>. Nowadays, the design of end-to-end architectures, capable of automatically learning these speech representations in a data-driven manner during their training process, has led to unprecedented advances in the field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Prajwal et al., 2022</a>]</cite>. Most of these approaches rely on convolutional neural networks, such as the so-called ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">He et al., 2016</a>]</cite>, to obtain a latent visual representation, and then, attention-based mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite> are used to model temporal relationships. In addition, self-supervised methods, complemented with acoustic cues during their estimation, have also been explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Ma et al., 2021a</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Shi et al., 2022</a>]</cite>. However, there are no studies on how these data-driven features can adapt or transfer their knowledge when dealing with different domains or languages which they were not explicitly trained for.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Visual Speech Recognition.</span> Albeit conventional paradigms were explored for VSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx57" title="" class="ltx_ref">Thangthai and Harvey, 2017</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite>, the current state of the art is dominated by end-to-end approaches based on powerful attention-mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Prajwal et al., 2022</a>]</cite>. On average, results of around 25-30% Word Error Rate (WER) were achieved for the English corpora LRS2-BBC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>]</cite> and LRS3-TED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Afouras et al., 2018b</a>]</cite>. However, by the use of large-scale pseudo-label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Ma et al., 2023</a>]</cite> or synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Liu et al., 2023</a>]</cite>, recent works have reached a new state of the art around 15% WER.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">Besides, interest in VSR for languages other than English has recently increased <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite>. A remarkable work in this regard is the one carried out by <span id="S2.p3.1.1" class="ltx_text ltx_font_bold">?</span>), where 8 non-English languages were explored presenting a new multi-lingual benchmark. However, the authors focused on audio-visual speech recognition/translation and did not report results for lipreading. Regarding Spanish VSR (a language also considered in our work), <span id="S2.p3.1.2" class="ltx_text ltx_font_bold">?</span>) reached recognition rates of around 50% WER for different Spanish corpora, using an end-to-end architecture, while <span id="S2.p3.1.3" class="ltx_text ltx_font_bold">?</span>) presented the challenging LIP-RTVE database, reporting baseline results (roughly 95% WER) using traditional visual speech features and a GMM-HMM model. Subsequently, although results around 60% WER were achieved for the LIP-RTVE corpus, the same authors focused their study on speaker-dependent scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2023</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">Due to the use of different databases, model architectures, and experimental setups, it is not easy to adequately compare all approaches explored in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>, <a href="#bib.bibx43" title="" class="ltx_ref">Nemani et al., 2023</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Speech Decoders:</span> Multiple studies in model comparison for auditory-based ASR have been developed. <span id="S2.p5.1.2" class="ltx_text ltx_font_bold">?</span>) presented a comparison between a conventional DNN-HMM model and an end-to-end RNN-based architecture. Their results showed that the DNN-HMM paradigm outperformed the end-to-end recognizer. <span id="S2.p5.1.3" class="ltx_text ltx_font_bold">?</span>) carried out a thorough comparative study focused on RNN- and Transformed-based end-to-end models, including HMM-based models in the comparison. Although their findings showed that Transformer-based models outperformed RNNs, they also demonstrated that a DNN-HMM model could reach state-of-the-art recognition rates.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p">Regarding VSR, to the best of our knowledge, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>]</cite> is the only work that includes a systems comparison. Specifically, it compared the CTC paradigm to the attention-based one, showing that, albeit each one offers different valuable properties, the attention-based approach performed better, probably due to its powerful context modeling. However, although databases of different natures and multiple architectures were explored in the literature, none of the previous works conducted a systematic study on how these different decoding paradigms behave for VSR, depending on the data available for training.</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text ltx_font_bold">Present Work.</span> Motivated by all these aspects, we present a comprehensive comparison of conventional hybrid DNN-HMM decoders and their state-of-the-art CTC/Attention counterpart for the continuous VSR task. We not only systematically compared both approaches based on the amount of data available for their estimation, but we also took into account different deployment aspects. In addition, by considering three benchmarking VSR datasets, we evaluated how robust our pre-trained data-driven visual speech features could be to domain- and language-mismatch scenarios.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.1.   Databases</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">LRS2-BBC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Afouras et al., 2018a</a>]</cite> is a large-scale English database composed of around 224 hours collected from BBC TV programs. It consists of a pre-training set with 96,318 samples (195 hours), a training set with 45,839 samples (28 hours), a validation set with 1,082 samples (0.6 hours), and a test set with 1,243 samples (0.5 hours). It offers more than 2 million running words with a vocabulary size of around 40k different words. This corpus represents our ideal scenario, since our visual speech encoder was explicitly trained for this task.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">LRS3-TED</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Afouras et al., 2018b</a>]</cite> is the largest publicly audio-visual English database offering around 438 hours. It was collected from TED talks, consisting of a ‘pre-train’ set with 118,516 samples (407 hours), a ‘train-val’ set with 31,982 samples (30 hours), and a test set with 1,321 samples (0.9 hours). It comprises more than 4 million running words with a vocabulary size of around 50k different words. This corpus represents our domain-mismatch scenario.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">LIP-RTVE</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite> is a challenging Spanish database collected from TV newscast programs, providing around 13 hours of data. Its speaker-independent partition consists of a training set with 7,142 samples (9 hours), a validation set with 1638 samples (2 hours), and a test set with 1572 samples (2 hours). It provides more than 100k running words with a vocabulary size of around 10k different words. This corpus represents our language-mismatch scenario.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.2.   Visual Speech Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">A pre-trained encoder is used to extract our 256-dimensional visual speech features. As reflected in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2. Visual Speech Encoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the encoder is based on the state-of-the-art architecture designed by <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">?</span>), where two different blocks are distinguished.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Visual Frontend.</span> A 3D convolutional layer with a kernel size of 7x7 pixels and a receptive field of 5 frames is used to deal with spatial relationships. Once the video stream data is flattened along the temporal dimension, a 2D ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">He et al., 2016</a>]</cite> focuses on capturing local visual patterns. The Swish activation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx51" title="" class="ltx_ref">Ramachandran et al., 2017</a>]</cite> was used. This visual frontend comprises about 11 million parameters.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Temporal Encoder.</span> A 12-layer Conformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Gulati et al., 2020</a>]</cite> is defined to capture both global and local speech interactions across time from the previous visual latent representation. Each layer is composed of four modules, namely two feed forward networks in a macaron style, a multi-head self-attention module, and a convolution module. Layer normalization precedes each module, while a residual connection and a final dropout are applied over its output. The main difference w.r.t. the original Transformer encoder architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite> is the convolution module, which is able to model local temporal speech patterns by using point- and depth-wise convolutions. This temporal encoder comprises about 32 million parameters.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2402.13004/assets/vsr-encoder-arch.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="503" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall architecture of our visual speech encoder. For simplicity, the initial layer normalization, the residual connection, and the final dropout of each module that compose the conformer encoder are omitted. Conv and FFN refer to Convolutional layer and Feed Forward Network, respectively.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.3.   Conventional Hybrid Decoder</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Morphological Model.</span> The design of our conventional DNN-HMM decoder was based on the Wall Street Journal recipe<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/kaldi-asr/kaldi/tree/master/egs/wsj/s5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kaldi-asr/kaldi/tree/master/egs/wsj/s5</a></span></span></span> provided by the Kaldi toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx47" title="" class="ltx_ref">Povey et al., 2011</a>]</cite>. First of all, we estimated a preliminary GMM-HMM to obtain temporal alignments. Then, we applied the so-called HiLDA technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">Potamianos et al., 2001</a>]</cite>, reducing our visual speech features to a 40-dimensional latent representation.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">Regarding our best DNN-HMM architecture, it consisted of two hidden layers of 1024 units, each followed by a Sigmoid activation function. We defined as input an 11-frame context window over the previous HiLDA features. The output layer dimension depended on the number of HMM-state labels defined by the preliminary GMM-HMM. Concretely: 3624, 3304, and 1968 HMM-state labels were defined for the LRS2-BBC, LRS3-TED, and LIP-RTVE corpora, respectively. Then, we estimated our DNN-HMM system based on a frame cross-entropy training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Hinton et al., 2012</a>]</cite>. In average terms, each DNN-HMM decoder comprised around 4 million parameters.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Another important aspect was the HMM’s topology. Due to the lower sample rate presented by visual cues compared to acoustic signals, our first experiments focused on the optimal HMM’s topology. By adding transitions and/or reducing the number of states, we found that, in all cases, a 3-state left-to-right topology with skip transitions to the final state was the best approach to fit the temporary nature of our visual data.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Lexicon Model.</span> For LRS2-BBC and LRS3-TED, we processed their corresponding training transcriptions with a phonemizer<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a href="www.github.com/Kyubyong/g2p" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.github.com/Kyubyong/g2p</a></span></span></span> based on the CMU pronunciation dictionary<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a href="www.speech.cs.cmu.edu/cgi-bin/cmudict" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.speech.cs.cmu.edu/cgi-bin/cmudict</a></span></span></span>. Similarly, for LIP-RTVE, we considered a phonemizer based on Spanish phonetic rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Quilis, 1997</a>]</cite>. However, the amount of training data of LIP-RTVE is not comparable to its English counterparts. For this reason, we used the text provided by the LIP-RTVE’s authors for the estimation of a language model, which offers around 80k phrases collected from different but contemporary TV newscasts<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>It comprises 1.5 million running words with a vocabulary size of around 45k different words</span></span></span>. Thus, a set of 39 and 24 phonemes were defined for English and Spanish, respectively. In both cases, the default <em id="S3.SS3.p4.1.2" class="ltx_emph ltx_font_italic">silence</em> phones of Kaldi were then included.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Language Model.</span> We used a 6-layer character-level Language Model (LM) based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite>. A more detailed description about this Transformer-based LM can be found in Subsection <a href="#S4.SS2" title="4.2. Transformer Language Model ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p">However, for this conventional paradigm, we applied an approach based on a combination of one-pass decoding and lattice re-scoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Lüscher et al., 2019</a>]</cite>. Consequently, we used an auxiliary n-gram language model for decoding before re-scoring with the Transformed-based LM. Hence, a 3-gram word-level LM was also estimated. For each English database, we used the transcriptions included in its corresponding training set, while for LIP-RTVE, we considered the aforementioned 80k phrases. The estimated n-gram LMs offered 100.5, 112.8, and 113.4 test perplexities, with 25, 16, and 193 out-of-vocabulary words, for LRS2-BBC, LRS3-TED, and LIP-RTVE, respectively.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.1" class="ltx_p"><span id="S3.SS3.p7.1.1" class="ltx_text ltx_font_bold">Decoding.</span> The decoder is defined as a weighted finite-state transducer integrating the morphological, lexicon, and language models. Readers are referred to <span id="S3.SS3.p7.1.2" class="ltx_text ltx_font_bold">?</span>) for more details.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.4.   CTC/Attention Decoder</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">Morphological Model.</span> This state-of-the-art approach was implemented using the ESPNet toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx63" title="" class="ltx_ref">Watanabe et al., 2018</a>]</cite>. Specifically, the decoder was composed of a 6-layer Transformer decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite> and a fully connected layer as the CTC-based decoding branch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Graves et al., 2006</a>]</cite>. By combining both paradigms, the model is able to adopt both the Markov assumptions of CTC (an aspect in harmony with the speech nature) and the flexibility of the non-sequential alignments provided by the attention-based decoder. As proposed by <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_bold">?</span>), the loss function is defined as follows:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\mathcal{L}=\alpha\log p_{ctc}(\textbf{y}|\textbf{x})+(1-\alpha)\log p_{attn}(\textbf{y}|\textbf{x})" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.5" xref="S3.E1.m1.3.3.5.cmml">ℒ</mi><mo id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">=</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">α</mi><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.4.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.4a" xref="S3.E1.m1.1.1.1.1.4.cmml">⁡</mo><msub id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.2.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.cmml">p</mi><mrow id="S3.E1.m1.1.1.1.1.4.2.3" xref="S3.E1.m1.1.1.1.1.4.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.3.2" xref="S3.E1.m1.1.1.1.1.4.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.1" xref="S3.E1.m1.1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3.3" xref="S3.E1.m1.1.1.1.1.4.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.1a" xref="S3.E1.m1.1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3.4" xref="S3.E1.m1.1.1.1.1.4.2.3.4.cmml">c</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2a.cmml">y</mtext><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3a.cmml">x</mtext></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">+</mo><mrow id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mn id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.2.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml">α</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.3.4" xref="S3.E1.m1.3.3.3.3.4.cmml"><mi id="S3.E1.m1.3.3.3.3.4.1" xref="S3.E1.m1.3.3.3.3.4.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.3.3.3.3.4a" xref="S3.E1.m1.3.3.3.3.4.cmml">⁡</mo><msub id="S3.E1.m1.3.3.3.3.4.2" xref="S3.E1.m1.3.3.3.3.4.2.cmml"><mi id="S3.E1.m1.3.3.3.3.4.2.2" xref="S3.E1.m1.3.3.3.3.4.2.2.cmml">p</mi><mrow id="S3.E1.m1.3.3.3.3.4.2.3" xref="S3.E1.m1.3.3.3.3.4.2.3.cmml"><mi id="S3.E1.m1.3.3.3.3.4.2.3.2" xref="S3.E1.m1.3.3.3.3.4.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.4.2.3.1" xref="S3.E1.m1.3.3.3.3.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.3.4.2.3.3" xref="S3.E1.m1.3.3.3.3.4.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.4.2.3.1a" xref="S3.E1.m1.3.3.3.3.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.3.4.2.3.4" xref="S3.E1.m1.3.3.3.3.4.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.4.2.3.1b" xref="S3.E1.m1.3.3.3.3.4.2.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.3.4.2.3.5" xref="S3.E1.m1.3.3.3.3.4.2.3.5.cmml">n</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.3a" xref="S3.E1.m1.3.3.3.3.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.1.2" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.3.3.2.1.1" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.3.3.3.3.2.1.1.2" xref="S3.E1.m1.3.3.3.3.2.1.1.2a.cmml">y</mtext><mo fence="false" id="S3.E1.m1.3.3.3.3.2.1.1.1" xref="S3.E1.m1.3.3.3.3.2.1.1.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.3.3.3.3.2.1.1.3" xref="S3.E1.m1.3.3.3.3.2.1.1.3a.cmml">x</mtext></mrow><mo stretchy="false" id="S3.E1.m1.3.3.3.3.2.1.3" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"></eq><ci id="S3.E1.m1.3.3.5.cmml" xref="S3.E1.m1.3.3.5">ℒ</ci><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><plus id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4"></plus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝛼</ci><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><log id="S3.E1.m1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.4.1"></log><apply id="S3.E1.m1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2">𝑝</ci><apply id="S3.E1.m1.1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3"><times id="S3.E1.m1.1.1.1.1.4.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.2">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.4">𝑐</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">y</mtext></ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">x</mtext></ci></apply></apply><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><times id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3"></times><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><minus id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">1</cn><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">𝛼</ci></apply><apply id="S3.E1.m1.3.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.3.4"><log id="S3.E1.m1.3.3.3.3.4.1.cmml" xref="S3.E1.m1.3.3.3.3.4.1"></log><apply id="S3.E1.m1.3.3.3.3.4.2.cmml" xref="S3.E1.m1.3.3.3.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.4.2.1.cmml" xref="S3.E1.m1.3.3.3.3.4.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.4.2.2.cmml" xref="S3.E1.m1.3.3.3.3.4.2.2">𝑝</ci><apply id="S3.E1.m1.3.3.3.3.4.2.3.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3"><times id="S3.E1.m1.3.3.3.3.4.2.3.1.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3.1"></times><ci id="S3.E1.m1.3.3.3.3.4.2.3.2.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3.2">𝑎</ci><ci id="S3.E1.m1.3.3.3.3.4.2.3.3.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3.3">𝑡</ci><ci id="S3.E1.m1.3.3.3.3.4.2.3.4.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3.4">𝑡</ci><ci id="S3.E1.m1.3.3.3.3.4.2.3.5.cmml" xref="S3.E1.m1.3.3.3.3.4.2.3.5">𝑛</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.3.3.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.3.3.2.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.1">conditional</csymbol><ci id="S3.E1.m1.3.3.3.3.2.1.1.2a.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.3.3.3.3.2.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.2">y</mtext></ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.3a.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.3.3.3.3.2.1.1.3.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3">x</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathcal{L}=\alpha\log p_{ctc}(\textbf{y}|\textbf{x})+(1-\alpha)\log p_{attn}(\textbf{y}|\textbf{x})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.5" class="ltx_p">where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="p_{ctc}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">p</mi><mrow id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.3.1a" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3.4" xref="S3.SS4.p3.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑝</ci><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><times id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3">𝑡</ci><ci id="S3.SS4.p3.1.m1.1.1.3.4.cmml" xref="S3.SS4.p3.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">p_{ctc}</annotation></semantics></math> and <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="p_{attn}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><msub id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">p</mi><mrow id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.p3.2.m2.1.1.3.2" xref="S3.SS4.p3.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.2.m2.1.1.3.1" xref="S3.SS4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.2.m2.1.1.3.3" xref="S3.SS4.p3.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.2.m2.1.1.3.1a" xref="S3.SS4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.2.m2.1.1.3.4" xref="S3.SS4.p3.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.2.m2.1.1.3.1b" xref="S3.SS4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.2.m2.1.1.3.5" xref="S3.SS4.p3.2.m2.1.1.3.5.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝑝</ci><apply id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><times id="S3.SS4.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.p3.2.m2.1.1.3.1"></times><ci id="S3.SS4.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.p3.2.m2.1.1.3.2">𝑎</ci><ci id="S3.SS4.p3.2.m2.1.1.3.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3.3">𝑡</ci><ci id="S3.SS4.p3.2.m2.1.1.3.4.cmml" xref="S3.SS4.p3.2.m2.1.1.3.4">𝑡</ci><ci id="S3.SS4.p3.2.m2.1.1.3.5.cmml" xref="S3.SS4.p3.2.m2.1.1.3.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">p_{attn}</annotation></semantics></math> denote the CTC and Attention posteriors, respectively. In both terms, <span id="S3.SS4.p3.5.1" class="ltx_text ltx_markedasmath ltx_font_bold">x</span> and <span id="S3.SS4.p3.5.2" class="ltx_text ltx_markedasmath ltx_font_bold">y</span> refer to the input visual stream and its corresponding character-level target, respectively. The <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">\alpha</annotation></semantics></math> weight balances the relative influence of each decoder.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p">It should be noted that this end-to-end approach is based on a character-level speech recognition. For LRS2-BBC and LRS3-TED, we considered a set of 41 characters, while for LIP-RTVE we used a set of 37 characters. In both cases, special characters were included, such as the ‘space’ and the ‘blank’ symbols. On average, each CTC/Attention decoder comprised around 9.5 million parameters.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.p5.1" class="ltx_p"><span id="S3.SS4.p5.1.1" class="ltx_text ltx_font_bold">Language Model.</span> We used a 6-layer character-level LM based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite>. More details about how it was estimated are found in Subsection <a href="#S4.SS2" title="4.2. Transformer Language Model ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS4.p6.1" class="ltx_p"><span id="S3.SS4.p6.1.1" class="ltx_text ltx_font_bold">Decoding.</span> The decoder integrates the attention- and CTC-based branches and the Transformer-based LM in a beam search process. Albeit it is the attention-based branch that leads this decoding process until predicting the end-of-sentence token, the rest of the components influence the search in a shallow fusion manner, as reflected in:</p>
</div>
<div id="S3.SS4.p7" class="ltx_para ltx_noindent">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="S=\lambda S_{ctc}+(1-\lambda)S_{attn}+\beta S_{lm}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">S</mi><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">​</mo><msub id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.3.3.2.cmml">S</mi><mrow id="S3.E2.m1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.3.3.1" xref="S3.E2.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.3.3.1a" xref="S3.E2.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.3.3.4" xref="S3.E2.m1.1.1.1.3.3.3.4.cmml">c</mi></mrow></msub></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">λ</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">​</mo><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">S</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1a" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.4" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1b" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.5" xref="S3.E2.m1.1.1.1.1.3.3.5.cmml">n</mi></mrow></msub></mrow><mo id="S3.E2.m1.1.1.1.2a" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.4" xref="S3.E2.m1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.4.2.cmml">β</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.4.1" xref="S3.E2.m1.1.1.1.4.1.cmml">​</mo><msub id="S3.E2.m1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.4.3.cmml"><mi id="S3.E2.m1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.4.3.2.cmml">S</mi><mrow id="S3.E2.m1.1.1.1.4.3.3" xref="S3.E2.m1.1.1.1.4.3.3.cmml"><mi id="S3.E2.m1.1.1.1.4.3.3.2" xref="S3.E2.m1.1.1.1.4.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.4.3.3.1" xref="S3.E2.m1.1.1.1.4.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.4.3.3.3" xref="S3.E2.m1.1.1.1.4.3.3.3.cmml">m</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝑆</ci><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><plus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><times id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">𝜆</ci><apply id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.2">𝑆</ci><apply id="S3.E2.m1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3.3"><times id="S3.E2.m1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3.3.1"></times><ci id="S3.E2.m1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3.3.3">𝑡</ci><ci id="S3.E2.m1.1.1.1.3.3.3.4.cmml" xref="S3.E2.m1.1.1.1.3.3.3.4">𝑐</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">𝜆</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝑆</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝑎</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">𝑡</ci><ci id="S3.E2.m1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.3.4">𝑡</ci><ci id="S3.E2.m1.1.1.1.1.3.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.3.5">𝑛</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.4"><times id="S3.E2.m1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.4.1"></times><ci id="S3.E2.m1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.4.2">𝛽</ci><apply id="S3.E2.m1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.4.3.1.cmml" xref="S3.E2.m1.1.1.1.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.4.3.2.cmml" xref="S3.E2.m1.1.1.1.4.3.2">𝑆</ci><apply id="S3.E2.m1.1.1.1.4.3.3.cmml" xref="S3.E2.m1.1.1.1.4.3.3"><times id="S3.E2.m1.1.1.1.4.3.3.1.cmml" xref="S3.E2.m1.1.1.1.4.3.3.1"></times><ci id="S3.E2.m1.1.1.1.4.3.3.2.cmml" xref="S3.E2.m1.1.1.1.4.3.3.2">𝑙</ci><ci id="S3.E2.m1.1.1.1.4.3.3.3.cmml" xref="S3.E2.m1.1.1.1.4.3.3.3">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">S=\lambda S_{ctc}+(1-\lambda)S_{attn}+\beta S_{lm}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p8" class="ltx_para ltx_noindent">
<p id="S3.SS4.p8.5" class="ltx_p">where <math id="S3.SS4.p8.1.m1.1" class="ltx_Math" alttext="S_{ctc}" display="inline"><semantics id="S3.SS4.p8.1.m1.1a"><msub id="S3.SS4.p8.1.m1.1.1" xref="S3.SS4.p8.1.m1.1.1.cmml"><mi id="S3.SS4.p8.1.m1.1.1.2" xref="S3.SS4.p8.1.m1.1.1.2.cmml">S</mi><mrow id="S3.SS4.p8.1.m1.1.1.3" xref="S3.SS4.p8.1.m1.1.1.3.cmml"><mi id="S3.SS4.p8.1.m1.1.1.3.2" xref="S3.SS4.p8.1.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.1.m1.1.1.3.1" xref="S3.SS4.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.1.m1.1.1.3.3" xref="S3.SS4.p8.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.1.m1.1.1.3.1a" xref="S3.SS4.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.1.m1.1.1.3.4" xref="S3.SS4.p8.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.1.m1.1b"><apply id="S3.SS4.p8.1.m1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.1.m1.1.1.1.cmml" xref="S3.SS4.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p8.1.m1.1.1.2.cmml" xref="S3.SS4.p8.1.m1.1.1.2">𝑆</ci><apply id="S3.SS4.p8.1.m1.1.1.3.cmml" xref="S3.SS4.p8.1.m1.1.1.3"><times id="S3.SS4.p8.1.m1.1.1.3.1.cmml" xref="S3.SS4.p8.1.m1.1.1.3.1"></times><ci id="S3.SS4.p8.1.m1.1.1.3.2.cmml" xref="S3.SS4.p8.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS4.p8.1.m1.1.1.3.3.cmml" xref="S3.SS4.p8.1.m1.1.1.3.3">𝑡</ci><ci id="S3.SS4.p8.1.m1.1.1.3.4.cmml" xref="S3.SS4.p8.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.1.m1.1c">S_{ctc}</annotation></semantics></math> and <math id="S3.SS4.p8.2.m2.1" class="ltx_Math" alttext="S_{attn}" display="inline"><semantics id="S3.SS4.p8.2.m2.1a"><msub id="S3.SS4.p8.2.m2.1.1" xref="S3.SS4.p8.2.m2.1.1.cmml"><mi id="S3.SS4.p8.2.m2.1.1.2" xref="S3.SS4.p8.2.m2.1.1.2.cmml">S</mi><mrow id="S3.SS4.p8.2.m2.1.1.3" xref="S3.SS4.p8.2.m2.1.1.3.cmml"><mi id="S3.SS4.p8.2.m2.1.1.3.2" xref="S3.SS4.p8.2.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.2.m2.1.1.3.1" xref="S3.SS4.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.2.m2.1.1.3.3" xref="S3.SS4.p8.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.2.m2.1.1.3.1a" xref="S3.SS4.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.2.m2.1.1.3.4" xref="S3.SS4.p8.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.2.m2.1.1.3.1b" xref="S3.SS4.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.2.m2.1.1.3.5" xref="S3.SS4.p8.2.m2.1.1.3.5.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.2.m2.1b"><apply id="S3.SS4.p8.2.m2.1.1.cmml" xref="S3.SS4.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.2.m2.1.1.1.cmml" xref="S3.SS4.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p8.2.m2.1.1.2.cmml" xref="S3.SS4.p8.2.m2.1.1.2">𝑆</ci><apply id="S3.SS4.p8.2.m2.1.1.3.cmml" xref="S3.SS4.p8.2.m2.1.1.3"><times id="S3.SS4.p8.2.m2.1.1.3.1.cmml" xref="S3.SS4.p8.2.m2.1.1.3.1"></times><ci id="S3.SS4.p8.2.m2.1.1.3.2.cmml" xref="S3.SS4.p8.2.m2.1.1.3.2">𝑎</ci><ci id="S3.SS4.p8.2.m2.1.1.3.3.cmml" xref="S3.SS4.p8.2.m2.1.1.3.3">𝑡</ci><ci id="S3.SS4.p8.2.m2.1.1.3.4.cmml" xref="S3.SS4.p8.2.m2.1.1.3.4">𝑡</ci><ci id="S3.SS4.p8.2.m2.1.1.3.5.cmml" xref="S3.SS4.p8.2.m2.1.1.3.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.2.m2.1c">S_{attn}</annotation></semantics></math> are the scores of the CTC and the Attention decoder, respectively, <math id="S3.SS4.p8.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.p8.3.m3.1a"><mi id="S3.SS4.p8.3.m3.1.1" xref="S3.SS4.p8.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.3.m3.1b"><ci id="S3.SS4.p8.3.m3.1.1.cmml" xref="S3.SS4.p8.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.3.m3.1c">\lambda</annotation></semantics></math> is their corresponding relative weight, and <math id="S3.SS4.p8.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS4.p8.4.m4.1a"><mi id="S3.SS4.p8.4.m4.1.1" xref="S3.SS4.p8.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.4.m4.1b"><ci id="S3.SS4.p8.4.m4.1.1.cmml" xref="S3.SS4.p8.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.4.m4.1c">\beta</annotation></semantics></math> and <math id="S3.SS4.p8.5.m5.1" class="ltx_Math" alttext="S_{lm}" display="inline"><semantics id="S3.SS4.p8.5.m5.1a"><msub id="S3.SS4.p8.5.m5.1.1" xref="S3.SS4.p8.5.m5.1.1.cmml"><mi id="S3.SS4.p8.5.m5.1.1.2" xref="S3.SS4.p8.5.m5.1.1.2.cmml">S</mi><mrow id="S3.SS4.p8.5.m5.1.1.3" xref="S3.SS4.p8.5.m5.1.1.3.cmml"><mi id="S3.SS4.p8.5.m5.1.1.3.2" xref="S3.SS4.p8.5.m5.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p8.5.m5.1.1.3.1" xref="S3.SS4.p8.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p8.5.m5.1.1.3.3" xref="S3.SS4.p8.5.m5.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p8.5.m5.1b"><apply id="S3.SS4.p8.5.m5.1.1.cmml" xref="S3.SS4.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p8.5.m5.1.1.1.cmml" xref="S3.SS4.p8.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p8.5.m5.1.1.2.cmml" xref="S3.SS4.p8.5.m5.1.1.2">𝑆</ci><apply id="S3.SS4.p8.5.m5.1.1.3.cmml" xref="S3.SS4.p8.5.m5.1.1.3"><times id="S3.SS4.p8.5.m5.1.1.3.1.cmml" xref="S3.SS4.p8.5.m5.1.1.3.1"></times><ci id="S3.SS4.p8.5.m5.1.1.3.2.cmml" xref="S3.SS4.p8.5.m5.1.1.3.2">𝑙</ci><ci id="S3.SS4.p8.5.m5.1.1.3.3.cmml" xref="S3.SS4.p8.5.m5.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p8.5.m5.1c">S_{lm}</annotation></semantics></math> refer to the LM influence weight and the LM score, respectively. Readers are referred to <span id="S3.SS4.p8.5.1" class="ltx_text ltx_font_bold">?</span>) for a more detailed description.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Experimental Setup</h2>

<figure id="S4.F2" class="ltx_figure"><svg id="S4.F2.pic1" class="ltx_picture ltx_centering" height="249.33" overflow="visible" version="1.1" width="436.22"><g transform="translate(0,249.33) matrix(1 0 0 -1 0 0) translate(35.63,0) translate(0,28.15) matrix(0.7 0.0 0.0 0.7 -35.63 -28.15)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(98.21,0) translate(0,66.53)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -47.3 -26.31 L -47.3 -20.4 M 0 -26.31 L 0 -20.4 M 47.3 -26.31 L 47.3 -20.4 M 94.61 -26.31 L 94.61 -20.4 M 141.91 -26.31 L 141.91 -20.4 M 189.22 -26.31 L 189.22 -20.4 M 236.52 -26.31 L 236.52 -20.4 M 283.82 -26.31 L 283.82 -20.4 M 331.13 -26.31 L 331.13 -20.4 M 378.43 -26.31 L 378.43 -20.4 M 425.74 -26.31 L 425.74 -20.4 M 473.04 -26.31 L 473.04 -20.4 M 520.35 -26.31 L 520.35 -20.4 M -47.3 289.38 L -47.3 283.47 M 0 289.38 L 0 283.47 M 47.3 289.38 L 47.3 283.47 M 94.61 289.38 L 94.61 283.47 M 141.91 289.38 L 141.91 283.47 M 189.22 289.38 L 189.22 283.47 M 236.52 289.38 L 236.52 283.47 M 283.82 289.38 L 283.82 283.47 M 331.13 289.38 L 331.13 283.47 M 378.43 289.38 L 378.43 283.47 M 425.74 289.38 L 425.74 283.47 M 473.04 289.38 L 473.04 283.47 M 520.35 289.38 L 520.35 283.47" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -47.31 -23.26 L -41.4 -23.26 M -47.31 12.53 L -41.4 12.53 M -47.31 48.32 L -41.4 48.32 M -47.31 84.11 L -41.4 84.11 M -47.31 119.9 L -41.4 119.9 M -47.31 155.69 L -41.4 155.69 M -47.31 191.49 L -41.4 191.49 M -47.31 227.28 L -41.4 227.28 M -47.31 263.07 L -41.4 263.07 M 520.35 -23.26 L 514.44 -23.26 M 520.35 12.53 L 514.44 12.53 M 520.35 48.32 L 514.44 48.32 M 520.35 84.11 L 514.44 84.11 M 520.35 119.9 L 514.44 119.9 M 520.35 155.69 L 514.44 155.69 M 520.35 191.49 L 514.44 191.49 M 520.35 227.28 L 514.44 227.28 M 520.35 263.07 L 514.44 263.07" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -47.31 -26.31 L -47.31 289.38 L 520.35 289.38 L 520.35 -26.31 L -47.31 -26.31 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.10.10.10.10.10.1.1" class="ltx_text">1</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 43.84 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.11.11.11.11.11.1.1" class="ltx_text">2</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 91.15 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.12.12.12.12.12.1.1" class="ltx_text">5</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 138.45 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.13.13.13.13.13.1.1" class="ltx_text">9</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 182.3 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.14.14.14.14.14.1.1" class="ltx_text">10</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 229.6 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.15.15.15.15.15.1.1" class="ltx_text">20</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 276.91 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.16.16.16.16.16.1.1" class="ltx_text">50</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 320.75 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.17.17.17.17.17.1.1" class="ltx_text">100</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 368.06 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.18.18.18.18.18.1.1" class="ltx_text">200</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 415.36 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.19.19.19.19.19.1.1" class="ltx_text">223</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 462.66 -40.11)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.20.20.20.20.20.1.1" class="ltx_text">437</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 -27.72)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 8.07)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 43.86)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 79.65)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 115.44)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 151.24)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">70</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 187.03)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">80</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -66.03 222.82)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">90</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -72.95 258.61)" fill="#000000" stroke="#000000"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><math id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">100</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M -47.31 -26.31 L 520.35 -26.31 L 520.35 289.38 L -47.31 289.38 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke-width="0.8pt" fill="#FF0000" stroke="#FF0000" color="#FF0000"><path d="M 0 184.69 L 47.3 182.54 L 94.61 147.82 L 141.91 142.09" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 184.69 L 0 187.91" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 2.77 185.6)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 184.69 L 0 181.46" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -2.77 183.77)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 182.54 L 47.3 186.12" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 50.07 183.81)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 182.54 L 47.3 178.96" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 44.54 181.26)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 147.82 L 94.61 151.76" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 97.38 149.45)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 147.82 L 94.61 143.88" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 91.84 146.19)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 141.91 142.09 L 141.91 146.03" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 144.68 143.72)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 141.91 142.09 L 141.91 138.16" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 139.15 140.46)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g><g stroke-width="0.8pt" fill="#FF0000" stroke="#FF0000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#FF0000"><path d="M 0 263.07 L 47.3 263.07 L 94.61 263.07 L 141.91 223.7" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 263.07 L 47.3 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 45 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 263.07 L 47.3 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 45 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 263.07 L 94.61 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 92.3 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 263.07 L 94.61 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 92.3 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 141.91 223.7 L 141.91 227.99" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 144.68 225.69)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 141.91 223.7 L 141.91 219.4" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 139.15 221.71)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#FF0000" stroke="#FF0000" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g><g stroke-width="0.8pt" fill="#0000FF" stroke="#0000FF" color="#0000FF"><path d="M 0 158.2 L 47.3 145.31 L 94.61 117.75 L 189.22 110.6 L 236.52 104.51 L 283.82 101.29 L 331.13 100.57 L 378.43 97.35 L 473.04 96.64" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 158.2 L 0 162.14" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 2.77 159.83)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 158.2 L 0 154.26" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -2.77 156.57)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 145.31 L 47.3 149.61" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 50.07 147.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 145.31 L 47.3 141.02" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 44.54 143.33)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 117.75 L 94.61 122.41" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 97.38 120.1)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 117.75 L 94.61 113.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 91.84 115.41)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 110.6 L 189.22 115.61" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 191.98 113.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 110.6 L 189.22 105.59" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 186.45 107.89)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 104.51 L 236.52 109.88" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 239.29 107.57)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 104.51 L 236.52 99.14" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 233.75 101.45)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 101.29 L 283.82 106.3" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 286.59 104)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 101.29 L 283.82 96.28" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 281.06 98.59)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 100.57 L 331.13 105.59" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 333.9 103.28)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 100.57 L 331.13 95.56" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 328.36 97.87)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 378.43 97.35 L 378.43 102.36" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 381.2 100.06)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 378.43 97.35 L 378.43 92.34" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 375.67 94.65)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 473.04 96.64 L 473.04 102.01" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 475.81 99.7)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 473.04 96.64 L 473.04 91.27" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 470.27 93.58)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g><g stroke-width="0.8pt" fill="#0000FF" stroke="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M 0 263.07 L 47.3 263.07 L 94.61 255.91 L 189.22 145.67 L 236.52 115.61 L 283.82 95.21 L 331.13 87.33 L 378.43 85.18 L 473.04 84.11" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 263.07 L 47.3 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 45 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 263.07 L 47.3 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 45 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 255.91 L 94.61 262.35" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 97.38 260.05)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 255.91 L 94.61 249.47" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 91.84 251.77)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 145.67 L 189.22 151.76" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 191.98 149.45)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 145.67 L 189.22 139.59" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 186.45 141.89)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 115.61 L 236.52 121.69" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 239.29 119.39)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 115.61 L 236.52 109.52" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 233.75 111.83)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 95.21 L 283.82 101.65" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 286.59 99.34)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 95.21 L 283.82 88.76" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 281.06 91.07)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 87.33 L 331.13 93.77" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 333.9 91.47)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 87.33 L 331.13 80.89" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 328.36 83.2)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 378.43 85.18 L 378.43 91.27" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 381.2 88.96)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 378.43 85.18 L 378.43 79.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 375.67 81.41)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 473.04 84.11 L 473.04 90.2" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 475.81 87.89)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 473.04 84.11 L 473.04 78.03" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 470.27 80.33)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#0000FF" stroke="#0000FF" stroke-width="0.4pt"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g><g stroke-width="0.8pt" fill="#333333" stroke="#333333" color="#333333"><path d="M 0 42.59 L 47.3 31.5 L 94.61 19.33 L 189.22 17.9 L 236.52 17.9 L 283.82 16.46 L 331.13 13.24 L 425.74 11.81" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 42.59 L 0 47.96" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 2.77 45.65)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 42.59 L 0 37.22" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 -2.77 39.53)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 31.5 L 47.3 36.51" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 50.07 34.2)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 31.5 L 47.3 26.49" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 44.54 28.79)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 19.33 L 94.61 24.7" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 97.38 22.39)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 19.33 L 94.61 13.96" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 91.84 16.26)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 17.9 L 189.22 23.26" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 191.98 20.96)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 17.9 L 189.22 12.53" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 186.45 14.83)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 17.9 L 236.52 23.26" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 239.29 20.96)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 17.9 L 236.52 12.53" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 233.75 14.83)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 16.46 L 283.82 21.83" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 286.59 19.53)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 16.46 L 283.82 11.1" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 281.06 13.4)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 13.24 L 331.13 18.25" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 333.9 15.95)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 13.24 L 331.13 8.23" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 328.36 10.54)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 425.74 11.81 L 425.74 17.18" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 428.5 14.87)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 425.74 11.81 L 425.74 6.44" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 422.97 8.75)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g><g stroke-width="0.8pt" fill="#333333" stroke="#333333" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#333333"><path d="M 0 263.07 L 47.3 217.26 L 94.61 30.78 L 189.22 18.61 L 236.52 15.03 L 283.82 10.02 L 331.13 7.87 L 425.74 5.37" style="fill:none"></path><g stroke-width="0.4pt"><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 0 263.07 L 0 263.07" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.31 260.3)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 217.26 L 47.3 221.91" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 50.07 219.6)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 47.3 217.26 L 47.3 212.6" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 44.54 214.91)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 30.78 L 94.61 36.51" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 97.38 34.2)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 94.61 30.78 L 94.61 25.05" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 91.84 27.36)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 18.61 L 189.22 24.34" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 191.98 22.03)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 189.22 18.61 L 189.22 12.89" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 186.45 15.19)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 15.03 L 236.52 20.4" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 239.29 18.1)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 236.52 15.03 L 236.52 9.66" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 233.75 11.97)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 10.02 L 283.82 15.39" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 286.59 13.08)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 283.82 10.02 L 283.82 4.65" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 281.06 6.96)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 7.87 L 331.13 12.89" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 333.9 10.58)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 331.13 7.87 L 331.13 2.86" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 328.36 5.17)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 425.74 5.37 L 425.74 10.74" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 428.5 8.43)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g><path d="M 425.74 5.37 L 425.74 0" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 422.97 2.31)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(0,2.77)" fill="#333333" stroke="#333333" stroke-width="0.4pt" color="#333333"><path d="M 0 2.77 L 0 -2.77" style="fill:none"></path></g></g></g></g><g></g></g><g stroke-width="0.8pt" fill="#FF0000" stroke="#FF0000" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#FF0000"><path d="M -1.96 182.73 L 1.96 186.64 M -1.96 186.64 L 1.96 182.73" style="fill:none"></path><path d="M 45.35 180.58 L 49.26 184.49 M 45.35 184.49 L 49.26 180.58" style="fill:none"></path><path d="M 92.65 145.86 L 96.57 149.78 M 92.65 149.78 L 96.57 145.86" style="fill:none"></path><path d="M 139.96 140.14 L 143.87 144.05 M 139.96 144.05 L 143.87 140.14" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#FF0000" stroke="#FF0000" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#FF0000"><path d="M -1.96 261.11 L 1.96 265.03 M -1.96 265.03 L 1.96 261.11" style="fill:none"></path><path d="M 45.35 261.11 L 49.26 265.03 M 45.35 265.03 L 49.26 261.11" style="fill:none"></path><path d="M 92.65 261.11 L 96.57 265.03 M 92.65 265.03 L 96.57 261.11" style="fill:none"></path><path d="M 139.96 221.74 L 143.87 225.66 M 139.96 225.66 L 143.87 221.74" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#0000FF" stroke="#0000FF" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M -1.96 156.24 L 1.96 160.16 M -1.96 160.16 L 1.96 156.24" style="fill:none"></path><path d="M 45.35 143.36 L 49.26 147.27 M 45.35 147.27 L 49.26 143.36" style="fill:none"></path><path d="M 92.65 115.8 L 96.57 119.71 M 92.65 119.71 L 96.57 115.8" style="fill:none"></path><path d="M 187.26 108.64 L 191.17 112.55 M 187.26 112.55 L 191.17 108.64" style="fill:none"></path><path d="M 234.56 102.56 L 238.48 106.47 M 234.56 106.47 L 238.48 102.56" style="fill:none"></path><path d="M 281.87 99.33 L 285.78 103.25 M 281.87 103.25 L 285.78 99.33" style="fill:none"></path><path d="M 329.17 98.62 L 333.09 102.53 M 329.17 102.53 L 333.09 98.62" style="fill:none"></path><path d="M 376.48 95.4 L 380.39 99.31 M 376.48 99.31 L 380.39 95.4" style="fill:none"></path><path d="M 471.08 94.68 L 475 98.59 M 471.08 98.59 L 475 94.68" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#0000FF" stroke="#0000FF" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#0000FF"><path d="M -1.96 261.11 L 1.96 265.03 M -1.96 265.03 L 1.96 261.11" style="fill:none"></path><path d="M 45.35 261.11 L 49.26 265.03 M 45.35 265.03 L 49.26 261.11" style="fill:none"></path><path d="M 92.65 253.95 L 96.57 257.87 M 92.65 257.87 L 96.57 253.95" style="fill:none"></path><path d="M 187.26 143.72 L 191.17 147.63 M 187.26 147.63 L 191.17 143.72" style="fill:none"></path><path d="M 234.56 113.65 L 238.48 117.56 M 234.56 117.56 L 238.48 113.65" style="fill:none"></path><path d="M 281.87 93.25 L 285.78 97.16 M 281.87 97.16 L 285.78 93.25" style="fill:none"></path><path d="M 329.17 85.38 L 333.09 89.29 M 329.17 89.29 L 333.09 85.38" style="fill:none"></path><path d="M 376.48 83.23 L 380.39 87.14 M 376.48 87.14 L 380.39 83.23" style="fill:none"></path><path d="M 471.08 82.15 L 475 86.07 M 471.08 86.07 L 475 82.15" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#333333" stroke="#333333" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#333333"><path d="M -1.96 40.64 L 1.96 44.55 M -1.96 44.55 L 1.96 40.64" style="fill:none"></path><path d="M 45.35 29.54 L 49.26 33.45 M 45.35 33.45 L 49.26 29.54" style="fill:none"></path><path d="M 92.65 17.37 L 96.57 21.28 M 92.65 21.28 L 96.57 17.37" style="fill:none"></path><path d="M 187.26 15.94 L 191.17 19.85 M 187.26 19.85 L 191.17 15.94" style="fill:none"></path><path d="M 234.56 15.94 L 238.48 19.85 M 234.56 19.85 L 238.48 15.94" style="fill:none"></path><path d="M 281.87 14.51 L 285.78 18.42 M 281.87 18.42 L 285.78 14.51" style="fill:none"></path><path d="M 329.17 11.29 L 333.09 15.2 M 329.17 15.2 L 333.09 11.29" style="fill:none"></path><path d="M 423.78 9.85 L 427.69 13.77 M 423.78 13.77 L 427.69 9.85" style="fill:none"></path></g><g stroke-width="0.8pt" fill="#333333" stroke="#333333" stroke-dasharray="none" stroke-dashoffset="0.0pt" color="#333333"><path d="M -1.96 261.11 L 1.96 265.03 M -1.96 265.03 L 1.96 261.11" style="fill:none"></path><path d="M 45.35 215.3 L 49.26 219.21 M 45.35 219.21 L 49.26 215.3" style="fill:none"></path><path d="M 92.65 28.82 L 96.57 32.74 M 92.65 32.74 L 96.57 28.82" style="fill:none"></path><path d="M 187.26 16.65 L 191.17 20.57 M 187.26 20.57 L 191.17 16.65" style="fill:none"></path><path d="M 234.56 13.08 L 238.48 16.99 M 234.56 16.99 L 238.48 13.08" style="fill:none"></path><path d="M 281.87 8.06 L 285.78 11.98 M 281.87 11.98 L 285.78 8.06" style="fill:none"></path><path d="M 329.17 5.92 L 333.09 9.83 M 329.17 9.83 L 333.09 5.92" style="fill:none"></path><path d="M 423.78 3.41 L 427.69 7.33 M 423.78 7.33 L 427.69 3.41" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 165.63 -59.22)" fill="#000000" stroke="#000000"><foreignObject width="142.94" height="12.3" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.21.21.21.21.21.1.1" class="ltx_text ltx_font_bold">Hours of Training Data</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -83.22 106.55)" fill="#000000" stroke="#000000"><foreignObject width="49.97" height="11.15" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.22.22.22.22.22.1.1" class="ltx_text ltx_font_bold">% WER</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 291.53 169.32 h 217.19 v 113.46 h -217.19 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 295.68 172.09)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 98.935)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="153.55" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.23.23.23.23.23.1.1.1.1.1" class="ltx_text">LIP-RTVE (DNN-HMM)</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.98)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#FF0000" stroke="#FF0000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="176.81" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.24.24.24.24.24.2.2.1.1.1" class="ltx_text">LIP-RTVE (CTC/Attention)</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 44.97)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="155.09" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.25.25.25.25.25.3.3.1.1.1" class="ltx_text">LRS3-TED (DNN-HMM)</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 62.95)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#0000FF" stroke="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="178.34" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.26.26.26.26.26.4.4.1.1.1" class="ltx_text">LRS3-TED (CTC/Attention)</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 80.94)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#333333" stroke="#333333" stroke-width="0.8pt" color="#333333"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="154.71" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.27.27.27.27.27.5.5.1.1.1" class="ltx_text">LRS2-BBC (DNN-HMM)</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 98.93)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#333333" stroke="#333333" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" color="#333333"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt"><path d="M 9.85 -1.96 L 13.77 1.96 M 9.85 1.96 L 13.77 -1.96" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="177.96" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible"><span id="S4.F2.pic1.28.28.28.28.28.6.6.1.1.1" class="ltx_text">LRS2-BBC (CTC/Attention)</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison in terms of performance (% WER) of the DNN-HMM and the CTC/Attention decoders based on the number of hours used to estimate both paradigms. The 9, 223, and 437 hours refers to the entire training set of the LIP-RTVE, LRS2-BBC, and LRS3-TED databases, respectively.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Experiments were conducted on a 12-core 3.50GHz Intel i7-7800X CPU and a GeForce RTX 2080 GPU with 8GB memory.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.1.   Visual Speech Encoder</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">In most of our experiments, the visual speech encoder used the weights publicly released by <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">?</span>) for the LRS2-BBC database, where more than 1000 hours of data from different databases (including the LRS3-TED corpus) were considered. Only in the case of the LIP-RTVE database, due to language mismatch, the encoder was fine-tuned by assembling the LRS2-BBC encoder and its corresponding CTC/Attention decoder pre-trained with the weights publicly released by <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">?</span>) for the Spanish language. In order to represent the situation in a data-scarcity scenario, we only used 1 hour of data from the LIP-RTVE training set. Implementation details about this encoder fine-tuning process can be found in Subsection <a href="#S4.SS3" title="4.3. Training Process ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.2.   Transformer Language Model</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In our experiments, our Transformer-based LM (used both for the DNN-HMM lattice re-scoring and the CTC/Attention decoder) was pre-trained using the weights publicly released by Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ma et al., 2022</a>]</cite> for both the English and Spanish language. Each of them was estimated using millions of characters collected from different databases corresponding to the language addressed. Nonetheless, it should be noted that, for the English LM, the transcriptions from the training sets of the LRS2-BBC and LRS3-TED databases were also considered. Therefore, to conduct a fair comparison, we fine-tuned the Spanish LM to the LIP-RTVE database using the 80k phrases provided by the original authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite>. Details on this LM fine-tuning process are described in Subsection <a href="#S4.SS3" title="4.3. Training Process ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>. Considering the same character vocabularies described in Subsection <a href="#S3.SS4" title="3.4. CTC/Attention Decoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, our LMs presented a character-level perplexity of around 3.0 for the corresponding test set of all the proposed databases. Each LM comprises around 50 million parameters.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.3.   Training Process</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Data Sets:</span> the official splits were kept, with slight variations for the English corpora. For the LRS2-BBC, the pre-training and training sets were condensed into one training set, comprising a total of 223 hours. Similarly, the ‘pre-train’ and ‘train-val’ sets from the LRS3-TED were used as a 437-hours training set. In both cases, utterances with more than 600 frames were excluded, as considered by <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Conventional Hybrid Decoder.</span> Although we explored different training setups, the toolkit’s default settings specified in Karel’s DNN-HMM implementation<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/steps/nnet/train.sh" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/steps/nnet/train.sh</a></span></span></span> provided the best recognition rates.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.2" class="ltx_p"><span id="S4.SS3.p3.2.1" class="ltx_text ltx_font_bold">CTC/Attention Decoder.</span> In all our experiments, we considered the settings specified by <span id="S4.SS3.p3.2.2" class="ltx_text ltx_font_bold">?</span>). Concretely, we used the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Kingma and Ba, 2014</a>]</cite> and the Noam scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite> with 25,000 warmup steps during 50 epochs with a batch size of 16 samples, yielding a peak learning rate of 4<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mo id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><times id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\times</annotation></semantics></math>10<sup id="S4.SS3.p3.2.3" class="ltx_sup">-4</sup>. Regarding the CTC/Attention balance, we set the <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">\alpha</annotation></semantics></math> weight of Equation <a href="#S3.E1" title="In 3.4. CTC/Attention Decoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to 0.1.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Fine-Tuning Settings.</span> The LM and the visual speech encoder were fine-tuned when addressing the LIP-RTVE database. In both cases, the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">I.Loshchilov and Hutter, 2019</a>]</cite> and a linear one-cycle scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">Smith and Topin, 2019</a>]</cite> were used during 5 epochs, yielding a peak learning rate of 5<math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mo id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><times id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">\times</annotation></semantics></math>10<sup id="S4.SS3.p4.1.2" class="ltx_sup">-5</sup>. Due to our memory limitations, the batch size was set to 1 sample. We explored the accumulating gradient strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Ott et al., 2018</a>]</cite>, but no significant differences were found, possibly because the normalization layers were still affected by the actual reduced batch size.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">4.4.   Inference Process</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Conventional Hybrid Decoder.</span> As considered by <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_bold">?</span>), we applied an approach based on a combination of one-pass decoding and lattice re-scoring. First of all, with a beam size of 18, we explored word insertion penalties from -5.0 to 5.0 and LM scales from 1 to 20. Once the best setting was determined, a lattice composed of the best 100 hypothesis for each test sample was obtained using the 3-gram word-level LM. Afterwards, the lattice was re-scored using the Transformer-based LM. In all the cases, the visual speech decoder was scaled by a factor of 0.1.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.2" class="ltx_p"><span id="S4.SS4.p2.2.1" class="ltx_text ltx_font_bold">CTC/Attention Decoder.</span> As considered by <span id="S4.SS4.p2.2.2" class="ltx_text ltx_font_bold">?</span>), we incorporated the Transformer-based LM in a shallow fusion manner. According to Equation <a href="#S3.E2" title="In 3.4. CTC/Attention Decoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we set the <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\beta</annotation></semantics></math> weight to 0.6 and 0.4 for English and Spanish, respectively. For English, we set a word insertion penalty of 0.5 and a beam size of 40, while for Spanish, we set the word insertion penalty and the beam size to 0.0 and 30, respectively. Regarding the CTC/Attention balance, we set the <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\lambda</annotation></semantics></math> weight of Equation <a href="#S3.E2" title="In 3.4. CTC/Attention Decoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to 0.1. It should be noted that we used the model averaged over the last 10 training epochs.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Evaluation Metric:</span> Experiment results were reported in terms of the well-known Word Error Rate (WER) with 95% confidence intervals using the method described by <span id="S4.SS4.p3.1.2" class="ltx_text ltx_font_bold">?</span>).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results &amp; Discussion</h2>

<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.16" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:242.2pt;height:165pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T1.16.16" class="ltx_p"><span id="S5.T1.16.16.16" class="ltx_text">
<span id="S5.T1.16.16.16.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T1.16.16.16.16.17.1" class="ltx_tr">
<span id="S5.T1.16.16.16.16.17.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S5.T1.16.16.16.16.17.1.1.1" class="ltx_text">
<span id="S5.T1.16.16.16.16.17.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.16.16.16.16.17.1.1.1.1.1" class="ltx_tr">
<span id="S5.T1.16.16.16.16.17.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.16.16.16.16.17.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S5.T1.16.16.16.16.17.1.1.1.1.2" class="ltx_tr">
<span id="S5.T1.16.16.16.16.17.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.16.16.16.16.17.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Hours</span></span></span>
</span></span></span>
<span id="S5.T1.16.16.16.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T1.16.16.16.16.17.1.2.1" class="ltx_text ltx_font_bold">DNN-HMM</span></span>
<span id="S5.T1.16.16.16.16.17.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S5.T1.16.16.16.16.17.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T1.16.16.16.16.17.1.4.1" class="ltx_text ltx_font_bold">CTC/Attention</span></span></span>
<span id="S5.T1.16.16.16.16.18.2" class="ltx_tr">
<span id="S5.T1.16.16.16.16.18.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.16.16.16.16.18.2.1.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T1.16.16.16.16.18.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.16.16.16.16.18.2.2.1" class="ltx_text ltx_font_bold">Time</span></span>
<span id="S5.T1.16.16.16.16.18.2.3" class="ltx_td ltx_th ltx_th_column"></span>
<span id="S5.T1.16.16.16.16.18.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.16.16.16.16.18.2.4.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T1.16.16.16.16.18.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T1.16.16.16.16.18.2.5.1" class="ltx_text ltx_font_bold">Time</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T1.2.2.2.2.2" class="ltx_tr">
<span id="S5.T1.2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">1</span></span>
<span id="S5.T1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">38.4<math id="S5.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.1.1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">3.9</span>
<span id="S5.T1.2.2.2.2.2.5" class="ltx_td ltx_border_t"></span>
<span id="S5.T1.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">100.0<math id="S5.T1.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.2.2.2.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.2.2.2.m1.1.1" xref="S5.T1.2.2.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T1.2.2.2.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.0</span>
<span id="S5.T1.2.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">3.3</span></span>
<span id="S5.T1.4.4.4.4.4" class="ltx_tr">
<span id="S5.T1.4.4.4.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.4.4.4.4.4.3.1" class="ltx_text ltx_font_bold">2</span></span>
<span id="S5.T1.3.3.3.3.3.1" class="ltx_td ltx_align_center">35.3<math id="S5.T1.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.3.3.3.3.3.1.m1.1a"><mo id="S5.T1.3.3.3.3.3.1.m1.1.1" xref="S5.T1.3.3.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.3.3.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.3.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T1.4.4.4.4.4.4" class="ltx_td ltx_align_center">6.3</span>
<span id="S5.T1.4.4.4.4.4.5" class="ltx_td"></span>
<span id="S5.T1.4.4.4.4.4.2" class="ltx_td ltx_align_center">87.2<math id="S5.T1.4.4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.4.4.4.4.4.2.m1.1a"><mo id="S5.T1.4.4.4.4.4.2.m1.1.1" xref="S5.T1.4.4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T1.4.4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.4.2.m1.1c">\pm</annotation></semantics></math>1.3</span>
<span id="S5.T1.4.4.4.4.4.6" class="ltx_td ltx_align_center">5.8</span></span>
<span id="S5.T1.6.6.6.6.6" class="ltx_tr">
<span id="S5.T1.6.6.6.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T1.6.6.6.6.6.3.1" class="ltx_text ltx_font_bold">5</span></span>
<span id="S5.T1.5.5.5.5.5.1" class="ltx_td ltx_align_center">31.9<math id="S5.T1.5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.5.5.5.5.5.1.m1.1a"><mo id="S5.T1.5.5.5.5.5.1.m1.1.1" xref="S5.T1.5.5.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.5.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.6.6.6.6.6.4" class="ltx_td ltx_align_center">9.4</span>
<span id="S5.T1.6.6.6.6.6.5" class="ltx_td"></span>
<span id="S5.T1.6.6.6.6.6.2" class="ltx_td ltx_align_center">35.1<math id="S5.T1.6.6.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.6.6.6.6.6.2.m1.1a"><mo id="S5.T1.6.6.6.6.6.2.m1.1.1" xref="S5.T1.6.6.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.6.6.6.2.m1.1.1.cmml" xref="S5.T1.6.6.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.6.6.2.m1.1c">\pm</annotation></semantics></math>1.6</span>
<span id="S5.T1.6.6.6.6.6.6" class="ltx_td ltx_align_center">15.8</span></span>
<span id="S5.T1.8.8.8.8.8" class="ltx_tr">
<span id="S5.T1.8.8.8.8.8.3" class="ltx_td ltx_align_center"><span id="S5.T1.8.8.8.8.8.3.1" class="ltx_text ltx_font_bold">10</span></span>
<span id="S5.T1.7.7.7.7.7.1" class="ltx_td ltx_align_center">31.5<math id="S5.T1.7.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.7.7.7.7.7.1.m1.1a"><mo id="S5.T1.7.7.7.7.7.1.m1.1.1" xref="S5.T1.7.7.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.7.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.7.7.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.8.8.8.8.8.4" class="ltx_td ltx_align_center">15.9</span>
<span id="S5.T1.8.8.8.8.8.5" class="ltx_td"></span>
<span id="S5.T1.8.8.8.8.8.2" class="ltx_td ltx_align_center">31.7<math id="S5.T1.8.8.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.8.8.8.8.8.2.m1.1a"><mo id="S5.T1.8.8.8.8.8.2.m1.1.1" xref="S5.T1.8.8.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T1.8.8.8.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.8.8.2.m1.1c">\pm</annotation></semantics></math>1.6</span>
<span id="S5.T1.8.8.8.8.8.6" class="ltx_td ltx_align_center">30.8</span></span>
<span id="S5.T1.10.10.10.10.10" class="ltx_tr">
<span id="S5.T1.10.10.10.10.10.3" class="ltx_td ltx_align_center"><span id="S5.T1.10.10.10.10.10.3.1" class="ltx_text ltx_font_bold">20</span></span>
<span id="S5.T1.9.9.9.9.9.1" class="ltx_td ltx_align_center">31.5<math id="S5.T1.9.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.9.9.9.9.9.1.m1.1a"><mo id="S5.T1.9.9.9.9.9.1.m1.1.1" xref="S5.T1.9.9.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S5.T1.9.9.9.9.9.1.m1.1.1.cmml" xref="S5.T1.9.9.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.9.9.9.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.10.10.10.10.10.4" class="ltx_td ltx_align_center">25.9</span>
<span id="S5.T1.10.10.10.10.10.5" class="ltx_td"></span>
<span id="S5.T1.10.10.10.10.10.2" class="ltx_td ltx_align_center">30.7<math id="S5.T1.10.10.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.10.10.10.10.10.2.m1.1a"><mo id="S5.T1.10.10.10.10.10.2.m1.1.1" xref="S5.T1.10.10.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S5.T1.10.10.10.10.10.2.m1.1.1.cmml" xref="S5.T1.10.10.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.10.10.10.2.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.10.10.10.10.10.6" class="ltx_td ltx_align_center">61.7</span></span>
<span id="S5.T1.12.12.12.12.12" class="ltx_tr">
<span id="S5.T1.12.12.12.12.12.3" class="ltx_td ltx_align_center"><span id="S5.T1.12.12.12.12.12.3.1" class="ltx_text ltx_font_bold">50</span></span>
<span id="S5.T1.11.11.11.11.11.1" class="ltx_td ltx_align_center">31.1<math id="S5.T1.11.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.11.11.11.11.11.1.m1.1a"><mo id="S5.T1.11.11.11.11.11.1.m1.1.1" xref="S5.T1.11.11.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S5.T1.11.11.11.11.11.1.m1.1.1.cmml" xref="S5.T1.11.11.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.11.11.11.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.12.12.12.12.12.4" class="ltx_td ltx_align_center">58.4</span>
<span id="S5.T1.12.12.12.12.12.5" class="ltx_td"></span>
<span id="S5.T1.12.12.12.12.12.2" class="ltx_td ltx_align_center">29.3<math id="S5.T1.12.12.12.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.12.12.12.12.12.2.m1.1a"><mo id="S5.T1.12.12.12.12.12.2.m1.1.1" xref="S5.T1.12.12.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S5.T1.12.12.12.12.12.2.m1.1.1.cmml" xref="S5.T1.12.12.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.12.12.12.2.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.12.12.12.12.12.6" class="ltx_td ltx_align_center">151.7</span></span>
<span id="S5.T1.14.14.14.14.14" class="ltx_tr">
<span id="S5.T1.14.14.14.14.14.3" class="ltx_td ltx_align_center"><span id="S5.T1.14.14.14.14.14.3.1" class="ltx_text ltx_font_bold">100</span></span>
<span id="S5.T1.13.13.13.13.13.1" class="ltx_td ltx_align_center">30.2<math id="S5.T1.13.13.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.13.13.13.13.13.1.m1.1a"><mo id="S5.T1.13.13.13.13.13.1.m1.1.1" xref="S5.T1.13.13.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T1.13.13.13.13.13.1.m1.1.1.cmml" xref="S5.T1.13.13.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.13.13.13.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T1.14.14.14.14.14.4" class="ltx_td ltx_align_center">114.0</span>
<span id="S5.T1.14.14.14.14.14.5" class="ltx_td"></span>
<span id="S5.T1.14.14.14.14.14.2" class="ltx_td ltx_align_center">28.7<math id="S5.T1.14.14.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.14.14.14.14.14.2.m1.1a"><mo id="S5.T1.14.14.14.14.14.2.m1.1.1" xref="S5.T1.14.14.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T1.14.14.14.14.14.2.m1.1.1.cmml" xref="S5.T1.14.14.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.14.14.14.2.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T1.14.14.14.14.14.6" class="ltx_td ltx_align_center">306.7</span></span>
<span id="S5.T1.16.16.16.16.16" class="ltx_tr">
<span id="S5.T1.16.16.16.16.16.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.16.16.16.16.16.3.1" class="ltx_text ltx_font_bold">223</span></span>
<span id="S5.T1.15.15.15.15.15.1" class="ltx_td ltx_align_center ltx_border_bb">29.8<math id="S5.T1.15.15.15.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.15.15.15.15.15.1.m1.1a"><mo id="S5.T1.15.15.15.15.15.1.m1.1.1" xref="S5.T1.15.15.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.15.15.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S5.T1.15.15.15.15.15.1.m1.1.1.cmml" xref="S5.T1.15.15.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.15.15.15.15.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.16.16.16.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">221.7</span>
<span id="S5.T1.16.16.16.16.16.5" class="ltx_td ltx_border_bb"></span>
<span id="S5.T1.16.16.16.16.16.2" class="ltx_td ltx_align_center ltx_border_bb">28.0<math id="S5.T1.16.16.16.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.16.16.16.16.16.2.m1.1a"><mo id="S5.T1.16.16.16.16.16.2.m1.1.1" xref="S5.T1.16.16.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.16.16.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S5.T1.16.16.16.16.16.2.m1.1.1.cmml" xref="S5.T1.16.16.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.16.16.16.16.16.2.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T1.16.16.16.16.16.6" class="ltx_td ltx_align_center ltx_border_bb">634.2</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the DNN-HMM and the CTC/Attention decoders for the LRS2-BBC database based on the number of hours used to estimate both paradigms. System performance (% WER) and training time (Time) expressed in minutes are reported. The 223 training hours refer to the entire training set.</figcaption>
</figure>
<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">LRS2-BBC.</span> Using the LRS2-BBC database is the ideal scenario, where the proposed encoder extracts the visual speech features for which it was explicitly trained. Table <a href="#S5.T1" title="Table 1 ‣ 5. Results &amp; Discussion ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reflects how the conventional paradigm is not only capable of obtaining state-of-the-art results, but also of significantly outperforming the CTC/Attention model in data-scarcity scenarios. Only when at least 10 hours of data were available, both approaches provided similar recognition rates. Besides, it should be mentioned that both paradigms presented a real time factor of around 0.75. However, despite offering a slightly better performance, the CTC/Attention estimation took more than twice the hybrid system training time.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.18" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:242.2pt;height:181.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T2.18.18" class="ltx_p"><span id="S5.T2.18.18.18" class="ltx_text">
<span id="S5.T2.18.18.18.18" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T2.18.18.18.18.19.1" class="ltx_tr">
<span id="S5.T2.18.18.18.18.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S5.T2.18.18.18.18.19.1.1.1" class="ltx_text">
<span id="S5.T2.18.18.18.18.19.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.18.18.18.18.19.1.1.1.1.1" class="ltx_tr">
<span id="S5.T2.18.18.18.18.19.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.18.18.18.18.19.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S5.T2.18.18.18.18.19.1.1.1.1.2" class="ltx_tr">
<span id="S5.T2.18.18.18.18.19.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.18.18.18.18.19.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Hours</span></span></span>
</span></span></span>
<span id="S5.T2.18.18.18.18.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T2.18.18.18.18.19.1.2.1" class="ltx_text ltx_font_bold">DNN-HMM</span></span>
<span id="S5.T2.18.18.18.18.19.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S5.T2.18.18.18.18.19.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T2.18.18.18.18.19.1.4.1" class="ltx_text ltx_font_bold">CTC/Attention</span></span></span>
<span id="S5.T2.18.18.18.18.20.2" class="ltx_tr">
<span id="S5.T2.18.18.18.18.20.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.18.18.18.18.20.2.1.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T2.18.18.18.18.20.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.18.18.18.18.20.2.2.1" class="ltx_text ltx_font_bold">Time</span></span>
<span id="S5.T2.18.18.18.18.20.2.3" class="ltx_td ltx_th ltx_th_column"></span>
<span id="S5.T2.18.18.18.18.20.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.18.18.18.18.20.2.4.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T2.18.18.18.18.20.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.18.18.18.18.20.2.5.1" class="ltx_text ltx_font_bold">Time</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T2.2.2.2.2.2" class="ltx_tr">
<span id="S5.T2.2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">1</span></span>
<span id="S5.T2.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">70.7<math id="S5.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math>1.1</span>
<span id="S5.T2.2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">4.4</span>
<span id="S5.T2.2.2.2.2.2.5" class="ltx_td ltx_border_t"></span>
<span id="S5.T2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">100.0<math id="S5.T2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.2.2.2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T2.2.2.2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.0</span>
<span id="S5.T2.2.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">2.5</span></span>
<span id="S5.T2.4.4.4.4.4" class="ltx_tr">
<span id="S5.T2.4.4.4.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T2.4.4.4.4.4.3.1" class="ltx_text ltx_font_bold">2</span></span>
<span id="S5.T2.3.3.3.3.3.1" class="ltx_td ltx_align_center">67.1<math id="S5.T2.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.3.3.3.3.3.1.m1.1a"><mo id="S5.T2.3.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T2.3.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.3.1.m1.1c">\pm</annotation></semantics></math>1.2</span>
<span id="S5.T2.4.4.4.4.4.4" class="ltx_td ltx_align_center">7.0</span>
<span id="S5.T2.4.4.4.4.4.5" class="ltx_td"></span>
<span id="S5.T2.4.4.4.4.4.2" class="ltx_td ltx_align_center">100.0<math id="S5.T2.4.4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.4.4.4.4.4.2.m1.1a"><mo id="S5.T2.4.4.4.4.4.2.m1.1.1" xref="S5.T2.4.4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S5.T2.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.4.2.m1.1c">\pm</annotation></semantics></math>0.0</span>
<span id="S5.T2.4.4.4.4.4.6" class="ltx_td ltx_align_center">5.0</span></span>
<span id="S5.T2.6.6.6.6.6" class="ltx_tr">
<span id="S5.T2.6.6.6.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T2.6.6.6.6.6.3.1" class="ltx_text ltx_font_bold">5</span></span>
<span id="S5.T2.5.5.5.5.5.1" class="ltx_td ltx_align_center">59.4<math id="S5.T2.5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.5.5.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T2.5.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math>1.3</span>
<span id="S5.T2.6.6.6.6.6.4" class="ltx_td ltx_align_center">9.4</span>
<span id="S5.T2.6.6.6.6.6.5" class="ltx_td"></span>
<span id="S5.T2.6.6.6.6.6.2" class="ltx_td ltx_align_center">98.0<math id="S5.T2.6.6.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.6.6.6.6.6.2.m1.1a"><mo id="S5.T2.6.6.6.6.6.2.m1.1.1" xref="S5.T2.6.6.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T2.6.6.6.6.6.2.m1.1.1.cmml" xref="S5.T2.6.6.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.6.2.m1.1c">\pm</annotation></semantics></math>1.8</span>
<span id="S5.T2.6.6.6.6.6.6" class="ltx_td ltx_align_center">13.3</span></span>
<span id="S5.T2.8.8.8.8.8" class="ltx_tr">
<span id="S5.T2.8.8.8.8.8.3" class="ltx_td ltx_align_center"><span id="S5.T2.8.8.8.8.8.3.1" class="ltx_text ltx_font_bold">10</span></span>
<span id="S5.T2.7.7.7.7.7.1" class="ltx_td ltx_align_center">57.4<math id="S5.T2.7.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.7.7.7.7.7.1.m1.1a"><mo id="S5.T2.7.7.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T2.7.7.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.7.7.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T2.8.8.8.8.8.4" class="ltx_td ltx_align_center">15.5</span>
<span id="S5.T2.8.8.8.8.8.5" class="ltx_td"></span>
<span id="S5.T2.8.8.8.8.8.2" class="ltx_td ltx_align_center">67.2<math id="S5.T2.8.8.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.8.8.8.8.8.2.m1.1a"><mo id="S5.T2.8.8.8.8.8.2.m1.1.1" xref="S5.T2.8.8.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T2.8.8.8.8.8.2.m1.1.1.cmml" xref="S5.T2.8.8.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.8.8.2.m1.1c">\pm</annotation></semantics></math>1.7</span>
<span id="S5.T2.8.8.8.8.8.6" class="ltx_td ltx_align_center">25.8</span></span>
<span id="S5.T2.10.10.10.10.10" class="ltx_tr">
<span id="S5.T2.10.10.10.10.10.3" class="ltx_td ltx_align_center"><span id="S5.T2.10.10.10.10.10.3.1" class="ltx_text ltx_font_bold">20</span></span>
<span id="S5.T2.9.9.9.9.9.1" class="ltx_td ltx_align_center">55.7<math id="S5.T2.9.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.9.9.9.9.9.1.m1.1a"><mo id="S5.T2.9.9.9.9.9.1.m1.1.1" xref="S5.T2.9.9.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S5.T2.9.9.9.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.9.9.9.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T2.10.10.10.10.10.4" class="ltx_td ltx_align_center">26.6</span>
<span id="S5.T2.10.10.10.10.10.5" class="ltx_td"></span>
<span id="S5.T2.10.10.10.10.10.2" class="ltx_td ltx_align_center">58.8<math id="S5.T2.10.10.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.10.10.10.10.10.2.m1.1a"><mo id="S5.T2.10.10.10.10.10.2.m1.1.1" xref="S5.T2.10.10.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S5.T2.10.10.10.10.10.2.m1.1.1.cmml" xref="S5.T2.10.10.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.10.10.10.2.m1.1c">\pm</annotation></semantics></math>1.7</span>
<span id="S5.T2.10.10.10.10.10.6" class="ltx_td ltx_align_center">52.5</span></span>
<span id="S5.T2.12.12.12.12.12" class="ltx_tr">
<span id="S5.T2.12.12.12.12.12.3" class="ltx_td ltx_align_center"><span id="S5.T2.12.12.12.12.12.3.1" class="ltx_text ltx_font_bold">50</span></span>
<span id="S5.T2.11.11.11.11.11.1" class="ltx_td ltx_align_center">54.8<math id="S5.T2.11.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.11.11.11.11.11.1.m1.1a"><mo id="S5.T2.11.11.11.11.11.1.m1.1.1" xref="S5.T2.11.11.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.11.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S5.T2.11.11.11.11.11.1.m1.1.1.cmml" xref="S5.T2.11.11.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.11.11.11.11.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T2.12.12.12.12.12.4" class="ltx_td ltx_align_center">60.6</span>
<span id="S5.T2.12.12.12.12.12.5" class="ltx_td"></span>
<span id="S5.T2.12.12.12.12.12.2" class="ltx_td ltx_align_center">53.1<math id="S5.T2.12.12.12.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.12.12.12.12.12.2.m1.1a"><mo id="S5.T2.12.12.12.12.12.2.m1.1.1" xref="S5.T2.12.12.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.12.12.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S5.T2.12.12.12.12.12.2.m1.1.1.cmml" xref="S5.T2.12.12.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.12.12.12.12.2.m1.1c">\pm</annotation></semantics></math>1.8</span>
<span id="S5.T2.12.12.12.12.12.6" class="ltx_td ltx_align_center">130.8</span></span>
<span id="S5.T2.14.14.14.14.14" class="ltx_tr">
<span id="S5.T2.14.14.14.14.14.3" class="ltx_td ltx_align_center"><span id="S5.T2.14.14.14.14.14.3.1" class="ltx_text ltx_font_bold">100</span></span>
<span id="S5.T2.13.13.13.13.13.1" class="ltx_td ltx_align_center">54.6<math id="S5.T2.13.13.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.13.13.13.13.13.1.m1.1a"><mo id="S5.T2.13.13.13.13.13.1.m1.1.1" xref="S5.T2.13.13.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.13.13.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T2.13.13.13.13.13.1.m1.1.1.cmml" xref="S5.T2.13.13.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.13.13.13.13.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T2.14.14.14.14.14.4" class="ltx_td ltx_align_center">115.5</span>
<span id="S5.T2.14.14.14.14.14.5" class="ltx_td"></span>
<span id="S5.T2.14.14.14.14.14.2" class="ltx_td ltx_align_center">50.9<math id="S5.T2.14.14.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.14.14.14.14.14.2.m1.1a"><mo id="S5.T2.14.14.14.14.14.2.m1.1.1" xref="S5.T2.14.14.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.14.14.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T2.14.14.14.14.14.2.m1.1.1.cmml" xref="S5.T2.14.14.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.14.14.14.14.2.m1.1c">\pm</annotation></semantics></math>1.8</span>
<span id="S5.T2.14.14.14.14.14.6" class="ltx_td ltx_align_center">261.7</span></span>
<span id="S5.T2.16.16.16.16.16" class="ltx_tr">
<span id="S5.T2.16.16.16.16.16.3" class="ltx_td ltx_align_center"><span id="S5.T2.16.16.16.16.16.3.1" class="ltx_text ltx_font_bold">200</span></span>
<span id="S5.T2.15.15.15.15.15.1" class="ltx_td ltx_align_center">53.7<math id="S5.T2.15.15.15.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.15.15.15.15.15.1.m1.1a"><mo id="S5.T2.15.15.15.15.15.1.m1.1.1" xref="S5.T2.15.15.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.15.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S5.T2.15.15.15.15.15.1.m1.1.1.cmml" xref="S5.T2.15.15.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.15.15.15.15.1.m1.1c">\pm</annotation></semantics></math>1.4</span>
<span id="S5.T2.16.16.16.16.16.4" class="ltx_td ltx_align_center">229.0</span>
<span id="S5.T2.16.16.16.16.16.5" class="ltx_td"></span>
<span id="S5.T2.16.16.16.16.16.2" class="ltx_td ltx_align_center">50.3<math id="S5.T2.16.16.16.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.16.16.16.16.16.2.m1.1a"><mo id="S5.T2.16.16.16.16.16.2.m1.1.1" xref="S5.T2.16.16.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.16.16.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S5.T2.16.16.16.16.16.2.m1.1.1.cmml" xref="S5.T2.16.16.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.16.16.16.16.2.m1.1c">\pm</annotation></semantics></math>1.7</span>
<span id="S5.T2.16.16.16.16.16.6" class="ltx_td ltx_align_center">524.2</span></span>
<span id="S5.T2.18.18.18.18.18" class="ltx_tr">
<span id="S5.T2.18.18.18.18.18.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.18.18.18.18.18.3.1" class="ltx_text ltx_font_bold">437</span></span>
<span id="S5.T2.17.17.17.17.17.1" class="ltx_td ltx_align_center ltx_border_bb">53.5<math id="S5.T2.17.17.17.17.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.17.17.17.17.17.1.m1.1a"><mo id="S5.T2.17.17.17.17.17.1.m1.1.1" xref="S5.T2.17.17.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.17.17.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S5.T2.17.17.17.17.17.1.m1.1.1.cmml" xref="S5.T2.17.17.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.17.17.17.17.1.m1.1c">\pm</annotation></semantics></math>1.5</span>
<span id="S5.T2.18.18.18.18.18.4" class="ltx_td ltx_align_center ltx_border_bb">358.2</span>
<span id="S5.T2.18.18.18.18.18.5" class="ltx_td ltx_border_bb"></span>
<span id="S5.T2.18.18.18.18.18.2" class="ltx_td ltx_align_center ltx_border_bb">50.0<math id="S5.T2.18.18.18.18.18.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.18.18.18.18.18.2.m1.1a"><mo id="S5.T2.18.18.18.18.18.2.m1.1.1" xref="S5.T2.18.18.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.18.18.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S5.T2.18.18.18.18.18.2.m1.1.1.cmml" xref="S5.T2.18.18.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.18.18.18.18.2.m1.1c">\pm</annotation></semantics></math>1.7</span>
<span id="S5.T2.18.18.18.18.18.6" class="ltx_td ltx_align_center ltx_border_bb">820.0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the DNN-HMM and the CTC/Attention decoders for the LRS3-TED database based on the number of hours used to estimate both paradigms. System performance (% WER) and training time (Time) expressed in minutes are reported. The 437 training hours refer to the entire training set.</figcaption>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">LRS3-TED.</span> It is considered our domain-mismatch scenario. First, it should be noted that this corpus was used during the pre-training stage of the visual speech encoder. However, due to the data-driven nature of the encoder and the fact that it was later fine-tuned to the LRS2-BBC database, the resulting features were expected to be worse than those extracted for the aforementioned corpus. Nonetheless, it allowed us to study whether the consequences of this deterioration in the quality of visual speech features could be mitigated when a larger amount of data is available.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">As Table <a href="#S5.T2" title="Table 2 ‣ 5. Results &amp; Discussion ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reflects, results comparable to state of the art (25-30% WER) were not achieved. Moreover, 20 hours were now necessary for both paradigms to offer a similar performance. The real time factor was around 0.82 and 0.75 for the DNN-HMM and CTC/Attention paradigm, respectively. Nonetheless, we can observe an analogous behaviour to that described for the LRS2-BBC database regarding data-scarcity scenarios, where the DNN-HMM would still be the best approach. Conversely, from 100 hours of data, the CTC/Attention showed significant differences w.r.t the conventional paradigm. It suggests that the state-of-the-art decoder could be more adaptable to poorer-quality speech representations.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">However, the DNN-HMM and CTC/Attention decoders converge when the availability of more data does not imply any improvement in terms of performance (with 20 and 50 hours of training data, respectively, differences are not significant w.r.t. using all available data). This fact would demonstrate that the quality of the visual speech encoder is a real limitation whose consequences are not mitigated by decoders despite having larger amounts of data.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.T3.11" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:237.2pt;height:99pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T3.11.11" class="ltx_p"><span id="S5.T3.11.11.11" class="ltx_text">
<span id="S5.T3.11.11.11.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T3.11.11.11.11.12.1" class="ltx_tr">
<span id="S5.T3.11.11.11.11.12.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S5.T3.11.11.11.11.12.1.1.1" class="ltx_text">
<span id="S5.T3.11.11.11.11.12.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.11.11.11.11.12.1.1.1.1.1" class="ltx_tr">
<span id="S5.T3.11.11.11.11.12.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.11.11.11.11.12.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S5.T3.11.11.11.11.12.1.1.1.1.2" class="ltx_tr">
<span id="S5.T3.11.11.11.11.12.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.11.11.11.11.12.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Hours</span></span></span>
</span></span></span>
<span id="S5.T3.11.11.11.11.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T3.11.11.11.11.12.1.2.1" class="ltx_text ltx_font_bold">DNN-HMM</span></span>
<span id="S5.T3.11.11.11.11.12.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S5.T3.11.11.11.11.12.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S5.T3.11.11.11.11.12.1.4.1" class="ltx_text ltx_font_bold">CTC/Attention</span></span></span>
<span id="S5.T3.11.11.11.11.13.2" class="ltx_tr">
<span id="S5.T3.11.11.11.11.13.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.11.11.11.11.13.2.1.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T3.11.11.11.11.13.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.11.11.11.11.13.2.2.1" class="ltx_text ltx_font_bold">Time</span></span>
<span id="S5.T3.11.11.11.11.13.2.3" class="ltx_td ltx_th ltx_th_column"></span>
<span id="S5.T3.11.11.11.11.13.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.11.11.11.11.13.2.4.1" class="ltx_text ltx_font_bold">% WER</span></span>
<span id="S5.T3.11.11.11.11.13.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.11.11.11.11.13.2.5.1" class="ltx_text ltx_font_bold">Time</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T3.3.3.3.3.3" class="ltx_tr">
<span id="S5.T3.3.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.3.3.3.3.3.4.1" class="ltx_text ltx_font_bold">1</span></span>
<span id="S5.T3.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">78.1<math id="S5.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.1.1.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.9</span>
<span id="S5.T3.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">3.5</span>
<span id="S5.T3.3.3.3.3.3.6" class="ltx_td ltx_border_t"></span>
<span id="S5.T3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S5.T3.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T3.2.2.2.2.2.2.m1.1a"><mo id="S5.T3.2.2.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.2.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.2.2.m1.1b"><gt id="S5.T3.2.2.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.2.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.2.2.m1.1c">&gt;</annotation></semantics></math>100.0<sup id="S5.T3.3.3.3.3.3.3.1" class="ltx_sup"><math id="S5.T3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T3.3.3.3.3.3.3.1.m1.1a"><mo id="S5.T3.3.3.3.3.3.3.1.m1.1.1" xref="S5.T3.3.3.3.3.3.3.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.3.3.1.m1.1b"><ci id="S5.T3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.3.3.3.3.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.3.3.1.m1.1c">\dagger</annotation></semantics></math></sup></span>
<span id="S5.T3.3.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">2.5</span></span>
<span id="S5.T3.6.6.6.6.6" class="ltx_tr">
<span id="S5.T3.6.6.6.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T3.6.6.6.6.6.4.1" class="ltx_text ltx_font_bold">2</span></span>
<span id="S5.T3.4.4.4.4.4.1" class="ltx_td ltx_align_center">77.5<math id="S5.T3.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.4.4.4.4.4.1.m1.1a"><mo id="S5.T3.4.4.4.4.4.1.m1.1.1" xref="S5.T3.4.4.4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T3.4.4.4.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.4.1.m1.1c">\pm</annotation></semantics></math>1.0</span>
<span id="S5.T3.6.6.6.6.6.5" class="ltx_td ltx_align_center">5.5</span>
<span id="S5.T3.6.6.6.6.6.6" class="ltx_td"></span>
<span id="S5.T3.6.6.6.6.6.3" class="ltx_td ltx_align_center"><math id="S5.T3.5.5.5.5.5.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T3.5.5.5.5.5.2.m1.1a"><mo id="S5.T3.5.5.5.5.5.2.m1.1.1" xref="S5.T3.5.5.5.5.5.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.5.2.m1.1b"><gt id="S5.T3.5.5.5.5.5.2.m1.1.1.cmml" xref="S5.T3.5.5.5.5.5.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.5.2.m1.1c">&gt;</annotation></semantics></math>100.0<sup id="S5.T3.6.6.6.6.6.3.1" class="ltx_sup"><math id="S5.T3.6.6.6.6.6.3.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T3.6.6.6.6.6.3.1.m1.1a"><mo id="S5.T3.6.6.6.6.6.3.1.m1.1.1" xref="S5.T3.6.6.6.6.6.3.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.6.3.1.m1.1b"><ci id="S5.T3.6.6.6.6.6.3.1.m1.1.1.cmml" xref="S5.T3.6.6.6.6.6.3.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.6.3.1.m1.1c">\dagger</annotation></semantics></math></sup></span>
<span id="S5.T3.6.6.6.6.6.7" class="ltx_td ltx_align_center">5.0</span></span>
<span id="S5.T3.9.9.9.9.9" class="ltx_tr">
<span id="S5.T3.9.9.9.9.9.4" class="ltx_td ltx_align_center"><span id="S5.T3.9.9.9.9.9.4.1" class="ltx_text ltx_font_bold">5</span></span>
<span id="S5.T3.7.7.7.7.7.1" class="ltx_td ltx_align_center">67.8<math id="S5.T3.7.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.7.7.7.7.7.1.m1.1a"><mo id="S5.T3.7.7.7.7.7.1.m1.1.1" xref="S5.T3.7.7.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T3.7.7.7.7.7.1.m1.1.1.cmml" xref="S5.T3.7.7.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.7.7.7.1.m1.1c">\pm</annotation></semantics></math>1.1</span>
<span id="S5.T3.9.9.9.9.9.5" class="ltx_td ltx_align_center">9.9</span>
<span id="S5.T3.9.9.9.9.9.6" class="ltx_td"></span>
<span id="S5.T3.9.9.9.9.9.3" class="ltx_td ltx_align_center"><math id="S5.T3.8.8.8.8.8.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T3.8.8.8.8.8.2.m1.1a"><mo id="S5.T3.8.8.8.8.8.2.m1.1.1" xref="S5.T3.8.8.8.8.8.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.8.8.2.m1.1b"><gt id="S5.T3.8.8.8.8.8.2.m1.1.1.cmml" xref="S5.T3.8.8.8.8.8.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.8.8.2.m1.1c">&gt;</annotation></semantics></math>100.0<sup id="S5.T3.9.9.9.9.9.3.1" class="ltx_sup"><math id="S5.T3.9.9.9.9.9.3.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T3.9.9.9.9.9.3.1.m1.1a"><mo id="S5.T3.9.9.9.9.9.3.1.m1.1.1" xref="S5.T3.9.9.9.9.9.3.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.9.9.3.1.m1.1b"><ci id="S5.T3.9.9.9.9.9.3.1.m1.1.1.cmml" xref="S5.T3.9.9.9.9.9.3.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.9.9.3.1.m1.1c">\dagger</annotation></semantics></math></sup></span>
<span id="S5.T3.9.9.9.9.9.7" class="ltx_td ltx_align_center">12.5</span></span>
<span id="S5.T3.11.11.11.11.11" class="ltx_tr">
<span id="S5.T3.11.11.11.11.11.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.11.11.11.11.11.3.1" class="ltx_text ltx_font_bold">9</span></span>
<span id="S5.T3.10.10.10.10.10.1" class="ltx_td ltx_align_center ltx_border_bb">66.2<math id="S5.T3.10.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.10.10.10.10.10.1.m1.1a"><mo id="S5.T3.10.10.10.10.10.1.m1.1.1" xref="S5.T3.10.10.10.10.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.10.10.1.m1.1b"><csymbol cd="latexml" id="S5.T3.10.10.10.10.10.1.m1.1.1.cmml" xref="S5.T3.10.10.10.10.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.10.10.1.m1.1c">\pm</annotation></semantics></math>1.1</span>
<span id="S5.T3.11.11.11.11.11.4" class="ltx_td ltx_align_center ltx_border_bb">16.6</span>
<span id="S5.T3.11.11.11.11.11.5" class="ltx_td ltx_border_bb"></span>
<span id="S5.T3.11.11.11.11.11.2" class="ltx_td ltx_align_center ltx_border_bb">89.0<math id="S5.T3.11.11.11.11.11.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.11.11.11.11.11.2.m1.1a"><mo id="S5.T3.11.11.11.11.11.2.m1.1.1" xref="S5.T3.11.11.11.11.11.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.11.11.11.2.m1.1b"><csymbol cd="latexml" id="S5.T3.11.11.11.11.11.2.m1.1.1.cmml" xref="S5.T3.11.11.11.11.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.11.11.11.2.m1.1c">\pm</annotation></semantics></math>1.2</span>
<span id="S5.T3.11.11.11.11.11.6" class="ltx_td ltx_align_center ltx_border_bb">23.3</span></span>
</span>
</span></span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S5.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.I1.ix1.1.1.m1.1b"><mo id="S5.I1.ix1.1.1.m1.1.1" xref="S5.I1.ix1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.1.1.m1.1c"><ci id="S5.I1.ix1.1.1.m1.1.1.cmml" xref="S5.I1.ix1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.1.1.m1.1d">\dagger</annotation></semantics></math></span> 
<div id="S5.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.ix1.p1.1" class="ltx_p"><span id="S5.I1.ix1.p1.1.1" class="ltx_text" style="font-size:90%;">due to a peculiarity of the WER metric</span></p>
</div>
</li>
</ul>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of the DNN-HMM and the CTC/Attention decoders for the LIP-RTVE database based on the number of hours used to estimate both paradigms. System performance (% WER) and training time (Time) in minutes are reported. The 9 training hours refer to the entire training set.</figcaption>
</figure>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">LIP-RTVE.</span> In the case of the LIP-RTVE database, we were not only faced with a data-scarcity scenario, but also with a mismatch in terms of language. These could be the reasons why our first results were not acceptable. Therefore, we decided to adapt the visual speech encoder as if we were in the worst possible scenario of our experiments: when only 1 hour of data was available. As described in Subsection <a href="#S3.SS2" title="3.2. Visual Speech Encoder ‣ 3. Method ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we fine-tuned the encoder in an end-to-end manner, obtaining a baseline model capable of reaching results around 88.6% WER.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p">Results in Table <a href="#S5.T3" title="Table 3 ‣ 5. Results &amp; Discussion ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show that, as in the rest of the studied scenarios, the conventional DNN-HMM decoder outperforms its CTC/Attention counterpart. Using around 10 hours of data w.r.t. only 1 hour enhances around 15% WER in relative terms for the DNN-HMM system, which is in harmony with the roughly 18% relative improvement observed for the LRS2-BBC and LRS3-TED corpora. Furthermore, we argue that one of the reasons that could be behind the success of the DNN-HMM paradigm was the word-level LM influence.</p>
</div>
<div id="S5.p7" class="ltx_para ltx_noindent">
<p id="S5.p7.1" class="ltx_p">We also investigated fine-tuning the entire end-to-end architecture using the whole training set of the LIP-RTVE database. Recognition rates of around 60% WER were obtained, which significantly improves the best performance obtained for the LIP-RTVE database to date. However, it should be noted that their estimate assumes more than six times the training time w.r.t the DNN-HMM model.</p>
</div>
<div id="S5.p8" class="ltx_para ltx_noindent">
<p id="S5.p8.1" class="ltx_p"><span id="S5.p8.1.1" class="ltx_text ltx_font_bold">Overall Analysis.</span> Figure <a href="#S4.F2" title="Figure 2 ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reflects how system performance degrades as visual speech features deteriorate from the ideal scenario (LRS2-BBC) to the language-mismatch (LIP-RTVE) scenario. This trend is not only an aspect we could expect, but it is also supported by Tseng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Tseng et al., 2023</a>]</cite>, whose study demonstrated the lack of generalization of different audio-visual self-supervised speech representations in multiple tasks. However, the interesting finding is that the conventional DNN-HMM, compared to its state-of-the-art counterpart, offers a significantly more robust approach when the quality of our visual speech features is not optimal. If, for instance, we analyze the scenario with 5 hours of training data, we can observe how the performance gap between the DNN-HMM and CTC/Attention paradigms in the language- and domain-mismatch scenario is significantly greater than in ideal settings, making the DNN-HMM paradigm a more suitable option when addressing the task in data-scarcity scenarios and non-optimal visual speech features.</p>
</div>
<div id="S5.p9" class="ltx_para ltx_noindent">
<p id="S5.p9.1" class="ltx_p"><span id="S5.p9.1.1" class="ltx_text ltx_font_bold">Findings.</span> According to the findings of our case study, different aspects might be helpful for future researchers and developers focused on designing VSR systems in data-scarcity scenarios with limited computational resources. One of the main aspects is that, even in ideal scenarios with any type of limitation, DNN-HMM decoders not only reach state-of-the-art performance rates but also offer significantly lower training time costs. Furthermore, the fewer number of parameters composing it would facilitate the deployment of this type of system. Similarly, when we suffer from data scarcity and/or lack of optimal self-supervised speech representations for our specific conditions, the state-of-the-art CTC/Attention architecture would not be a recommendable option.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusions &amp; Future Work</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this work, we presented, to the best of our knowledge, the first thorough comparative study on the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart for the visual speech recognition task. Unlike those comparative studies focused on auditory-based ASR, we also systematically investigated how these different decoding paradigms behave based on the amount of data available for their estimation. As reflected in Figure <a href="#S4.F2" title="Figure 2 ‣ 4. Experimental Setup ‣ Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our results showed that the conventional approach achieved recognition rates comparable to the state of the art, significantly outperforming the CTC/Attention model in data-scarcity scenarios. In addition, the DNN-HMM approach offered valuable properties, such as reduced training time and fewer parameters. Finally, by exploring databases of different natures, experiments suggest that further research should still focus on improving the robustness of visual speech representations for data scarcity, as well as domain- and language-mismatch scenarios.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">For this reason, one of our future lines of research is not only studying how state-of-the-art visual speech features can generalize to other tasks and domains, but also extending our work toward estimating and evaluating robust multi-lingual visual speech representations using the MuAViC benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Anwar et al., 2023</a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Acknowledgements</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">This work was partially supported by Grant CIACIF/2021/295 funded by Generalitat Valenciana and by Grant PID2021-124719OB-I00 under project LLEER (PID2021-124719OB-100) funded by MCIN/AEI/10.13039/501100011033/ and by ERDF, EU A way of making Europe.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Bibliographical References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al., 2018a</span>
<span class="ltx_bibblock">
Afouras, T., Chung, J. S., Senior, A., Vinyals, O., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2018a).

</span>
<span class="ltx_bibblock">Deep audio-visual speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on PAMI</span>, 44(12):8717–8727.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al., 2018b</span>
<span class="ltx_bibblock">
Afouras, T., Chung, J.-S., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2018b).

</span>
<span class="ltx_bibblock">LRS3-TED: a large-scale dataset for visual speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.00496</span>.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al., 2023</span>
<span class="ltx_bibblock">
Anwar, M., Shi, B., Goswami, V., Hsu, W.-N., Pino, J., and Wang, C.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">Proc. InterSpeech</span>, pages 4064–4068.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bear and Harvey, 2016</span>
<span class="ltx_bibblock">
Bear, H. and Harvey, R.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Decoding visemes: Improving machine lip-reading.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 2009–2013. IEEE.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bear et al., 2014a</span>
<span class="ltx_bibblock">
Bear, H., Harvey, R., Theobald, B., and Lan, Y.

</span>
<span class="ltx_bibblock">(2014a).

</span>
<span class="ltx_bibblock">Resolution limits on visual speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">ICIP</span>, pages 1371–1375. IEEE.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bear et al., 2014b</span>
<span class="ltx_bibblock">
Bear, H., Harvey, R., Theobald, B., and Lan, Y.

</span>
<span class="ltx_bibblock">(2014b).

</span>
<span class="ltx_bibblock">Which phoneme-to-viseme maps best improve visual-only computer lip-reading?

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">International Symposium on Visual Computing</span>, pages 230–239. Springer.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besle et al., 2004</span>
<span class="ltx_bibblock">
Besle, J., Fort, A., Delpuech, C., and Giard, M.-H.

</span>
<span class="ltx_bibblock">(2004).

</span>
<span class="ltx_bibblock">Bimodal speech: early suppressive visual effects in human auditory cortex.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">European journal of Neuroscience</span>, 20(8):2225–2234.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisani and Ney, 2004</span>
<span class="ltx_bibblock">
Bisani, M. and Ney, H.

</span>
<span class="ltx_bibblock">(2004).

</span>
<span class="ltx_bibblock">Bootstrap estimates for confidence intervals in asr performance evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, volume 1, pages 409–412.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bregler and Konig, 1994</span>
<span class="ltx_bibblock">
Bregler, C. and Konig, Y.

</span>
<span class="ltx_bibblock">(1994).

</span>
<span class="ltx_bibblock">”eigenlips” for robust speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, ii:II/669–II/672.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al., 2016</span>
<span class="ltx_bibblock">
Chan, W., Jaitly, N., Le, Q., and Vinyals, O.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 4960–4964.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cox et al., 2008</span>
<span class="ltx_bibblock">
Cox, S. J., Harvey, R. W., Lan, Y., Newman, J. L., and Theobald, B.-J.

</span>
<span class="ltx_bibblock">(2008).

</span>
<span class="ltx_bibblock">The challenge of multispeaker lip-reading.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">AVSP</span>, pages 179–184.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denby et al., 2010</span>
<span class="ltx_bibblock">
Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J. M., and Brumberg, J. S.

</span>
<span class="ltx_bibblock">(2010).

</span>
<span class="ltx_bibblock">Silent speech interfaces.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, 52(4):270–287.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al., 2018</span>
<span class="ltx_bibblock">
Dong, L., Xu, S., and Xu, B.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 5884–5888.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dungan et al., 2018</span>
<span class="ltx_bibblock">
Dungan, L., Karaali, A., and Harte, N.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">The impact of reduced video quality on visual speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">ICIP</span>, pages 2560–2564.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ezz et al., 2020</span>
<span class="ltx_bibblock">
Ezz, M., Mostafa, A. M., and Nasr, A. A.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">A silent password recognition framework based on lip analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:55354–55371.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez-Lopez and Sukno, 2018</span>
<span class="ltx_bibblock">
Fernandez-Lopez, A. and Sukno, F. M.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Survey on automatic lip-reading in the era of deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">Image and Vision Computing</span>, 78:53–72.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernández-López and Sukno, 2017</span>
<span class="ltx_bibblock">
Fernández-López, A. and Sukno, F.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Optimizing phoneme-to-viseme mapping for continuous lip-reading in spanish.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">International Joint Conference on Computer Vision, Imaging and Computer Graphics</span>, pages 305–328. Springer.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gales and Young, 2008</span>
<span class="ltx_bibblock">
Gales, M. and Young, S.

</span>
<span class="ltx_bibblock">(2008).

</span>
<span class="ltx_bibblock"><span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">The application of hidden Markov models in speech recognition</span>.

</span>
<span class="ltx_bibblock">Now Publishers Inc.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gimeno-Gómez and Martínez-Hinarejos, 2021</span>
<span class="ltx_bibblock">
Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Analysis of Visual Features for Continuous Lipreading in Spanish.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">Proc. IberSPEECH</span>, pages 220–224.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gimeno-Gómez and Martínez-Hinarejos, 2022</span>
<span class="ltx_bibblock">
Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">Proceedings of LREC</span>, pages 2750–2758, June.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gimeno-Gómez and Martínez-Hinarejos, 2023</span>
<span class="ltx_bibblock">
Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Comparing speaker adaptation methods for visual speech recognition for continuous spanish.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 13(11).

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonzalez-Lopez et al., 2020</span>
<span class="ltx_bibblock">
Gonzalez-Lopez, J. A., Gomez-Alanis, A., Martín Doñas, J. M., Pérez-Córdoba, J. L., and Gomez, A. M.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Silent speech interfaces for speech restoration: A review.

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:177995–178021.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves and Jaitly, 2014</span>
<span class="ltx_bibblock">
Graves, A. and Jaitly, N.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Towards end-to-end speech recognition with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">ICML</span>, pages 1764–1772.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al., 2006</span>
<span class="ltx_bibblock">
Graves, A., Fernández, S., Gomez, F., and Schmidhuber, J.

</span>
<span class="ltx_bibblock">(2006).

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd ICML</span>, pages 369–376.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati et al., 2020</span>
<span class="ltx_bibblock">
Gulati, A., Qin, J., Chiu, C. C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang, R.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented Transformer for Speech Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 5036–5040.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al., 2016</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 770–778.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al., 2012</span>
<span class="ltx_bibblock">
Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B.

</span>
<span class="ltx_bibblock">(2012).

</span>
<span class="ltx_bibblock">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.1.1" class="ltx_text ltx_font_italic">Signal Processing Magazine</span>, 29(6):82–97.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">I.Loshchilov and Hutter, 2019</span>
<span class="ltx_bibblock">
I.Loshchilov and Hutter, F.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">ICLR</span>.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juang and Rabiner, 1991</span>
<span class="ltx_bibblock">
Juang, B. H. and Rabiner, L. R.

</span>
<span class="ltx_bibblock">(1991).

</span>
<span class="ltx_bibblock">Hidden markov models for speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx29.1.1" class="ltx_text ltx_font_italic">Technometrics</span>, 33(3):251–272.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juang, 1991</span>
<span class="ltx_bibblock">
Juang, B.

</span>
<span class="ltx_bibblock">(1991).

</span>
<span class="ltx_bibblock">Speech recognition in adverse environments.

</span>
<span class="ltx_bibblock"><span id="bib.bibx30.1.1" class="ltx_text ltx_font_italic">Computer Speech &amp; Language</span>, 5(3):275–294.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karita et al., 2019</span>
<span class="ltx_bibblock">
Karita, S., Chen, N., Hayashi, T., Hori, T., Inaguma, H., Jiang, Z., Someki, M., Soplin, N., Yamamoto, R., Wang, X., Watanabe, S., Yoshimura, T., and Zhang, W.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">A comparative study on transformer vs rnn in speech applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.1.1" class="ltx_text ltx_font_italic">ASRU</span>, pages 449–456.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al., 2021</span>
<span class="ltx_bibblock">
Kim, Y. J., Heo, H.-S., Choe, S., Chung, S.-W., Kwon, Y., Lee, B.-J., Kwon, Y., and Chung, J. S.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Look Who’s Talking: Active Speaker Detection in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3675–3679.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba, 2014</span>
<span class="ltx_bibblock">
Kingma, D. and Ba, J.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx33.1.1" class="ltx_text ltx_font_italic">Proc. of the 2nd ICLR</span>.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al., 2023</span>
<span class="ltx_bibblock">
Liu, X., Lakomkin, E., Vougioukas, K., Ma, P., Chen, H., Xie, R., Doulaty, M., Moritz, N., Kolar, J., Petridis, S., Pantic, M., and Fuegen, C.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Synthvsr: Scaling up visual speech recognition with synthetic supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 18806–18815.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lüscher et al., 2019</span>
<span class="ltx_bibblock">
Lüscher, C., Beck, E., Irie, K., Kitza, M., Michel, W., Zeyer, A., Schlüter, R., and Ney, H.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">RWTH ASR Systems for LibriSpeech: Hybrid vs Attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx35.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 231–235.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2021a</span>
<span class="ltx_bibblock">
Ma, P., Mira, R., Petridis, S., Schuller, B., and Pantic, M.

</span>
<span class="ltx_bibblock">(2021a).

</span>
<span class="ltx_bibblock">LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx36.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3011–3015.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2021b</span>
<span class="ltx_bibblock">
Ma, P., Petridis, S., and Pantic, M.

</span>
<span class="ltx_bibblock">(2021b).

</span>
<span class="ltx_bibblock">End-to-end audio-visual speech recognition with conformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx37.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 7613–7617.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2022</span>
<span class="ltx_bibblock">
Ma, P., Petridis, S., and Pantic, M.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Visual speech recognition for multiple languages in the wild.

</span>
<span class="ltx_bibblock"><span id="bib.bibx38.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, 4(11):930–939.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2023</span>
<span class="ltx_bibblock">
Ma, P., Haliassos, A., Fernandez-Lopez, A., Chen, H., Petridis, S., and Pantic, M.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Auto-avsr: Audio-visual speech recognition with automatic labels.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx39.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 1–5.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matthews et al., 2002</span>
<span class="ltx_bibblock">
Matthews, I., Cootes, T. F., Bangham, J. A., Cox, S., and Harvey, R.

</span>
<span class="ltx_bibblock">(2002).

</span>
<span class="ltx_bibblock">Extraction of visual features for lipreading.

</span>
<span class="ltx_bibblock"><span id="bib.bibx40.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on PAMI</span>, 24(2):198–213.

</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McGurk and MacDonald, 1976</span>
<span class="ltx_bibblock">
McGurk, H. and MacDonald, J.

</span>
<span class="ltx_bibblock">(1976).

</span>
<span class="ltx_bibblock">Hearing lips and seeing voices.

</span>
<span class="ltx_bibblock"><span id="bib.bibx41.1.1" class="ltx_text ltx_font_italic">Nature</span>, 264(5588):746–748.

</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al., 2008</span>
<span class="ltx_bibblock">
Mohri, M., Pereira, F., and Riley, M.

</span>
<span class="ltx_bibblock">(2008).

</span>
<span class="ltx_bibblock">Speech recognition with weighted finite-state transducers.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx42.1.1" class="ltx_text ltx_font_italic">Springer Handbook of Speech Processing</span>, pages 559–584. Springer.

</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nemani et al., 2023</span>
<span class="ltx_bibblock">
Nemani, P., Krishna, G. S., Supriya, K., and Kumar, S.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Speaker independent vsr: A systematic review and futuristic applications.

</span>
<span class="ltx_bibblock"><span id="bib.bibx43.1.1" class="ltx_text ltx_font_italic">Image and Vision Computing</span>, 138:104787.

</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al., 2018</span>
<span class="ltx_bibblock">
Ott, M., Edunov, S., Grangier, D., and Auli, M.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Scaling neural machine translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx44.1.1" class="ltx_text ltx_font_italic">Proc. of the 3rd Conference on Machine Translation</span>, pages 1–9. ACL.

</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Potamianos et al., 2001</span>
<span class="ltx_bibblock">
Potamianos, G., Luettin, J., and Neti, C.

</span>
<span class="ltx_bibblock">(2001).

</span>
<span class="ltx_bibblock">Hierarchical discriminant features for audio-visual lvcsr.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx45.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, volume 1, pages 165–168.

</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Potamianos et al., 2003</span>
<span class="ltx_bibblock">
Potamianos, G., Neti, C., Gravier, G., Garg, A., and Senior, A. W.

</span>
<span class="ltx_bibblock">(2003).

</span>
<span class="ltx_bibblock">Recent advances in the automatic recognition of audiovisual speech.

</span>
<span class="ltx_bibblock"><span id="bib.bibx46.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE</span>, 91(9):1306–1326.

</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Povey et al., 2011</span>
<span class="ltx_bibblock">
Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K.

</span>
<span class="ltx_bibblock">(2011).

</span>
<span class="ltx_bibblock">The kaldi speech recognition toolkit.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx47.1.1" class="ltx_text ltx_font_italic">ASRU</span>, number EPFL-CONF-192584.

</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al., 2021</span>
<span class="ltx_bibblock">
Prajwal, K., Momeni, L., Afouras, T., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Visual keyword spotting with attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx48.1.1" class="ltx_text ltx_font_italic">British Machine Vision Conference (BMVC)</span>.

</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al., 2022</span>
<span class="ltx_bibblock">
Prajwal, K. R., Afouras, T., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Sub-word level lip reading with visual attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx49.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 5162–5172.

</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quilis, 1997</span>
<span class="ltx_bibblock">
Quilis, A.

</span>
<span class="ltx_bibblock">(1997).

</span>
<span class="ltx_bibblock"><span id="bib.bibx50.1.1" class="ltx_text ltx_font_italic">Principios de fonología y fonética españolas</span>, volume 43 of <span id="bib.bibx50.2.2" class="ltx_text ltx_font_italic">Cuadernos de lengua española</span>.

</span>
<span class="ltx_bibblock">Arco libros.

</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramachandran et al., 2017</span>
<span class="ltx_bibblock">
Ramachandran, P., Zoph, B., and Le, Q. V.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Searching for activation functions.

</span>
<span class="ltx_bibblock"><span id="bib.bibx51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.05941</span>.

</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al., 2022</span>
<span class="ltx_bibblock">
Shi, B., Hsu, W. N., Lakhotia, K., and Mohamed, A.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Learning audio-visual speech representation by masked multimodal cluster prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bibx52.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.02184</span>.

</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith and Topin, 2019</span>
<span class="ltx_bibblock">
Smith, L. N. and Topin, N.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Super-convergence: very fast training of neural networks using large learning rates.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx53.1.1" class="ltx_text ltx_font_italic">AI and ML for Multi-Domain Operations Applications</span>, volume 11006, pages 369–386. SPIE.

</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Son Chung et al., 2017</span>
<span class="ltx_bibblock">
Son Chung, J., Senior, A., Vinyals, O., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Lip reading sentences in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx54.1.1" class="ltx_text ltx_font_italic">Proc. of the CVPR</span>, pages 6447–6456.

</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stafylakis and Tzimiropoulos, 2018</span>
<span class="ltx_bibblock">
Stafylakis, T. and Tzimiropoulos, G.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Zero-shot keyword spotting for visual speech recognition in-the-wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx55.1.1" class="ltx_text ltx_font_italic">Proc. of the ECCV</span>, pages 513–529.

</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al., 2021</span>
<span class="ltx_bibblock">
Tao, R., Pan, Z., Das, R., Qian, X., Shou, M., and Li, h.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx56.1.1" class="ltx_text ltx_font_italic">Proc. of the 29th ACM International Conference on Multimedia</span>, page 3927–3935. Association for Computing Machinery.

</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thangthai and Harvey, 2017</span>
<span class="ltx_bibblock">
Thangthai, K. and Harvey, R.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Improving computer lipreading via dnn sequence discriminative training techniques.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx57.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3657–3661.

</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thangthai, 2018</span>
<span class="ltx_bibblock">
Thangthai, K.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock"><span id="bib.bibx58.1.1" class="ltx_text ltx_font_italic">Computer lipreading via hybrid deep neural network hidden Markov models</span>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, University of East Anglia.

</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tseng et al., 2023</span>
<span class="ltx_bibblock">
Tseng, Y., Berry, L., Chen, Y.-T., Chiu, I., Lin, H.-H., Liu, M., Peng, P., Shih, Y.-J., Wang, H.-Y., Wu, H., Huang, P.-Y., Lai, C.-M., Li, S.-W., Harwath, D., Tsao, Y., Watanabe, S., Mohamed, A., Feng, C.-L., and Lee, H.-Y.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Av-superb: A multi-task evaluation benchmark for audio-visual representation models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.10787</span>.

</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al., 2017</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bibx60.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 30:6000–6010.

</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al., 2019</span>
<span class="ltx_bibblock">
Wang, D., Wang, X., and Lv, S.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">An overview of end-to-end automatic speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx61.1.1" class="ltx_text ltx_font_italic">Symmetry</span>, 11(8).

</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe et al., 2017</span>
<span class="ltx_bibblock">
Watanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Hybrid ctc/attention architecture for end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx62.1.1" class="ltx_text ltx_font_italic">IEEE JSTSP</span>, 11(8):1240–1253.

</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe et al., 2018</span>
<span class="ltx_bibblock">
Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., E. Y. Soplin, N., Heymann, J., Wiesner, M., Chen, N., Renduchintala, A., and Ochiai, T.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">ESPnet: End-to-End Speech Processing Toolkit.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx63.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2207–2211.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.13003" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.13004" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.13004">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.13004" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.13005" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:15:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
