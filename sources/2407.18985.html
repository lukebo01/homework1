<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.18985] Implementation and Applications of WakeWords Integrated with Speaker Recognition: A Case Study</title><meta property="og:description" content="This paper explores the application of artificial intelligence techniques in audio and voice processing, focusing on the integration of wake words and speaker recognition for secure access in embedded systems. With the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Implementation and Applications of WakeWords Integrated with Speaker Recognition: A Case Study">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Implementation and Applications of WakeWords Integrated with Speaker Recognition: A Case Study">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.18985">

<!--Generated on Mon Aug  5 16:25:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Audio Processing,  Wake Words,  Speaker Recognition,  Embedded Systems,  Synthetic Data
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Implementation and Applications of WakeWords Integrated with Speaker Recognition: A Case Study
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">


<span id="id1.1.id1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="id1.1.id1.1" class="ltx_p">Alexandre Costa Ferro Filho</span>
<span id="id1.1.id1.2" class="ltx_p ltx_align_center"><span id="id1.1.id1.2.1" class="ltx_text ltx_font_italic">Computer Institute</span></span>
<span id="id1.1.id1.3" class="ltx_p ltx_align_center"><span id="id1.1.id1.3.1" class="ltx_text ltx_font_italic">Goias Federal University
<br class="ltx_break"></span>Goiânia, Brazil</span>
<span id="id1.1.id1.4" class="ltx_p ltx_align_center">alexandre_ferro@discente.ufg.br</span>
</span>
 
<span id="id2.2.id2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="id2.2.id2.1" class="ltx_p">Elisa Ayumi Masasi de Oliveira</span>
<span id="id2.2.id2.2" class="ltx_p ltx_align_center"><span id="id2.2.id2.2.1" class="ltx_text ltx_font_italic">Computer Institute</span></span>
<span id="id2.2.id2.3" class="ltx_p ltx_align_center"><span id="id2.2.id2.3.1" class="ltx_text ltx_font_italic">Goias Federal University
<br class="ltx_break"></span>Goiânia, Brazil</span>
<span id="id2.2.id2.4" class="ltx_p ltx_align_center">ayumi@discente.ufg.br</span>
</span>

<br class="ltx_break">
<br class="ltx_break"> 
<span id="id3.3.id3" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="id3.3.id3.1" class="ltx_p">Iago Alves Brito</span>
<span id="id3.3.id3.2" class="ltx_p ltx_align_center"><span id="id3.3.id3.2.1" class="ltx_text ltx_font_italic">Computer Institute</span></span>
<span id="id3.3.id3.3" class="ltx_p ltx_align_center"><span id="id3.3.id3.3.1" class="ltx_text ltx_font_italic">Goias Federal University
<br class="ltx_break"></span>Goiânia, Brazil</span>
<span id="id3.3.id3.4" class="ltx_p ltx_align_center">iagoalves@discente.ufg.br</span>
</span>
 
<span id="id4.4.id4" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:195.1pt;">
<span id="id4.4.id4.1" class="ltx_p">Pedro Martins Bittencourt</span>
<span id="id4.4.id4.2" class="ltx_p ltx_align_center"><span id="id4.4.id4.2.1" class="ltx_text ltx_font_italic">Computer Institute</span></span>
<span id="id4.4.id4.3" class="ltx_p ltx_align_center"><span id="id4.4.id4.3.1" class="ltx_text ltx_font_italic">Goias Federal University
<br class="ltx_break"></span>Goiânia, Brazil</span>
<span id="id4.4.id4.4" class="ltx_p ltx_align_center">bittencourtpedro@discente.ufg.br</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">This paper explores the application of artificial intelligence techniques in audio and voice processing, focusing on the integration of wake words and speaker recognition for secure access in embedded systems. With the growing prevalence of voice-activated devices such as Amazon Alexa, ensuring secure and user-specific interactions has become paramount. Our study aims to enhance the security framework of these systems by leveraging wake words for initial activation and speaker recognition to validate user permissions. By incorporating these AI-driven methodologies, we propose a robust solution that restricts system usage to authorized individuals, thereby mitigating unauthorized access risks. This research delves into the algorithms and technologies underpinning wake word detection and speaker recognition, evaluates their effectiveness in real-world applications, and discusses the potential for their implementation in various embedded systems, emphasizing security and user convenience. The findings underscore the feasibility and advantages of employing these AI techniques to create secure, user-friendly voice-activated systems.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Audio Processing, Wake Words, Speaker Recognition, Embedded Systems, Synthetic Data

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the popularization of voice-activated devices has transformed the way users interact with technology. Devices like Amazon Alexa, Google Home, and Apple Siri have popularized voice as a convenient interface for accessing information, controlling smart home devices, and performing various tasks. However, as the adoption of these devices grows, the concern for security and privacy becomes an important discussed topic, since the unauthorized access to these systems can lead to breaches of personal information and unauthorized control of connected devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address these concerns, the integration of wake words and speaker recognition in voice-activated systems has emerged as a promising solution. Wake words, such as ”Alexa” or ”Hey Siri” serve as verbal triggers that activate the device and prepare it to receive commands. Speaker recognition, on the other hand, involves identifying and verifying the identity of the speaker based on their unique vocal characteristics. Together, these technologies can enhance the security framework of voice-activated systems by ensuring that only authorized users can interact with the device.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To develop the wake word detection model, synthetic data was employed to augment the training dataset. The use of synthetic data is a crucial aspect of our approach, as it allows for the creation of diverse and extensive training examples, which are essential for building robust and accurate models. This technique is particularly valuable when dealing with the scarcity of real-world data, especially for specific wake words or in scenarios where data privacy concerns limit the availability of authentic recordings. The synthetic data generation process involved creating varied and realistic audio samples that mimic the characteristics of natural speech, thus providing the model with a broad range of vocal patterns and environmental conditions to learn from.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In the following sections, we provide an overview of the current state of wake word detection and speaker recognition technologies, describe the methodology and experimental setup used in our research, present the results of our experiments, and discuss the implications of our findings. Finally, we conclude with a discussion of future research directions and the potential for further advancements in this field.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Voice-Activated Systems and Security Concerns</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Voice-activated systems have become increasingly prevalent in everyday life, offering hands-free convenience and intuitive interfaces for interacting with technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These systems rely on Natural Language Processing (NLP) and machine learning (ML) to understand and respond to user commands. However, the widespread use of voice commands also introduces significant security and privacy challenges. Unauthorized users can potentially access sensitive information or control connected devices, leading to security breaches.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Wake Words</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Wake words are predefined phrases used to activate voice-activated systems, signaling that the device is ready to receive commands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This mechanism serves as an initial security layer by requiring a specific trigger before any interaction can occur. The detection of wake words must be both precise and efficient, ensuring that the system responds only to intentional activations while minimizing false positives and negatives.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Speaker Recognition</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Speaker recognition technology identifies and verifies individuals based on their unique vocal characteristics. This process involves two main stages: speaker identification and speaker verification. Speaker identification determines the identity of the speaker from a group of known individuals, while speaker verification confirms whether the speaker matches a claimed identity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Speaker recognition enhances security in voice-activated systems by restricting access to authorized users. This capability is critical for applications involving sensitive information or critical functions, where it is essential to ensure that only specific individuals can issue commands.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Data Collection and Synthetic Data Generation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In developing robust wake word detection and speaker recognition systems, data quality and diversity are crucial. For wake word detection, the training dataset included both real and synthetic audio samples. The real data comprised a small variety of wake word samples from 3 different speakers. However, synthetic data was generated to supplement the real dataset, representing the majority part of the dataset. This synthetic data was created using text-to-speech (TTS) systems and audio augmentation techniques, such as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, producing a wide range of samples that mimic natural variations in pronunciation, background noise, and recording conditions. This approach is particularly useful in our scenario where authentic data is limited. The inclusion of synthetic data allows for more comprehensive training, improving the model’s ability to generalize across different speakers and environmental conditions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Wake Word Detection Model</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The wake word detection system was built using a deep learning architecture designed to handle the variability and complexity of speech data. The model architecture incorporated a Convolutional Neural Network (CNN) for feature extraction. The use of CNN layers enabled the model to learn hierarchical representations of the audio data, which is crucial for accurately detecting the presence of wake words, where, in this case, the wake word ”Hey, Gris!” was chosen.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To enhance the performance of wake word models, we investigated and found that including the use of synthetic data is beneficial to model performance when using the Google proposed model
c.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Speaker Recognition System</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The speaker recognition component aimed to authenticate users by verifying their identity based on voice characteristics. For this purpose, we employed the Titanet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, a state-of-the-art model designed for robust speaker recognition. Titanet, known for its efficiency and accuracy, utilizes an end-to-end deep learning approach that integrates feature extraction and classification within a unified framework. This architecture, represented in figure <a href="#S3.F1" title="Figure 1 ‣ III-C Speaker Recognition System ‣ III Methodology ‣ Implementation and Applications of WakeWords Integrated with Speaker Recognition: A Case Study" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is particularly well-suited for processing variable-length audio segments and capturing the nuanced vocal characteristics that distinguish different speakers.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Titanet consists of multiple layers, including residual blocks and attention mechanisms, which enhance the model’s ability to focus on relevant features in the audio data. The training process for Titanet involved a large dataset of labeled speaker data, ensuring that the model could learn to accurately differentiate between speakers. The use of attention mechanisms allowed Titanet to dynamically weigh different parts of the input audio, improving its capacity to handle variations in speaking style and environment.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.18985/assets/TitaNet.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>NVIDIA TitaNet model architecture.</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To enhance security and reduce the likelihood of spoofing or impersonation, the system incorporated anti-spoofing measures, including the detection of synthetic voices and other anomalies that could indicate unauthorized access attempts.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Empirical Evaluation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The effectiveness of our systems was assessed through an empirical evaluation conducted in real-world environments. The primary focus was on evaluating the response time and reliability of both the wake word detection and speaker recognition models under various conditions, including different noise levels and speaker variations. This practical testing aimed to simulate real-world scenarios where these systems would typically be deployed, such as in homes or offices.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Preliminary results indicate that the system effectively recognizes wake words and accurately identifies speakers, even in challenging acoustic environments. The use of synthetic data during training has significantly enhanced the model’s robustness, allowing it to generalize well across different voices and background noises. However, while these results are promising, they were primarily based on qualitative observations. Future evaluations would benefit from a more detailed quantitative analysis, which could offer a clearer understanding of the system’s performance and highlight specific areas for improvement. This approach would provide a more comprehensive assessment of the system’s capabilities and help guide further refinement and optimization efforts.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study has demonstrated the potential of integrating artificial intelligence techniques, such as wake word detection and speaker recognition, to enhance the security of voice-activated systems in embedded environments. By leveraging wake words for system activation and speaker recognition for user verification, the approach restricts system usage to authorized individuals, thereby mitigating the risks of unauthorized access. The use of synthetic data in training the wake word detection model was crucial, enabling the system to generalize across diverse speakers and environmental conditions, while the Titanet architecture provided robust speaker verification.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The empirical evaluation confirmed the effectiveness of these AI-driven methodologies in real-world settings, showcasing their ability to create secure and user-friendly voice-activated systems. The integration with ROS and Docker streamlined the development and deployment processes, ensuring adaptability across various hardware platforms. Future research could focus on optimizing models for low-resource environments, enhancing anti-spoofing measures, and expanding the use of synthetic data to further secure and refine voice-activated technologies.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Ko, V. Peddinti, D. Povey, and S. Khudanpur.

</span>
<span class="ltx_bibblock">Data augmentation for deep neural network acoustic modeling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</span>, pages 3586–3589, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Nithin Rao Koluguri, Taejin Park, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Titanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 8102–8106. IEEE, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Lau, B. Zimmerman, and F. Schaub.

</span>
<span class="ltx_bibblock">Alexa, are you listening? privacy perceptions, concerns and privacy-seeking behaviors with smart speakers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 ACM Conference on Human Factors in Computing Systems</span>, pages 1–12. ACM, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Michael Price, James Glass, and Anantha P Chandrakasan.

</span>
<span class="ltx_bibblock">14.4 a scalable speech recognizer with deep-neural-network acoustic models and voice-activated power gating.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Solid-State Circuits Conference (ISSCC)</span>, pages 244–245. IEEE, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chan Zhen Yue and Shum Ping.

</span>
<span class="ltx_bibblock">Voice activated smart home design and implementation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2017 2nd International Conference on Frontiers of Sensors Technologies (ICFST)</span>, pages 489–492. IEEE, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chan Zhen Yue and Shum Ping.

</span>
<span class="ltx_bibblock">Voice activated smart home design and implementation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">2017 2nd International Conference on Frontiers of Sensors Technologies (ICFST)</span>, pages 489–492. IEEE, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.18984" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.18985" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.18985">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.18985" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.18987" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 16:25:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
