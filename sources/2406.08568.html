<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.08568] Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1].</title><meta property="og:description" content="ASR research has achieved impressive performance in recent years and has significant potential for enabling access for  people with dysarthria (PwD) in  augmentative and alternative communication (AAC) and home environ…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1].">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1].">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.08568">

<!--Generated on Fri Jul  5 20:45:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]Wing-ZinLeung
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]MattiasCross
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]AntonRagni
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]StefanGoetze



<span id="p1.3.4" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Training Data Augmentation for Dysarthric Automatic Speech Recognition 
<br class="ltx_break">by Text-to-Dysarthric-Speech Synthesis<span id="id10.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1].</span></span></span>
</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span id="id12.id1" class="ltx_ERROR undefined" lang="en">\Ac</span>
<p id="id13.id2" class="ltx_p"><span id="id13.id2.1" class="ltx_text" lang="en">ASR research has achieved impressive performance in recent years and has significant potential for enabling access for  <span title="people with dysarthria" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">people with dysarthria</span></span> (<abbr title="people with dysarthria" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PwD</span></abbr>) in  <span title="augmentative and alternative communication" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">augmentative and alternative communication</span></span> (<abbr title="augmentative and alternative communication" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AAC</span></abbr>) and home environment systems. However,
progress in  <span title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">dysarthric ASR</span></span> (<abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr>) has been limited by high variability in dysarthric speech and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using  <span title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">text-to-dysarthic-speech</span></span> (<abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr>) synthesis for finetuning large  <span title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">automatic speech recognition</span></span> (<abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr>) models is effective for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr>. Specifically, diffusion-based  <span title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">text-to-speech</span></span> (<abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr>) models can produce speech samples similar to dysarthric speech that can be used as additional training data for fine-tuning <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> foundation models, in this case Whisper.
Results show improved synthesis metrics and <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> performance for the proposed multi-speaker diffusion-based <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> data augmentation for <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> fine-tuning compared to current <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> baselines.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Dysarthric speech recognition, diffusion, text-to-speech synthesis, data augmentation
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Dysarthria is a type of  <span title="motor speech disorder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">motor speech disorder</span></span> (<abbr title="motor speech disorder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSD</span></abbr>) that reflects abnormalities in motor movements required for speech production <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The psychosocial impact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and restrictions on functioning and participation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for <abbr title="people with dysarthria" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PwD</span></abbr> are well documented <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. <span id="S1.p1.1.1" class="ltx_ERROR undefined">\Ac</span>DASR has important implications for <abbr title="augmentative and alternative communication" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AAC</span></abbr> devices and home environmental control systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Although the accuracy of <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> systems for typical speech has improved significantly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, there are challenges inherent with <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr>. Due to high inter- and intra-speaker variability in dysarthric speech and limited public availability of dysarthric data, generic <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models usually do not generalise well to dysarthric speakers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Baseline <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, typically trained on larger amounts of typical speech data, can be adapted to domains with limited data availability, such as dysarthric speech recordings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Model adaptation approaches show improved performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
and deep learning in combination with data augmentation techniques to address data sparsity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have achieved  <span title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">state-of-the-art</span></span> (<abbr title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SotA</span></abbr>) performance for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Transformer-based models have not been adequately explored for dysarthric speech as such architectures require a significant amount of training data that is not publicly available. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> implements a spatial  <span title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">convolutional neural network</span></span> (<abbr title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr>) with multi-head Transformers (pre-trained on control speaker data) to recognise visual representations of whole-word dysarthric speech, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> use dysarthric and typical speech corpora with data augmentation techniques to implement two-step parameter adjustments to train a dysarthric Transformer model. A  <span title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">recurrent neural network</span></span> (<abbr title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RNN</span></abbr>)-Transducer model has been trained on the Euphonia dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which contains over <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="1400" display="inline"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">1400</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn type="integer" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">1400</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">1400</annotation></semantics></math> hours of audio data, however, the dataset is not publicly available. Representations from foundation <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models have been used as input features for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> systems (e.g. Wav2Vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>), but foundation models have not yet been adapted for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Data augmentation techniques have been widely studied for typical speech tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, but data augmentation for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> requires further research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Spectro-temporal differences between typical and dysarthric speech (e.g. speaking rate) have influenced approaches, such as  <span title="vocal tract length perturbation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">vocal tract length perturbation</span></span> (<abbr title="vocal tract length perturbation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VTLP</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, tempo-stretching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and speed perturbation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Although slower speaking rates and modifications to the spectral envelope can be modelled, perceptual dysarthric speech characteristics (e.g. articulatory imprecision or voice quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>) are not captured. Subsequently, <span title="generative adverserial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">generative adverserial networks</span></span> have been applied to speed-perturbed typical speech for speech synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and  <span title="voice conversion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">voice conversion</span></span> (<abbr title="voice conversion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VC</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Also, Transformer-based systems have been implemented for <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Recently,  <span title="diffusion probabilistic modelling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">diffusion probabilistic modelling</span></span> (<abbr title="diffusion probabilistic modelling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DPM</span></abbr>) has been applied to a <abbr title="voice conversion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VC</span></abbr> task for dysarthric data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The results demonstrate improved  <span title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">word error rate</span></span> (<abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr>) performance of <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> systems using augmented data, and subjective evaluations by human expert listeners show that severity characteristics of dysarthric speech are captured in the synthesis. This paper proposes (i) to create <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> training data by <abbr title="diffusion probabilistic modelling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DPM</span></abbr>, training Grad-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> from scratch on dysarthric data to (ii) analyse the use of additional augmented data only to finetune large <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models (here Whisper), i.e. without matched control speaker data.
The remainder of the paper is structured as follows: <a href="#S2" title="2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 2</span></a> describes the <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> system and the <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model adaptation. Experiments are described in <a href="#S3" title="3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3</span></a> and their results are presented in Section <a href="#S4" title="4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. <a href="#S5" title="5 Conclusion ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 5</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dysarthric Speech Synthesis</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To synthesise dysarthric mel-spectrogram data <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">{\mathbf{X}}</annotation></semantics></math> for <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> augmentation, we train the Grad-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> model.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Grad-TTS code adapted from <a target="_blank" href="https://github.com/huawei-noah/Speech-Backbones" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huawei-noah/Speech-Backbones</a>. Implementation available at
<a target="_blank" href="https://github.com/WingZLeung/TTDS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/WingZLeung/TTDS</a>.
</span></span></span></p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="{\mathbf{X}}=\textbf{Grad-TTS}({\mathbf{y}},s_{\mathrm{E}})" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">𝐗</mi><mo id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.2.2.1.3a.cmml">Grad-TTS</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">𝐲</mi><mo id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.2.cmml">,</mo><msub id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">s</mi><mi mathvariant="normal" id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">E</mi></msub><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.4" xref="S2.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></eq><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">𝐗</ci><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><times id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.2"></times><ci id="S2.E1.m1.2.2.1.3a.cmml" xref="S2.E1.m1.2.2.1.3"><mtext class="ltx_mathvariant_bold" id="S2.E1.m1.2.2.1.3.cmml" xref="S2.E1.m1.2.2.1.3">Grad-TTS</mtext></ci><interval closure="open" id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝐲</ci><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2">𝑠</ci><ci id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">E</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">{\mathbf{X}}=\textbf{Grad-TTS}({\mathbf{y}},s_{\mathrm{E}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.19" class="ltx_p">from scratch on dysarthric data to augment the limited available dysarthric speech data for a speaker identity <math id="S2.SS1.p1.2.m1.1" class="ltx_Math" alttext="s_{\mathrm{E}}" display="inline"><semantics id="S2.SS1.p1.2.m1.1a"><msub id="S2.SS1.p1.2.m1.1.1" xref="S2.SS1.p1.2.m1.1.1.cmml"><mi id="S2.SS1.p1.2.m1.1.1.2" xref="S2.SS1.p1.2.m1.1.1.2.cmml">s</mi><mi mathvariant="normal" id="S2.SS1.p1.2.m1.1.1.3" xref="S2.SS1.p1.2.m1.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m1.1b"><apply id="S2.SS1.p1.2.m1.1.1.cmml" xref="S2.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m1.1.1.1.cmml" xref="S2.SS1.p1.2.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m1.1.1.2.cmml" xref="S2.SS1.p1.2.m1.1.1.2">𝑠</ci><ci id="S2.SS1.p1.2.m1.1.1.3.cmml" xref="S2.SS1.p1.2.m1.1.1.3">E</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m1.1c">s_{\mathrm{E}}</annotation></semantics></math>. Grad-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> produces mel-spectrograms matrices <math id="S2.SS1.p1.3.m2.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.3.m2.1a"><mi id="S2.SS1.p1.3.m2.1.1" xref="S2.SS1.p1.3.m2.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m2.1b"><ci id="S2.SS1.p1.3.m2.1.1.cmml" xref="S2.SS1.p1.3.m2.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m2.1c">{\mathbf{X}}</annotation></semantics></math> of size <math id="S2.SS1.p1.4.m3.1" class="ltx_Math" alttext="F\times L" display="inline"><semantics id="S2.SS1.p1.4.m3.1a"><mrow id="S2.SS1.p1.4.m3.1.1" xref="S2.SS1.p1.4.m3.1.1.cmml"><mi id="S2.SS1.p1.4.m3.1.1.2" xref="S2.SS1.p1.4.m3.1.1.2.cmml">F</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.4.m3.1.1.1" xref="S2.SS1.p1.4.m3.1.1.1.cmml">×</mo><mi id="S2.SS1.p1.4.m3.1.1.3" xref="S2.SS1.p1.4.m3.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m3.1b"><apply id="S2.SS1.p1.4.m3.1.1.cmml" xref="S2.SS1.p1.4.m3.1.1"><times id="S2.SS1.p1.4.m3.1.1.1.cmml" xref="S2.SS1.p1.4.m3.1.1.1"></times><ci id="S2.SS1.p1.4.m3.1.1.2.cmml" xref="S2.SS1.p1.4.m3.1.1.2">𝐹</ci><ci id="S2.SS1.p1.4.m3.1.1.3.cmml" xref="S2.SS1.p1.4.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m3.1c">F\times L</annotation></semantics></math>, with <math id="S2.SS1.p1.5.m4.1" class="ltx_Math" alttext="F=80" display="inline"><semantics id="S2.SS1.p1.5.m4.1a"><mrow id="S2.SS1.p1.5.m4.1.1" xref="S2.SS1.p1.5.m4.1.1.cmml"><mi id="S2.SS1.p1.5.m4.1.1.2" xref="S2.SS1.p1.5.m4.1.1.2.cmml">F</mi><mo id="S2.SS1.p1.5.m4.1.1.1" xref="S2.SS1.p1.5.m4.1.1.1.cmml">=</mo><mn id="S2.SS1.p1.5.m4.1.1.3" xref="S2.SS1.p1.5.m4.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m4.1b"><apply id="S2.SS1.p1.5.m4.1.1.cmml" xref="S2.SS1.p1.5.m4.1.1"><eq id="S2.SS1.p1.5.m4.1.1.1.cmml" xref="S2.SS1.p1.5.m4.1.1.1"></eq><ci id="S2.SS1.p1.5.m4.1.1.2.cmml" xref="S2.SS1.p1.5.m4.1.1.2">𝐹</ci><cn type="integer" id="S2.SS1.p1.5.m4.1.1.3.cmml" xref="S2.SS1.p1.5.m4.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m4.1c">F=80</annotation></semantics></math> in this work, and <math id="S2.SS1.p1.6.m5.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.p1.6.m5.1a"><mi id="S2.SS1.p1.6.m5.1.1" xref="S2.SS1.p1.6.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m5.1b"><ci id="S2.SS1.p1.6.m5.1.1.cmml" xref="S2.SS1.p1.6.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m5.1c">L</annotation></semantics></math> being the variable number of frames. This starts by forming initial distributions centred on the Grad-TTS text-encoder output <math id="S2.SS1.p1.7.m6.1" class="ltx_Math" alttext="{\mathbf{E}_{\theta}}({\mathbf{y}})" display="inline"><semantics id="S2.SS1.p1.7.m6.1a"><mrow id="S2.SS1.p1.7.m6.1.2" xref="S2.SS1.p1.7.m6.1.2.cmml"><msub id="S2.SS1.p1.7.m6.1.2.2" xref="S2.SS1.p1.7.m6.1.2.2.cmml"><mi id="S2.SS1.p1.7.m6.1.2.2.2" xref="S2.SS1.p1.7.m6.1.2.2.2.cmml">𝐄</mi><mi id="S2.SS1.p1.7.m6.1.2.2.3" xref="S2.SS1.p1.7.m6.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.7.m6.1.2.1" xref="S2.SS1.p1.7.m6.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.7.m6.1.2.3.2" xref="S2.SS1.p1.7.m6.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.7.m6.1.2.3.2.1" xref="S2.SS1.p1.7.m6.1.2.cmml">(</mo><mi id="S2.SS1.p1.7.m6.1.1" xref="S2.SS1.p1.7.m6.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.SS1.p1.7.m6.1.2.3.2.2" xref="S2.SS1.p1.7.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m6.1b"><apply id="S2.SS1.p1.7.m6.1.2.cmml" xref="S2.SS1.p1.7.m6.1.2"><times id="S2.SS1.p1.7.m6.1.2.1.cmml" xref="S2.SS1.p1.7.m6.1.2.1"></times><apply id="S2.SS1.p1.7.m6.1.2.2.cmml" xref="S2.SS1.p1.7.m6.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m6.1.2.2.1.cmml" xref="S2.SS1.p1.7.m6.1.2.2">subscript</csymbol><ci id="S2.SS1.p1.7.m6.1.2.2.2.cmml" xref="S2.SS1.p1.7.m6.1.2.2.2">𝐄</ci><ci id="S2.SS1.p1.7.m6.1.2.2.3.cmml" xref="S2.SS1.p1.7.m6.1.2.2.3">𝜃</ci></apply><ci id="S2.SS1.p1.7.m6.1.1.cmml" xref="S2.SS1.p1.7.m6.1.1">𝐲</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m6.1c">{\mathbf{E}_{\theta}}({\mathbf{y}})</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with trainable parameters <math id="S2.SS1.p1.8.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p1.8.m7.1a"><mi id="S2.SS1.p1.8.m7.1.1" xref="S2.SS1.p1.8.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m7.1b"><ci id="S2.SS1.p1.8.m7.1.1.cmml" xref="S2.SS1.p1.8.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m7.1c">\theta</annotation></semantics></math> for a given text-sequence vector <math id="S2.SS1.p1.9.m8.1" class="ltx_Math" alttext="{\mathbf{y}}" display="inline"><semantics id="S2.SS1.p1.9.m8.1a"><mi id="S2.SS1.p1.9.m8.1.1" xref="S2.SS1.p1.9.m8.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m8.1b"><ci id="S2.SS1.p1.9.m8.1.1.cmml" xref="S2.SS1.p1.9.m8.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m8.1c">{\mathbf{y}}</annotation></semantics></math>, and applying a diffusion process (cf. (<a href="#S2.E4" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)) to denoise the initial distribution of the noisy mel-spectrogram matrix <math id="S2.SS1.p1.10.m9.3" class="ltx_Math" alttext="{\mathbf{X}}_{T}\sim\mathcal{N}({\mathbf{E}_{\theta}}({\mathbf{y}}),\mathbf{I})" display="inline"><semantics id="S2.SS1.p1.10.m9.3a"><mrow id="S2.SS1.p1.10.m9.3.3" xref="S2.SS1.p1.10.m9.3.3.cmml"><msub id="S2.SS1.p1.10.m9.3.3.3" xref="S2.SS1.p1.10.m9.3.3.3.cmml"><mi id="S2.SS1.p1.10.m9.3.3.3.2" xref="S2.SS1.p1.10.m9.3.3.3.2.cmml">𝐗</mi><mi id="S2.SS1.p1.10.m9.3.3.3.3" xref="S2.SS1.p1.10.m9.3.3.3.3.cmml">T</mi></msub><mo id="S2.SS1.p1.10.m9.3.3.2" xref="S2.SS1.p1.10.m9.3.3.2.cmml">∼</mo><mrow id="S2.SS1.p1.10.m9.3.3.1" xref="S2.SS1.p1.10.m9.3.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.10.m9.3.3.1.3" xref="S2.SS1.p1.10.m9.3.3.1.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m9.3.3.1.2" xref="S2.SS1.p1.10.m9.3.3.1.2.cmml">​</mo><mrow id="S2.SS1.p1.10.m9.3.3.1.1.1" xref="S2.SS1.p1.10.m9.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m9.3.3.1.1.1.2" xref="S2.SS1.p1.10.m9.3.3.1.1.2.cmml">(</mo><mrow id="S2.SS1.p1.10.m9.3.3.1.1.1.1" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.cmml"><msub id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.cmml"><mi id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.2" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.2.cmml">𝐄</mi><mi id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.3" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m9.3.3.1.1.1.1.1" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.1.cmml">​</mo><mrow id="S2.SS1.p1.10.m9.3.3.1.1.1.1.3.2" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m9.3.3.1.1.1.1.3.2.1" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.cmml">(</mo><mi id="S2.SS1.p1.10.m9.1.1" xref="S2.SS1.p1.10.m9.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.SS1.p1.10.m9.3.3.1.1.1.1.3.2.2" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p1.10.m9.3.3.1.1.1.3" xref="S2.SS1.p1.10.m9.3.3.1.1.2.cmml">,</mo><mi id="S2.SS1.p1.10.m9.2.2" xref="S2.SS1.p1.10.m9.2.2.cmml">𝐈</mi><mo stretchy="false" id="S2.SS1.p1.10.m9.3.3.1.1.1.4" xref="S2.SS1.p1.10.m9.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m9.3b"><apply id="S2.SS1.p1.10.m9.3.3.cmml" xref="S2.SS1.p1.10.m9.3.3"><csymbol cd="latexml" id="S2.SS1.p1.10.m9.3.3.2.cmml" xref="S2.SS1.p1.10.m9.3.3.2">similar-to</csymbol><apply id="S2.SS1.p1.10.m9.3.3.3.cmml" xref="S2.SS1.p1.10.m9.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m9.3.3.3.1.cmml" xref="S2.SS1.p1.10.m9.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.10.m9.3.3.3.2.cmml" xref="S2.SS1.p1.10.m9.3.3.3.2">𝐗</ci><ci id="S2.SS1.p1.10.m9.3.3.3.3.cmml" xref="S2.SS1.p1.10.m9.3.3.3.3">𝑇</ci></apply><apply id="S2.SS1.p1.10.m9.3.3.1.cmml" xref="S2.SS1.p1.10.m9.3.3.1"><times id="S2.SS1.p1.10.m9.3.3.1.2.cmml" xref="S2.SS1.p1.10.m9.3.3.1.2"></times><ci id="S2.SS1.p1.10.m9.3.3.1.3.cmml" xref="S2.SS1.p1.10.m9.3.3.1.3">𝒩</ci><interval closure="open" id="S2.SS1.p1.10.m9.3.3.1.1.2.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1"><apply id="S2.SS1.p1.10.m9.3.3.1.1.1.1.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1"><times id="S2.SS1.p1.10.m9.3.3.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.1"></times><apply id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.2.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.2">𝐄</ci><ci id="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.3.cmml" xref="S2.SS1.p1.10.m9.3.3.1.1.1.1.2.3">𝜃</ci></apply><ci id="S2.SS1.p1.10.m9.1.1.cmml" xref="S2.SS1.p1.10.m9.1.1">𝐲</ci></apply><ci id="S2.SS1.p1.10.m9.2.2.cmml" xref="S2.SS1.p1.10.m9.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m9.3c">{\mathbf{X}}_{T}\sim\mathcal{N}({\mathbf{E}_{\theta}}({\mathbf{y}}),\mathbf{I})</annotation></semantics></math> into an estimate of a training-distribution sample matrix <math id="S2.SS1.p1.11.m10.1" class="ltx_Math" alttext="{\mathbf{X}}_{0}" display="inline"><semantics id="S2.SS1.p1.11.m10.1a"><msub id="S2.SS1.p1.11.m10.1.1" xref="S2.SS1.p1.11.m10.1.1.cmml"><mi id="S2.SS1.p1.11.m10.1.1.2" xref="S2.SS1.p1.11.m10.1.1.2.cmml">𝐗</mi><mn id="S2.SS1.p1.11.m10.1.1.3" xref="S2.SS1.p1.11.m10.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m10.1b"><apply id="S2.SS1.p1.11.m10.1.1.cmml" xref="S2.SS1.p1.11.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m10.1.1.1.cmml" xref="S2.SS1.p1.11.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.11.m10.1.1.2.cmml" xref="S2.SS1.p1.11.m10.1.1.2">𝐗</ci><cn type="integer" id="S2.SS1.p1.11.m10.1.1.3.cmml" xref="S2.SS1.p1.11.m10.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m10.1c">{\mathbf{X}}_{0}</annotation></semantics></math>, with <math id="S2.SS1.p1.12.m11.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S2.SS1.p1.12.m11.1a"><mi id="S2.SS1.p1.12.m11.1.1" xref="S2.SS1.p1.12.m11.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m11.1b"><ci id="S2.SS1.p1.12.m11.1.1.cmml" xref="S2.SS1.p1.12.m11.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m11.1c">\mathbf{I}</annotation></semantics></math> being the identity matrix, and <math id="S2.SS1.p1.13.m12.1" class="ltx_Math" alttext="T=1.0" display="inline"><semantics id="S2.SS1.p1.13.m12.1a"><mrow id="S2.SS1.p1.13.m12.1.1" xref="S2.SS1.p1.13.m12.1.1.cmml"><mi id="S2.SS1.p1.13.m12.1.1.2" xref="S2.SS1.p1.13.m12.1.1.2.cmml">T</mi><mo id="S2.SS1.p1.13.m12.1.1.1" xref="S2.SS1.p1.13.m12.1.1.1.cmml">=</mo><mn id="S2.SS1.p1.13.m12.1.1.3" xref="S2.SS1.p1.13.m12.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m12.1b"><apply id="S2.SS1.p1.13.m12.1.1.cmml" xref="S2.SS1.p1.13.m12.1.1"><eq id="S2.SS1.p1.13.m12.1.1.1.cmml" xref="S2.SS1.p1.13.m12.1.1.1"></eq><ci id="S2.SS1.p1.13.m12.1.1.2.cmml" xref="S2.SS1.p1.13.m12.1.1.2">𝑇</ci><cn type="float" id="S2.SS1.p1.13.m12.1.1.3.cmml" xref="S2.SS1.p1.13.m12.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m12.1c">T=1.0</annotation></semantics></math>. The text encoder <math id="S2.SS1.p1.14.m13.1" class="ltx_Math" alttext="{\mathbf{E}_{\theta}}" display="inline"><semantics id="S2.SS1.p1.14.m13.1a"><msub id="S2.SS1.p1.14.m13.1.1" xref="S2.SS1.p1.14.m13.1.1.cmml"><mi id="S2.SS1.p1.14.m13.1.1.2" xref="S2.SS1.p1.14.m13.1.1.2.cmml">𝐄</mi><mi id="S2.SS1.p1.14.m13.1.1.3" xref="S2.SS1.p1.14.m13.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.14.m13.1b"><apply id="S2.SS1.p1.14.m13.1.1.cmml" xref="S2.SS1.p1.14.m13.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.14.m13.1.1.1.cmml" xref="S2.SS1.p1.14.m13.1.1">subscript</csymbol><ci id="S2.SS1.p1.14.m13.1.1.2.cmml" xref="S2.SS1.p1.14.m13.1.1.2">𝐄</ci><ci id="S2.SS1.p1.14.m13.1.1.3.cmml" xref="S2.SS1.p1.14.m13.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.14.m13.1c">{\mathbf{E}_{\theta}}</annotation></semantics></math> has the same transformer architecture and training objective as Glow-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The sampling process from <math id="S2.SS1.p1.15.m14.1" class="ltx_Math" alttext="{\mathbf{X}}_{T}" display="inline"><semantics id="S2.SS1.p1.15.m14.1a"><msub id="S2.SS1.p1.15.m14.1.1" xref="S2.SS1.p1.15.m14.1.1.cmml"><mi id="S2.SS1.p1.15.m14.1.1.2" xref="S2.SS1.p1.15.m14.1.1.2.cmml">𝐗</mi><mi id="S2.SS1.p1.15.m14.1.1.3" xref="S2.SS1.p1.15.m14.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.15.m14.1b"><apply id="S2.SS1.p1.15.m14.1.1.cmml" xref="S2.SS1.p1.15.m14.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.15.m14.1.1.1.cmml" xref="S2.SS1.p1.15.m14.1.1">subscript</csymbol><ci id="S2.SS1.p1.15.m14.1.1.2.cmml" xref="S2.SS1.p1.15.m14.1.1.2">𝐗</ci><ci id="S2.SS1.p1.15.m14.1.1.3.cmml" xref="S2.SS1.p1.15.m14.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.15.m14.1c">{\mathbf{X}}_{T}</annotation></semantics></math> to <math id="S2.SS1.p1.16.m15.1" class="ltx_Math" alttext="{\mathbf{X}}_{0}" display="inline"><semantics id="S2.SS1.p1.16.m15.1a"><msub id="S2.SS1.p1.16.m15.1.1" xref="S2.SS1.p1.16.m15.1.1.cmml"><mi id="S2.SS1.p1.16.m15.1.1.2" xref="S2.SS1.p1.16.m15.1.1.2.cmml">𝐗</mi><mn id="S2.SS1.p1.16.m15.1.1.3" xref="S2.SS1.p1.16.m15.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.16.m15.1b"><apply id="S2.SS1.p1.16.m15.1.1.cmml" xref="S2.SS1.p1.16.m15.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.16.m15.1.1.1.cmml" xref="S2.SS1.p1.16.m15.1.1">subscript</csymbol><ci id="S2.SS1.p1.16.m15.1.1.2.cmml" xref="S2.SS1.p1.16.m15.1.1.2">𝐗</ci><cn type="integer" id="S2.SS1.p1.16.m15.1.1.3.cmml" xref="S2.SS1.p1.16.m15.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.16.m15.1c">{\mathbf{X}}_{0}</annotation></semantics></math> (cf. <a href="#S2.E4" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) is defined as the reverse of a forward diffusion process from <math id="S2.SS1.p1.17.m16.1" class="ltx_Math" alttext="{\mathbf{X}}_{0}" display="inline"><semantics id="S2.SS1.p1.17.m16.1a"><msub id="S2.SS1.p1.17.m16.1.1" xref="S2.SS1.p1.17.m16.1.1.cmml"><mi id="S2.SS1.p1.17.m16.1.1.2" xref="S2.SS1.p1.17.m16.1.1.2.cmml">𝐗</mi><mn id="S2.SS1.p1.17.m16.1.1.3" xref="S2.SS1.p1.17.m16.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.17.m16.1b"><apply id="S2.SS1.p1.17.m16.1.1.cmml" xref="S2.SS1.p1.17.m16.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.17.m16.1.1.1.cmml" xref="S2.SS1.p1.17.m16.1.1">subscript</csymbol><ci id="S2.SS1.p1.17.m16.1.1.2.cmml" xref="S2.SS1.p1.17.m16.1.1.2">𝐗</ci><cn type="integer" id="S2.SS1.p1.17.m16.1.1.3.cmml" xref="S2.SS1.p1.17.m16.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.17.m16.1c">{\mathbf{X}}_{0}</annotation></semantics></math> to <math id="S2.SS1.p1.18.m17.1" class="ltx_Math" alttext="{\mathbf{X}}_{T}" display="inline"><semantics id="S2.SS1.p1.18.m17.1a"><msub id="S2.SS1.p1.18.m17.1.1" xref="S2.SS1.p1.18.m17.1.1.cmml"><mi id="S2.SS1.p1.18.m17.1.1.2" xref="S2.SS1.p1.18.m17.1.1.2.cmml">𝐗</mi><mi id="S2.SS1.p1.18.m17.1.1.3" xref="S2.SS1.p1.18.m17.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.18.m17.1b"><apply id="S2.SS1.p1.18.m17.1.1.cmml" xref="S2.SS1.p1.18.m17.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.18.m17.1.1.1.cmml" xref="S2.SS1.p1.18.m17.1.1">subscript</csymbol><ci id="S2.SS1.p1.18.m17.1.1.2.cmml" xref="S2.SS1.p1.18.m17.1.1.2">𝐗</ci><ci id="S2.SS1.p1.18.m17.1.1.3.cmml" xref="S2.SS1.p1.18.m17.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.18.m17.1c">{\mathbf{X}}_{T}</annotation></semantics></math> with change in <math id="S2.SS1.p1.19.m18.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.19.m18.1a"><mi id="S2.SS1.p1.19.m18.1.1" xref="S2.SS1.p1.19.m18.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.19.m18.1b"><ci id="S2.SS1.p1.19.m18.1.1.cmml" xref="S2.SS1.p1.19.m18.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.19.m18.1c">{\mathbf{X}}</annotation></semantics></math> denoted as</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="{\mathrm{d}}{\mathbf{X}}=\frac{1}{2}({\mathbf{E}_{\theta}}({\mathbf{y}})-{\mathbf{X}})\beta(t){\mathrm{d}}t+\sqrt{\beta(t)}{\mathrm{d}}{\mathbf{W}}_{t}" display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><mi mathvariant="normal" id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.3.1" xref="S2.E2.m1.4.4.3.1.cmml">​</mo><mi id="S2.E2.m1.4.4.3.3" xref="S2.E2.m1.4.4.3.3.cmml">𝐗</mi></mrow><mo id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mfrac id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml"><mn id="S2.E2.m1.4.4.1.1.3.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml">1</mn><mn id="S2.E2.m1.4.4.1.1.3.3" xref="S2.E2.m1.4.4.1.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.2.cmml">𝐄</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.1.1.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.2.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.2.3.2.1" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">𝐲</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.2.3.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml">−</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.cmml">𝐗</mi></mrow><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2a" xref="S2.E2.m1.4.4.1.1.2.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.4" xref="S2.E2.m1.4.4.1.1.4.cmml">β</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2b" xref="S2.E2.m1.4.4.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.5.2" xref="S2.E2.m1.4.4.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.5.2.1" xref="S2.E2.m1.4.4.1.1.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">t</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.5.2.2" xref="S2.E2.m1.4.4.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2c" xref="S2.E2.m1.4.4.1.1.2.cmml">​</mo><mi mathvariant="normal" id="S2.E2.m1.4.4.1.1.6" xref="S2.E2.m1.4.4.1.1.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2d" xref="S2.E2.m1.4.4.1.1.2.cmml">​</mo><mi id="S2.E2.m1.4.4.1.1.7" xref="S2.E2.m1.4.4.1.1.7.cmml">t</mi></mrow><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.2.cmml">+</mo><mrow id="S2.E2.m1.4.4.1.3" xref="S2.E2.m1.4.4.1.3.cmml"><msqrt id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml">β</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.4.2" xref="S2.E2.m1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.4.2.1" xref="S2.E2.m1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E2.m1.1.1.1.4.2.2" xref="S2.E2.m1.1.1.1.cmml">)</mo></mrow></mrow></msqrt><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.3.1" xref="S2.E2.m1.4.4.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.E2.m1.4.4.1.3.2" xref="S2.E2.m1.4.4.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.3.1a" xref="S2.E2.m1.4.4.1.3.1.cmml">​</mo><msub id="S2.E2.m1.4.4.1.3.3" xref="S2.E2.m1.4.4.1.3.3.cmml"><mi id="S2.E2.m1.4.4.1.3.3.2" xref="S2.E2.m1.4.4.1.3.3.2.cmml">𝐖</mi><mi id="S2.E2.m1.4.4.1.3.3.3" xref="S2.E2.m1.4.4.1.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"></eq><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><times id="S2.E2.m1.4.4.3.1.cmml" xref="S2.E2.m1.4.4.3.1"></times><ci id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2">d</ci><ci id="S2.E2.m1.4.4.3.3.cmml" xref="S2.E2.m1.4.4.3.3">𝐗</ci></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><plus id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1.2"></plus><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1.1"><times id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"><divide id="S2.E2.m1.4.4.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3"></divide><cn type="integer" id="S2.E2.m1.4.4.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2">1</cn><cn type="integer" id="S2.E2.m1.4.4.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.3.3">2</cn></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"></minus><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2"><times id="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.1"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.2">𝐄</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.3">𝜃</ci></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐲</ci></apply><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3">𝐗</ci></apply><ci id="S2.E2.m1.4.4.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.4">𝛽</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑡</ci><ci id="S2.E2.m1.4.4.1.1.6.cmml" xref="S2.E2.m1.4.4.1.1.6">d</ci><ci id="S2.E2.m1.4.4.1.1.7.cmml" xref="S2.E2.m1.4.4.1.1.7">𝑡</ci></apply><apply id="S2.E2.m1.4.4.1.3.cmml" xref="S2.E2.m1.4.4.1.3"><times id="S2.E2.m1.4.4.1.3.1.cmml" xref="S2.E2.m1.4.4.1.3.1"></times><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><root id="S2.E2.m1.1.1a.cmml" xref="S2.E2.m1.1.1"></root><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3">𝛽</ci><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">𝑡</ci></apply></apply><ci id="S2.E2.m1.4.4.1.3.2.cmml" xref="S2.E2.m1.4.4.1.3.2">d</ci><apply id="S2.E2.m1.4.4.1.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.3.3.2.cmml" xref="S2.E2.m1.4.4.1.3.3.2">𝐖</ci><ci id="S2.E2.m1.4.4.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">{\mathrm{d}}{\mathbf{X}}=\frac{1}{2}({\mathbf{E}_{\theta}}({\mathbf{y}})-{\mathbf{X}})\beta(t){\mathrm{d}}t+\sqrt{\beta(t)}{\mathrm{d}}{\mathbf{W}}_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.20" class="ltx_p">where <math id="S2.SS1.p1.20.m1.1" class="ltx_Math" alttext="{\mathbf{W}}" display="inline"><semantics id="S2.SS1.p1.20.m1.1a"><mi id="S2.SS1.p1.20.m1.1.1" xref="S2.SS1.p1.20.m1.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.20.m1.1b"><ci id="S2.SS1.p1.20.m1.1.1.cmml" xref="S2.SS1.p1.20.m1.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.20.m1.1c">{\mathbf{W}}</annotation></semantics></math> is a random process. The forward process in (<a href="#S2.E2" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is a continuous mean-reverting variance-preserving  <span title="stochastic differential equation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">stochastic differential equation</span></span> (<abbr title="stochastic differential equation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SDE</span></abbr>). The linear noise schedule</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="\beta(t)=\beta_{0}+(\beta_{T}-\beta_{0})t" display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mrow id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml"><mi id="S2.E3.m1.2.2.3.2" xref="S2.E3.m1.2.2.3.2.cmml">β</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.3.1" xref="S2.E3.m1.2.2.3.1.cmml">​</mo><mrow id="S2.E3.m1.2.2.3.3.2" xref="S2.E3.m1.2.2.3.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.3.3.2.1" xref="S2.E3.m1.2.2.3.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E3.m1.2.2.3.3.2.2" xref="S2.E3.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml">=</mo><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.cmml"><msub id="S2.E3.m1.2.2.1.3" xref="S2.E3.m1.2.2.1.3.cmml"><mi id="S2.E3.m1.2.2.1.3.2" xref="S2.E3.m1.2.2.1.3.2.cmml">β</mi><mn id="S2.E3.m1.2.2.1.3.3" xref="S2.E3.m1.2.2.1.3.3.cmml">0</mn></msub><mo id="S2.E3.m1.2.2.1.2" xref="S2.E3.m1.2.2.1.2.cmml">+</mo><mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml"><mrow id="S2.E3.m1.2.2.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml"><msub id="S2.E3.m1.2.2.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.cmml">β</mi><mi id="S2.E3.m1.2.2.1.1.1.1.1.2.3" xref="S2.E3.m1.2.2.1.1.1.1.1.2.3.cmml">T</mi></msub><mo id="S2.E3.m1.2.2.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.cmml">−</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.1.1.3.2.cmml">β</mi><mn id="S2.E3.m1.2.2.1.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">​</mo><mi id="S2.E3.m1.2.2.1.1.3" xref="S2.E3.m1.2.2.1.1.3.cmml">t</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><eq id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"></eq><apply id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3"><times id="S2.E3.m1.2.2.3.1.cmml" xref="S2.E3.m1.2.2.3.1"></times><ci id="S2.E3.m1.2.2.3.2.cmml" xref="S2.E3.m1.2.2.3.2">𝛽</ci><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">𝑡</ci></apply><apply id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.2.2.1"><plus id="S2.E3.m1.2.2.1.2.cmml" xref="S2.E3.m1.2.2.1.2"></plus><apply id="S2.E3.m1.2.2.1.3.cmml" xref="S2.E3.m1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.3.1.cmml" xref="S2.E3.m1.2.2.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.3.2.cmml" xref="S2.E3.m1.2.2.1.3.2">𝛽</ci><cn type="integer" id="S2.E3.m1.2.2.1.3.3.cmml" xref="S2.E3.m1.2.2.1.3.3">0</cn></apply><apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1.1"><times id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.2"></times><apply id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1"><minus id="S2.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1"></minus><apply id="S2.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2">𝛽</ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S2.E3.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.2">𝛽</ci><cn type="integer" id="S2.E3.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3">0</cn></apply></apply><ci id="S2.E3.m1.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\beta(t)=\beta_{0}+(\beta_{T}-\beta_{0})t</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.36" class="ltx_p">starts at <math id="S2.SS1.p1.21.m1.1" class="ltx_Math" alttext="\beta_{0}" display="inline"><semantics id="S2.SS1.p1.21.m1.1a"><msub id="S2.SS1.p1.21.m1.1.1" xref="S2.SS1.p1.21.m1.1.1.cmml"><mi id="S2.SS1.p1.21.m1.1.1.2" xref="S2.SS1.p1.21.m1.1.1.2.cmml">β</mi><mn id="S2.SS1.p1.21.m1.1.1.3" xref="S2.SS1.p1.21.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.21.m1.1b"><apply id="S2.SS1.p1.21.m1.1.1.cmml" xref="S2.SS1.p1.21.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.21.m1.1.1.1.cmml" xref="S2.SS1.p1.21.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.21.m1.1.1.2.cmml" xref="S2.SS1.p1.21.m1.1.1.2">𝛽</ci><cn type="integer" id="S2.SS1.p1.21.m1.1.1.3.cmml" xref="S2.SS1.p1.21.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.21.m1.1c">\beta_{0}</annotation></semantics></math> and ends at <math id="S2.SS1.p1.22.m2.1" class="ltx_Math" alttext="\beta_{T}" display="inline"><semantics id="S2.SS1.p1.22.m2.1a"><msub id="S2.SS1.p1.22.m2.1.1" xref="S2.SS1.p1.22.m2.1.1.cmml"><mi id="S2.SS1.p1.22.m2.1.1.2" xref="S2.SS1.p1.22.m2.1.1.2.cmml">β</mi><mi id="S2.SS1.p1.22.m2.1.1.3" xref="S2.SS1.p1.22.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.22.m2.1b"><apply id="S2.SS1.p1.22.m2.1.1.cmml" xref="S2.SS1.p1.22.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.22.m2.1.1.1.cmml" xref="S2.SS1.p1.22.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.22.m2.1.1.2.cmml" xref="S2.SS1.p1.22.m2.1.1.2">𝛽</ci><ci id="S2.SS1.p1.22.m2.1.1.3.cmml" xref="S2.SS1.p1.22.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.22.m2.1c">\beta_{T}</annotation></semantics></math> to control the perturbation gradient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Intuitively, <math id="S2.SS1.p1.23.m3.1" class="ltx_Math" alttext="\beta(t)" display="inline"><semantics id="S2.SS1.p1.23.m3.1a"><mrow id="S2.SS1.p1.23.m3.1.2" xref="S2.SS1.p1.23.m3.1.2.cmml"><mi id="S2.SS1.p1.23.m3.1.2.2" xref="S2.SS1.p1.23.m3.1.2.2.cmml">β</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.23.m3.1.2.1" xref="S2.SS1.p1.23.m3.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.23.m3.1.2.3.2" xref="S2.SS1.p1.23.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.23.m3.1.2.3.2.1" xref="S2.SS1.p1.23.m3.1.2.cmml">(</mo><mi id="S2.SS1.p1.23.m3.1.1" xref="S2.SS1.p1.23.m3.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p1.23.m3.1.2.3.2.2" xref="S2.SS1.p1.23.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.23.m3.1b"><apply id="S2.SS1.p1.23.m3.1.2.cmml" xref="S2.SS1.p1.23.m3.1.2"><times id="S2.SS1.p1.23.m3.1.2.1.cmml" xref="S2.SS1.p1.23.m3.1.2.1"></times><ci id="S2.SS1.p1.23.m3.1.2.2.cmml" xref="S2.SS1.p1.23.m3.1.2.2">𝛽</ci><ci id="S2.SS1.p1.23.m3.1.1.cmml" xref="S2.SS1.p1.23.m3.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.23.m3.1c">\beta(t)</annotation></semantics></math> increases the amount of noise perturbation of <math id="S2.SS1.p1.24.m4.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.24.m4.1a"><mi id="S2.SS1.p1.24.m4.1.1" xref="S2.SS1.p1.24.m4.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.24.m4.1b"><ci id="S2.SS1.p1.24.m4.1.1.cmml" xref="S2.SS1.p1.24.m4.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.24.m4.1c">{\mathbf{X}}</annotation></semantics></math> as <math id="S2.SS1.p1.25.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p1.25.m5.1a"><mi id="S2.SS1.p1.25.m5.1.1" xref="S2.SS1.p1.25.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.25.m5.1b"><ci id="S2.SS1.p1.25.m5.1.1.cmml" xref="S2.SS1.p1.25.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.25.m5.1c">t</annotation></semantics></math> increases. The amount of noise added to <math id="S2.SS1.p1.26.m6.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.26.m6.1a"><mi id="S2.SS1.p1.26.m6.1.1" xref="S2.SS1.p1.26.m6.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.26.m6.1b"><ci id="S2.SS1.p1.26.m6.1.1.cmml" xref="S2.SS1.p1.26.m6.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.26.m6.1c">{\mathbf{X}}</annotation></semantics></math> at each time step <math id="S2.SS1.p1.27.m7.1" class="ltx_Math" alttext="{\mathrm{d}}t" display="inline"><semantics id="S2.SS1.p1.27.m7.1a"><mrow id="S2.SS1.p1.27.m7.1.1" xref="S2.SS1.p1.27.m7.1.1.cmml"><mi mathvariant="normal" id="S2.SS1.p1.27.m7.1.1.2" xref="S2.SS1.p1.27.m7.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.27.m7.1.1.1" xref="S2.SS1.p1.27.m7.1.1.1.cmml">​</mo><mi id="S2.SS1.p1.27.m7.1.1.3" xref="S2.SS1.p1.27.m7.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.27.m7.1b"><apply id="S2.SS1.p1.27.m7.1.1.cmml" xref="S2.SS1.p1.27.m7.1.1"><times id="S2.SS1.p1.27.m7.1.1.1.cmml" xref="S2.SS1.p1.27.m7.1.1.1"></times><ci id="S2.SS1.p1.27.m7.1.1.2.cmml" xref="S2.SS1.p1.27.m7.1.1.2">d</ci><ci id="S2.SS1.p1.27.m7.1.1.3.cmml" xref="S2.SS1.p1.27.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.27.m7.1c">{\mathrm{d}}t</annotation></semantics></math> increases proportionally with <math id="S2.SS1.p1.28.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p1.28.m8.1a"><mi id="S2.SS1.p1.28.m8.1.1" xref="S2.SS1.p1.28.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.28.m8.1b"><ci id="S2.SS1.p1.28.m8.1.1.cmml" xref="S2.SS1.p1.28.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.28.m8.1c">t</annotation></semantics></math>. The encoder term <math id="S2.SS1.p1.29.m9.1" class="ltx_Math" alttext="{\mathbf{E}_{\theta}}({\mathbf{y}})" display="inline"><semantics id="S2.SS1.p1.29.m9.1a"><mrow id="S2.SS1.p1.29.m9.1.2" xref="S2.SS1.p1.29.m9.1.2.cmml"><msub id="S2.SS1.p1.29.m9.1.2.2" xref="S2.SS1.p1.29.m9.1.2.2.cmml"><mi id="S2.SS1.p1.29.m9.1.2.2.2" xref="S2.SS1.p1.29.m9.1.2.2.2.cmml">𝐄</mi><mi id="S2.SS1.p1.29.m9.1.2.2.3" xref="S2.SS1.p1.29.m9.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.29.m9.1.2.1" xref="S2.SS1.p1.29.m9.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.29.m9.1.2.3.2" xref="S2.SS1.p1.29.m9.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.29.m9.1.2.3.2.1" xref="S2.SS1.p1.29.m9.1.2.cmml">(</mo><mi id="S2.SS1.p1.29.m9.1.1" xref="S2.SS1.p1.29.m9.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.SS1.p1.29.m9.1.2.3.2.2" xref="S2.SS1.p1.29.m9.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.29.m9.1b"><apply id="S2.SS1.p1.29.m9.1.2.cmml" xref="S2.SS1.p1.29.m9.1.2"><times id="S2.SS1.p1.29.m9.1.2.1.cmml" xref="S2.SS1.p1.29.m9.1.2.1"></times><apply id="S2.SS1.p1.29.m9.1.2.2.cmml" xref="S2.SS1.p1.29.m9.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.29.m9.1.2.2.1.cmml" xref="S2.SS1.p1.29.m9.1.2.2">subscript</csymbol><ci id="S2.SS1.p1.29.m9.1.2.2.2.cmml" xref="S2.SS1.p1.29.m9.1.2.2.2">𝐄</ci><ci id="S2.SS1.p1.29.m9.1.2.2.3.cmml" xref="S2.SS1.p1.29.m9.1.2.2.3">𝜃</ci></apply><ci id="S2.SS1.p1.29.m9.1.1.cmml" xref="S2.SS1.p1.29.m9.1.1">𝐲</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.29.m9.1c">{\mathbf{E}_{\theta}}({\mathbf{y}})</annotation></semantics></math> ensures that the distribution of <math id="S2.SS1.p1.30.m10.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.30.m10.1a"><mi id="S2.SS1.p1.30.m10.1.1" xref="S2.SS1.p1.30.m10.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.30.m10.1b"><ci id="S2.SS1.p1.30.m10.1.1.cmml" xref="S2.SS1.p1.30.m10.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.30.m10.1c">{\mathbf{X}}</annotation></semantics></math> is centered on the text-encoder prediction for any <math id="S2.SS1.p1.31.m11.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p1.31.m11.1a"><mi id="S2.SS1.p1.31.m11.1.1" xref="S2.SS1.p1.31.m11.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.31.m11.1b"><ci id="S2.SS1.p1.31.m11.1.1.cmml" xref="S2.SS1.p1.31.m11.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.31.m11.1c">t</annotation></semantics></math>. Default values for Grad-TTS are <math id="S2.SS1.p1.32.m12.1" class="ltx_Math" alttext="\beta_{0}=0.05" display="inline"><semantics id="S2.SS1.p1.32.m12.1a"><mrow id="S2.SS1.p1.32.m12.1.1" xref="S2.SS1.p1.32.m12.1.1.cmml"><msub id="S2.SS1.p1.32.m12.1.1.2" xref="S2.SS1.p1.32.m12.1.1.2.cmml"><mi id="S2.SS1.p1.32.m12.1.1.2.2" xref="S2.SS1.p1.32.m12.1.1.2.2.cmml">β</mi><mn id="S2.SS1.p1.32.m12.1.1.2.3" xref="S2.SS1.p1.32.m12.1.1.2.3.cmml">0</mn></msub><mo id="S2.SS1.p1.32.m12.1.1.1" xref="S2.SS1.p1.32.m12.1.1.1.cmml">=</mo><mn id="S2.SS1.p1.32.m12.1.1.3" xref="S2.SS1.p1.32.m12.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.32.m12.1b"><apply id="S2.SS1.p1.32.m12.1.1.cmml" xref="S2.SS1.p1.32.m12.1.1"><eq id="S2.SS1.p1.32.m12.1.1.1.cmml" xref="S2.SS1.p1.32.m12.1.1.1"></eq><apply id="S2.SS1.p1.32.m12.1.1.2.cmml" xref="S2.SS1.p1.32.m12.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.32.m12.1.1.2.1.cmml" xref="S2.SS1.p1.32.m12.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.32.m12.1.1.2.2.cmml" xref="S2.SS1.p1.32.m12.1.1.2.2">𝛽</ci><cn type="integer" id="S2.SS1.p1.32.m12.1.1.2.3.cmml" xref="S2.SS1.p1.32.m12.1.1.2.3">0</cn></apply><cn type="float" id="S2.SS1.p1.32.m12.1.1.3.cmml" xref="S2.SS1.p1.32.m12.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.32.m12.1c">\beta_{0}=0.05</annotation></semantics></math> and <math id="S2.SS1.p1.33.m13.1" class="ltx_Math" alttext="\beta_{T}=20" display="inline"><semantics id="S2.SS1.p1.33.m13.1a"><mrow id="S2.SS1.p1.33.m13.1.1" xref="S2.SS1.p1.33.m13.1.1.cmml"><msub id="S2.SS1.p1.33.m13.1.1.2" xref="S2.SS1.p1.33.m13.1.1.2.cmml"><mi id="S2.SS1.p1.33.m13.1.1.2.2" xref="S2.SS1.p1.33.m13.1.1.2.2.cmml">β</mi><mi id="S2.SS1.p1.33.m13.1.1.2.3" xref="S2.SS1.p1.33.m13.1.1.2.3.cmml">T</mi></msub><mo id="S2.SS1.p1.33.m13.1.1.1" xref="S2.SS1.p1.33.m13.1.1.1.cmml">=</mo><mn id="S2.SS1.p1.33.m13.1.1.3" xref="S2.SS1.p1.33.m13.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.33.m13.1b"><apply id="S2.SS1.p1.33.m13.1.1.cmml" xref="S2.SS1.p1.33.m13.1.1"><eq id="S2.SS1.p1.33.m13.1.1.1.cmml" xref="S2.SS1.p1.33.m13.1.1.1"></eq><apply id="S2.SS1.p1.33.m13.1.1.2.cmml" xref="S2.SS1.p1.33.m13.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.33.m13.1.1.2.1.cmml" xref="S2.SS1.p1.33.m13.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.33.m13.1.1.2.2.cmml" xref="S2.SS1.p1.33.m13.1.1.2.2">𝛽</ci><ci id="S2.SS1.p1.33.m13.1.1.2.3.cmml" xref="S2.SS1.p1.33.m13.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S2.SS1.p1.33.m13.1.1.3.cmml" xref="S2.SS1.p1.33.m13.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.33.m13.1c">\beta_{T}=20</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, the choice is inspired by diffusion image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> where <math id="S2.SS1.p1.34.m14.2" class="ltx_Math" alttext="{\mathbf{X}}_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})" display="inline"><semantics id="S2.SS1.p1.34.m14.2a"><mrow id="S2.SS1.p1.34.m14.2.3" xref="S2.SS1.p1.34.m14.2.3.cmml"><msub id="S2.SS1.p1.34.m14.2.3.2" xref="S2.SS1.p1.34.m14.2.3.2.cmml"><mi id="S2.SS1.p1.34.m14.2.3.2.2" xref="S2.SS1.p1.34.m14.2.3.2.2.cmml">𝐗</mi><mi id="S2.SS1.p1.34.m14.2.3.2.3" xref="S2.SS1.p1.34.m14.2.3.2.3.cmml">T</mi></msub><mo id="S2.SS1.p1.34.m14.2.3.1" xref="S2.SS1.p1.34.m14.2.3.1.cmml">∼</mo><mrow id="S2.SS1.p1.34.m14.2.3.3" xref="S2.SS1.p1.34.m14.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.34.m14.2.3.3.2" xref="S2.SS1.p1.34.m14.2.3.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.34.m14.2.3.3.1" xref="S2.SS1.p1.34.m14.2.3.3.1.cmml">​</mo><mrow id="S2.SS1.p1.34.m14.2.3.3.3.2" xref="S2.SS1.p1.34.m14.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.34.m14.2.3.3.3.2.1" xref="S2.SS1.p1.34.m14.2.3.3.3.1.cmml">(</mo><mn id="S2.SS1.p1.34.m14.1.1" xref="S2.SS1.p1.34.m14.1.1.cmml">𝟎</mn><mo id="S2.SS1.p1.34.m14.2.3.3.3.2.2" xref="S2.SS1.p1.34.m14.2.3.3.3.1.cmml">,</mo><mi id="S2.SS1.p1.34.m14.2.2" xref="S2.SS1.p1.34.m14.2.2.cmml">𝐈</mi><mo stretchy="false" id="S2.SS1.p1.34.m14.2.3.3.3.2.3" xref="S2.SS1.p1.34.m14.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.34.m14.2b"><apply id="S2.SS1.p1.34.m14.2.3.cmml" xref="S2.SS1.p1.34.m14.2.3"><csymbol cd="latexml" id="S2.SS1.p1.34.m14.2.3.1.cmml" xref="S2.SS1.p1.34.m14.2.3.1">similar-to</csymbol><apply id="S2.SS1.p1.34.m14.2.3.2.cmml" xref="S2.SS1.p1.34.m14.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.34.m14.2.3.2.1.cmml" xref="S2.SS1.p1.34.m14.2.3.2">subscript</csymbol><ci id="S2.SS1.p1.34.m14.2.3.2.2.cmml" xref="S2.SS1.p1.34.m14.2.3.2.2">𝐗</ci><ci id="S2.SS1.p1.34.m14.2.3.2.3.cmml" xref="S2.SS1.p1.34.m14.2.3.2.3">𝑇</ci></apply><apply id="S2.SS1.p1.34.m14.2.3.3.cmml" xref="S2.SS1.p1.34.m14.2.3.3"><times id="S2.SS1.p1.34.m14.2.3.3.1.cmml" xref="S2.SS1.p1.34.m14.2.3.3.1"></times><ci id="S2.SS1.p1.34.m14.2.3.3.2.cmml" xref="S2.SS1.p1.34.m14.2.3.3.2">𝒩</ci><interval closure="open" id="S2.SS1.p1.34.m14.2.3.3.3.1.cmml" xref="S2.SS1.p1.34.m14.2.3.3.3.2"><cn type="integer" id="S2.SS1.p1.34.m14.1.1.cmml" xref="S2.SS1.p1.34.m14.1.1">0</cn><ci id="S2.SS1.p1.34.m14.2.2.cmml" xref="S2.SS1.p1.34.m14.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.34.m14.2c">{\mathbf{X}}_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})</annotation></semantics></math>. This work explores reduced <math id="S2.SS1.p1.35.m15.1" class="ltx_Math" alttext="\beta_{T}" display="inline"><semantics id="S2.SS1.p1.35.m15.1a"><msub id="S2.SS1.p1.35.m15.1.1" xref="S2.SS1.p1.35.m15.1.1.cmml"><mi id="S2.SS1.p1.35.m15.1.1.2" xref="S2.SS1.p1.35.m15.1.1.2.cmml">β</mi><mi id="S2.SS1.p1.35.m15.1.1.3" xref="S2.SS1.p1.35.m15.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.35.m15.1b"><apply id="S2.SS1.p1.35.m15.1.1.cmml" xref="S2.SS1.p1.35.m15.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.35.m15.1.1.1.cmml" xref="S2.SS1.p1.35.m15.1.1">subscript</csymbol><ci id="S2.SS1.p1.35.m15.1.1.2.cmml" xref="S2.SS1.p1.35.m15.1.1.2">𝛽</ci><ci id="S2.SS1.p1.35.m15.1.1.3.cmml" xref="S2.SS1.p1.35.m15.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.35.m15.1c">\beta_{T}</annotation></semantics></math> to reflect that the <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> diffusion process starts from structured data <math id="S2.SS1.p1.36.m16.3" class="ltx_Math" alttext="{\mathbf{X}}_{T}\sim\mathcal{N}({\mathbf{E}_{\theta}}({\mathbf{y}}),\mathbf{I})" display="inline"><semantics id="S2.SS1.p1.36.m16.3a"><mrow id="S2.SS1.p1.36.m16.3.3" xref="S2.SS1.p1.36.m16.3.3.cmml"><msub id="S2.SS1.p1.36.m16.3.3.3" xref="S2.SS1.p1.36.m16.3.3.3.cmml"><mi id="S2.SS1.p1.36.m16.3.3.3.2" xref="S2.SS1.p1.36.m16.3.3.3.2.cmml">𝐗</mi><mi id="S2.SS1.p1.36.m16.3.3.3.3" xref="S2.SS1.p1.36.m16.3.3.3.3.cmml">T</mi></msub><mo id="S2.SS1.p1.36.m16.3.3.2" xref="S2.SS1.p1.36.m16.3.3.2.cmml">∼</mo><mrow id="S2.SS1.p1.36.m16.3.3.1" xref="S2.SS1.p1.36.m16.3.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.36.m16.3.3.1.3" xref="S2.SS1.p1.36.m16.3.3.1.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.36.m16.3.3.1.2" xref="S2.SS1.p1.36.m16.3.3.1.2.cmml">​</mo><mrow id="S2.SS1.p1.36.m16.3.3.1.1.1" xref="S2.SS1.p1.36.m16.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.36.m16.3.3.1.1.1.2" xref="S2.SS1.p1.36.m16.3.3.1.1.2.cmml">(</mo><mrow id="S2.SS1.p1.36.m16.3.3.1.1.1.1" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.cmml"><msub id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.cmml"><mi id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.2" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.2.cmml">𝐄</mi><mi id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.3" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.36.m16.3.3.1.1.1.1.1" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.1.cmml">​</mo><mrow id="S2.SS1.p1.36.m16.3.3.1.1.1.1.3.2" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.36.m16.3.3.1.1.1.1.3.2.1" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.cmml">(</mo><mi id="S2.SS1.p1.36.m16.1.1" xref="S2.SS1.p1.36.m16.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.SS1.p1.36.m16.3.3.1.1.1.1.3.2.2" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p1.36.m16.3.3.1.1.1.3" xref="S2.SS1.p1.36.m16.3.3.1.1.2.cmml">,</mo><mi id="S2.SS1.p1.36.m16.2.2" xref="S2.SS1.p1.36.m16.2.2.cmml">𝐈</mi><mo stretchy="false" id="S2.SS1.p1.36.m16.3.3.1.1.1.4" xref="S2.SS1.p1.36.m16.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.36.m16.3b"><apply id="S2.SS1.p1.36.m16.3.3.cmml" xref="S2.SS1.p1.36.m16.3.3"><csymbol cd="latexml" id="S2.SS1.p1.36.m16.3.3.2.cmml" xref="S2.SS1.p1.36.m16.3.3.2">similar-to</csymbol><apply id="S2.SS1.p1.36.m16.3.3.3.cmml" xref="S2.SS1.p1.36.m16.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.36.m16.3.3.3.1.cmml" xref="S2.SS1.p1.36.m16.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.36.m16.3.3.3.2.cmml" xref="S2.SS1.p1.36.m16.3.3.3.2">𝐗</ci><ci id="S2.SS1.p1.36.m16.3.3.3.3.cmml" xref="S2.SS1.p1.36.m16.3.3.3.3">𝑇</ci></apply><apply id="S2.SS1.p1.36.m16.3.3.1.cmml" xref="S2.SS1.p1.36.m16.3.3.1"><times id="S2.SS1.p1.36.m16.3.3.1.2.cmml" xref="S2.SS1.p1.36.m16.3.3.1.2"></times><ci id="S2.SS1.p1.36.m16.3.3.1.3.cmml" xref="S2.SS1.p1.36.m16.3.3.1.3">𝒩</ci><interval closure="open" id="S2.SS1.p1.36.m16.3.3.1.1.2.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1"><apply id="S2.SS1.p1.36.m16.3.3.1.1.1.1.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1"><times id="S2.SS1.p1.36.m16.3.3.1.1.1.1.1.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.1"></times><apply id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.2.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.2">𝐄</ci><ci id="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.3.cmml" xref="S2.SS1.p1.36.m16.3.3.1.1.1.1.2.3">𝜃</ci></apply><ci id="S2.SS1.p1.36.m16.1.1.cmml" xref="S2.SS1.p1.36.m16.1.1">𝐲</ci></apply><ci id="S2.SS1.p1.36.m16.2.2.cmml" xref="S2.SS1.p1.36.m16.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.36.m16.3c">{\mathbf{X}}_{T}\sim\mathcal{N}({\mathbf{E}_{\theta}}({\mathbf{y}}),\mathbf{I})</annotation></semantics></math> rather than an unassuming Gaussian; the effect of these hyperparameters has not yet been explored for dysarthric <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> to the authors' knowledge. The reverse diffusion process</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.4" class="ltx_Math" alttext="{\mathrm{d}}{\mathbf{X}}=\frac{1}{2}(({\mathbf{E}_{\theta}}({\mathbf{y}})-{\mathbf{X}})-\nabla\log p_{t}({\mathbf{X}}))\beta(t){\mathrm{d}}t" display="block"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.3" xref="S2.E4.m1.4.4.3.cmml"><mi mathvariant="normal" id="S2.E4.m1.4.4.3.2" xref="S2.E4.m1.4.4.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.3.1" xref="S2.E4.m1.4.4.3.1.cmml">​</mo><mi id="S2.E4.m1.4.4.3.3" xref="S2.E4.m1.4.4.3.3.cmml">𝐗</mi></mrow><mo id="S2.E4.m1.4.4.2" xref="S2.E4.m1.4.4.2.cmml">=</mo><mrow id="S2.E4.m1.4.4.1" xref="S2.E4.m1.4.4.1.cmml"><mfrac id="S2.E4.m1.4.4.1.3" xref="S2.E4.m1.4.4.1.3.cmml"><mn id="S2.E4.m1.4.4.1.3.2" xref="S2.E4.m1.4.4.1.3.2.cmml">1</mn><mn id="S2.E4.m1.4.4.1.3.3" xref="S2.E4.m1.4.4.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.2" xref="S2.E4.m1.4.4.1.2.cmml">​</mo><mrow id="S2.E4.m1.4.4.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.cmml"><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml">𝐄</mi><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.3.2.2" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S2.E4.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml">𝐗</mi></mrow><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E4.m1.4.4.1.1.1.1.2" xref="S2.E4.m1.4.4.1.1.1.1.2.cmml">−</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.3.cmml"><mrow id="S2.E4.m1.4.4.1.1.1.1.3.2" xref="S2.E4.m1.4.4.1.1.1.1.3.2.cmml"><mrow id="S2.E4.m1.4.4.1.1.1.1.3.2.1" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1.cmml"><mo rspace="0.167em" id="S2.E4.m1.4.4.1.1.1.1.3.2.1.1" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1.1.cmml">∇</mo><mi id="S2.E4.m1.4.4.1.1.1.1.3.2.1.2" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1.2.cmml">log</mi></mrow><mo lspace="0.167em" id="S2.E4.m1.4.4.1.1.1.1.3.2a" xref="S2.E4.m1.4.4.1.1.1.1.3.2.cmml">⁡</mo><msub id="S2.E4.m1.4.4.1.1.1.1.3.2.2" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2.cmml"><mi id="S2.E4.m1.4.4.1.1.1.1.3.2.2.2" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2.2.cmml">p</mi><mi id="S2.E4.m1.4.4.1.1.1.1.3.2.2.3" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2.3.cmml">t</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.1.1.1.3.1" xref="S2.E4.m1.4.4.1.1.1.1.3.1.cmml">​</mo><mrow id="S2.E4.m1.4.4.1.1.1.1.3.3.2" xref="S2.E4.m1.4.4.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.3.3.2.1" xref="S2.E4.m1.4.4.1.1.1.1.3.cmml">(</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">𝐗</mi><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.1.3.3.2.2" xref="S2.E4.m1.4.4.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E4.m1.4.4.1.1.1.3" xref="S2.E4.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.2a" xref="S2.E4.m1.4.4.1.2.cmml">​</mo><mi id="S2.E4.m1.4.4.1.4" xref="S2.E4.m1.4.4.1.4.cmml">β</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.2b" xref="S2.E4.m1.4.4.1.2.cmml">​</mo><mrow id="S2.E4.m1.4.4.1.5.2" xref="S2.E4.m1.4.4.1.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.1.5.2.1" xref="S2.E4.m1.4.4.1.cmml">(</mo><mi id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml">t</mi><mo stretchy="false" id="S2.E4.m1.4.4.1.5.2.2" xref="S2.E4.m1.4.4.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.2c" xref="S2.E4.m1.4.4.1.2.cmml">​</mo><mi mathvariant="normal" id="S2.E4.m1.4.4.1.6" xref="S2.E4.m1.4.4.1.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.1.2d" xref="S2.E4.m1.4.4.1.2.cmml">​</mo><mi id="S2.E4.m1.4.4.1.7" xref="S2.E4.m1.4.4.1.7.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.2.cmml" xref="S2.E4.m1.4.4.2"></eq><apply id="S2.E4.m1.4.4.3.cmml" xref="S2.E4.m1.4.4.3"><times id="S2.E4.m1.4.4.3.1.cmml" xref="S2.E4.m1.4.4.3.1"></times><ci id="S2.E4.m1.4.4.3.2.cmml" xref="S2.E4.m1.4.4.3.2">d</ci><ci id="S2.E4.m1.4.4.3.3.cmml" xref="S2.E4.m1.4.4.3.3">𝐗</ci></apply><apply id="S2.E4.m1.4.4.1.cmml" xref="S2.E4.m1.4.4.1"><times id="S2.E4.m1.4.4.1.2.cmml" xref="S2.E4.m1.4.4.1.2"></times><apply id="S2.E4.m1.4.4.1.3.cmml" xref="S2.E4.m1.4.4.1.3"><divide id="S2.E4.m1.4.4.1.3.1.cmml" xref="S2.E4.m1.4.4.1.3"></divide><cn type="integer" id="S2.E4.m1.4.4.1.3.2.cmml" xref="S2.E4.m1.4.4.1.3.2">1</cn><cn type="integer" id="S2.E4.m1.4.4.1.3.3.cmml" xref="S2.E4.m1.4.4.1.3.3">2</cn></apply><apply id="S2.E4.m1.4.4.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1"><minus id="S2.E4.m1.4.4.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.2"></minus><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1"><minus id="S2.E4.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.1"></minus><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2"><times id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.2">𝐄</ci><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.2.2.3">𝜃</ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝐲</ci></apply><ci id="S2.E4.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.1.1.1.3">𝐗</ci></apply><apply id="S2.E4.m1.4.4.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3"><times id="S2.E4.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.1"></times><apply id="S2.E4.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2"><apply id="S2.E4.m1.4.4.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1"><ci id="S2.E4.m1.4.4.1.1.1.1.3.2.1.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1.1">∇</ci><log id="S2.E4.m1.4.4.1.1.1.1.3.2.1.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.1.2"></log></apply><apply id="S2.E4.m1.4.4.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.1.1.1.1.3.2.2.1.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2">subscript</csymbol><ci id="S2.E4.m1.4.4.1.1.1.1.3.2.2.2.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2.2">𝑝</ci><ci id="S2.E4.m1.4.4.1.1.1.1.3.2.2.3.cmml" xref="S2.E4.m1.4.4.1.1.1.1.3.2.2.3">𝑡</ci></apply></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝐗</ci></apply></apply><ci id="S2.E4.m1.4.4.1.4.cmml" xref="S2.E4.m1.4.4.1.4">𝛽</ci><ci id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3">𝑡</ci><ci id="S2.E4.m1.4.4.1.6.cmml" xref="S2.E4.m1.4.4.1.6">d</ci><ci id="S2.E4.m1.4.4.1.7.cmml" xref="S2.E4.m1.4.4.1.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">{\mathrm{d}}{\mathbf{X}}=\frac{1}{2}(({\mathbf{E}_{\theta}}({\mathbf{y}})-{\mathbf{X}})-\nabla\log p_{t}({\mathbf{X}}))\beta(t){\mathrm{d}}t</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.43" class="ltx_p">is defined by a probability-flow  <span title="ordinary differential equation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">ordinary differential equation</span></span> (<abbr title="ordinary differential equation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ODE</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Computing the log-density gradient <math id="S2.SS1.p1.37.m1.1" class="ltx_Math" alttext="\nabla\log p_{t}({\mathbf{X}})" display="inline"><semantics id="S2.SS1.p1.37.m1.1a"><mrow id="S2.SS1.p1.37.m1.1.2" xref="S2.SS1.p1.37.m1.1.2.cmml"><mrow id="S2.SS1.p1.37.m1.1.2.2" xref="S2.SS1.p1.37.m1.1.2.2.cmml"><mrow id="S2.SS1.p1.37.m1.1.2.2.1" xref="S2.SS1.p1.37.m1.1.2.2.1.cmml"><mo rspace="0.167em" id="S2.SS1.p1.37.m1.1.2.2.1.1" xref="S2.SS1.p1.37.m1.1.2.2.1.1.cmml">∇</mo><mi id="S2.SS1.p1.37.m1.1.2.2.1.2" xref="S2.SS1.p1.37.m1.1.2.2.1.2.cmml">log</mi></mrow><mo lspace="0.167em" id="S2.SS1.p1.37.m1.1.2.2a" xref="S2.SS1.p1.37.m1.1.2.2.cmml">⁡</mo><msub id="S2.SS1.p1.37.m1.1.2.2.2" xref="S2.SS1.p1.37.m1.1.2.2.2.cmml"><mi id="S2.SS1.p1.37.m1.1.2.2.2.2" xref="S2.SS1.p1.37.m1.1.2.2.2.2.cmml">p</mi><mi id="S2.SS1.p1.37.m1.1.2.2.2.3" xref="S2.SS1.p1.37.m1.1.2.2.2.3.cmml">t</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p1.37.m1.1.2.1" xref="S2.SS1.p1.37.m1.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.37.m1.1.2.3.2" xref="S2.SS1.p1.37.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.37.m1.1.2.3.2.1" xref="S2.SS1.p1.37.m1.1.2.cmml">(</mo><mi id="S2.SS1.p1.37.m1.1.1" xref="S2.SS1.p1.37.m1.1.1.cmml">𝐗</mi><mo stretchy="false" id="S2.SS1.p1.37.m1.1.2.3.2.2" xref="S2.SS1.p1.37.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.37.m1.1b"><apply id="S2.SS1.p1.37.m1.1.2.cmml" xref="S2.SS1.p1.37.m1.1.2"><times id="S2.SS1.p1.37.m1.1.2.1.cmml" xref="S2.SS1.p1.37.m1.1.2.1"></times><apply id="S2.SS1.p1.37.m1.1.2.2.cmml" xref="S2.SS1.p1.37.m1.1.2.2"><apply id="S2.SS1.p1.37.m1.1.2.2.1.cmml" xref="S2.SS1.p1.37.m1.1.2.2.1"><ci id="S2.SS1.p1.37.m1.1.2.2.1.1.cmml" xref="S2.SS1.p1.37.m1.1.2.2.1.1">∇</ci><log id="S2.SS1.p1.37.m1.1.2.2.1.2.cmml" xref="S2.SS1.p1.37.m1.1.2.2.1.2"></log></apply><apply id="S2.SS1.p1.37.m1.1.2.2.2.cmml" xref="S2.SS1.p1.37.m1.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.37.m1.1.2.2.2.1.cmml" xref="S2.SS1.p1.37.m1.1.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.37.m1.1.2.2.2.2.cmml" xref="S2.SS1.p1.37.m1.1.2.2.2.2">𝑝</ci><ci id="S2.SS1.p1.37.m1.1.2.2.2.3.cmml" xref="S2.SS1.p1.37.m1.1.2.2.2.3">𝑡</ci></apply></apply><ci id="S2.SS1.p1.37.m1.1.1.cmml" xref="S2.SS1.p1.37.m1.1.1">𝐗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.37.m1.1c">\nabla\log p_{t}({\mathbf{X}})</annotation></semantics></math> in (<a href="#S2.E4" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), a.k.a. the <em id="S2.SS1.p1.43.1" class="ltx_emph ltx_font_italic">score</em>, is intractable, hence a trainable estimator <math id="S2.SS1.p1.38.m2.5" class="ltx_Math" alttext="{\mathbf{S}_{\theta}}({\mathbf{X}},t,{\mathbf{E}_{\theta}}({\mathbf{y}}),s_{\mathrm{E}})" display="inline"><semantics id="S2.SS1.p1.38.m2.5a"><mrow id="S2.SS1.p1.38.m2.5.5" xref="S2.SS1.p1.38.m2.5.5.cmml"><msub id="S2.SS1.p1.38.m2.5.5.4" xref="S2.SS1.p1.38.m2.5.5.4.cmml"><mi id="S2.SS1.p1.38.m2.5.5.4.2" xref="S2.SS1.p1.38.m2.5.5.4.2.cmml">𝐒</mi><mi id="S2.SS1.p1.38.m2.5.5.4.3" xref="S2.SS1.p1.38.m2.5.5.4.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.38.m2.5.5.3" xref="S2.SS1.p1.38.m2.5.5.3.cmml">​</mo><mrow id="S2.SS1.p1.38.m2.5.5.2.2" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml"><mo stretchy="false" id="S2.SS1.p1.38.m2.5.5.2.2.3" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml">(</mo><mi id="S2.SS1.p1.38.m2.2.2" xref="S2.SS1.p1.38.m2.2.2.cmml">𝐗</mi><mo id="S2.SS1.p1.38.m2.5.5.2.2.4" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml">,</mo><mi id="S2.SS1.p1.38.m2.3.3" xref="S2.SS1.p1.38.m2.3.3.cmml">t</mi><mo id="S2.SS1.p1.38.m2.5.5.2.2.5" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml">,</mo><mrow id="S2.SS1.p1.38.m2.4.4.1.1.1" xref="S2.SS1.p1.38.m2.4.4.1.1.1.cmml"><msub id="S2.SS1.p1.38.m2.4.4.1.1.1.2" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2.cmml"><mi id="S2.SS1.p1.38.m2.4.4.1.1.1.2.2" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2.2.cmml">𝐄</mi><mi id="S2.SS1.p1.38.m2.4.4.1.1.1.2.3" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.38.m2.4.4.1.1.1.1" xref="S2.SS1.p1.38.m2.4.4.1.1.1.1.cmml">​</mo><mrow id="S2.SS1.p1.38.m2.4.4.1.1.1.3.2" xref="S2.SS1.p1.38.m2.4.4.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.38.m2.4.4.1.1.1.3.2.1" xref="S2.SS1.p1.38.m2.4.4.1.1.1.cmml">(</mo><mi id="S2.SS1.p1.38.m2.1.1" xref="S2.SS1.p1.38.m2.1.1.cmml">𝐲</mi><mo stretchy="false" id="S2.SS1.p1.38.m2.4.4.1.1.1.3.2.2" xref="S2.SS1.p1.38.m2.4.4.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p1.38.m2.5.5.2.2.6" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml">,</mo><msub id="S2.SS1.p1.38.m2.5.5.2.2.2" xref="S2.SS1.p1.38.m2.5.5.2.2.2.cmml"><mi id="S2.SS1.p1.38.m2.5.5.2.2.2.2" xref="S2.SS1.p1.38.m2.5.5.2.2.2.2.cmml">s</mi><mi mathvariant="normal" id="S2.SS1.p1.38.m2.5.5.2.2.2.3" xref="S2.SS1.p1.38.m2.5.5.2.2.2.3.cmml">E</mi></msub><mo stretchy="false" id="S2.SS1.p1.38.m2.5.5.2.2.7" xref="S2.SS1.p1.38.m2.5.5.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.38.m2.5b"><apply id="S2.SS1.p1.38.m2.5.5.cmml" xref="S2.SS1.p1.38.m2.5.5"><times id="S2.SS1.p1.38.m2.5.5.3.cmml" xref="S2.SS1.p1.38.m2.5.5.3"></times><apply id="S2.SS1.p1.38.m2.5.5.4.cmml" xref="S2.SS1.p1.38.m2.5.5.4"><csymbol cd="ambiguous" id="S2.SS1.p1.38.m2.5.5.4.1.cmml" xref="S2.SS1.p1.38.m2.5.5.4">subscript</csymbol><ci id="S2.SS1.p1.38.m2.5.5.4.2.cmml" xref="S2.SS1.p1.38.m2.5.5.4.2">𝐒</ci><ci id="S2.SS1.p1.38.m2.5.5.4.3.cmml" xref="S2.SS1.p1.38.m2.5.5.4.3">𝜃</ci></apply><vector id="S2.SS1.p1.38.m2.5.5.2.3.cmml" xref="S2.SS1.p1.38.m2.5.5.2.2"><ci id="S2.SS1.p1.38.m2.2.2.cmml" xref="S2.SS1.p1.38.m2.2.2">𝐗</ci><ci id="S2.SS1.p1.38.m2.3.3.cmml" xref="S2.SS1.p1.38.m2.3.3">𝑡</ci><apply id="S2.SS1.p1.38.m2.4.4.1.1.1.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1"><times id="S2.SS1.p1.38.m2.4.4.1.1.1.1.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1.1"></times><apply id="S2.SS1.p1.38.m2.4.4.1.1.1.2.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.38.m2.4.4.1.1.1.2.1.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.38.m2.4.4.1.1.1.2.2.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2.2">𝐄</ci><ci id="S2.SS1.p1.38.m2.4.4.1.1.1.2.3.cmml" xref="S2.SS1.p1.38.m2.4.4.1.1.1.2.3">𝜃</ci></apply><ci id="S2.SS1.p1.38.m2.1.1.cmml" xref="S2.SS1.p1.38.m2.1.1">𝐲</ci></apply><apply id="S2.SS1.p1.38.m2.5.5.2.2.2.cmml" xref="S2.SS1.p1.38.m2.5.5.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.38.m2.5.5.2.2.2.1.cmml" xref="S2.SS1.p1.38.m2.5.5.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.38.m2.5.5.2.2.2.2.cmml" xref="S2.SS1.p1.38.m2.5.5.2.2.2.2">𝑠</ci><ci id="S2.SS1.p1.38.m2.5.5.2.2.2.3.cmml" xref="S2.SS1.p1.38.m2.5.5.2.2.2.3">E</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.38.m2.5c">{\mathbf{S}_{\theta}}({\mathbf{X}},t,{\mathbf{E}_{\theta}}({\mathbf{y}}),s_{\mathrm{E}})</annotation></semantics></math> is necessary. <math id="S2.SS1.p1.39.m3.1" class="ltx_Math" alttext="{\mathbf{S}_{\theta}}" display="inline"><semantics id="S2.SS1.p1.39.m3.1a"><msub id="S2.SS1.p1.39.m3.1.1" xref="S2.SS1.p1.39.m3.1.1.cmml"><mi id="S2.SS1.p1.39.m3.1.1.2" xref="S2.SS1.p1.39.m3.1.1.2.cmml">𝐒</mi><mi id="S2.SS1.p1.39.m3.1.1.3" xref="S2.SS1.p1.39.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.39.m3.1b"><apply id="S2.SS1.p1.39.m3.1.1.cmml" xref="S2.SS1.p1.39.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.39.m3.1.1.1.cmml" xref="S2.SS1.p1.39.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.39.m3.1.1.2.cmml" xref="S2.SS1.p1.39.m3.1.1.2">𝐒</ci><ci id="S2.SS1.p1.39.m3.1.1.3.cmml" xref="S2.SS1.p1.39.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.39.m3.1c">{\mathbf{S}_{\theta}}</annotation></semantics></math> is a U-Net model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> that must be trained on labelled speech data with a score-matching objective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. When training on a dataset with multiple speakers, a speaker identity parameter <math id="S2.SS1.p1.40.m4.1" class="ltx_Math" alttext="s_{\mathrm{E}}" display="inline"><semantics id="S2.SS1.p1.40.m4.1a"><msub id="S2.SS1.p1.40.m4.1.1" xref="S2.SS1.p1.40.m4.1.1.cmml"><mi id="S2.SS1.p1.40.m4.1.1.2" xref="S2.SS1.p1.40.m4.1.1.2.cmml">s</mi><mi mathvariant="normal" id="S2.SS1.p1.40.m4.1.1.3" xref="S2.SS1.p1.40.m4.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.40.m4.1b"><apply id="S2.SS1.p1.40.m4.1.1.cmml" xref="S2.SS1.p1.40.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.40.m4.1.1.1.cmml" xref="S2.SS1.p1.40.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.40.m4.1.1.2.cmml" xref="S2.SS1.p1.40.m4.1.1.2">𝑠</ci><ci id="S2.SS1.p1.40.m4.1.1.3.cmml" xref="S2.SS1.p1.40.m4.1.1.3">E</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.40.m4.1c">s_{\mathrm{E}}</annotation></semantics></math> is given to control generation such that the output is faithful to the target speaker. The probability-flow <abbr title="ordinary differential equation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ODE</span></abbr> (<a href="#S2.E4" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) is solved backwards in time from <math id="S2.SS1.p1.41.m5.1" class="ltx_Math" alttext="{\mathbf{X}}_{T}" display="inline"><semantics id="S2.SS1.p1.41.m5.1a"><msub id="S2.SS1.p1.41.m5.1.1" xref="S2.SS1.p1.41.m5.1.1.cmml"><mi id="S2.SS1.p1.41.m5.1.1.2" xref="S2.SS1.p1.41.m5.1.1.2.cmml">𝐗</mi><mi id="S2.SS1.p1.41.m5.1.1.3" xref="S2.SS1.p1.41.m5.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.41.m5.1b"><apply id="S2.SS1.p1.41.m5.1.1.cmml" xref="S2.SS1.p1.41.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.41.m5.1.1.1.cmml" xref="S2.SS1.p1.41.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.41.m5.1.1.2.cmml" xref="S2.SS1.p1.41.m5.1.1.2">𝐗</ci><ci id="S2.SS1.p1.41.m5.1.1.3.cmml" xref="S2.SS1.p1.41.m5.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.41.m5.1c">{\mathbf{X}}_{T}</annotation></semantics></math> to <math id="S2.SS1.p1.42.m6.1" class="ltx_Math" alttext="{\mathbf{X}}_{0}" display="inline"><semantics id="S2.SS1.p1.42.m6.1a"><msub id="S2.SS1.p1.42.m6.1.1" xref="S2.SS1.p1.42.m6.1.1.cmml"><mi id="S2.SS1.p1.42.m6.1.1.2" xref="S2.SS1.p1.42.m6.1.1.2.cmml">𝐗</mi><mn id="S2.SS1.p1.42.m6.1.1.3" xref="S2.SS1.p1.42.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.42.m6.1b"><apply id="S2.SS1.p1.42.m6.1.1.cmml" xref="S2.SS1.p1.42.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.42.m6.1.1.1.cmml" xref="S2.SS1.p1.42.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.42.m6.1.1.2.cmml" xref="S2.SS1.p1.42.m6.1.1.2">𝐗</ci><cn type="integer" id="S2.SS1.p1.42.m6.1.1.3.cmml" xref="S2.SS1.p1.42.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.42.m6.1c">{\mathbf{X}}_{0}</annotation></semantics></math> using the first-order Euler scheme. The HiFi-GAN vocoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is then used to transform the mel-spectrograms <math id="S2.SS1.p1.43.m7.1" class="ltx_Math" alttext="{\mathbf{X}}" display="inline"><semantics id="S2.SS1.p1.43.m7.1a"><mi id="S2.SS1.p1.43.m7.1.1" xref="S2.SS1.p1.43.m7.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.43.m7.1b"><ci id="S2.SS1.p1.43.m7.1.1.cmml" xref="S2.SS1.p1.43.m7.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.43.m7.1c">{\mathbf{X}}</annotation></semantics></math> into audio waveforms. This pipeline produces speaker-specific augmentation data for the subsequent <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model to capture the nuances of dysarthric speakers. Although augmentation methods exist, which finetune a pretrained Grad-TTS model for <abbr title="voice conversion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VC</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, i.e. for a speech-to-speech task, we explore training Grad-TTS from scratch, i.e. for the more flexible <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> task and without reliance on matched control speech data nor typical <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> pre-training, providing augmentation that only requires labelled dysarthric data. 
</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Adaptation for Dysarthric ASR</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.8" class="ltx_p">The data synthesised in <a href="#S2.SS1" title="2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 2.1</span></a> will be used to finetune the Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> multilingual models.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Whisper finetune code adapted from <a target="_blank" href="https://github.com/vasistalodagala/whisper-finetune" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/vasistalodagala/whisper-finetune</a>. </span></span></span> Whisper is based on an encoder-decoder Transformer architecture with <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">12</annotation></semantics></math> encoder and <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><cn type="integer" id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">12</annotation></semantics></math> decoder layers and is a weakly supervised model trained using up to <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="680" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mn id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">680</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><cn type="integer" id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">680</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">680</annotation></semantics></math>k hours of labelled typical speech data. To date, Transformer models have not been adequately explored in dysarthric <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> due to data sparsity issues, and adaptation of large-scale pre-trained <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> foundation models have not been previously explored. The Whisper model is fine-tuned using labelled data. Parameters in the feature encoder (<math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mn id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><cn type="integer" id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">2</annotation></semantics></math> x conv), model encoder and decoder layers were not frozen. The Whisper-medium (WM) model has <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="763.9" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mn id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">763.9</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><cn type="float" id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">763.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">763.9</annotation></semantics></math>M parameters (<math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="762.3" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><mn id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">762.3</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><cn type="float" id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">762.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">762.3</annotation></semantics></math>M trainable parameters), and the Whisper-large (WL) model has <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="1543.3" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mn id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">1543.3</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><cn type="float" id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">1543.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">1543.3</annotation></semantics></math>M parameters (<math id="S2.SS2.p1.8.m8.1" class="ltx_Math" alttext="1541.4" display="inline"><semantics id="S2.SS2.p1.8.m8.1a"><mn id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml">1541.4</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><cn type="float" id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">1541.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">1541.4</annotation></semantics></math>M trainable parameters). Finally, SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> has been shown to improve the performance of dysarthric <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> with synthesised data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and therefore the Whisper models were also trained with and without SpecAugment in this work. SpecAugment is directly applied to the feature inputs of the <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model, time warping the features, and masking blocks of frequency channels &amp; blocks of time steps. Models with SpecAugment were optimised on the probability of frequency and time masking.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">TORGO</em> database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> containing dysarthric speech is introduced in <a href="#S3.SS1" title="3.1 TORGO Dysarthic Speech Dataset ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3.1</span></a>, and the respective data splits &amp; training methods for dysarthric speech synthesis and <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models are described in Sections <a href="#S3.SS2" title="3.2 Text-to-Speech Synthesis ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and <a href="#S3.SS3" title="3.3 Dysarthric ASR ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, respectively.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>TORGO Dysarthic Speech Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The <em id="S3.SS1.p1.2.1" class="ltx_emph ltx_font_italic">TORGO</em> database<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>TORGO database: <a target="_blank" href="http://www.cs.toronto.edu/~complingweb/data/TORGO/torgo.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.cs.toronto.edu/~complingweb/data/TORGO/torgo.html</a></span></span></span> contains approx. <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">21</annotation></semantics></math> hours of aligned acoustic and 3D articulatory feature data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, i.e. much less than usually used for <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> training. The data was gathered from <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">8</annotation></semantics></math> American English dysarthric speakers (with a diagnosis of  <span title="amyotrophic lateral sclerosis" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">amyotrophic lateral sclerosis</span></span> (<abbr title="amyotrophic lateral sclerosis" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ALS</span></abbr>), or  <span title="cerebral palsy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">cerebral palsy</span></span> (<abbr title="cerebral palsy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CP</span></abbr>)), denoted as <em id="S3.SS1.p1.2.2" class="ltx_emph ltx_font_italic">TORGO dysarthric</em>, and seven control speakers that are age-gender-matched to the dysarthric speakers, denoted as <em id="S3.SS1.p1.2.3" class="ltx_emph ltx_font_italic">TORGO control</em>. Utterances with no transcription or that were too short to contain speech were discarded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. All speakers had the same prompts, and therefore there is a large overlap in word and sentence prompts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Since the TORGO dataset does not provide pre-defined data splits, a  <span title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">leave-one-speaker-out</span></span> (<abbr title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LOSO</span></abbr>) approach is commonly implemented for <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. 
<br class="ltx_break">The dysarthric speakers in <em id="S3.SS1.p1.2.4" class="ltx_emph ltx_font_italic">TORGO</em> were assessed by a  <span title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">speech and language therapist</span></span> (<abbr title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SLT</span></abbr>) using the  <span title="Frenchay Dysarthria Assessment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Frenchay Dysarthria Assessment</span></span> (<abbr title="Frenchay Dysarthria Assessment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FDA</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. The dysarthria severity ratings of the <em id="S3.SS1.p1.2.5" class="ltx_emph ltx_font_italic">TORGO</em> dysarthric speakers are displayed in <a href="#S3.T1" title="Table 1 ‣ 3.1 TORGO Dysarthic Speech Dataset ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>. `F' and `M' denote gender, and the numeral denotes the participant number in the dataset.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dysarthria severity for the <em id="S3.T1.2.1" class="ltx_emph ltx_font_italic">TORGO</em> database.</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.3.1" class="ltx_tr">
<td id="S3.T1.3.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.3.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.3.1.2.1" class="ltx_text" style="font-size:80%;">Severe</span></td>
<td id="S3.T1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.3.1.3.1" class="ltx_text" style="font-size:80%;">Mod.-Sev.</span></td>
<td id="S3.T1.3.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.3.1.4.1" class="ltx_text" style="font-size:80%;">Moderate</span></td>
<td id="S3.T1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.3.1.5.1" class="ltx_text" style="font-size:80%;">Mild</span></td>
</tr>
<tr id="S3.T1.3.2" class="ltx_tr">
<td id="S3.T1.3.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.3.2.1.1" class="ltx_text" style="font-size:80%;">Participant</span></td>
<td id="S3.T1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T1.3.2.2.1" class="ltx_text"></span><span id="S3.T1.3.2.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S3.T1.3.2.2.3" class="ltx_text" style="font-size:80%;">
<span id="S3.T1.3.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.3.2.2.3.1.1" class="ltx_tr">
<span id="S3.T1.3.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">F01, M01,</span></span>
<span id="S3.T1.3.2.2.3.1.2" class="ltx_tr">
<span id="S3.T1.3.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">M02, M04</span></span>
</span></span><span id="S3.T1.3.2.2.4" class="ltx_text"></span><span id="S3.T1.3.2.2.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S3.T1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.3.2.3.1" class="ltx_text" style="font-size:80%;">M05</span></td>
<td id="S3.T1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.3.2.4.1" class="ltx_text" style="font-size:80%;">F03</span></td>
<td id="S3.T1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.3.2.5.1" class="ltx_text" style="font-size:80%;">F04, M03</span></td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Text-to-Speech Synthesis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.7" class="ltx_p">For dysarthric speech synthesis, we train the Grad-TTS models (cf. <a href="#S2.SS1" title="2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 2.1</span></a>) using <em id="S3.SS2.p1.7.1" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> from scratch. Previous implementations of <abbr title="diffusion probabilistic modelling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DPM</span></abbr> <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> pre-train the synthesis model on age-gender-matched typical speech and finetune on dysarthric speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The Grad-TTS models require training and validation data for a given speaker to train a speaker embedding <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="s_{\mathrm{E}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">s</mi><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">E</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">s_{\mathrm{E}}</annotation></semantics></math> in (<a href="#S2.E1" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). <em id="S3.SS2.p1.7.2" class="ltx_emph ltx_font_italic">TORGO</em> does not have pre-defined data splits. Therefore, data splits were created for <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> training by pairing array and head microphones (of the same utterance) and then randomly splitting utterances into train, validation and test data splits in an <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">80</annotation></semantics></math>%, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">10</annotation></semantics></math>%, <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn type="integer" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">10</annotation></semantics></math>% ratio per speaker. A systematic approach was considered, e.g. considering the distribution of single/multi-word utterances, and distribution of utterances across splits. However, analysis in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> shows there are <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="951" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">951</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn type="integer" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">951</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">951</annotation></semantics></math>-<math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="969" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mn id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">969</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><cn type="integer" id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">969</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">969</annotation></semantics></math> unique utterances (across <math id="S3.SS2.p1.7.m7.2" class="ltx_Math" alttext="16,158" display="inline"><semantics id="S3.SS2.p1.7.m7.2a"><mrow id="S3.SS2.p1.7.m7.2.3.2" xref="S3.SS2.p1.7.m7.2.3.1.cmml"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">16</mn><mo id="S3.SS2.p1.7.m7.2.3.2.1" xref="S3.SS2.p1.7.m7.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.7.m7.2.2" xref="S3.SS2.p1.7.m7.2.2.cmml">158</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.2b"><list id="S3.SS2.p1.7.m7.2.3.1.cmml" xref="S3.SS2.p1.7.m7.2.3.2"><cn type="integer" id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">16</cn><cn type="integer" id="S3.SS2.p1.7.m7.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2">158</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.2c">16,158</annotation></semantics></math> recordings), and not all dysarthric speakers completed recordings of all utterances. Once the <em id="S3.SS2.p1.7.3" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> models are trained, the transcripts for all splits are input to the trained models to synthesise additional training data for <abbr title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LOSO</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model adaptation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As Grad-TTS has not yet been adequately explored with dysarthric speech, we investigate <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\beta_{T}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">β</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝛽</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\beta_{T}</annotation></semantics></math> hyperparameter values in (<a href="#S2.E3" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Additionally, in the interest of using as little dysarthric data as possible, we investigate three conditions:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">An <em id="S3.I1.ix1.p1.1.1" class="ltx_emph ltx_font_italic"> <span title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">all-speaker</span></span> (<abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr>) model</em> is trained using the data of all dysarthric speakers (i.e. the <em id="S3.I1.ix1.p1.1.2" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> data). The <abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr> model is trained using the training and validation splits of the entire dataset, and used to synthesise training data for all speakers.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><em id="S3.I1.ix2.p1.1.1" class="ltx_emph ltx_font_italic"><span id="S3.I1.ix2.p1.1.1.1" class="ltx_ERROR undefined">\Ac</span>SSp models</em> are trained using a single dysarthric speaker's data (i.e. a model is trained using one speaker's training and validation data, and this model is used to synthesise data for the same speaker).</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><em id="S3.I1.ix3.p1.1.1" class="ltx_emph ltx_font_italic"><span id="S3.I1.ix3.p1.1.1.1" class="ltx_ERROR undefined">\Ac</span>DSpG models</em>: the <em id="S3.I1.ix3.p1.1.2" class="ltx_emph ltx_font_italic">TORGO</em> dysarthric speakers are partitioned into two groups by dysarthria severity rating on the <abbr title="Frenchay Dysarthria Assessment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FDA</span></abbr> (cf. <a href="#S3.T1" title="Table 1 ‣ 3.1 TORGO Dysarthic Speech Dataset ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>). Severe and mod-severe speakers (i.e. F01, M01, M02, M04, M05) form  <span title="Group 1" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Group <math class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation-xml encoding="MathML-Content"><cn type="integer">1</cn></annotation-xml><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span> (<abbr title="Group 1" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">G<math class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation-xml encoding="MathML-Content"><cn type="integer">1</cn></annotation-xml><annotation encoding="application/x-tex">1</annotation></semantics></math></span></abbr>), and mild and moderate speakers (i.e. F03, F04, M03)  <span title="Group 2" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Group <math class="ltx_Math" alttext="2" display="inline"><semantics><mn>2</mn><annotation-xml encoding="MathML-Content"><cn type="integer">2</cn></annotation-xml><annotation encoding="application/x-tex">2</annotation></semantics></math></span></span> (<abbr title="Group 2" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">G<math class="ltx_Math" alttext="2" display="inline"><semantics><mn>2</mn><annotation-xml encoding="MathML-Content"><cn type="integer">2</cn></annotation-xml><annotation encoding="application/x-tex">2</annotation></semantics></math></span></abbr>). The <span id="S3.I1.ix3.p1.1.3" class="ltx_ERROR undefined">\Ac</span>DSpG <abbr title="Group 1" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">G<math class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation-xml encoding="MathML-Content"><cn type="integer">1</cn></annotation-xml><annotation encoding="application/x-tex">1</annotation></semantics></math></span></abbr> model is trained on <abbr title="Group 1" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">G<math class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation-xml encoding="MathML-Content"><cn type="integer">1</cn></annotation-xml><annotation encoding="application/x-tex">1</annotation></semantics></math></span></abbr> speakers' training and validation data, and used to synthesise data for these speakers.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Although control speaker data is not used to train models for dysarthric speech synthesis, <abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr> and  <span title="single-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">single-speaker</span></span> (<abbr title="single-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSp</span></abbr>) <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">TORGO control</em> <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> models were trained in the same manner to compare evaluation metrics. Finally, Grad-TTS models are optimised on hyperparameters of learning rate, epochs and batch size.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dysarthric ASR</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">The Whisper <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model is finetuned on a composition of real (TORGO) data and synthetic data (created by <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> in <a href="#S2.SS1" title="2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 2.1</span></a>). A cumulative ratio of additional synthetic training data for data augmentation (from <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math>-<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">100</annotation></semantics></math>% in <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><cn type="integer" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">10</annotation></semantics></math>% increments) is implemented. The <em id="S3.SS3.p1.3.1" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> models are trained using the <abbr title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LOSO</span></abbr> methodology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> to create  <span title="speaker independent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">speaker independent</span></span> (<abbr title="speaker independent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SI</span></abbr>) models: for a given target speaker, the data of the remaining speakers are used to train the model, which is tested on the given target speaker's data (and therefore the target speaker's data is not seen by the <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model).
Values for learning rate, warm-up, epochs, and batch size hyperparameters are optimised during training.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation metrics</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To evaluate the quality of the synthesised dysarthric speech, the  <span title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">mean cepstral distortion</span></span> (<abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr>) is used as an objective metric, and subjective evaluation by a human expert listener was conducted. The performance of dysarthric <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> systems are measured by <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr>.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Mean Cepstral Distortion (MCD)</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">The <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> is defined as the Euclidian distance between the synthesised and reference mel spectra, and is computed by alignment with  <span title="dynamic time warping" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">dynamic time warping</span></span> (<abbr title="dynamic time warping" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DTW</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. The <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> has been shown to have correlation to subjective test results in speech synthesis analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, although this has not been adequately investigated with dysarthric speech.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Subjective Evaluation</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.6" class="ltx_p">Subjective evaluation by expert listeners (<abbr title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">SLTs</span></abbr>) has been used to measure the presence and severity of dysarthric speech characteristics in synthesised speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. For this study, an <abbr title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SLT</span></abbr> with <math id="S3.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="&gt;10" display="inline"><semantics id="S3.SS4.SSS2.p1.1.m1.1a"><mrow id="S3.SS4.SSS2.p1.1.m1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.1.1.2" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS4.SSS2.p1.1.m1.1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S3.SS4.SSS2.p1.1.m1.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.1b"><apply id="S3.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"><gt id="S3.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.1c">&gt;10</annotation></semantics></math> years of experience assessing and diagnosing speech disorders perceptually evaluated the synthesised data. For every dysarthric speaker, <math id="S3.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS4.SSS2.p1.2.m2.1a"><mn id="S3.SS4.SSS2.p1.2.m2.1.1" xref="S3.SS4.SSS2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.2.m2.1c">20</annotation></semantics></math> audio samples from the TORGO database and <math id="S3.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS4.SSS2.p1.3.m3.1a"><mn id="S3.SS4.SSS2.p1.3.m3.1.1" xref="S3.SS4.SSS2.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.3.m3.1b"><cn type="integer" id="S3.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS2.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.3.m3.1c">20</annotation></semantics></math> synthetic audio samples were randomly selected. The selected audio samples were presented to the <abbr title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SLT</span></abbr> individually in random order. Every audio sample was rated on overall dysarthria severity on a <math id="S3.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS4.SSS2.p1.4.m4.1a"><mn id="S3.SS4.SSS2.p1.4.m4.1.1" xref="S3.SS4.SSS2.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.4.m4.1b"><cn type="integer" id="S3.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS2.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.4.m4.1c">5</annotation></semantics></math>-point scale (between <math id="S3.SS4.SSS2.p1.5.m5.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS4.SSS2.p1.5.m5.1a"><mn id="S3.SS4.SSS2.p1.5.m5.1.1" xref="S3.SS4.SSS2.p1.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.5.m5.1b"><cn type="integer" id="S3.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS2.p1.5.m5.1.1">0</cn></annotation-xml></semantics></math> for none and <math id="S3.SS4.SSS2.p1.6.m6.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS4.SSS2.p1.6.m6.1a"><mn id="S3.SS4.SSS2.p1.6.m6.1.1" xref="S3.SS4.SSS2.p1.6.m6.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.6.m6.1b"><cn type="integer" id="S3.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS2.p1.6.m6.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.6.m6.1c">4</annotation></semantics></math> for severe) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The mean scores were calculated for TORGO and synthetic audio per speaker, to allow comparison of the presence and severity of dysarthric speech characteristics.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Word Error Rate (WER)</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">Inference was performed with the pretrained baseline Whisper model, and adapted dysarthric Whisper models on a given target speaker (adapted models were trained on the remaining speakers in a <abbr title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LOSO</span></abbr> methodology). Transcripts were processed with Whisper's English text normalizer,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Whisper normalizer: <a target="_blank" href="https://github.com/openai/whisper/blob/main/whisper/normalizers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper/blob/main/whisper/normalizers</a></span></span></span> and <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> calculated between the processed reference and hypothesis transcripts. Average (Avg.) <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> is calculated as the average of single-speaker <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> scores, and severity group averages calculated as the average scores of speakers in the group (to allow direct comparison to similar studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>). The overall (Ovl.) <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> score, commonly used to assess <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> for typical speech was also calculated by computing the <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> score for transcripts across all speakers.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Text to Speech Synthesis</h3>

<div id="S4.SS1.p1" class="ltx_para">
<span id="S4.SS1.p1.1" class="ltx_ERROR undefined">\Ac</span>
<p id="S4.SS1.p1.2" class="ltx_p">MCD results for the <em id="S4.SS1.p1.2.1" class="ltx_emph ltx_font_italic">TORGO control</em> (C) and <em id="S4.SS1.p1.2.2" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> (D) <abbr title="text-to-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTS</span></abbr> models are shown in <a href="#S4.T2" title="Table 2 ‣ 4.1 Text to Speech Synthesis ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>MCD for GradTTS synthesis trained on <em id="S4.T2.4.1" class="ltx_emph ltx_font_italic">TORGO</em> data.</figcaption>
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:102.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(102.3pt,-24.1pt) scale(1.8940972076421,1.8940972076421) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.2.3" class="ltx_tr">
<td id="S4.T2.2.2.3.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.2.2.3.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.2.3.2.1" class="ltx_text" style="font-size:80%;">C ASp</span></td>
<td id="S4.T2.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.2.3.3.1" class="ltx_text" style="font-size:80%;">C SSp</span></td>
<td id="S4.T2.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.2.3.4.1" class="ltx_text" style="font-size:80%;">D ASp</span></td>
<td id="S4.T2.2.2.3.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.2.3.5.1" class="ltx_text" style="font-size:80%;">D DSpG</span></td>
<td id="S4.T2.2.2.3.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.2.3.6.1" class="ltx_text" style="font-size:80%;">D SSp</span></td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\beta_{T}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">β</mi><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝛽</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\beta_{T}</annotation></semantics></math><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;"> = 10</span>
</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.61</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.62</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.61</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.71</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.81</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center">
<math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\beta_{T}" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><msub id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T2.2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.2.1.m1.1.1.2.cmml">β</mi><mi mathsize="80%" id="S4.T2.2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.2.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><apply id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.2">𝛽</ci><ci id="S4.T2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\beta_{T}</annotation></semantics></math><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:80%;"> = 20</span>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:80%;">6.75</span></td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.3.1" class="ltx_text" style="font-size:80%;">7.92</span></td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:80%;">6.98</span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.5.1" class="ltx_text" style="font-size:80%;">7.49</span></td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.6.1" class="ltx_text" style="font-size:80%;">7.80</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">Comparing <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> values for <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> models with <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\beta_{T}=10" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><msub id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2.2" xref="S4.SS1.p2.1.m1.1.1.2.2.cmml">β</mi><mi id="S4.SS1.p2.1.m1.1.1.2.3" xref="S4.SS1.p2.1.m1.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.2">𝛽</ci><ci id="S4.SS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\beta_{T}=10</annotation></semantics></math> and <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\beta_{T}=20" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><msub id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2.2" xref="S4.SS1.p2.2.m2.1.1.2.2.cmml">β</mi><mi id="S4.SS1.p2.2.m2.1.1.2.3" xref="S4.SS1.p2.2.m2.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><eq id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></eq><apply id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2.2">𝛽</ci><ci id="S4.SS1.p2.2.m2.1.1.2.3.cmml" xref="S4.SS1.p2.2.m2.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\beta_{T}=20</annotation></semantics></math> in (<a href="#S2.E3" title="In 2.1 Dysarthric Speech Synthesis ‣ 2 Methodology ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), the <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\beta_{T}=10" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">β</mi><mi id="S4.SS1.p2.3.m3.1.1.2.3" xref="S4.SS1.p2.3.m3.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">𝛽</ci><ci id="S4.SS1.p2.3.m3.1.1.2.3.cmml" xref="S4.SS1.p2.3.m3.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\beta_{T}=10</annotation></semantics></math> models show lower <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> for control (C) and dysarthric (D) groups for all conditions <abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr>, <abbr title="single-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SSp</span></abbr> &amp;  <span title="dysarthria-severity-group speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">dysarthria-severity-group speaker</span></span> (<abbr title="dysarthria-severity-group speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DSpG</span></abbr>) (as defined in <a href="#S3.SS2" title="3.2 Text-to-Speech Synthesis ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3.2</span></a>). Results indicate a slight tendency that more data leads to better (lower) MCD, however, for this the <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> difference is minor. 
<br class="ltx_break">An <abbr title="speech and language therapist" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SLT</span></abbr> conducted a subjective evaluation (cf. <a href="#S3.SS4.SSS2" title="3.4.2 Subjective Evaluation ‣ 3.4 Evaluation metrics ‣ 3 Experimental Setup ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3.4.2</span></a>) of TORGO data and data synthesised from the best model (i.e. D <abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr>), and the averaged ratings for dysarthria severity, their difference
as well as the <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> metrics are displayed in <a href="#S4.T3" title="Table 3 ‣ 4.1 Text to Speech Synthesis ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. The ratings show that dysarthric speech characteristics are present in the synthesised samples, but there are differences in the level of severity.
The Kendall's Tau coefficient between average dysarthria severity scores and severity group <abbr title="mean cepstral distortion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MCD</span></abbr> scores is <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="-0.67" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mo id="S4.SS1.p2.4.m4.1.1a" xref="S4.SS1.p2.4.m4.1.1.cmml">−</mo><mn id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">0.67</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><minus id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"></minus><cn type="float" id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">0.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">-0.67</annotation></semantics></math>, indicating a strong negative association between the two ranked variables.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Subjective evaluation of dysarthic data. </figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.2.1" class="ltx_text" style="font-size:80%;">Severe</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.3.1" class="ltx_text" style="font-size:80%;">Mod.-Sev.</span></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">Moderate</span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">Mild</span></td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.1.1" class="ltx_text" style="font-size:80%;">D severity ref.</span></td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.1" class="ltx_text" style="font-size:80%;">3.28</span></td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.3.1" class="ltx_text" style="font-size:80%;">2.45</span></td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.4.1" class="ltx_text" style="font-size:80%;">1.70</span></td>
<td id="S4.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.5.1" class="ltx_text" style="font-size:80%;">0.275</span></td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.1.1" class="ltx_text" style="font-size:80%;">D severity syn.</span></td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.2.1" class="ltx_text" style="font-size:80%;">2.63</span></td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.3.1" class="ltx_text" style="font-size:80%;">2.55</span></td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.4.1" class="ltx_text" style="font-size:80%;">1.35</span></td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.5.1" class="ltx_text" style="font-size:80%;">1.05</span></td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.4.1.1" class="ltx_text" style="font-size:80%;">Difference</span></td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.4.2.1" class="ltx_text" style="font-size:80%;">0.65</span></td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.4.3.1" class="ltx_text" style="font-size:80%;">-0.10</span></td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.4.4.1" class="ltx_text" style="font-size:80%;">0.35</span></td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.4.5.1" class="ltx_text" style="font-size:80%;">-0.78</span></td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.5.1.1" class="ltx_text" style="font-size:80%;">MCD</span></td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.5.2.1" class="ltx_text" style="font-size:80%;">5.72</span></td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.5.3.1" class="ltx_text" style="font-size:80%;">7.09</span></td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.5.4.1" class="ltx_text" style="font-size:80%;">5.88</span></td>
<td id="S4.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.5.5.1" class="ltx_text" style="font-size:80%;">6.44</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>ASR model adaptation</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Whisper baseline performance</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The pretrained Whisper models (without any finetuning) are used for inference on the <em id="S4.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">TORGO control</em> (C) and <em id="S4.SS2.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">TORGO dysarthric</em> (D) data to establish baseline performance in the following. As a <abbr title="leave-one-speaker-out" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LOSO</span></abbr> approach is used for <em id="S4.SS2.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">TORGO</em> <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> model adaptation, inference is performed on the whole dataset. <a href="#S4.T4" title="Table 4 ‣ 4.2.1 Whisper baseline performance ‣ 4.2 ASR model adaptation ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> shows the performance in <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> for the  <span title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Whisper-medium</span></span> (<abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr>) and  <span title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Whisper-large</span></span> (<abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr>) baseline models.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>WER in % for the Whisper medium (WM) and large (WL) baseline models. C denotes control and D dysarthric.</figcaption>
<div id="S4.T4.16" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:159.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(94.6pt,-34.9pt) scale(1.77470481767973,1.77470481767973) ;">
<table id="S4.T4.16.16" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.16.16.17" class="ltx_tr">
<td id="S4.T4.16.16.17.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T4.16.16.17.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.16.16.17.2.1" class="ltx_text" style="font-size:80%;">Sev.</span></td>
<td id="S4.T4.16.16.17.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.16.16.17.3.1" class="ltx_text" style="font-size:80%;">M.-S.</span></td>
<td id="S4.T4.16.16.17.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.16.16.17.4.1" class="ltx_text" style="font-size:80%;">Mod.</span></td>
<td id="S4.T4.16.16.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.16.16.17.5.1" class="ltx_text" style="font-size:80%;">Mild</span></td>
<td id="S4.T4.16.16.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.16.16.17.6.1" class="ltx_text" style="font-size:80%;">Avg.</span></td>
<td id="S4.T4.16.16.17.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.16.16.17.7.1" class="ltx_text" style="font-size:80%;">Ovl.</span></td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.3.1" class="ltx_text" style="font-size:80%;">C WM</span></td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.2.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.2.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.2.2.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="17.93" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">17.93</mn><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><cn type="float" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">17.93</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">17.93</annotation></semantics></math></td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="13.69" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mn mathsize="80%" id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">13.69</mn><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><cn type="float" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">13.69</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">13.69</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.4.4.3.1" class="ltx_text" style="font-size:80%;">C WL</span></td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.4.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.4.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.4.4.4.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.4.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.4.4.4.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.4.4.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="12.31" display="inline"><semantics id="S4.T4.3.3.3.1.m1.1a"><mn mathsize="80%" id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">12.31</mn><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><cn type="float" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">12.31</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">12.31</annotation></semantics></math></td>
<td id="S4.T4.4.4.4.2" class="ltx_td ltx_align_center"><math id="S4.T4.4.4.4.2.m1.1" class="ltx_Math" alttext="11.92" display="inline"><semantics id="S4.T4.4.4.4.2.m1.1a"><mn mathsize="80%" id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">11.92</mn><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><cn type="float" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">11.92</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">11.92</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.10.10.10" class="ltx_tr">
<td id="S4.T4.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.10.10.10.7.1" class="ltx_text" style="font-size:80%;">D WM</span></td>
<td id="S4.T4.5.5.5.1" class="ltx_td ltx_align_center"><math id="S4.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="123.37" display="inline"><semantics id="S4.T4.5.5.5.1.m1.1a"><mn mathsize="80%" id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">123.37</mn><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><cn type="float" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">123.37</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">123.37</annotation></semantics></math></td>
<td id="S4.T4.6.6.6.2" class="ltx_td ltx_align_center"><math id="S4.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="168.83" display="inline"><semantics id="S4.T4.6.6.6.2.m1.1a"><mn mathsize="80%" id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">168.83</mn><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><cn type="float" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">168.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">168.83</annotation></semantics></math></td>
<td id="S4.T4.7.7.7.3" class="ltx_td ltx_align_center"><math id="S4.T4.7.7.7.3.m1.1" class="ltx_Math" alttext="45.96" display="inline"><semantics id="S4.T4.7.7.7.3.m1.1a"><mn mathsize="80%" id="S4.T4.7.7.7.3.m1.1.1" xref="S4.T4.7.7.7.3.m1.1.1.cmml">45.96</mn><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.3.m1.1b"><cn type="float" id="S4.T4.7.7.7.3.m1.1.1.cmml" xref="S4.T4.7.7.7.3.m1.1.1">45.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.3.m1.1c">45.96</annotation></semantics></math></td>
<td id="S4.T4.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T4.8.8.8.4.m1.1" class="ltx_Math" alttext="10.79" display="inline"><semantics id="S4.T4.8.8.8.4.m1.1a"><mn mathsize="80%" id="S4.T4.8.8.8.4.m1.1.1" xref="S4.T4.8.8.8.4.m1.1.1.cmml">10.79</mn><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.4.m1.1b"><cn type="float" id="S4.T4.8.8.8.4.m1.1.1.cmml" xref="S4.T4.8.8.8.4.m1.1.1">10.79</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.4.m1.1c">10.79</annotation></semantics></math></td>
<td id="S4.T4.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T4.9.9.9.5.m1.1" class="ltx_Math" alttext="91.23" display="inline"><semantics id="S4.T4.9.9.9.5.m1.1a"><mn mathsize="80%" id="S4.T4.9.9.9.5.m1.1.1" xref="S4.T4.9.9.9.5.m1.1.1.cmml">91.23</mn><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.5.m1.1b"><cn type="float" id="S4.T4.9.9.9.5.m1.1.1.cmml" xref="S4.T4.9.9.9.5.m1.1.1">91.23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.5.m1.1c">91.23</annotation></semantics></math></td>
<td id="S4.T4.10.10.10.6" class="ltx_td ltx_align_center"><math id="S4.T4.10.10.10.6.m1.1" class="ltx_Math" alttext="84.90" display="inline"><semantics id="S4.T4.10.10.10.6.m1.1a"><mn mathsize="80%" id="S4.T4.10.10.10.6.m1.1.1" xref="S4.T4.10.10.10.6.m1.1.1.cmml">84.90</mn><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.6.m1.1b"><cn type="float" id="S4.T4.10.10.10.6.m1.1.1.cmml" xref="S4.T4.10.10.10.6.m1.1.1">84.90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.6.m1.1c">84.90</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.16.16.16" class="ltx_tr">
<td id="S4.T4.16.16.16.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.16.16.16.7.1" class="ltx_text" style="font-size:80%;">D WL</span></td>
<td id="S4.T4.11.11.11.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="126.20" display="inline"><semantics id="S4.T4.11.11.11.1.m1.1a"><mn mathsize="80%" id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">126.20</mn><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><cn type="float" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">126.20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">126.20</annotation></semantics></math></td>
<td id="S4.T4.12.12.12.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.12.12.12.2.m1.1" class="ltx_Math" alttext="187.72" display="inline"><semantics id="S4.T4.12.12.12.2.m1.1a"><mn mathsize="80%" id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">187.72</mn><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><cn type="float" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">187.72</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">187.72</annotation></semantics></math></td>
<td id="S4.T4.13.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.13.13.13.3.m1.1" class="ltx_Math" alttext="32.42" display="inline"><semantics id="S4.T4.13.13.13.3.m1.1a"><mn mathsize="80%" id="S4.T4.13.13.13.3.m1.1.1" xref="S4.T4.13.13.13.3.m1.1.1.cmml">32.42</mn><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.3.m1.1b"><cn type="float" id="S4.T4.13.13.13.3.m1.1.1.cmml" xref="S4.T4.13.13.13.3.m1.1.1">32.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.3.m1.1c">32.42</annotation></semantics></math></td>
<td id="S4.T4.14.14.14.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T4.14.14.14.4.m1.1" class="ltx_Math" alttext="10.38" display="inline"><semantics id="S4.T4.14.14.14.4.m1.1a"><mn mathsize="80%" id="S4.T4.14.14.14.4.m1.1.1" xref="S4.T4.14.14.14.4.m1.1.1.cmml">10.38</mn><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.4.m1.1b"><cn type="float" id="S4.T4.14.14.14.4.m1.1.1.cmml" xref="S4.T4.14.14.14.4.m1.1.1">10.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.4.m1.1c">10.38</annotation></semantics></math></td>
<td id="S4.T4.15.15.15.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T4.15.15.15.5.m1.1" class="ltx_Math" alttext="93.21" display="inline"><semantics id="S4.T4.15.15.15.5.m1.1a"><mn mathsize="80%" id="S4.T4.15.15.15.5.m1.1.1" xref="S4.T4.15.15.15.5.m1.1.1.cmml">93.21</mn><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.5.m1.1b"><cn type="float" id="S4.T4.15.15.15.5.m1.1.1.cmml" xref="S4.T4.15.15.15.5.m1.1.1">93.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.5.m1.1c">93.21</annotation></semantics></math></td>
<td id="S4.T4.16.16.16.6" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.16.16.16.6.m1.1" class="ltx_Math" alttext="82.49" display="inline"><semantics id="S4.T4.16.16.16.6.m1.1a"><mn mathsize="80%" id="S4.T4.16.16.16.6.m1.1.1" xref="S4.T4.16.16.16.6.m1.1.1.cmml">82.49</mn><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.6.m1.1b"><cn type="float" id="S4.T4.16.16.16.6.m1.1.1.cmml" xref="S4.T4.16.16.16.6.m1.1.1">82.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.6.m1.1c">82.49</annotation></semantics></math></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.2" class="ltx_p">The <abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr> and <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> models for the <em id="S4.SS2.SSS1.p2.2.1" class="ltx_emph ltx_font_italic">TORGO control</em> (non-dysarthric) data achieve overall <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">WERs</span></abbr> of <math id="S4.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="13.69" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mn id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml">13.69</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><cn type="float" id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1">13.69</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">13.69</annotation></semantics></math> and <math id="S4.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="11.92" display="inline"><semantics id="S4.SS2.SSS1.p2.2.m2.1a"><mn id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml">11.92</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.1b"><cn type="float" id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1">11.92</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.1c">11.92</annotation></semantics></math>%,
respectively, and results for dysarthic speech shows much higher <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr>. The <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> model has a relatively lower overall <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> score on dysarthric speakers but a higher <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> in average over speakers due to to relatively poorer performance for severe and moderate-severe dysarthric speakers.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Whisper model adaptation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The Whisper medium (<abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr>) and large (<abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr>) models are finetuned on a composition of real data (TORGO) and an increasing percentage of synthetic data synthesised by the best <abbr title="text-to-dysarthic-speech" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TTDS</span></abbr> model, i.e. D <abbr title="all-speaker" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASp</span></abbr> with <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\beta_{T}=10" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><msub id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.2.cmml">β</mi><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><eq id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.2">𝛽</ci><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\beta_{T}=10</annotation></semantics></math>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>WER in % for TORGO WM model adaptation. Best performance in bold-face. <sup id="S4.T5.17.1" class="ltx_sup"><span id="S4.T5.17.1.1" class="ltx_text ltx_font_italic">∗</span></sup> only 1 speaker.</figcaption>
<table id="S4.T5.15" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.4.2" class="ltx_tr">
<td id="S4.T5.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T5.4.2.3.1" class="ltx_text" style="font-size:80%;">Aug. %</span></td>
<td id="S4.T5.4.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.4.2.4.1" class="ltx_text" style="font-size:80%;">Sev.</span></td>
<td id="S4.T5.3.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T5.3.1.1.1" class="ltx_text" style="font-size:80%;">M.-Sev.</span><sup id="S4.T5.3.1.1.2" class="ltx_sup"><span id="S4.T5.3.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup>
</td>
<td id="S4.T5.4.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T5.4.2.2.1" class="ltx_text" style="font-size:80%;">Mod.</span><sup id="S4.T5.4.2.2.2" class="ltx_sup"><span id="S4.T5.4.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup>
</td>
<td id="S4.T5.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T5.4.2.5.1" class="ltx_text" style="font-size:80%;">Mild</span></td>
<td id="S4.T5.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T5.4.2.6.1" class="ltx_text" style="font-size:80%;">Avg.</span></td>
<td id="S4.T5.4.2.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.4.2.7.1" class="ltx_text" style="font-size:80%;">Ovl.</span></td>
</tr>
<tr id="S4.T5.5.3" class="ltx_tr">
<td id="S4.T5.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T5.5.3.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.T5.5.3.1.m1.1a"><mn mathsize="80%" id="S4.T5.5.3.1.m1.1.1" xref="S4.T5.5.3.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.T5.5.3.1.m1.1b"><cn type="integer" id="S4.T5.5.3.1.m1.1.1.cmml" xref="S4.T5.5.3.1.m1.1.1">0</cn></annotation-xml></semantics></math></td>
<td id="S4.T5.5.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.5.3.2.1" class="ltx_text" style="font-size:80%;">70.25</span></td>
<td id="S4.T5.5.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.5.3.3.1" class="ltx_text" style="font-size:80%;">145.76</span></td>
<td id="S4.T5.5.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.5.3.4.1" class="ltx_text" style="font-size:80%;">28.54</span></td>
<td id="S4.T5.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.5.3.5.1" class="ltx_text" style="font-size:80%;">3.44</span></td>
<td id="S4.T5.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.5.3.6.1" class="ltx_text" style="font-size:80%;">57.77</span></td>
<td id="S4.T5.5.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.5.3.7.1" class="ltx_text" style="font-size:80%;">56.14</span></td>
</tr>
<tr id="S4.T5.6.4" class="ltx_tr">
<td id="S4.T5.6.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.6.4.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.T5.6.4.1.m1.1a"><mn mathsize="80%" id="S4.T5.6.4.1.m1.1.1" xref="S4.T5.6.4.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.4.1.m1.1b"><cn type="integer" id="S4.T5.6.4.1.m1.1.1.cmml" xref="S4.T5.6.4.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.4.1.m1.1c">10</annotation></semantics></math></td>
<td id="S4.T5.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.6.4.2.1" class="ltx_text" style="font-size:80%;">38.33</span></td>
<td id="S4.T5.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.6.4.3.1" class="ltx_text" style="font-size:80%;">24.83</span></td>
<td id="S4.T5.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.6.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.3</span></td>
<td id="S4.T5.6.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.6.4.5.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S4.T5.6.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.6.4.6.1" class="ltx_text" style="font-size:80%;">25.80</span></td>
<td id="S4.T5.6.4.7" class="ltx_td ltx_align_center"><span id="S4.T5.6.4.7.1" class="ltx_text" style="font-size:80%;">24.3</span></td>
</tr>
<tr id="S4.T5.7.5" class="ltx_tr">
<td id="S4.T5.7.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.7.5.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.T5.7.5.1.m1.1a"><mn mathsize="80%" id="S4.T5.7.5.1.m1.1.1" xref="S4.T5.7.5.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.T5.7.5.1.m1.1b"><cn type="integer" id="S4.T5.7.5.1.m1.1.1.cmml" xref="S4.T5.7.5.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.5.1.m1.1c">20</annotation></semantics></math></td>
<td id="S4.T5.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T5.7.5.2.1" class="ltx_text" style="font-size:80%;">60.30</span></td>
<td id="S4.T5.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T5.7.5.3.1" class="ltx_text" style="font-size:80%;">23.23</span></td>
<td id="S4.T5.7.5.4" class="ltx_td ltx_align_center"><span id="S4.T5.7.5.4.1" class="ltx_text" style="font-size:80%;">24.38</span></td>
<td id="S4.T5.7.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.7.5.5.1" class="ltx_text" style="font-size:80%;">3.33</span></td>
<td id="S4.T5.7.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.7.5.6.1" class="ltx_text" style="font-size:80%;">36.93</span></td>
<td id="S4.T5.7.5.7" class="ltx_td ltx_align_center"><span id="S4.T5.7.5.7.1" class="ltx_text" style="font-size:80%;">35.05</span></td>
</tr>
<tr id="S4.T5.8.6" class="ltx_tr">
<td id="S4.T5.8.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.8.6.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.T5.8.6.1.m1.1a"><mn mathsize="80%" id="S4.T5.8.6.1.m1.1.1" xref="S4.T5.8.6.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.6.1.m1.1b"><cn type="integer" id="S4.T5.8.6.1.m1.1.1.cmml" xref="S4.T5.8.6.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.6.1.m1.1c">30</annotation></semantics></math></td>
<td id="S4.T5.8.6.2" class="ltx_td ltx_align_center"><span id="S4.T5.8.6.2.1" class="ltx_text" style="font-size:80%;">33.38</span></td>
<td id="S4.T5.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.8.6.3.1" class="ltx_text" style="font-size:80%;">24.13</span></td>
<td id="S4.T5.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.8.6.4.1" class="ltx_text" style="font-size:80%;">22.66</span></td>
<td id="S4.T5.8.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.8.6.5.1" class="ltx_text" style="font-size:80%;">3.13</span></td>
<td id="S4.T5.8.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.8.6.6.1" class="ltx_text" style="font-size:80%;">23.32</span></td>
<td id="S4.T5.8.6.7" class="ltx_td ltx_align_center"><span id="S4.T5.8.6.7.1" class="ltx_text" style="font-size:80%;">22.27</span></td>
</tr>
<tr id="S4.T5.9.7" class="ltx_tr">
<td id="S4.T5.9.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.9.7.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.T5.9.7.1.m1.1a"><mn mathsize="80%" id="S4.T5.9.7.1.m1.1.1" xref="S4.T5.9.7.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.T5.9.7.1.m1.1b"><cn type="integer" id="S4.T5.9.7.1.m1.1.1.cmml" xref="S4.T5.9.7.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.7.1.m1.1c">40</annotation></semantics></math></td>
<td id="S4.T5.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T5.9.7.2.1" class="ltx_text" style="font-size:80%;">31.87</span></td>
<td id="S4.T5.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T5.9.7.3.1" class="ltx_text" style="font-size:80%;">18.98</span></td>
<td id="S4.T5.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T5.9.7.4.1" class="ltx_text" style="font-size:80%;">23.13</span></td>
<td id="S4.T5.9.7.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.9.7.5.1" class="ltx_text" style="font-size:80%;">3.23</span></td>
<td id="S4.T5.9.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.9.7.6.1" class="ltx_text" style="font-size:80%;">22.00</span></td>
<td id="S4.T5.9.7.7" class="ltx_td ltx_align_center"><span id="S4.T5.9.7.7.1" class="ltx_text" style="font-size:80%;">20.08</span></td>
</tr>
<tr id="S4.T5.10.8" class="ltx_tr">
<td id="S4.T5.10.8.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.10.8.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.T5.10.8.1.m1.1a"><mn mathsize="80%" id="S4.T5.10.8.1.m1.1.1" xref="S4.T5.10.8.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.T5.10.8.1.m1.1b"><cn type="integer" id="S4.T5.10.8.1.m1.1.1.cmml" xref="S4.T5.10.8.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.8.1.m1.1c">50</annotation></semantics></math></td>
<td id="S4.T5.10.8.2" class="ltx_td ltx_align_center"><span id="S4.T5.10.8.2.1" class="ltx_text" style="font-size:80%;">31.69</span></td>
<td id="S4.T5.10.8.3" class="ltx_td ltx_align_center"><span id="S4.T5.10.8.3.1" class="ltx_text" style="font-size:80%;">24.9</span></td>
<td id="S4.T5.10.8.4" class="ltx_td ltx_align_center"><span id="S4.T5.10.8.4.1" class="ltx_text" style="font-size:80%;">26.57</span></td>
<td id="S4.T5.10.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.10.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.07</span></td>
<td id="S4.T5.10.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.10.8.6.1" class="ltx_text" style="font-size:80%;">23.05</span></td>
<td id="S4.T5.10.8.7" class="ltx_td ltx_align_center"><span id="S4.T5.10.8.7.1" class="ltx_text" style="font-size:80%;">22.41</span></td>
</tr>
<tr id="S4.T5.11.9" class="ltx_tr">
<td id="S4.T5.11.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.11.9.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.T5.11.9.1.m1.1a"><mn mathsize="80%" id="S4.T5.11.9.1.m1.1.1" xref="S4.T5.11.9.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.T5.11.9.1.m1.1b"><cn type="integer" id="S4.T5.11.9.1.m1.1.1.cmml" xref="S4.T5.11.9.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.9.1.m1.1c">60</annotation></semantics></math></td>
<td id="S4.T5.11.9.2" class="ltx_td ltx_align_center"><span id="S4.T5.11.9.2.1" class="ltx_text" style="font-size:80%;">31.83</span></td>
<td id="S4.T5.11.9.3" class="ltx_td ltx_align_center"><span id="S4.T5.11.9.3.1" class="ltx_text" style="font-size:80%;">20.03</span></td>
<td id="S4.T5.11.9.4" class="ltx_td ltx_align_center"><span id="S4.T5.11.9.4.1" class="ltx_text" style="font-size:80%;">24.38</span></td>
<td id="S4.T5.11.9.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.11.9.5.1" class="ltx_text" style="font-size:80%;">3.86</span></td>
<td id="S4.T5.11.9.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.11.9.6.1" class="ltx_text" style="font-size:80%;">22.43</span></td>
<td id="S4.T5.11.9.7" class="ltx_td ltx_align_center"><span id="S4.T5.11.9.7.1" class="ltx_text" style="font-size:80%;">21.77</span></td>
</tr>
<tr id="S4.T5.12.10" class="ltx_tr">
<td id="S4.T5.12.10.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.12.10.1.m1.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S4.T5.12.10.1.m1.1a"><mn mathsize="80%" id="S4.T5.12.10.1.m1.1.1" xref="S4.T5.12.10.1.m1.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.T5.12.10.1.m1.1b"><cn type="integer" id="S4.T5.12.10.1.m1.1.1.cmml" xref="S4.T5.12.10.1.m1.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.10.1.m1.1c">70</annotation></semantics></math></td>
<td id="S4.T5.12.10.2" class="ltx_td ltx_align_center"><span id="S4.T5.12.10.2.1" class="ltx_text" style="font-size:80%;">33.65</span></td>
<td id="S4.T5.12.10.3" class="ltx_td ltx_align_center"><span id="S4.T5.12.10.3.1" class="ltx_text" style="font-size:80%;">20.93</span></td>
<td id="S4.T5.12.10.4" class="ltx_td ltx_align_center"><span id="S4.T5.12.10.4.1" class="ltx_text" style="font-size:80%;">23.49</span></td>
<td id="S4.T5.12.10.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.12.10.5.1" class="ltx_text" style="font-size:80%;">3.96</span></td>
<td id="S4.T5.12.10.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.12.10.6.1" class="ltx_text" style="font-size:80%;">23.37</span></td>
<td id="S4.T5.12.10.7" class="ltx_td ltx_align_center"><span id="S4.T5.12.10.7.1" class="ltx_text" style="font-size:80%;">22.31</span></td>
</tr>
<tr id="S4.T5.13.11" class="ltx_tr">
<td id="S4.T5.13.11.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.13.11.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T5.13.11.1.m1.1a"><mn mathsize="80%" id="S4.T5.13.11.1.m1.1.1" xref="S4.T5.13.11.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T5.13.11.1.m1.1b"><cn type="integer" id="S4.T5.13.11.1.m1.1.1.cmml" xref="S4.T5.13.11.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.11.1.m1.1c">80</annotation></semantics></math></td>
<td id="S4.T5.13.11.2" class="ltx_td ltx_align_center"><span id="S4.T5.13.11.2.1" class="ltx_text" style="font-size:80%;">30.45</span></td>
<td id="S4.T5.13.11.3" class="ltx_td ltx_align_center"><span id="S4.T5.13.11.3.1" class="ltx_text" style="font-size:80%;">21.28</span></td>
<td id="S4.T5.13.11.4" class="ltx_td ltx_align_center"><span id="S4.T5.13.11.4.1" class="ltx_text" style="font-size:80%;">25.89</span></td>
<td id="S4.T5.13.11.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.13.11.5.1" class="ltx_text" style="font-size:80%;">3.55</span></td>
<td id="S4.T5.13.11.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.13.11.6.1" class="ltx_text" style="font-size:80%;">22.01</span></td>
<td id="S4.T5.13.11.7" class="ltx_td ltx_align_center"><span id="S4.T5.13.11.7.1" class="ltx_text" style="font-size:80%;">21.71</span></td>
</tr>
<tr id="S4.T5.14.12" class="ltx_tr">
<td id="S4.T5.14.12.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T5.14.12.1.m1.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S4.T5.14.12.1.m1.1a"><mn mathsize="80%" id="S4.T5.14.12.1.m1.1.1" xref="S4.T5.14.12.1.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S4.T5.14.12.1.m1.1b"><cn type="integer" id="S4.T5.14.12.1.m1.1.1.cmml" xref="S4.T5.14.12.1.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.12.1.m1.1c">90</annotation></semantics></math></td>
<td id="S4.T5.14.12.2" class="ltx_td ltx_align_center"><span id="S4.T5.14.12.2.1" class="ltx_text" style="font-size:80%;">30.45</span></td>
<td id="S4.T5.14.12.3" class="ltx_td ltx_align_center"><span id="S4.T5.14.12.3.1" class="ltx_text" style="font-size:80%;">21.28</span></td>
<td id="S4.T5.14.12.4" class="ltx_td ltx_align_center"><span id="S4.T5.14.12.4.1" class="ltx_text" style="font-size:80%;">25.89</span></td>
<td id="S4.T5.14.12.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.14.12.5.1" class="ltx_text" style="font-size:80%;">3.55</span></td>
<td id="S4.T5.14.12.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.14.12.6.1" class="ltx_text" style="font-size:80%;">22.01</span></td>
<td id="S4.T5.14.12.7" class="ltx_td ltx_align_center"><span id="S4.T5.14.12.7.1" class="ltx_text" style="font-size:80%;">21.71</span></td>
</tr>
<tr id="S4.T5.15.13" class="ltx_tr">
<td id="S4.T5.15.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T5.15.13.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.T5.15.13.1.m1.1a"><mn mathsize="80%" id="S4.T5.15.13.1.m1.1.1" xref="S4.T5.15.13.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.T5.15.13.1.m1.1b"><cn type="integer" id="S4.T5.15.13.1.m1.1.1.cmml" xref="S4.T5.15.13.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.13.1.m1.1c">100</annotation></semantics></math></td>
<td id="S4.T5.15.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.15.13.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">28.49</span></td>
<td id="S4.T5.15.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.15.13.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">17.87</span></td>
<td id="S4.T5.15.13.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.15.13.4.1" class="ltx_text" style="font-size:80%;">26.14</span></td>
<td id="S4.T5.15.13.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.15.13.5.1" class="ltx_text" style="font-size:80%;">3.82</span></td>
<td id="S4.T5.15.13.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.15.13.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20.70</span></td>
<td id="S4.T5.15.13.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.15.13.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20.45</span></td>
</tr>
</table>
</figure>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.6" class="ltx_p"><a href="#S4.T5" title="Table 5 ‣ 4.2.2 Whisper model adaptation ‣ 4.2 ASR model adaptation ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a> shows the results of the <abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr> adaptation, since <abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr> is the smaller model and showed better performance than <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> for more severe dysarthric data. Adaptation using only real dysarthric data (i.e. no synthetic data) achieves an overall <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> score of <math id="S4.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="56.14" display="inline"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mn id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml">56.14</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><cn type="float" id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1">56.14</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">56.14</annotation></semantics></math>%, i.e. performes significantly better than the baseline in <a href="#S4.T4" title="Table 4 ‣ 4.2.1 Whisper baseline performance ‣ 4.2 ASR model adaptation ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>. Synthetic data further improves performance, with the best performance achieved using <math id="S4.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mn id="S4.SS2.SSS2.p2.2.m2.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">100</annotation></semantics></math>% additional synthesised training data, i.e. the training speakers in LOSO adaptation,
reducing overall and average <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> to <math id="S4.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="20.45" display="inline"><semantics id="S4.SS2.SSS2.p2.3.m3.1a"><mn id="S4.SS2.SSS2.p2.3.m3.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.cmml">20.45</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.3.m3.1b"><cn type="float" id="S4.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1">20.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.3.m3.1c">20.45</annotation></semantics></math>% and <math id="S4.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="20.70" display="inline"><semantics id="S4.SS2.SSS2.p2.4.m4.1a"><mn id="S4.SS2.SSS2.p2.4.m4.1.1" xref="S4.SS2.SSS2.p2.4.m4.1.1.cmml">20.70</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.4.m4.1b"><cn type="float" id="S4.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1">20.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.4.m4.1c">20.70</annotation></semantics></math>%, respectively. Performance gains can be observed in particular for severe and moderate-severe dysarthric speech. The <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> model also performed best with <math id="S4.SS2.SSS2.p2.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.SSS2.p2.5.m5.1a"><mn id="S4.SS2.SSS2.p2.5.m5.1.1" xref="S4.SS2.SSS2.p2.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.5.m5.1b"><cn type="integer" id="S4.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.5.m5.1c">100</annotation></semantics></math>% additional synthesised data (not explicitly shown here), and the trend is maintained when the Whisper models are additionally trained with SpecAugment. <a href="#S4.T6" title="Table 6 ‣ 4.2.2 Whisper model adaptation ‣ 4.2 ASR model adaptation ‣ 4 Results ‣ Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech SynthesisThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]." class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a> compares the <abbr title="word error rate" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WER</span></abbr> performance of the <abbr title="Whisper-medium" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WM</span></abbr> and <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> with <math id="S4.SS2.SSS2.p2.6.m6.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.SSS2.p2.6.m6.1a"><mn id="S4.SS2.SSS2.p2.6.m6.1.1" xref="S4.SS2.SSS2.p2.6.m6.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.6.m6.1b"><cn type="integer" id="S4.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.6.m6.1c">100</annotation></semantics></math>% additional synthetic data (with and without SpecAugment) to recent <abbr title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SotA</span></abbr> benchmarks on the <em id="S4.SS2.SSS2.p2.6.1" class="ltx_emph ltx_font_italic">TORGO</em> <abbr title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASR</span></abbr> task. The proposed Grad-TTS augmented Whisper model adaptation outperforms all baseline models on the same task. The <abbr title="Whisper-large" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">WL</span></abbr> with SpecAugment shows best performance overall.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>TORGO WER performance in comparison to benchmarks. +denotes SpecAugment.</figcaption>
<div id="S4.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:184pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(25.9pt,-11.0pt) scale(1.13552485636376,1.13552485636376) ;">
<table id="S4.T6.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.4.4.5" class="ltx_tr">
<td id="S4.T6.4.4.5.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T6.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.2.1" class="ltx_text" style="font-size:80%;">Sev.</span></td>
<td id="S4.T6.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.3.1" class="ltx_text" style="font-size:80%;">M.-Sev.</span></td>
<td id="S4.T6.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.4.1" class="ltx_text" style="font-size:80%;">Mod</span></td>
<td id="S4.T6.4.4.5.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.5.1" class="ltx_text" style="font-size:80%;">Mild</span></td>
<td id="S4.T6.4.4.5.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.6.1" class="ltx_text" style="font-size:80%;">Avg.</span></td>
<td id="S4.T6.4.4.5.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.4.4.5.7.1" class="ltx_text" style="font-size:80%;">Ovl.</span></td>
</tr>
<tr id="S4.T6.4.4.6" class="ltx_tr">
<td id="S4.T6.4.4.6.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T6.4.4.6.1.1" class="ltx_text" style="font-size:80%;">LF-MMI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.4.4.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T6.4.4.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T6.4.4.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.4.4.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.4.4.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.4.4.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.4.4.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.6.1" class="ltx_text" style="font-size:80%;">42.90</span></td>
<td id="S4.T6.4.4.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.6.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.4.4.7" class="ltx_tr">
<td id="S4.T6.4.4.7.1" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.7.1.1" class="ltx_text" style="font-size:80%;">FS2 &amp; D-HMM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.4.4.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S4.T6.4.4.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T6.4.4.7.2" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.2.1" class="ltx_text" style="font-size:80%;">55.88</span></td>
<td id="S4.T6.4.4.7.3" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.3.1" class="ltx_text" style="font-size:80%;">49.60</span></td>
<td id="S4.T6.4.4.7.4" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.4.1" class="ltx_text" style="font-size:80%;">36.80</span></td>
<td id="S4.T6.4.4.7.5" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.5.1" class="ltx_text" style="font-size:80%;">12.60</span></td>
<td id="S4.T6.4.4.7.6" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.6.1" class="ltx_text" style="font-size:80%;">39.20</span></td>
<td id="S4.T6.4.4.7.7" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.7.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.4.4.8" class="ltx_tr">
<td id="S4.T6.4.4.8.1" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.8.1.1" class="ltx_text" style="font-size:80%;">FMLLR-DNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.4.4.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T6.4.4.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T6.4.4.8.2" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.2.1" class="ltx_text" style="font-size:80%;">43.29</span></td>
<td id="S4.T6.4.4.8.3" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.3.1" class="ltx_text" style="font-size:80%;">44.05</span></td>
<td id="S4.T6.4.4.8.4" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.4.1" class="ltx_text" style="font-size:80%;">35.93</span></td>
<td id="S4.T6.4.4.8.5" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.5.1" class="ltx_text" style="font-size:80%;">11.65</span></td>
<td id="S4.T6.4.4.8.6" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.6.1" class="ltx_text" style="font-size:80%;">34.55</span></td>
<td id="S4.T6.4.4.8.7" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.8.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.4.4.9" class="ltx_tr">
<td id="S4.T6.4.4.9.1" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.9.1.1" class="ltx_text" style="font-size:80%;">SD-CTL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.4.4.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T6.4.4.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T6.4.4.9.2" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.2.1" class="ltx_text" style="font-size:80%;">68.24</span></td>
<td id="S4.T6.4.4.9.3" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.3.1" class="ltx_text" style="font-size:80%;">33.15</span></td>
<td id="S4.T6.4.4.9.4" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.4.1" class="ltx_text" style="font-size:80%;">22.84</span></td>
<td id="S4.T6.4.4.9.5" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.5.1" class="ltx_text" style="font-size:80%;">10.35</span></td>
<td id="S4.T6.4.4.9.6" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.6.1" class="ltx_text" style="font-size:80%;">30.76</span></td>
<td id="S4.T6.4.4.9.7" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.9.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.4.4.10" class="ltx_tr">
<td id="S4.T6.4.4.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.4.4.10.1.1" class="ltx_text" style="font-size:80%;">GTTS &amp; WM</span></td>
<td id="S4.T6.4.4.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.2.1" class="ltx_text" style="font-size:80%;">28.49</span></td>
<td id="S4.T6.4.4.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.3.1" class="ltx_text" style="font-size:80%;">17.87</span></td>
<td id="S4.T6.4.4.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.4.1" class="ltx_text" style="font-size:80%;">26.14</span></td>
<td id="S4.T6.4.4.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.5.1" class="ltx_text" style="font-size:80%;">3.82</span></td>
<td id="S4.T6.4.4.10.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.6.1" class="ltx_text" style="font-size:80%;">20.70</span></td>
<td id="S4.T6.4.4.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.4.4.10.7.1" class="ltx_text" style="font-size:80%;">20.45</span></td>
</tr>
<tr id="S4.T6.4.4.4" class="ltx_tr">
<td id="S4.T6.4.4.4.5" class="ltx_td ltx_align_left"><span id="S4.T6.4.4.4.5.1" class="ltx_text" style="font-size:80%;">GTTS &amp; WM+</span></td>
<td id="S4.T6.4.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.4.6.1" class="ltx_text" style="font-size:80%;">25.82</span></td>
<td id="S4.T6.4.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.4.7.1" class="ltx_text" style="font-size:80%;">14.95</span></td>
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center"><math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="5.26" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mn mathsize="80%" id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml">5.26</mn><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><cn type="float" id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">5.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">5.26</annotation></semantics></math></td>
<td id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center"><math id="S4.T6.2.2.2.2.m1.1" class="ltx_Math" alttext="4.28" display="inline"><semantics id="S4.T6.2.2.2.2.m1.1a"><mn mathsize="80%" id="S4.T6.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.m1.1.1.cmml">4.28</mn><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.m1.1b"><cn type="float" id="S4.T6.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.m1.1.1">4.28</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.m1.1c">4.28</annotation></semantics></math></td>
<td id="S4.T6.3.3.3.3" class="ltx_td ltx_align_center"><math id="S4.T6.3.3.3.3.m1.1" class="ltx_Math" alttext="19.16" display="inline"><semantics id="S4.T6.3.3.3.3.m1.1a"><mn mathsize="80%" id="S4.T6.3.3.3.3.m1.1.1" xref="S4.T6.3.3.3.3.m1.1.1.cmml">19.16</mn><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.m1.1b"><cn type="float" id="S4.T6.3.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.3.m1.1.1">19.16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.m1.1c">19.16</annotation></semantics></math></td>
<td id="S4.T6.4.4.4.4" class="ltx_td ltx_align_center"><math id="S4.T6.4.4.4.4.m1.1" class="ltx_Math" alttext="18.35" display="inline"><semantics id="S4.T6.4.4.4.4.m1.1a"><mn mathsize="80%" id="S4.T6.4.4.4.4.m1.1.1" xref="S4.T6.4.4.4.4.m1.1.1.cmml">18.35</mn><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.4.m1.1b"><cn type="float" id="S4.T6.4.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.4.m1.1.1">18.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.4.m1.1c">18.35</annotation></semantics></math></td>
</tr>
<tr id="S4.T6.4.4.11" class="ltx_tr">
<td id="S4.T6.4.4.11.1" class="ltx_td ltx_align_left"><span id="S4.T6.4.4.11.1.1" class="ltx_text" style="font-size:80%;">GTTS &amp; WL</span></td>
<td id="S4.T6.4.4.11.2" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.2.1" class="ltx_text" style="font-size:80%;">46.39</span></td>
<td id="S4.T6.4.4.11.3" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.3.1" class="ltx_text" style="font-size:80%;">18.29</span></td>
<td id="S4.T6.4.4.11.4" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.4.1" class="ltx_text" style="font-size:80%;">24.27</span></td>
<td id="S4.T6.4.4.11.5" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.5.1" class="ltx_text" style="font-size:80%;">3.03</span></td>
<td id="S4.T6.4.4.11.6" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.6.1" class="ltx_text" style="font-size:80%;">25.84</span></td>
<td id="S4.T6.4.4.11.7" class="ltx_td ltx_align_center"><span id="S4.T6.4.4.11.7.1" class="ltx_text" style="font-size:80%;">25.28</span></td>
</tr>
<tr id="S4.T6.4.4.12" class="ltx_tr">
<td id="S4.T6.4.4.12.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T6.4.4.12.1.1" class="ltx_text" style="font-size:80%;">GTTS &amp; WL+</span></td>
<td id="S4.T6.4.4.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">23.30</span></td>
<td id="S4.T6.4.4.12.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">13.98</span></td>
<td id="S4.T6.4.4.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.27</span></td>
<td id="S4.T6.4.4.12.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">2.57</span></td>
<td id="S4.T6.4.4.12.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">16.93</span></td>
<td id="S4.T6.4.4.12.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.4.4.12.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">16.68</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work showed that it is possible to train Grad-TTS from scratch without matched control data to synthesise samples with dysarthric speech characteristics. A <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\beta_{T}=10" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><msub id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml"><mi id="S5.p1.1.m1.1.1.2.2" xref="S5.p1.1.m1.1.1.2.2.cmml">β</mi><mi id="S5.p1.1.m1.1.1.2.3" xref="S5.p1.1.m1.1.1.2.3.cmml">T</mi></msub><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><eq id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></eq><apply id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.2.1.cmml" xref="S5.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.2.cmml" xref="S5.p1.1.m1.1.1.2.2">𝛽</ci><ci id="S5.p1.1.m1.1.1.2.3.cmml" xref="S5.p1.1.m1.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\beta_{T}=10</annotation></semantics></math> schedule improves sample quality for the typical and dysarthric speech data used. The results show that Whisper can be finetuned for <abbr title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SotA</span></abbr> <abbr title="dysarthric ASR" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DASR</span></abbr> on <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">TORGO</em>, and that data augmentation is beneficial. The amount of synthesised data required is dependent on the severity of the dysarthric speaker.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. R. Duffy, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Motor Speech Disorders</em>.   Elsevier, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Walshe, <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Clinical cases in dysarthria</em>.   Milton Park, Abingdon, Oxon New York, NY: Routledge, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. D. Page and K. M. Yorkston, ``Communicative participation in dysarthria: Perspectives for management,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Brain sciences</em>, vol. 12, no. 4, p. 420, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z. C. Clarke, S. Judge, K. Fryer, S. Cunningham, J. Toogood, and M. S. Hawley, ``A qualitative study exploring the effect of communicating with partially intelligible speech,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Augmentative and Alternative Communication</em>, vol. 39, no. 2, pp. 110–122, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Yusufali, R. K. Moore, and S. Goetze, ``Refining Text Input for Augmentative and Alternative Communication (AAC) Devices: Analysing Language Model Layers for Optimisation,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024</em>, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. J. Higginbotham, H. Shane, S. Russell, and K. Caves, ``Access to AAC: Present, past, and future,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Augmentative and Alternative Communication</em>, vol. 23, no. 3, pp. 243–257, 2007.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Goetze, N. Moritz, J.-E. Appell, M. Meis, C. Bartsch, and J. Bitzer, ``Acoustic user interfaces for ambient-assisted living technologies,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Informatics for Health and Social Care</em>, 2010.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N. Moritz, K. Adiloğlu, J. Anemüller, S. Goetze, and B. Kollmeier, ``Multi-channel speech enhancement and amplitude modulation analysis for noise robust automatic speech recognition,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol. 46, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. Zheng, A. Xiao, G. Keren, D. Le, F. Zhang, C. Fuegen, O. Kalinli, Y. Saraf, and A. Mohamed, ``Scaling ASR Improves Zero and Few Shot Learning,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. S. Hawley, P. Enderby, P. Green, S. Cunningham, S. Brownsell, J. Carmichael, M. Parker, A. Hatzis, P. O’Neill, and R. Palmer, ``A speech-controlled environmental control system for people with severe dysarthria,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Medical Engineering &amp; Physics</em>, vol. 29, no. 5, pp. 586–593, 2007.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F. Xiong, J. Barker, and H. Christensen, ``Phonetic analysis of dysarthric speech tempo and applications to robust personalised dysarthric speech recognition,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019</em>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Machine Learning</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
E. Hermann and M. Magimai.-Doss, ``Dysarthric Speech Recognition with Lattice-Free MMI,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP20)</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
B. Vachhani, C. Bhat, B. Das, and S. K. Kopparapu, ``Deep autoencoder based speech features for improved dysarthric speech recognition.'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2017, pp. 1854–1858.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C. Bhat and H. Strik, ``Automatic assessment of sentence-level dysarthria intelligibility using blstm,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 14, no. 2, pp. 322–330, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Almadhor, R. Irfan, J. Gao, N. Saleem, H. T. Rauf, and S. Kadry, ``E2E-DASR: End-to-end deep learning-based dysarthric automatic speech recognition,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 222, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. R. Shahamiri, ``Speech vision: An end-to-end deep learning-based dysarthric automatic speech recognition system,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. on Neural Systems and Rehabilitation Eng.</em>, vol. 29, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood, R. Cave, K. Seaver, M. Ladewig, J. Tobin, M. Brenner, P. Q. Nelson <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Disordered speech data collection: lessons learned at 1 million utterances from project euphonia,'' <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Hernandez, P. A. Pérez-Toro, E. Nöth, J. R. Orozco-Arroyave, A. Maier, and S. H. Yang, ``Cross-lingual self-supervised speech representations for improved dysarthric speech recognition,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.01670</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. P. Violeta, W.-C. Huang, and T. Toda, ``Investigating self-supervised pretraining frameworks for pathological speech recognition,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15431</em>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Z. Chen, A. Rosenberg, Y. Zhang, G. Wang, B. Ramabhadran, and P. J. Moreno, ``Improving Speech Recognition Using GAN-Based Speech Synthesis and Contrastive Unspoken Text Selection,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z. Jin, M. Geng, X. Xie, J. Yu, S. Liu, X. Liu, and H. Meng, ``Adversarial data augmentation for disordered speech recognition,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 4803–4807.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
N. Kanda, R. Takeda, and Y. Obuchi, ``Elastic spectral distortion for low resource speech recognition with deep neural networks,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</em>, 2013.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Vachhani, C. Bhat, and S. K. Kopparapu, ``Data augmentation using healthy speech for dysarthric speech recognition.'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2018, pp. 471–475.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Geng, X. Xie, S. Liu, J. Yu, S. Hu, X. Liu, and H. Meng, ``Investigation of data augmentation techniques for disordered speech recognition,'' <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.05562</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Jin, M. Geng, J. Deng, T. Wang, S. Hu, G. Li, and X. Liu, ``Personalized adversarial data augmentation for dysarthric and elderly speech recognition,'' <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W.-Z. Zheng, J.-Y. Han, C.-Y. Chen, Y.-J. Chang, and Y.-H. Lai, ``Improving the efficiency of dysarthria voice conversion system based on data augmentation,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Systems and Rehabilitation Engineering</em>, vol. 31, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Soleymanpour, M. T. Johnson, R. Soleymanpour, and J. Berry, ``Synthesizing Dysarthric Speech using Multi-talker TTS for Dysarthric Speech Recognition,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP22)</em>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
E. Hermann and M. Magimai.-Doss, ``Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023, pp. 156–160.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
H. Wang, T. Thebaud, J. Villalba, M. Sydnor, B. Lammers, N. Dehak, and L. Moro-Velazquez, ``DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, ``Grad-tts: A diffusion probabilistic model for text-to-speech,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Kim, S. Kim, J. Kong, and S. Yoon, ``Glow-tts: A generative flow for text-to-speech via monotonic alignment search,'' <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, ``Score-based generative modeling through stochastic differential equations,'' in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae, ``HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,'' <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, ``SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,'' in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2019</em>, 2019, pp. 2613–2617.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
F. Rudzicz, A. K. Namasivayam, and T. Wolff, ``The TORGO database of acoustic and articulatory speech from speakers with dysarthria,'' <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, vol. 46, no. 4, pp. 523–541, Mar 2011.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Z. Yue, F. Xiong, H. Christensen, and J. Barker, ``Exploring appropriate acoustic and language modelling choices for continuous dysarthric speech recognition,'' in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020</em>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
P. Enderby, <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Frenchay Dysarthria Assessment</em>.   Pro-Ed, 1983.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
H. Wang, V. Ravichandran, M. Rao, B. Lammers, M. Sydnor, N. Maragakis, A. A. Butala, J. Zhang, L. Clawson, V. Chovaz, and L. Moro-Velazquez, ``Improving fairness for spoken language understanding in atypical speech with text-to-speech,'' in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Workshop on Synthetic Data Generation with Generative AI</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Kominek, T. Schultz, and A. W. Black, ``Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion.'' in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">SLTU</em>, 2008, pp. 63–68.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. N. Chadha, J. H. Nirmal, and P. Kachare, ``A comparative performance of various speech analysis-synthesis techniques,'' <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Int. J. of Signal Processing Systems</em>, vol. 2, no. 1, pp. 17–22, 2014.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Soleymanpour, M. T. Johnson, R. Soleymanpour, and J. Berry, ``Accurate synthesis of dysarthric speech for asr data augmentation,'' <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.08438</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
N. M. Joy and S. Umesh, ``Improving acoustic models in torgo dysarthric speech database,'' <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Systems and Rehabilitation Engineering</em>, vol. 26, pp. 637–645, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
F. Xiong, J. Barker, Z. Yue, and H. Christensen, ``Source domain data selection for improved transfer learning targeting dysarthric speech recognition,'' in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP20)</em>, May 2020.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Wing-Zin Leung"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="dysarthric automatic speech recognition (ASR)"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="dysarthric automatic speech recognition (ASR)"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech Synthesis"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.08567" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.08568" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.08568">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.08568" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.08569" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:45:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
