<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.10056] TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition</title><meta property="og:description" content="This paper presents a novel deep neural network-based architecture tailored for Speech Emotion Recognition (SER). The architecture capitalises on dense interconnections among multiple layers of bidirectional dilated co…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.10056">

<!--Generated on Sat Oct  5 20:32:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TBDM-Net: Bidirectional Dense Networks with Gender Information for 
<br class="ltx_break">Speech Emotion Recognition </h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper presents a novel deep neural network-based architecture tailored for Speech Emotion Recognition (SER). The architecture capitalises on dense interconnections among multiple layers of bidirectional dilated convolutions. A linear kernel dynamically fuses the outputs of these layers to yield the final emotion class prediction. This innovative architecture is denoted as <span id="id1.id1.1" class="ltx_text ltx_font_bold">TBDM-Net</span>: <span id="id1.id1.2" class="ltx_text ltx_font_bold">T</span>emporally-Aware <span id="id1.id1.3" class="ltx_text ltx_font_bold">B</span>i-directional <span id="id1.id1.4" class="ltx_text ltx_font_bold">D</span>ense <span id="id1.id1.5" class="ltx_text ltx_font_bold">M</span>ulti-Scale <span id="id1.id1.6" class="ltx_text ltx_font_bold">Net</span>work. We conduct a comprehensive performance evaluation of <span id="id1.id1.7" class="ltx_text ltx_font_typewriter">TBDM-Net</span>, including an ablation study, across six widely-acknowledged SER datasets for unimodal speech emotion recognition. Additionally, we explore the influence of gender-informed emotion prediction by appending either golden or predicted gender labels to the architecture’s inputs or predictions. The implementation of <span id="id1.id1.8" class="ltx_text ltx_font_typewriter">TBDM-Net</span> is accessible at: <a target="_blank" href="https://github.com/adrianastan/tbdm-net" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/adrianastan/tbdm-net</a>.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
speech emotion recognition, dense nets, convolutions, bidirectional layers, SER</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.10056/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.10.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>The TBDM-Net architecture. The forward and reverse time speech representations are passed through a series of Temporally-Aware Blocks (TABs). The intermediate bidirectional representations are concatenated (<math id="S1.F1.5.m1.1" class="ltx_Math" alttext="g_{k}" display="inline"><semantics id="S1.F1.5.m1.1b"><msub id="S1.F1.5.m1.1.1" xref="S1.F1.5.m1.1.1.cmml"><mi id="S1.F1.5.m1.1.1.2" xref="S1.F1.5.m1.1.1.2.cmml">g</mi><mi id="S1.F1.5.m1.1.1.3" xref="S1.F1.5.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.5.m1.1c"><apply id="S1.F1.5.m1.1.1.cmml" xref="S1.F1.5.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.5.m1.1.1.1.cmml" xref="S1.F1.5.m1.1.1">subscript</csymbol><ci id="S1.F1.5.m1.1.1.2.cmml" xref="S1.F1.5.m1.1.1.2">𝑔</ci><ci id="S1.F1.5.m1.1.1.3.cmml" xref="S1.F1.5.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m1.1d">g_{k}</annotation></semantics></math>), passed through a dimension reduction convolutional block and averaged to obtain a final concatenation of different time-scale representations (<math id="S1.F1.6.m2.1" class="ltx_Math" alttext="g_{k}^{\prime}" display="inline"><semantics id="S1.F1.6.m2.1b"><msubsup id="S1.F1.6.m2.1.1" xref="S1.F1.6.m2.1.1.cmml"><mi id="S1.F1.6.m2.1.1.2.2" xref="S1.F1.6.m2.1.1.2.2.cmml">g</mi><mi id="S1.F1.6.m2.1.1.2.3" xref="S1.F1.6.m2.1.1.2.3.cmml">k</mi><mo id="S1.F1.6.m2.1.1.3" xref="S1.F1.6.m2.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S1.F1.6.m2.1c"><apply id="S1.F1.6.m2.1.1.cmml" xref="S1.F1.6.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.6.m2.1.1.1.cmml" xref="S1.F1.6.m2.1.1">superscript</csymbol><apply id="S1.F1.6.m2.1.1.2.cmml" xref="S1.F1.6.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.6.m2.1.1.2.1.cmml" xref="S1.F1.6.m2.1.1">subscript</csymbol><ci id="S1.F1.6.m2.1.1.2.2.cmml" xref="S1.F1.6.m2.1.1.2.2">𝑔</ci><ci id="S1.F1.6.m2.1.1.2.3.cmml" xref="S1.F1.6.m2.1.1.2.3">𝑘</ci></apply><ci id="S1.F1.6.m2.1.1.3.cmml" xref="S1.F1.6.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m2.1d">g_{k}^{\prime}</annotation></semantics></math>). Each TABs input is passed forward to the next modules through dense concatenative connections. The final representation dynamically fuses the multi-scale representations (<math id="S1.F1.7.m3.1" class="ltx_Math" alttext="g_{df}" display="inline"><semantics id="S1.F1.7.m3.1b"><msub id="S1.F1.7.m3.1.1" xref="S1.F1.7.m3.1.1.cmml"><mi id="S1.F1.7.m3.1.1.2" xref="S1.F1.7.m3.1.1.2.cmml">g</mi><mrow id="S1.F1.7.m3.1.1.3" xref="S1.F1.7.m3.1.1.3.cmml"><mi id="S1.F1.7.m3.1.1.3.2" xref="S1.F1.7.m3.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S1.F1.7.m3.1.1.3.1" xref="S1.F1.7.m3.1.1.3.1.cmml">​</mo><mi id="S1.F1.7.m3.1.1.3.3" xref="S1.F1.7.m3.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.F1.7.m3.1c"><apply id="S1.F1.7.m3.1.1.cmml" xref="S1.F1.7.m3.1.1"><csymbol cd="ambiguous" id="S1.F1.7.m3.1.1.1.cmml" xref="S1.F1.7.m3.1.1">subscript</csymbol><ci id="S1.F1.7.m3.1.1.2.cmml" xref="S1.F1.7.m3.1.1.2">𝑔</ci><apply id="S1.F1.7.m3.1.1.3.cmml" xref="S1.F1.7.m3.1.1.3"><times id="S1.F1.7.m3.1.1.3.1.cmml" xref="S1.F1.7.m3.1.1.3.1"></times><ci id="S1.F1.7.m3.1.1.3.2.cmml" xref="S1.F1.7.m3.1.1.3.2">𝑑</ci><ci id="S1.F1.7.m3.1.1.3.3.cmml" xref="S1.F1.7.m3.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.m3.1d">g_{df}</annotation></semantics></math>), and is passed through a fully connected layer (<math id="S1.F1.8.m4.1" class="ltx_Math" alttext="FC" display="inline"><semantics id="S1.F1.8.m4.1b"><mrow id="S1.F1.8.m4.1.1" xref="S1.F1.8.m4.1.1.cmml"><mi id="S1.F1.8.m4.1.1.2" xref="S1.F1.8.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.F1.8.m4.1.1.1" xref="S1.F1.8.m4.1.1.1.cmml">​</mo><mi id="S1.F1.8.m4.1.1.3" xref="S1.F1.8.m4.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.8.m4.1c"><apply id="S1.F1.8.m4.1.1.cmml" xref="S1.F1.8.m4.1.1"><times id="S1.F1.8.m4.1.1.1.cmml" xref="S1.F1.8.m4.1.1.1"></times><ci id="S1.F1.8.m4.1.1.2.cmml" xref="S1.F1.8.m4.1.1.2">𝐹</ci><ci id="S1.F1.8.m4.1.1.3.cmml" xref="S1.F1.8.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.m4.1d">FC</annotation></semantics></math>) to output emotion class probabilities.</figcaption>
</figure>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_t"><span id="S1.T1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></th>
<th id="S1.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Emotions</span></th>
<th id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Language</span></th>
<th id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">No. samples</span></th>
<th id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.2.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Duration</span></th>
<th id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S1.T1.2.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Speakers</span></th>
<th id="S1.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.2.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">M/F</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.2.2.1" class="ltx_tr">
<td id="S1.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S1.T1.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CASIA</span><span id="S1.T1.2.2.1.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.2.1.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.T1.2.2.1.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.2.2.1.2.1" class="ltx_text" style="font-size:80%;">angry, fearful, happy, neutral, sad, surprised</span></td>
<td id="S1.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.2.1.3.1" class="ltx_text" style="font-size:80%;">Chinese</span></td>
<td id="S1.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.2.1.4.1" class="ltx_text" style="font-size:80%;">1200</span></td>
<td id="S1.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.2.1.5.1" class="ltx_text" style="font-size:80%;">43’</span></td>
<td id="S1.T1.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.2.1.6.1" class="ltx_text" style="font-size:80%;">8</span></td>
<td id="S1.T1.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.2.2.1.7.1" class="ltx_text" style="font-size:80%;">4/4</span></td>
</tr>
<tr id="S1.T1.2.3.2" class="ltx_tr">
<td id="S1.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S1.T1.2.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">EMOVO</span><span id="S1.T1.2.3.2.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.3.2.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.T1.2.3.2.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.2.3.2.2.1" class="ltx_text" style="font-size:80%;">angry, disgusted, fearful, happy, neutral, sad, surprised</span></td>
<td id="S1.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.3.2.3.1" class="ltx_text" style="font-size:80%;">Italian</span></td>
<td id="S1.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.3.2.4.1" class="ltx_text" style="font-size:80%;">588</span></td>
<td id="S1.T1.2.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.3.2.5.1" class="ltx_text" style="font-size:80%;">30’</span></td>
<td id="S1.T1.2.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.3.2.6.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S1.T1.2.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.2.3.2.7.1" class="ltx_text" style="font-size:80%;">3/3</span></td>
</tr>
<tr id="S1.T1.2.4.3" class="ltx_tr">
<td id="S1.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S1.T1.2.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">EMODB</span><span id="S1.T1.2.4.3.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.4.3.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.T1.2.4.3.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.4.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.2.4.3.2.1" class="ltx_text" style="font-size:80%;">angry, bored, disgusted, fearful, happy, neutral, sad</span></td>
<td id="S1.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.4.3.3.1" class="ltx_text" style="font-size:80%;">German</span></td>
<td id="S1.T1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.4.3.4.1" class="ltx_text" style="font-size:80%;">535</span></td>
<td id="S1.T1.2.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.4.3.5.1" class="ltx_text" style="font-size:80%;">24’</span></td>
<td id="S1.T1.2.4.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.4.3.6.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S1.T1.2.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.2.4.3.7.1" class="ltx_text" style="font-size:80%;">5/5</span></td>
</tr>
<tr id="S1.T1.2.5.4" class="ltx_tr">
<td id="S1.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S1.T1.2.5.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IEMOCAP</span><span id="S1.T1.2.5.4.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.5.4.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.T1.2.5.4.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.5.4.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.2.5.4.2.1" class="ltx_text" style="font-size:80%;">angry, happy, neutral, sad</span></td>
<td id="S1.T1.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.5.4.3.1" class="ltx_text" style="font-size:80%;">English</span></td>
<td id="S1.T1.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.5.4.4.1" class="ltx_text" style="font-size:80%;">5531</span></td>
<td id="S1.T1.2.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.5.4.5.1" class="ltx_text" style="font-size:80%;">11h 37’</span></td>
<td id="S1.T1.2.5.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.5.4.6.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S1.T1.2.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.2.5.4.7.1" class="ltx_text" style="font-size:80%;">5/5</span></td>
</tr>
<tr id="S1.T1.2.6.5" class="ltx_tr">
<td id="S1.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_t">
<span id="S1.T1.2.6.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RAVDESS</span><span id="S1.T1.2.6.5.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.6.5.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.T1.2.6.5.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.6.5.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.2.6.5.2.1" class="ltx_text" style="font-size:80%;">angry, calm, disgusted, happy, fearful, sad, surprised</span></td>
<td id="S1.T1.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.6.5.3.1" class="ltx_text" style="font-size:80%;">English</span></td>
<td id="S1.T1.2.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.6.5.4.1" class="ltx_text" style="font-size:80%;">1440</span></td>
<td id="S1.T1.2.6.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.6.5.5.1" class="ltx_text" style="font-size:80%;">1h 28’</span></td>
<td id="S1.T1.2.6.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.6.5.6.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S1.T1.2.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.2.6.5.7.1" class="ltx_text" style="font-size:80%;">12/12</span></td>
</tr>
<tr id="S1.T1.2.7.6" class="ltx_tr">
<td id="S1.T1.2.7.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_t">
<span id="S1.T1.2.7.6.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SAVEE</span><span id="S1.T1.2.7.6.1.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.7.6.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.T1.2.7.6.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.7.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S1.T1.2.7.6.2.1" class="ltx_text" style="font-size:80%;">angry, disgusted, fearful, happy, neutral, sad, surprised</span></td>
<td id="S1.T1.2.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S1.T1.2.7.6.3.1" class="ltx_text" style="font-size:80%;">English</span></td>
<td id="S1.T1.2.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S1.T1.2.7.6.4.1" class="ltx_text" style="font-size:80%;">480</span></td>
<td id="S1.T1.2.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S1.T1.2.7.6.5.1" class="ltx_text" style="font-size:80%;">30’</span></td>
<td id="S1.T1.2.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S1.T1.2.7.6.6.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S1.T1.2.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.2.7.6.7.1" class="ltx_text" style="font-size:80%;">4/0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.5.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Evaluated SER datasets. The M/F column reports the number of male and female speakers in the dataset.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech emotion recognition (SER) plays an important role in adding a new dimension to speech-enabled applications. As opposed to the linguistic content which can be to a large extent controlled by the speaker, emotions pose a harder task when the speakers intends to limiting or control them in conversations.
Understanding emotions expressed through speech can significantly improve human-computer interaction, enabling machines to respond more appropriately to user needs and preferences.
However, a major problem in SER-related tasks is still the lack of accurate training data, as many of the available datasets contain only acted speech, i.e. various emotions are elicited by actors in a controlled scenario. Thus, benchmarking<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://emosuperb.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://emosuperb.github.io/</a></span></span></span> SER can still pose challenges for real-life data deployments and evaluation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As is the case for many other research fields, SER systems are to a large extent based on deep neural networks with increasingly complex architectures. An important research direction is also the use multimodal (e.g. text, image, audio or video) characteristics. However, for this study we rely solely on the information available within the speech signal.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Some of the most notable results in this area are those reported by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Light-SERNet focuses on reducing the computational complexity of SER and uses a network based only on convolutional layers and local feature learning blocks.
Convolutional networks were also used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Wen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> introduce an architecture based on capsule nets and transfer learning. It also deals with cross-corpus evaluation, for which an adversarial module is used. Gradient-based adversary learning framework that jointly estimates the SER labels while also normalising speaker characteristics is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The method is based on large pretrained models and also analyses the use of small labelled datasets.
Croitoru et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> describe a novel general method for learning rate adjustments during the training process where the learning rate of the layers closer to the network’s output are adjusted more finely. The proposed method is evaluated over several distinct prediction tasks, including speech emotion recognition. The architecture used in SER is based on a transformer and a dense net structure.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Schuller et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> attempt to preserve the emotional saturation of speech frames by employing frame-level speech features and attention-based LSTM recurrent neural networks. Recurrent layers were also adopted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and combined with Wavelet transforms and 1D CNNs to extract multiresolution representations of the input speech.
The Dual-TBNet model of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed the combination of 1D convolutional layers, transformer, and BiLSTM modules, to maximise the robustness of the independent speech abstractions which are then fused to provide the final emotion decision.
Notably, Ye et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> introduce <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">TIM-Net</span>, which outperforms previous methods on six standard SER datasets. Inspired by this, we draw from their work and implement several modifications to enhance overall emotion prediction accuracy.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Building upon prior research, this paper introduces a novel architecture for Speech Emotion Recognition (SER) classification. The architecture employs temporally-aware bidirectional dense networks, referred to as Temporally-Aware Bi-directional Dense Multi-Scale Network (<span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">TBDM-Net</span>). The primary contributions of the paper can be summarised as follows: (i) the introduction of a new deep architecture for SER; (ii) an assessment of the proposed architecture across six multilingual SER datasets; (iii) an ablation study to analyse the impact of each architectural module on final performance; and (iv) an examination of the influence of speaker gender information on emotion classification accuracy.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>TBDM-Net Architecture</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span>
uses a series of temporally-aware convolution blocks (<span id="S2.p1.1.2" class="ltx_text ltx_font_typewriter">TABs</span>) with incremental dilation coefficients over the direct and inverse time representation of the input speech signal. All forward and reverse temporal blocks are densely connected. These connections enable the network to exploit previously computed features and reduce the number of required parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
The intermediate representations obtained from each TAB are summed with their reverse time correspondents. The concatenation of the forward and reverse time temporal blocks’ output is passed through a dimension reduction convolutional layer, such that all intermediate representations have the same dimension.
All TAB representations are then concatenated and dynamically fused. The final emotion prediction probabilities are obtained through a simple feed forward layer.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The activation function employed within the Temporal Attention Blocks (TABs) is the Gaussian Error Linear Unit (GELU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. GELU is characterised by its smoothness, maintaining a continuous first derivative across its range, which fosters stable and efficient optimisation processes. Its attribute of granting non-zero gradients for both positive and negative inputs facilitates unrestricted information flow during both forward and backward propagation. Additionally, GELU has demonstrated efficacy in addressing the vanishing gradient issue in deeper neural networks.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The complete architecture is presented in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Its implementation is available at: <a target="_blank" href="https://github.com/adrianastan/tbdm-net" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/adrianastan/tbdm-net</a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.6.6" class="ltx_tr">
<th id="S3.T2.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_t"><span id="S3.T2.6.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></th>
<th id="S3.T2.6.6.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.6.6.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<th id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UAR<math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WAR<math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t"><span id="S3.T2.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1<math id="S3.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T2.3.3.3.1.m1.1.1" xref="S3.T2.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.m1.1b"><ci id="S3.T2.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T2.6.6.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.6.6.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></th>
<th id="S3.T2.6.6.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.6.6.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<th id="S3.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UAR<math id="S3.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.4.4.4.1.m1.1a"><mo stretchy="false" id="S3.T2.4.4.4.1.m1.1.1" xref="S3.T2.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.1.m1.1b"><ci id="S3.T2.4.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T2.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WAR<math id="S3.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.5.5.5.1.m1.1a"><mo stretchy="false" id="S3.T2.5.5.5.1.m1.1.1" xref="S3.T2.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.1.m1.1b"><ci id="S3.T2.5.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T2.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1<math id="S3.T2.6.6.6.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.6.6.6.1.m1.1a"><mo stretchy="false" id="S3.T2.6.6.6.1.m1.1.1" xref="S3.T2.6.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.1.m1.1b"><ci id="S3.T2.6.6.6.1.m1.1.1.cmml" xref="S3.T2.6.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.6.7.1" class="ltx_tr">
<td id="S3.T2.6.7.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt" rowspan="4"><span id="S3.T2.6.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CASIA</span></td>
<td id="S3.T2.6.7.1.2" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.7.1.2.1" class="ltx_text" style="font-size:80%;">Dual-TBNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.7.1.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T2.6.7.1.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.7.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.7.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.70</span></td>
<td id="S3.T2.6.7.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.7.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.7.1.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T2.6.7.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.80</span></td>
<td id="S3.T2.6.7.1.6" class="ltx_td ltx_align_left ltx_border_tt" rowspan="4"><span id="S3.T2.6.7.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">EMODB</span></td>
<td id="S3.T2.6.7.1.7" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.7.1.7.1" class="ltx_text" style="font-size:80%;">Dual-TBNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.7.1.7.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T2.6.7.1.7.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.7.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.7.1.8.1" class="ltx_text" style="font-size:80%;">84.10</span></td>
<td id="S3.T2.6.7.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.7.1.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.7.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.6.7.1.10.1" class="ltx_text" style="font-size:80%;">84.30</span></td>
</tr>
<tr id="S3.T2.6.8.2" class="ltx_tr">
<td id="S3.T2.6.8.2.1" class="ltx_td ltx_align_left">
<span id="S3.T2.6.8.2.1.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.8.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.8.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.8.2.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.8.2.2.1" class="ltx_text" style="font-size:80%;">89.60</span></td>
<td id="S3.T2.6.8.2.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.8.2.3.1" class="ltx_text" style="font-size:80%;">90.40</span></td>
<td id="S3.T2.6.8.2.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.8.2.4.1" class="ltx_text" style="font-size:80%;">89.90</span></td>
<td id="S3.T2.6.8.2.5" class="ltx_td ltx_align_left">
<span id="S3.T2.6.8.2.5.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.8.2.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.8.2.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.8.2.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.8.2.6.1" class="ltx_text" style="font-size:80%;">91.09</span></td>
<td id="S3.T2.6.8.2.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.8.2.7.1" class="ltx_text" style="font-size:80%;">89.30</span></td>
<td id="S3.T2.6.8.2.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.8.2.8.1" class="ltx_text" style="font-size:80%;">89.00</span></td>
</tr>
<tr id="S3.T2.6.9.3" class="ltx_tr">
<td id="S3.T2.6.9.3.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.9.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.9.3.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.9.3.2.1" class="ltx_text" style="font-size:80%;">86.54</span></td>
<td id="S3.T2.6.9.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.9.3.3.1" class="ltx_text" style="font-size:80%;">85.66</span></td>
<td id="S3.T2.6.9.3.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.9.3.4.1" class="ltx_text" style="font-size:80%;">85.77</span></td>
<td id="S3.T2.6.9.3.5" class="ltx_td ltx_align_left"><span id="S3.T2.6.9.3.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.9.3.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.9.3.6.1" class="ltx_text" style="font-size:80%;">90.01</span></td>
<td id="S3.T2.6.9.3.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.9.3.7.1" class="ltx_text" style="font-size:80%;">88.23</span></td>
<td id="S3.T2.6.9.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.9.3.8.1" class="ltx_text" style="font-size:80%;">88.30</span></td>
</tr>
<tr id="S3.T2.6.10.4" class="ltx_tr">
<td id="S3.T2.6.10.4.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.10.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.10.4.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.10.4.2.1" class="ltx_text" style="font-size:80%;">91.01</span></td>
<td id="S3.T2.6.10.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.10.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90.50</span></td>
<td id="S3.T2.6.10.4.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.10.4.4.1" class="ltx_text" style="font-size:80%;">90.54</span></td>
<td id="S3.T2.6.10.4.5" class="ltx_td ltx_align_left"><span id="S3.T2.6.10.4.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.10.4.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.10.4.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">92.94</span></td>
<td id="S3.T2.6.10.4.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.10.4.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.40</span></td>
<td id="S3.T2.6.10.4.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.10.4.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.55</span></td>
</tr>
<tr id="S3.T2.6.11.5" class="ltx_tr">
<td id="S3.T2.6.11.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_tt" rowspan="4"><span id="S3.T2.6.11.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">EMOVO</span></td>
<td id="S3.T2.6.11.5.2" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.11.5.2.1" class="ltx_text" style="font-size:80%;">1BTPDN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.11.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S3.T2.6.11.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.11.5.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.11.5.3.1" class="ltx_text" style="font-size:80%;">74.31</span></td>
<td id="S3.T2.6.11.5.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.11.5.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.11.5.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T2.6.11.5.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.11.5.6" class="ltx_td ltx_align_left ltx_border_tt" rowspan="4"><span id="S3.T2.6.11.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IEMOCAP</span></td>
<td id="S3.T2.6.11.5.7" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.11.5.7.1" class="ltx_text" style="font-size:80%;">SCFA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.11.5.7.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.T2.6.11.5.7.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.11.5.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.11.5.8.1" class="ltx_text" style="font-size:80%;">67.91</span></td>
<td id="S3.T2.6.11.5.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.11.5.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.11.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.6.11.5.10.1" class="ltx_text" style="font-size:80%;">66.42</span></td>
</tr>
<tr id="S3.T2.6.12.6" class="ltx_tr">
<td id="S3.T2.6.12.6.1" class="ltx_td ltx_align_left">
<span id="S3.T2.6.12.6.1.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.12.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.12.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.12.6.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.12.6.2.1" class="ltx_text" style="font-size:80%;">84.60</span></td>
<td id="S3.T2.6.12.6.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.12.6.3.1" class="ltx_text" style="font-size:80%;">85.80</span></td>
<td id="S3.T2.6.12.6.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.12.6.4.1" class="ltx_text" style="font-size:80%;">84.10</span></td>
<td id="S3.T2.6.12.6.5" class="ltx_td ltx_align_left">
<span id="S3.T2.6.12.6.5.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.12.6.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.12.6.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.12.6.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.12.6.6.1" class="ltx_text" style="font-size:80%;">68.70</span></td>
<td id="S3.T2.6.12.6.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.12.6.7.1" class="ltx_text" style="font-size:80%;">67.90</span></td>
<td id="S3.T2.6.12.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.12.6.8.1" class="ltx_text" style="font-size:80%;">67.40</span></td>
</tr>
<tr id="S3.T2.6.13.7" class="ltx_tr">
<td id="S3.T2.6.13.7.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.13.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.13.7.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.13.7.2.1" class="ltx_text" style="font-size:80%;">84.20</span></td>
<td id="S3.T2.6.13.7.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.13.7.3.1" class="ltx_text" style="font-size:80%;">82.12</span></td>
<td id="S3.T2.6.13.7.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.13.7.4.1" class="ltx_text" style="font-size:80%;">82.05</span></td>
<td id="S3.T2.6.13.7.5" class="ltx_td ltx_align_left"><span id="S3.T2.6.13.7.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.13.7.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.13.7.6.1" class="ltx_text" style="font-size:80%;">71.78</span></td>
<td id="S3.T2.6.13.7.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.13.7.7.1" class="ltx_text" style="font-size:80%;">70.05</span></td>
<td id="S3.T2.6.13.7.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.13.7.8.1" class="ltx_text" style="font-size:80%;">70.19</span></td>
</tr>
<tr id="S3.T2.6.14.8" class="ltx_tr">
<td id="S3.T2.6.14.8.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.14.8.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.14.8.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.14.8.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">88.10</span></td>
<td id="S3.T2.6.14.8.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.14.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">87.06</span></td>
<td id="S3.T2.6.14.8.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.14.8.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">87.19</span></td>
<td id="S3.T2.6.14.8.5" class="ltx_td ltx_align_left"><span id="S3.T2.6.14.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.14.8.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.14.8.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">73.28</span></td>
<td id="S3.T2.6.14.8.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.14.8.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">71.88</span></td>
<td id="S3.T2.6.14.8.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.14.8.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">71.94</span></td>
</tr>
<tr id="S3.T2.6.15.9" class="ltx_tr">
<td id="S3.T2.6.15.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_tt" rowspan="4"><span id="S3.T2.6.15.9.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RAVDESS</span></td>
<td id="S3.T2.6.15.9.2" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.15.9.2.1" class="ltx_text" style="font-size:80%;">WMA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.15.9.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.T2.6.15.9.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.15.9.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.15.9.3.1" class="ltx_text" style="font-size:80%;">81.40</span></td>
<td id="S3.T2.6.15.9.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.15.9.4.1" class="ltx_text" style="font-size:80%;">81.20</span></td>
<td id="S3.T2.6.15.9.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T2.6.15.9.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.15.9.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_tt" rowspan="4"><span id="S3.T2.6.15.9.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SAVEE</span></td>
<td id="S3.T2.6.15.9.7" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.6.15.9.7.1" class="ltx_text" style="font-size:80%;">Dual-TBNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.15.9.7.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T2.6.15.9.7.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.15.9.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.15.9.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">83.30</span></td>
<td id="S3.T2.6.15.9.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.6.15.9.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.6.15.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.6.15.9.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">82.10</span></td>
</tr>
<tr id="S3.T2.6.16.10" class="ltx_tr">
<td id="S3.T2.6.16.10.1" class="ltx_td ltx_align_left">
<span id="S3.T2.6.16.10.1.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.16.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.16.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.16.10.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.16.10.2.1" class="ltx_text" style="font-size:80%;">88.00</span></td>
<td id="S3.T2.6.16.10.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.16.10.3.1" class="ltx_text" style="font-size:80%;">89.30</span></td>
<td id="S3.T2.6.16.10.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.16.10.4.1" class="ltx_text" style="font-size:80%;">88.10</span></td>
<td id="S3.T2.6.16.10.5" class="ltx_td ltx_align_left">
<span id="S3.T2.6.16.10.5.1" class="ltx_text" style="font-size:80%;">TIM-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.6.16.10.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T2.6.16.10.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.6.16.10.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.16.10.6.1" class="ltx_text" style="font-size:80%;">80.09</span></td>
<td id="S3.T2.6.16.10.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.16.10.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">81.19</span></td>
<td id="S3.T2.6.16.10.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.16.10.8.1" class="ltx_text" style="font-size:80%;">78.20</span></td>
</tr>
<tr id="S3.T2.6.17.11" class="ltx_tr">
<td id="S3.T2.6.17.11.1" class="ltx_td ltx_align_left"><span id="S3.T2.6.17.11.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.17.11.2" class="ltx_td ltx_align_center"><span id="S3.T2.6.17.11.2.1" class="ltx_text" style="font-size:80%;">85.29</span></td>
<td id="S3.T2.6.17.11.3" class="ltx_td ltx_align_center"><span id="S3.T2.6.17.11.3.1" class="ltx_text" style="font-size:80%;">84.30</span></td>
<td id="S3.T2.6.17.11.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T2.6.17.11.4.1" class="ltx_text" style="font-size:80%;">84.40</span></td>
<td id="S3.T2.6.17.11.5" class="ltx_td ltx_align_left"><span id="S3.T2.6.17.11.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::BT</span></td>
<td id="S3.T2.6.17.11.6" class="ltx_td ltx_align_center"><span id="S3.T2.6.17.11.6.1" class="ltx_text" style="font-size:80%;">78.47</span></td>
<td id="S3.T2.6.17.11.7" class="ltx_td ltx_align_center"><span id="S3.T2.6.17.11.7.1" class="ltx_text" style="font-size:80%;">77.49</span></td>
<td id="S3.T2.6.17.11.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.6.17.11.8.1" class="ltx_text" style="font-size:80%;">77.90</span></td>
</tr>
<tr id="S3.T2.6.18.12" class="ltx_tr">
<td id="S3.T2.6.18.12.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T2.6.18.12.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.18.12.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.6.18.12.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.60</span></td>
<td id="S3.T2.6.18.12.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.6.18.12.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90.97</span></td>
<td id="S3.T2.6.18.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr"><span id="S3.T2.6.18.12.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.02</span></td>
<td id="S3.T2.6.18.12.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T2.6.18.12.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net::300</span></td>
<td id="S3.T2.6.18.12.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.6.18.12.6.1" class="ltx_text" style="font-size:80%;">81.21</span></td>
<td id="S3.T2.6.18.12.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.6.18.12.7.1" class="ltx_text" style="font-size:80%;">80.41</span></td>
<td id="S3.T2.6.18.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.6.18.12.8.1" class="ltx_text" style="font-size:80%;">80.60</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.23.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>TBDM-Net’s evaluation. The results are based on the average performance over a 10-fold cross validation report on the unweighted average recall (<span id="S3.T2.24.2" class="ltx_text ltx_font_bold">UAR</span>), weighted average recall (<span id="S3.T2.25.3" class="ltx_text ltx_font_bold">WAR</span>), and weighted <span id="S3.T2.26.4" class="ltx_text ltx_font_bold">F1</span>-score. <span id="S3.T2.27.5" class="ltx_text ltx_font_typewriter">TBDM-Net-BT</span> is the best model in terms of training set <math id="S3.T2.8.m1.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.T2.8.m1.1b"><mrow id="S3.T2.8.m1.1.1" xref="S3.T2.8.m1.1.1.cmml"><mi id="S3.T2.8.m1.1.1.2" xref="S3.T2.8.m1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.T2.8.m1.1.1.1" xref="S3.T2.8.m1.1.1.1.cmml">​</mo><mi id="S3.T2.8.m1.1.1.3" xref="S3.T2.8.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.T2.8.m1.1.1.1b" xref="S3.T2.8.m1.1.1.1.cmml">​</mo><mi id="S3.T2.8.m1.1.1.4" xref="S3.T2.8.m1.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.8.m1.1c"><apply id="S3.T2.8.m1.1.1.cmml" xref="S3.T2.8.m1.1.1"><times id="S3.T2.8.m1.1.1.1.cmml" xref="S3.T2.8.m1.1.1.1"></times><ci id="S3.T2.8.m1.1.1.2.cmml" xref="S3.T2.8.m1.1.1.2">𝑊</ci><ci id="S3.T2.8.m1.1.1.3.cmml" xref="S3.T2.8.m1.1.1.3">𝐴</ci><ci id="S3.T2.8.m1.1.1.4.cmml" xref="S3.T2.8.m1.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.m1.1d">WAR</annotation></semantics></math>, while <span id="S3.T2.28.6" class="ltx_text ltx_font_typewriter">TBDM-Net-300</span> is the model obtained at the end of the 300 epoch training process. Best results are marked in boldface. </figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Speech datasets and features</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">For the evaluation
we used six standard speech emotion recognition datasets
An overview of these datasets is shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We note that for the IEMOCAP dataset, due to the high class imbalance in the original dataset, we select only 4 classes, and relabel the <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">excited</em> class into <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">happy</em>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Common approach for the IEMOCAP dataset in other studies.</span></span></span> The final subset contains 5531 speech samples.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">From Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> it can be noticed that there is no complete overlap between all sets of emotions annotated or rendered within the speech corpora. There is also a high variation among the amount of speech data, as well as the number and gender of the speakers.
One other important characteristic of this dataset selection is its multilingual aspect: IEMOCAP, RAVDESS and SAVEE are English datasets, CASIA is Chinese, EMOVO is Italian, and EMODB is German. All these features of the data selection enable us to perform a thorough analysis of the proposed architecture across several dimensions of speech and emotion variation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we use 39 Mel frequency cepstral coefficient (MFCC) representations extracted with the default settings in the Librosa module.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://librosa.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://librosa.org/</a></span></span></span> A fixed number of frames, different across the datasets, are used to represent the utterances.
Shorter utterances are zero-padded left and right, while longer utterances are cropped to a central segment.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Feature sets are available from the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>: <a target="_blank" href="https://github.com/Jiaxin-Ye/TIM-Net_SER" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Jiaxin-Ye/TIM-Net_SER</a></span></span></span> The maximum temporal dimension of the MFCC representations is between 172 frames for the CASIA dataset, and 606 frames for the IEMOCAP dataset.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Objective measures and training procedure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Because of the varying sample sizes and emotions within each dataset, we opted to assess the performance of our network independently on each speech dataset through a 10-fold cross-validation process. The folds are chosen randomly, without any dependency on speaker or emotion.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.8" class="ltx_p">As objective measures, we used the <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">unweighted average recall (<math id="S3.SS2.p2.1.1.m1.1" class="ltx_Math" alttext="UAR" display="inline"><semantics id="S3.SS2.p2.1.1.m1.1a"><mrow id="S3.SS2.p2.1.1.m1.1.1" xref="S3.SS2.p2.1.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.1.m1.1.1.2" xref="S3.SS2.p2.1.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.1.m1.1.1.1" xref="S3.SS2.p2.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.1.1.m1.1.1.3" xref="S3.SS2.p2.1.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.1.m1.1.1.1a" xref="S3.SS2.p2.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.1.1.m1.1.1.4" xref="S3.SS2.p2.1.1.m1.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.1.m1.1b"><apply id="S3.SS2.p2.1.1.m1.1.1.cmml" xref="S3.SS2.p2.1.1.m1.1.1"><times id="S3.SS2.p2.1.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.1.m1.1.1.2">𝑈</ci><ci id="S3.SS2.p2.1.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.1.m1.1.1.3">𝐴</ci><ci id="S3.SS2.p2.1.1.m1.1.1.4.cmml" xref="S3.SS2.p2.1.1.m1.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.1.m1.1c">UAR</annotation></semantics></math>)</span>, <span id="S3.SS2.p2.2.2" class="ltx_text ltx_font_bold">weighted average recall (<math id="S3.SS2.p2.2.2.m1.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.SS2.p2.2.2.m1.1a"><mrow id="S3.SS2.p2.2.2.m1.1.1" xref="S3.SS2.p2.2.2.m1.1.1.cmml"><mi id="S3.SS2.p2.2.2.m1.1.1.2" xref="S3.SS2.p2.2.2.m1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.2.m1.1.1.1" xref="S3.SS2.p2.2.2.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.2.2.m1.1.1.3" xref="S3.SS2.p2.2.2.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.2.m1.1.1.1a" xref="S3.SS2.p2.2.2.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.2.2.m1.1.1.4" xref="S3.SS2.p2.2.2.m1.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.2.m1.1b"><apply id="S3.SS2.p2.2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.2.m1.1.1"><times id="S3.SS2.p2.2.2.m1.1.1.1.cmml" xref="S3.SS2.p2.2.2.m1.1.1.1"></times><ci id="S3.SS2.p2.2.2.m1.1.1.2.cmml" xref="S3.SS2.p2.2.2.m1.1.1.2">𝑊</ci><ci id="S3.SS2.p2.2.2.m1.1.1.3.cmml" xref="S3.SS2.p2.2.2.m1.1.1.3">𝐴</ci><ci id="S3.SS2.p2.2.2.m1.1.1.4.cmml" xref="S3.SS2.p2.2.2.m1.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.2.m1.1c">WAR</annotation></semantics></math>)</span> and <span id="S3.SS2.p2.8.3" class="ltx_text ltx_font_bold">F1-score</span> (<math id="S3.SS2.p2.3.m1.1" class="ltx_Math" alttext="F1" display="inline"><semantics id="S3.SS2.p2.3.m1.1a"><mrow id="S3.SS2.p2.3.m1.1.1" xref="S3.SS2.p2.3.m1.1.1.cmml"><mi id="S3.SS2.p2.3.m1.1.1.2" xref="S3.SS2.p2.3.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m1.1.1.1" xref="S3.SS2.p2.3.m1.1.1.1.cmml">​</mo><mn id="S3.SS2.p2.3.m1.1.1.3" xref="S3.SS2.p2.3.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m1.1b"><apply id="S3.SS2.p2.3.m1.1.1.cmml" xref="S3.SS2.p2.3.m1.1.1"><times id="S3.SS2.p2.3.m1.1.1.1.cmml" xref="S3.SS2.p2.3.m1.1.1.1"></times><ci id="S3.SS2.p2.3.m1.1.1.2.cmml" xref="S3.SS2.p2.3.m1.1.1.2">𝐹</ci><cn type="integer" id="S3.SS2.p2.3.m1.1.1.3.cmml" xref="S3.SS2.p2.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m1.1c">F1</annotation></semantics></math>).
In standard implementations of multi-class classification, the <math id="S3.SS2.p2.4.m2.1" class="ltx_Math" alttext="UAR" display="inline"><semantics id="S3.SS2.p2.4.m2.1a"><mrow id="S3.SS2.p2.4.m2.1.1" xref="S3.SS2.p2.4.m2.1.1.cmml"><mi id="S3.SS2.p2.4.m2.1.1.2" xref="S3.SS2.p2.4.m2.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m2.1.1.1" xref="S3.SS2.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.4.m2.1.1.3" xref="S3.SS2.p2.4.m2.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m2.1.1.1a" xref="S3.SS2.p2.4.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.4.m2.1.1.4" xref="S3.SS2.p2.4.m2.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m2.1b"><apply id="S3.SS2.p2.4.m2.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1"><times id="S3.SS2.p2.4.m2.1.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1.1"></times><ci id="S3.SS2.p2.4.m2.1.1.2.cmml" xref="S3.SS2.p2.4.m2.1.1.2">𝑈</ci><ci id="S3.SS2.p2.4.m2.1.1.3.cmml" xref="S3.SS2.p2.4.m2.1.1.3">𝐴</ci><ci id="S3.SS2.p2.4.m2.1.1.4.cmml" xref="S3.SS2.p2.4.m2.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m2.1c">UAR</annotation></semantics></math> is equal to the global prediction accuracy.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://www.evidentlyai.com/classification-metrics/multi-class-metrics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.evidentlyai.com/classification-metrics/multi-class-metrics</a></span></span></span> The <math id="S3.SS2.p2.5.m3.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.SS2.p2.5.m3.1a"><mrow id="S3.SS2.p2.5.m3.1.1" xref="S3.SS2.p2.5.m3.1.1.cmml"><mi id="S3.SS2.p2.5.m3.1.1.2" xref="S3.SS2.p2.5.m3.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m3.1.1.1" xref="S3.SS2.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.5.m3.1.1.3" xref="S3.SS2.p2.5.m3.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m3.1.1.1a" xref="S3.SS2.p2.5.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.5.m3.1.1.4" xref="S3.SS2.p2.5.m3.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m3.1b"><apply id="S3.SS2.p2.5.m3.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1"><times id="S3.SS2.p2.5.m3.1.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1.1"></times><ci id="S3.SS2.p2.5.m3.1.1.2.cmml" xref="S3.SS2.p2.5.m3.1.1.2">𝑊</ci><ci id="S3.SS2.p2.5.m3.1.1.3.cmml" xref="S3.SS2.p2.5.m3.1.1.3">𝐴</ci><ci id="S3.SS2.p2.5.m3.1.1.4.cmml" xref="S3.SS2.p2.5.m3.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m3.1c">WAR</annotation></semantics></math> is a weighted measure which takes into account the number of samples from each class within the test set. <math id="S3.SS2.p2.6.m4.1" class="ltx_Math" alttext="UAR" display="inline"><semantics id="S3.SS2.p2.6.m4.1a"><mrow id="S3.SS2.p2.6.m4.1.1" xref="S3.SS2.p2.6.m4.1.1.cmml"><mi id="S3.SS2.p2.6.m4.1.1.2" xref="S3.SS2.p2.6.m4.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m4.1.1.1" xref="S3.SS2.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.6.m4.1.1.3" xref="S3.SS2.p2.6.m4.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m4.1.1.1a" xref="S3.SS2.p2.6.m4.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.6.m4.1.1.4" xref="S3.SS2.p2.6.m4.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m4.1b"><apply id="S3.SS2.p2.6.m4.1.1.cmml" xref="S3.SS2.p2.6.m4.1.1"><times id="S3.SS2.p2.6.m4.1.1.1.cmml" xref="S3.SS2.p2.6.m4.1.1.1"></times><ci id="S3.SS2.p2.6.m4.1.1.2.cmml" xref="S3.SS2.p2.6.m4.1.1.2">𝑈</ci><ci id="S3.SS2.p2.6.m4.1.1.3.cmml" xref="S3.SS2.p2.6.m4.1.1.3">𝐴</ci><ci id="S3.SS2.p2.6.m4.1.1.4.cmml" xref="S3.SS2.p2.6.m4.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m4.1c">UAR</annotation></semantics></math>, <math id="S3.SS2.p2.7.m5.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.SS2.p2.7.m5.1a"><mrow id="S3.SS2.p2.7.m5.1.1" xref="S3.SS2.p2.7.m5.1.1.cmml"><mi id="S3.SS2.p2.7.m5.1.1.2" xref="S3.SS2.p2.7.m5.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m5.1.1.1" xref="S3.SS2.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.7.m5.1.1.3" xref="S3.SS2.p2.7.m5.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m5.1.1.1a" xref="S3.SS2.p2.7.m5.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.7.m5.1.1.4" xref="S3.SS2.p2.7.m5.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m5.1b"><apply id="S3.SS2.p2.7.m5.1.1.cmml" xref="S3.SS2.p2.7.m5.1.1"><times id="S3.SS2.p2.7.m5.1.1.1.cmml" xref="S3.SS2.p2.7.m5.1.1.1"></times><ci id="S3.SS2.p2.7.m5.1.1.2.cmml" xref="S3.SS2.p2.7.m5.1.1.2">𝑊</ci><ci id="S3.SS2.p2.7.m5.1.1.3.cmml" xref="S3.SS2.p2.7.m5.1.1.3">𝐴</ci><ci id="S3.SS2.p2.7.m5.1.1.4.cmml" xref="S3.SS2.p2.7.m5.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m5.1c">WAR</annotation></semantics></math> and <math id="S3.SS2.p2.8.m6.1" class="ltx_Math" alttext="F1-score" display="inline"><semantics id="S3.SS2.p2.8.m6.1a"><mrow id="S3.SS2.p2.8.m6.1.1" xref="S3.SS2.p2.8.m6.1.1.cmml"><mrow id="S3.SS2.p2.8.m6.1.1.2" xref="S3.SS2.p2.8.m6.1.1.2.cmml"><mi id="S3.SS2.p2.8.m6.1.1.2.2" xref="S3.SS2.p2.8.m6.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m6.1.1.2.1" xref="S3.SS2.p2.8.m6.1.1.2.1.cmml">​</mo><mn id="S3.SS2.p2.8.m6.1.1.2.3" xref="S3.SS2.p2.8.m6.1.1.2.3.cmml">1</mn></mrow><mo id="S3.SS2.p2.8.m6.1.1.1" xref="S3.SS2.p2.8.m6.1.1.1.cmml">−</mo><mrow id="S3.SS2.p2.8.m6.1.1.3" xref="S3.SS2.p2.8.m6.1.1.3.cmml"><mi id="S3.SS2.p2.8.m6.1.1.3.2" xref="S3.SS2.p2.8.m6.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m6.1.1.3.1" xref="S3.SS2.p2.8.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.8.m6.1.1.3.3" xref="S3.SS2.p2.8.m6.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m6.1.1.3.1a" xref="S3.SS2.p2.8.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.8.m6.1.1.3.4" xref="S3.SS2.p2.8.m6.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m6.1.1.3.1b" xref="S3.SS2.p2.8.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.8.m6.1.1.3.5" xref="S3.SS2.p2.8.m6.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m6.1.1.3.1c" xref="S3.SS2.p2.8.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p2.8.m6.1.1.3.6" xref="S3.SS2.p2.8.m6.1.1.3.6.cmml">e</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m6.1b"><apply id="S3.SS2.p2.8.m6.1.1.cmml" xref="S3.SS2.p2.8.m6.1.1"><minus id="S3.SS2.p2.8.m6.1.1.1.cmml" xref="S3.SS2.p2.8.m6.1.1.1"></minus><apply id="S3.SS2.p2.8.m6.1.1.2.cmml" xref="S3.SS2.p2.8.m6.1.1.2"><times id="S3.SS2.p2.8.m6.1.1.2.1.cmml" xref="S3.SS2.p2.8.m6.1.1.2.1"></times><ci id="S3.SS2.p2.8.m6.1.1.2.2.cmml" xref="S3.SS2.p2.8.m6.1.1.2.2">𝐹</ci><cn type="integer" id="S3.SS2.p2.8.m6.1.1.2.3.cmml" xref="S3.SS2.p2.8.m6.1.1.2.3">1</cn></apply><apply id="S3.SS2.p2.8.m6.1.1.3.cmml" xref="S3.SS2.p2.8.m6.1.1.3"><times id="S3.SS2.p2.8.m6.1.1.3.1.cmml" xref="S3.SS2.p2.8.m6.1.1.3.1"></times><ci id="S3.SS2.p2.8.m6.1.1.3.2.cmml" xref="S3.SS2.p2.8.m6.1.1.3.2">𝑠</ci><ci id="S3.SS2.p2.8.m6.1.1.3.3.cmml" xref="S3.SS2.p2.8.m6.1.1.3.3">𝑐</ci><ci id="S3.SS2.p2.8.m6.1.1.3.4.cmml" xref="S3.SS2.p2.8.m6.1.1.3.4">𝑜</ci><ci id="S3.SS2.p2.8.m6.1.1.3.5.cmml" xref="S3.SS2.p2.8.m6.1.1.3.5">𝑟</ci><ci id="S3.SS2.p2.8.m6.1.1.3.6.cmml" xref="S3.SS2.p2.8.m6.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m6.1c">F1-score</annotation></semantics></math> enable us to directly compare our results against the previously published methods.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.3" class="ltx_p">The <span id="S3.SS2.p3.3.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span>’s architecture uses 6 temporal blocks for all speech datasets.
The dilation rates are: 1, 2, 4, 8, 16, and 32, respectively. The number of filters for each convolution is equal to the number of MFCC coefficients (i.e. 39), and use a kernel size of 2.
The models were trained for 300 epochs, using an ADAM optimiser with a learning rate of <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="1e-3" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mrow id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.2.1" xref="S3.SS2.p3.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">−</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><minus id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></minus><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><times id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">1</cn><ci id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">1e-3</annotation></semantics></math>, and <math id="S3.SS2.p3.2.m2.2" class="ltx_Math" alttext="betas=(0.93,0.98)" display="inline"><semantics id="S3.SS2.p3.2.m2.2a"><mrow id="S3.SS2.p3.2.m2.2.3" xref="S3.SS2.p3.2.m2.2.3.cmml"><mrow id="S3.SS2.p3.2.m2.2.3.2" xref="S3.SS2.p3.2.m2.2.3.2.cmml"><mi id="S3.SS2.p3.2.m2.2.3.2.2" xref="S3.SS2.p3.2.m2.2.3.2.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.3.2.1" xref="S3.SS2.p3.2.m2.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.3.2.3" xref="S3.SS2.p3.2.m2.2.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.3.2.1a" xref="S3.SS2.p3.2.m2.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.3.2.4" xref="S3.SS2.p3.2.m2.2.3.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.3.2.1b" xref="S3.SS2.p3.2.m2.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.3.2.5" xref="S3.SS2.p3.2.m2.2.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.3.2.1c" xref="S3.SS2.p3.2.m2.2.3.2.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.3.2.6" xref="S3.SS2.p3.2.m2.2.3.2.6.cmml">s</mi></mrow><mo id="S3.SS2.p3.2.m2.2.3.1" xref="S3.SS2.p3.2.m2.2.3.1.cmml">=</mo><mrow id="S3.SS2.p3.2.m2.2.3.3.2" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.2.3.3.2.1" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml">(</mo><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">0.93</mn><mo id="S3.SS2.p3.2.m2.2.3.3.2.2" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p3.2.m2.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml">0.98</mn><mo stretchy="false" id="S3.SS2.p3.2.m2.2.3.3.2.3" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.2b"><apply id="S3.SS2.p3.2.m2.2.3.cmml" xref="S3.SS2.p3.2.m2.2.3"><eq id="S3.SS2.p3.2.m2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.2.3.1"></eq><apply id="S3.SS2.p3.2.m2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.2.3.2"><times id="S3.SS2.p3.2.m2.2.3.2.1.cmml" xref="S3.SS2.p3.2.m2.2.3.2.1"></times><ci id="S3.SS2.p3.2.m2.2.3.2.2.cmml" xref="S3.SS2.p3.2.m2.2.3.2.2">𝑏</ci><ci id="S3.SS2.p3.2.m2.2.3.2.3.cmml" xref="S3.SS2.p3.2.m2.2.3.2.3">𝑒</ci><ci id="S3.SS2.p3.2.m2.2.3.2.4.cmml" xref="S3.SS2.p3.2.m2.2.3.2.4">𝑡</ci><ci id="S3.SS2.p3.2.m2.2.3.2.5.cmml" xref="S3.SS2.p3.2.m2.2.3.2.5">𝑎</ci><ci id="S3.SS2.p3.2.m2.2.3.2.6.cmml" xref="S3.SS2.p3.2.m2.2.3.2.6">𝑠</ci></apply><interval closure="open" id="S3.SS2.p3.2.m2.2.3.3.1.cmml" xref="S3.SS2.p3.2.m2.2.3.3.2"><cn type="float" id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">0.93</cn><cn type="float" id="S3.SS2.p3.2.m2.2.2.cmml" xref="S3.SS2.p3.2.m2.2.2">0.98</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.2c">betas=(0.93,0.98)</annotation></semantics></math>. The batch size was set to 64. No early stopping or learning rate scheduler were used.
The best model across the 300 epochs in terms of training set <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.1a" xref="S3.SS2.p3.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.3.m3.1.1.4" xref="S3.SS2.p3.3.m3.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></times><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑊</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝐴</ci><ci id="S3.SS2.p3.3.m3.1.1.4.cmml" xref="S3.SS2.p3.3.m3.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">WAR</annotation></semantics></math> was saved, and used to provide the intermediate evaluation results for the respective fold. A separate result is extracted from the model obtained after 300 epochs of training.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>No major accuracy improvements were obtained beyond the 300 epoch training step.</span></span></span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Baseline results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The results of <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span>’s evaluation are shown in Table <a href="#S3.T2" title="Table 2 ‣ 3 Evaluation ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The table includes the performance of <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">TBDM-Net</span> as evaluated in two scenarios: (i) the best model in terms of training subset <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_typewriter">WAR</span> (<span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_typewriter">TBDM-Net::BT</span>); and (ii) the model obtained after 300 epochs of training (<span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_typewriter">TBDM-Net::300</span>). The table also introduces the recomputed results for the <span id="S3.SS3.p1.1.6" class="ltx_text ltx_font_typewriter">TIM-Net</span> architecture using the authors’ official implementation.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>These results differ from the ones introduced in the original paper, as the author’s initial evaluation made use of the test set data for the best model selection.</span></span></span> The same set of input features and number of training epochs as for <span id="S3.SS3.p1.1.7" class="ltx_text ltx_font_typewriter">TBDM-Net</span> were used.
We also present the top performance measures for each dataset as reported by previously published peer-reviewed methods.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Observations reveal that <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span> demonstrates heightened performance across nearly all assessed datasets. We posit that facilitating connectivity between the Temporal Attention Blocks (TABs) at varying resolutions and incorporating bidirectional paths enhances the capture of emotional abstraction. However, exceptions arise with the CASIA and SAVEE datasets, where the Dual-TBNet architecture distinctly outperforms. Further analysis is necessary to ascertain the reasons behind the diminished accuracy of <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">TBDM-Net</span> on these datasets.
A notable increase in performance is obtained for the EMOVO, RAVDESS and IEMOCAP datasets in terms of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="UAR" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1a" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.4" xref="S3.SS3.p2.1.m1.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑈</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝐴</ci><ci id="S3.SS3.p2.1.m1.1.1.4.cmml" xref="S3.SS3.p2.1.m1.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">UAR</annotation></semantics></math>–with over 3% absolute recall increase. It is important to notice that IEMOCAP is the largest dataset, and the one which poses most challenges across all published studies in SER. These results are encouraging, as we are planning to use the proposed architecture in real-life scenarios where large volumes of data and less well-defined emotion elicitation would be found.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.1.1" class="ltx_tr">
<th id="S3.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_t"></th>
<th id="S3.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ReLU</span></th>
<th id="S3.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">w/o BD</span></th>
<th id="S3.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">w/o MS</span></th>
<th id="S3.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.2.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5 TABs</span></th>
<th id="S3.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.2.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TBDM-Net</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.1" class="ltx_tr">
<th id="S3.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_t"><span id="S3.T3.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UAR</span></th>
<td id="S3.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.2.2.1.2.1" class="ltx_text" style="font-size:80%;">90.76</span></td>
<td id="S3.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.2.2.1.3.1" class="ltx_text" style="font-size:80%;">90.09</span></td>
<td id="S3.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.2.2.1.4.1" class="ltx_text" style="font-size:80%;">90.81</span></td>
<td id="S3.T3.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.2.1.5.1" class="ltx_text" style="font-size:80%;">91.08</span></td>
<td id="S3.T3.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.60</span></td>
</tr>
<tr id="S3.T3.2.3.2" class="ltx_tr">
<th id="S3.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l"><span id="S3.T3.2.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WAR</span></th>
<td id="S3.T3.2.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T3.2.3.2.2.1" class="ltx_text" style="font-size:80%;">90.34</span></td>
<td id="S3.T3.2.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.2.3.2.3.1" class="ltx_text" style="font-size:80%;">89.99</span></td>
<td id="S3.T3.2.3.2.4" class="ltx_td ltx_align_center"><span id="S3.T3.2.3.2.4.1" class="ltx_text" style="font-size:80%;">90.69</span></td>
<td id="S3.T3.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.2.3.2.5.1" class="ltx_text" style="font-size:80%;">90.90</span></td>
<td id="S3.T3.2.3.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.2.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90.97</span></td>
</tr>
<tr id="S3.T3.2.4.3" class="ltx_tr">
<th id="S3.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l"><span id="S3.T3.2.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1</span></th>
<td id="S3.T3.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.2.4.3.2.1" class="ltx_text" style="font-size:80%;">90.42</span></td>
<td id="S3.T3.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.2.4.3.3.1" class="ltx_text" style="font-size:80%;">90.06</span></td>
<td id="S3.T3.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T3.2.4.3.4.1" class="ltx_text" style="font-size:80%;">90.80</span></td>
<td id="S3.T3.2.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T3.2.4.3.5.1" class="ltx_text" style="font-size:80%;">90.84</span></td>
<td id="S3.T3.2.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T3.2.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.02</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.7.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Ablation study on the <span id="S3.T3.8.2" class="ltx_text ltx_font_bold">RAVDESS</span> dataset. The results are based on a 10-fold cross validation of the model trained for 300 epochs. Best results are marked in boldface.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Ablation study</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We also introduce the results of an ablation study over the RAVDESS dataset. Similar results were found for the other datasets, as well. The study includes the following modifications made to <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span>, with all the other hyperparameters of the architecture and training step having been frozen:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">activation function (<span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">ReLU</span>) - we use the ReLU activation function in the TABs instead of GELU;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">directionality (<span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">w/o BD</span>) - we compare the bidirectional(BD) network with a forward time-only network variation;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">multi-scale (<span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">w/o MS</span>) - our proposed model uses multi-scale (MS) fusion of the intermediary states between TABs. We compare this with a variation which only uses the last state, disregarding the others;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">number of TABs (<span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">5 TABs</span>) - the proposed model uses 6 TABs, and we compare it with a variation containing only 5 TABs.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.2" class="ltx_p">The results are shown in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Baseline results ‣ 3 Evaluation ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
It can be noticed that in terms of both <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="UAR" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1a" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.1.m1.1.1.4" xref="S3.SS4.p3.1.m1.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><times id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></times><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑈</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝐴</ci><ci id="S3.SS4.p3.1.m1.1.1.4.cmml" xref="S3.SS4.p3.1.m1.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">UAR</annotation></semantics></math> and <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="WAR" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mrow id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.2.m2.1.1.1a" xref="S3.SS4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.2.m2.1.1.4" xref="S3.SS4.p3.2.m2.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><times id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1"></times><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝑊</ci><ci id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">𝐴</ci><ci id="S3.SS4.p3.2.m2.1.1.4.cmml" xref="S3.SS4.p3.2.m2.1.1.4">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">WAR</annotation></semantics></math>, all modifications to the network yield performance increments, with the number of TABs having the least impact over the final results. This is encouraging especially if the network needs to be optimised for real-time applications.
Most improvement is obtained from the use of bidirectional modules.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Gender-informed results</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Given that previous literature strongly motivates the use of gender-differentiated systems in SER-related tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we perform a similar evaluation for the <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span> architecture. We also examine how real-life applications would perform the task when the speaker gender is also estimated from the input speech.
Therefore, we first build a gender classifier based on large pretrained models’ derived speech embeddings. Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> over such embeddings showed that the TitaNet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> exhibits a high correlation between its embeddings and the speaker’s gender. The TitaNet-L<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/titanet_large" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/titanet_large</a></span></span></span> pretrained model was selected and the corresponding embeddings extracted. To ensure a good classification performance, several classifiers were trained on the development partition of the VOXCELEB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and tested on the VOXCELEB test partition, as well as the RAVDESS dataset. The results are shown in Table <a href="#S3.T4" title="Table 4 ‣ 3.5 Gender-informed results ‣ 3 Evaluation ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The SVC-based gender classifier’s prediction in binary or probabilistic formats was used in the follow-up evaluation.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">
<span id="S3.T4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Accuracy [%]</span><span id="S3.T4.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="S3.T4.1.2.1" class="ltx_tr">
<th id="S3.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T4.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Classifier</span></th>
<th id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S3.T4.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VOXCELEB</span></th>
<th id="S3.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S3.T4.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RAVDESS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.3.1" class="ltx_tr">
<th id="S3.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T4.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Logistic Regression</span></th>
<td id="S3.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.3.1.2.1" class="ltx_text" style="font-size:80%;">93.45</span></td>
<td id="S3.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.3.1.3.1" class="ltx_text" style="font-size:80%;">81.56</span></td>
</tr>
<tr id="S3.T4.1.4.2" class="ltx_tr">
<th id="S3.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T4.1.4.2.1.1" class="ltx_text" style="font-size:80%;">MLP Classifier</span></th>
<td id="S3.T4.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.4.2.2.1" class="ltx_text" style="font-size:80%;">99.83</span></td>
<td id="S3.T4.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.4.2.3.1" class="ltx_text" style="font-size:80%;">82.70</span></td>
</tr>
<tr id="S3.T4.1.5.3" class="ltx_tr">
<th id="S3.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T4.1.5.3.1.1" class="ltx_text" style="font-size:80%;">Random Forest</span></th>
<td id="S3.T4.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.5.3.2.1" class="ltx_text" style="font-size:80%;">98.85</span></td>
<td id="S3.T4.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.5.3.3.1" class="ltx_text" style="font-size:80%;">87.32</span></td>
</tr>
<tr id="S3.T4.1.6.4" class="ltx_tr">
<th id="S3.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T4.1.6.4.1.1" class="ltx_text" style="font-size:80%;">XGBoost</span></th>
<td id="S3.T4.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.6.4.2.1" class="ltx_text" style="font-size:80%;">98.15</span></td>
<td id="S3.T4.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.6.4.3.1" class="ltx_text" style="font-size:80%;">90.90</span></td>
</tr>
<tr id="S3.T4.1.7.5" class="ltx_tr">
<th id="S3.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T4.1.7.5.1.1" class="ltx_text" style="font-size:80%;">Support Vector Classifier</span></th>
<td id="S3.T4.1.7.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T4.1.7.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">99.97</span></td>
<td id="S3.T4.1.7.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T4.1.7.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.26</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.11.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Gender classifier accuracy evaluation. The classifiers were trained on the <em id="S3.T4.12.2" class="ltx_emph ltx_font_italic">VOXCELEB-dev</em> dataset and evaluated on the <em id="S3.T4.13.3" class="ltx_emph ltx_font_italic">VOXCELEB-test</em> and <em id="S3.T4.14.4" class="ltx_emph ltx_font_italic">RAVDESS</em> datasets. Best results are marked in boldface.</figcaption>
</figure>
<figure id="S3.T5" class="ltx_table">
<table id="S3.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.3.3" class="ltx_tr">
<th id="S3.T5.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<th id="S3.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UAR<math id="S3.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T5.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.m1.1b"><ci id="S3.T5.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T5.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WAR<math id="S3.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T5.2.2.2.1.m1.1.1" xref="S3.T5.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.1.m1.1b"><ci id="S3.T5.2.2.2.1.m1.1.1.cmml" xref="S3.T5.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T5.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1<math id="S3.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T5.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T5.3.3.3.1.m1.1.1" xref="S3.T5.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.3.3.3.1.m1.1b"><ci id="S3.T5.3.3.3.1.m1.1.1.cmml" xref="S3.T5.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr id="S3.T5.3.4.1" class="ltx_tr">
<th id="S3.T5.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T5.3.4.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:full</span></th>
<th id="S3.T5.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T5.3.4.1.2.1" class="ltx_text" style="font-size:80%;">91.60</span></th>
<th id="S3.T5.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T5.3.4.1.3.1" class="ltx_text" style="font-size:80%;">90.97</span></th>
<th id="S3.T5.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T5.3.4.1.4.1" class="ltx_text" style="font-size:80%;">91.02</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.3.5.1" class="ltx_tr">
<th id="S3.T5.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.5.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:M</span></th>
<td id="S3.T5.3.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.5.1.2.1" class="ltx_text" style="font-size:80%;">87.02</span></td>
<td id="S3.T5.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.5.1.3.1" class="ltx_text" style="font-size:80%;">85.00</span></td>
<td id="S3.T5.3.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.5.1.4.1" class="ltx_text" style="font-size:80%;">84.91</span></td>
</tr>
<tr id="S3.T5.3.6.2" class="ltx_tr">
<th id="S3.T5.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T5.3.6.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:F</span></th>
<td id="S3.T5.3.6.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.6.2.2.1" class="ltx_text" style="font-size:80%;">91.23</span></td>
<td id="S3.T5.3.6.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.6.2.3.1" class="ltx_text" style="font-size:80%;">90.13</span></td>
<td id="S3.T5.3.6.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.6.2.4.1" class="ltx_text" style="font-size:80%;">90.31</span></td>
</tr>
<tr id="S3.T5.3.7.3" class="ltx_tr">
<th id="S3.T5.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T5.3.7.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">split:M</span></th>
<td id="S3.T5.3.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.7.3.2.1" class="ltx_text" style="font-size:80%;">90.86</span></td>
<td id="S3.T5.3.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.7.3.3.1" class="ltx_text" style="font-size:80%;">89.44</span></td>
<td id="S3.T5.3.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.7.3.4.1" class="ltx_text" style="font-size:80%;">89.24</span></td>
</tr>
<tr id="S3.T5.3.8.4" class="ltx_tr">
<th id="S3.T5.3.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T5.3.8.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">split:F</span></th>
<td id="S3.T5.3.8.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.8.4.2.1" class="ltx_text" style="font-size:80%;">90.71</span></td>
<td id="S3.T5.3.8.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.8.4.3.1" class="ltx_text" style="font-size:80%;">90.41</span></td>
<td id="S3.T5.3.8.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.8.4.4.1" class="ltx_text" style="font-size:80%;">90.71</span></td>
</tr>
<tr id="S3.T5.3.9.5" class="ltx_tr">
<th id="S3.T5.3.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T5.3.9.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:golden-labels</span></th>
<td id="S3.T5.3.9.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.9.5.2.1" class="ltx_text" style="font-size:80%;">90.75</span></td>
<td id="S3.T5.3.9.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.9.5.3.1" class="ltx_text" style="font-size:80%;">90.41</span></td>
<td id="S3.T5.3.9.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.9.5.4.1" class="ltx_text" style="font-size:80%;">90.44</span></td>
</tr>
<tr id="S3.T5.3.10.6" class="ltx_tr">
<th id="S3.T5.3.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T5.3.10.6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:binary</span></th>
<td id="S3.T5.3.10.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.10.6.2.1" class="ltx_text" style="font-size:80%;">89.83</span></td>
<td id="S3.T5.3.10.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.10.6.3.1" class="ltx_text" style="font-size:80%;">89.37</span></td>
<td id="S3.T5.3.10.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.10.6.4.1" class="ltx_text" style="font-size:80%;">89.38</span></td>
</tr>
<tr id="S3.T5.3.11.7" class="ltx_tr">
<th id="S3.T5.3.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T5.3.11.7.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:probabilities</span></th>
<td id="S3.T5.3.11.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.11.7.2.1" class="ltx_text" style="font-size:80%;">90.04</span></td>
<td id="S3.T5.3.11.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.11.7.3.1" class="ltx_text" style="font-size:80%;">89.51</span></td>
<td id="S3.T5.3.11.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.11.7.4.1" class="ltx_text" style="font-size:80%;">89.50</span></td>
</tr>
<tr id="S3.T5.3.12.8" class="ltx_tr">
<th id="S3.T5.3.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T5.3.12.8.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:golden-labels</span></th>
<td id="S3.T5.3.12.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.12.8.2.1" class="ltx_text" style="font-size:80%;">91.34</span></td>
<td id="S3.T5.3.12.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.12.8.3.1" class="ltx_text" style="font-size:80%;">91.18</span></td>
<td id="S3.T5.3.12.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.3.12.8.4.1" class="ltx_text" style="font-size:80%;">91.22</span></td>
</tr>
<tr id="S3.T5.3.13.9" class="ltx_tr">
<th id="S3.T5.3.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T5.3.13.9.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:binary</span></th>
<td id="S3.T5.3.13.9.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.13.9.2.1" class="ltx_text" style="font-size:80%;">91.07</span></td>
<td id="S3.T5.3.13.9.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.13.9.3.1" class="ltx_text" style="font-size:80%;">90.90</span></td>
<td id="S3.T5.3.13.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T5.3.13.9.4.1" class="ltx_text" style="font-size:80%;">90.88</span></td>
</tr>
<tr id="S3.T5.3.14.10" class="ltx_tr">
<th id="S3.T5.3.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T5.3.14.10.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:probabilities</span></th>
<td id="S3.T5.3.14.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T5.3.14.10.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.70</span></td>
<td id="S3.T5.3.14.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T5.3.14.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.31</span></td>
<td id="S3.T5.3.14.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T5.3.14.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">91.33</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.9.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>Gender-informed TBDM-Net 10-fold cross-validation results on the <span id="S3.T5.10.2" class="ltx_text ltx_font_bold">RAVDESS</span> corpus. Best results are marked in boldface.</figcaption>
</figure>
<figure id="S3.T6" class="ltx_table">
<table id="S3.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.3.3" class="ltx_tr">
<th id="S3.T6.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T6.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<th id="S3.T6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UAR<math id="S3.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T6.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T6.1.1.1.1.m1.1.1" xref="S3.T6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.1.m1.1b"><ci id="S3.T6.1.1.1.1.m1.1.1.cmml" xref="S3.T6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T6.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WAR<math id="S3.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T6.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T6.2.2.2.1.m1.1.1" xref="S3.T6.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.2.2.2.1.m1.1b"><ci id="S3.T6.2.2.2.1.m1.1.1.cmml" xref="S3.T6.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S3.T6.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T6.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1<math id="S3.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T6.3.3.3.1.m1.1a"><mo stretchy="false" id="S3.T6.3.3.3.1.m1.1.1" xref="S3.T6.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.3.3.3.1.m1.1b"><ci id="S3.T6.3.3.3.1.m1.1.1.cmml" xref="S3.T6.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr id="S3.T6.3.4.1" class="ltx_tr">
<th id="S3.T6.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T6.3.4.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:full</span></th>
<th id="S3.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T6.3.4.1.2.1" class="ltx_text" style="font-size:80%;">73.28</span></th>
<th id="S3.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T6.3.4.1.3.1" class="ltx_text" style="font-size:80%;">71.88</span></th>
<th id="S3.T6.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T6.3.4.1.4.1" class="ltx_text" style="font-size:80%;">71.94</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.3.5.1" class="ltx_tr">
<th id="S3.T6.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T6.3.5.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:M</span></th>
<td id="S3.T6.3.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T6.3.5.1.2.1" class="ltx_text" style="font-size:80%;">72.72</span></td>
<td id="S3.T6.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T6.3.5.1.3.1" class="ltx_text" style="font-size:80%;">70.53</span></td>
<td id="S3.T6.3.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T6.3.5.1.4.1" class="ltx_text" style="font-size:80%;">70.75</span></td>
</tr>
<tr id="S3.T6.3.6.2" class="ltx_tr">
<th id="S3.T6.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T6.3.6.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">baseline:F</span></th>
<td id="S3.T6.3.6.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.6.2.2.1" class="ltx_text" style="font-size:80%;">73.73</span></td>
<td id="S3.T6.3.6.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.6.2.3.1" class="ltx_text" style="font-size:80%;">72.61</span></td>
<td id="S3.T6.3.6.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.6.2.4.1" class="ltx_text" style="font-size:80%;">72.59</span></td>
</tr>
<tr id="S3.T6.3.7.3" class="ltx_tr">
<th id="S3.T6.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T6.3.7.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">split:M</span></th>
<td id="S3.T6.3.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.7.3.2.1" class="ltx_text" style="font-size:80%;">72.62</span></td>
<td id="S3.T6.3.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.7.3.3.1" class="ltx_text" style="font-size:80%;">70.05</span></td>
<td id="S3.T6.3.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.7.3.4.1" class="ltx_text" style="font-size:80%;">70.23</span></td>
</tr>
<tr id="S3.T6.3.8.4" class="ltx_tr">
<th id="S3.T6.3.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T6.3.8.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">split:F</span></th>
<td id="S3.T6.3.8.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.8.4.2.1" class="ltx_text" style="font-size:80%;">72.58</span></td>
<td id="S3.T6.3.8.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.8.4.3.1" class="ltx_text" style="font-size:80%;">71.40</span></td>
<td id="S3.T6.3.8.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.8.4.4.1" class="ltx_text" style="font-size:80%;">71.65</span></td>
</tr>
<tr id="S3.T6.3.9.5" class="ltx_tr">
<th id="S3.T6.3.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T6.3.9.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:golden-labels</span></th>
<td id="S3.T6.3.9.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.9.5.2.1" class="ltx_text" style="font-size:80%;">72.03</span></td>
<td id="S3.T6.3.9.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.9.5.3.1" class="ltx_text" style="font-size:80%;">70.72</span></td>
<td id="S3.T6.3.9.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.9.5.4.1" class="ltx_text" style="font-size:80%;">70.88</span></td>
</tr>
<tr id="S3.T6.3.10.6" class="ltx_tr">
<th id="S3.T6.3.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T6.3.10.6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:binary</span></th>
<td id="S3.T6.3.10.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.10.6.2.1" class="ltx_text" style="font-size:80%;">71.45</span></td>
<td id="S3.T6.3.10.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.10.6.3.1" class="ltx_text" style="font-size:80%;">70.18</span></td>
<td id="S3.T6.3.10.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.10.6.4.1" class="ltx_text" style="font-size:80%;">70.33</span></td>
</tr>
<tr id="S3.T6.3.11.7" class="ltx_tr">
<th id="S3.T6.3.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T6.3.11.7.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">post-hoc:probabilities</span></th>
<td id="S3.T6.3.11.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.11.7.2.1" class="ltx_text" style="font-size:80%;">71.69</span></td>
<td id="S3.T6.3.11.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.11.7.3.1" class="ltx_text" style="font-size:80%;">70.43</span></td>
<td id="S3.T6.3.11.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.11.7.4.1" class="ltx_text" style="font-size:80%;">70.60</span></td>
</tr>
<tr id="S3.T6.3.12.8" class="ltx_tr">
<th id="S3.T6.3.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T6.3.12.8.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:golden-labels</span></th>
<td id="S3.T6.3.12.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.12.8.2.1" class="ltx_text" style="font-size:80%;">73.68</span></td>
<td id="S3.T6.3.12.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.12.8.3.1" class="ltx_text" style="font-size:80%;">71.94</span></td>
<td id="S3.T6.3.12.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T6.3.12.8.4.1" class="ltx_text" style="font-size:80%;">71.97</span></td>
</tr>
<tr id="S3.T6.3.13.9" class="ltx_tr">
<th id="S3.T6.3.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S3.T6.3.13.9.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:binary</span></th>
<td id="S3.T6.3.13.9.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.13.9.2.1" class="ltx_text" style="font-size:80%;">73.47</span></td>
<td id="S3.T6.3.13.9.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.13.9.3.1" class="ltx_text" style="font-size:80%;">71.77</span></td>
<td id="S3.T6.3.13.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T6.3.13.9.4.1" class="ltx_text" style="font-size:80%;">71.84</span></td>
</tr>
<tr id="S3.T6.3.14.10" class="ltx_tr">
<th id="S3.T6.3.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T6.3.14.10.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">pre-hoc:probabilities</span></th>
<td id="S3.T6.3.14.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T6.3.14.10.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">73.69</span></td>
<td id="S3.T6.3.14.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T6.3.14.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">72.22</span></td>
<td id="S3.T6.3.14.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T6.3.14.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">72.26</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T6.9.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>Gender-informed TBDM-Net 10-fold cross validation results on the <span id="S3.T6.10.2" class="ltx_text ltx_font_bold">IEMOCAP</span> corpus. Best results are marked in boldface.</figcaption>
</figure>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">The speaker’s gender information was added to the <span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span> architecture either as post-hoc boosting information or concatenated to the input MFCC representations over the coefficients’ dimensions. The results are reported over the RAVDESS and IEMOCAP datasets. 11 SER systems are evaluated in terms of 10-fold cross-validation, and their results are shown in Tables <a href="#S3.T5" title="Table 5 ‣ 3.5 Gender-informed results ‣ 3 Evaluation ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S3.T6" title="Table 6 ‣ 3.5 Gender-informed results ‣ 3 Evaluation ‣ TBDM-Net: Bidirectional Dense Networks with Gender Information for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. They pertain to the following setups:</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter">baseline:full</span> - model trained on the entire train partition and evaluated on the entire test partition;</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter">baseline:M</span> - model trained on the entire train partition and evaluated on the male subset of the test partition;</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter">baseline:F</span> - model trained on the entire train partition and evaluated on the female subset of the test partition;</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_typewriter">split:M</span> - the male subset of the dataset split into train and test partitions.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p"><span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_typewriter">split:F</span> - the female subset of the dataset split into train and test partitions.</p>
</div>
</li>
<li id="S3.I2.i6" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i6.p1" class="ltx_para">
<p id="S3.I2.i6.p1.1" class="ltx_p"><span id="S3.I2.i6.p1.1.1" class="ltx_text ltx_font_typewriter">post-hoc:golden-labels</span> - based on the golden label for the speaker’s gender, the corresponding gender-dependent model is used to make the final emotion prediction;</p>
</div>
</li>
<li id="S3.I2.i7" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i7.p1" class="ltx_para">
<p id="S3.I2.i7.p1.1" class="ltx_p"><span id="S3.I2.i7.p1.1.1" class="ltx_text ltx_font_typewriter">post-hoc:binary</span> - the binary prediction of the pre-trained gender classifier is used to select the appropriate gender-dependent model;</p>
</div>
</li>
<li id="S3.I2.i8" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i8.p1" class="ltx_para">
<p id="S3.I2.i8.p1.1" class="ltx_p"><span id="S3.I2.i8.p1.1.1" class="ltx_text ltx_font_typewriter">post-hoc:probabilities</span> - the probabilistic predictions of the gender classifier are used to weight the predictions of the two gender-dependent models;</p>
</div>
</li>
<li id="S3.I2.i9" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i9.p1" class="ltx_para">
<p id="S3.I2.i9.p1.1" class="ltx_p"><span id="S3.I2.i9.p1.1.1" class="ltx_text ltx_font_typewriter">pre-hoc:golden-labels</span> - the golden label for the gender is concatenated to the MFCC representation at input;</p>
</div>
</li>
<li id="S3.I2.i10" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i10.p1" class="ltx_para">
<p id="S3.I2.i10.p1.1" class="ltx_p"><span id="S3.I2.i10.p1.1.1" class="ltx_text ltx_font_typewriter">pre-hoc:binary</span> - the binary prediction of the gender classifier is concatenated to the MFCC representation at input;</p>
</div>
</li>
<li id="S3.I2.i11" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i11.p1" class="ltx_para">
<p id="S3.I2.i11.p1.1" class="ltx_p"><span id="S3.I2.i11.p1.1.1" class="ltx_text ltx_font_typewriter">pre-hoc:probabilities</span> - the probabilistic output of the gender classifier is concatenated to the MFCC representations at input.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">It is apparent that incorporating gender information into the input features as gender probability vectors leads to enhanced performance on both evaluated datasets. Nevertheless, the enhancements are only incremental, suggesting the need for a more thorough analysis and a refined feature merging strategy.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper we introduced a novel densely connected deep architecture with temporally-aware blocks, which is able to perform accurate unimodal speech emotion recognition. The experimental results show that <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">TBDM-Net</span> outperforms the state-of-the art results on 6 multilingual SER datasets. An ablation study revealed that the most important aspect of the architecture is the use of bidirectional representations of the spoken output. However, this bidirectionality incurs additional complexity when used in real-time applications. In a separate evaluation, we investigated the effects of gender information over the model’s performance. Results showed that <span id="S4.p1.1.2" class="ltx_text ltx_font_typewriter">TBDM-Net</span> is to a large extent invariant to the speaker’s gender, and that adding such information to the network only slightly improves the overall accuracy of the SER system.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In our future efforts, we aim to utilise the existing architecture for call-centre interactions by consolidating certain emotion categories found in the datasets. Additionally, we seek to explore whether further adjustments to the network could streamline its computational demands, making it more suitable for real-time usage. Crucially, we plan to assess the algorithms’ performance across different datasets to evaluate their predictive capabilities beyond their original distribution.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jianhua Tao, Fangzhou Liu, Meng Zhang, and Huibin Jia,

</span>
<span class="ltx_bibblock">“Design of Speech Corpus for Mandarin Text to Speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Blizzard Challenge Workshop</span>, 2008.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Giovanni Costantini, Iacopo Iaderola, Andrea Paoloni, and Massimiliano Todisco,

</span>
<span class="ltx_bibblock">“EMOVO Corpus: an Italian Emotional Speech Database,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Conference on Language Resources and
Evaluation</span>, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Felix Burkhardt, Astrid Paeschke, M. Rolfes, Walter F. Sendlmeier, and Benjamin
Weiss,

</span>
<span class="ltx_bibblock">“A database of German emotional speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2005.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe) Kazemzadeh,
Emily Mower Provost, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and
Shrikanth S. Narayanan,

</span>
<span class="ltx_bibblock">“IEMOCAP: interactive emotional dyadic motion capture database,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Language Resources and Evaluation</span>, vol. 42, pp. 335–359, 2008.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Steven R. Livingstone and Frank A. Russo,

</span>
<span class="ltx_bibblock">“The Ryerson Audio-Visual Database of Emotional Speech and Song
(RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North
American English,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">PLoS ONE</span>, vol. 13, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Haq and P.J.B. Jackson,

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Machine Audition: Principles, Algorithms and Systems</span>, chapter
Multimodal Emotion Recognition, pp. 398–423,

</span>
<span class="ltx_bibblock">IGI Global, Hershey PA, Aug. 2010.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Arya Aftab, Alireza Morsali, Shahrokh Ghaemmaghami, and Benoît Champagne,

</span>
<span class="ltx_bibblock">“LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for
Speech Emotion Recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pp. 6912–6916, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
George Trigeorgis, Fabien Ringeval, Raymond Brueckner, Erik Marchi, Mihalis A.
Nicolaou, Björn Schuller, and Stefanos Zafeiriou,

</span>
<span class="ltx_bibblock">“Adieu features? End-to-end speech emotion recognition using a deep
convolutional recurrent network,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</span>, 2016, pp. 5200–5204.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Wootaek Lim, Daeyoung Jang, and Taejin Lee,

</span>
<span class="ltx_bibblock">“Speech emotion recognition using convolutional and Recurrent
Neural Networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2016 Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference (APSIPA)</span>, 2016, pp. 1–4.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Xin-Cheng Wen, Jiaxin Ye, Yan Luo, Yong Xu, Xuan-Ze Wang, Chang-Li Wu,
and Kun-Hong Liu,

</span>
<span class="ltx_bibblock">“CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed
Task Net for Single-Corpus and Cross-Corpus Speech Emotion Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the Thirty-First International Joint
Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29
July 2022</span>, Luc De Raedt, Ed. 2022, pp. 2305–2311, ijcai.org.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Itai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais, and Ron Hoory,

</span>
<span class="ltx_bibblock">“Speaker Normalization for Self-Supervised Speech Emotion
Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</span>, 2022, pp. 7342–7346.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Florinel-Alin Croitoru, Nicolae-Catalin Ristea, Radu Tudor Ionescu, and
N. Sebe,

</span>
<span class="ltx_bibblock">“LeRaC: Learning Rate Curriculum,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/2205.09180, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yue Xie, Ruiyu Liang, Zhenlin Liang, Chengwei Huang, Cairong Zou, and Björn
Schuller,

</span>
<span class="ltx_bibblock">“Speech emotion classification using attention-based lstm,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</span>, vol. 27, no. 11, pp. 1675–1685, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Aditya Dutt and Paul Gader,

</span>
<span class="ltx_bibblock">“Wavelet multiresolution analysis based speech emotion recognition
system using 1d cnn lstm networks,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</span>, vol. 31, pp. 2043–2054, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zheng Liu, Xin Kang, and Fuji Ren,

</span>
<span class="ltx_bibblock">“Dual-TBNet: Improving the Robustness of Speech Features via
Dual-Transformer-BiLSTM for Speech Emotion Recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</span>, vol. 31, pp. 2193–2203, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiaxin Ye, Xin cheng Wen, Yujie Wei, Yong Xu, Kunhong Liu, and Hongming Shan,

</span>
<span class="ltx_bibblock">“Temporal Modeling Matters: A Novel Temporal Emotional Modeling
Approach for Speech Emotion Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,

</span>
<span class="ltx_bibblock">“Densely Connected Convolutional Networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, Los Alamitos, CA, USA, jul 2017, pp. 2261–2269, IEEE
Computer Society.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel,

</span>
<span class="ltx_bibblock">“Gaussian Error Linear Units (GELUs),”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv 1606.08415</span>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yesim Ulgen Sonmez and Asaf Varol,

</span>
<span class="ltx_bibblock">“A Speech Emotion Recognition Model Based on Multi-Level Local
Binary and Local Ternary Patterns,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8, pp. 190784–190796, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Huan Zhao, Bo Li, and Zixing Zhang,

</span>
<span class="ltx_bibblock">“Speaker-aware Cross-modal Fusion Architecture for Conversational
Emotion Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, 2023, pp. 2718–2722.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Rui Xia, Jun Deng, Björn Schuller, and Yang Liu,

</span>
<span class="ltx_bibblock">“Modeling gender information for emotion recognition using
Denoising autoencoder,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2014 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</span>, 2014, pp. 990–994.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Puneet Mishra and Ruchir Sharma,

</span>
<span class="ltx_bibblock">“Gender Differentiated Convolutional Neural Networks for Speech
Emotion Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2020 12th International Congress on Ultra Modern
Telecommunications and Control Systems and Workshops (ICUMT)</span>, 2020, pp.
142–148.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Adriana Stan,

</span>
<span class="ltx_bibblock">“Residual Information in Deep Speaker Embedding Architectures,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Mathematics</span>, vol. 10, no. 21, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Nithin Rao Koluguri, Taejin Park, and Boris Ginsburg,

</span>
<span class="ltx_bibblock">“TitaNet: Neural Model for speaker representation with 1D
Depth-wise separable convolutions and global context,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv:2110.04410</span>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Arsha Nagrani, Joon Son Chung, and Andrew Zisserman,

</span>
<span class="ltx_bibblock">“VoxCeleb: A Large-Scale Speaker Identification Dataset,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2017</span>. 2017, ISCA.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.10055" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.10056" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.10056">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.10056" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.10057" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 20:32:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
