<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.03809] Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation</title><meta property="og:description" content="This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational speech-to-text translation. Due to the limited availability of speech translation data …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.03809">

<!--Generated on Mon Aug  5 17:07:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiia Sildam, Andra Velve, Tanel Alumäe
<br class="ltx_break">Department of Software Science 
<br class="ltx_break">Tallinn University of Technology, Estonia
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational speech-to-text translation. Due to the limited availability of speech translation data for Estonian, we created additional training data by web scraping and synthesizing data from speech recognition datasets using machine translation. We evaluated three publicly available end-to-end models: Whisper, OWSM 3.1, and SeamlessM4T. Our results indicate that fine-tuning with synthetic data enhances translation accuracy by a large margin, with SeamlessM4T matching or surpassing cascaded speech translation systems that use state-of-the-art speech recognition and machine translation models.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Tiia Sildam, Andra Velve, Tanel Alumäe</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center">Department of Software Science</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">Tallinn University of Technology, Estonia</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Estonian language, spoken by around one million native speakers, has benefited significantly from the Estonian Language Technology Program in the last decades <cite class="ltx_cite ltx_citemacro_cite">Rehm et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. This initiative has fostered advancements in several key areas, such as automatic speech recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">Alumäe et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> and machine translation (MT) <cite class="ltx_cite ltx_citemacro_cite">Tättar et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>. These improvements are largely due to investments in collecting relevant training data and the successful application of large multilingual pretrained models. Another crucial area of language technology is spoken language translation, which is essential for maintaining smaller languages like Estonian in today’s digital world. This technology enables native speakers of a small language to access foreign language content more easily and allows for the broader dissemination of native language content.
However, one of the significant challenges in developing these technologies is the lack of adequate training data for Estonian, particularly in conversational speech. This shortage hampers the ability to further enhance and refine speech translation tools.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this study, we explore the finetuning of three publicly available end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational speech translation tasks and evaluate their accuracy against the cascaded spoken language translation approach.
Given the scarcity of speech translation datasets containing significant amounts of conversational speech for these translation directions, we explore two methods to generate additional data: synthesizing speech translation training data from ASR training data using machine translation, and scraping data (e.g., videos with subtitles) from the internet. We evaluate these models and finetuning approaches using automatic metrics (BLEU and BLEURT) on realistic conversational speech evaluation sets.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The main contribution of this paper is demonstrating that leading large publicly available end-to-end multilingual speech translation models can be fine-tuned to excel in translation tasks involving relatively low-resource languages by using synthetic data generated from diverse ASR training data. Another innovative aspect of the paper is showing that OpenAI’s Whisper, originally trained only for translating into English, serves as an effective base model that can be finetuned for other speech translation directions. Additionally, we release an evaluation set for Estonian-English-Russian spoken language translation, which includes conversational speech recordings “from the wild”, complete with manual transcripts and professionally produced translations<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/alumae/k6net6lke-benchmark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/alumae/k6net6lke-benchmark</a></span></span></span>. The best-trained speech translation models are publicly available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Finetuned Whisper: <a target="_blank" href="https://huggingface.co/TalTechNLP/whisper-large-v3-et-en-ru.translate" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/TalTechNLP/whisper-large-v3-et-en-ru.translate</a></span></span></span>. An example of an Estonian TV news broadcast with English and Russian subtitles generated by our finetuned Whisper model is available at <a target="_blank" href="https://www.youtube.com/watch?v=rZPqauCYfXI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=rZPqauCYfXI</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Available models</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, an overview of publicly accessible models suitable for speech translation in the targeted translation directions of our study will be provided.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Cascaded spoken language translation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The cascaded speech translation method involves initially using an ASR system to transcribe speech, followed by translating these transcriptions with a text-to-text MT system. Presently, one of the most widely used multilingual ASR model available to the public is OpenAI’s Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. In our tests, we utilized the most effective <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">large-v3</span> model of Whisper to transcribe English and Russian speech. For Estonian, we used the same model, which was finetuned with 1334 hours of Estonian data available publicly from the TalTech Estonian Speech Dataset 1.0<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://cs.taltech.ee/staff/tanel.alumae/data/est-pub-asr-data/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cs.taltech.ee/staff/tanel.alumae/data/est-pub-asr-data/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Alumäe et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>. During the development of this paper, the leading publicly accessible text-to-text MT model for translations involving Estonian was Meta’s NLLB-200 <cite class="ltx_cite ltx_citemacro_cite">NLLB Team et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. The NLLB model is available in various sizes, with the largest being the mixture-of-experts (MoE) version, which requires 350 GB of storage. For practical reasons, we opted for the largest dense model, which has 3.3 billion parameters. Machine translation to and from Estonian via text is also well supported by several proprietary vendors via API calls, such as Google and DeepL. The NLP research group at Tartu University offers a publicly accessible NMT system <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">Neurotõlge<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_upright">4</span></span><a target="_blank" href="https://neurotolge.ee/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://neurotolge.ee/</a></span></span></span></span> that is effective for Estonian MT tasks <cite class="ltx_cite ltx_citemacro_cite">Tättar et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>, and it also provides a free web API for batch processing. OpenAI’s GPT models are also capable of conducting machine translation through prompting.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>End-to-end spoken language translation</h3>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">CoVoST 2</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">FLEURS</th>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<th id="S2.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">#Parameters</th>
<th id="S2.T1.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">est-eng</th>
<th id="S2.T1.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t">eng-est</th>
<th id="S2.T1.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">est-eng</th>
<th id="S2.T1.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">eng-est</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.3.1" class="ltx_tr">
<th id="S2.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Whisper large-v3</th>
<th id="S2.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1.55B</th>
<td id="S2.T1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">15.0</td>
<td id="S2.T1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S2.T1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">18.7</td>
<td id="S2.T1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">N/A</td>
</tr>
<tr id="S2.T1.1.4.2" class="ltx_tr">
<th id="S2.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OWSM 3.1 EBF</th>
<th id="S2.T1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1B</th>
<td id="S2.T1.1.4.2.3" class="ltx_td ltx_align_right">?</td>
<td id="S2.T1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r">7.7</td>
<td id="S2.T1.1.4.2.5" class="ltx_td ltx_align_right">?</td>
<td id="S2.T1.1.4.2.6" class="ltx_td ltx_align_right">?</td>
</tr>
<tr id="S2.T1.1.5.3" class="ltx_tr">
<th id="S2.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">SeamlessM4T-v2 large</th>
<th id="S2.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">2.3B</th>
<td id="S2.T1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_b">27.7</td>
<td id="S2.T1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">29.3</td>
<td id="S2.T1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_b">31.6</td>
<td id="S2.T1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_b">22.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Speech translation BLEU scores of different publicly available models. N/A denotes that the model is not capable of translating in this direction, and question marks denote scores that are not reported.</figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Several publicly available multilingual end-to-end spoken language translation models have recently emerged. OpenAI’s Whisper model can perform translation to English from all its supported speech recognition languages. Other translation directions are not supported by this model. The reported BLEU score for Estonian-to-English translation for the <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">large-v2</span> version of Whisper is 18.7, measured on the FLEURS dataset <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> and 15.0, measured on the CoVoST 2 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite> dataset. Both of those datasets contain read speech. Whisper uses Transformer encoder-decoder architecture.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The Open Whisper-style Speech Model (OWSM) <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023b</a>)</cite> reproduces Whisper-style training using a diverse combination
of publicly available datasets and the open-source toolkit ESPnet <cite class="ltx_cite ltx_citemacro_cite">Watanabe et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. It supports multilingual automatic speech recognition
(ASR) and any-to-any speech translation (ST). The latest release of the model (3.1 EBF) uses the E-Branchformer <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> architecture in the encoder and Transformer in the decoder. The 1 billion parameter “base” version of OWSM 3.1 EBF has a reported BLEU score of 7.7 on the English-to-Estonian translation direction, measured on CoVoST 2.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The third publicly available multilingual speech translation model originates from Meta’s SeamlessM4T project <cite class="ltx_cite ltx_citemacro_cite">Seamless Communication et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. SeamlessM4T translation models are capable of translating both speech and text modalities, and they can produce both text and speech output. Around 100 languages are supported, although speech output is supported for a much smaller subset of languages. While both Whisper and OWSM models are trained end-to-end from scratch, SeamlessM4T uses a more complicated process for training. First, a self-supervised speech encoder model w2v-BERT 2.0 is pretrained, using a corpus of 4.5M hours of unlabeled audio data covering more than 143 languages. This model is then bridged with the NLLB text-to-text translation model, using special adapter layers that map encoded and time-compressed speech features to the same semantic space as text tokens. This composed model is then finetuned for speech-to-text and speech-to-speech translation tasks, using paired text-text, speech-text and speech-speech data scraped from the web and aligned using a dedicated multimodal embedding and alignment model <cite class="ltx_cite ltx_citemacro_cite">Duquenne et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. The <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">SeamlessM4T-large-v2</span> reports a BLEU score of 29.3 on English-Estonian and 27.7 on Estonian-English test sets of CoVoST 2. On FLEURS, this model has a BLEU score of 22.4 on English-Estonian and 31.6 on Estonian-English speech-to-text test sets.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The out-of-the-box BLEU scores of the described models on Estonian-English speech translation tasks are reported in Table <a href="#S2.T1" title="Table 1 ‣ 2.2 End-to-end spoken language translation ‣ 2 Available models ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Although the scores are measured on test sets containing only read speech, the scores suggest that these models could be finetuned to perform well also on more conversational speech that is known to be more difficult to translate.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Whisper and OWSM models are designed to handle audio recordings of any length due to the integrated speech segmentation in their decoders. These models effectively generate time-stamped, subtitle-like transcripts, marking each decoded word block with start and end times. In the process of long-form decoding, the models work on 30-second segments of speech at a time, shifting the processing window by 30 seconds (or less) to start where the last decoded word block ended after each decoding step. On the other hand, SeamlessM4T models are limited to processing shorter, utterance-like speech segments, and their translation quality drops substantially with longer segments, often only translating the initial part of the segment. To address this, long recordings must be initially divided into shorter, speaker-consistent segments, typically no longer than 20 seconds, using voice activity detection and speaker segmentation technologies.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The main focus of our work is finetuning publicly available speech translation models using additional data. Since there are no conversational speech translation datasets that include Estonian, we experiment with generating additional data on our own using two methods: web scraping and data synthesis. We compare the performance of all three existing speech translation models before and after finetuning with the same data.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Although Whisper is originally trained to perform only multilingual speech recognition and speech translation to English, it has been shown that it can perform speech translation to other directions with surprisingly high accuracy by changing only the prefix of the decoder. For example, <cite class="ltx_cite ltx_citemacro_citet">Peng et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite> showed that by only modifying the prompt, Whisper can achieve 18.1 BLEU score on the English-German speech translation test set from the MuST-C corpus <cite class="ltx_cite ltx_citemacro_cite">Gangi et al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>. Therefore, we were relatively confident that Whisper can be finetuned for all translation directions that we were interested in.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The design of Whisper’s prompt does not support the specification of alternative translation directions. Consequently, we finetuned Whisper using extra speech translation data by employing the “transcribe” prompt, where the language specified in the prompt matched the intended target language. At the inference stage, the expected target language was set in the prompt, but the source language remained unspecified to the model.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">On all datasets, Whisper was finetuned<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Finetuning code: <a target="_blank" href="https://github.com/alumae/pl-whisper-finetuner" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/alumae/pl-whisper-finetuner</a></span></span></span> for three epochs over the additional translation datasets. A learning rate schedule with a peak rate of 1e-04 was used, with 500 warmup steps and a linearly decaying schedule towards 0 after the warmup. An effective batch size of 64 was used. Stochastic weight averaging (SWA) <cite class="ltx_cite ltx_citemacro_cite">Izmailov et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> with a learning rate of 1e-05 was applied during the last epoch. Adam optimizer was used.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The OWSM 3.1 EBF model underwent finetuning over five epochs, utilizing a batch size of 320 and a maximum learning rate of 2.0e-04, accompanied by a warmup phase of 600 steps. A label smoothing technique was employed with a smoothing factor of 0.1. During training, a multitask encoder-decoder/CTC loss method was used (with source language transcript as supervision for the CTC head), setting the CTC loss weight at 0.3. The majority of these hyperparameters were adopted directly from the ESPnet’s training recipe for the OWSM 3.1 EBF model without further adjustments.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The SeamlessM4T model was finetuned using a batch size of 48, peak learning rate of 1e-06 with 100 warmup steps. This finetuning setup integrated automatic early stopping that measured the model’s loss on heldout training data after every 1000 model updates and stopped training when the loss didn’t improve during the last 10 evaluations. This usually happened during the second epoch.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">For Whisper and OWSM, the training data was compiled to segments of maximally 30 seconds in length, which usually involved concatenating the transcripts of several adjacent utterances from the long-form training audio, together with the corresponding audio chunks (including the audio between transcription end and start times). The SeamlessM4T model was finetuned using the original utterances and/or subtitle segments.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">All finetuning experiments were conducted using four Nvidia A100 (80GB) GPUs.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">A dedicated evaluation dataset was compiled for this project, using data from public sources (e.g. YouTube). When collecting evaluation data, we tried to ensure that it contains mostly long conversational speech recordings with different levels of spontaneousness, such as press conferences, TV talkshows, YouTube videos, and broadcast news with many interviews. Length of evaluation datasets for all directions varied between 3 and 4.6h. Evaluation data is described in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Evaluation data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Direction</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Duration</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">#Files</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Estonian to Eng/Rus</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">4.15h</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">7</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left">English to Estonian</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">3.05h</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center">5</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_b">Russian to Estonian</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">4.51h</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Amount of evaluation data per translaton direction.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Estonian evaluation data was manually transcribed. English and Russian data was all retrieved from YouTube and we relied on the manually created captions of the videos (after some manual post-editing). We took extra care to select such videos that have good quality verbatim captions. The translations for the evaluation data were created by professional translators in Estonia, using both audio transcriptions and audio files as source data.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Evaluation data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> lists ASR word error rates (WER) of Whisper-based models on the evaluation data. The model <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">whisper-large-v3-est</span> stands for Whisper’s <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">large-v3</span> model, finetuned using 1334 hours of Estonian ASR training data.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">WERs were calculated using ASR hypotheses from Whisper’s long-form decoding mechanism. Due to that, reference sentences are not aligned with hypotheses. WERs were calculated after removing punctuation, lowercasing both hypotheses and references, and aligning words in the hypotheses with references, using minimum WER segmentation (<span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">mwerSegmenter</span>) <cite class="ltx_cite ltx_citemacro_cite">Matusov et al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> via the SLTev toolkit <cite class="ltx_cite ltx_citemacro_cite">Ansari et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">It must be noted that Whisper is generally very accurate on English and Russian evaluation data. The surprisingly high WER (compared to the results published by <cite class="ltx_cite ltx_citemacro_citet">Radford et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>) is mostly caused by occasional hallucinations that repeat some segment transcripts many times.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">English</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">whisper-large-v3</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">24.5%</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left">Russian</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left">whisper-large-v3</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_right">21.1%</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left">Estonian</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left">whisper-large-v3</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_right">26.6%</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b">Estonian</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_b">whisper-large-v3-est</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_b">9.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Whisper’s speech recognition WER on evaluation data.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In order to finetune the end-to-end speech translation models to perform better in translation directons involving Estonian conversational speech, we experimented with collecting additional data from the web, and synthesizing additional data from ASR training data using MT.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">There are some publicly available speech translation datasets that include a relatively small amount of Estonian. The dataset with the largest amount of Estonian is CoVoST 2 with 364 hours of Estonian-English data and 3 hours of English-Estonian data. However, CoVoST 2 includes exclusively read speech and short sentences. The VoxPopuli corpus <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> also contains some Estonian speech, originating from the European Parliamant sessions, but only 3 hours of that are transcribed. Due to the small size or out-of-domain nature, we did not use those datasets for finetuning.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Scraping web data</h4>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2" class="ltx_tr">
<th id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.2.2.3.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">est <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></span></th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">
<math id="S4.T4.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.2.2.2.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold"> est</span>
</th>
</tr>
<tr id="S4.T4.2.3.1" class="ltx_tr">
<th id="S4.T4.2.3.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.3.1.2.1" class="ltx_text ltx_font_bold">eng</span></th>
<th id="S4.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.3.1.3.1" class="ltx_text ltx_font_bold">rus</span></th>
<th id="S4.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.3.1.4.1" class="ltx_text ltx_font_bold">eng</span></th>
<th id="S4.T4.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.3.1.5.1" class="ltx_text ltx_font_bold">rus</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.4.1" class="ltx_tr">
<th id="S4.T4.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ETV+</th>
<td id="S4.T4.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">182.7</td>
</tr>
<tr id="S4.T4.2.5.2" class="ltx_tr">
<th id="S4.T4.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TED</th>
<td id="S4.T4.2.5.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.5.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.5.2.4" class="ltx_td ltx_align_center">41.2</td>
<td id="S4.T4.2.5.2.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T4.2.6.3" class="ltx_tr">
<th id="S4.T4.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TV7</th>
<td id="S4.T4.2.6.3.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.6.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.6.3.4" class="ltx_td ltx_align_center">16.4</td>
<td id="S4.T4.2.6.3.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T4.2.7.4" class="ltx_tr">
<th id="S4.T4.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">YouTube</th>
<td id="S4.T4.2.7.4.2" class="ltx_td ltx_align_center">39.6</td>
<td id="S4.T4.2.7.4.3" class="ltx_td ltx_align_center">18.2</td>
<td id="S4.T4.2.7.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.7.4.5" class="ltx_td ltx_align_center">433.9</td>
</tr>
<tr id="S4.T4.2.8.5" class="ltx_tr">
<th id="S4.T4.2.8.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="S4.T4.2.8.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">39.6</td>
<td id="S4.T4.2.8.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">18.2</td>
<td id="S4.T4.2.8.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">57.6</td>
<td id="S4.T4.2.8.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">616.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Amount of training data in hours per translation direction, derived from subtitled online videos.</figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Language</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.2.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Estonian</span></span>
</span>
</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.3.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">English</span></span>
</span>
</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.4.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Russian</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Sources</th>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.1.2.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.2.1.2.1.1.1" class="ltx_text ltx_font_bold">TalTech Estonian Speech Dataset 1.0</span></span>
</span>
</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.1.3.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">Gigaspeech</span> (subset M):
<br class="ltx_break">Audiobooks: 260h 
<br class="ltx_break">Podcasts: 350h 
<br class="ltx_break">YouTube: 390 h</span>
</span>
</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.1.4.1.1" class="ltx_p" style="width:119.5pt;"><span id="S4.T5.1.2.1.4.1.1.1" class="ltx_text ltx_font_bold">DW Russian</span>: 45h 
<br class="ltx_break"><span id="S4.T5.1.2.1.4.1.1.2" class="ltx_text ltx_font_bold">TEDx talks:</span> 57h</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<th id="S4.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Total</th>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T5.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.2.1.1" class="ltx_p" style="width:119.5pt;">1334h</span>
</span>
</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T5.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.3.1.1" class="ltx_p" style="width:119.5pt;">1000h</span>
</span>
</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T5.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.4.1.1" class="ltx_p" style="width:119.5pt;">102h</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Amount of source-language ASR training data, used as input for creating synthetic speech translation data.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Given the relatively small number of Estonian speakers, the amount of speech data available on the web for training speech translation models is limited. We aimed to find data featuring long-form conversational speech (rather than individual utterances) since Whisper and OWSM require 30-second speech segments for training to develop models capable of transcribing long-form speech. We avoided sources with machine-generated subtitles.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">We identified several good sources: ETV+ (a Russian-language TV channel of Estonian state media), TED talks with Estonian subtitles, TV7 (an international TV channel with Christian background), and various YouTube channels with consistently good subtitles.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‣ 4.2.1 Scraping web data ‣ 4.2 Training data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> lists the amount of data we found for each translation direction. As can be seen, the sizes vary significantly across the four translation directions we target.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Synthetic data</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">There are two primary methods for generating synthetic data to train speech translation models: (1) using speech synthesis to create source speech data from existing MT training data, and (2) using MT to generate target text data from existing source language ASR training data. We chose the second method because we already had substantial amount of Estonian ASR training data from various conversational sources, and the current Estonian-to-English and Estonian-to-Russian MT systems produce relatively high-quality translations. The main drawback of the first method is the lack of MT training corpora that include transcribed conversational speech, making it challenging to achieve a wide variety of speakers and natural-sounding speech through speech synthesis.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">As Estonian source speech data, we used the data available publicly from the TalTech Estonian Speech Dataset 1.0. It contains mostly speech from broadcast sources, with an emphasis on conversation speech, such as interviews and talk shows. In addition, it contains speech recordings from various conferences and seminars, and a relatively small amount of speech from the Estonian Parliament. All the speech data consists of long-form speech and has been manually transcribed and time-aligned with speech at an utterance level.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">When searching for training data for English and Russian speech, we found it challenging to locate high-quality, long-form conversational speech data transcribed at the recording level with orthographic annotation, as needed for finetuning Whisper and OWSM models. For English, we used a subset of the Gigaspeech corpus <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>, which includes long-form recordings (audiobooks, podcasts, and YouTube videos) transcribed at the utterance level. However, these utterances are uppercased, and only a limited set of punctuation marks (“.,!?”) are retained. To enhance the suitability of these transcripts as MT source data, we applied true-casing using a custom implementation. This implementation uses spaCy to split utterances into sentences and then uppercases sentence start tokens, proper nouns, and certain special words (such as <span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_italic">I</span>).</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">For Russian, we couldn’t find any open datasets that contain sufficient amount of transcribed long-form speech data. A popular choice for training Russian ASR models is the Russian Open STT Dataset<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/snakers4/open_stt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/snakers4/open_stt</a></span></span></span> which contains over 20 000 hours of transcribed Russian speech. However, this dataset contains exclusively relatively short utterances. Although most of the data in this dataset originates from long-form speech recordings, it is not possible to reconstruct homogeneous 30-second speech segments with the corresponding transcripts from this data, as the utterance IDs have been randomized. Therefore, we used two online sources as the Russian speech data, both of which come with good quality captions: Russian TEDx talks and the Russian language YouTube channel of the <span id="S4.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_italic">Deutsche Welle</span> (DW) news broadcaster <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://www.youtube.com/dwrussianreporter" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/dwrussianreporter</a></span></span></span>.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p">The total amounts of ASR datasets used as input for synthesizing MT-based speech translation data are listed in Table <a href="#S4.T5" title="Table 5 ‣ 4.2.1 Scraping web data ‣ 4.2 Training data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
For creating synthetic data for speech translation, the transcripts were machine-translated. We used Google Translate for translating Estonian and English language pair directions. Russian and Estonian language pair translations were done with University of Tartu’s <span id="S4.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_italic">Neurotõlge</span> MT system. Those choices were based on our budget, as well as on the reference transcript MT evaluation results in Table <a href="#S4.T6" title="Table 6 ‣ 4.2.2 Synthetic data ‣ 4.2 Training data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:445.7pt;height:487pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T6.4.4" class="ltx_p"><span id="S4.T6.4.4.4" class="ltx_text">
<span id="S4.T6.4.4.4.4" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T6.4.4.4.4.5.1" class="ltx_tr">
<span id="S4.T6.4.4.4.4.5.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.5.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="S4.T6.4.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.5.1.2.1" class="ltx_text ltx_font_bold">Finetuned</span></span>
<span id="S4.T6.4.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_5" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.5.1.3.1" class="ltx_text ltx_font_bold">BLEU</span></span>
<span id="S4.T6.4.4.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_5" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.5.1.4.1" class="ltx_text ltx_font_bold">BLEURT</span></span></span>
<span id="S4.T6.4.4.4.4.4" class="ltx_tr">
<span id="S4.T6.4.4.4.4.4.5" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.4.6.1" class="ltx_text ltx_font_bold">web</span></span>
<span id="S4.T6.4.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.4.7.1" class="ltx_text ltx_font_bold">synt.</span></span>
<span id="S4.T6.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">est <math id="S4.T6.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T6.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T6.1.1.1.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></span></span>
<span id="S4.T6.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S4.T6.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T6.2.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T6.2.2.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T6.2.2.2.2.2.2.1" class="ltx_text ltx_font_bold"> est</span></span>
<span id="S4.T6.4.4.4.4.4.8" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.3.3.3.3.3.3.1" class="ltx_text ltx_font_bold">est <math id="S4.T6.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T6.3.3.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T6.3.3.3.3.3.3.1.m1.1.1" xref="S4.T6.3.3.3.3.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.3.3.1.m1.1b"><ci id="S4.T6.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.3.3.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.3.3.1.m1.1c">\rightarrow</annotation></semantics></math></span></span>
<span id="S4.T6.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S4.T6.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T6.4.4.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T6.4.4.4.4.4.4.m1.1.1" xref="S4.T6.4.4.4.4.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.4.4.4.m1.1b"><ci id="S4.T6.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.4.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T6.4.4.4.4.4.4.1" class="ltx_text ltx_font_bold"> est</span></span>
<span id="S4.T6.4.4.4.4.4.9" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T6.4.4.4.4.6.2" class="ltx_tr">
<span id="S4.T6.4.4.4.4.6.2.1" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.6.2.2" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.6.2.3" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.6.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.4.1" class="ltx_text ltx_font_bold">eng</span></span>
<span id="S4.T6.4.4.4.4.6.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.5.1" class="ltx_text ltx_font_bold">rus</span></span>
<span id="S4.T6.4.4.4.4.6.2.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.6.1" class="ltx_text ltx_font_bold">eng</span></span>
<span id="S4.T6.4.4.4.4.6.2.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.7.1" class="ltx_text ltx_font_bold">rus</span></span>
<span id="S4.T6.4.4.4.4.6.2.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.8.1" class="ltx_text ltx_font_bold">avg</span></span>
<span id="S4.T6.4.4.4.4.6.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.9.1" class="ltx_text ltx_font_bold">eng</span></span>
<span id="S4.T6.4.4.4.4.6.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.10.1" class="ltx_text ltx_font_bold">rus</span></span>
<span id="S4.T6.4.4.4.4.6.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.11.1" class="ltx_text ltx_font_bold">eng</span></span>
<span id="S4.T6.4.4.4.4.6.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.12.1" class="ltx_text ltx_font_bold">rus</span></span>
<span id="S4.T6.4.4.4.4.6.2.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.6.2.13.1" class="ltx_text ltx_font_bold">avg</span></span></span>
<span id="S4.T6.4.4.4.4.7.3" class="ltx_tr">
<span id="S4.T6.4.4.4.4.7.3.1" class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_5" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.7.3.1.1" class="ltx_text ltx_font_italic">Text-to-text translation using reference transcripts</span></span>
<span id="S4.T6.4.4.4.4.7.3.2" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.3" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.4" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.5" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.6" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.7" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.8" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.7.3.9" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T6.4.4.4.4.8.4" class="ltx_tr">
<span id="S4.T6.4.4.4.4.8.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Ref. + NLLB-200 3.3B</span>
<span id="S4.T6.4.4.4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.8.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.8.4.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">31.4</span>
<span id="S4.T6.4.4.4.4.8.4.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">25.2</span>
<span id="S4.T6.4.4.4.4.8.4.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">21.5</span>
<span id="S4.T6.4.4.4.4.8.4.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">19.2</span>
<span id="S4.T6.4.4.4.4.8.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">24.3</span>
<span id="S4.T6.4.4.4.4.8.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.652</span>
<span id="S4.T6.4.4.4.4.8.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.665</span>
<span id="S4.T6.4.4.4.4.8.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.529</span>
<span id="S4.T6.4.4.4.4.8.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.574</span>
<span id="S4.T6.4.4.4.4.8.4.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.605</span></span>
<span id="S4.T6.4.4.4.4.9.5" class="ltx_tr">
<span id="S4.T6.4.4.4.4.9.5.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Ref. + GPT3.5-turbo</span>
<span id="S4.T6.4.4.4.4.9.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.9.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.9.5.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">36.1</span>
<span id="S4.T6.4.4.4.4.9.5.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">28.3</span>
<span id="S4.T6.4.4.4.4.9.5.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">21.3</span>
<span id="S4.T6.4.4.4.4.9.5.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">23.8</span>
<span id="S4.T6.4.4.4.4.9.5.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">27.4</span>
<span id="S4.T6.4.4.4.4.9.5.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.696</span>
<span id="S4.T6.4.4.4.4.9.5.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.703</span>
<span id="S4.T6.4.4.4.4.9.5.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.593</span>
<span id="S4.T6.4.4.4.4.9.5.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.665</span>
<span id="S4.T6.4.4.4.4.9.5.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.664</span></span>
<span id="S4.T6.4.4.4.4.10.6" class="ltx_tr">
<span id="S4.T6.4.4.4.4.10.6.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Ref. + GPT4</span>
<span id="S4.T6.4.4.4.4.10.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.10.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.10.6.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">38.3</span>
<span id="S4.T6.4.4.4.4.10.6.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">31.3</span>
<span id="S4.T6.4.4.4.4.10.6.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">19.9</span>
<span id="S4.T6.4.4.4.4.10.6.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">24.6</span>
<span id="S4.T6.4.4.4.4.10.6.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">28.5</span>
<span id="S4.T6.4.4.4.4.10.6.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.702</span>
<span id="S4.T6.4.4.4.4.10.6.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.721</span>
<span id="S4.T6.4.4.4.4.10.6.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.609</span>
<span id="S4.T6.4.4.4.4.10.6.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.656</span>
<span id="S4.T6.4.4.4.4.10.6.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.672</span></span>
<span id="S4.T6.4.4.4.4.11.7" class="ltx_tr">
<span id="S4.T6.4.4.4.4.11.7.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Ref. + Google Translate</span>
<span id="S4.T6.4.4.4.4.11.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.11.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.11.7.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">38.9</span>
<span id="S4.T6.4.4.4.4.11.7.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.1</span>
<span id="S4.T6.4.4.4.4.11.7.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">25.4</span>
<span id="S4.T6.4.4.4.4.11.7.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">24.2</span>
<span id="S4.T6.4.4.4.4.11.7.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">28.7</span>
<span id="S4.T6.4.4.4.4.11.7.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.690</span>
<span id="S4.T6.4.4.4.4.11.7.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.686</span>
<span id="S4.T6.4.4.4.4.11.7.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.576</span>
<span id="S4.T6.4.4.4.4.11.7.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.655</span>
<span id="S4.T6.4.4.4.4.11.7.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.652</span></span>
<span id="S4.T6.4.4.4.4.12.8" class="ltx_tr">
<span id="S4.T6.4.4.4.4.12.8.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Ref. + Neurotõlge</span>
<span id="S4.T6.4.4.4.4.12.8.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.12.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.12.8.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">34.8</span>
<span id="S4.T6.4.4.4.4.12.8.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">29.3</span>
<span id="S4.T6.4.4.4.4.12.8.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">24.7</span>
<span id="S4.T6.4.4.4.4.12.8.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">23.7</span>
<span id="S4.T6.4.4.4.4.12.8.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">28.1</span>
<span id="S4.T6.4.4.4.4.12.8.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.656</span>
<span id="S4.T6.4.4.4.4.12.8.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.672</span>
<span id="S4.T6.4.4.4.4.12.8.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.558</span>
<span id="S4.T6.4.4.4.4.12.8.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.619</span>
<span id="S4.T6.4.4.4.4.12.8.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.626</span></span>
<span id="S4.T6.4.4.4.4.13.9" class="ltx_tr">
<span id="S4.T6.4.4.4.4.13.9.1" class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_5" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.13.9.1.1" class="ltx_text ltx_font_italic">Cascaded speech translation systems</span></span>
<span id="S4.T6.4.4.4.4.13.9.2" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.3" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.4" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.5" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.6" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.7" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.8" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.13.9.9" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T6.4.4.4.4.14.10" class="ltx_tr">
<span id="S4.T6.4.4.4.4.14.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper + NLLB-200 3.3B</span>
<span id="S4.T6.4.4.4.4.14.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.14.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.14.10.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">28.8</span>
<span id="S4.T6.4.4.4.4.14.10.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">23.1</span>
<span id="S4.T6.4.4.4.4.14.10.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.4</span>
<span id="S4.T6.4.4.4.4.14.10.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">13.2</span>
<span id="S4.T6.4.4.4.4.14.10.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">20.1</span>
<span id="S4.T6.4.4.4.4.14.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.568</span>
<span id="S4.T6.4.4.4.4.14.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.568</span>
<span id="S4.T6.4.4.4.4.14.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.439</span>
<span id="S4.T6.4.4.4.4.14.10.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.537</span>
<span id="S4.T6.4.4.4.4.14.10.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.528</span></span>
<span id="S4.T6.4.4.4.4.15.11" class="ltx_tr">
<span id="S4.T6.4.4.4.4.15.11.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper + GPT3.5-turbo</span>
<span id="S4.T6.4.4.4.4.15.11.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.15.11.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.15.11.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">32.9</span>
<span id="S4.T6.4.4.4.4.15.11.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.5</span>
<span id="S4.T6.4.4.4.4.15.11.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">15.1</span>
<span id="S4.T6.4.4.4.4.15.11.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">18.3</span>
<span id="S4.T6.4.4.4.4.15.11.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">23.2</span>
<span id="S4.T6.4.4.4.4.15.11.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.649</span>
<span id="S4.T6.4.4.4.4.15.11.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.656</span>
<span id="S4.T6.4.4.4.4.15.11.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.470</span>
<span id="S4.T6.4.4.4.4.15.11.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.621</span>
<span id="S4.T6.4.4.4.4.15.11.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.599</span></span>
<span id="S4.T6.4.4.4.4.16.12" class="ltx_tr">
<span id="S4.T6.4.4.4.4.16.12.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper + GPT4</span>
<span id="S4.T6.4.4.4.4.16.12.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.16.12.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.16.12.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">35.1</span>
<span id="S4.T6.4.4.4.4.16.12.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">29.8</span>
<span id="S4.T6.4.4.4.4.16.12.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.3</span>
<span id="S4.T6.4.4.4.4.16.12.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">18.3</span>
<span id="S4.T6.4.4.4.4.16.12.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">24.9</span>
<span id="S4.T6.4.4.4.4.16.12.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.647</span>
<span id="S4.T6.4.4.4.4.16.12.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.687</span>
<span id="S4.T6.4.4.4.4.16.12.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.507</span>
<span id="S4.T6.4.4.4.4.16.12.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.625</span>
<span id="S4.T6.4.4.4.4.16.12.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.617</span></span>
<span id="S4.T6.4.4.4.4.17.13" class="ltx_tr">
<span id="S4.T6.4.4.4.4.17.13.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper + Google Translate</span>
<span id="S4.T6.4.4.4.4.17.13.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.17.13.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.17.13.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">35.2</span>
<span id="S4.T6.4.4.4.4.17.13.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">23.8</span>
<span id="S4.T6.4.4.4.4.17.13.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">17.4</span>
<span id="S4.T6.4.4.4.4.17.13.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.1</span>
<span id="S4.T6.4.4.4.4.17.13.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">22.9</span>
<span id="S4.T6.4.4.4.4.17.13.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.628</span>
<span id="S4.T6.4.4.4.4.17.13.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.617</span>
<span id="S4.T6.4.4.4.4.17.13.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.481</span>
<span id="S4.T6.4.4.4.4.17.13.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.585</span>
<span id="S4.T6.4.4.4.4.17.13.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.578</span></span>
<span id="S4.T6.4.4.4.4.18.14" class="ltx_tr">
<span id="S4.T6.4.4.4.4.18.14.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper + Neurotõlge</span>
<span id="S4.T6.4.4.4.4.18.14.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.18.14.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.18.14.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">31.9</span>
<span id="S4.T6.4.4.4.4.18.14.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.6</span>
<span id="S4.T6.4.4.4.4.18.14.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.1</span>
<span id="S4.T6.4.4.4.4.18.14.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.0</span>
<span id="S4.T6.4.4.4.4.18.14.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">22.7</span>
<span id="S4.T6.4.4.4.4.18.14.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.598</span>
<span id="S4.T6.4.4.4.4.18.14.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.612</span>
<span id="S4.T6.4.4.4.4.18.14.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.458</span>
<span id="S4.T6.4.4.4.4.18.14.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.566</span>
<span id="S4.T6.4.4.4.4.18.14.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.559</span></span>
<span id="S4.T6.4.4.4.4.19.15" class="ltx_tr">
<span id="S4.T6.4.4.4.4.19.15.1" class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_5" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.19.15.1.1" class="ltx_text ltx_font_italic">Public end-to-end speech translation models</span></span>
<span id="S4.T6.4.4.4.4.19.15.2" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.3" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.4" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.5" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.6" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.7" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.8" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.19.15.9" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T6.4.4.4.4.20.16" class="ltx_tr">
<span id="S4.T6.4.4.4.4.20.16.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper-large-v3</span>
<span id="S4.T6.4.4.4.4.20.16.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">14.9</span>
<span id="S4.T6.4.4.4.4.20.16.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.451</span>
<span id="S4.T6.4.4.4.4.20.16.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.20.16.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span></span>
<span id="S4.T6.4.4.4.4.21.17" class="ltx_tr">
<span id="S4.T6.4.4.4.4.21.17.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">OWSM 3.1 EBF</span>
<span id="S4.T6.4.4.4.4.21.17.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.21.17.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.21.17.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.5</span>
<span id="S4.T6.4.4.4.4.21.17.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.0</span>
<span id="S4.T6.4.4.4.4.21.17.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.6</span>
<span id="S4.T6.4.4.4.4.21.17.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.0</span>
<span id="S4.T6.4.4.4.4.21.17.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">0.5</span>
<span id="S4.T6.4.4.4.4.21.17.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.176</span>
<span id="S4.T6.4.4.4.4.21.17.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.153</span>
<span id="S4.T6.4.4.4.4.21.17.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.147</span>
<span id="S4.T6.4.4.4.4.21.17.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.095</span>
<span id="S4.T6.4.4.4.4.21.17.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.143</span></span>
<span id="S4.T6.4.4.4.4.22.18" class="ltx_tr">
<span id="S4.T6.4.4.4.4.22.18.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">SeamlessM4T v2 (large)</span>
<span id="S4.T6.4.4.4.4.22.18.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.22.18.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.22.18.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">13.2</span>
<span id="S4.T6.4.4.4.4.22.18.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.2</span>
<span id="S4.T6.4.4.4.4.22.18.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">6.4</span>
<span id="S4.T6.4.4.4.4.22.18.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">13.9</span>
<span id="S4.T6.4.4.4.4.22.18.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">12.4</span>
<span id="S4.T6.4.4.4.4.22.18.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.348</span>
<span id="S4.T6.4.4.4.4.22.18.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.426</span>
<span id="S4.T6.4.4.4.4.22.18.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.227</span>
<span id="S4.T6.4.4.4.4.22.18.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.448</span>
<span id="S4.T6.4.4.4.4.22.18.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.362</span></span>
<span id="S4.T6.4.4.4.4.23.19" class="ltx_tr">
<span id="S4.T6.4.4.4.4.23.19.1" class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_8" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T6.4.4.4.4.23.19.1.1" class="ltx_text ltx_font_italic">Public end-to-end speech translation models</span> after finetuning</span>
<span id="S4.T6.4.4.4.4.23.19.2" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.23.19.3" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.23.19.4" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.23.19.5" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T6.4.4.4.4.23.19.6" class="ltx_td ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T6.4.4.4.4.24.20" class="ltx_tr">
<span id="S4.T6.4.4.4.4.24.20.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper-large-v3</span>
<span id="S4.T6.4.4.4.4.24.20.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.24.20.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.24.20.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">17.9</span>
<span id="S4.T6.4.4.4.4.24.20.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">11.7</span>
<span id="S4.T6.4.4.4.4.24.20.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">13.1</span>
<span id="S4.T6.4.4.4.4.24.20.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">14.3</span>
<span id="S4.T6.4.4.4.4.24.20.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">14.2</span>
<span id="S4.T6.4.4.4.4.24.20.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.496</span>
<span id="S4.T6.4.4.4.4.24.20.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.413</span>
<span id="S4.T6.4.4.4.4.24.20.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.433</span>
<span id="S4.T6.4.4.4.4.24.20.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.523</span>
<span id="S4.T6.4.4.4.4.24.20.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">.466</span></span>
<span id="S4.T6.4.4.4.4.25.21" class="ltx_tr">
<span id="S4.T6.4.4.4.4.25.21.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper-large-v3</span>
<span id="S4.T6.4.4.4.4.25.21.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.25.21.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.25.21.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">33.2</span>
<span id="S4.T6.4.4.4.4.25.21.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.1</span>
<span id="S4.T6.4.4.4.4.25.21.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">14.5</span>
<span id="S4.T6.4.4.4.4.25.21.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">14.8</span>
<span id="S4.T6.4.4.4.4.25.21.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">22.2</span>
<span id="S4.T6.4.4.4.4.25.21.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.611</span>
<span id="S4.T6.4.4.4.4.25.21.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.605</span>
<span id="S4.T6.4.4.4.4.25.21.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.363</span>
<span id="S4.T6.4.4.4.4.25.21.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.500</span>
<span id="S4.T6.4.4.4.4.25.21.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.520</span></span>
<span id="S4.T6.4.4.4.4.26.22" class="ltx_tr">
<span id="S4.T6.4.4.4.4.26.22.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Whisper-large-v3</span>
<span id="S4.T6.4.4.4.4.26.22.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.26.22.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.26.22.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">33.0</span>
<span id="S4.T6.4.4.4.4.26.22.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">25.5</span>
<span id="S4.T6.4.4.4.4.26.22.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">17.3</span>
<span id="S4.T6.4.4.4.4.26.22.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.3</span>
<span id="S4.T6.4.4.4.4.26.22.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">23.1</span>
<span id="S4.T6.4.4.4.4.26.22.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.614</span>
<span id="S4.T6.4.4.4.4.26.22.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.603</span>
<span id="S4.T6.4.4.4.4.26.22.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.458</span>
<span id="S4.T6.4.4.4.4.26.22.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.549</span>
<span id="S4.T6.4.4.4.4.26.22.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.560</span></span>
<span id="S4.T6.4.4.4.4.27.23" class="ltx_tr">
<span id="S4.T6.4.4.4.4.27.23.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">OWSM 3.1 EBF</span>
<span id="S4.T6.4.4.4.4.27.23.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.27.23.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.27.23.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">25.8</span>
<span id="S4.T6.4.4.4.4.27.23.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">18.7</span>
<span id="S4.T6.4.4.4.4.27.23.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">11.9</span>
<span id="S4.T6.4.4.4.4.27.23.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">8.5</span>
<span id="S4.T6.4.4.4.4.27.23.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">16.2</span>
<span id="S4.T6.4.4.4.4.27.23.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.541</span>
<span id="S4.T6.4.4.4.4.27.23.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.463</span>
<span id="S4.T6.4.4.4.4.27.23.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.377</span>
<span id="S4.T6.4.4.4.4.27.23.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.360</span>
<span id="S4.T6.4.4.4.4.27.23.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.435</span></span>
<span id="S4.T6.4.4.4.4.28.24" class="ltx_tr">
<span id="S4.T6.4.4.4.4.28.24.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">SeamlessM4T v2 (large)</span>
<span id="S4.T6.4.4.4.4.28.24.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.28.24.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.28.24.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">19.3</span>
<span id="S4.T6.4.4.4.4.28.24.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">14.4</span>
<span id="S4.T6.4.4.4.4.28.24.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">6.1</span>
<span id="S4.T6.4.4.4.4.28.24.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">4.3</span>
<span id="S4.T6.4.4.4.4.28.24.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">11.0</span>
<span id="S4.T6.4.4.4.4.28.24.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.468</span>
<span id="S4.T6.4.4.4.4.28.24.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.488</span>
<span id="S4.T6.4.4.4.4.28.24.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.234</span>
<span id="S4.T6.4.4.4.4.28.24.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.261</span>
<span id="S4.T6.4.4.4.4.28.24.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.363</span></span>
<span id="S4.T6.4.4.4.4.29.25" class="ltx_tr">
<span id="S4.T6.4.4.4.4.29.25.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">SeamlessM4T v2 (large)</span>
<span id="S4.T6.4.4.4.4.29.25.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T6.4.4.4.4.29.25.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.29.25.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">35.4</span>
<span id="S4.T6.4.4.4.4.29.25.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.8</span>
<span id="S4.T6.4.4.4.4.29.25.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">18.8</span>
<span id="S4.T6.4.4.4.4.29.25.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">16.4</span>
<span id="S4.T6.4.4.4.4.29.25.8" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">24.4</span>
<span id="S4.T6.4.4.4.4.29.25.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.618</span>
<span id="S4.T6.4.4.4.4.29.25.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.603</span>
<span id="S4.T6.4.4.4.4.29.25.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.482</span>
<span id="S4.T6.4.4.4.4.29.25.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.494</span>
<span id="S4.T6.4.4.4.4.29.25.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">.549</span></span>
<span id="S4.T6.4.4.4.4.30.26" class="ltx_tr">
<span id="S4.T6.4.4.4.4.30.26.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">SeamlessM4T v2 (large)</span>
<span id="S4.T6.4.4.4.4.30.26.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.30.26.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✓</span>
<span id="S4.T6.4.4.4.4.30.26.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">34.7</span>
<span id="S4.T6.4.4.4.4.30.26.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">25.9</span>
<span id="S4.T6.4.4.4.4.30.26.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">19.1</span>
<span id="S4.T6.4.4.4.4.30.26.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">12.9</span>
<span id="S4.T6.4.4.4.4.30.26.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">23.1</span>
<span id="S4.T6.4.4.4.4.30.26.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">.617</span>
<span id="S4.T6.4.4.4.4.30.26.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">.605</span>
<span id="S4.T6.4.4.4.4.30.26.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">.470</span>
<span id="S4.T6.4.4.4.4.30.26.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">.426</span>
<span id="S4.T6.4.4.4.4.30.26.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">.529</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of baseline scores, cascaded systems, off-the-shelf end-to-end models and finetuned end-to-end models.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We based our evaluation on two metrics: BLEU and BLEURT <cite class="ltx_cite ltx_citemacro_cite">Sellam et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>. BLEURT is a learned metric, trained on subjective human evaluations scores of machine translation references and the corresponding MT candidates. BLEURT outputs scores that usually in the range of 0..1 (with 1 being a perfect match) and is found to be better correlated with human judgements in several languages. We used the multilingual BLEURT-20-D12 model introduced by <cite class="ltx_cite ltx_citemacro_citet">Pu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">BLEU and BLEURT scores are calculated after aligning words in the translation candidates with references, using <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">mwerSegmenter</span> via the SLTev toolkit.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results and discussion</h3>

<figure id="S4.T7" class="ltx_table">
<div id="S4.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:588.6pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T7.1.1" class="ltx_p"><span id="S4.T7.1.1.1" class="ltx_text">
<span id="S4.T7.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T7.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T7.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="S4.T7.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4" style="background-color:#00FFFF;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#00FFFF;">Whisper + Google Translate</span></span>
<span id="S4.T7.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4" style="background-color:#FFFF00;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="background-color:#FFFF00;">Whisper-large-v3 ft.</span></span>
<span id="S4.T7.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4" style="background-color:#FFBFBF;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="background-color:#FFBFBF;">SeamlessM4T ft.</span></span></span>
<span id="S4.T7.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T7.1.1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-eng</span>
<span id="S4.T7.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-rus</span>
<span id="S4.T7.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">eng-est</span>
<span id="S4.T7.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">rus-est</span>
<span id="S4.T7.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-eng</span>
<span id="S4.T7.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-rus</span>
<span id="S4.T7.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">eng-est</span>
<span id="S4.T7.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">rus-est</span>
<span id="S4.T7.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-eng</span>
<span id="S4.T7.1.1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">est-rus</span>
<span id="S4.T7.1.1.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">eng-est</span>
<span id="S4.T7.1.1.1.1.2.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">rus-est</span></span>
<span id="S4.T7.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T7.1.1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#00FFFF;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.3.3.1.1" class="ltx_text ltx_font_bold" style="background-color:#00FFFF;">Whisper + Google Translate</span></span>
<span id="S4.T7.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.3.3.6" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.7" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.8" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.9" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.10" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.11" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.12" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.3.3.13" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T7.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T7.1.1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#FFFF00;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold" style="background-color:#FFFF00;">Whisper-large-v3 (finetuned)</span></span>
<span id="S4.T7.1.1.1.1.4.4.2" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.3" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.4" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.5" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.4.4.10" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.11" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.12" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.4.4.13" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></span></span>
<span id="S4.T7.1.1.1.1.5.5" class="ltx_tr">
<span id="S4.T7.1.1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="background-color:#FFBFBF;padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T7.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold" style="background-color:#FFBFBF;">SeamlessM4T (finetuned)</span></span>
<span id="S4.T7.1.1.1.1.5.5.2" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.3" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.4" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.5" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.6" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.7" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.8" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.9" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T7.1.1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">-</span>
<span id="S4.T7.1.1.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">-</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Statistically significant differences between systems, based on BLEU scores: if one of the models is significantly better than the other, the corresponding cell is colored using the corresponding color.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Evaluation results, together with several baselines, are presented in Table <a href="#S4.T6" title="Table 6 ‣ 4.2.2 Synthetic data ‣ 4.2 Training data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The first section of rows in the table compares the performance of different MT systems on reference transcripts. It can be seen that while there are substantial differences between the proprietary systems among individual translation directions, the average scores in terms of both BLEU and BLEURT are surprisingly similar. The fully open source NLLB-200 model however doesn’t reach the accuracy of the top proprietary systems.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">The next section compares MT systems, when using automatically generated transcripts as input. For Russian and English, we used the Whisper <span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_italic">large-v3</span> model, while for Estonian, the finetuned Whisper model was used. All transcripts were generated using a beam size of 5, with speech activity detection activated in order to exclude non-speech segments from input. It can be seen that for Estonian source speech, using ASR instead of references transcripts deteriorates BLEU scores by around 3 points, while for Russian and English, the decrease in accuracy is larger, which is probably tied to the relatively low WER of Whisper on these datasets, as evident from Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Evaluation data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">The third section of rows compares the out-of-the-box performance of three publicly available end-to-end speech translation models. Whisper produced a segmented transcript directly from the long-form speech recordings, while for OWSM and SeamlessM4T, we segmented the speech into single-speaker chunks using pyannote 3.1 <cite class="ltx_cite ltx_citemacro_cite">Plaquet and Bredin (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. Decoding was performed using beam size of 5 for all models.
The BLEU scores of SeamlessM4T demonstrate the complexity of translating automatically segmented conversational speech, compared to read speech consisting of single utterances: compared to the BLEU scores of the same model on CoVoST 2 and FLEURS test data shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.2 End-to-end spoken language translation ‣ 2 Available models ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the scores on our evaluation data are lower by a large margin. Contrary to the Estonian-English results on CoVoST 2, Whisper outperforms SeamlessM4T on our data, suggesting that Whisper is better suited for processing conversational speech. OWSM 3.1 EBF, which has a BLEU score of 7.7 on English-Estonian CoVoST 2 data, has close to zero scores on our data in all directions.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">The last section of the table compares end-to-end speech translation models after finetuning with synthetic and/or web-scraped data. For Estonian-English and Estonian-Russian, finetuning on synthetic dataset outperforms web data by a large margin, which is expected based on the fact that the Estonian ASR comes from similar domains as evaluation data. In general, SeamlessM4T benefits more than Whisper from finetuning on properly segmented ASR data than from subtitles. This can be explained by the fact that subtitle start and end times are not always properly aligned with speech. For SeamlessM4T, which is finetuned on individual subtitle lines and the corresponding speech segments, this causes the training data to be often corrupted. Whisper, on the other hand, is trained on 30-second chunks of speech that fit typically several lines of subtitles, and the proper subtitle timing is not as important.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p">Apart from a few outliers, the performance of SeamlessM4T and Whisper are similar, especially in terms of BLEURT scores. This confirms our speculation that Whisper can be finetuned to translate into other directions than it was originally trained for. The performance of OWSM 3.1 EBF is however noticeably lower than for other models after finetuning on synthetic data and in order to save compute time we didn’t even finetune it on other datasets.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p">Since the differences between the BLEU scores from applying different models are relatively small, we used the Wilcoxon signed-rank test to assess whether the difference between the scores was statistically significant. We used BLEU scores of individual evaluation files as input to the paired test. Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Results and discussion ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> compares the difference between three systems: cascaded system involving Whisper and Google Translate and Whisper and SeamlessM4T end-to-end models, both finetuned using synthetic speech translation data. It can be seen that the best overall performance is achieved by the finetuned SeamlessM4T model, since no other model is significantly better in any of the directons, while it outperforms both the cascaded system and finetuned Whisper in the Estonian-Russian direction.</p>
</div>
<div id="S4.SS4.p8" class="ltx_para">
<p id="S4.SS4.p8.1" class="ltx_p">Although we haven’t performed proper human evaluation of the MT outputs, subjective evaluation by the authors suggests that our best Estonian-English and Estonian-Russian models produce translations that are accurate, fluent and therefore usable in many practical situations (see a translated TV news broadcast at <a target="_blank" href="https://www.youtube.com/watch?v=rZPqauCYfXI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/watch?v=rZPqauCYfXI</a>). For the opposite direction, the translations have a substantially lower quality by subjective evaluation. These findings correlate with BLEURT scores in Table <a href="#S4.T6" title="Table 6 ‣ 4.2.2 Synthetic data ‣ 4.2 Training data ‣ 4 Experimental results ‣ Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we demonstrated the effectiveness of finetuning end-to-end models for Estonian conversational speech translation using synthetic and web-scraped data. Our experiments revealed that synthetic data derived from ASR training corpora significantly enhances model performance, especially for Whisper and SeamlessM4T models. While all three evaluated models benefited from additional training data, SeamlessM4T worked the most consistently in all directions, indicating its robustness in handling conversational speech translation tasks. The best finetuned models are already usable for Estonian-English and Estonian-English directions for real-world speech data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The future direction of our research is experimenting with simultaneous speech translation where using end-to-end models is crucial.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alumäe et al. (2023)</span>
<span class="ltx_bibblock">
Tanel Alumäe, Joonas Kalda, Külliki Bode, and Martin Kaitsa. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.nodalida-1.49" title="" class="ltx_ref ltx_href">Automatic closed captioning for Estonian live broadcasts</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The 24th Nordic Conference on Computational Linguistics (NoDaLiDa)</em>, pages 492–499, Tórshavn, Faroe Islands.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ansari et al. (2021)</span>
<span class="ltx_bibblock">
Ebrahim Ansari, Ondrej Bojar, Barry Haddow, and Mohammad Mahmoudi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.EACL-DEMOS.9" title="" class="ltx_ref ltx_href">SLTEV: comprehensive evaluation of spoken language translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, EACL 2021</em>, pages 71–79, Online.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You, and Zhiyong Yan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/INTERSPEECH.2021-1965" title="" class="ltx_ref ltx_href">GigaSpeech: An evolving, multi-domain ASR corpus with 10 000 hours of transcribed audio</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association</em>, pages 3670–3674, Brno, Czechia.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2022)</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SLT54892.2023.10023141" title="" class="ltx_ref ltx_href">FLEURS: Few-shot learning evaluation of universal representations of speech</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Spoken Language Technology Workshop, SLT 2022</em>, pages 798–805, Doha, Qatar.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duquenne et al. (2023)</span>
<span class="ltx_bibblock">
Paul-Ambroise Duquenne, Holger Schwenk, and Benoît Sagot. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2308.11466" title="" class="ltx_ref ltx_href">SONAR: sentence-level multimodal and language-agnostic representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.11466.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gangi et al. (2019)</span>
<span class="ltx_bibblock">
Mattia Antonino Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/N19-1202" title="" class="ltx_ref ltx_href">MuST-C: a multilingual speech translation corpus</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, Volume 1 (Long and Short Papers)</em>, pages 2012–2017, USA.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izmailov et al. (2018)</span>
<span class="ltx_bibblock">
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://auai.org/uai2018/proceedings/papers/313.pdf" title="" class="ltx_ref ltx_href">Averaging weights leads to wider optima and better generalization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</em>, pages 876–885, Monterey, California, USA.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu Jeong Han, and Shinji Watanabe. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SLT54892.2023.10022656" title="" class="ltx_ref ltx_href">E-Branchformer: Branchformer with enhanced merging for speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Spoken Language Technology Workshop, SLT 2022</em>, pages 84–91, Doha, Qatar.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matusov et al. (2005)</span>
<span class="ltx_bibblock">
Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.isca-speech.org/archive/iwslt_05/slt5_138.html" title="" class="ltx_ref ltx_href">Evaluating machine translation output with automatic sentence segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2005 International Workshop on Spoken Language Translation, IWSLT 2005</em>, pages 138–144, Pittsburgh, PA, USA.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2207.04672" title="" class="ltx_ref ltx_href">No language left behind: Scaling human-centered machine translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2207.04672.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023a)</span>
<span class="ltx_bibblock">
Puyuan Peng, Brian Yan, Shinji Watanabe, and David Harwath. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2305.11095" title="" class="ltx_ref ltx_href">Prompting the hidden talent of web-scale speech models for zero-shot task generalization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.11095.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan S. Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-Weon Jung, Soumi Maiti, and Shinji Watanabe. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389676" title="" class="ltx_ref ltx_href">Reproducing Whisper-style training using an open-source toolkit and publicly available data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2023</em>, pages 1–8, Taipei, Taiwan.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plaquet and Bredin (2023)</span>
<span class="ltx_bibblock">
Alexis Plaquet and Hervé Bredin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.13025" title="" class="ltx_ref ltx_href">Powerset multi-class cross entropy loss for neural speaker diarization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.13025.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu et al. (2021)</span>
<span class="ltx_bibblock">
Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.58" title="" class="ltx_ref ltx_href">Learning compact metrics for MT</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event /</em>, pages 751–762, Punta Cana, Dominican Republic.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/radford23a.html" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2023, 23-29 July 2023</em>, volume 202 of <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 28492–28518, Honolulu, Hawaii, USA.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rehm et al. (2020)</span>
<span class="ltx_bibblock">
Georg Rehm, Katrin Marheinecke, Stefanie Hegele, Stelios Piperidis, Kalina Bontcheva, Jan Hajic, Khalid Choukri, Andrejs Vasiljevs, Gerhard Backfried, Christoph Prinz, José Manuél Gómez-Pérez, Luc Meertens, Paul Lukowicz, Josef van Genabith, Andrea Lösch, Philipp Slusallek, Morten Irgens, Patrick Gatellier, Joachim Köhler, Laure Le Bars, Dimitra Anastasiou, Albina Auksoriute, Núria Bel, António Branco, Gerhard Budin, Walter Daelemans, Koenraad De Smedt, Radovan Garabík, Maria Gavriilidou, Dagmar Gromann, Svetla Koeva, Simon Krek, Cvetana Krstev, Krister Lindén, Bernardo Magnini, Jan Odijk, Maciej Ogrodniczuk, Eiríkur Rögnvaldsson, Mike Rosner, Bolette S. Pedersen, Inguna Skadina, Marko Tadic, Dan Tufis, Tamás Váradi, Kadri Vider, Andy Way, and François Yvon. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.lrec-1.407/" title="" class="ltx_ref ltx_href">The European language technology landscape in 2020: Language-centric and human-centric AI for cross-cultural communication in multilingual Europe</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020</em>, pages 3322–3332, Marseille, France.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seamless Communication et al. (2023)</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. 2023.

</span>
<span class="ltx_bibblock">Seamless: Multilingual expressive and streaming speech translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.05187</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.704" title="" class="ltx_ref ltx_href">BLEURT: learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</em>, pages 7881–7892, Online.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tättar et al. (2022)</span>
<span class="ltx_bibblock">
Andre Tättar, Taido Purason, Hele-Andra Kuulmets, Agnes Luhtaru, Liisa Rätsep, Maali Tars, Marcis Pinnis, Toms Bergmanis, and Mark Fishel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.22364/BJMC.2022.10.3.15" title="" class="ltx_ref ltx_href">Open and competitive multilingual neural machine translation in production</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Balt. J. Mod. Comput.</em>, 10(3).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, and Emmanuel Dupoux. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.ACL-LONG.80" title="" class="ltx_ref ltx_href">VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers)</em>, pages 993–1003, Virtual Event.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, and Juan Miguel Pino. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2007.10310" title="" class="ltx_ref ltx_href">CoVoST 2: A massively multilingual speech-to-text translation corpus</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2007.10310.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe et al. (2018)</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/INTERSPEECH.2018-1456" title="" class="ltx_ref ltx_href">ESPnet: End-to-end speech processing toolkit</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association</em>, pages 2207–2211, Hyderabad, India.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.03808" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.03809" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.03809">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.03809" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.03810" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:07:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
