<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.07989] Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</title><meta property="og:description" content="Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.07989">

<!--Generated on Sun May  5 23:25:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Any2Point: Empowering Any-modality Large Models
<br class="ltx_break">for Efficient 3D Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yiwen Tang<sup id="id1.1.id1" class="ltx_sup">*1,3</sup>,
Jiaming Liu<sup id="id2.2.id2" class="ltx_sup">*2</sup>,
Dong Wang<sup id="id3.3.id3" class="ltx_sup">1</sup>,
Zhigang Wang<sup id="id4.4.id4" class="ltx_sup">1</sup>
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_bold">Shanghang Zhang<sup id="id5.5.id5.1" class="ltx_sup"><span id="id5.5.id5.1.1" class="ltx_text ltx_font_medium">2</span></sup>,
Bin Zhao<sup id="id5.5.id5.2" class="ltx_sup"><span id="id5.5.id5.2.1" class="ltx_text ltx_font_medium">1,3</span></sup>,
Xuelong Li<sup id="id5.5.id5.3" class="ltx_sup"><span id="id5.5.id5.3.1" class="ltx_text ltx_font_medium">1,4</span></sup>
<br class="ltx_break"><sup id="id5.5.id5.4" class="ltx_sup"><span id="id5.5.id5.4.1" class="ltx_text ltx_font_medium">1</span></sup></span>Shanghai AI Lab
<sup id="id6.6.id6" class="ltx_sup">2</sup>Peking University
<br class="ltx_break"><sup id="id7.7.id7" class="ltx_sup">3</sup>Northwestern Polytechnical University <sup id="id8.8.id8" class="ltx_sup">4</sup>China Telecom 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce <span id="id9.id1.1" class="ltx_text ltx_font_bold">Any2Point</span>, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors. Then, within each transformer block, we insert an any-to-3D guided adapter module for parameter-efficient fine-tuning. The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality transformers. We conduct extensive experiments to showcase the effectiveness and efficiency of our method. The code is released at <a target="_blank" href="https://github.com/Ivan-Tang-3D/Any2Point" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Ivan-Tang-3D/Any2Point</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Driven by the growing volume of model parameters and training data, large foundation models have gained unprecedented attention in a diverse array of domains and tasks. Numerous large models have been pre-trained for natural language process, including BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, and GPT series <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>); Floridi and Chiriatti (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, as well as visual understanding like DINOV2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, MAE <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>); Wei et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>); Xie et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite>, and ViT-22B <cite class="ltx_cite ltx_citemacro_cite">Dehghani et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>. Existing works <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>); Chen et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Jia et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> also explore efficient fine-tuning techniques to transfer pre-trained large models to a variety of downstream tasks, consistently achieving excellent performance. Meanwhile, 3D visual understanding <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>); Dai et al. (<a href="#bib.bib2" title="" class="ltx_ref">2017</a>); Qi et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017a</a>); Guo et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023b</a>)</cite> is also a significant topic, with its rich geometric representation contributing to the development of many applications (e.g., robotics <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>); Guo et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023c</a>)</cite> and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>); Pan et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>). Unfortunately, due to a lack of large-scale 3D data, the efforts towards 3D foundational modal are significantly lagging compared to language and 2D vision. Specifically, the acquisition and annotation of high-quality 3D data requires expensive resources and human labor, while synthetic 3D data training falls short of distribution diversity and real-world applications.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.07989/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Overview of Any2Point.</span> We propose a general framework for any-to-3D learning, which is shared for any modalities with parameter-efficient fine-tuning.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Therefore, some previous works have transferred pre-trained models from other modalities (mainly 2D vision) to 3D modality, leveraging sufficient pre-trained knowledge from diverse sources. We categorize existing 2D-to-3D works into two groups. <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">1) Data modality transformation.</span> This type of approach involves projecting 3D point clouds into 2D images <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>); Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>); Zhu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>, which are subsequently fed into 2D pre-trained models. Despite the promising performance on downstream tasks, the process of modality transformation inevitably causes the loss of spatial information in 3D data, hindering the full potential for 3D understanding. <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">2) Cross-modality knowledge distillation.</span> These approaches involve the pre-training knowledge transfer from 2D or vision-language models to a newly trained 3D model <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023b</a>); Dong et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>); Xue et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>. They are not only required to forward propagate both the 2D and 3D models during training, but also highly rely on the large-scale paired 2D-3D data. This leads to substantial computation costs and data engineering, limiting their ability for efficient implementation. Besides the aforementioned issues, more importantly, current methods mostly focus on the model adaption from 2D vision to 3D point clouds, rather than a shared methodology for other modalities.
Therefore, we pose a question: <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">can we develop a general any-to-3D paradigm that empowers any-modality large models for efficient and effective point cloud understanding?</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this issue, we propose Any2Point, a unified any-to-3D framework that transfers any 1D (language) or 2D (image/audio) large models to 3D domains with Parameter-Efficient Fine-Tuning (PEFT), as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Different from prior methods, our Any2Point avoids the point cloud projection, thereby mitigating the 3D information loss, and directly fine-tunes pre-trained models from source modalities, which saves resources by knowledge distillation. Specifically, given an any-modality pre-trained transformer, we first introduce a 3D-to-any (1D or 2D) virtual projection mechanism. This mechanism establishes a positional mapping between the input 3D points and their virtually projected 1D lines or 2D planes. This enables us to encode 3D coordinates using the original positional embeddings of the source modality of pre-trained large models.
In this way, we no longer need to conduct a true projection losing 3D geometries, while better promoting the pre-trained transformer to acquire 3D features with their original 1D/2D positional priors.
Then, for each transformer block, we insert an any-to-3D guided adapter module for PEFT. This adapter leverages the 1D/2D spatial guidance to aggregate the local semantics of 3D tokens, facilitating fine-grained feature interaction. Afterward, we perform an adaptive ensemble for the 3D features guided by different 1D/2D priors, which attains superior 3D representations.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Extensive experiments across various tasks demonstrate that our Any2Point framework achieves superior performance compared to current 3D pre-trained models, while utilizing only 1.0% of the trainable parameters. Using the pre-trained CLIP Text Encoder <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, Any2Point fine-tunes only 0.8M parameters and attains 91.9% on ScanObjectNN <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>, outperforming the previous state-of-the-art (SOTA) 3D pre-trained model by +1.3%, and 94.3% on ModelNet40 <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite>. Furthermore, Any2Point also achieves comparable results and efficiency by utilizing other pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>); Touvron et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>); Gong et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>); Girdhar et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> of different modalities, including 2D vision, language, and audio, validating the robustness of our approach. The contributions of our paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To enable a general any-to-3D transferring framework, we propose Any2Point, which empowers any-modality pre-trained large models (e.g., 2D vision, language, and audio) for efficient 3D understanding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce two techniques, i.e., 3D-to-any virtual projection and any-to-3D guided adapter, to effectively overcome the issues within current methods, such as 3D geometry loss and excessive resource cost.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Any2Point achieves superior performance compared to previous SOTA 3D pre-trained models across various tasks. Notably, these competitive results remain consistent by leveraging pre-trained models from different modalities, e.g., 2D vision, language, and audio.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Large-scale pre-trained models have achieved remarkable downstream performance in language, 2D vision, and audio processing. In the field of natural language field, BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> revolutionized natural language understanding by pre-training deep bidirectional representations from unlabeled text.
Building on this, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> enhances BERT by optimizing training and data scale, significantly boosting performance across language understanding benchmarks. GPT <cite class="ltx_cite ltx_citemacro_cite">Floridi and Chiriatti (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Radford et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> pioneers in generating coherent, contextually relevant text using a transformer model pre-trained on a diverse corpus of text. CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> further bridges visual and linguistic information by pre-training on a vast collection of image-text pairs.
In 2D vision, DeiT <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> achieves efficient image classification with transformers, using data augmentation and knowledge distillation for minimal data reliance. DINO V2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> also advances self-supervised learning with vision transformers through innovative self-distillation, requiring no labels. MAE <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> further proposes an asymmetric encoder and decoder to reconstruct images from masked data.
For audio processing, AST <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> transforms audio recognition to spectrograms by applying a vision transformer. SSAST <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> leverages self-supervised learning on unlabeled data with transformers for enhanced audio classification performance. ImageBind <cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> unifies multi-modal space by jointly training on multi-modal data including audio, improving multi-modal understanding and generation. Our method first utilizes the abundant knowledge from large models of any modalities and achieves 3D understanding capacity.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>2D-to-3D Transfer Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The paradigm of 2D-to-3D transfer learning aims to leverage the rich contextual and textural knowledge in the 2D domain to boost 3D understanding. Some works propose specific designs for 3D learning guided by 2D pre-trained knowledge, and achieve promising 3D understanding performance.
Image2Point <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite> proposes to transfer 2D semantics to 3D by convolutional layer inflating. ACT <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite> employs pre-trained 2D and language Transformers as cross-modal teachers for 3D learning via discrete variational autoencoding and prompt tuning. ULIP <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite> enhances 3D understanding performance by unifying image, text, and 3D point cloud representations. ReCon <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> leverages contrastive cross-modal learning and generative models for knowledge transfer. Meanwhile, PointCLIP V1 <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>)</cite> and V2 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite> first adopt CLIP’s 2D pre-trained knowledge on different 3D downstream tasks via projecting 3D point clouds to 2D images as input to the pre-trained backbone. P2P <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> also proposes 2D-to-3D projection through a learnable coloring module. Our approach skips the step of projecting point clouds, which reduces the loss of 3D geometric information, and fine-tunes pre-trained models instead of using computationally expensive knowledge distillation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parameter-Efficient Fine-tuning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The pre-training and fine-tuning paradigm has been proven highly effective in various tasks such as 2D visual recognition, language understanding, text-to-image generation, and audio recognition. However, fully fine-tuning the whole model becomes impractical as model volume increases exponentially. In contrast, the parameter-efficient fine-tuning (PEFT) methods <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Jia et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Chen et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Houlsby et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>); Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> aim at only updating a tiny part of the model’s parameters while freezing the rest parts, which have been proven to be effective and efficient on multiple popular pre-trained models such as BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>,
GPT <cite class="ltx_cite ltx_citemacro_cite">Floridi and Chiriatti (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, and Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">Rombach et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>.
The PEFT approaches can be generally categorized into three principal streams
namely prompt tuning <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Yang et al. (<a href="#bib.bib48" title="" class="ltx_ref">2024</a>)</cite>, reparameterization <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>, and adapters <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Houlsby et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>); Zhang et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023a</a>)</cite>. These techniques customize pre-trained models for specific tasks by fine-tuning prompts, modifying model parameters without altering the original architecture, and inserting lightweight trainable layers, respectively. Recently, Point-PEFT <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> first introduced PEFT techniques into 3D domains. In this paper, the Any2Point framework utilizes PEFT techniques to transfer Any-Modality pre-trained models to 3D understanding tasks at a low computational and storage cost.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Any2Point</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In Section <a href="#S3.SS1" title="3.1 Method Overview ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we first provide a paradigm overview of Any2Point, including the problem definition and network architecture. Then, in Section <a href="#S3.SS2" title="3.2 3D-to-any Virtual Projection ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and <a href="#S3.SS3" title="3.3 Any-to-3D Guided Adapter ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we respectively elaborate on the methodologies of our proposed two techniques for adapting any-modality large models for 3D domains.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.07989/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold">Overall Pipeline of Any2Point.</span> For efficiently fine-tuning Any-modality pre-trained models, our Any2Point framework contains two components: a 3D-to-any Virtual Projection, which pairs the pre-trained positional encodings with 3D tokens to avoid the 3D geometric information loss, and an Any-to-3D Guided Adapter to effectively grasp local structures. </figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Method Overview</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Problem Definition.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Given a pre-trained transformer from any modality, e.g., vision, language, and audio, our objective is to empower it with 3D understanding capabilities in an effective and efficient manner. Instead of employing full fine-tuning on 3D data, we seek a parameter-efficient solution with the source transformers frozen, since their large-scale parameters might cause high computation cost and over-fitting issues on the limited 3D dataset.
We generally divide the source models into two categories according to their pre-training data dimension, denoted as 1D and 2D transformers. The 1D transformers are specialized in processing sequential data, exemplified by language models like RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, and CLIP’s text encoder <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>. The 2D transformers are expert at 2D spatial data, including vision models, e.g., DINOv2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> and DeiT <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, and audio models, e.g., ImageBind Audio Encoder <cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> and SSAST <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Model Pipeline.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The overall paradigm of Any2Point is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To encode the input point cloud, we discard the original embedding modules in source transformers, e.g., tokenizers in 1D language models and convolutions in 2D vision/audio models, and employ a 3D mini-network for point cloud tokenization.
On top of this, the encoded 3D tokens are fed first into a 3D-to-any virtual projection module for positional encoding, and then into the frozen 1D/2D transformer with any-to-3D guided adapters. The former mechanism aims to assign each 3D token with positional information within the source modality, and the latter is designed for adaptive 1D/2D-guided 3D representation learning, which we will detail in the following sections. Note that, as the source transformers are kept frozen, only the initial tokenization network and the inserted adapters are learnable for parameter-efficient fine-tuning.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.07989/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">3D-to-any Virtual Projection.</span> To prevent the loss of 3D geometric information, the module assigns 3D tokens with the positional encodings that are paired with the pre-trained model.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>3D-to-any Virtual Projection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Many current 2D-to-3D methods <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>); Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>); Wang et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> project 3D point clouds into multi-view images to meet the input modality of pre-trained 2D models. This dimension reduction process potentially leads to the information loss of 3D geometries and deep measurements, enabling insufficient 3D feature encoding. In addition, these approaches are merely validated on the large models within 2D images, without considering other modalities like language and audio. Therefore, we propose a 3D-to-any virtual projection strategy that mitigates the geometric loss, and is generalizable to any 1D/2D pre-trained models, as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Model Pipeline. ‣ 3.1 Method Overview ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Tokenization in 3D Space.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.5" class="ltx_p">To avoid any information degradation, we directly tokenize the input point cloud within the 3D space for the subsequent 1D/2D transformer. Specifically, we employ a 3D mini-network containing small-scale parameters, which is a lighter-weight variant of Point-PN <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2023c</a>); Zhu et al. (<a href="#bib.bib56" title="" class="ltx_ref">2024</a>)</cite> . The tokenization process involves Farthest Point Sampling (FPS) <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017a</a>)</cite> for point number downsampling, <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation></semantics></math>-Nearest Neighbor (<math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">k</annotation></semantics></math>-NN) algorithm for local aggregation, and learnable linear layers for feature encoding. After this, we transform the raw point clouds into high-dimensional vectors, obtaining <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">N</annotation></semantics></math> 3D tokens as <math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\{T_{i}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><msubsup id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3"><eq id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\{T_{i}\}_{i=1}^{N}</annotation></semantics></math>, with <math id="S3.SS2.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\{p^{3D}_{i}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.1a"><msubsup id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.3.cmml">i</mi><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3"><times id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.2">3</cn><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3"><eq id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.1c">\{p^{3D}_{i}\}_{i=1}^{N}</annotation></semantics></math> denoting their 3D coordinates.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Motivations for Virtual Projection.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Positional encodings (PEs) serve as the only indicator for positional information to the transformer model, since the inner attention mechanism is permutation-invariant, treating every token at different orders all the same. Therefore, a straightforward way for 1D/2D transformers to comprehend 3D positional information is to integrate new 3D PEs with 3D tokens. However, the source transformers are pre-trained paired with their original PEs in 1D/2D space, which leads to semantic discrepancy between the frozen 1D/2D weights and newly learned 3D PEs. To address this issue, we virtually project 3D tokens into the source modality, and obtain the corresponding 1D/2D PEs for better aligning with the transformers.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">3D-to-2D Virtual Projection.</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.7" class="ltx_p">For 2D transformers in 2D vision and audio modalities, we virtually project each 3D coordinate, e.g., <math id="S3.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="p^{3D}_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><msubsup id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml">p</mi><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">i</mi><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3"><times id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.2">3</cn><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">p^{3D}_{i}</annotation></semantics></math>, into <math id="S3.SS2.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">M</annotation></semantics></math> views, deriving the corresponding 2D coordinates as <math id="S3.SS2.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="\{p^{2D}_{ij}\}_{j=1}^{M}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.3.m3.1a"><msubsup id="S3.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.2.cmml">p</mi><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3"><times id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.2">2</cn><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3"><times id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3"><eq id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.3.m3.1c">\{p^{2D}_{ij}\}_{j=1}^{M}</annotation></semantics></math>. The <math id="S3.SS2.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.4.m4.1c">M</annotation></semantics></math> different perspectives are capable of providing diverse positional relations within 2D space. We adopt a simple projection in PointCLIP <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>)</cite> without learnable parameters. Importantly, we do not truly produce the projected multi-view images, but only aim to obtain the virtual 2D positions. Then, according to the original 2D PEs within pre-trained transformers, we assign each 3D token, e.g., <math id="S3.SS2.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.5.m5.1a"><msub id="S3.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.5.m5.1c">T_{i}</annotation></semantics></math>, with <math id="S3.SS2.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.6.m6.1a"><mi id="S3.SS2.SSS0.Px3.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.6.m6.1b"><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.6.m6.1c">M</annotation></semantics></math> different PEs, denoted as <math id="S3.SS2.SSS0.Px3.p1.7.m7.1" class="ltx_Math" alttext="\{\operatorname{PE}^{2D}(p^{2D}_{ij})\}_{j=1}^{M}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.7.m7.1a"><msubsup id="S3.SS2.SSS0.Px3.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml"><msup id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.2.cmml">PE</mi><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.3.cmml">D</mi></mrow></msup><mo id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2a" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml">⁡</mo><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml">(</mo><msubsup id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.2.cmml">p</mi><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.3" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.7.m7.1b"><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2"><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.2">PE</ci><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3"><times id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.2">2</cn><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.1.1.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3"><times id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.2">2</cn><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3"><times id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.1"></times><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.2">𝑖</ci><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.1.1.1.2.2.1.3.3">𝑗</ci></apply></apply></apply></set><apply id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3"><eq id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.7.m7.1c">\{\operatorname{PE}^{2D}(p^{2D}_{ij})\}_{j=1}^{M}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">3D-to-1D Virtual Projection.</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.8" class="ltx_p">Similarly, for 1D transformers in language modality, we virtually project the 3D coordinates into different 1D lines. To align the number with 2D modality, we also select <math id="S3.SS2.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.1.m1.1c">M</annotation></semantics></math> lines passing through the center of the point cloud with <math id="S3.SS2.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.2.m2.1c">M</annotation></semantics></math> uniform rotation angles. For simplicity, we suppose the point cloud center as the origin, the unit direction vectors of <math id="S3.SS2.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.3.m3.1c">M</annotation></semantics></math> lines as <math id="S3.SS2.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="\{\vec{v}^{1D}_{j}\}_{j=1}^{M}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.4.m4.1a"><msubsup id="S3.SS2.SSS0.Px4.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.cmml"><mrow id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.1.cmml">→</mo></mover><mi id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.3.cmml">j</mi><mrow id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2"><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.1">→</ci><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.2.2">𝑣</ci></apply><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3"><times id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.2">1</cn><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3"><eq id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.4.m4.1c">\{\vec{v}^{1D}_{j}\}_{j=1}^{M}</annotation></semantics></math>, and the point coordinate, <math id="S3.SS2.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="p^{3D}_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.5.m5.1a"><msubsup id="S3.SS2.SSS0.Px4.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.2" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.2.cmml">p</mi><mi id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.3.cmml">i</mi><mrow id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.2" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.1" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.3" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3"><times id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.2">3</cn><ci id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS2.SSS0.Px4.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.5.m5.1c">p^{3D}_{i}</annotation></semantics></math>, vectorized as <math id="S3.SS2.SSS0.Px4.p1.6.m6.1" class="ltx_Math" alttext="\vec{p}^{3D}_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.6.m6.1a"><msubsup id="S3.SS2.SSS0.Px4.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.cmml"><mover accent="true" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.2" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.2.cmml">p</mi><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.1" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.1.cmml">→</mo></mover><mi id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.3.cmml">i</mi><mrow id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.2" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.1" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.3" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.3.cmml">D</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2"><ci id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.1">→</ci><ci id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.2.2">𝑝</ci></apply><apply id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3"><times id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.2">3</cn><ci id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS2.SSS0.Px4.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.6.m6.1c">\vec{p}^{3D}_{i}</annotation></semantics></math>. Then, the 1D coordinate of point <math id="S3.SS2.SSS0.Px4.p1.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px4.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px4.p1.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px4.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.7.m7.1c">i</annotation></semantics></math> in line <math id="S3.SS2.SSS0.Px4.p1.8.m8.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.8.m8.1a"><mi id="S3.SS2.SSS0.Px4.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px4.p1.8.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.8.m8.1b"><ci id="S3.SS2.SSS0.Px4.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.8.m8.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.8.m8.1c">j</annotation></semantics></math> is formulated by the dot production of</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="p^{1D}_{ij}=\vec{v}^{1D}_{j}\cdot\vec{p}^{3D}_{i}," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">p</mi><mrow id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.2.3.3.cmml">j</mi></mrow><mrow id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.3.3.cmml">D</mi></mrow></msubsup><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><msubsup id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.2.cmml">v</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3.2.2.2.1" xref="S3.E1.m1.1.1.1.1.3.2.2.2.1.cmml">→</mo></mover><mi id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">j</mi><mrow id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.3.2.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.2.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.2.2.3.1" xref="S3.E1.m1.1.1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.2.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.3.cmml">D</mi></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><msubsup id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.2.cmml">p</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3.3.2.2.1" xref="S3.E1.m1.1.1.1.1.3.3.2.2.1.cmml">→</mo></mover><mi id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">i</mi><mrow id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.3.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.2.3.1" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.3.cmml">D</mi></mrow></msubsup></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">𝑝</ci><apply id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"><times id="S3.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.2">1</cn><ci id="S3.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.3">𝐷</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><times id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1">⋅</ci><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2"><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2.1">→</ci><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2.2">𝑣</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3"><times id="S3.E1.m1.1.1.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3.2">1</cn><ci id="S3.E1.m1.1.1.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3.3">𝐷</ci></apply></apply><ci id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">𝑗</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2"><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2.1">→</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2.2">𝑝</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3"><times id="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.2">3</cn><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.3">𝐷</ci></apply></apply><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">p^{1D}_{ij}=\vec{v}^{1D}_{j}\cdot\vec{p}^{3D}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px4.p1.11" class="ltx_p">denoting the projected length. In this way, we refer to the original 1D PEs, and assign each 3D token, e.g., <math id="S3.SS2.SSS0.Px4.p1.9.m1.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.9.m1.1a"><msub id="S3.SS2.SSS0.Px4.p1.9.m1.1.1" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.9.m1.1b"><apply id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px4.p1.9.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.9.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.9.m1.1c">T_{i}</annotation></semantics></math>, with <math id="S3.SS2.SSS0.Px4.p1.10.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.10.m2.1a"><mi id="S3.SS2.SSS0.Px4.p1.10.m2.1.1" xref="S3.SS2.SSS0.Px4.p1.10.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.10.m2.1b"><ci id="S3.SS2.SSS0.Px4.p1.10.m2.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.10.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.10.m2.1c">M</annotation></semantics></math> different PEs as <math id="S3.SS2.SSS0.Px4.p1.11.m3.1" class="ltx_Math" alttext="\{\operatorname{PE}^{1D}(p^{1D}_{ij})\}_{j=1}^{M}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.11.m3.1a"><msubsup id="S3.SS2.SSS0.Px4.p1.11.m3.1.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.cmml"><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml"><msup id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.2.cmml">PE</mi><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.3.cmml">D</mi></mrow></msup><mo id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2a" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml">⁡</mo><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml">(</mo><msubsup id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.cmml"><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.2.cmml">p</mi><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.cmml"><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.cmml"><mn id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.2" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.1" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.3" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.11.m3.1b"><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1">superscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1">subscript</csymbol><set id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1"><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2"><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.2">PE</ci><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3"><times id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.1.1.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.2">𝑝</ci><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3"><times id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.2">1</cn><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3"><times id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.1"></times><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.2">𝑖</ci><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.1.1.1.2.2.1.3.3">𝑗</ci></apply></apply></apply></set><apply id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3"><eq id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.1"></eq><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.SSS0.Px4.p1.11.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.11.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.11.m3.1c">\{\operatorname{PE}^{1D}(p^{1D}_{ij})\}_{j=1}^{M}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Encoding 3D Positions in 1D/2D PEs.</h4>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">After acquiring the corresponding 1D/2D PEs, we average them as an overall positional indicator, and incorporate it with the 3D token, e.g., <math id="S3.SS2.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S3.SS2.SSS0.Px5.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px5.p1.1.m1.1c">T_{i}</annotation></semantics></math>, by</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="T_{i}^{in}=T_{i}+\frac{1}{M}\sum_{j=1}^{M}\operatorname{PE}^{1D/2D}(p^{1D/2D}_{ij})." display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.cmml">T</mi><mi id="S3.E2.m1.1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.1.4.2.3.cmml">i</mi><mrow id="S3.E2.m1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.4.3.cmml"><mi id="S3.E2.m1.1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.4.3.1" xref="S3.E2.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.4.3.3" xref="S3.E2.m1.1.1.1.1.4.3.3.cmml">n</mi></mrow></msubsup><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.4.2" xref="S3.E2.m1.1.1.1.1.2.4.2.cmml">T</mi><mi id="S3.E2.m1.1.1.1.1.2.4.3" xref="S3.E2.m1.1.1.1.1.2.4.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml"><mfrac id="S3.E2.m1.1.1.1.1.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.4.cmml"><mn id="S3.E2.m1.1.1.1.1.2.2.4.2" xref="S3.E2.m1.1.1.1.1.2.2.4.2.cmml">1</mn><mi id="S3.E2.m1.1.1.1.1.2.2.4.3" xref="S3.E2.m1.1.1.1.1.2.2.4.3.cmml">M</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml"><munderover id="S3.E2.m1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.1.2.2.2.3.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.2.cmml">j</mi><mo id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.3.cmml">M</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml"><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">PE</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml">D</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">/</mo><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">D</mi></mrow></msup><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.2a" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml">⁡</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.2.cmml">p</mi><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.3.cmml">j</mi></mrow><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.cmml"><mn id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.3.cmml">D</mi></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.1.cmml">/</mo><mn id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></eq><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.4">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2">𝑇</ci><ci id="S3.E2.m1.1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3"><times id="S3.E2.m1.1.1.1.1.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.4.3.1"></times><ci id="S3.E2.m1.1.1.1.1.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.4.3.2">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.4.3.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3.3">𝑛</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><plus id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2">𝑇</ci><ci id="S3.E2.m1.1.1.1.1.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.4.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"></times><apply id="S3.E2.m1.1.1.1.1.2.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4"><divide id="S3.E2.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4"></divide><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.2">1</cn><ci id="S3.E2.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.3">𝑀</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.2.2.2.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3"><eq id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.2">𝑗</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.3">𝑀</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2">PE</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2"><divide id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.1"></divide><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.1"></times><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.3">𝐷</ci></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">2</cn></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝐷</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.2">𝑝</ci><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.1"></times><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2"><divide id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.1"></divide><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.1"></times><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.2">1</cn><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.2.3">𝐷</ci></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.2.3">2</cn></apply><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3"><times id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.2">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">T_{i}^{in}=T_{i}+\frac{1}{M}\sum_{j=1}^{M}\operatorname{PE}^{1D/2D}(p^{1D/2D}_{ij}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px5.p1.2" class="ltx_p">With this approach, we inject sufficient positional information of the source modality into 3D tokens to better collaborate with the frozen transformer, while mitigating the information loss of the true projection.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.07989/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="433" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.2.1" class="ltx_text ltx_font_bold">Any-to-3D Guided Adapter.</span> Inserted into every transformer block, the adapter leverages the 1D/2D-guided Local Aggregation module to capture 3D local semantics and utilizes the Adaptive Any-to-3D Ensemble to obtain high-quality features.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Any-to-3D Guided Adapter</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Different from existing distillation-based methods <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023b</a>); Guo et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023a</a>)</cite> training a new 3D network, we directly feed the encoded 3D tokens <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\{T^{in}_{ij}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msubsup id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.2.cmml">T</mi><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.3.cmml">n</mi></mrow></msubsup><mo stretchy="false" id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p1.1.m1.1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p1.1.m1.1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><set id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.2">𝑇</ci><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3"><times id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.3.3">𝑛</ci></apply></apply><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3"><times id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set><apply id="S3.SS3.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><eq id="S3.SS3.p1.1.m1.1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.1"></eq><ci id="S3.SS3.p1.1.m1.1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\{T^{in}_{ij}\}_{i=1}^{N}</annotation></semantics></math> to the pre-trained 1D/2D transformer. Although the PEs of 3D tokens have been aligned with the source model, the entirely frozen weights pre-trained by other modalities are still restricted to learning superior 3D representations. Considering this, we introduce a learnable any-to-3D guided adapter within each transformer block, as shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Encoding 3D Positions in 1D/2D PEs. ‣ 3.2 3D-to-any Virtual Projection ‣ 3 Any2Point ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The adapters are inserted after the Feed-Forward Networks (FFNs), and further incorporate 1D/2D-prior knowledge for parameter-efficient fine-tuning.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Motivations for Inserting Adapters.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">The self-attention mechanisms within source transformers normally focus on long-range token interaction in global contexts, which lacks local feature extraction. However, the detailed spatial geometries are also significant for the fine-grained understanding of 3D shapes. To complement the gap, we utilize the proposed adapter layers for specifically capturing 3D semantics within local neighborhoods. In addition, as the source transformers are powered by 1D/2D PEs as discussed above, the naive FPS and <math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">k</annotation></semantics></math>-NN for 3D local grouping might cause positional discrepancy. Therefore, we further design a 1D/2D-guided aggregation strategy and an adaptive any-to-3D ensemble approach for robust 3D fine-grained encoding.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">1D/2D-guided Local Aggregation.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.6" class="ltx_p">Within the adapter, we first group 3D tokens into different local neighborhoods guided by 1D/2D positional priors, which better align the adopted 1D/2D PEs. For <math id="S3.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">M</annotation></semantics></math> different views/lines, we conduct <math id="S3.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">M</annotation></semantics></math> concurrent local aggregation process to make the best of different projection perspectives. Specifically, for 2D transformers, we divide each virtually projected image, e.g., the <math id="S3.SS3.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">j</annotation></semantics></math>-th view, into uniform local 2D patches, and group the 3D tokens within the same patch into a neighborhood, according to their 2D positions <math id="S3.SS3.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\{p^{2D}_{ij}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><msubsup id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml"><mrow id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.2.cmml">p</mi><mrow id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><set id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1"><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.2">𝑝</ci><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3"><times id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.1"></times><cn type="integer" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.2">2</cn><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3"><times id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3"><eq id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.1"></eq><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">\{p^{2D}_{ij}\}_{i=1}^{N}</annotation></semantics></math>. For 1D transformers, we similarly divide each virtually projected line, e.g., the <math id="S3.SS3.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">j</annotation></semantics></math>-th direction, into uniform local 1D segments, and group the 3D tokens within different segments referring to their 1D positions <math id="S3.SS3.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\{p^{1D}_{ij}\}_{i=1}^{N}" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><msubsup id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.2.cmml">p</mi><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.cmml"><mn id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.3.cmml">D</mi></mrow></msubsup><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">superscript</csymbol><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><set id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.2">𝑝</ci><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3"><times id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.1"></times><cn type="integer" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.2">1</cn><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3"><times id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3"><eq id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.1"></eq><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">\{p^{1D}_{ij}\}_{i=1}^{N}</annotation></semantics></math>. On top of this, we adopt a self-attention layer for 3D tokens within each 1D/2D neighborhoods, performing local feature interaction guided by 1D/2D priors. Then we employ the operations of pooling and propagation to propagate the local aggregated feature to every points within the same neighborhood.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Adaptive Any-to-3D Ensemble.</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.7" class="ltx_p">After the parallel local aggregation, we obtain <math id="S3.SS3.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">M</annotation></semantics></math> sets of 3D tokens, each representing a 2D view or 1D line. As different projection perspectives normally showcase different significance for 3D representations, we propose an adaptive any-to-3D ensemble approach to aggregate the <math id="S3.SS3.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.2.m2.1c">M</annotation></semantics></math> features for each token. We denote the <math id="S3.SS3.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.3.m3.1c">i</annotation></semantics></math>-th 3D token with <math id="S3.SS3.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.4.m4.1c">M</annotation></semantics></math> sets of features at this stage as <math id="S3.SS3.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="\{F_{ij}\}_{j=1}^{M}" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.5.m5.1a"><msubsup id="S3.SS3.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.cmml"><mrow id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.2.cmml">F</mi><mrow id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.2" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.2" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.2.cmml">j</mi><mo id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1">superscript</csymbol><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1">subscript</csymbol><set id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1"><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.2">𝐹</ci><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3"><times id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3"><eq id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.1"></eq><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.5.m5.1c">\{F_{ij}\}_{j=1}^{M}</annotation></semantics></math>. To properly indicate the relative importance of each view/line, we additionally employ a 3D feature transformation branch independent of the <math id="S3.SS3.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.6.m6.1a"><mi id="S3.SS3.SSS0.Px3.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px3.p1.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.6.m6.1b"><ci id="S3.SS3.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.6.m6.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.6.m6.1c">M</annotation></semantics></math> 2D-guided local aggregation. This non-parametric branch only contains the local grouping in 3D space, feature average pooling within local groups, and propagation operations, converting the 3D token before the adapter into a feature baseline for adaptive ensemble, denoted as <math id="S3.SS3.SSS0.Px3.p1.7.m7.1" class="ltx_Math" alttext="B_{i}" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.7.m7.1a"><msub id="S3.SS3.SSS0.Px3.p1.7.m7.1.1" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.2" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1.2.cmml">B</mi><mi id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.3" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.7.m7.1b"><apply id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1.2">𝐵</ci><ci id="S3.SS3.SSS0.Px3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.7.m7.1c">B_{i}</annotation></semantics></math>. Then, we calculate the relative weights for different views/lines by the cosine similarity, and finally aggregate their features to obtain the final output as</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="T_{i}^{out}=\frac{1}{M}\sum_{j=1}^{M}\operatorname{Sim}(B_{i},F_{ij})." display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><msubsup id="S3.E3.m1.2.2.1.1.4" xref="S3.E3.m1.2.2.1.1.4.cmml"><mi id="S3.E3.m1.2.2.1.1.4.2.2" xref="S3.E3.m1.2.2.1.1.4.2.2.cmml">T</mi><mi id="S3.E3.m1.2.2.1.1.4.2.3" xref="S3.E3.m1.2.2.1.1.4.2.3.cmml">i</mi><mrow id="S3.E3.m1.2.2.1.1.4.3" xref="S3.E3.m1.2.2.1.1.4.3.cmml"><mi id="S3.E3.m1.2.2.1.1.4.3.2" xref="S3.E3.m1.2.2.1.1.4.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.4.3.1" xref="S3.E3.m1.2.2.1.1.4.3.1.cmml">​</mo><mi id="S3.E3.m1.2.2.1.1.4.3.3" xref="S3.E3.m1.2.2.1.1.4.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.4.3.1a" xref="S3.E3.m1.2.2.1.1.4.3.1.cmml">​</mo><mi id="S3.E3.m1.2.2.1.1.4.3.4" xref="S3.E3.m1.2.2.1.1.4.3.4.cmml">t</mi></mrow></msubsup><mo id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><mfrac id="S3.E3.m1.2.2.1.1.2.4" xref="S3.E3.m1.2.2.1.1.2.4.cmml"><mn id="S3.E3.m1.2.2.1.1.2.4.2" xref="S3.E3.m1.2.2.1.1.2.4.2.cmml">1</mn><mi id="S3.E3.m1.2.2.1.1.2.4.3" xref="S3.E3.m1.2.2.1.1.2.4.3.cmml">M</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.2.3" xref="S3.E3.m1.2.2.1.1.2.3.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.2.2" xref="S3.E3.m1.2.2.1.1.2.2.cmml"><munderover id="S3.E3.m1.2.2.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.3.cmml"><mo movablelimits="false" id="S3.E3.m1.2.2.1.1.2.2.3.2.2" xref="S3.E3.m1.2.2.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E3.m1.2.2.1.1.2.2.3.2.3" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.3.2.3.2" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.2.cmml">j</mi><mo id="S3.E3.m1.2.2.1.1.2.2.3.2.3.1" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E3.m1.2.2.1.1.2.2.3.2.3.3" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.2.2.1.1.2.2.3.3" xref="S3.E3.m1.2.2.1.1.2.2.3.3.cmml">M</mi></munderover><mrow id="S3.E3.m1.2.2.1.1.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml"><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Sim</mi><mo id="S3.E3.m1.2.2.1.1.2.2.2.2a" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">⁡</mo><mrow id="S3.E3.m1.2.2.1.1.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">B</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.2.2.2.2.2.4" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.2.cmml">F</mi><mrow id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.2" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.1" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.3" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E3.m1.2.2.1.1.2.2.2.2.2.5" xref="S3.E3.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"></eq><apply id="S3.E3.m1.2.2.1.1.4.cmml" xref="S3.E3.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.4.1.cmml" xref="S3.E3.m1.2.2.1.1.4">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.4.2.cmml" xref="S3.E3.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.4.2.1.cmml" xref="S3.E3.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.4.2.2.cmml" xref="S3.E3.m1.2.2.1.1.4.2.2">𝑇</ci><ci id="S3.E3.m1.2.2.1.1.4.2.3.cmml" xref="S3.E3.m1.2.2.1.1.4.2.3">𝑖</ci></apply><apply id="S3.E3.m1.2.2.1.1.4.3.cmml" xref="S3.E3.m1.2.2.1.1.4.3"><times id="S3.E3.m1.2.2.1.1.4.3.1.cmml" xref="S3.E3.m1.2.2.1.1.4.3.1"></times><ci id="S3.E3.m1.2.2.1.1.4.3.2.cmml" xref="S3.E3.m1.2.2.1.1.4.3.2">𝑜</ci><ci id="S3.E3.m1.2.2.1.1.4.3.3.cmml" xref="S3.E3.m1.2.2.1.1.4.3.3">𝑢</ci><ci id="S3.E3.m1.2.2.1.1.4.3.4.cmml" xref="S3.E3.m1.2.2.1.1.4.3.4">𝑡</ci></apply></apply><apply id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><times id="S3.E3.m1.2.2.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.3"></times><apply id="S3.E3.m1.2.2.1.1.2.4.cmml" xref="S3.E3.m1.2.2.1.1.2.4"><divide id="S3.E3.m1.2.2.1.1.2.4.1.cmml" xref="S3.E3.m1.2.2.1.1.2.4"></divide><cn type="integer" id="S3.E3.m1.2.2.1.1.2.4.2.cmml" xref="S3.E3.m1.2.2.1.1.2.4.2">1</cn><ci id="S3.E3.m1.2.2.1.1.2.4.3.cmml" xref="S3.E3.m1.2.2.1.1.2.4.3">𝑀</ci></apply><apply id="S3.E3.m1.2.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2"><apply id="S3.E3.m1.2.2.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3">subscript</csymbol><sum id="S3.E3.m1.2.2.1.1.2.2.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.2.2"></sum><apply id="S3.E3.m1.2.2.1.1.2.2.3.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3"><eq id="S3.E3.m1.2.2.1.1.2.2.3.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.1"></eq><ci id="S3.E3.m1.2.2.1.1.2.2.3.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.2">𝑗</ci><cn type="integer" id="S3.E3.m1.2.2.1.1.2.2.3.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.2.2.1.1.2.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3.3">𝑀</ci></apply><apply id="S3.E3.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">Sim</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2">𝐵</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.2">𝐹</ci><apply id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3"><times id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.1"></times><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.2">𝑖</ci><ci id="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2.2.2.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">T_{i}^{out}=\frac{1}{M}\sum_{j=1}^{M}\operatorname{Sim}(B_{i},F_{ij}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.SSS0.Px3.p1.8" class="ltx_p">With the ensemble strategy, we integrate <math id="S3.SS3.SSS0.Px3.p1.8.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.SSS0.Px3.p1.8.m1.1a"><mi id="S3.SS3.SSS0.Px3.p1.8.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.8.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.8.m1.1b"><ci id="S3.SS3.SSS0.Px3.p1.8.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.8.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.8.m1.1c">M</annotation></semantics></math> different features with dynamic weights, enabling the adapter to adaptively determine which view/line is more critical, contributing to high-quality adapted features.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we conduct extensive experiments on the ScanObjectNN <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> and ModelNet40 <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite> datasets. We first introduce the fine-tuning settings and implementation details in Section <a href="#S4.SS1" title="4.1 Experimental Settings ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Then, in Section <a href="#S4.SS2" title="4.2 Quantitative Analysis ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we present the main experiment of transferring any-modality large models (language, 2D image and audio) to 3D classification tasks. Finally, in Section <a href="#S4.SS3" title="4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we conduct ablation studies to evaluate each component within our proposed Any2Point framework.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">ScanObjectNN.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">The ScanObjectNN dataset <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> consists of real-world 3D object scans, categorized into 15 distinct classes. We select the most challenging PB-T50-RS split to test the performance of the Any2Point framework without the voting strategy. For all models, we employ the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> and the CosineAnnealing scheduler. The initial learning rate is set to 5e-4, with a weight decay factor of 0.05. We fine-tune the model for 300 epochs with a batch size of 32. For data augmentation, we use Random scaling, translation, and rotation. For language, 2D vision, and audio modalities, we respectively select the CLIP Text Encoder <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, DINO V2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, and ImageBind Audio Encoder <cite class="ltx_cite ltx_citemacro_cite">Girdhar et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> as pre-trained models. For these three models, the transformer architecture is the same: a 12-block encoder with 768 feature channels and 1,024 input point number. The hyperparameter M in the 3D-to-any Virtual Projection is set to 6 with identical angles for the Any-Modality Transformers.
To match the shape of the original PEs within pre-trained models, we virtually project 3D points into a 1D line segment of length 77 with a line size of 2 in the language modality; a 2D plane measuring 512x512 with a patch size of 26 in the 2D vision modality; and a 2D plane sized 192x304 with a patch size of 16 in the audio modality.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">ModelNet40.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The ModelNet40 dataset <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite> consists of 40 categories of synthesized 3D CAD models, with 9,843 training samples and 2,468 test samples. In our experiments on ModelNet40, we adopt the same fine-tuning settings and the same pre-trained models as in ScanObjectNN. For data augmentation, we utilize default random scaling and translation. Notably, during the testing process, we do not employ the voting strategy.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.6.1" class="ltx_text ltx_font_bold">Comparisons on accuracy with previous methods on 3D classification datasets.</span> We report the pre-training modality (Pre-train), the number of learnable parameters (#Param) on the "PB-T50-RS" split of ScanObjectNN (SCAN.) and ModelNet40 (MN.).<sup id="S4.T1.7.2" class="ltx_sup">†</sup> indicates utilizing the voting strategy.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.3.2" class="ltx_tr">
<td id="S4.T1.3.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">
<span id="S4.T1.3.2.1.1" class="ltx_text"></span> <span id="S4.T1.3.2.1.2" class="ltx_text">
<span id="S4.T1.3.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.3.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.3.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="S4.T1.3.2.1.3" class="ltx_text"></span></td>
<td id="S4.T1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.3.2.2.1" class="ltx_text ltx_font_bold">Pre-train</span></td>
<td id="S4.T1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.3.2.3.1" class="ltx_text ltx_font_bold">#Param(M)</span></td>
<td id="S4.T1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.3.2.4.1" class="ltx_text ltx_font_bold">SCAN.(%)</span></td>
<td id="S4.T1.3.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.3.2.5.1" class="ltx_text ltx_font_bold">MN.(%)</span></td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Point-NN <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.9</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">81.8</td>
</tr>
<tr id="S4.T1.3.4" class="ltx_tr">
<td id="S4.T1.3.4.1" class="ltx_td ltx_align_left ltx_border_r">PointNet <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017a</a>)</cite>
</td>
<td id="S4.T1.3.4.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.4.3" class="ltx_td ltx_align_center ltx_border_r">3.5</td>
<td id="S4.T1.3.4.4" class="ltx_td ltx_align_center ltx_border_r">68.0</td>
<td id="S4.T1.3.4.5" class="ltx_td ltx_align_center">89.2</td>
</tr>
<tr id="S4.T1.3.5" class="ltx_tr">
<td id="S4.T1.3.5.1" class="ltx_td ltx_align_left ltx_border_r">PointNet++ <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib29" title="" class="ltx_ref">2017b</a>)</cite>
</td>
<td id="S4.T1.3.5.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.5.3" class="ltx_td ltx_align_center ltx_border_r">1.5</td>
<td id="S4.T1.3.5.4" class="ltx_td ltx_align_center ltx_border_r">77.9</td>
<td id="S4.T1.3.5.5" class="ltx_td ltx_align_center">90.7</td>
</tr>
<tr id="S4.T1.3.6" class="ltx_tr">
<td id="S4.T1.3.6.1" class="ltx_td ltx_align_left ltx_border_r">DGCNN<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T1.3.6.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.6.3" class="ltx_td ltx_align_center ltx_border_r">1.8</td>
<td id="S4.T1.3.6.4" class="ltx_td ltx_align_center ltx_border_r">78.1</td>
<td id="S4.T1.3.6.5" class="ltx_td ltx_align_center">92.9</td>
</tr>
<tr id="S4.T1.3.7" class="ltx_tr">
<td id="S4.T1.3.7.1" class="ltx_td ltx_align_left ltx_border_r">PointMLP <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.7.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.7.3" class="ltx_td ltx_align_center ltx_border_r">12.6</td>
<td id="S4.T1.3.7.4" class="ltx_td ltx_align_center ltx_border_r">85.4</td>
<td id="S4.T1.3.7.5" class="ltx_td ltx_align_center">94.1</td>
</tr>
<tr id="S4.T1.3.8" class="ltx_tr">
<td id="S4.T1.3.8.1" class="ltx_td ltx_align_left ltx_border_r">Point-PN <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S4.T1.3.8.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.8.3" class="ltx_td ltx_align_center ltx_border_r">0.8</td>
<td id="S4.T1.3.8.4" class="ltx_td ltx_align_center ltx_border_r">87.1</td>
<td id="S4.T1.3.8.5" class="ltx_td ltx_align_center">93.8</td>
</tr>
<tr id="S4.T1.3.9" class="ltx_tr">
<td id="S4.T1.3.9.1" class="ltx_td ltx_align_left ltx_border_r">PointNeXt <cite class="ltx_cite ltx_citemacro_cite">Qian et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.9.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S4.T1.3.9.3" class="ltx_td ltx_align_center ltx_border_r">1.4</td>
<td id="S4.T1.3.9.4" class="ltx_td ltx_align_center ltx_border_r">87.7</td>
<td id="S4.T1.3.9.5" class="ltx_td ltx_align_center">94.0</td>
</tr>
<tr id="S4.T1.3.10" class="ltx_tr">
<td id="S4.T1.3.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Point-BERT <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D</td>
<td id="S4.T1.3.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.1</td>
<td id="S4.T1.3.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.1</td>
<td id="S4.T1.3.10.5" class="ltx_td ltx_align_center ltx_border_t">92.7</td>
</tr>
<tr id="S4.T1.3.11" class="ltx_tr">
<td id="S4.T1.3.11.1" class="ltx_td ltx_align_left ltx_border_r">   w/ Point-PEFT <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S4.T1.3.11.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T1.3.11.3" class="ltx_td ltx_align_center ltx_border_r">0.6</td>
<td id="S4.T1.3.11.4" class="ltx_td ltx_align_center ltx_border_r">85.0</td>
<td id="S4.T1.3.11.5" class="ltx_td ltx_align_center">93.4</td>
</tr>
<tr id="S4.T1.3.12" class="ltx_tr">
<td id="S4.T1.3.12.1" class="ltx_td ltx_align_left ltx_border_r">Point-MAE <cite class="ltx_cite ltx_citemacro_cite">Pang et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.12.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T1.3.12.3" class="ltx_td ltx_align_center ltx_border_r">22.1</td>
<td id="S4.T1.3.12.4" class="ltx_td ltx_align_center ltx_border_r">85.2</td>
<td id="S4.T1.3.12.5" class="ltx_td ltx_align_center">93.2</td>
</tr>
<tr id="S4.T1.3.13" class="ltx_tr">
<td id="S4.T1.3.13.1" class="ltx_td ltx_align_left ltx_border_r">Point-M2AE <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib50" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S4.T1.3.13.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T1.3.13.3" class="ltx_td ltx_align_center ltx_border_r">15.3</td>
<td id="S4.T1.3.13.4" class="ltx_td ltx_align_center ltx_border_r">86.4</td>
<td id="S4.T1.3.13.5" class="ltx_td ltx_align_center">93.4</td>
</tr>
<tr id="S4.T1.3.1" class="ltx_tr">
<td id="S4.T1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">P2P-HorNet <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D</td>
<td id="S4.T1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.2</td>
<td id="S4.T1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.3</td>
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">94.0<sup id="S4.T1.3.1.1.1" class="ltx_sup">†</sup>
</td>
</tr>
<tr id="S4.T1.3.14" class="ltx_tr">
<td id="S4.T1.3.14.1" class="ltx_td ltx_align_left ltx_border_r">ACT <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.3.14.2" class="ltx_td ltx_align_center ltx_border_r">3D+2D</td>
<td id="S4.T1.3.14.3" class="ltx_td ltx_align_center ltx_border_r">22.1</td>
<td id="S4.T1.3.14.4" class="ltx_td ltx_align_center ltx_border_r">88.2</td>
<td id="S4.T1.3.14.5" class="ltx_td ltx_align_center">93.7</td>
</tr>
<tr id="S4.T1.3.15" class="ltx_tr">
<td id="S4.T1.3.15.1" class="ltx_td ltx_align_left ltx_border_r">I2P-MAE <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S4.T1.3.15.2" class="ltx_td ltx_align_center ltx_border_r">3D+2D</td>
<td id="S4.T1.3.15.3" class="ltx_td ltx_align_center ltx_border_r">12.9</td>
<td id="S4.T1.3.15.4" class="ltx_td ltx_align_center ltx_border_r">90.1</td>
<td id="S4.T1.3.15.5" class="ltx_td ltx_align_center">93.7</td>
</tr>
<tr id="S4.T1.3.16" class="ltx_tr">
<td id="S4.T1.3.16.1" class="ltx_td ltx_align_left ltx_border_r">ReCon <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S4.T1.3.16.2" class="ltx_td ltx_align_center ltx_border_r">3D+2D+Language</td>
<td id="S4.T1.3.16.3" class="ltx_td ltx_align_center ltx_border_r">43.6</td>
<td id="S4.T1.3.16.4" class="ltx_td ltx_align_center ltx_border_r">90.6</td>
<td id="S4.T1.3.16.5" class="ltx_td ltx_align_center">94.1</td>
</tr>
<tr id="S4.T1.3.17" class="ltx_tr">
<td id="S4.T1.3.17.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.3.17.1.1" class="ltx_text">Any2Point</span></td>
<td id="S4.T1.3.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Audio</td>
<td id="S4.T1.3.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.17.3.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T1.3.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.17.4.1" class="ltx_text ltx_font_bold">87.0</span></td>
<td id="S4.T1.3.17.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.17.5.1" class="ltx_text ltx_font_bold">92.7</span></td>
</tr>
<tr id="S4.T1.3.18" class="ltx_tr">
<td id="S4.T1.3.18.1" class="ltx_td ltx_align_center ltx_border_r">2D</td>
<td id="S4.T1.3.18.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.18.2.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T1.3.18.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.18.3.1" class="ltx_text ltx_font_bold">87.7</span></td>
<td id="S4.T1.3.18.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.18.4.1" class="ltx_text ltx_font_bold">93.2</span></td>
</tr>
<tr id="S4.T1.3.19" class="ltx_tr">
<td id="S4.T1.3.19.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Language</td>
<td id="S4.T1.3.19.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.19.2.1" class="ltx_text ltx_font_bold">0.9</span></td>
<td id="S4.T1.3.19.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.19.3.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S4.T1.3.19.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.19.4.1" class="ltx_text ltx_font_bold">94.3</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The results are shown in Table <a href="#S4.T1" title="Table 1 ‣ ModelNet40. ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It is observed that: (i) On the 3D real-world object dataset ScanObjectNN, the Any2Point framework achieves 91.9%, 87.7%, and 87.0% accuracy based on Language (CLIP-Text), 2D Vision (DINO V2-B), and Audio (ImageBind-Audio) modalities, respectively. Compared to the previous SOTA method (ReCon), 1D language pre-trained Any2Point achieves a 1.3% improvement with only 0.9M learnable parameters. For the 2D (Vision/Audio) modalities, Any2Point significantly outperforms Point-M2AE, which is the SOTA method pre-trained only on 3D datasets, by 0.6% and 1.3%, respectively. This reveals that our framework is capable of fully exploiting pre-trained knowledge from other modalities to solve 3D recognition tasks.
(ii) On the 3D synthetic object dataset ModelNet40, across the Language, 2D Vision, and Audio modalities, our Any2Point framework attains 94.3%, 93.2%, and 92.7%.
Our framework exclusively utilizes one pre-trained model in the 1D language modality, achieving a 0.2% improvement over the previous SOTA method (ReCon), and reducing 42.7M learnable parameters.
For 2D modalities, Any2Point demonstrates performance on par with models pre-trained exclusively on 3D datasets.
(iii) Surprisingly, whether on the ScanObjectNN or the ModelNet40 dataset, the Any2Point framework maintains a performance trend where 1D modality (language) outperforms 2D modalities (image and audio). Large language models provide abundant spatial and semantic information in low-dimensional spaces to assist in 3D learning. This trend is further validated in the upcoming Section <a href="#S4.SS3" title="4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we conduct extensive ablation studies to explore the effectiveness of different components within our Any2Point framework. We adopt CLIP-Text (1D) and DINO V2 (2D) as the pre-trained transformer, and report the classification accuracy (%) on the "PB-T50-RS" split of the ScanObjectNN dataset.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.2.1" class="ltx_text ltx_font_bold">Ablation Study on Different PEFT Methods.</span> We report the number of learnable parameters (#P) and classification accuracy(%) of CLIP-Text (1D.) and DINO V2 (2D.) on the "PB-T50-RS" split of the ScanObjectNN dataset.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.3.1" class="ltx_tr">
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">
<span id="S4.T2.3.1.1.1" class="ltx_text"></span> <span id="S4.T2.3.1.1.2" class="ltx_text">
<span id="S4.T2.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="S4.T2.3.1.1.3" class="ltx_text"></span></td>
<td id="S4.T2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.1.2.1" class="ltx_text ltx_font_bold">#P(M)</span></td>
<td id="S4.T2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.1.3.1" class="ltx_text ltx_font_bold">1D.(%)</span></td>
<td id="S4.T2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.1.4.1" class="ltx_text ltx_font_bold">2D.(%)</span></td>
</tr>
<tr id="S4.T2.3.2" class="ltx_tr">
<td id="S4.T2.3.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Full Fine-Tuning</td>
<td id="S4.T2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.3</td>
<td id="S4.T2.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.9</td>
<td id="S4.T2.3.2.4" class="ltx_td ltx_align_center ltx_border_t">85.3</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Prompt Tuning <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.4</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.1</td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_t">86.4</td>
</tr>
<tr id="S4.T2.3.4" class="ltx_tr">
<td id="S4.T2.3.4.1" class="ltx_td ltx_align_left ltx_border_r">Adapter Tuning <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T2.3.4.2" class="ltx_td ltx_align_center ltx_border_r">0.4</td>
<td id="S4.T2.3.4.3" class="ltx_td ltx_align_center ltx_border_r">89.6</td>
<td id="S4.T2.3.4.4" class="ltx_td ltx_align_center">85.9</td>
</tr>
<tr id="S4.T2.3.5" class="ltx_tr">
<td id="S4.T2.3.5.1" class="ltx_td ltx_align_left ltx_border_r">LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S4.T2.3.5.2" class="ltx_td ltx_align_center ltx_border_r">0.9</td>
<td id="S4.T2.3.5.3" class="ltx_td ltx_align_center ltx_border_r">86.3</td>
<td id="S4.T2.3.5.4" class="ltx_td ltx_align_center">85.1</td>
</tr>
<tr id="S4.T2.3.6" class="ltx_tr">
<td id="S4.T2.3.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">Any2Point</td>
<td id="S4.T2.3.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.3.6.2.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T2.3.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.3.6.3.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S4.T2.3.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.3.6.4.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.2.1" class="ltx_text ltx_font_bold">Ablation Study on Main Components.</span>
To validate the effectiveness of 3D-to-any Virtual Projection (V.P.) and Any-to-3D Guided Adapter (G.A.).</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.3.1" class="ltx_tr">
<td id="S4.T3.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T3.3.1.1.1" class="ltx_text"></span> <span id="S4.T3.3.1.1.2" class="ltx_text">
<span id="S4.T3.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3D-to-any V.P.</span></span>
</span></span><span id="S4.T3.3.1.1.3" class="ltx_text"></span></td>
<td id="S4.T3.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T3.3.1.2.1" class="ltx_text"></span> <span id="S4.T3.3.1.2.2" class="ltx_text">
<span id="S4.T3.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.3.1.2.2.1.1" class="ltx_tr">
<span id="S4.T3.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Any-to-3D G.A.</span></span>
</span></span><span id="S4.T3.3.1.2.3" class="ltx_text"></span></td>
<td id="S4.T3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.3.1.3.1" class="ltx_text ltx_font_bold">#P(M)</span></td>
<td id="S4.T3.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.3.1.4.1" class="ltx_text ltx_font_bold">1D.(%)</span></td>
<td id="S4.T3.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.3.1.5.1" class="ltx_text ltx_font_bold">2D.(%)</span></td>
</tr>
<tr id="S4.T3.3.2" class="ltx_tr">
<td id="S4.T3.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3</td>
<td id="S4.T3.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.7</td>
<td id="S4.T3.3.2.5" class="ltx_td ltx_align_center ltx_border_t">86.1</td>
</tr>
<tr id="S4.T3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.3</td>
<td id="S4.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">89.3</td>
<td id="S4.T3.3.3.5" class="ltx_td ltx_align_center">86.6</td>
</tr>
<tr id="S4.T3.3.4" class="ltx_tr">
<td id="S4.T3.3.4.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_r">0.8</td>
<td id="S4.T3.3.4.4" class="ltx_td ltx_align_center ltx_border_r">90.9</td>
<td id="S4.T3.3.4.5" class="ltx_td ltx_align_center">87.6</td>
</tr>
<tr id="S4.T3.3.5" class="ltx_tr">
<td id="S4.T3.3.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T3.3.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T3.3.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T3.3.5.3.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T3.3.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T3.3.5.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S4.T3.3.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.5.5.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
</table>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Comparison with traditional PEFT methods.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">As demonstrated in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our Any-to-3D Guided Adapter significantly outperforms traditional PEFT techniques when utilizing pre-trained models from 1D or 2D modalities.
In comparison to Prompt Tuning <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, it achieves improvements of 2.8% and 1.3%; compared to Adapter Tuning <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>, it achieves improvements of 2.3% and 1.8%; and in contrast to Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>, it achieves improvements of 5.6% and 2.6%, respectively.
The experimental results demonstrate that our Any-to-3D Guided Adapter can efficiently mine and integrate pre-trained knowledge from other modalities to understand the semantics of 3D objects. Unlike other methods, our framework leverages 1D/2D spatial guidance to aggregate the local semantics of 3D tokens, capturing the local fine-grained information of 3D objects.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Effectiveness of Main Components.</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">As shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, to substantiate the efficacy of our proposed methods, we conducted ablation experiments by progressively incorporating each component into the baseline. The first row indicates the baseline configuration, which consists of the 3D tokenizer, the pre-trained transformer, and the task head, with updates applied only to the tokenizer and head. Introducing the 3D-to-any Virtual Projection resulted in performance improvements to 89.3% in the 1D modality and 86.6% in the 2D modality. This suggests that using virtual projection, rather than true projection, helps mitigate the loss of 3D spatial information caused by modality conversion. Following the inclusion of the Any-to-3D Guided Adapter, performance in the 1D modality surged to 90.9%, while in the 2D modality, it rose to 87.6%, with a focus on local structures leading to greater improvements. Introducing both aforementioned methods simultaneously led to a surge in performance to 91.9% in the 1D modality and a rise to 87.7% in the 2D modality, effectively showcasing the effectiveness of our comprehensive framework.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.2.1" class="ltx_text ltx_font_bold">Ablation Study on 3D-to-any Virtual Projection.</span> Sinusoidal, Learnable and 3D-to-any V.P. refer to sinusoidal positional encoding, learnable positional encoding and 3D-to-any Virtual Projection.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.3.1" class="ltx_tr">
<td id="S4.T4.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.3.1.1.1" class="ltx_text"></span> <span id="S4.T4.3.1.1.2" class="ltx_text">
<span id="S4.T4.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Sinusoidal</span></span>
</span></span><span id="S4.T4.3.1.1.3" class="ltx_text"></span></td>
<td id="S4.T4.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.3.1.2.1" class="ltx_text"></span> <span id="S4.T4.3.1.2.2" class="ltx_text">
<span id="S4.T4.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.2.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Learnable</span></span>
</span></span><span id="S4.T4.3.1.2.3" class="ltx_text"></span></td>
<td id="S4.T4.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.3.1.3.1" class="ltx_text"></span> <span id="S4.T4.3.1.3.2" class="ltx_text">
<span id="S4.T4.3.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.1.3.2.1.1" class="ltx_tr">
<span id="S4.T4.3.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3D-to-any V.P.</span></span>
</span></span><span id="S4.T4.3.1.3.3" class="ltx_text"></span></td>
<td id="S4.T4.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.3.1.4.1" class="ltx_text ltx_font_bold">1D.(%)</span></td>
<td id="S4.T4.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.5.1" class="ltx_text ltx_font_bold">2D.(%)</span></td>
</tr>
<tr id="S4.T4.3.2" class="ltx_tr">
<td id="S4.T4.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T4.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T4.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.9</td>
<td id="S4.T4.3.2.5" class="ltx_td ltx_align_center ltx_border_t">87.6</td>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_r">87.4</td>
<td id="S4.T4.3.3.5" class="ltx_td ltx_align_center">86.0</td>
</tr>
<tr id="S4.T4.3.4" class="ltx_tr">
<td id="S4.T4.3.4.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.3.4.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.3.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T4.3.4.4" class="ltx_td ltx_align_center ltx_border_r">90.5</td>
<td id="S4.T4.3.4.5" class="ltx_td ltx_align_center">86.5</td>
</tr>
<tr id="S4.T4.3.5" class="ltx_tr">
<td id="S4.T4.3.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S4.T4.3.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S4.T4.3.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T4.3.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.3.5.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S4.T4.3.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.5.5.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S4.T5.2.1" class="ltx_text ltx_font_bold">Ablation Study on Any-to-3D Guided Adapter.</span>
To validate the effectiveness of 1D/2D-guided Local Aggregation (L.A.) and Adaptive (Ada.) Any-to-3D Ensemble (Ens.).
</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.3.1" class="ltx_tr">
<td id="S4.T5.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T5.3.1.1.1" class="ltx_text"></span> <span id="S4.T5.3.1.1.2" class="ltx_text">
<span id="S4.T5.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T5.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D/2D-guided L.A.</span></span>
</span></span><span id="S4.T5.3.1.1.3" class="ltx_text"></span></td>
<td id="S4.T5.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T5.3.1.2.1" class="ltx_text"></span> <span id="S4.T5.3.1.2.2" class="ltx_text">
<span id="S4.T5.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.3.1.2.2.1.1" class="ltx_tr">
<span id="S4.T5.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Ada. Any-to-3D Ens.</span></span>
</span></span><span id="S4.T5.3.1.2.3" class="ltx_text"></span></td>
<td id="S4.T5.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T5.3.1.3.1" class="ltx_text ltx_font_bold">#P(M)</span></td>
<td id="S4.T5.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T5.3.1.4.1" class="ltx_text ltx_font_bold">1D.(%)</span></td>
<td id="S4.T5.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.5.1" class="ltx_text ltx_font_bold">2D.(%)</span></td>
</tr>
<tr id="S4.T5.3.2" class="ltx_tr">
<td id="S4.T5.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.25</td>
<td id="S4.T5.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.3</td>
<td id="S4.T5.3.2.5" class="ltx_td ltx_align_center ltx_border_t">86.6</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.8</td>
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r">90.2</td>
<td id="S4.T5.3.3.5" class="ltx_td ltx_align_center">86.8</td>
</tr>
<tr id="S4.T5.3.4" class="ltx_tr">
<td id="S4.T5.3.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T5.3.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T5.3.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.4.3.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T5.3.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.3.4.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S4.T5.3.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.3.4.5.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S4.T6.2.1" class="ltx_text ltx_font_bold">More Results on ScanObjectNN.</span></figcaption>
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.3.1" class="ltx_tr">
<td id="S4.T6.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">
<span id="S4.T6.3.1.1.1" class="ltx_text"></span><span id="S4.T6.3.1.1.2" class="ltx_text">
<span id="S4.T6.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T6.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T6.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span><span id="S4.T6.3.1.1.3" class="ltx_text"></span></td>
<td id="S4.T6.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T6.3.1.2.1" class="ltx_text ltx_font_bold">Pre-train</span></td>
<td id="S4.T6.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T6.3.1.3.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T6.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T6.3.1.4.1" class="ltx_text ltx_font_bold">#Param(M)</span></td>
<td id="S4.T6.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T6.3.1.5.1" class="ltx_text ltx_font_bold">SCAN.(%)</span></td>
</tr>
<tr id="S4.T6.3.2" class="ltx_tr">
<td id="S4.T6.3.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T6.3.2.1.1" class="ltx_text">Any2Point</span></td>
<td id="S4.T6.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Audio</td>
<td id="S4.T6.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SSAST <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T6.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.3.2.4.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T6.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.3.2.5.1" class="ltx_text ltx_font_bold">87.1</span></td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<td id="S4.T6.3.3.1" class="ltx_td ltx_align_center ltx_border_r">2D</td>
<td id="S4.T6.3.3.2" class="ltx_td ltx_align_center ltx_border_r">DeiT <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.3.3.3.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T6.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T6.3.3.4.1" class="ltx_text ltx_font_bold">87.3</span></td>
</tr>
<tr id="S4.T6.3.4" class="ltx_tr">
<td id="S4.T6.3.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Language</td>
<td id="S4.T6.3.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T6.3.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T6.3.4.3.1" class="ltx_text ltx_font_bold">0.9</span></td>
<td id="S4.T6.3.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.3.4.4.1" class="ltx_text ltx_font_bold">89.7</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Effects of 3D-to-any Virtual Projection.</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">In Table <a href="#S4.T4" title="Table 4 ‣ Effectiveness of Main Components. ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we investigated the effects of employing different positional encoding methods on the Any2Point framework. The first row indicates the absence of any positional encoding. Introducing sinusoidal positional encoding or learnable positional encoding led to a certain degree of performance degradation. This is due to the conflict between the newly introduced positional information and the inherent semantics within the source modality transformer. On the other hand, employing 3D-to-any Virtual Projection resulted in respective improvements of 1.0% and 0.1% accuracy. The results demonstrate that using original 1D/2D positional priors can promote the pre-trained transformer to acquire 3D features.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Components of Any-to-3D Guided Adapter.</h4>

<div id="S4.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px4.p1.1" class="ltx_p">As shown in Table <a href="#S4.T5" title="Table 5 ‣ Effectiveness of Main Components. ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we conduct ablation experiments by incrementally adding components to the Any-to-3D Guided Adapter. The first row signifies the baseline adapter, consisting of only an MLP with bottleneck layers. By incorporating 1D/2D-guided Local Aggregation, composed of local aggregation in 1D/2D spaces, self-attention interactions, pooling, and propagation, our approach achieves performance gains of 0.9% and 0.2%. Leveraging the positional priors from the pre-trained model facilitates mining fine-grained 3D structural information from different perspectives. The Adaptive Any-to-3D Ensemble brings further improvements of 1.7% and 0.9% for 1D and 2D modalities, effectively integrating parallel features in accordance with 3D structural features. The experiments demonstrate the effectiveness of each component in our Any-to-3D Guided Adapter to gather 3D local geometric information, complementing the global attention in the pre-trained model.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">More Results on Performance Trend.</h4>

<div id="S4.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px5.p1.1" class="ltx_p">To further validate our previous findings that the Any2Point framework, based on 1D Language pre-trained models, significantly outperforms those based on 2D modalities (Vision/Audio) in the 3D object recognition task, we conduct additional experiments in Table <a href="#S4.T6" title="Table 6 ‣ Effectiveness of Main Components. ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. On the "PB-T50-RS" split of ScanObjectNN dataset, we select RoBERTa (1D), DeiT (2D Vision), and SSAST (Audio) as the pre-trained models, with fine-tuning settings consistent with our previous experiments. These models achieve performance of 89.7%, 87.3%, and 87.1%, respectively. The performance trend across modalities is observed: 1D language &gt; 2D Vision &gt; 2D Audio. We suspect that due to the pre-training data, large language models possess stronger semantic information compared to other modalities, which is beneficial for the deep understanding of different 3D objects.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Visualization</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we opt to validate the efficacy of the proposed 3D-to-any Virtual Projection and the Any-to-3D Guided Adapter by visualizing on the ScanObjectNN test set, utilizing the CLIP-Text Encoder (1D) <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and DINO V2 (2D) <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2404.07989/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S5.F5.4.1" class="ltx_text ltx_font_bold">Visualization of Different Positional Encoding Methods.</span> For the 1D/2D modalities, we visualize the attention scores of the <span id="S5.F5.5.2" class="ltx_text ltx_font_bold">[CLS]</span> token to other point cloud tokens, utilizing sinusoidal positional encoding, learnable positional encoding, and 3D-to-any Virtual Projection. <span id="S5.F5.6.3" class="ltx_text ltx_font_bold">The red color indicates higher values.</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2404.07989/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S5.F6.3.1" class="ltx_text ltx_font_bold">Visualization of Effects of Any-to-3D Guided Adapter.</span>For 1D/2D modalities, we visualize the clusters of the similarities between the <span id="S5.F6.4.2" class="ltx_text ltx_font_bold">[CLS]</span> token and other point features, with the number of clusters set to 3. It is conducted for the complete Any-to-3D Guided Adapter, for replacing the Adaptive Any-to-3D Ensemble with maxmean pooling (Maxmean Pool.), and for performing local aggregation solely based on 3D positions (3D-guided L.A.).</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Different Positional Encoding Methods</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our 3D-to-any Virtual Projection fully exploits the positional encoding paired with the pre-trained model, injecting the source modality spatial knowledge into the 3D tokens during fine-tuning. In Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Visualization ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, when using sinusoidal positional encodings, learnable positional encodings, and our 3D-to-any Virtual Projection respectively, we visualize the attention scores of the [CLS] token to other point cloud tokens. As illustrated, for the 1D language modality, learnable positional encodings grasp useless information. After applying the commonly used sinusoidal positional encodings in Large Language Models, they fail to capture the critical 3D semantics. However, our method focuses more on the salient object parts, such as the armrests and wheels of chairs, and the legs of tables. For the 2D visual modality, learnable encodings are slightly better than sinusoidal positional encodings, as 2D pre-trained models mainly adopt the learnable encoding method. Meanwhile, our method directly recognizes the whole object and its key parts, for example, giving high weights to the chair’s backrest.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Effects of Any-to-3D Guided Adapter</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">The Any-to-3D Guided Adapter captures the 3D fine-grained information through interactions within the local regions of the source modality. In Figure <a href="#S5.F6" title="Figure 6 ‣ 5 Visualization ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we visualize the clustering results of the similarities between the [CLS] token and other point token features, utilizing the complete Any-to-3D Guided Adapter, replacing the Adaptive Any-to-3D Ensemble with maxmean pooling, and further only using 3D positional information. As shown, for simple objects like chairs (1<sup id="S5.SS2.p1.2.1" class="ltx_sup"><span id="S5.SS2.p1.2.1.1" class="ltx_text ltx_font_italic">st</span></sup> row), our method effectively distinguishes between the chair’s backrest, armrests, seat, and wheels, whereas removing components fails to capture the differences between key parts. For more challenging objects like shelves (2<sup id="S5.SS2.p1.2.2" class="ltx_sup"><span id="S5.SS2.p1.2.2.1" class="ltx_text ltx_font_italic">nd</span></sup> row), removing any components leads to semantic confusion of the object, while our approach clearly differentiates the shelf’s base, middle layer, and backrest. These experiments indicate that each component within the Adapter effectively utilizes the positional information from different modalities to promote the 3D structure extraction.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Additional Ablation Study</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we conduct ablation studies to explore the effectiveness of different components and hyper-parameters, which are not discussed in the main text. We select CLIP-Text <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> (1D) and DINO V2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> (2D) as pre-trained models to compare the accuracy (%) on the "PB-T50-RS" split of ScanObjectNN <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S6.T7.3.1" class="ltx_text ltx_font_bold">Ablation Study on Adapter Positions and Depths.</span> The ‘After’, ‘Before’, and ‘Parallel’ denote block structure of putting the <span id="S6.T7.4.2" class="ltx_text ltx_font_italic">Any-to-3D Guided Adapter</span> after, before, and in parallel to the FFN layer respectively. The insertion depth denotes the number of earlier blocks. We report the classification accuracy of CLIP-Text <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> (1D.) and DINO V2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> (2D.) on the "PB-T50-RS" split of ScanObjectNN <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>.</figcaption>
<table id="S6.T7.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T7.5.1" class="ltx_tr">
<td id="S6.T7.5.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">         Positions of Any-to-3D Guided Adapter</td>
<td id="S6.T7.5.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T7.5.1.2.1" class="ltx_text"></span> <span id="S6.T7.5.1.2.2" class="ltx_text">
<span id="S6.T7.5.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.5.1.2.2.1.1" class="ltx_tr">
<span id="S6.T7.5.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_rowspan ltx_rowspan_2"><span id="S6.T7.5.1.2.2.1.1.1.1" class="ltx_text">
<span id="S6.T7.5.1.2.2.1.1.1.1.1" class="ltx_inline-block">
<span id="S6.T7.5.1.2.2.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S6.T7.5.1.2.2.1.1.1.1.1.2" class="ltx_p">Insertion</span>
<span id="S6.T7.5.1.2.2.1.1.1.1.1.3" class="ltx_p"></span>
<span id="S6.T7.5.1.2.2.1.1.1.1.1.4" class="ltx_p">Depth</span>
</span></span></span></span>
</span></span><span id="S6.T7.5.1.2.3" class="ltx_text"></span></td>
<td id="S6.T7.5.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T7.5.1.3.1" class="ltx_text"></span> <span id="S6.T7.5.1.3.2" class="ltx_text">
<span id="S6.T7.5.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.5.1.3.2.1.1" class="ltx_tr">
<span id="S6.T7.5.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_rowspan ltx_rowspan_2"><span id="S6.T7.5.1.3.2.1.1.1.1" class="ltx_text">
<span id="S6.T7.5.1.3.2.1.1.1.1.1" class="ltx_inline-block">
<span id="S6.T7.5.1.3.2.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S6.T7.5.1.3.2.1.1.1.1.1.2" class="ltx_p">1D.</span>
<span id="S6.T7.5.1.3.2.1.1.1.1.1.3" class="ltx_p"></span>
<span id="S6.T7.5.1.3.2.1.1.1.1.1.4" class="ltx_p">(%)</span>
</span></span></span></span>
</span></span><span id="S6.T7.5.1.3.3" class="ltx_text"></span></td>
<td id="S6.T7.5.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T7.5.1.4.1" class="ltx_text"></span> <span id="S6.T7.5.1.4.2" class="ltx_text">
<span id="S6.T7.5.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.5.1.4.2.1.1" class="ltx_tr">
<span id="S6.T7.5.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_rowspan ltx_rowspan_2"><span id="S6.T7.5.1.4.2.1.1.1.1" class="ltx_text">
<span id="S6.T7.5.1.4.2.1.1.1.1.1" class="ltx_inline-block">
<span id="S6.T7.5.1.4.2.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S6.T7.5.1.4.2.1.1.1.1.1.2" class="ltx_p">2D.</span>
<span id="S6.T7.5.1.4.2.1.1.1.1.1.3" class="ltx_p"></span>
<span id="S6.T7.5.1.4.2.1.1.1.1.1.4" class="ltx_p">(%)</span>
</span></span></span></span>
</span></span><span id="S6.T7.5.1.4.3" class="ltx_text"></span></td>
</tr>
<tr id="S6.T7.5.2" class="ltx_tr">
<td id="S6.T7.5.2.1" class="ltx_td ltx_align_center ltx_border_t">    After</td>
<td id="S6.T7.5.2.2" class="ltx_td ltx_align_center ltx_border_t">  Before</td>
<td id="S6.T7.5.2.3" class="ltx_td ltx_align_center ltx_border_t">  Parallel</td>
<td id="S6.T7.5.2.4" class="ltx_td"></td>
<td id="S6.T7.5.2.5" class="ltx_td"></td>
<td id="S6.T7.5.2.6" class="ltx_td"></td>
</tr>
<tr id="S6.T7.5.3" class="ltx_tr">
<td id="S6.T7.5.3.1" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S6.T7.5.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.3.4" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S6.T7.5.3.5" class="ltx_td ltx_align_center ltx_border_t">90.4</td>
<td id="S6.T7.5.3.6" class="ltx_td ltx_align_center ltx_border_t">86.9</td>
</tr>
<tr id="S6.T7.5.4" class="ltx_tr">
<td id="S6.T7.5.4.1" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.4.4" class="ltx_td ltx_align_center">6</td>
<td id="S6.T7.5.4.5" class="ltx_td ltx_align_center">90.6</td>
<td id="S6.T7.5.4.6" class="ltx_td ltx_align_center">87.1</td>
</tr>
<tr id="S6.T7.5.5" class="ltx_tr">
<td id="S6.T7.5.5.1" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.5.4" class="ltx_td ltx_align_center">9</td>
<td id="S6.T7.5.5.5" class="ltx_td ltx_align_center">90.2</td>
<td id="S6.T7.5.5.6" class="ltx_td ltx_align_center">87.2</td>
</tr>
<tr id="S6.T7.5.6" class="ltx_tr">
<td id="S6.T7.5.6.1" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.6.4" class="ltx_td ltx_align_center">12</td>
<td id="S6.T7.5.6.5" class="ltx_td ltx_align_center"><span id="S6.T7.5.6.5.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S6.T7.5.6.6" class="ltx_td ltx_align_center"><span id="S6.T7.5.6.6.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
<tr id="S6.T7.5.7" class="ltx_tr">
<td id="S6.T7.5.7.1" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.7.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S6.T7.5.7.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.7.4" class="ltx_td ltx_align_center">3</td>
<td id="S6.T7.5.7.5" class="ltx_td ltx_align_center">89.9</td>
<td id="S6.T7.5.7.6" class="ltx_td ltx_align_center">86.2</td>
</tr>
<tr id="S6.T7.5.8" class="ltx_tr">
<td id="S6.T7.5.8.1" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.8.2" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.8.4" class="ltx_td ltx_align_center">6</td>
<td id="S6.T7.5.8.5" class="ltx_td ltx_align_center">91.1</td>
<td id="S6.T7.5.8.6" class="ltx_td ltx_align_center">86.4</td>
</tr>
<tr id="S6.T7.5.9" class="ltx_tr">
<td id="S6.T7.5.9.1" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.9.2" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.9.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.9.4" class="ltx_td ltx_align_center">9</td>
<td id="S6.T7.5.9.5" class="ltx_td ltx_align_center">90.4</td>
<td id="S6.T7.5.9.6" class="ltx_td ltx_align_center">86.4</td>
</tr>
<tr id="S6.T7.5.10" class="ltx_tr">
<td id="S6.T7.5.10.1" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.10.2" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.10.3" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.10.4" class="ltx_td ltx_align_center">12</td>
<td id="S6.T7.5.10.5" class="ltx_td ltx_align_center">91.3</td>
<td id="S6.T7.5.10.6" class="ltx_td ltx_align_center">87.0</td>
</tr>
<tr id="S6.T7.5.11" class="ltx_tr">
<td id="S6.T7.5.11.1" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.11.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T7.5.11.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S6.T7.5.11.4" class="ltx_td ltx_align_center">3</td>
<td id="S6.T7.5.11.5" class="ltx_td ltx_align_center">90.3</td>
<td id="S6.T7.5.11.6" class="ltx_td ltx_align_center">86.7</td>
</tr>
<tr id="S6.T7.5.12" class="ltx_tr">
<td id="S6.T7.5.12.1" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.12.2" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.12.3" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.12.4" class="ltx_td ltx_align_center">6</td>
<td id="S6.T7.5.12.5" class="ltx_td ltx_align_center">91.5</td>
<td id="S6.T7.5.12.6" class="ltx_td ltx_align_center">86.8</td>
</tr>
<tr id="S6.T7.5.13" class="ltx_tr">
<td id="S6.T7.5.13.1" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.13.2" class="ltx_td ltx_align_center">-</td>
<td id="S6.T7.5.13.3" class="ltx_td ltx_align_center">✓</td>
<td id="S6.T7.5.13.4" class="ltx_td ltx_align_center">9</td>
<td id="S6.T7.5.13.5" class="ltx_td ltx_align_center">90.4</td>
<td id="S6.T7.5.13.6" class="ltx_td ltx_align_center">86.8</td>
</tr>
<tr id="S6.T7.5.14" class="ltx_tr">
<td id="S6.T7.5.14.1" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S6.T7.5.14.2" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S6.T7.5.14.3" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S6.T7.5.14.4" class="ltx_td ltx_align_center ltx_border_bb">12</td>
<td id="S6.T7.5.14.5" class="ltx_td ltx_align_center ltx_border_bb">90.7</td>
<td id="S6.T7.5.14.6" class="ltx_td ltx_align_center ltx_border_bb">87.1</td>
</tr>
</table>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>The Adapter Positions and Depths.</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In Table <a href="#S6.T7" title="Table 7 ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we further conducted ablation studies on the positions and depths of the proposed Any-to-3D Guided Adapter. As shown in Table <a href="#S6.T7" title="Table 7 ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the best performance is achieved when the adapter is placed after the Feed Forward Networks (FFNs) and at a depth of 12. This is because, when placed after the FFN layers, the globally interacted point cloud features undergo local aggregation within the adapter, extracting the fine-grained structures of the 3D point clouds. Moreover, deeper insertion allows the adapter to leverage both low-level and high-level pre-trained knowledge to process the point cloud information. It is important to note that for all pre-trained models, we inserted the Any-to-3D Guided Adapter after the FFN layers of all blocks.</p>
</div>
<figure id="S6.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="S6.T8.2.1" class="ltx_text ltx_font_bold">Ablation Study on the Components of 3D-to-any Virtual Projection.</span> The 2D Proj.V1 and 2D Proj.V2 denote the projection methods used in PointCLIP V1 <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>)</cite> and PointCLIP V2 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>, respectively. The 1D Proj. refers to the projection method depicted in the main text. Meanwhile, we ablate the projection view number (Num.) on the pre-trained models.</figcaption>
<table id="S6.T8.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T8.3.1" class="ltx_tr">
<td id="S6.T8.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T8.3.1.1.1" class="ltx_text"></span> <span id="S6.T8.3.1.1.2" class="ltx_text">
<span id="S6.T8.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.1.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D Proj.V1</span></span>
</span></span><span id="S6.T8.3.1.1.3" class="ltx_text"></span></td>
<td id="S6.T8.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T8.3.1.2.1" class="ltx_text"></span> <span id="S6.T8.3.1.2.2" class="ltx_text">
<span id="S6.T8.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.2.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D Proj.V2</span></span>
</span></span><span id="S6.T8.3.1.2.3" class="ltx_text"></span></td>
<td id="S6.T8.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T8.3.1.3.1" class="ltx_text"></span> <span id="S6.T8.3.1.3.2" class="ltx_text">
<span id="S6.T8.3.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.3.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D Proj.</span></span>
</span></span><span id="S6.T8.3.1.3.3" class="ltx_text"></span></td>
<td id="S6.T8.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T8.3.1.4.1" class="ltx_text"></span> <span id="S6.T8.3.1.4.2" class="ltx_text">
<span id="S6.T8.3.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.4.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">View Num.</span></span>
</span></span><span id="S6.T8.3.1.4.3" class="ltx_text"></span></td>
<td id="S6.T8.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T8.3.1.5.1" class="ltx_text"></span> <span id="S6.T8.3.1.5.2" class="ltx_text">
<span id="S6.T8.3.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.5.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D. (%)</span></span>
</span></span><span id="S6.T8.3.1.5.3" class="ltx_text"></span></td>
<td id="S6.T8.3.1.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T8.3.1.6.1" class="ltx_text"></span> <span id="S6.T8.3.1.6.2" class="ltx_text">
<span id="S6.T8.3.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T8.3.1.6.2.1.1" class="ltx_tr">
<span id="S6.T8.3.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D. (%)</span></span>
</span></span><span id="S6.T8.3.1.6.3" class="ltx_text"></span></td>
</tr>
<tr id="S6.T8.3.2" class="ltx_tr">
<td id="S6.T8.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S6.T8.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S6.T8.3.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T8.3.2.6" class="ltx_td ltx_align_center ltx_border_t">86.2</td>
</tr>
<tr id="S6.T8.3.3" class="ltx_tr">
<td id="S6.T8.3.3.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T8.3.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.3.4" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S6.T8.3.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T8.3.3.6" class="ltx_td ltx_align_center"><span id="S6.T8.3.3.6.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
<tr id="S6.T8.3.4" class="ltx_tr">
<td id="S6.T8.3.4.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T8.3.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.4.4" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S6.T8.3.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T8.3.4.6" class="ltx_td ltx_align_center">87.2</td>
</tr>
<tr id="S6.T8.3.5" class="ltx_tr">
<td id="S6.T8.3.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S6.T8.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S6.T8.3.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T8.3.5.6" class="ltx_td ltx_align_center">86.4</td>
</tr>
<tr id="S6.T8.3.6" class="ltx_tr">
<td id="S6.T8.3.6.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.6.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T8.3.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.6.4" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S6.T8.3.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T8.3.6.6" class="ltx_td ltx_align_center">87.1</td>
</tr>
<tr id="S6.T8.3.7" class="ltx_tr">
<td id="S6.T8.3.7.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.7.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T8.3.7.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.7.4" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S6.T8.3.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S6.T8.3.7.6" class="ltx_td ltx_align_center">86.5</td>
</tr>
<tr id="S6.T8.3.8" class="ltx_tr">
<td id="S6.T8.3.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T8.3.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S6.T8.3.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S6.T8.3.8.5" class="ltx_td ltx_align_center">90.6</td>
<td id="S6.T8.3.8.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T8.3.9" class="ltx_tr">
<td id="S6.T8.3.9.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.9.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T8.3.9.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T8.3.9.4" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S6.T8.3.9.5" class="ltx_td ltx_align_center"><span id="S6.T8.3.9.5.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S6.T8.3.9.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T8.3.10" class="ltx_tr">
<td id="S6.T8.3.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S6.T8.3.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S6.T8.3.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S6.T8.3.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8</td>
<td id="S6.T8.3.10.5" class="ltx_td ltx_align_center ltx_border_bb">89.8</td>
<td id="S6.T8.3.10.6" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>The Components of 3D-to-any Virtual Projection.</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In Table <a href="#S6.T8" title="Table 8 ‣ 6.1 The Adapter Positions and Depths. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we validated the impact of different projection methods and the number of projection views on the pre-trained models for different modalities. As shown in Table <a href="#S6.T8" title="Table 8 ‣ 6.1 The Adapter Positions and Depths. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, for the 1D/2D modalities, the optimal performance is obtained when the number of views is set to 6. Meanwhile, for the 2D modality, the simple projection in PointCLIP <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>)</cite> performs better than the more complex projection in PointCLIP V2 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>. The findings suggest that employing an appropriate number of projection views sufficiently captures the diversity and complexity inherent in 3D data in low-dimensional spaces. Furthermore, a simple projection method proves adequate for representing the fine-grained structure and data characteristics of 3D point clouds. It is worth noting that we use simple projection <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022b</a>)</cite> and 6 views for all pre-trained models of any modalities.</p>
</div>
<figure id="S6.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="S6.T9.2.1" class="ltx_text ltx_font_bold">Ablation Study on the Different Local Neighborhood Sizes.</span> We conduct ablation experiments for different combinations of 1D line sizes, 2D patch sizes, and 3D grid sizes.</figcaption>
<table id="S6.T9.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T9.3.1" class="ltx_tr">
<td id="S6.T9.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T9.3.1.1.1" class="ltx_text"></span> <span id="S6.T9.3.1.1.2" class="ltx_text">
<span id="S6.T9.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T9.3.1.1.2.1.1" class="ltx_tr">
<span id="S6.T9.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Patch Size</span></span>
</span></span><span id="S6.T9.3.1.1.3" class="ltx_text"></span></td>
<td id="S6.T9.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T9.3.1.2.1" class="ltx_text"></span> <span id="S6.T9.3.1.2.2" class="ltx_text">
<span id="S6.T9.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T9.3.1.2.2.1.1" class="ltx_tr">
<span id="S6.T9.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Line Size</span></span>
</span></span><span id="S6.T9.3.1.2.3" class="ltx_text"></span></td>
<td id="S6.T9.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T9.3.1.3.1" class="ltx_text"></span> <span id="S6.T9.3.1.3.2" class="ltx_text">
<span id="S6.T9.3.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T9.3.1.3.2.1.1" class="ltx_tr">
<span id="S6.T9.3.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Grid Size</span></span>
</span></span><span id="S6.T9.3.1.3.3" class="ltx_text"></span></td>
<td id="S6.T9.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T9.3.1.4.1" class="ltx_text"></span> <span id="S6.T9.3.1.4.2" class="ltx_text">
<span id="S6.T9.3.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T9.3.1.4.2.1.1" class="ltx_tr">
<span id="S6.T9.3.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D. (%)</span></span>
</span></span><span id="S6.T9.3.1.4.3" class="ltx_text"></span></td>
<td id="S6.T9.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T9.3.1.5.1" class="ltx_text"></span> <span id="S6.T9.3.1.5.2" class="ltx_text">
<span id="S6.T9.3.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T9.3.1.5.2.1.1" class="ltx_tr">
<span id="S6.T9.3.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D. (%)</span></span>
</span></span><span id="S6.T9.3.1.5.3" class="ltx_text"></span></td>
</tr>
<tr id="S6.T9.3.2" class="ltx_tr">
<td id="S6.T9.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
<td id="S6.T9.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T9.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.08</td>
<td id="S6.T9.3.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T9.3.2.5" class="ltx_td ltx_align_center ltx_border_t">86.5</td>
</tr>
<tr id="S6.T9.3.3" class="ltx_tr">
<td id="S6.T9.3.3.1" class="ltx_td ltx_align_center ltx_border_r">26</td>
<td id="S6.T9.3.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.08</td>
<td id="S6.T9.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T9.3.3.5" class="ltx_td ltx_align_center">87.2</td>
</tr>
<tr id="S6.T9.3.4" class="ltx_tr">
<td id="S6.T9.3.4.1" class="ltx_td ltx_align_center ltx_border_r">34</td>
<td id="S6.T9.3.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.4.3" class="ltx_td ltx_align_center ltx_border_r">0.08</td>
<td id="S6.T9.3.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T9.3.4.5" class="ltx_td ltx_align_center">86.4</td>
</tr>
<tr id="S6.T9.3.5" class="ltx_tr">
<td id="S6.T9.3.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
<td id="S6.T9.3.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T9.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16</td>
<td id="S6.T9.3.5.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T9.3.5.5" class="ltx_td ltx_align_center ltx_border_t">87.0</td>
</tr>
<tr id="S6.T9.3.6" class="ltx_tr">
<td id="S6.T9.3.6.1" class="ltx_td ltx_align_center ltx_border_r">26</td>
<td id="S6.T9.3.6.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.6.3" class="ltx_td ltx_align_center ltx_border_r">0.16</td>
<td id="S6.T9.3.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T9.3.6.5" class="ltx_td ltx_align_center"><span id="S6.T9.3.6.5.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
<tr id="S6.T9.3.7" class="ltx_tr">
<td id="S6.T9.3.7.1" class="ltx_td ltx_align_center ltx_border_r">34</td>
<td id="S6.T9.3.7.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.7.3" class="ltx_td ltx_align_center ltx_border_r">0.16</td>
<td id="S6.T9.3.7.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T9.3.7.5" class="ltx_td ltx_align_center">86.0</td>
</tr>
<tr id="S6.T9.3.8" class="ltx_tr">
<td id="S6.T9.3.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T9.3.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S6.T9.3.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.08</td>
<td id="S6.T9.3.8.4" class="ltx_td ltx_align_center ltx_border_t">90.9</td>
<td id="S6.T9.3.8.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T9.3.9" class="ltx_tr">
<td id="S6.T9.3.9.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.9.2" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S6.T9.3.9.3" class="ltx_td ltx_align_center ltx_border_r">0.08</td>
<td id="S6.T9.3.9.4" class="ltx_td ltx_align_center"><span id="S6.T9.3.9.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S6.T9.3.9.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T9.3.10" class="ltx_tr">
<td id="S6.T9.3.10.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.10.2" class="ltx_td ltx_align_center ltx_border_r">3</td>
<td id="S6.T9.3.10.3" class="ltx_td ltx_align_center ltx_border_r">0.08</td>
<td id="S6.T9.3.10.4" class="ltx_td ltx_align_center">90.8</td>
<td id="S6.T9.3.10.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T9.3.11" class="ltx_tr">
<td id="S6.T9.3.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T9.3.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S6.T9.3.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16</td>
<td id="S6.T9.3.11.4" class="ltx_td ltx_align_center ltx_border_t">88.9</td>
<td id="S6.T9.3.11.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T9.3.12" class="ltx_tr">
<td id="S6.T9.3.12.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T9.3.12.2" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S6.T9.3.12.3" class="ltx_td ltx_align_center ltx_border_r">0.16</td>
<td id="S6.T9.3.12.4" class="ltx_td ltx_align_center">89.3</td>
<td id="S6.T9.3.12.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T9.3.13" class="ltx_tr">
<td id="S6.T9.3.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S6.T9.3.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">3</td>
<td id="S6.T9.3.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.16</td>
<td id="S6.T9.3.13.4" class="ltx_td ltx_align_center ltx_border_bb">91.1</td>
<td id="S6.T9.3.13.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</table>
</figure>
<figure id="S6.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span><span id="S6.T10.2.1" class="ltx_text ltx_font_bold">Ablation Study on the 1D/2D-guided Local Aggregation.</span> To further emphasize the significance of 1D/2D-guided Local Aggregation (L.A.), we conduct additional experiments on the "PB-T50-RS" split of the ScanObjectNN <cite class="ltx_cite ltx_citemacro_cite">Uy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> dataset.</figcaption>
<table id="S6.T10.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T10.3.1" class="ltx_tr">
<td id="S6.T10.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T10.3.1.1.1" class="ltx_text"></span> <span id="S6.T10.3.1.1.2" class="ltx_text">
<span id="S6.T10.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T10.3.1.1.2.1.1" class="ltx_tr">
<span id="S6.T10.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3D-guided L.A.</span></span>
</span></span><span id="S6.T10.3.1.1.3" class="ltx_text"></span></td>
<td id="S6.T10.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T10.3.1.2.1" class="ltx_text"></span> <span id="S6.T10.3.1.2.2" class="ltx_text">
<span id="S6.T10.3.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T10.3.1.2.2.1.1" class="ltx_tr">
<span id="S6.T10.3.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D-guided L.A.</span></span>
</span></span><span id="S6.T10.3.1.2.3" class="ltx_text"></span></td>
<td id="S6.T10.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S6.T10.3.1.3.1" class="ltx_text"></span> <span id="S6.T10.3.1.3.2" class="ltx_text">
<span id="S6.T10.3.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T10.3.1.3.2.1.1" class="ltx_tr">
<span id="S6.T10.3.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D-guided L.A.</span></span>
</span></span><span id="S6.T10.3.1.3.3" class="ltx_text"></span></td>
<td id="S6.T10.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T10.3.1.4.1" class="ltx_text"></span> <span id="S6.T10.3.1.4.2" class="ltx_text">
<span id="S6.T10.3.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T10.3.1.4.2.1.1" class="ltx_tr">
<span id="S6.T10.3.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">1D. (%)</span></span>
</span></span><span id="S6.T10.3.1.4.3" class="ltx_text"></span></td>
<td id="S6.T10.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T10.3.1.5.1" class="ltx_text"></span> <span id="S6.T10.3.1.5.2" class="ltx_text">
<span id="S6.T10.3.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T10.3.1.5.2.1.1" class="ltx_tr">
<span id="S6.T10.3.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2D. (%)</span></span>
</span></span><span id="S6.T10.3.1.5.3" class="ltx_text"></span></td>
</tr>
<tr id="S6.T10.3.2" class="ltx_tr">
<td id="S6.T10.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S6.T10.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T10.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T10.3.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T10.3.2.5" class="ltx_td ltx_align_center ltx_border_t">86.2</td>
</tr>
<tr id="S6.T10.3.3" class="ltx_tr">
<td id="S6.T10.3.3.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T10.3.3.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S6.T10.3.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S6.T10.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T10.3.3.5" class="ltx_td ltx_align_center"><span id="S6.T10.3.3.5.1" class="ltx_text ltx_font_bold">87.7</span></td>
</tr>
<tr id="S6.T10.3.4" class="ltx_tr">
<td id="S6.T10.3.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S6.T10.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T10.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S6.T10.3.4.4" class="ltx_td ltx_align_center ltx_border_t">87.4</td>
<td id="S6.T10.3.4.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S6.T10.3.5" class="ltx_tr">
<td id="S6.T10.3.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S6.T10.3.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S6.T10.3.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S6.T10.3.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T10.3.5.4.1" class="ltx_text ltx_font_bold">91.9</span></td>
<td id="S6.T10.3.5.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</table>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>The Influences of Different Local Neighborhood Sizes.</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In Table <a href="#S6.T9" title="Table 9 ‣ 6.2 The Components of 3D-to-any Virtual Projection. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we investigated the performance impact of various combinations of 1D line sizes, 2D patch sizes, and 3D grid sizes <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite> during the 1D/2D-guided local aggregation stage. As demonstrated in Table <a href="#S6.T9" title="Table 9 ‣ 6.2 The Components of 3D-to-any Virtual Projection. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, when the 1D line size and 2D patch size are set to moderate values of 2 and 26, respectively, remarkable performance is attained. These findings indicate that an appropriate local aggregation size enhances the model’s comprehension of 3D local information, whereas excessively large or small sizes could lead to the loss of critical features. For diverse tasks, we have consistently employed similar local aggregation sizes for pre-trained transformers of 1D/2D modalities.</p>
</div>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2404.07989/assets/x7.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S6.F7.3.1" class="ltx_text ltx_font_bold">Visualization of Different 3D Positional Embeddings.</span> For the 1D/2D modalities, we visualize the cosine similarity between the 3D positional embeddings (PEs) of selected point and those of other tokens on the airplane and lamp, respectively. The first column represents the averaging of 3D PEs obtained from 6 views, while the subsequent columns show the 3D PEs under different views. The black circle area in the first column represents the range where I select random point. <span id="S6.F7.4.2" class="ltx_text ltx_font_bold">The red color indicates higher similarity.</span></figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>The Importance of 1D/2D-guided Local Aggregation.</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In Table <a href="#S6.T10" title="Table 10 ‣ 6.2 The Components of 3D-to-any Virtual Projection. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we validated the advantages of utilizing positional priors from the source modalities that are compatible with the pre-trained model, by injecting the positional priors of different modalities (1D/2D/3D) into the local aggregation step within the Any-to-3D Guided Adapter. As shown in Table <a href="#S6.T10" title="Table 10 ‣ 6.2 The Components of 3D-to-any Virtual Projection. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, compared to using the original 3D spatial knowledge, our method exhibits a performance improvement of 1.5%-4.5% for 2D/1D modalities. This demonstrates that our proposed method effectively addresses the issue of positional discrepancy between the original 3D positions and the pre-trained model.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Additional Visualization</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We conduct visualization experiments on the test split of the ModelNet40 dataset <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2015</a>)</cite> utilizing the CLIP-Text <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> (1D) and DINO V2 <cite class="ltx_cite ltx_citemacro_cite">Oquab et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> (2D).</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>The Significance of Encoding 3D Positions in 1D/2D PEs</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">To demonstrate the effect of our method that uses 1D/2D Positional Embeddings (PEs) from source modalities to encode 3D positions, we randomly selected one point on a 3D object (inside the black circle in the first column) and computed the cosine similarity between the positional embedding of the point and those of other tokens. We compared 3D PEs assigned under different views (the six columns on the right) and the final 3D PEs obtained by averaging over M views (the first column), where M is set to 6. As shown in Figure <a href="#S6.F7" title="Figure 7 ‣ 6.3 The Influences of Different Local Neighborhood Sizes. ‣ 6 Additional Ablation Study ‣ Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the first column shows higher similarity in areas closer to our selected point, while in other columns, high similarity areas are scattered at farther locations. For example, in the first and second rows, we selected a point on the nose of a plane, and in the third and fourth rows, we selected a point on the base of a lamp. In our method (the first column), similarity values decreases with distance, showing a transition from strong to weak, whereas in the other columns, the distribution appears irregular. It indicates that our proposed 3D positional embeddings implicitly establish spatial relationships in 3D space.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In conclusion, our paper proposes Any2Point to enable a general any-to-3D transferring framework, empowering any-modality pre-trained large models (e.g., 2D vision, language, and audio) for efficient 3D understanding. Within Any2Point framework, we introduce two techniques, named 3D-to-any virtual projection and any-to-3D guided adapter, to extract 3D structure knowledge while efficiently fine-tuning pre-trained models. This enables us to overcome issues within current methods, such as 3D geometry loss and excessive resource cost. Our extensive experiments across various tasks demonstrate the superior performance and efficiency of Any2Point compared to previous SOTA 3D pre-trained models, achieving remarkable results with only a fraction of the trainable parameters.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022]</span>
<span class="ltx_bibblock">
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.

</span>
<span class="ltx_bibblock">Adaptformer: Adapting vision transformers for scalable visual recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:16664–16678, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2017]</span>
<span class="ltx_bibblock">
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner.

</span>
<span class="ltx_bibblock">Scannet: Richly-annotated 3d reconstructions of indoor scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 5828–5839, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al. [2023]</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.

</span>
<span class="ltx_bibblock">Scaling vision transformers to 22 billion parameters.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 7480–7512. PMLR, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2018]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2022]</span>
<span class="ltx_bibblock">
Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma.

</span>
<span class="ltx_bibblock">Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08320</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2020]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floridi and Chiriatti [2020]</span>
<span class="ltx_bibblock">
Luciano Floridi and Massimo Chiriatti.

</span>
<span class="ltx_bibblock">Gpt-3: Its nature, scope, limits, and consequences.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Minds and Machines</em>, 30:681–694, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar et al. [2023]</span>
<span class="ltx_bibblock">
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.

</span>
<span class="ltx_bibblock">Imagebind: One embedding space to bind them all.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 15180–15190, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2021]</span>
<span class="ltx_bibblock">
Yuan Gong, Yu-An Chung, and James Glass.

</span>
<span class="ltx_bibblock">Ast: Audio spectrogram transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.01778</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2022]</span>
<span class="ltx_bibblock">
Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass.

</span>
<span class="ltx_bibblock">Ssast: Self-supervised audio spectrogram transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pages 10699–10709, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2023a]</span>
<span class="ltx_bibblock">
Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzhi Li, and Pheng-Ann Heng.

</span>
<span class="ltx_bibblock">Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.14007</em>, 2023a.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2023b]</span>
<span class="ltx_bibblock">
Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, et al.

</span>
<span class="ltx_bibblock">Point-bind &amp; point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.00615</em>, 2023b.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2023c]</span>
<span class="ltx_bibblock">
Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, and Xuelong Li.

</span>
<span class="ltx_bibblock">Viewrefer: Grasp the multi-view knowledge for 3d visual grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 15326–15337. IEEE Computer Society, 2023c.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2022]</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 16000–16009, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. [2019]</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 2790–2799. PMLR, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. [2022]</span>
<span class="ltx_bibblock">
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.

</span>
<span class="ltx_bibblock">Visual prompt tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 709–727. Springer, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong.

</span>
<span class="ltx_bibblock">Manipllm: Embodied multimodal large language model for object-centric robotic manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.16217</em>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Jiaming Liu, Senqiao Yang, Peidong Jia, Ming Lu, Yandong Guo, Wei Xue, and Shanghang Zhang.

</span>
<span class="ltx_bibblock">Vida: Homeostatic visual domain adapter for continual test time adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04344</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.

</span>
<span class="ltx_bibblock">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.07602</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2017]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2022]</span>
<span class="ltx_bibblock">
Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu.

</span>
<span class="ltx_bibblock">Rethinking network design and local geometry in point cloud: A simple residual mlp framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07123</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. [2023]</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2023]</span>
<span class="ltx_bibblock">
Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Li Liu, and Shanghang Zhang.

</span>
<span class="ltx_bibblock">Renderocc: Vision-centric 3d occupancy prediction with 2d rendering supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.09502</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al. [2022]</span>
<span class="ltx_bibblock">
Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan.

</span>
<span class="ltx_bibblock">Masked autoencoders for point cloud self-supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 604–621. Springer, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2017a]</span>
<span class="ltx_bibblock">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Pointnet: Deep learning on point sets for 3d classification and segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 652–660, 2017a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2017b]</span>
<span class="ltx_bibblock">
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Pointnet++: Deep hierarchical feature learning on point sets in a metric space.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017b.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2023]</span>
<span class="ltx_bibblock">
Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi.

</span>
<span class="ltx_bibblock">Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.02318</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. [2022]</span>
<span class="ltx_bibblock">
Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">Pointnext: Revisiting pointnet++ with improved training and scaling strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:23192–23204, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2019]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. [2022]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10684–10695, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2024]</span>
<span class="ltx_bibblock">
Yiwen Tang, Ray Zhang, Zoey Guo, Xianzheng Ma, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li.

</span>
<span class="ltx_bibblock">Point-peft: Parameter-efficient fine-tuning for 3d pre-trained models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 5171–5179, 2024.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2021]</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Training data-efficient image transformers &amp; distillation through attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 10347–10357. PMLR, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uy et al. [2019]</span>
<span class="ltx_bibblock">
Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.

</span>
<span class="ltx_bibblock">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 1588–1597, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.

</span>
<span class="ltx_bibblock">Dynamic graph cnn for learning on point clouds.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (tog)</em>, 38(5):1–12, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu.

</span>
<span class="ltx_bibblock">P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:14388–14402, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2022]</span>
<span class="ltx_bibblock">
Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Masked feature prediction for self-supervised visual pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 14668–14678, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2022]</span>
<span class="ltx_bibblock">
Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">Point transformer v2: Grouped vector attention and partition-based pooling.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:33330–33342, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2015]</span>
<span class="ltx_bibblock">
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.

</span>
<span class="ltx_bibblock">3d shapenets: A deep representation for volumetric shapes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 1912–1920, 2015.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2022]</span>
<span class="ltx_bibblock">
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.

</span>
<span class="ltx_bibblock">Simmim: A simple framework for masked image modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 9653–9663, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2022]</span>
<span class="ltx_bibblock">
Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu, Xiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka.

</span>
<span class="ltx_bibblock">Image2point: 3d point-cloud understanding with 2d image pretrained models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 638–656. Springer, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. [2023]</span>
<span class="ltx_bibblock">
Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese.

</span>
<span class="ltx_bibblock">Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 1179–1189, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023]</span>
<span class="ltx_bibblock">
Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, and Shanghang Zhang.

</span>
<span class="ltx_bibblock">Lidar-llm: Exploring the potential of large language models for 3d lidar understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14074</em>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2024]</span>
<span class="ltx_bibblock">
Senqiao Yang, Jiarui Wu, Jiaming Liu, Xiaoqi Li, Qizhe Zhang, Mingjie Pan, Yulu Gan, Zehui Chen, and Shanghang Zhang.

</span>
<span class="ltx_bibblock">Exploring sparse visual prompt for domain adaptive dense prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 16334–16342, 2024.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022]</span>
<span class="ltx_bibblock">
Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu.

</span>
<span class="ltx_bibblock">Point-bert: Pre-training 3d point cloud transformers with masked point modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 19313–19322, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022a]</span>
<span class="ltx_bibblock">
Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:27061–27074, 2022a.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022b]</span>
<span class="ltx_bibblock">
Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Pointclip: Point cloud understanding by clip.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 8552–8562, 2022b.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao.

</span>
<span class="ltx_bibblock">Llama-adapter: Efficient fine-tuning of language models with zero-init attention.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16199</em>, 2023a.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 21769–21780, 2023b.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023c]</span>
<span class="ltx_bibblock">
Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, and Jianbo Shi.

</span>
<span class="ltx_bibblock">Starting from non-parametric networks for 3d point cloud analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 5344–5353, 2023c.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao.

</span>
<span class="ltx_bibblock">Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 2639–2650, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2024]</span>
<span class="ltx_bibblock">
Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, and Peng Gao.

</span>
<span class="ltx_bibblock">No time to train: Empowering non-parametric networks for few-shot 3d scene segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.04050</em>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.07988" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.07989" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.07989">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.07989" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.07990" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 23:25:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
