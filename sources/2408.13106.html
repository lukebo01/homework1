<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13106] NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks</title><meta property="og:description" content="Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are comp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13106">

<!--Generated on Thu Sep  5 13:22:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
self-supervised learning,  speech recognition,  speaker diarization,  spoken language understanding
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="id1.1.id1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1" class="ltx_td ltx_align_center">He Huang, Taejin Park, Kunal Dhawan, Ivan Medennikov, Krishna C. Puvvada,</span></span>
<span id="id1.1.id1.2.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1" class="ltx_td ltx_align_center">Nithin Rao Koluguri, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</span></span>
<span id="id1.1.id1.3.3" class="ltx_tr">
<span id="id1.1.id1.3.3.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.3.3.1.1" class="ltx_text ltx_font_italic">NVIDIA, Santa Clara, CA, USA</span></span></span>
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive due to using transformer encoder and lack of sub-sampling. In this paper, we propose a new self-supervised learning model termed as <em id="id2.id1.1" class="ltx_emph ltx_font_italic">pretraiNed Encoder for Speech Tasks</em> (<span id="id2.id1.2" class="ltx_text ltx_font_bold">NEST</span>). Specifically, we adopt the FastConformer architecture, which has an 8x sub-sampling rate and is faster than Transformer or Conformer architectures. Instead of clustering-based token generation, we resort to fixed random projection for its simplicity and effectiveness. We also propose a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that the proposed NEST improves over existing self-supervised models on a variety of speech processing tasks. Code and checkpoints will be publicly available via NVIDIA NeMo toolkit<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">https://github.com/NVIDIA/NeMo</span></span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
self-supervised learning, speech recognition, speaker diarization, spoken language understanding

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Most recent speech self-supervised models are inspired by the BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> model, which learn text token embedding by predicting the target of the masked positions given the context of the unmasked ones.
Among them, are two main streams of <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">contrastive</em> and <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">predictive</em> models. The <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">contrastive</em> approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> quantizes the speech features into a set of target feature <em id="S1.p1.1.4" class="ltx_emph ltx_font_italic">vectors</em> and trains with a contrastive loss using the positive and negative target features. Meanwhile, the <em id="S1.p1.1.5" class="ltx_emph ltx_font_italic">predictive</em> approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> quantizes the speech features into <em id="S1.p1.1.6" class="ltx_emph ltx_font_italic">tokens</em> and train with masked token prediction loss as in BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In addition to the two approaches, some works also learn from the masked auto-encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> approach and train speech self-supervised models with a <em id="S1.p1.1.7" class="ltx_emph ltx_font_italic">reconstruction</em> objective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One representative work of <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">contrastive</em> models is Wav2vec-2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which demonstrates initializing ASR models from SSL checkpoints can outperform previous semi-supervised and train-from-scratch ASR models. Later, Wav2vec-C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> improves over Wav2vec-2.0 by adding a consistency loss to reconstruct the quantized embedding, similar to VQ-VAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. XLS-R <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> extends Wav2vec-2.0 to multilingual setting and shows impressive performance on multilingual speech recognition and translation.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.13106/assets/figures/nest_tasks.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>NEST serves as a bird nest that incubates the variety of speech task models.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, as a pioneer work of the <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">predictive</em> approach, generates the target tokens by running k-means clustering on the middle layer features extracted from anohter SSL model that is pretrained for a small number of steps. Then, W2v-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposes to combine the training objectives of both Wav2vec-2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> by applying contrastive loss on the middle layer output while predictive loss at the final output layer. Later, BEST-RQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> shows that the clustering based token generation can be replaced by simple fixed random-projection quantization, and this simple modification is able to match or outperform HuBERT on ASR.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2408.13106/assets/figures/nest-model2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) The proposed NEST architecture. (b) Two ways to use NEST encoder: (left) use as weight initialization for tasks that require more parameters (<span id="S1.F2.3.1" class="ltx_text ltx_font_italic">e.g.</span>, speech recognition); (right) learn weighted summation of features from different layers of the frozen NEST for tasks that require less trainable parameters (<span id="S1.F2.4.2" class="ltx_text ltx_font_italic">e.g.,</span> speaker verification).</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to improve performance on speaker tasks, WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposes a noisy speech augmentation technique and a <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">denoising masked token prediction</em> objective, by adding a speech segment of a different speaker to the current speech and training the model to predict the target tokens generated using original clean speech. XEUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> further extends WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> by adding a de-reverberation task and extending to multilingual data of 1M hours.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">However, previous SSL models also have their different limitations. First, some models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> use CNN+transformer architecture which has a short frame length of 20ms, which slows down the model’s inference speed. Second, the HuBERT-style quantization is very computationally expensive, which could take 20% of total training time according to XEUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Third, although BEST-RQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> uses Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> encoder with 40ms frame length and simple random quantization, it lacks the ability to explicitly tell one speaker from another, which limits its performance on speaker tasks (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, speaker diarization).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we tackle all these challenges and bring the best practices from previous works, which constitute the proposed <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">pretraiNed Encoder for Speech Tasks</em> (<span id="S1.p6.1.2" class="ltx_text ltx_font_bold">NEST</span>) framework. Our contributions are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A new speech SSL model that achieves SOTA performance with a simplified and more efficient framework.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Experiments show that NEST can help achieve SOTA performance on a variety of downstream tasks (ASR, AST, SLU, SD, etc).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Different from previous SSL works that focus mostly on downstream tasks with limited data, we also show that NEST can benefit speech recognition and translation even when data is relatively larger.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">To the best of our knowledge, we are the first to show that SSL model trained on English data can also help improve speech recognition on other languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Approach</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce the proposed NEST framework for learning efficient self-supervised speech encoder, as illustrated in Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Speech Encoder</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Current SOTA speech SSL models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> mostly use transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> or Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as speech encoder, which have either 20ms or 40ms frame length. Here we choose the more efficient FastConformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> which applies 8x convolutional sub-sampling on the input Mel-spectrogram before the following FastConformer layers, resulting in an 80ms frame length that can significantly reduce the sequence length to be processed by self-attention layers.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Speech Augmentation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We augment the input speech with random noise or speech of another speaker, similar to the techniques proposed in WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, we generalize the augmentation in three ways: (1) the length of augmentation audio is sampled between 0.4 and 0.6 of the primary audio length, instead of a fixed 0.5 ratio; (2) the length of augmentation audio is randomly split into 1, 2 or 3 segments with uniform probability, such that the augmentation is scattered to different positions of the primary audio; (3) instead of using single negative speaker, for each segment with speaker augmentation, we randomly select a different speaker from other speakers in the same batch, such that there can be more speakers in the resulted audios.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results on SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> benchmark for multi-task evaluation on SSL speech encoders.</figcaption>
<div id="S2.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:69.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-175.0pt,28.1pt) scale(0.553303924616124,0.553303924616124) ;">
<table id="S2.T1.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.7.7.8.1" class="ltx_tr">
<th id="S2.T1.7.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.1.1" class="ltx_text">Model</span></th>
<th id="S2.T1.7.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.2.1" class="ltx_text">Params</span></th>
<td id="S2.T1.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.3.1" class="ltx_text">SSL Data (hrs)</span></td>
<td id="S2.T1.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Speaker</td>
<td id="S2.T1.7.7.8.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Content</td>
<td id="S2.T1.7.7.8.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ParaLinguistics</td>
</tr>
<tr id="S2.T1.7.7.7" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">SID (Acc <math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">SV (EER <math id="S2.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">SD (DER <math id="S2.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S2.T1.3.3.3.3.m1.1.1" xref="S2.T1.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.m1.1b"><ci id="S2.T1.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">PR (PER <math id="S2.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S2.T1.4.4.4.4.m1.1.1" xref="S2.T1.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.m1.1b"><ci id="S2.T1.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r">ASR (WER <math id="S2.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S2.T1.5.5.5.5.m1.1.1" xref="S2.T1.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.m1.1b"><ci id="S2.T1.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r">KS (Acc <math id="S2.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S2.T1.6.6.6.6.m1.1.1" xref="S2.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.m1.1b"><ci id="S2.T1.6.6.6.6.m1.1.1.cmml" xref="S2.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r">ER (Acc <math id="S2.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.7.7.7.7.m1.1a"><mo stretchy="false" id="S2.T1.7.7.7.7.m1.1.1" xref="S2.T1.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.m1.1b"><ci id="S2.T1.7.7.7.7.m1.1.1.cmml" xref="S2.T1.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S2.T1.7.7.9.2" class="ltx_tr">
<th id="S2.T1.7.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">WavLM-base++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S2.T1.7.7.9.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">95M</th>
<td id="S2.T1.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-96K</td>
<td id="S2.T1.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.84</td>
<td id="S2.T1.7.7.9.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.26</td>
<td id="S2.T1.7.7.9.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.07</td>
<td id="S2.T1.7.7.9.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.07</td>
<td id="S2.T1.7.7.9.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5.59</td>
<td id="S2.T1.7.7.9.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.69</td>
<td id="S2.T1.7.7.9.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">67.98</td>
</tr>
<tr id="S2.T1.7.7.10.3" class="ltx_tr">
<th id="S2.T1.7.7.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">WavLM-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S2.T1.7.7.10.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">316M</th>
<td id="S2.T1.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En-96K</td>
<td id="S2.T1.7.7.10.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.25</td>
<td id="S2.T1.7.7.10.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.04</td>
<td id="S2.T1.7.7.10.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.47</td>
<td id="S2.T1.7.7.10.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.09</td>
<td id="S2.T1.7.7.10.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.44</td>
<td id="S2.T1.7.7.10.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.40</td>
<td id="S2.T1.7.7.10.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.03</td>
</tr>
<tr id="S2.T1.7.7.11.4" class="ltx_tr">
<th id="S2.T1.7.7.11.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">XEUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<th id="S2.T1.7.7.11.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">577M</th>
<td id="S2.T1.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MulLing-1M</td>
<td id="S2.T1.7.7.11.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.70</td>
<td id="S2.T1.7.7.11.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.16</td>
<td id="S2.T1.7.7.11.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.11</td>
<td id="S2.T1.7.7.11.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.21</td>
<td id="S2.T1.7.7.11.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.34</td>
<td id="S2.T1.7.7.11.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.7.7.11.4.9.1" class="ltx_text ltx_font_bold">98.32</span></td>
<td id="S2.T1.7.7.11.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.7.7.11.4.10.1" class="ltx_text ltx_font_bold">71.08</span></td>
</tr>
<tr id="S2.T1.7.7.12.5" class="ltx_tr">
<th id="S2.T1.7.7.12.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">NEST-L</th>
<th id="S2.T1.7.7.12.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">108M</th>
<td id="S2.T1.7.7.12.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-100K</td>
<td id="S2.T1.7.7.12.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">94.94</td>
<td id="S2.T1.7.7.12.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.85</td>
<td id="S2.T1.7.7.12.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.28</td>
<td id="S2.T1.7.7.12.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.95</td>
<td id="S2.T1.7.7.12.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.49</td>
<td id="S2.T1.7.7.12.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.85</td>
<td id="S2.T1.7.7.12.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">68.12</td>
</tr>
<tr id="S2.T1.7.7.13.6" class="ltx_tr">
<th id="S2.T1.7.7.13.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-XL</th>
<th id="S2.T1.7.7.13.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">600M</th>
<td id="S2.T1.7.7.13.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">En-100K</td>
<td id="S2.T1.7.7.13.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.4.1" class="ltx_text ltx_font_bold">95.76</span></td>
<td id="S2.T1.7.7.13.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.5.1" class="ltx_text ltx_font_bold">2.49</span></td>
<td id="S2.T1.7.7.13.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.6.1" class="ltx_text ltx_font_bold">1.89</span></td>
<td id="S2.T1.7.7.13.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.7.1" class="ltx_text ltx_font_bold">1.80</span></td>
<td id="S2.T1.7.7.13.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.8.1" class="ltx_text ltx_font_bold">3.19</span></td>
<td id="S2.T1.7.7.13.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">97.11</td>
<td id="S2.T1.7.7.13.6.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">69.94</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results on multi-lingual ASR with punctuation and capitalization. Performance is evaluated by word error rate (WER) including native punctuation and capitalization from the source datasets.</figcaption>
<div id="S2.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:76.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-193.6pt,34.0pt) scale(0.528233443041959,0.528233443041959) ;">
<table id="S2.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S2.T2.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.2.1" class="ltx_text">Params</span></th>
<td id="S2.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.3.1" class="ltx_text">Data (hrs)</span></td>
<td id="S2.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">En</td>
<td id="S2.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">De</td>
<td id="S2.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Es</td>
<td id="S2.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Fr</td>
<td id="S2.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.8.1" class="ltx_text">Avg</span></td>
</tr>
<tr id="S2.T2.1.1.2.2" class="ltx_tr">
<td id="S2.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
</tr>
<tr id="S2.T2.1.1.3.3" class="ltx_tr">
<th id="S2.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SeamlessM4T-medium-v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S2.T2.1.1.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">1.2B</th>
<td id="S2.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4M</td>
<td id="S2.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.20</td>
<td id="S2.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.02</td>
<td id="S2.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.25</td>
<td id="S2.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.20</td>
<td id="S2.T2.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.43</td>
<td id="S2.T2.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.01</td>
<td id="S2.T2.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">17.34</td>
<td id="S2.T2.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.49</td>
<td id="S2.T2.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">13.11</td>
</tr>
<tr id="S2.T2.1.1.4.4" class="ltx_tr">
<th id="S2.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">SeamlessM4T-large-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S2.T2.1.1.4.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">2.3B</th>
<td id="S2.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4M</td>
<td id="S2.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.13</td>
<td id="S2.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.77</td>
<td id="S2.T2.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.4.4.6.1" class="ltx_text ltx_font_bold">7.53</span></td>
<td id="S2.T2.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.39</td>
<td id="S2.T2.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.671</td>
<td id="S2.T2.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.53</td>
<td id="S2.T2.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.4.4.10.1" class="ltx_text ltx_font_bold">14.37</span></td>
<td id="S2.T2.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.13</td>
<td id="S2.T2.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.44</td>
</tr>
<tr id="S2.T2.1.1.5.5" class="ltx_tr">
<th id="S2.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Whisper-large-v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<th id="S2.T2.1.1.5.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1.5B</th>
<td id="S2.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5M</td>
<td id="S2.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.73</td>
<td id="S2.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.42</td>
<td id="S2.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.24</td>
<td id="S2.T2.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.41</td>
<td id="S2.T2.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.95</td>
<td id="S2.T2.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.31</td>
<td id="S2.T2.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.35</td>
<td id="S2.T2.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.58</td>
<td id="S2.T2.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.49</td>
</tr>
<tr id="S2.T2.1.1.6.6" class="ltx_tr">
<th id="S2.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Canary-1b <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<th id="S2.T2.1.1.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1B</th>
<td id="S2.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86k</td>
<td id="S2.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.4.1" class="ltx_text ltx_font_bold">12.46</span></td>
<td id="S2.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.5.1" class="ltx_text ltx_font_bold">7.52</span></td>
<td id="S2.T2.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.71</td>
<td id="S2.T2.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.32</td>
<td id="S2.T2.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.8.1" class="ltx_text ltx_font_bold">8.28</span></td>
<td id="S2.T2.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.56</td>
<td id="S2.T2.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.46</td>
<td id="S2.T2.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.11.1" class="ltx_text ltx_font_bold">8.78</span></td>
<td id="S2.T2.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.76</td>
</tr>
<tr id="S2.T2.1.1.7.7" class="ltx_tr">
<th id="S2.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">FastConformer-XL-hybrid (ASR init)</th>
<th id="S2.T2.1.1.7.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">600M</th>
<td id="S2.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14K</td>
<td id="S2.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.78</td>
<td id="S2.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.21</td>
<td id="S2.T2.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.17</td>
<td id="S2.T2.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.69</td>
<td id="S2.T2.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.75</td>
<td id="S2.T2.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.19</td>
<td id="S2.T2.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.42</td>
<td id="S2.T2.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.89</td>
<td id="S2.T2.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.76</td>
</tr>
<tr id="S2.T2.1.1.8.8" class="ltx_tr">
<th id="S2.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">NEST-XL-hybrid</th>
<th id="S2.T2.1.1.8.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">600M</th>
<td id="S2.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">14K</td>
<td id="S2.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">14.43</td>
<td id="S2.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">7.58</td>
<td id="S2.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">8.07</td>
<td id="S2.T2.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.7.1" class="ltx_text ltx_font_bold">11.83</span></td>
<td id="S2.T2.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">8.70</td>
<td id="S2.T2.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.9.1" class="ltx_text ltx_font_bold">9.27</span></td>
<td id="S2.T2.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">16.18</td>
<td id="S2.T2.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">9.74</td>
<td id="S2.T2.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.12.1" class="ltx_text ltx_font_bold">10.72</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Speech Quantization</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We resort to BEST-RQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as our speech quantization method. Specifically, we use a single randomly initialized and frozen codebook of 8192 vocabulary and 16 dimension features. A randomly initialized and frozen linear layer is applied to the input Mel-spectrogram features to project them into the same dimension as the codebook, then a nearest neighbor search is applied to obtain the target tokens.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Feature Masking</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.4" class="ltx_p">We employ a random block-wise masking mechanism on the input Mel-spectrogram features, where each frame in the input has a probability <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="p_{m}" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><msub id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">𝑝</ci><ci id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">p_{m}</annotation></semantics></math> as being selected as the start of a masking block. After randomly selecting a set of starting frames, we mask <math id="S2.SS4.p1.2.m2.1" class="ltx_Math" alttext="l_{m}" display="inline"><semantics id="S2.SS4.p1.2.m2.1a"><msub id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml"><mi id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">l</mi><mi id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.p1.2.m2.1.1.2">𝑙</ci><ci id="S2.SS4.p1.2.m2.1.1.3.cmml" xref="S2.SS4.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">l_{m}</annotation></semantics></math> consecutive frames for each of the starting frames. Note that there could be overlapping between two masked blocks, which allows for arbitrary lengths in the resulting masked segments that do not overlap with each other. We use <math id="S2.SS4.p1.3.m3.1" class="ltx_Math" alttext="p_{m}=0.01" display="inline"><semantics id="S2.SS4.p1.3.m3.1a"><mrow id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml"><msub id="S2.SS4.p1.3.m3.1.1.2" xref="S2.SS4.p1.3.m3.1.1.2.cmml"><mi id="S2.SS4.p1.3.m3.1.1.2.2" xref="S2.SS4.p1.3.m3.1.1.2.2.cmml">p</mi><mi id="S2.SS4.p1.3.m3.1.1.2.3" xref="S2.SS4.p1.3.m3.1.1.2.3.cmml">m</mi></msub><mo id="S2.SS4.p1.3.m3.1.1.1" xref="S2.SS4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS4.p1.3.m3.1.1.3" xref="S2.SS4.p1.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b"><apply id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1"><eq id="S2.SS4.p1.3.m3.1.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1.1"></eq><apply id="S2.SS4.p1.3.m3.1.1.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.2.1.cmml" xref="S2.SS4.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.2.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2.2">𝑝</ci><ci id="S2.SS4.p1.3.m3.1.1.2.3.cmml" xref="S2.SS4.p1.3.m3.1.1.2.3">𝑚</ci></apply><cn type="float" id="S2.SS4.p1.3.m3.1.1.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">p_{m}=0.01</annotation></semantics></math> and <math id="S2.SS4.p1.4.m4.1" class="ltx_Math" alttext="l_{m}=40" display="inline"><semantics id="S2.SS4.p1.4.m4.1a"><mrow id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml"><msub id="S2.SS4.p1.4.m4.1.1.2" xref="S2.SS4.p1.4.m4.1.1.2.cmml"><mi id="S2.SS4.p1.4.m4.1.1.2.2" xref="S2.SS4.p1.4.m4.1.1.2.2.cmml">l</mi><mi id="S2.SS4.p1.4.m4.1.1.2.3" xref="S2.SS4.p1.4.m4.1.1.2.3.cmml">m</mi></msub><mo id="S2.SS4.p1.4.m4.1.1.1" xref="S2.SS4.p1.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS4.p1.4.m4.1.1.3" xref="S2.SS4.p1.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.1b"><apply id="S2.SS4.p1.4.m4.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1"><eq id="S2.SS4.p1.4.m4.1.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1.1"></eq><apply id="S2.SS4.p1.4.m4.1.1.2.cmml" xref="S2.SS4.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.p1.4.m4.1.1.2.1.cmml" xref="S2.SS4.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS4.p1.4.m4.1.1.2.2.cmml" xref="S2.SS4.p1.4.m4.1.1.2.2">𝑙</ci><ci id="S2.SS4.p1.4.m4.1.1.2.3.cmml" xref="S2.SS4.p1.4.m4.1.1.2.3">𝑚</ci></apply><cn type="integer" id="S2.SS4.p1.4.m4.1.1.3.cmml" xref="S2.SS4.p1.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.1c">l_{m}=40</annotation></semantics></math> in all our experiments.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.5.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.6.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Since quantization is performed before the convolutional sub-sampling, there is a mismatch in the lengths between the predicted tokens and target tokens. To mathc the sequence lengths, target tokens are averaged for every 8 frames, then apply threshold of 0.9 to select frames to be taken into loss calculation. Cross-entropy loss is applied on selected tokens determined by the input masks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset and Settings</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We train the NEST-L (108M) and NEST-XL (600M) models using 100K hours of both English speech data, including 60K hours from LibriLight <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, 24K hours from English subset of Voxpopuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and about 20K hours sampled data from the combination of Fisher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Switchboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, WSJ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, NSC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, People’s Speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The audios for speech augmentation is randomly selected within each batch, while we use noise audios from MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and Freesound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We train the models with global batch size of 2048 for 80K steps on 128 NVIDIA A100 GPUs, with Noam annealing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and peak learning rate of 0.004, weight decay of 1e-3, gradient clipping 1.0 and warm-up of 25K steps. We set the speech augmentation probability as 0.2, among which we set noise and speech augmentation probabilities as 0.1 and 0.9 respectively. Code and checkpoint will be publicly available.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Results on SUPERB Multi-task Speech Processing</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We evaluate our model’s performance on the SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> benchmark for multi-task evaluation on self-supervised speech models. For speech recognition (ASR), phoneme recognition (PR) and speaker diarization (SD) tasks, we use the architecture in the left part of Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b) and a simple linear layer as the task decoder. We train ASR and PR with CTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> loss, while the SD task is trained with permutation invariant loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. For speaker identification/verification (SID/SV), keyword spotting (KS) and emotion recognition (ER) tasks, we resort to the architecture presented in the right part of Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), and use the ECAPA-TDNN-small <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as the task decoder. We following the same train/val/test splits as in the SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and train the models for 100 epochs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As presented in Table <a href="#S2.T1" title="TABLE I ‣ II-B Speech Augmentation ‣ II Approach ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, our NEST-L model is able to outperform WavLM-base++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with similar size of parameters on all tasks, and also outperforms WavLM-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that is 3x as large on speaker verification (SV), speaker diarization (SD) and phoneme recognition (PR). When compared with the XEUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> model that is trained on 10x data, we can see that our NEST-XL model is still able to achieve better performance on all speaker and content tasks, with especially large improvements on speaker verification, speaker diarization and phoneme recognition. Overall, we are able to achieve new state-of-the-art results on SID, SV, SD, PR and ASR tasks compared with WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that has data size as well as XEUS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that is trained on much large data, demonstrating the effectiveness of NEST when applied on various downstream speech processing tasks.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results on speech translation from English to German, French and Spanish. BLEU score is used as the metric, while punctuation and capitalization are included in metric calculation. “*” indicates second best performance.</figcaption>
<div id="S3.T3.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:55.9pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.1pt,17.4pt) scale(0.614266974150471,0.614266974150471) ;">
<table id="S3.T3.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.12.12.12" class="ltx_tr">
<th id="S3.T3.12.12.12.13" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T3.12.12.12.14" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T3.12.12.12.15" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T3.2.2.2.2.m1.1.1" xref="S3.T3.2.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T3.3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.3.m1.1b"><ci id="S3.T3.3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S3.T3.4.4.4.4.m1.1.1" xref="S3.T3.4.4.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.4.m1.1b"><ci id="S3.T3.4.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S3.T3.5.5.5.5.m1.1.1" xref="S3.T3.5.5.5.5.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.5.m1.1b"><ci id="S3.T3.5.5.5.5.m1.1.1.cmml" xref="S3.T3.5.5.5.5.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.5.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S3.T3.6.6.6.6.m1.1.1" xref="S3.T3.6.6.6.6.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.6.m1.1b"><ci id="S3.T3.6.6.6.6.m1.1.1.cmml" xref="S3.T3.6.6.6.6.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.6.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.7.7.7.7.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.7.7.7.7.m1.1a"><mo stretchy="false" id="S3.T3.7.7.7.7.m1.1.1" xref="S3.T3.7.7.7.7.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.7.m1.1b"><ci id="S3.T3.7.7.7.7.m1.1.1.cmml" xref="S3.T3.7.7.7.7.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.7.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.8.8.8.8.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.8.8.8.8.m1.1a"><mo stretchy="false" id="S3.T3.8.8.8.8.m1.1.1" xref="S3.T3.8.8.8.8.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.8.m1.1b"><ci id="S3.T3.8.8.8.8.m1.1.1.cmml" xref="S3.T3.8.8.8.8.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.8.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.9.9.9.9.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.9.9.9.9.m1.1a"><mo stretchy="false" id="S3.T3.9.9.9.9.m1.1.1" xref="S3.T3.9.9.9.9.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.9.m1.1b"><ci id="S3.T3.9.9.9.9.m1.1.1.cmml" xref="S3.T3.9.9.9.9.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.9.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.10.10.10.10.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.10.10.10.10.m1.1a"><mo stretchy="false" id="S3.T3.10.10.10.10.m1.1.1" xref="S3.T3.10.10.10.10.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.10.10.m1.1b"><ci id="S3.T3.10.10.10.10.m1.1.1.cmml" xref="S3.T3.10.10.10.10.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.10.10.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.11.11.11.11.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.11.11.11.11.m1.1a"><mo stretchy="false" id="S3.T3.11.11.11.11.m1.1.1" xref="S3.T3.11.11.11.11.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.11.11.m1.1b"><ci id="S3.T3.11.11.11.11.m1.1.1.cmml" xref="S3.T3.11.11.11.11.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.11.11.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.12.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.12.12.12.12.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.12.12.12.12.m1.1a"><mo stretchy="false" id="S3.T3.12.12.12.12.m1.1.1" xref="S3.T3.12.12.12.12.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.12.12.m1.1b"><ci id="S3.T3.12.12.12.12.m1.1.1.cmml" xref="S3.T3.12.12.12.12.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.12.12.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
</tr>
<tr id="S3.T3.12.12.13.1" class="ltx_tr">
<th id="S3.T3.12.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SeamlessM4T-medium</th>
<th id="S3.T3.12.12.13.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">1.2B</th>
<td id="S3.T3.12.12.13.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4M</td>
<td id="S3.T3.12.12.13.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">28.03</td>
<td id="S3.T3.12.12.13.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">38.44</td>
<td id="S3.T3.12.12.13.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">30.50*</td>
<td id="S3.T3.12.12.13.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">9.65</td>
<td id="S3.T3.12.12.13.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.23</td>
<td id="S3.T3.12.12.13.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">8.64</td>
<td id="S3.T3.12.12.13.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">28.30</td>
<td id="S3.T3.12.12.13.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">21.05</td>
<td id="S3.T3.12.12.13.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">37.36</td>
<td id="S3.T3.12.12.13.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">21.99</td>
<td id="S3.T3.12.12.13.1.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.24</td>
<td id="S3.T3.12.12.13.1.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.50</td>
</tr>
<tr id="S3.T3.12.12.14.2" class="ltx_tr">
<th id="S3.T3.12.12.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">SeamlessM4T-v2-large</th>
<th id="S3.T3.12.12.14.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">2.3B</th>
<td id="S3.T3.12.12.14.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4M</td>
<td id="S3.T3.12.12.14.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.96</td>
<td id="S3.T3.12.12.14.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.32</td>
<td id="S3.T3.12.12.14.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.33</td>
<td id="S3.T3.12.12.14.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.48</td>
<td id="S3.T3.12.12.14.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.89</td>
<td id="S3.T3.12.12.14.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.04</td>
<td id="S3.T3.12.12.14.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.10.1" class="ltx_text ltx_font_bold">33.17</span></td>
<td id="S3.T3.12.12.14.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.11.1" class="ltx_text ltx_font_bold">23.72</span></td>
<td id="S3.T3.12.12.14.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.12.1" class="ltx_text ltx_font_bold">43.05</span></td>
<td id="S3.T3.12.12.14.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.87</td>
<td id="S3.T3.12.12.14.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.31</td>
<td id="S3.T3.12.12.14.2.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.80</td>
</tr>
<tr id="S3.T3.12.12.15.3" class="ltx_tr">
<th id="S3.T3.12.12.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Canary-1B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<th id="S3.T3.12.12.15.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1B</th>
<td id="S3.T3.12.12.15.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86K</td>
<td id="S3.T3.12.12.15.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.4.1" class="ltx_text ltx_font_bold">32.53</span></td>
<td id="S3.T3.12.12.15.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.5.1" class="ltx_text ltx_font_bold">40.84</span></td>
<td id="S3.T3.12.12.15.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.6.1" class="ltx_text ltx_font_bold">30.65</span></td>
<td id="S3.T3.12.12.15.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.7.1" class="ltx_text ltx_font_bold">23.83</span></td>
<td id="S3.T3.12.12.15.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.8.1" class="ltx_text ltx_font_bold">35.73</span></td>
<td id="S3.T3.12.12.15.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.9.1" class="ltx_text ltx_font_bold">28.28</span></td>
<td id="S3.T3.12.12.15.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.15*</td>
<td id="S3.T3.12.12.15.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.66*</td>
<td id="S3.T3.12.12.15.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.76*</td>
<td id="S3.T3.12.12.15.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.13.1" class="ltx_text ltx_font_bold">29.50</span></td>
<td id="S3.T3.12.12.15.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.14.1" class="ltx_text ltx_font_bold">33.07</span></td>
<td id="S3.T3.12.12.15.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.15.1" class="ltx_text ltx_font_bold">33.23</span></td>
</tr>
<tr id="S3.T3.12.12.16.4" class="ltx_tr">
<th id="S3.T3.12.12.16.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">NEST-XL-Transformer</th>
<th id="S3.T3.12.12.16.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">1B</th>
<td id="S3.T3.12.12.16.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">42K</td>
<td id="S3.T3.12.12.16.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">30.87*</td>
<td id="S3.T3.12.12.16.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">39.95*</td>
<td id="S3.T3.12.12.16.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">30.01</td>
<td id="S3.T3.12.12.16.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">22.82*</td>
<td id="S3.T3.12.12.16.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">34.92*</td>
<td id="S3.T3.12.12.16.4.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">27.99*</td>
<td id="S3.T3.12.12.16.4.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">29.50</td>
<td id="S3.T3.12.12.16.4.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">22.61</td>
<td id="S3.T3.12.12.16.4.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">39.27</td>
<td id="S3.T3.12.12.16.4.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">27.73*</td>
<td id="S3.T3.12.12.16.4.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">32.51*</td>
<td id="S3.T3.12.12.16.4.15" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">32.42*</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>DER results on speaker diarization. EEND-GLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> applies additional clustering after end-to-end diarizer, so we exclude it from the comparison for fairness. “*” indicates second best.</figcaption>
<div id="S3.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:145.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.8pt,11.3pt) scale(0.865249451761731,0.865249451761731) ;">
<table id="S3.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.2.1" class="ltx_tr">
<th id="S3.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T4.1.1.2.1.1.1" class="ltx_text">Model</span></th>
<td id="S3.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DIHARD3 eval</td>
<td id="S3.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">CALLHOME-part2</td>
</tr>
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S3.T4.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.T4.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.1.1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.1.1.1.m1.1b"><leq id="S3.T4.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.1.1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.1.1.1.m1.1c">\leq</annotation></semantics></math>4 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.1.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.0</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">4 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.1.3.2" class="ltx_tr">
<th id="S3.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">EEND-GLA-small <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S3.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.39</td>
<td id="S3.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6.94</td>
<td id="S3.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.42</td>
<td id="S3.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.49</td>
</tr>
<tr id="S3.T4.1.1.4.3" class="ltx_tr">
<th id="S3.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">EEND-EDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<td id="S3.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.2.1" class="ltx_text ltx_font_bold">15.55</span></td>
<td id="S3.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.3.1" class="ltx_text ltx_font_bold">7.83</span></td>
<td id="S3.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.29</td>
<td id="S3.T4.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.5.1" class="ltx_text ltx_font_bold">17.59</span></td>
</tr>
<tr id="S3.T4.1.1.5.4" class="ltx_tr">
<th id="S3.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NeMo MSDD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S3.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.40</td>
<td id="S3.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.41</td>
<td id="S3.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.45</td>
<td id="S3.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.49</td>
</tr>
<tr id="S3.T4.1.1.6.5" class="ltx_tr">
<th id="S3.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">RandFC-L-Linear</th>
<td id="S3.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.68</td>
<td id="S3.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.17</td>
<td id="S3.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">19.16</td>
<td id="S3.T4.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">24.47</td>
</tr>
<tr id="S3.T4.1.1.7.6" class="ltx_tr">
<th id="S3.T4.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NEST-L-Linear</th>
<td id="S3.T4.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.63</td>
<td id="S3.T4.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.56</td>
<td id="S3.T4.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.85</td>
<td id="S3.T4.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.73</td>
</tr>
<tr id="S3.T4.1.1.8.7" class="ltx_tr">
<th id="S3.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-L-Sortformer <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite>
</th>
<td id="S3.T4.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17.06*</td>
<td id="S3.T4.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.91*</td>
<td id="S3.T4.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10.34*</td>
<td id="S3.T4.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">23.07</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Results on Multi-lingual Speech Recognition</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Besides multi-task evaluation, we also study if an SSL model trained on single language can help other languages. To this end, we train an ASR model on four different languages: English (En), German (De), French (Fr), Spanish (Es). Specifically, we train an ASR model using NEST-XL as weight initialization and the hybrid-CTC-RNNT loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The training data comprises of 2.5K hours of German speech (MCV, MLS, Voxpopuli), 8.5K hours of English speech (MCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, MLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, Voxpopuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, SPGI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, Europarl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, NSC1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>), 1.4K hours of Spanish speech (MCV, MLS, Voxpopuli, Fisher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) and 1.9K hours of French speech (MCV, MLS, Voxpopuli). For baselines, we train another model using an English ASR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> as weight initialization, and also include some of the best ASR models like Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We run all models with the same beam size 5 without extra language models on test sets of MCV-16.1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Voxpopuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">From the last two rows of Table <a href="#S2.T2" title="TABLE II ‣ II-B Speech Augmentation ‣ II Approach ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that NEST can help achieve better WER on all datasets than the model with ASR pretrained initialization, which shows that NEST can help improve ASR performance on languages that is not seen during SSL pretraining. In addition, when compared with other SOTA ASR models (Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) trained with much more parameters and data, we are still able to match the performance of Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on averaged WER across all languages. On some of the datasets, although there is still a gap between our model’s performance and that of the SOTA models trained with much more data, we can still see that NEST can be used as an efficient way to obtain superior ASR performance comparable to models trained on massive datasets.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Results on Speech Translation</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">We further study how NEST can help speech-to-text translation (AST) and present the results in Table <a href="#S3.T3" title="TABLE III ‣ III-B Results on SUPERB Multi-task Speech Processing ‣ III Experiments ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We use the same model architecture and training procedure as proposed in Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, while the training data contains 42K hours of English ASR data with machine generated translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> from English (En) to German (De), French (Fr) and Spanish (Es) text. We compare our model with other state-of-the-art AST models SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on Europarl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, mExpresso <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and FLEURS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> test sets. As we can see, given the same number of parameters, due to much less training data used in our model, there is still a gap between Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and our model on all evaluated datasets. Also, given that Canary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is initialized with a multi-lingual ASR encoder that is pretrained on all of the evaluated languages, it is expected that Canary performs better than the NEST initialization. Nonetheless, our model is able to outperform SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and achieves the second best average BLEU scores on En<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mo stretchy="false" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\rightarrow</annotation></semantics></math>De, En<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mo stretchy="false" id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\rightarrow</annotation></semantics></math>Es and En<math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mo stretchy="false" id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\rightarrow</annotation></semantics></math>Fr translations, showing that the proposed NEST model is able to help achieve impressive AST performance with less data.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Results on Speaker Diarization</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">To evaluate our model’s performance on speaker diarization, we follow the Sortformer <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite> architecture and attach 18 layers of transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> after the NEST encoder. We also train a simpler baseline with only FastConformer encoder and a linear decoder, with either random initialization or NEST initialization. For training data, we use a combination of real data (Fisher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) and simulated data (composed from LibriSeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, SRE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>) generated by the NeMo speech data simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, with a total of 6.4K hours. We compare with state-of-the-art diarization models EEND-EDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and EEND-GLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and show the results in Table <a href="#S3.T4" title="TABLE IV ‣ III-B Results on SUPERB Multi-task Speech Processing ‣ III Experiments ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">By comparing RandFC-L-Linear and NEST-L-Linear, we can see that NEST is able to significantly reduce the DER of a simple FC-Linear baseline, showing the importance of using NEST in speaker diarization. We can also notice that Sortformer <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite> with NEST initialization is able to achieve second best results on three of the four evaluated test sets, which are comparable to the state-of-the-art EEND-EDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results on SLURP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> benchmark for speech joint intent detection and slot filling.</figcaption>
<div id="S3.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:144pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.8pt,12.1pt) scale(0.855036540700733,0.855036540700733) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SSL</td>
</tr>
<tr id="S3.T5.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Data (hrs)</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Intent</td>
</tr>
<tr id="S3.T5.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Precision</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Recall</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">F1</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.2.1" class="ltx_tr">
<th id="S3.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SpeechBrain-Hubert-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<td id="S3.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">LL-60K</td>
<td id="S3.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">89.37</td>
<td id="S3.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.54</td>
<td id="S3.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.44</td>
<td id="S3.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.96</td>
</tr>
<tr id="S3.T5.1.1.3.2" class="ltx_tr">
<th id="S3.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ESPnet-Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</th>
<td id="S3.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.30</td>
<td id="S3.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.40</td>
</tr>
<tr id="S3.T5.1.1.4.3" class="ltx_tr">
<th id="S3.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Open-BEST-RQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S3.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS-960</td>
<td id="S3.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.80</td>
<td id="S3.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
</tr>
<tr id="S3.T5.1.1.5.4" class="ltx_tr">
<th id="S3.T5.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Wav2vec-CTI-RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<td id="S3.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS-960</td>
<td id="S3.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.92</td>
<td id="S3.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.66</td>
</tr>
<tr id="S3.T5.1.1.6.5" class="ltx_tr">
<th id="S3.T5.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NeMo-SSL-FC-Trans-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S3.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LL-60K</td>
<td id="S3.T5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.40</td>
<td id="S3.T5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.90</td>
<td id="S3.T5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.65</td>
<td id="S3.T5.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.22</td>
</tr>
<tr id="S3.T5.1.1.7.6" class="ltx_tr">
<th id="S3.T5.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">NEST-L-Transformer</th>
<td id="S3.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-100K</td>
<td id="S3.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.1.1.7.6.3.1" class="ltx_text ltx_font_bold">89.79</span></td>
<td id="S3.T5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.55</td>
<td id="S3.T5.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.1.1.7.6.5.1" class="ltx_text ltx_font_bold">78.70</span>
</td>
<td id="S3.T5.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">79.61</td>
</tr>
<tr id="S3.T5.1.1.8.7" class="ltx_tr">
<th id="S3.T5.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-XL-Transformer</th>
<td id="S3.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">En-100K</td>
<td id="S3.T5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">89.04</td>
<td id="S3.T5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.1.1.8.7.4.1" class="ltx_text ltx_font_bold">82.35</span>
</td>
<td id="S3.T5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">78.36</td>
<td id="S3.T5.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.1.1.8.7.6.1" class="ltx_text ltx_font_bold">80.31</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Results on Spoken Language Understanding</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">For spoken language understanding, we focus on the <em id="S3.SS6.p1.1.1" class="ltx_emph ltx_font_italic">joint intent detection and slot filling</em> task and evaluate our model’s performance using the SLURP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> dataset. Specifically, we attach a transformer decoder to the NEST encoder, and use the same hyper-parameter setting as in NeMo-SLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. We compare with other SSL-based end-to-end SLU models and show the results in Table <a href="#S3.T5" title="TABLE V ‣ III-E Results on Speaker Diarization ‣ III Experiments ‣ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. For fair comparison, we do not include the ASR pretrained baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> as we focus on SSL.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">As we can see, among all SSL-based SLU models, the proposed NEST model achieves the best performance on both intent detection accuracy and slot filling F1 scores. We also notice that scaling up from NEST-L to NEST-XL does bring some improvement on precision score on slot filling, but do not have significant effects on other metrics. In addition, compared with the NeMo-SSL-FC-Trans-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> baseline, we can see a more than 2% absolute improvement on F1 score by merely replacing the SSL speech encoder with NEST while keeping other hyper-parameters the same, which demonstrate the instant benefits that NEST can bring to existing speech processing models.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we propose a self-supervised model <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">NEST </span>, which is faster than previous SSL models by applying FastConformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as the speech encoder. We simplify the audio quantization by applying fixed random-projection quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and also generalize the noisy speech augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to allow for more randomness and speakers. Extensive experiments on multiple speech processing tasks show that the NEST model can help achieve state-of-the-art performance. Code, configurations and checkpoints are also publicly available through NVIDIA NeMo toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Devlin, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 12 449–12 460, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Sadhu, D. He, C.-W. Huang, S. H. Mallidi, M. Wu, A. Rastrow, A. Stolcke, J. Droppo, and R. Maas, “Wav2vec-c: A self-supervised model for speech representation learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.08393</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-supervised learning of discrete speech representations,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.05453</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Jiang, W. Li, M. Cao, W. Zou, and X. Li, “Speech simclr: Combining contrastive and reconstruction objective for self-supervised speech representation learning,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.13991</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM transactions on audio, speech, and language processing</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, “Self-supervised learning with random-projection quantizer for speech recognition,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 3915–3924.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. Chen, W. Zhang, Y. Peng, X. Li, J. Tian, J. Shi, X. Chang, S. Maiti, K. Livescu, and S. Watanabe, “Towards robust speech representation learning for thousands of languages,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.00837</em>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 000–16 009.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec: A general framework for self-supervised learning in speech, vision and language,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 1298–1312.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Van Den Oord, O. Vinyals <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Neural discrete representation learning,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. Von Platen, Y. Saraf, J. Pino <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Xls-r: Self-supervised cross-lingual speech representation learning at scale,” <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, “W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2021, pp. 244–250.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Conformer: Convolution-augmented transformer for speech recognition,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.08100</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Vaswani, “Attention is all you need,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03762</em>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Rekesh, N. R. Koluguri, S. Kriman, S. Majumdar, V. Noroozi, H. Huang, O. Hrinchuk, K. Puvvada, A. Kumar, J. Balam <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Fast conformer with linearly scalable attention for efficient speech recognition,” in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Superb: Speech processing universal performance benchmark,” <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Seamless: Multilingual expressive and streaming speech translation,” <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.05187</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. C. Puvvada, P. Żelasko, H. Huang, O. Hrinchuk, N. R. Koluguri, K. Dhawan, S. Majumdar, E. Rastorgueva, Z. Chen, V. Lavrukhin <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Less is more: Accurate speech recognition &amp; translation without web-scale data,” <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Libri-light: A benchmark for asr with limited or no supervision,” in <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 7669–7673.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, “Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00390</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C. Cieri, D. Miller, and K. Walker, “The fisher corpus: A resource for the next generations of speech-to-text.” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">LREC</em>, vol. 4, 2004, pp. 69–71.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. H. John J. Godfrey, “Switchboard-1 release 2,” <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC97S62" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC97S62</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. S. Garofolo, D. Graff, D. Paul, and D. Pallett, “Csr-i (wsj0) complete,” <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC93S6A" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC93S6A</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. X. Koh, A. Mislan, K. Khoo, B. Ang, W. Ang, C. Ng, and Y. Tan, “Building the singapore english national speech corpus,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Malay</em>, vol. 20, no. 25.0, pp. 19–3, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. Galvez, G. Diamos, J. Ciro, J. F. Cerón, K. Achorn, A. Gopi, D. Kanter, M. Lam, M. Mazumder, and V. J. Reddi, “The people’s speech: A large-scale diverse english speech recognition dataset for commercial usage,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09344</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. Snyder, G. Chen, and D. Povey, “Musan: A music, speech, and noise corpus,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
E. Fonseca, J. Pons Puig, X. Favory, F. Font Corbera, D. Bogdanov, A. Ferraro, S. Oramas, A. Porter, and X. Serra, “Freesound datasets: a platform for the creation of open audio datasets,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Hu X, Cunningham SJ, Turnbull D, Duan Z, editors. Proceedings of the 18th ISMIR Conference; 2017 oct 23-27; Suzhou, China.[Canada]: International Society for Music Information Retrieval; 2017. p. 486-93.</em>   International Society for Music Information Retrieval (ISMIR), 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine learning</em>, 2006, pp. 369–376.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with self-attention,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2019, pp. 296–303.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, “Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.07143</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Horiguchi, S. Watanabe, P. García, Y. Takashima, and Y. Kawaguchi, “Online neural diarization of unlimited numbers of speakers using global and local attractors,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 31, pp. 706–720, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and P. Garcia, “Encoder-decoder based attractors for end-to-end neural diarization,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 1493–1507, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. J. Park, N. R. Koluguri, J. Balam, and B. Ginsburg, “Multi-scale speaker diarization with dynamic scale weighting,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15974</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
V. Noroozi, S. Majumdar, A. Kumar, J. Balam, and B. Ginsburg, “Stateful conformer with cache-based inference for streaming automatic speech recognition,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 12 041–12 045.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice: A massively-multilingual speech corpus,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</em>, 2020, pp. 4211–4215.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “Mls: A large-scale multilingual dataset for speech research,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.03411</em>, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. K. O’Neill, V. Lavrukhin, S. Majumdar, V. Noroozi, Y. Zhang, O. Kuchaiev, J. Balam, Y. Dovzhenko, K. Freyberg, M. D. Shulman <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Spgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition,” <em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.02014</em>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Iranzo-Sánchez, J. A. Silvestre-Cerda, J. Jorge, N. Roselló, A. Giménez, A. Sanchis, J. Civera, and A. Juan, “Europarl-st: A multilingual corpus for speech translation of parliamentary debates,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 8229–8233.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
NVIDIA, “Nemo english fastconformer-rnnt asr model,” <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
——, “Megatron multilingual translation model,” <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
META, “mexpresso (multilingual expresso),” <a target="_blank" href="https://huggingface.co/facebook/seamless-expressive#mexpresso-multilingual-expresso" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/facebook/seamless-expressive#mexpresso-multilingual-expresso</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, “Fleurs: Few-shot learning evaluation of universal representations of speech,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</em>.   IEEE, 2023, pp. 798–805.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
G. R. Doddington, M. A. Przybocki, A. F. Martin, and D. A. Reynolds, “The nist speaker recognition evaluation–overview, methodology, systems, results, perspective,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Speech communication</em>, vol. 31, no. 2-3, pp. 225–254, 2000.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
NIST, “Nist speaker recognition evaluation (sre),” <a target="_blank" href="https://www.nist.gov/itl/iad/mig/speaker-recognition" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nist.gov/itl/iad/mig/speaker-recognition</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
T. J. Park, H. Huang, C. Hooper, N. Koluguri, K. Dhawan, A. Jukic, J. Balam, and B. Ginsburg, “Property-aware multi-speaker data simulation: A probabilistic modelling technique for synthetic data generation,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.12371</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, “Slurp: A spoken language understanding resource package,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.13205</em>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02735</em>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S. Arora, S. Dalmia, P. Denisov, X. Chang, Y. Ueda, Y. Peng, Y. Zhang, S. Kumar, K. Ganesan, B. Yan <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Espnet-slu: Advancing spoken language understanding through espnet,” in <em id="bib.bib52.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 7167–7171.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R. Whetten, T. Parcollet, M. Dinarelli, and Y. Estève, “Open implementation and study of best-rq for speech processing,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.04296</em>, 2024.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S. Seo, D. Kwak, and B. Lee, “Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 7152–7156.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
H. Huang, J. Balam, and B. Ginsburg, “Leveraging pretrained asr encoders for effective and efficient end-to-end speech intent classification and slot filling,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
NVIDIA, “Nemo toolkit,” <a target="_blank" href="https://github.com/NVIDIA/NeMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13105" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13106" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13106">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13106" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13107" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:22:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
