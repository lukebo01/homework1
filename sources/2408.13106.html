<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13106] NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks</title><meta property="og:description" content="Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are compâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13106">

<!--Generated on Thu Sep  5 13:22:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
self-supervised learning,  speech recognition,  speaker diarization,  spoken language understanding
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="id1.1.id1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1" class="ltx_td ltx_align_center">He Huang, Taejin Park, Kunal Dhawan, Ivan Medennikov, Krishna C. Puvvada,</span></span>
<span id="id1.1.id1.2.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1" class="ltx_td ltx_align_center">Nithin Rao Koluguri, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</span></span>
<span id="id1.1.id1.3.3" class="ltx_tr">
<span id="id1.1.id1.3.3.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.3.3.1.1" class="ltx_text ltx_font_italic">NVIDIA, Santa Clara, CA, USA</span></span></span>
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive due to using transformer encoder and lack of sub-sampling. In this paper, we propose a new self-supervised learning model termed as <em id="id2.id1.1" class="ltx_emph ltx_font_italic">pretraiNed Encoder for Speech Tasks</em> (<span id="id2.id1.2" class="ltx_text ltx_font_bold">NEST</span>). Specifically, we adopt the FastConformer architecture, which has an 8x sub-sampling rate and is faster than Transformer or Conformer architectures. Instead of clustering-based token generation, we resort to fixed random projection for its simplicity and effectiveness. We also propose a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that the proposed NESTÂ improves over existing self-supervised models on a variety of speech processing tasks. Code and checkpoints will be publicly available via NVIDIA NeMo toolkit<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">https://github.com/NVIDIA/NeMo</span></span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
self-supervised learning, speech recognition, speaker diarization, spoken language understanding

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Most recent speech self-supervised models are inspired by the BERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> model, which learn text token embedding by predicting the target of the masked positions given the context of the unmasked ones.
Among them, are two main streams of <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">contrastive</em> and <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">predictive</em> models. The <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">contrastive</em> approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> quantizes the speech features into a set of target feature <em id="S1.p1.1.4" class="ltx_emph ltx_font_italic">vectors</em> and trains with a contrastive loss using the positive and negative target features. Meanwhile, the <em id="S1.p1.1.5" class="ltx_emph ltx_font_italic">predictive</em> approachÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> quantizes the speech features into <em id="S1.p1.1.6" class="ltx_emph ltx_font_italic">tokens</em> and train with masked token prediction loss as in BERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In addition to the two approaches, some works also learn from the masked auto-encodingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> approach and train speech self-supervised models with a <em id="S1.p1.1.7" class="ltx_emph ltx_font_italic">reconstruction</em> objectiveÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One representative work of <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">contrastive</em> models is Wav2vec-2.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which demonstrates initializing ASR models from SSL checkpoints can outperform previous semi-supervised and train-from-scratch ASR models. Later, Wav2vec-CÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> improves over Wav2vec-2.0 by adding a consistency loss to reconstruct the quantized embedding, similar to VQ-VAEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. XLS-RÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> extends Wav2vec-2.0 to multilingual setting and shows impressive performance on multilingual speech recognition and translation.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.13106/assets/figures/nest_tasks.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>NEST serves as a bird nest that incubates the variety of speech task models.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">HuBERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, as a pioneer work of the <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">predictive</em> approach, generates the target tokens by running k-means clustering on the middle layer features extracted from anohter SSL model that is pretrained for a small number of steps. Then, W2v-BERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposes to combine the training objectives of both Wav2vec-2.0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and HuBERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> by applying contrastive loss on the middle layer output while predictive loss at the final output layer. Later, BEST-RQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> shows that the clustering based token generation can be replaced by simple fixed random-projection quantization, and this simple modification is able to match or outperform HuBERT on ASR.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2408.13106/assets/figures/nest-model2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) The proposed NESTÂ architecture. (b) Two ways to use NESTÂ encoder: (left) use as weight initialization for tasks that require more parameters (<span id="S1.F2.3.1" class="ltx_text ltx_font_italic">e.g.</span>, speech recognition); (right) learn weighted summation of features from different layers of the frozen NESTÂ for tasks that require less trainable parameters (<span id="S1.F2.4.2" class="ltx_text ltx_font_italic">e.g.,</span> speaker verification).</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to improve performance on speaker tasks, WavLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposes a noisy speech augmentation technique and a <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">denoising masked token prediction</em> objective, by adding a speech segment of a different speaker to the current speech and training the model to predict the target tokens generated using original clean speech. XEUSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> further extends WavLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> by adding a de-reverberation task and extending to multilingual data of 1M hours.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">However, previous SSL models also have their different limitations. First, some modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> use CNN+transformer architecture which has a short frame length of 20ms, which slows down the modelâ€™s inference speed. Second, the HuBERT-style quantization is very computationally expensive, which could take 20% of total training time according to XEUSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Third, although BEST-RQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> uses ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> encoder with 40ms frame length and simple random quantization, it lacks the ability to explicitly tell one speaker from another, which limits its performance on speaker tasks (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, speaker diarization).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we tackle all these challenges and bring the best practices from previous works, which constitute the proposed <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">pretraiNed Encoder for Speech Tasks</em> (<span id="S1.p6.1.2" class="ltx_text ltx_font_bold">NEST</span>)Â framework. Our contributions are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A new speech SSL model that achieves SOTA performance with a simplified and more efficient framework.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Experiments show that NESTÂ can help achieve SOTA performance on a variety of downstream tasks (ASR, AST, SLU, SD, etc).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Different from previous SSL works that focus mostly on downstream tasks with limited data, we also show that NESTÂ can benefit speech recognition and translation even when data is relatively larger.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">To the best of our knowledge, we are the first to show that SSL model trained on English data can also help improve speech recognition on other languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Approach</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce the proposed NESTÂ framework for learning efficient self-supervised speech encoder, as illustrated in FigureÂ <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Speech Encoder</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Current SOTA speech SSL modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> mostly use transformer encoderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> or ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as speech encoder, which have either 20ms or 40ms frame length. Here we choose the more efficient FastConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> which applies 8x convolutional sub-sampling on the input Mel-spectrogram before the following FastConformer layers, resulting in an 80ms frame length that can significantly reduce the sequence length to be processed by self-attention layers.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Speech Augmentation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We augment the input speech with random noise or speech of another speaker, similar to the techniques proposed in WavLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, we generalize the augmentation in three ways: (1) the length of augmentation audio is sampled between 0.4 and 0.6 of the primary audio length, instead of a fixed 0.5 ratio; (2) the length of augmentation audio is randomly split into 1, 2 or 3 segments with uniform probability, such that the augmentation is scattered to different positions of the primary audio; (3) instead of using single negative speaker, for each segment with speaker augmentation, we randomly select a different speaker from other speakers in the same batch, such that there can be more speakers in the resulted audios.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results on SUPERBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> benchmark for multi-task evaluation on SSL speech encoders.</figcaption>
<div id="S2.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:69.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-175.0pt,28.1pt) scale(0.553303924616124,0.553303924616124) ;">
<table id="S2.T1.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.7.7.8.1" class="ltx_tr">
<th id="S2.T1.7.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.1.1" class="ltx_text">Model</span></th>
<th id="S2.T1.7.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.2.1" class="ltx_text">Params</span></th>
<td id="S2.T1.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.7.7.8.1.3.1" class="ltx_text">SSL Data (hrs)</span></td>
<td id="S2.T1.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Speaker</td>
<td id="S2.T1.7.7.8.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Content</td>
<td id="S2.T1.7.7.8.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ParaLinguistics</td>
</tr>
<tr id="S2.T1.7.7.7" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">SID (Acc <math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">SV (EER <math id="S2.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">SD (DER <math id="S2.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S2.T1.3.3.3.3.m1.1.1" xref="S2.T1.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.m1.1b"><ci id="S2.T1.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">PR (PER <math id="S2.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S2.T1.4.4.4.4.m1.1.1" xref="S2.T1.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.m1.1b"><ci id="S2.T1.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r">ASR (WER <math id="S2.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S2.T1.5.5.5.5.m1.1.1" xref="S2.T1.5.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.m1.1b"><ci id="S2.T1.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r">KS (Acc <math id="S2.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S2.T1.6.6.6.6.m1.1.1" xref="S2.T1.6.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.m1.1b"><ci id="S2.T1.6.6.6.6.m1.1.1.cmml" xref="S2.T1.6.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r">ER (Acc <math id="S2.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.7.7.7.7.m1.1a"><mo stretchy="false" id="S2.T1.7.7.7.7.m1.1.1" xref="S2.T1.7.7.7.7.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.m1.1b"><ci id="S2.T1.7.7.7.7.m1.1.1.cmml" xref="S2.T1.7.7.7.7.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S2.T1.7.7.9.2" class="ltx_tr">
<th id="S2.T1.7.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">WavLM-base++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S2.T1.7.7.9.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">95M</th>
<td id="S2.T1.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-96K</td>
<td id="S2.T1.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.84</td>
<td id="S2.T1.7.7.9.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.26</td>
<td id="S2.T1.7.7.9.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.07</td>
<td id="S2.T1.7.7.9.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4.07</td>
<td id="S2.T1.7.7.9.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5.59</td>
<td id="S2.T1.7.7.9.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.69</td>
<td id="S2.T1.7.7.9.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">67.98</td>
</tr>
<tr id="S2.T1.7.7.10.3" class="ltx_tr">
<th id="S2.T1.7.7.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">WavLM-largeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S2.T1.7.7.10.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">316M</th>
<td id="S2.T1.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En-96K</td>
<td id="S2.T1.7.7.10.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.25</td>
<td id="S2.T1.7.7.10.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.04</td>
<td id="S2.T1.7.7.10.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.47</td>
<td id="S2.T1.7.7.10.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.09</td>
<td id="S2.T1.7.7.10.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.44</td>
<td id="S2.T1.7.7.10.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.40</td>
<td id="S2.T1.7.7.10.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.03</td>
</tr>
<tr id="S2.T1.7.7.11.4" class="ltx_tr">
<th id="S2.T1.7.7.11.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">XEUSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<th id="S2.T1.7.7.11.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">577M</th>
<td id="S2.T1.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MulLing-1M</td>
<td id="S2.T1.7.7.11.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.70</td>
<td id="S2.T1.7.7.11.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.16</td>
<td id="S2.T1.7.7.11.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.11</td>
<td id="S2.T1.7.7.11.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.21</td>
<td id="S2.T1.7.7.11.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.34</td>
<td id="S2.T1.7.7.11.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.7.7.11.4.9.1" class="ltx_text ltx_font_bold">98.32</span></td>
<td id="S2.T1.7.7.11.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.7.7.11.4.10.1" class="ltx_text ltx_font_bold">71.08</span></td>
</tr>
<tr id="S2.T1.7.7.12.5" class="ltx_tr">
<th id="S2.T1.7.7.12.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">NEST-L</th>
<th id="S2.T1.7.7.12.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">108M</th>
<td id="S2.T1.7.7.12.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-100K</td>
<td id="S2.T1.7.7.12.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">94.94</td>
<td id="S2.T1.7.7.12.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.85</td>
<td id="S2.T1.7.7.12.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.28</td>
<td id="S2.T1.7.7.12.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.95</td>
<td id="S2.T1.7.7.12.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.49</td>
<td id="S2.T1.7.7.12.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">96.85</td>
<td id="S2.T1.7.7.12.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">68.12</td>
</tr>
<tr id="S2.T1.7.7.13.6" class="ltx_tr">
<th id="S2.T1.7.7.13.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-XL</th>
<th id="S2.T1.7.7.13.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">600M</th>
<td id="S2.T1.7.7.13.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">En-100K</td>
<td id="S2.T1.7.7.13.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.4.1" class="ltx_text ltx_font_bold">95.76</span></td>
<td id="S2.T1.7.7.13.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.5.1" class="ltx_text ltx_font_bold">2.49</span></td>
<td id="S2.T1.7.7.13.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.6.1" class="ltx_text ltx_font_bold">1.89</span></td>
<td id="S2.T1.7.7.13.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.7.1" class="ltx_text ltx_font_bold">1.80</span></td>
<td id="S2.T1.7.7.13.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.7.7.13.6.8.1" class="ltx_text ltx_font_bold">3.19</span></td>
<td id="S2.T1.7.7.13.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">97.11</td>
<td id="S2.T1.7.7.13.6.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">69.94</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results on multi-lingual ASR with punctuation and capitalization. Performance is evaluated by word error rate (WER) including native punctuation and capitalization from the source datasets.</figcaption>
<div id="S2.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:76.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-193.6pt,34.0pt) scale(0.528233443041959,0.528233443041959) ;">
<table id="S2.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S2.T2.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.2.1" class="ltx_text">Params</span></th>
<td id="S2.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.3.1" class="ltx_text">Data (hrs)</span></td>
<td id="S2.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">En</td>
<td id="S2.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">De</td>
<td id="S2.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Es</td>
<td id="S2.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Fr</td>
<td id="S2.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.1.1.1.1.8.1" class="ltx_text">Avg</span></td>
</tr>
<tr id="S2.T2.1.1.2.2" class="ltx_tr">
<td id="S2.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
<td id="S2.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r">MCV16.1</td>
<td id="S2.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r">Voxpopuli</td>
</tr>
<tr id="S2.T2.1.1.3.3" class="ltx_tr">
<th id="S2.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SeamlessM4T-medium-v1Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S2.T2.1.1.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">1.2B</th>
<td id="S2.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4M</td>
<td id="S2.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.20</td>
<td id="S2.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.02</td>
<td id="S2.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.25</td>
<td id="S2.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.20</td>
<td id="S2.T2.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.43</td>
<td id="S2.T2.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.01</td>
<td id="S2.T2.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">17.34</td>
<td id="S2.T2.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.49</td>
<td id="S2.T2.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">13.11</td>
</tr>
<tr id="S2.T2.1.1.4.4" class="ltx_tr">
<th id="S2.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">SeamlessM4T-large-v2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S2.T2.1.1.4.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">2.3B</th>
<td id="S2.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4M</td>
<td id="S2.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.13</td>
<td id="S2.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.77</td>
<td id="S2.T2.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.4.4.6.1" class="ltx_text ltx_font_bold">7.53</span></td>
<td id="S2.T2.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.39</td>
<td id="S2.T2.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.671</td>
<td id="S2.T2.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.53</td>
<td id="S2.T2.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.4.4.10.1" class="ltx_text ltx_font_bold">14.37</span></td>
<td id="S2.T2.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.13</td>
<td id="S2.T2.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.44</td>
</tr>
<tr id="S2.T2.1.1.5.5" class="ltx_tr">
<th id="S2.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Whisper-large-v3Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<th id="S2.T2.1.1.5.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1.5B</th>
<td id="S2.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5M</td>
<td id="S2.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.73</td>
<td id="S2.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.42</td>
<td id="S2.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.24</td>
<td id="S2.T2.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.41</td>
<td id="S2.T2.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.95</td>
<td id="S2.T2.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.31</td>
<td id="S2.T2.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.35</td>
<td id="S2.T2.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.58</td>
<td id="S2.T2.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.49</td>
</tr>
<tr id="S2.T2.1.1.6.6" class="ltx_tr">
<th id="S2.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Canary-1bÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<th id="S2.T2.1.1.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1B</th>
<td id="S2.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86k</td>
<td id="S2.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.4.1" class="ltx_text ltx_font_bold">12.46</span></td>
<td id="S2.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.5.1" class="ltx_text ltx_font_bold">7.52</span></td>
<td id="S2.T2.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.71</td>
<td id="S2.T2.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.32</td>
<td id="S2.T2.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.8.1" class="ltx_text ltx_font_bold">8.28</span></td>
<td id="S2.T2.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.56</td>
<td id="S2.T2.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.46</td>
<td id="S2.T2.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.1.1.6.6.11.1" class="ltx_text ltx_font_bold">8.78</span></td>
<td id="S2.T2.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.76</td>
</tr>
<tr id="S2.T2.1.1.7.7" class="ltx_tr">
<th id="S2.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">FastConformer-XL-hybrid (ASR init)</th>
<th id="S2.T2.1.1.7.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">600M</th>
<td id="S2.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14K</td>
<td id="S2.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.78</td>
<td id="S2.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.21</td>
<td id="S2.T2.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.17</td>
<td id="S2.T2.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.69</td>
<td id="S2.T2.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.75</td>
<td id="S2.T2.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.19</td>
<td id="S2.T2.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.42</td>
<td id="S2.T2.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.89</td>
<td id="S2.T2.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.76</td>
</tr>
<tr id="S2.T2.1.1.8.8" class="ltx_tr">
<th id="S2.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">NEST-XL-hybrid</th>
<th id="S2.T2.1.1.8.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">600M</th>
<td id="S2.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">14K</td>
<td id="S2.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">14.43</td>
<td id="S2.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">7.58</td>
<td id="S2.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">8.07</td>
<td id="S2.T2.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.7.1" class="ltx_text ltx_font_bold">11.83</span></td>
<td id="S2.T2.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">8.70</td>
<td id="S2.T2.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.9.1" class="ltx_text ltx_font_bold">9.27</span></td>
<td id="S2.T2.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">16.18</td>
<td id="S2.T2.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">9.74</td>
<td id="S2.T2.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S2.T2.1.1.8.8.12.1" class="ltx_text ltx_font_bold">10.72</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Speech Quantization</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We resort to BEST-RQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as our speech quantization method. Specifically, we use a single randomly initialized and frozen codebook of 8192 vocabulary and 16 dimension features. A randomly initialized and frozen linear layer is applied to the input Mel-spectrogram features to project them into the same dimension as the codebook, then a nearest neighbor search is applied to obtain the target tokens.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Feature Masking</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.4" class="ltx_p">We employ a random block-wise masking mechanism on the input Mel-spectrogram features, where each frame in the input has a probability <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="p_{m}" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><msub id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">ğ‘</ci><ci id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">p_{m}</annotation></semantics></math> as being selected as the start of a masking block. After randomly selecting a set of starting frames, we mask <math id="S2.SS4.p1.2.m2.1" class="ltx_Math" alttext="l_{m}" display="inline"><semantics id="S2.SS4.p1.2.m2.1a"><msub id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml"><mi id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">l</mi><mi id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.p1.2.m2.1.1.2">ğ‘™</ci><ci id="S2.SS4.p1.2.m2.1.1.3.cmml" xref="S2.SS4.p1.2.m2.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">l_{m}</annotation></semantics></math> consecutive frames for each of the starting frames. Note that there could be overlapping between two masked blocks, which allows for arbitrary lengths in the resulting masked segments that do not overlap with each other. We use <math id="S2.SS4.p1.3.m3.1" class="ltx_Math" alttext="p_{m}=0.01" display="inline"><semantics id="S2.SS4.p1.3.m3.1a"><mrow id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml"><msub id="S2.SS4.p1.3.m3.1.1.2" xref="S2.SS4.p1.3.m3.1.1.2.cmml"><mi id="S2.SS4.p1.3.m3.1.1.2.2" xref="S2.SS4.p1.3.m3.1.1.2.2.cmml">p</mi><mi id="S2.SS4.p1.3.m3.1.1.2.3" xref="S2.SS4.p1.3.m3.1.1.2.3.cmml">m</mi></msub><mo id="S2.SS4.p1.3.m3.1.1.1" xref="S2.SS4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS4.p1.3.m3.1.1.3" xref="S2.SS4.p1.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b"><apply id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1"><eq id="S2.SS4.p1.3.m3.1.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1.1"></eq><apply id="S2.SS4.p1.3.m3.1.1.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.2.1.cmml" xref="S2.SS4.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.2.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2.2">ğ‘</ci><ci id="S2.SS4.p1.3.m3.1.1.2.3.cmml" xref="S2.SS4.p1.3.m3.1.1.2.3">ğ‘š</ci></apply><cn type="float" id="S2.SS4.p1.3.m3.1.1.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">p_{m}=0.01</annotation></semantics></math> and <math id="S2.SS4.p1.4.m4.1" class="ltx_Math" alttext="l_{m}=40" display="inline"><semantics id="S2.SS4.p1.4.m4.1a"><mrow id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml"><msub id="S2.SS4.p1.4.m4.1.1.2" xref="S2.SS4.p1.4.m4.1.1.2.cmml"><mi id="S2.SS4.p1.4.m4.1.1.2.2" xref="S2.SS4.p1.4.m4.1.1.2.2.cmml">l</mi><mi id="S2.SS4.p1.4.m4.1.1.2.3" xref="S2.SS4.p1.4.m4.1.1.2.3.cmml">m</mi></msub><mo id="S2.SS4.p1.4.m4.1.1.1" xref="S2.SS4.p1.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS4.p1.4.m4.1.1.3" xref="S2.SS4.p1.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.1b"><apply id="S2.SS4.p1.4.m4.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1"><eq id="S2.SS4.p1.4.m4.1.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1.1"></eq><apply id="S2.SS4.p1.4.m4.1.1.2.cmml" xref="S2.SS4.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.p1.4.m4.1.1.2.1.cmml" xref="S2.SS4.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS4.p1.4.m4.1.1.2.2.cmml" xref="S2.SS4.p1.4.m4.1.1.2.2">ğ‘™</ci><ci id="S2.SS4.p1.4.m4.1.1.2.3.cmml" xref="S2.SS4.p1.4.m4.1.1.2.3">ğ‘š</ci></apply><cn type="integer" id="S2.SS4.p1.4.m4.1.1.3.cmml" xref="S2.SS4.p1.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.1c">l_{m}=40</annotation></semantics></math> in all our experiments.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.5.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.6.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Since quantization is performed before the convolutional sub-sampling, there is a mismatch in the lengths between the predicted tokens and target tokens. To mathc the sequence lengths, target tokens are averaged for every 8 frames, then apply threshold of 0.9 to select frames to be taken into loss calculation. Cross-entropy loss is applied on selected tokens determined by the input masks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset and Settings</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We train the NEST-L (108M) and NEST-XL (600M) models using 100K hours of both English speech data, including 60K hours from LibriLightÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, 24K hours from English subset of VoxpopuliÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and about 20K hours sampled data from the combination of FisherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, SwitchboardÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, WSJÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, NSCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Peopleâ€™s SpeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The audios for speech augmentation is randomly selected within each batch, while we use noise audios from MUSANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and FreesoundÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We train the models with global batch size of 2048 for 80K steps on 128 NVIDIA A100 GPUs, with Noam annealingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and peak learning rate of 0.004, weight decay of 1e-3, gradient clipping 1.0 and warm-up of 25K steps. We set the speech augmentation probability as 0.2, among which we set noise and speech augmentation probabilities as 0.1 and 0.9 respectively. Code and checkpoint will be publicly available.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Results on SUPERB Multi-task Speech Processing</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We evaluate our modelâ€™s performance on the SUPERBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> benchmark for multi-task evaluation on self-supervised speech models. For speech recognition (ASR), phoneme recognition (PR) and speaker diarization (SD) tasks, we use the architecture in the left part of FigureÂ <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b) and a simple linear layer as the task decoder. We train ASR and PR with CTCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> loss, while the SD task is trained with permutation invariant lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. For speaker identification/verification (SID/SV), keyword spotting (KS) and emotion recognition (ER) tasks, we resort to the architecture presented in the right part of FigureÂ <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b), and use the ECAPA-TDNN-smallÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as the task decoder. We following the same train/val/test splits as in the SUPERBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and train the models for 100 epochs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As presented in TableÂ <a href="#S2.T1" title="TABLE I â€£ II-B Speech Augmentation â€£ II Approach â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, our NEST-L model is able to outperform WavLM-base++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with similar size of parameters on all tasks, and also outperforms WavLM-largeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that is 3x as large on speaker verification (SV), speaker diarization (SD) and phoneme recognition (PR). When compared with the XEUSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> model that is trained on 10x data, we can see that our NEST-XL model is still able to achieve better performance on all speaker and content tasks, with especially large improvements on speaker verification, speaker diarization and phoneme recognition. Overall, we are able to achieve new state-of-the-art results on SID, SV, SD, PR and ASR tasks compared with WavLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that has data size as well as XEUSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that is trained on much large data, demonstrating the effectiveness of NESTÂ when applied on various downstream speech processing tasks.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results on speech translation from English to German, French and Spanish. BLEU score is used as the metric, while punctuation and capitalization are included in metric calculation. â€œ*â€ indicates second best performance.</figcaption>
<div id="S3.T3.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:55.9pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.1pt,17.4pt) scale(0.614266974150471,0.614266974150471) ;">
<table id="S3.T3.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.12.12.12" class="ltx_tr">
<th id="S3.T3.12.12.12.13" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T3.12.12.12.14" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T3.12.12.12.15" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T3.2.2.2.2.m1.1.1" xref="S3.T3.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T3.3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.3.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.3.m1.1b"><ci id="S3.T3.3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.3.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S3.T3.4.4.4.4.m1.1.1" xref="S3.T3.4.4.4.4.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.4.m1.1b"><ci id="S3.T3.4.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.4.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S3.T3.5.5.5.5.m1.1.1" xref="S3.T3.5.5.5.5.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.5.m1.1b"><ci id="S3.T3.5.5.5.5.m1.1.1.cmml" xref="S3.T3.5.5.5.5.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.5.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S3.T3.6.6.6.6.m1.1.1" xref="S3.T3.6.6.6.6.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.6.m1.1b"><ci id="S3.T3.6.6.6.6.m1.1.1.cmml" xref="S3.T3.6.6.6.6.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.6.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.7.7.7.7.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.7.7.7.7.m1.1a"><mo stretchy="false" id="S3.T3.7.7.7.7.m1.1.1" xref="S3.T3.7.7.7.7.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.7.m1.1b"><ci id="S3.T3.7.7.7.7.m1.1.1.cmml" xref="S3.T3.7.7.7.7.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.7.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.8.8.8.8.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.8.8.8.8.m1.1a"><mo stretchy="false" id="S3.T3.8.8.8.8.m1.1.1" xref="S3.T3.8.8.8.8.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.8.m1.1b"><ci id="S3.T3.8.8.8.8.m1.1.1.cmml" xref="S3.T3.8.8.8.8.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.8.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.9.9.9.9.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.9.9.9.9.m1.1a"><mo stretchy="false" id="S3.T3.9.9.9.9.m1.1.1" xref="S3.T3.9.9.9.9.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.9.m1.1b"><ci id="S3.T3.9.9.9.9.m1.1.1.cmml" xref="S3.T3.9.9.9.9.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.9.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
<td id="S3.T3.10.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.10.10.10.10.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.10.10.10.10.m1.1a"><mo stretchy="false" id="S3.T3.10.10.10.10.m1.1.1" xref="S3.T3.10.10.10.10.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.10.10.m1.1b"><ci id="S3.T3.10.10.10.10.m1.1.1.cmml" xref="S3.T3.10.10.10.10.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.10.10.m1.1c">\rightarrow</annotation></semantics></math>De</td>
<td id="S3.T3.11.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.11.11.11.11.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.11.11.11.11.m1.1a"><mo stretchy="false" id="S3.T3.11.11.11.11.m1.1.1" xref="S3.T3.11.11.11.11.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.11.11.m1.1b"><ci id="S3.T3.11.11.11.11.m1.1.1.cmml" xref="S3.T3.11.11.11.11.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.11.11.m1.1c">\rightarrow</annotation></semantics></math>Es</td>
<td id="S3.T3.12.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">En<math id="S3.T3.12.12.12.12.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T3.12.12.12.12.m1.1a"><mo stretchy="false" id="S3.T3.12.12.12.12.m1.1.1" xref="S3.T3.12.12.12.12.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.12.12.m1.1b"><ci id="S3.T3.12.12.12.12.m1.1.1.cmml" xref="S3.T3.12.12.12.12.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.12.12.m1.1c">\rightarrow</annotation></semantics></math>Fr</td>
</tr>
<tr id="S3.T3.12.12.13.1" class="ltx_tr">
<th id="S3.T3.12.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SeamlessM4T-medium</th>
<th id="S3.T3.12.12.13.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt">1.2B</th>
<td id="S3.T3.12.12.13.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4M</td>
<td id="S3.T3.12.12.13.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">28.03</td>
<td id="S3.T3.12.12.13.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">38.44</td>
<td id="S3.T3.12.12.13.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">30.50*</td>
<td id="S3.T3.12.12.13.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">9.65</td>
<td id="S3.T3.12.12.13.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.23</td>
<td id="S3.T3.12.12.13.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">8.64</td>
<td id="S3.T3.12.12.13.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">28.30</td>
<td id="S3.T3.12.12.13.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">21.05</td>
<td id="S3.T3.12.12.13.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">37.36</td>
<td id="S3.T3.12.12.13.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">21.99</td>
<td id="S3.T3.12.12.13.1.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.24</td>
<td id="S3.T3.12.12.13.1.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.50</td>
</tr>
<tr id="S3.T3.12.12.14.2" class="ltx_tr">
<th id="S3.T3.12.12.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">SeamlessM4T-v2-large</th>
<th id="S3.T3.12.12.14.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">2.3B</th>
<td id="S3.T3.12.12.14.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4M</td>
<td id="S3.T3.12.12.14.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.96</td>
<td id="S3.T3.12.12.14.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.32</td>
<td id="S3.T3.12.12.14.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.33</td>
<td id="S3.T3.12.12.14.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.48</td>
<td id="S3.T3.12.12.14.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.89</td>
<td id="S3.T3.12.12.14.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.04</td>
<td id="S3.T3.12.12.14.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.10.1" class="ltx_text ltx_font_bold">33.17</span></td>
<td id="S3.T3.12.12.14.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.11.1" class="ltx_text ltx_font_bold">23.72</span></td>
<td id="S3.T3.12.12.14.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.14.2.12.1" class="ltx_text ltx_font_bold">43.05</span></td>
<td id="S3.T3.12.12.14.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.87</td>
<td id="S3.T3.12.12.14.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.31</td>
<td id="S3.T3.12.12.14.2.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.80</td>
</tr>
<tr id="S3.T3.12.12.15.3" class="ltx_tr">
<th id="S3.T3.12.12.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Canary-1BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<th id="S3.T3.12.12.15.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">1B</th>
<td id="S3.T3.12.12.15.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86K</td>
<td id="S3.T3.12.12.15.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.4.1" class="ltx_text ltx_font_bold">32.53</span></td>
<td id="S3.T3.12.12.15.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.5.1" class="ltx_text ltx_font_bold">40.84</span></td>
<td id="S3.T3.12.12.15.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.6.1" class="ltx_text ltx_font_bold">30.65</span></td>
<td id="S3.T3.12.12.15.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.7.1" class="ltx_text ltx_font_bold">23.83</span></td>
<td id="S3.T3.12.12.15.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.8.1" class="ltx_text ltx_font_bold">35.73</span></td>
<td id="S3.T3.12.12.15.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.9.1" class="ltx_text ltx_font_bold">28.28</span></td>
<td id="S3.T3.12.12.15.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.15*</td>
<td id="S3.T3.12.12.15.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.66*</td>
<td id="S3.T3.12.12.15.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.76*</td>
<td id="S3.T3.12.12.15.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.13.1" class="ltx_text ltx_font_bold">29.50</span></td>
<td id="S3.T3.12.12.15.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.14.1" class="ltx_text ltx_font_bold">33.07</span></td>
<td id="S3.T3.12.12.15.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.12.12.15.3.15.1" class="ltx_text ltx_font_bold">33.23</span></td>
</tr>
<tr id="S3.T3.12.12.16.4" class="ltx_tr">
<th id="S3.T3.12.12.16.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">NEST-XL-Transformer</th>
<th id="S3.T3.12.12.16.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">1B</th>
<td id="S3.T3.12.12.16.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">42K</td>
<td id="S3.T3.12.12.16.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">30.87*</td>
<td id="S3.T3.12.12.16.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">39.95*</td>
<td id="S3.T3.12.12.16.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">30.01</td>
<td id="S3.T3.12.12.16.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">22.82*</td>
<td id="S3.T3.12.12.16.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">34.92*</td>
<td id="S3.T3.12.12.16.4.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">27.99*</td>
<td id="S3.T3.12.12.16.4.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">29.50</td>
<td id="S3.T3.12.12.16.4.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">22.61</td>
<td id="S3.T3.12.12.16.4.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">39.27</td>
<td id="S3.T3.12.12.16.4.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">27.73*</td>
<td id="S3.T3.12.12.16.4.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">32.51*</td>
<td id="S3.T3.12.12.16.4.15" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">32.42*</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>DER results on speaker diarization. EEND-GLAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> applies additional clustering after end-to-end diarizer, so we exclude it from the comparison for fairness. â€œ*â€ indicates second best.</figcaption>
<div id="S3.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:145.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.8pt,11.3pt) scale(0.865249451761731,0.865249451761731) ;">
<table id="S3.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.2.1" class="ltx_tr">
<th id="S3.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T4.1.1.2.1.1.1" class="ltx_text">Model</span></th>
<td id="S3.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DIHARD3 eval</td>
<td id="S3.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">CALLHOME-part2</td>
</tr>
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S3.T4.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.T4.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.1.1.1.m1.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.1.1.1.m1.1b"><leq id="S3.T4.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.1.1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.1.1.1.m1.1c">\leq</annotation></semantics></math>4 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.1.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.0</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
<td id="S3.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">
<table id="S3.T4.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">4 speakers,</td>
</tr>
<tr id="S3.T4.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T4.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">collar=0.25</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.1.3.2" class="ltx_tr">
<th id="S3.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">EEND-GLA-smallÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S3.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.39</td>
<td id="S3.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6.94</td>
<td id="S3.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.42</td>
<td id="S3.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.49</td>
</tr>
<tr id="S3.T4.1.1.4.3" class="ltx_tr">
<th id="S3.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">EEND-EDAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<td id="S3.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.2.1" class="ltx_text ltx_font_bold">15.55</span></td>
<td id="S3.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.3.1" class="ltx_text ltx_font_bold">7.83</span></td>
<td id="S3.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.29</td>
<td id="S3.T4.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.5.1" class="ltx_text ltx_font_bold">17.59</span></td>
</tr>
<tr id="S3.T4.1.1.5.4" class="ltx_tr">
<th id="S3.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NeMo MSDDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S3.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.40</td>
<td id="S3.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.41</td>
<td id="S3.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.45</td>
<td id="S3.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.49</td>
</tr>
<tr id="S3.T4.1.1.6.5" class="ltx_tr">
<th id="S3.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">RandFC-L-Linear</th>
<td id="S3.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.68</td>
<td id="S3.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.17</td>
<td id="S3.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">19.16</td>
<td id="S3.T4.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">24.47</td>
</tr>
<tr id="S3.T4.1.1.7.6" class="ltx_tr">
<th id="S3.T4.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NEST-L-Linear</th>
<td id="S3.T4.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.63</td>
<td id="S3.T4.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.56</td>
<td id="S3.T4.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.85</td>
<td id="S3.T4.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.73</td>
</tr>
<tr id="S3.T4.1.1.8.7" class="ltx_tr">
<th id="S3.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-L-SortformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite>
</th>
<td id="S3.T4.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17.06*</td>
<td id="S3.T4.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.91*</td>
<td id="S3.T4.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10.34*</td>
<td id="S3.T4.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">23.07</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Results on Multi-lingual Speech Recognition</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Besides multi-task evaluation, we also study if an SSL model trained on single language can help other languages. To this end, we train an ASR model on four different languages: English (En), German (De), French (Fr), Spanish (Es). Specifically, we train an ASR model using NEST-XL as weight initialization and the hybrid-CTC-RNNT lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The training data comprises of 2.5K hours of German speech (MCV, MLS, Voxpopuli), 8.5K hours of English speech (MCVÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, MLSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, VoxpopuliÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, SPGIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, EuroparlÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, LibriSpeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, NSC1Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>), 1.4K hours of Spanish speech (MCV, MLS, Voxpopuli, FisherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) and 1.9K hours of French speech (MCV, MLS, Voxpopuli). For baselines, we train another model using an English ASR modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> as weight initialization, and also include some of the best ASR models like WhisperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, SeamlessM4TÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We run all models with the same beam size 5 without extra language models on test sets of MCV-16.1Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and VoxpopuliÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">From the last two rows of TableÂ <a href="#S2.T2" title="TABLE II â€£ II-B Speech Augmentation â€£ II Approach â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that NESTÂ can help achieve better WER on all datasets than the model with ASR pretrained initialization, which shows that NESTÂ can help improve ASR performance on languages that is not seen during SSL pretraining. In addition, when compared with other SOTA ASR models (WhisperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, SeamlessM4TÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) trained with much more parameters and data, we are still able to match the performance of CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on averaged WER across all languages. On some of the datasets, although there is still a gap between our modelâ€™s performance and that of the SOTA models trained with much more data, we can still see that NESTÂ can be used as an efficient way to obtain superior ASR performance comparable to models trained on massive datasets.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Results on Speech Translation</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p">We further study how NESTÂ can help speech-to-text translation (AST) and present the results in TableÂ <a href="#S3.T3" title="TABLE III â€£ III-B Results on SUPERB Multi-task Speech Processing â€£ III Experiments â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We use the same model architecture and training procedure as proposed in CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, while the training data contains 42K hours of English ASR data with machine generated translationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> from English (En) to German (De), French (Fr) and Spanish (Es) text. We compare our model with other state-of-the-art AST models SeamlessM4TÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on EuroparlÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, mExpressoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and FLEURSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> test sets. As we can see, given the same number of parameters, due to much less training data used in our model, there is still a gap between CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and our model on all evaluated datasets. Also, given that CanaryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is initialized with a multi-lingual ASR encoder that is pretrained on all of the evaluated languages, it is expected that Canary performs better than the NESTÂ initialization. Nonetheless, our model is able to outperform SeamlessM4TÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and achieves the second best average BLEU scores on En<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mo stretchy="false" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\rightarrow</annotation></semantics></math>De, En<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mo stretchy="false" id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\rightarrow</annotation></semantics></math>Es and En<math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mo stretchy="false" id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\rightarrow</annotation></semantics></math>Fr translations, showing that the proposed NESTÂ model is able to help achieve impressive AST performance with less data.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Results on Speaker Diarization</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">To evaluate our modelâ€™s performance on speaker diarization, we follow the SortformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite> architecture and attach 18 layers of transformer encoderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> after the NESTÂ encoder. We also train a simpler baseline with only FastConformer encoder and a linear decoder, with either random initialization or NESTÂ initialization. For training data, we use a combination of real data (FisherÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) and simulated data (composed from LibriSeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, SREÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>) generated by the NeMo speech data simulatorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, with a total of 6.4K hours. We compare with state-of-the-art diarization models EEND-EDAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and EEND-GLAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and show the results in TableÂ <a href="#S3.T4" title="TABLE IV â€£ III-B Results on SUPERB Multi-task Speech Processing â€£ III Experiments â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">By comparing RandFC-L-Linear and NEST-L-Linear, we can see that NESTÂ is able to significantly reduce the DER of a simple FC-Linear baseline, showing the importance of using NESTÂ in speaker diarization. We can also notice that SortformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sortformer</span>]</cite> with NESTÂ initialization is able to achieve second best results on three of the four evaluated test sets, which are comparable to the state-of-the-art EEND-EDAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results on SLURPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> benchmark for speech joint intent detection and slot filling.</figcaption>
<div id="S3.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:144pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.8pt,12.1pt) scale(0.855036540700733,0.855036540700733) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SSL</td>
</tr>
<tr id="S3.T5.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Data (hrs)</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Intent</td>
</tr>
<tr id="S3.T5.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Precision</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Recall</td>
</tr>
</table>
</th>
<th id="S3.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SLURP-</td>
</tr>
<tr id="S3.T5.1.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T5.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">F1</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.2.1" class="ltx_tr">
<th id="S3.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">SpeechBrain-Hubert-largeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<td id="S3.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">LL-60K</td>
<td id="S3.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">89.37</td>
<td id="S3.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.54</td>
<td id="S3.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.44</td>
<td id="S3.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.96</td>
</tr>
<tr id="S3.T5.1.1.3.2" class="ltx_tr">
<th id="S3.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ESPnet-ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</th>
<td id="S3.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.30</td>
<td id="S3.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.40</td>
</tr>
<tr id="S3.T5.1.1.4.3" class="ltx_tr">
<th id="S3.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Open-BEST-RQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S3.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS-960</td>
<td id="S3.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.80</td>
<td id="S3.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
</tr>
<tr id="S3.T5.1.1.5.4" class="ltx_tr">
<th id="S3.T5.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Wav2vec-CTI-RoBERTaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<td id="S3.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS-960</td>
<td id="S3.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.92</td>
<td id="S3.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.66</td>
</tr>
<tr id="S3.T5.1.1.6.5" class="ltx_tr">
<th id="S3.T5.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">NeMo-SSL-FC-Trans-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S3.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LL-60K</td>
<td id="S3.T5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.40</td>
<td id="S3.T5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.90</td>
<td id="S3.T5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.65</td>
<td id="S3.T5.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.22</td>
</tr>
<tr id="S3.T5.1.1.7.6" class="ltx_tr">
<th id="S3.T5.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">NEST-L-Transformer</th>
<td id="S3.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">En-100K</td>
<td id="S3.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.1.1.7.6.3.1" class="ltx_text ltx_font_bold">89.79</span></td>
<td id="S3.T5.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">80.55</td>
<td id="S3.T5.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T5.1.1.7.6.5.1" class="ltx_text ltx_font_bold">78.70</span>
</td>
<td id="S3.T5.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">79.61</td>
</tr>
<tr id="S3.T5.1.1.8.7" class="ltx_tr">
<th id="S3.T5.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">NEST-XL-Transformer</th>
<td id="S3.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">En-100K</td>
<td id="S3.T5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">89.04</td>
<td id="S3.T5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.1.1.8.7.4.1" class="ltx_text ltx_font_bold">82.35</span>
</td>
<td id="S3.T5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">78.36</td>
<td id="S3.T5.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.1.1.8.7.6.1" class="ltx_text ltx_font_bold">80.31</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Results on Spoken Language Understanding</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">For spoken language understanding, we focus on the <em id="S3.SS6.p1.1.1" class="ltx_emph ltx_font_italic">joint intent detection and slot filling</em> task and evaluate our modelâ€™s performance using the SLURPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> dataset. Specifically, we attach a transformer decoder to the NESTÂ encoder, and use the same hyper-parameter setting as in NeMo-SLUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. We compare with other SSL-based end-to-end SLU models and show the results in TableÂ <a href="#S3.T5" title="TABLE V â€£ III-E Results on Speaker Diarization â€£ III Experiments â€£ NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. For fair comparison, we do not include the ASR pretrained baselineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> as we focus on SSL.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">As we can see, among all SSL-based SLU models, the proposed NESTÂ model achieves the best performance on both intent detection accuracy and slot filling F1 scores. We also notice that scaling up from NEST-L to NEST-XL does bring some improvement on precision score on slot filling, but do not have significant effects on other metrics. In addition, compared with the NeMo-SSL-FC-Trans-LÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> baseline, we can see a more than 2% absolute improvement on F1 score by merely replacing the SSL speech encoder with NESTÂ while keeping other hyper-parameters the same, which demonstrate the instant benefits that NESTÂ can bring to existing speech processing models.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we propose a self-supervised model <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">NESTÂ </span>, which is faster than previous SSL models by applying FastConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as the speech encoder. We simplify the audio quantization by applying fixed random-projection quantizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and also generalize the noisy speech augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to allow for more randomness and speakers. Extensive experiments on multiple speech processing tasks show that the NESTÂ model can help achieve state-of-the-art performance. Code, configurations and checkpoints are also publicly available through NVIDIA NeMo toolkitÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.Â Devlin, â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.Â Baevski, Y.Â Zhou, A.Â Mohamed, and M.Â Auli, â€œwav2vec 2.0: A framework for self-supervised learning of speech representations,â€ <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 33, pp. 12â€‰449â€“12â€‰460, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.Â Sadhu, D.Â He, C.-W. Huang, S.Â H. Mallidi, M.Â Wu, A.Â Rastrow, A.Â Stolcke, J.Â Droppo, and R.Â Maas, â€œWav2vec-c: A self-supervised model for speech representation learning,â€ <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.08393</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A.Â Baevski, S.Â Schneider, and M.Â Auli, â€œvq-wav2vec: Self-supervised learning of discrete speech representations,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.05453</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D.Â Jiang, W.Â Li, M.Â Cao, W.Â Zou, and X.Â Li, â€œSpeech simclr: Combining contrastive and reconstruction objective for self-supervised speech representation learning,â€ <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.13991</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.Â Bolte, Y.-H.Â H. Tsai, K.Â Lakhotia, R.Â Salakhutdinov, and A.Â Mohamed, â€œHubert: Self-supervised speech representation learning by masked prediction of hidden units,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM transactions on audio, speech, and language processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C.-C. Chiu, J.Â Qin, Y.Â Zhang, J.Â Yu, and Y.Â Wu, â€œSelf-supervised learning with random-projection quantizer for speech recognition,â€ in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2022, pp. 3915â€“3924.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang, Z.Â Chen, Y.Â Wu, S.Â Liu, Z.Â Chen, J.Â Li, N.Â Kanda, T.Â Yoshioka, X.Â Xiao <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œWavlm: Large-scale self-supervised pre-training for full stack speech processing,â€ <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1505â€“1518, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W.Â Chen, W.Â Zhang, Y.Â Peng, X.Â Li, J.Â Tian, J.Â Shi, X.Â Chang, S.Â Maiti, K.Â Livescu, and S.Â Watanabe, â€œTowards robust speech representation learning for thousands of languages,â€ <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.00837</em>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K.Â He, X.Â Chen, S.Â Xie, Y.Â Li, P.Â DollÃ¡r, and R.Â Girshick, â€œMasked autoencoders are scalable vision learners,â€ in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16â€‰000â€“16â€‰009.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.Â Baevski, W.-N. Hsu, Q.Â Xu, A.Â Babu, J.Â Gu, and M.Â Auli, â€œData2vec: A general framework for self-supervised learning in speech, vision and language,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2022, pp. 1298â€“1312.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.Â Van DenÂ Oord, O.Â Vinyals <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œNeural discrete representation learning,â€ <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.Â Babu, C.Â Wang, A.Â Tjandra, K.Â Lakhotia, Q.Â Xu, N.Â Goyal, K.Â Singh, P.Â VonÂ Platen, Y.Â Saraf, J.Â Pino <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œXls-r: Self-supervised cross-lingual speech representation learning at scale,â€ <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y.-A. Chung, Y.Â Zhang, W.Â Han, C.-C. Chiu, J.Â Qin, R.Â Pang, and Y.Â Wu, â€œW2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2021, pp. 244â€“250.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Gulati, J.Â Qin, C.-C. Chiu, N.Â Parmar, Y.Â Zhang, J.Â Yu, W.Â Han, S.Â Wang, Z.Â Zhang, Y.Â Wu <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œConformer: Convolution-augmented transformer for speech recognition,â€ <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.08100</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Vaswani, â€œAttention is all you need,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03762</em>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D.Â Rekesh, N.Â R. Koluguri, S.Â Kriman, S.Â Majumdar, V.Â Noroozi, H.Â Huang, O.Â Hrinchuk, K.Â Puvvada, A.Â Kumar, J.Â Balam <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œFast conformer with linearly scalable attention for efficient speech recognition,â€ in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2023, pp. 1â€“8.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I.Â J. Lai, K.Â Lakhotia, Y.Â Y. Lin, A.Â T. Liu, J.Â Shi, X.Â Chang, G.-T. Lin <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œSuperb: Speech processing universal performance benchmark,â€ <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L.Â Barrault, Y.-A. Chung, M.Â C. Meglioli, D.Â Dale, N.Â Dong, M.Â Duppenthaler, P.-A. Duquenne, B.Â Ellis, H.Â Elsahar, J.Â Haaheim <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œSeamless: Multilingual expressive and streaming speech translation,â€ <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.05187</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, T.Â Xu, G.Â Brockman, C.Â McLeavey, and I.Â Sutskever, â€œRobust speech recognition via large-scale weak supervision,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.Â Â Â PMLR, 2023, pp. 28â€‰492â€“28â€‰518.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K.Â C. Puvvada, P.Â Å»elasko, H.Â Huang, O.Â Hrinchuk, N.Â R. Koluguri, K.Â Dhawan, S.Â Majumdar, E.Â Rastorgueva, Z.Â Chen, V.Â Lavrukhin <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œLess is more: Accurate speech recognition &amp; translation without web-scale data,â€ <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â Kahn, M.Â Riviere, W.Â Zheng, E.Â Kharitonov, Q.Â Xu, P.-E. MazarÃ©, J.Â Karadayi, V.Â Liptchinsky, R.Â Collobert, C.Â Fuegen <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œLibri-light: A benchmark for asr with limited or no supervision,â€ in <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 7669â€“7673.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C.Â Wang, M.Â Riviere, A.Â Lee, A.Â Wu, C.Â Talnikar, D.Â Haziza, M.Â Williamson, J.Â Pino, and E.Â Dupoux, â€œVoxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,â€ <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00390</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C.Â Cieri, D.Â Miller, and K.Â Walker, â€œThe fisher corpus: A resource for the next generations of speech-to-text.â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">LREC</em>, vol.Â 4, 2004, pp. 69â€“71.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E.Â H. John J.Â Godfrey, â€œSwitchboard-1 release 2,â€ <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC97S62" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC97S62</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.Â S. Garofolo, D.Â Graff, D.Â Paul, and D.Â Pallett, â€œCsr-i (wsj0) complete,â€ <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC93S6A" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC93S6A</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.Â X. Koh, A.Â Mislan, K.Â Khoo, B.Â Ang, W.Â Ang, C.Â Ng, and Y.Â Tan, â€œBuilding the singapore english national speech corpus,â€ <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Malay</em>, vol.Â 20, no. 25.0, pp. 19â€“3, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D.Â Galvez, G.Â Diamos, J.Â Ciro, J.Â F. CerÃ³n, K.Â Achorn, A.Â Gopi, D.Â Kanter, M.Â Lam, M.Â Mazumder, and V.Â J. Reddi, â€œThe peopleâ€™s speech: A large-scale diverse english speech recognition dataset for commercial usage,â€ <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09344</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D.Â Snyder, G.Â Chen, and D.Â Povey, â€œMusan: A music, speech, and noise corpus,â€ <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
E.Â Fonseca, J.Â PonsÂ Puig, X.Â Favory, F.Â FontÂ Corbera, D.Â Bogdanov, A.Â Ferraro, S.Â Oramas, A.Â Porter, and X.Â Serra, â€œFreesound datasets: a platform for the creation of open audio datasets,â€ in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Hu X, Cunningham SJ, Turnbull D, Duan Z, editors. Proceedings of the 18th ISMIR Conference; 2017 oct 23-27; Suzhou, China.[Canada]: International Society for Music Information Retrieval; 2017. p. 486-93.</em>Â Â Â International Society for Music Information Retrieval (ISMIR), 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A.Â Graves, S.Â FernÃ¡ndez, F.Â Gomez, and J.Â Schmidhuber, â€œConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine learning</em>, 2006, pp. 369â€“376.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y.Â Fujita, N.Â Kanda, S.Â Horiguchi, Y.Â Xue, K.Â Nagamatsu, and S.Â Watanabe, â€œEnd-to-end neural speaker diarization with self-attention,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2019, pp. 296â€“303.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
B.Â Desplanques, J.Â Thienpondt, and K.Â Demuynck, â€œEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,â€ <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.07143</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S.Â Horiguchi, S.Â Watanabe, P.Â GarcÃ­a, Y.Â Takashima, and Y.Â Kawaguchi, â€œOnline neural diarization of unlimited numbers of speakers using global and local attractors,â€ <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 31, pp. 706â€“720, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S.Â Horiguchi, Y.Â Fujita, S.Â Watanabe, Y.Â Xue, and P.Â Garcia, â€œEncoder-decoder based attractors for end-to-end neural diarization,â€ <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 30, pp. 1493â€“1507, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T.Â J. Park, N.Â R. Koluguri, J.Â Balam, and B.Â Ginsburg, â€œMulti-scale speaker diarization with dynamic scale weighting,â€ <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15974</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
V.Â Noroozi, S.Â Majumdar, A.Â Kumar, J.Â Balam, and B.Â Ginsburg, â€œStateful conformer with cache-based inference for streaming automatic speech recognition,â€ in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2024, pp. 12â€‰041â€“12â€‰045.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R.Â Ardila, M.Â Branson, K.Â Davis, M.Â Henretty, M.Â Kohler, J.Â Meyer, R.Â Morais, L.Â Saunders, F.Â M. Tyers, and G.Â Weber, â€œCommon voice: A massively-multilingual speech corpus,â€ in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</em>, 2020, pp. 4211â€“4215.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
V.Â Pratap, Q.Â Xu, A.Â Sriram, G.Â Synnaeve, and R.Â Collobert, â€œMls: A large-scale multilingual dataset for speech research,â€ <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.03411</em>, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P.Â K. Oâ€™Neill, V.Â Lavrukhin, S.Â Majumdar, V.Â Noroozi, Y.Â Zhang, O.Â Kuchaiev, J.Â Balam, Y.Â Dovzhenko, K.Â Freyberg, M.Â D. Shulman <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œSpgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition,â€ <em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.02014</em>, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J.Â Iranzo-SÃ¡nchez, J.Â A. Silvestre-Cerda, J.Â Jorge, N.Â RosellÃ³, A.Â GimÃ©nez, A.Â Sanchis, J.Â Civera, and A.Â Juan, â€œEuroparl-st: A multilingual corpus for speech translation of parliamentary debates,â€ in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 8229â€“8233.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V.Â Panayotov, G.Â Chen, D.Â Povey, and S.Â Khudanpur, â€œLibrispeech: an asr corpus based on public domain audio books,â€ in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.Â Â Â IEEE, 2015, pp. 5206â€“5210.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
NVIDIA, â€œNemo english fastconformer-rnnt asr model,â€ <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
â€”â€”, â€œMegatron multilingual translation model,â€ <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
META, â€œmexpresso (multilingual expresso),â€ <a target="_blank" href="https://huggingface.co/facebook/seamless-expressive#mexpresso-multilingual-expresso" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/facebook/seamless-expressive#mexpresso-multilingual-expresso</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A.Â Conneau, M.Â Ma, S.Â Khanuja, Y.Â Zhang, V.Â Axelrod, S.Â Dalmia, J.Â Riesa, C.Â Rivera, and A.Â Bapna, â€œFleurs: Few-shot learning evaluation of universal representations of speech,â€ in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</em>.Â Â Â IEEE, 2023, pp. 798â€“805.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
G.Â R. Doddington, M.Â A. Przybocki, A.Â F. Martin, and D.Â A. Reynolds, â€œThe nist speaker recognition evaluationâ€“overview, methodology, systems, results, perspective,â€ <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Speech communication</em>, vol.Â 31, no. 2-3, pp. 225â€“254, 2000.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
NIST, â€œNist speaker recognition evaluation (sre),â€ <a target="_blank" href="https://www.nist.gov/itl/iad/mig/speaker-recognition" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nist.gov/itl/iad/mig/speaker-recognition</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
T.Â J. Park, H.Â Huang, C.Â Hooper, N.Â Koluguri, K.Â Dhawan, A.Â Jukic, J.Â Balam, and B.Â Ginsburg, â€œProperty-aware multi-speaker data simulation: A probabilistic modelling technique for synthetic data generation,â€ <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.12371</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
E.Â Bastianelli, A.Â Vanzo, P.Â Swietojanski, and V.Â Rieser, â€œSlurp: A spoken language understanding resource package,â€ <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.13205</em>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y.Â Wang, A.Â Boumadane, and A.Â Heba, â€œA fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding,â€ <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02735</em>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S.Â Arora, S.Â Dalmia, P.Â Denisov, X.Â Chang, Y.Â Ueda, Y.Â Peng, Y.Â Zhang, S.Â Kumar, K.Â Ganesan, B.Â Yan <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œEspnet-slu: Advancing spoken language understanding through espnet,â€ in <em id="bib.bib52.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 7167â€“7171.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R.Â Whetten, T.Â Parcollet, M.Â Dinarelli, and Y.Â EstÃ¨ve, â€œOpen implementation and study of best-rq for speech processing,â€ <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.04296</em>, 2024.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S.Â Seo, D.Â Kwak, and B.Â Lee, â€œIntegration of pre-trained networks with continuous token interface for end-to-end spoken language understanding,â€ in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 7152â€“7156.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
H.Â Huang, J.Â Balam, and B.Â Ginsburg, â€œLeveraging pretrained asr encoders for effective and efficient end-to-end speech intent classification and slot filling,â€ <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
NVIDIA, â€œNemo toolkit,â€ <a target="_blank" href="https://github.com/NVIDIA/NeMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13105" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13106" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13106">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13106" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13107" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:22:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
