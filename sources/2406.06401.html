<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.06401] INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition</title><meta property="og:description" content="We revisit the INTERSPEECH 2009 Emotion Challenge – the first ever speech emotion recognition (SER) challenge – and evaluate a series of deep learning models that are representative of the major advances in SER researc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.06401">

<!--Generated on Fri Jul  5 20:57:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span>
<p id="p1.2" class="ltx_p">ffiliation=1]AndreasTriantafyllopoulos
ffiliation=1]AntonBatliner
ffiliation=2]SimonRampp
ffiliation=1]ManuelMilling
ffiliation=1,2,3]BjörnSchuller</p>
</div>
<h1 class="ltx_title ltx_title_document">INTERSPEECH 2009 Emotion Challenge Revisited: 
<br class="ltx_break">Benchmarking 15 Years of Progress in Speech Emotion Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">We revisit the INTERSPEECH 2009 Emotion Challenge – the first ever speech emotion recognition (SER) challenge – and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then.
We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search.
Results are always reported on the official test set with a separate validation set only used for early stopping.
Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning.
Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark.
An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic.
</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Speech emotion recognition, Deep learning
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Standardised benchmarks form the backbone of reproducible science and enable the research community to showcase its progress towards a common objective.
<span title="" class="ltx_glossaryref">Speech emotion recognition (SER)</span> is one subfield of speech science where several benchmarks exist, mostly in the form of challenges like:
the INTERSPEECH 2009 Emotion Challenge (ComParE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>,
the first official challenge on SER,
which was followed by several iterations covering a wide gamut of paralinguistic tasks;
the Audio-Visual Emotion Challenge (AVEC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>;
MuSe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>;
OMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>;
just recently, the Odyssey 2024 <span title="" class="ltx_glossaryref">SER</span> challenge; and others.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, while such challenges form excellent `proving grounds' for the prevalent methods at a particular <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">Zeitgeist</em>, they are rarely revisited when newer approaches emerge.
Furthermore, popular datasets oftentimes suffer from non-standardised folds (as in the case of IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>) or iterative releases (like the recent MSP-Podcast <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, with a new version being released almost every year), which makes it harder to obtain a consistent comparison of all different methods.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the present contribution – and on the occasion of its 15<sup id="S1.p3.1.1" class="ltx_sup">th</sup> anniversary – we focus exclusively on FAU-AIBO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>, the dataset used for the first-ever <span title="" class="ltx_glossaryref">SER</span> challenge in 2009.
The challenge contained two alternative formulations of emotion:
a 2-class problem, where participants had to differentiate between <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">negative</em> and <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">non-negative</em> emotions;
and a 5-class problem, where participants had to classify an utterance as <em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">angry</em> (A), <em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">neutral</em> (N), <em id="S1.p3.1.6" class="ltx_emph ltx_font_italic">motherese</em>/<em id="S1.p3.1.7" class="ltx_emph ltx_font_italic">joyful</em> (P), <em id="S1.p3.1.8" class="ltx_emph ltx_font_italic">emphatic</em> (E), with a 5<sup id="S1.p3.1.9" class="ltx_sup">th</sup> <em id="S1.p3.1.10" class="ltx_emph ltx_font_italic">rest</em> (R) class.
As the challenge ran before the advent of the `<span title="" class="ltx_glossaryref">deep learning (DL)</span> era', participants never benefited from these advances.
Moreover,
as newer datasets emerged over time, FAU-AIBO has been relatively overlooked.
Specifically, although <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> has been referred to rather often – being a standard reference to an early paper presenting spontaneous emotions, clear partitioning, and baselines – to the best of our knowledge, in the last 15 years there has been only a limited number of studies that unequivocally used the same configuration of train and test partition with identical number of items in each class.
In fact, sometimes it is explicitly mentioned that ``the results presented in this paper are not directly
comparable with those found using the 2009-challenge data'' <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>.
On a positive note, this makes it a perfect test case for a long-overdue retrospective, centred on the question of whether the community has `solved' or at least substantially progressed on the problem of <span title="" class="ltx_glossaryref">SER</span> in the intervening years.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To answer this question, we run a large-scale study of several `high-profile' advances that have emerged after 2009:
We start from larger, more comprehensive feature sets which defined the <span title="" class="ltx_glossaryref">SER</span> landscape until ca. 2016, where we train both <span title="" class="ltx_glossaryref">multi-layered perceptrons (MLPs)</span> on their static and <span title="" class="ltx_glossaryref">long short-term memory recurrent neural networks (LSTMs)</span> on their dynamic versions.
After that, we move on to the first end-to-end <span title="" class="ltx_glossaryref">convolutional recurrent neural networks (CRNNs)</span> as well as spectrogram-based <span title="" class="ltx_glossaryref">convolutional neural networks (CNNs)</span> benefiting from transfer learning.
Finally, we investigate the more recent transformers pre-trained with <span title="" class="ltx_glossaryref">self-supervised learning (SSL)</span>.
We note that such previous large-scale experiments have only been carried out within a particular architecture family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>, <a href="#bib.bibx11" title="" class="ltx_ref">11</a>, <a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>.
We additionally analyse our results with respect to inter-model agreement, examine whether hard-to-classify cases are also those where human annotators disagree the most, and try to measure whether progress is monotonic with respect to the year a model was introduced or its size (i. e., computational complexity).
We note that we exclude advances on linguistics due to space limitations.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Previous work on FAU-AIBO</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Challenge:</span>
The official challenge baseline consisted of HMM modelling of dynamic features or <span title="" class="ltx_glossaryref">support vector machine (SVM)</span> modelling of static features each combined with SMOTE oversampling to mitigate class imbalance and a separate standardisation applied on the training and test sets
 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, with static features yielding moderately better performance.
The challenge featured two winners:The <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">open performance</em> sub-challenge was won by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>, who employed <span title="" class="ltx_glossaryref">Gaussian mixture models (GMMs)</span> trained on cepstral and expert prosodic and vocal tract features;
The <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">classifier performance</em> sub-challenge was won by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>, who used the official static features but with a divide-and-conquer cascade classification approach.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite> obtained the best performance on the 5-class problem by employing a fusion of different <span title="" class="ltx_glossaryref">GMMs</span> trained on functionals (but were only <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="0.1\%" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mn id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">0.1</mn><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">0.1\%</annotation></semantics></math> better than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>).
A review found an overall tendency towards smaller, carefully-designed features over `brute-force' approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>, and a fusion of the top approaches led to additional performance gains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Beyond the challenge:</span>
Researchers continued to improve performance on FAU-AIBO after the end of the challenge.
Closely related to our work, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> both report better performance than the challenge winners using transfer-learning from <span title="" class="ltx_glossaryref">CNNs</span> pre-trained on image data; however, the former use different splits than the challenge and the latter perform a very extensive hyperparameter search (a total of <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S2.p2.1.m1.1a"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><cn type="integer" id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">15</annotation></semantics></math> hyperparameters were optimised, resulting in a search space much larger than the one we employ here).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">We use the official dataset of the INTERSPEECH 2009 Emotion Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, FAU-AIBO.
It is a dataset of German children's speech collected in a Wizard-of-Oz scenario and annotated on the word-level for the presence of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">11</annotation></semantics></math> emotional/communicative states by <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">5</annotation></semantics></math> raters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>.
Subsequently, segmented words have been aggregated to meaningful chunks using manual semantic and prosodic criteria.
Accordingly, annotated states have been mapped to <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">2</annotation></semantics></math>- and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">5</annotation></semantics></math>-class categorisation using a set of heuristics, which forms a final dataset of <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="18\,216" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mn id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">18 216</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><cn type="integer" id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">18216</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">18\,216</annotation></semantics></math> chunks used for the challenge.
The data is heavily imbalanced towards the neutral/non-negative classes.
The data was collected from two schools, with one school set aside for testing (<em id="S3.SS1.p1.5.1" class="ltx_emph ltx_font_italic">Mont</em>) and one set aside for training (<em id="S3.SS1.p1.5.2" class="ltx_emph ltx_font_italic">Ohm</em>); we use the same partitioning for our experiments.
Additionally, we create a small validation set comprising the last two speakers of the training set (speakers are denoted by number IDs): <em id="S3.SS1.p1.5.3" class="ltx_emph ltx_font_italic">Ohm_31</em> and <em id="S3.SS1.p1.5.4" class="ltx_emph ltx_font_italic">Ohm_32</em>, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>.
Note that the data are extensively documented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The Computational Paralinguistics Challenge (ComParE) continued the first challenge for a further 14 years.
Our selection of models is largely based on the ComParE series'
baselines and best-performing winners of each year and on prevalent trends in the last decade of <span title="" class="ltx_glossaryref">SER</span> research.
We briefly describe each model below, but also include an appendix with linked model states as well as plan to release the source code for our experiments<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="github.com/ATriantafyllopoulos/is24-interspeech09-ser-revisited" title="" class="ltx_ref ltx_url ltx_font_typewriter">github.com/ATriantafyllopoulos/is24-interspeech09-ser-revisited</a></span></span></span>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.9" class="ltx_p"><span id="S3.SS2.p2.9.1" class="ltx_text ltx_font_bold">'09-'16: openSMILE feature sets</span> – In the follow-up iterations of the ComParE Challenge, newer versions of paralinguistic features were introduced.
In general, these feature sets were larger and covered a wider gamut of acoustic and prosodic features.
However, that period saw a parallel pursuit for <em id="S3.SS2.p2.9.2" class="ltx_emph ltx_font_italic">smaller</em> expert-driven feature sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>.
To accommodate both, we use both the static (`functionals') and dynamic (`low-level descriptors') versions of the official <span id="S3.SS2.p2.9.3" class="ltx_text ltx_font_smallcaps">IS09</span>-<span id="S3.SS2.p2.9.4" class="ltx_text ltx_font_smallcaps">IS13</span> and <span id="S3.SS2.p2.9.5" class="ltx_text ltx_font_smallcaps">IS16</span> feature sets, as well as the <span id="S3.SS2.p2.9.6" class="ltx_text ltx_font_smallcaps">eGeMAPS</span> feature set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite> as provided in the latest version of the openSMILE toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>.
For the <em id="S3.SS2.p2.9.7" class="ltx_emph ltx_font_italic">dynamic</em> features, we used a <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">2</annotation></semantics></math>-layered <span title="" class="ltx_glossaryref">LSTM</span> model with <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><cn type="integer" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">32</annotation></semantics></math> hidden units, followed by mean pooling over time, one hidden linear layer with <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><cn type="integer" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">32</annotation></semantics></math> neurons and ReLU acivation, and one output linear layer; all hidden layers are followed by a dropout of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><cn type="float" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">0.5</annotation></semantics></math>; these models are denoted with <sup id="S3.SS2.p2.9.8" class="ltx_sup"><span id="S3.SS2.p2.9.8.1" class="ltx_text ltx_font_italic">d</span></sup>.
Additionally, we train <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mn id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><cn type="integer" id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">3</annotation></semantics></math>-layered <span title="" class="ltx_glossaryref">MLPs</span> with <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mn id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><cn type="integer" id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">64</annotation></semantics></math> hidden units each, a dropout of <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mn id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><cn type="float" id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">0.5</annotation></semantics></math>, and ReLU activation for the <em id="S3.SS2.p2.9.9" class="ltx_emph ltx_font_italic">static</em> features; these models are denoted with <sup id="S3.SS2.p2.9.10" class="ltx_sup"><span id="S3.SS2.p2.9.10.1" class="ltx_text ltx_font_italic">s</span></sup>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.7" class="ltx_p"><span id="S3.SS2.p3.7.1" class="ltx_text ltx_font_bold">'12-'23: ImageNet pre-training</span> – Following the introduction of ImageNet and the first <span title="" class="ltx_glossaryref">CNNs</span> trained on it in 2012, such networks were subsequently introduced in the audio and speech domains by substituting images with (pictorial representations of) spectrograms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>, <a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> – a practice that is relevant to this day, with audio transformer models oftentimes initialised with states pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>.
We use <span id="S3.SS2.p3.7.2" class="ltx_text ltx_font_smallcaps">AlexNet</span>, <span id="S3.SS2.p3.7.3" class="ltx_text ltx_font_smallcaps">Resnet50</span>, all versions of <span id="S3.SS2.p3.7.4" class="ltx_text ltx_font_smallcaps">VGG</span> (<sup id="S3.SS2.p3.7.5" class="ltx_sup"><span id="S3.SS2.p3.7.5.1" class="ltx_text ltx_font_italic">11,13,16,19</span></sup>), EfficientNet-B0 (<span id="S3.SS2.p3.7.6" class="ltx_text ltx_font_smallcaps">EffNet</span>), the tiny, small, base, and large versions of <span id="S3.SS2.p3.7.7" class="ltx_text ltx_font_smallcaps">ConvNeXt</span> (<sup id="S3.SS2.p3.7.8" class="ltx_sup"><span id="S3.SS2.p3.7.8.1" class="ltx_text ltx_font_italic">t,b,s,l</span></sup>), and the tiny, base, and small versions of the <span id="S3.SS2.p3.7.9" class="ltx_text ltx_font_smallcaps">Swin</span> Transformer (<sup id="S3.SS2.p3.7.10" class="ltx_sup"><span id="S3.SS2.p3.7.10.1" class="ltx_text ltx_font_italic">t,b,s</span></sup>).
In all cases, we use the best-performing model state on ImageNet as available in the <span id="S3.SS2.p3.7.11" class="ltx_text ltx_font_smallcaps">torchvision-v0.16.0</span> package.As features, we always used the Mel-spectrograms generated for <span id="S3.SS2.p3.7.12" class="ltx_text ltx_font_smallcaps">CNN14</span> (see below), i. e., <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mn id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">64</annotation></semantics></math> Mels with a window size of <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mn id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><cn type="integer" id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">32</annotation></semantics></math> ms and a hop size of <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mn id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><cn type="integer" id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">10</annotation></semantics></math> ms; the resulting matrices were then replicated over the three dimensions to generate the <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mn id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><cn type="integer" id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">3</annotation></semantics></math>-channel input that is required by models designed for computer vision tasks.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.2" class="ltx_p"><span id="S3.SS2.p4.2.3" class="ltx_text ltx_font_bold">'16-'23: End-to-end</span> – Subsequent years saw the introduction of <em id="S3.SS2.p4.2.4" class="ltx_emph ltx_font_italic">end-to-end</em> models, i. e., models trained directly on raw audio input for the target task without any prior feature pre-processing. These models were especially successful in the case of time-continous <span title="" class="ltx_glossaryref">SER</span>, which requires predicting the emotion of very short audio frames, and essentially follow the <span title="" class="ltx_glossaryref">CRNN</span> architecture.
We use two particular instantiations introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite> (<span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_smallcaps">CRNN<sup id="S3.SS2.p4.1.1.1" class="ltx_sup"><span id="S3.SS2.p4.1.1.1.1" class="ltx_text ltx_font_italic">18</span></sup></span>) and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite> (<span id="S3.SS2.p4.2.2" class="ltx_text ltx_font_smallcaps">CRNN<sup id="S3.SS2.p4.2.2.1" class="ltx_sup"><span id="S3.SS2.p4.2.2.1.1" class="ltx_text ltx_font_italic">19</span></sup></span>).</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">'16-'23: Supervised audio pre-training</span>
– In parallel to ImageNet pre-training, there were also efforts to collect similar large-scale datasets for audio where networks could be pre-trained in a supervised fashion.
Two notable examples are VoxCeleb and AudioSet, both collected from YouTube, with the former targeted to speaker identification and the latter to general audio tagging.
VoxCeleb formed the basis for training speaker embedding models (i. e., `x-vectors') using <span title="" class="ltx_glossaryref">time delay neural networks (TDNNs)</span>, of which we use a more recent and improved attention-based model (<span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_smallcaps">ETDNN</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>.
AudioSet in turn inspired the use of VGG-based convolutional networks, such as <span id="S3.SS2.p5.1.3" class="ltx_text ltx_font_smallcaps">CNN14</span> introduced in <span title="" class="ltx_glossaryref">pretrained audio neural networks (PANNs)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite> which was shown to also transfer well to <span title="" class="ltx_glossaryref">SER</span> tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>, and later, transformer-based models such as <span id="S3.SS2.p5.1.4" class="ltx_text ltx_font_smallcaps">AST</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>.
In addition, the introduction of the <span id="S3.SS2.p5.1.5" class="ltx_text ltx_font_smallcaps">Whisper</span> architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>
led to a renaissance of supervised training for <span title="" class="ltx_glossaryref">automatic speech recognition (ASR)</span> and we thus include it in our experiments – albeit only the three smallest available variants (<sup id="S3.SS2.p5.1.6" class="ltx_sup"><span id="S3.SS2.p5.1.6.1" class="ltx_text ltx_font_italic">t,b,s</span></sup>) due to hardware constraints.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.6" class="ltx_p"><span id="S3.SS2.p6.6.6" class="ltx_text ltx_font_bold">'20-'23: Self-supervised audio pre-training</span>
– The introduction of transformers and the advent of self-supervised pre-training for computer vision and <span title="" class="ltx_glossaryref">natural language processing (NLP)</span> also propagated to the speech and audio domain.
The two dominant architectures here are wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>, which includes a convolutional backend followed by a transformer decoder trained to reconstruct its own quantised intermediate representations, and HuBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite>, a full-transformer model trained on masked token prediction.
These models have yielded significant advances in <span title="" class="ltx_glossaryref">SER</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>, which was partially accredited to their ability to simultaneously encode linguistic and paralinguistic information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>.
In this work, we use the pretrained states from the <em id="S3.SS2.p6.6.7" class="ltx_emph ltx_font_italic">base</em> and <em id="S3.SS2.p6.6.8" class="ltx_emph ltx_font_italic">large</em> variants of wav2vec2.0 and HuBert (<span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S3.SS2.p6.1.1.1" class="ltx_sup"><span id="S3.SS2.p6.1.1.1.1" class="ltx_text ltx_font_italic">b,l</span></sup></span>, <span id="S3.SS2.p6.2.2" class="ltx_text ltx_font_smallcaps">HUB<sup id="S3.SS2.p6.2.2.1" class="ltx_sup"><span id="S3.SS2.p6.2.2.1.1" class="ltx_text ltx_font_italic">b,l</span></sup></span>), a multilingual model trained on VoxPopuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite> (<span id="S3.SS2.p6.3.3" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S3.SS2.p6.3.3.1" class="ltx_sup"><span id="S3.SS2.p6.3.3.1.1" class="ltx_text ltx_font_italic">m</span></sup></span>), a <em id="S3.SS2.p6.6.9" class="ltx_emph ltx_font_italic">`robust'</em> version of wav2vec2.0 trained on more data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite> (<span id="S3.SS2.p6.4.4" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S3.SS2.p6.4.4.1" class="ltx_sup"><span id="S3.SS2.p6.4.4.1.1" class="ltx_text ltx_font_italic">r</span></sup></span>), as well as the pruned version of that model further fine-tuned for dimensional <span title="" class="ltx_glossaryref">SER</span> on MSP-Podcast <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite> (<span id="S3.SS2.p6.5.5" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S3.SS2.p6.5.5.1" class="ltx_sup"><span id="S3.SS2.p6.5.5.1.1" class="ltx_text ltx_font_italic">e</span></sup></span>).
Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>, we add an output <math id="S3.SS2.p6.6.m1.1" class="ltx_Math" alttext="2-" display="inline"><semantics id="S3.SS2.p6.6.m1.1a"><mrow id="S3.SS2.p6.6.m1.1.1" xref="S3.SS2.p6.6.m1.1.1.cmml"><mn id="S3.SS2.p6.6.m1.1.1.2" xref="S3.SS2.p6.6.m1.1.1.2.cmml">2</mn><mo id="S3.SS2.p6.6.m1.1.1.3" xref="S3.SS2.p6.6.m1.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m1.1b"><apply id="S3.SS2.p6.6.m1.1.1.cmml" xref="S3.SS2.p6.6.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p6.6.m1.1.1.1.cmml" xref="S3.SS2.p6.6.m1.1.1">limit-from</csymbol><cn type="integer" id="S3.SS2.p6.6.m1.1.1.2.cmml" xref="S3.SS2.p6.6.m1.1.1.2">2</cn><minus id="S3.SS2.p6.6.m1.1.1.3.cmml" xref="S3.SS2.p6.6.m1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m1.1c">2-</annotation></semantics></math>layered <span title="" class="ltx_glossaryref">MLP</span> which takes the pooled hidden embeddings of the last layer as input.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiments</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.13" class="ltx_p">To constrain our space of hyperparameters, we conduct two experimental phases.
In the first <span id="S3.SS3.p1.13.1" class="ltx_text ltx_font_bold">exploration phase</span>, we test all <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="43" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">43</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">43</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">43</annotation></semantics></math> investigated models using a fixed set of hyperparameters.
Specifically, we use the Adam optimiser with a learning rate of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="float" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">0.0001</annotation></semantics></math> and a batch size of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><cn type="integer" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">4</annotation></semantics></math> for <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="integer" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">30</annotation></semantics></math> epochs.
In the following <span id="S3.SS3.p1.13.2" class="ltx_text ltx_font_bold">tuning phase</span>, we further optimise a larger set of hyperparameters for the <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="integer" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">5</annotation></semantics></math> best-performing models from the exploration phase, doing a grid search over optimisers <math id="S3.SS3.p1.6.m6.2" class="ltx_Math" alttext="\{\text{Adam},\text{SGD}\}" display="inline"><semantics id="S3.SS3.p1.6.m6.2a"><mrow id="S3.SS3.p1.6.m6.2.3.2" xref="S3.SS3.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.6.m6.2.3.2.1" xref="S3.SS3.p1.6.m6.2.3.1.cmml">{</mo><mtext id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1a.cmml">Adam</mtext><mo id="S3.SS3.p1.6.m6.2.3.2.2" xref="S3.SS3.p1.6.m6.2.3.1.cmml">,</mo><mtext id="S3.SS3.p1.6.m6.2.2" xref="S3.SS3.p1.6.m6.2.2a.cmml">SGD</mtext><mo stretchy="false" id="S3.SS3.p1.6.m6.2.3.2.3" xref="S3.SS3.p1.6.m6.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.2b"><set id="S3.SS3.p1.6.m6.2.3.1.cmml" xref="S3.SS3.p1.6.m6.2.3.2"><ci id="S3.SS3.p1.6.m6.1.1a.cmml" xref="S3.SS3.p1.6.m6.1.1"><mtext id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">Adam</mtext></ci><ci id="S3.SS3.p1.6.m6.2.2a.cmml" xref="S3.SS3.p1.6.m6.2.2"><mtext id="S3.SS3.p1.6.m6.2.2.cmml" xref="S3.SS3.p1.6.m6.2.2">SGD</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.2c">\{\text{Adam},\text{SGD}\}</annotation></semantics></math>, learning rates <math id="S3.SS3.p1.7.m7.3" class="ltx_Math" alttext="\{0.01,0.001,0.0001\}" display="inline"><semantics id="S3.SS3.p1.7.m7.3a"><mrow id="S3.SS3.p1.7.m7.3.4.2" xref="S3.SS3.p1.7.m7.3.4.1.cmml"><mo stretchy="false" id="S3.SS3.p1.7.m7.3.4.2.1" xref="S3.SS3.p1.7.m7.3.4.1.cmml">{</mo><mn id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">0.01</mn><mo id="S3.SS3.p1.7.m7.3.4.2.2" xref="S3.SS3.p1.7.m7.3.4.1.cmml">,</mo><mn id="S3.SS3.p1.7.m7.2.2" xref="S3.SS3.p1.7.m7.2.2.cmml">0.001</mn><mo id="S3.SS3.p1.7.m7.3.4.2.3" xref="S3.SS3.p1.7.m7.3.4.1.cmml">,</mo><mn id="S3.SS3.p1.7.m7.3.3" xref="S3.SS3.p1.7.m7.3.3.cmml">0.0001</mn><mo stretchy="false" id="S3.SS3.p1.7.m7.3.4.2.4" xref="S3.SS3.p1.7.m7.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.3b"><set id="S3.SS3.p1.7.m7.3.4.1.cmml" xref="S3.SS3.p1.7.m7.3.4.2"><cn type="float" id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">0.01</cn><cn type="float" id="S3.SS3.p1.7.m7.2.2.cmml" xref="S3.SS3.p1.7.m7.2.2">0.001</cn><cn type="float" id="S3.SS3.p1.7.m7.3.3.cmml" xref="S3.SS3.p1.7.m7.3.3">0.0001</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.3c">\{0.01,0.001,0.0001\}</annotation></semantics></math>, and batch sizes <math id="S3.SS3.p1.8.m8.3" class="ltx_Math" alttext="\{4,8,16\}" display="inline"><semantics id="S3.SS3.p1.8.m8.3a"><mrow id="S3.SS3.p1.8.m8.3.4.2" xref="S3.SS3.p1.8.m8.3.4.1.cmml"><mo stretchy="false" id="S3.SS3.p1.8.m8.3.4.2.1" xref="S3.SS3.p1.8.m8.3.4.1.cmml">{</mo><mn id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">4</mn><mo id="S3.SS3.p1.8.m8.3.4.2.2" xref="S3.SS3.p1.8.m8.3.4.1.cmml">,</mo><mn id="S3.SS3.p1.8.m8.2.2" xref="S3.SS3.p1.8.m8.2.2.cmml">8</mn><mo id="S3.SS3.p1.8.m8.3.4.2.3" xref="S3.SS3.p1.8.m8.3.4.1.cmml">,</mo><mn id="S3.SS3.p1.8.m8.3.3" xref="S3.SS3.p1.8.m8.3.3.cmml">16</mn><mo stretchy="false" id="S3.SS3.p1.8.m8.3.4.2.4" xref="S3.SS3.p1.8.m8.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.3b"><set id="S3.SS3.p1.8.m8.3.4.1.cmml" xref="S3.SS3.p1.8.m8.3.4.2"><cn type="integer" id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">4</cn><cn type="integer" id="S3.SS3.p1.8.m8.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2">8</cn><cn type="integer" id="S3.SS3.p1.8.m8.3.3.cmml" xref="S3.SS3.p1.8.m8.3.3">16</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.3c">\{4,8,16\}</annotation></semantics></math>, while training each configuration for <math id="S3.SS3.p1.9.m9.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS3.p1.9.m9.1a"><mn id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><cn type="integer" id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">50</annotation></semantics></math> epochs.
In total, this results in <math id="S3.SS3.p1.10.m10.1" class="ltx_Math" alttext="43" display="inline"><semantics id="S3.SS3.p1.10.m10.1a"><mn id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml">43</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><cn type="integer" id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1">43</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">43</annotation></semantics></math> runs for the exploration phase and an additional <math id="S3.SS3.p1.11.m11.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S3.SS3.p1.11.m11.1a"><mn id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><cn type="integer" id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">90</annotation></semantics></math> runs for the tuning phase (for each of the two challenge tasks).
To account for variable-length sequences in training, we
randomly cropped/padded all chunks to a fixed length of <math id="S3.SS3.p1.12.m12.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS3.p1.12.m12.1a"><mn id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.1b"><cn type="integer" id="S3.SS3.p1.12.m12.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.1c">3</annotation></semantics></math> seconds (different cropping/padding was applied on each instance across different epochs; random seed was fixed and can be reproduced) when using dynamic features (including the raw audio); during inference, we used the original utterances, only padding those shorter than <math id="S3.SS3.p1.13.m13.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p1.13.m13.1a"><mn id="S3.SS3.p1.13.m13.1.1" xref="S3.SS3.p1.13.m13.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m13.1b"><cn type="integer" id="S3.SS3.p1.13.m13.1.1.cmml" xref="S3.SS3.p1.13.m13.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m13.1c">2</annotation></semantics></math> seconds with silence.
Our loss function is the standard categorical cross-entropy, where – in order to account for the severe class imbalance – we further weigh the contribution of each instance by the inverse frequency of its true label on the training set, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>.
Except for <span id="S3.SS3.p1.13.3" class="ltx_text ltx_font_smallcaps">W2V2</span> and <span id="S3.SS3.p1.13.4" class="ltx_text ltx_font_smallcaps">HUB</span>, where we freeze the feature extractors, all model parameters are fine-tuned.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In all cases, we use the defined validation set (comprising two speakers from the original training set) to select the best-performing epoch for each model, which we then proceed to evaluate on the test set.
Strictly speaking, this results in different training data from some challenge participants, as they typically retrained their models on the entire training set after optimising their hyperparameters (e. g., via cross-validation).
However, given the propensity of <span title="" class="ltx_glossaryref">deep neural networks (DNNs)</span> to overfit when trained too long, we found the use of such a validation set necessary for early stopping.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results &amp; Discussion</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.50.5.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.8.4" class="ltx_text" style="font-size:90%;">
UAR results for all tested models in the <span id="S4.T1.8.4.1" class="ltx_text ltx_font_bold">exploration phase</span> using our standard hypermarameters (Adam, <math id="S4.T1.5.1.m1.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S4.T1.5.1.m1.1b"><mn id="S4.T1.5.1.m1.1.1" xref="S4.T1.5.1.m1.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.1.m1.1c"><cn type="float" id="S4.T1.5.1.m1.1.1.cmml" xref="S4.T1.5.1.m1.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.1.m1.1d">0.01</annotation></semantics></math>, <math id="S4.T1.6.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.T1.6.2.m2.1b"><mn id="S4.T1.6.2.m2.1.1" xref="S4.T1.6.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.2.m2.1c"><cn type="integer" id="S4.T1.6.2.m2.1.1.cmml" xref="S4.T1.6.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.2.m2.1d">4</annotation></semantics></math>) for both the <math id="S4.T1.7.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T1.7.3.m3.1b"><mn id="S4.T1.7.3.m3.1.1" xref="S4.T1.7.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.3.m3.1c"><cn type="integer" id="S4.T1.7.3.m3.1.1.cmml" xref="S4.T1.7.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.3.m3.1d">2</annotation></semantics></math>- and the <math id="S4.T1.8.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T1.8.4.m4.1b"><mn id="S4.T1.8.4.m4.1.1" xref="S4.T1.8.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T1.8.4.m4.1c"><cn type="integer" id="S4.T1.8.4.m4.1.1.cmml" xref="S4.T1.8.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.4.m4.1d">5</annotation></semantics></math>-class model.
</span></figcaption>
<table id="S4.T1.47" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.10.2" class="ltx_tr">
<th id="S4.T1.10.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<th id="S4.T1.10.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">Year</th>
<td id="S4.T1.9.1.1" class="ltx_td ltx_align_left ltx_border_tt">
<math id="S4.T1.9.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T1.9.1.1.m1.1a"><mn id="S4.T1.9.1.1.m1.1.1" xref="S4.T1.9.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.1.1.m1.1b"><cn type="integer" id="S4.T1.9.1.1.m1.1.1.cmml" xref="S4.T1.9.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.1.1.m1.1c">2</annotation></semantics></math>-class</td>
<td id="S4.T1.10.2.2" class="ltx_td ltx_align_left ltx_border_tt">
<math id="S4.T1.10.2.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T1.10.2.2.m1.1a"><mn id="S4.T1.10.2.2.m1.1.1" xref="S4.T1.10.2.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T1.10.2.2.m1.1b"><cn type="integer" id="S4.T1.10.2.2.m1.1.1.cmml" xref="S4.T1.10.2.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.2.2.m1.1c">5</annotation></semantics></math>-class</td>
</tr>
<tr id="S4.T1.11.3" class="ltx_tr">
<th id="S4.T1.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.11.3.1.1" class="ltx_text ltx_font_smallcaps">IS09<sup id="S4.T1.11.3.1.1.1" class="ltx_sup"><span id="S4.T1.11.3.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.11.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">2009</th>
<td id="S4.T1.11.3.3" class="ltx_td ltx_align_left ltx_border_t" style="background-color:#C4C4C4;"><span id="S4.T1.11.3.3.1" class="ltx_text" style="background-color:#C4C4C4;">.620</span></td>
<td id="S4.T1.11.3.4" class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FCFCFC;"><span id="S4.T1.11.3.4.1" class="ltx_text" style="background-color:#FCFCFC;">.294</span></td>
</tr>
<tr id="S4.T1.12.4" class="ltx_tr">
<th id="S4.T1.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.12.4.1.1" class="ltx_text ltx_font_smallcaps">IS09<sup id="S4.T1.12.4.1.1.1" class="ltx_sup"><span id="S4.T1.12.4.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.12.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2009</th>
<td id="S4.T1.12.4.3" class="ltx_td ltx_align_left" style="background-color:#858585;"><span id="S4.T1.12.4.3.1" class="ltx_text" style="background-color:#858585;">.670</span></td>
<td id="S4.T1.12.4.4" class="ltx_td ltx_align_left" style="background-color:#BFBFBF;"><span id="S4.T1.12.4.4.1" class="ltx_text" style="background-color:#BFBFBF;">.347</span></td>
</tr>
<tr id="S4.T1.13.5" class="ltx_tr">
<th id="S4.T1.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.13.5.1.1" class="ltx_text ltx_font_smallcaps">IS10<sup id="S4.T1.13.5.1.1.1" class="ltx_sup"><span id="S4.T1.13.5.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.13.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2010</th>
<td id="S4.T1.13.5.3" class="ltx_td ltx_align_left" style="background-color:#828282;"><span id="S4.T1.13.5.3.1" class="ltx_text" style="background-color:#828282;">.670</span></td>
<td id="S4.T1.13.5.4" class="ltx_td ltx_align_left" style="background-color:#BFBFBF;"><span id="S4.T1.13.5.4.1" class="ltx_text" style="background-color:#BFBFBF;">.348</span></td>
</tr>
<tr id="S4.T1.14.6" class="ltx_tr">
<th id="S4.T1.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.14.6.1.1" class="ltx_text ltx_font_smallcaps">IS10<sup id="S4.T1.14.6.1.1.1" class="ltx_sup"><span id="S4.T1.14.6.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.14.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2010</th>
<td id="S4.T1.14.6.3" class="ltx_td ltx_align_left" style="background-color:#6B6B6B;"><span id="S4.T1.14.6.3.1" class="ltx_text" style="background-color:#6B6B6B;">.689</span></td>
<td id="S4.T1.14.6.4" class="ltx_td ltx_align_left" style="background-color:#808080;"><span id="S4.T1.14.6.4.1" class="ltx_text" style="background-color:#808080;">.406</span></td>
</tr>
<tr id="S4.T1.15.7" class="ltx_tr">
<th id="S4.T1.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.15.7.1.1" class="ltx_text ltx_font_smallcaps">IS11<sup id="S4.T1.15.7.1.1.1" class="ltx_sup"><span id="S4.T1.15.7.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.15.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2011</th>
<td id="S4.T1.15.7.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.15.7.3.1" class="ltx_text" style="background-color:#FFFFFF;">.499</span></td>
<td id="S4.T1.15.7.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.15.7.4.1" class="ltx_text" style="background-color:#FFFFFF;">.201</span></td>
</tr>
<tr id="S4.T1.16.8" class="ltx_tr">
<th id="S4.T1.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.16.8.1.1" class="ltx_text ltx_font_smallcaps">IS11<sup id="S4.T1.16.8.1.1.1" class="ltx_sup"><span id="S4.T1.16.8.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.16.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2011</th>
<td id="S4.T1.16.8.3" class="ltx_td ltx_align_left" style="background-color:#858585;"><span id="S4.T1.16.8.3.1" class="ltx_text" style="background-color:#858585;">.670</span></td>
<td id="S4.T1.16.8.4" class="ltx_td ltx_align_left" style="background-color:#A3A3A3;"><span id="S4.T1.16.8.4.1" class="ltx_text" style="background-color:#A3A3A3;">.373</span></td>
</tr>
<tr id="S4.T1.17.9" class="ltx_tr">
<th id="S4.T1.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.17.9.1.1" class="ltx_text ltx_font_smallcaps">IS12<sup id="S4.T1.17.9.1.1.1" class="ltx_sup"><span id="S4.T1.17.9.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.17.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2012</th>
<td id="S4.T1.17.9.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.17.9.3.1" class="ltx_text" style="background-color:#FFFFFF;">.501</span></td>
<td id="S4.T1.17.9.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.17.9.4.1" class="ltx_text" style="background-color:#FFFFFF;">.201</span></td>
</tr>
<tr id="S4.T1.47.40.1" class="ltx_tr">
<th id="S4.T1.47.40.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.40.1.1.1" class="ltx_text ltx_font_smallcaps">AlexNet</span></th>
<th id="S4.T1.47.40.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2012</th>
<td id="S4.T1.47.40.1.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.47.40.1.3.1" class="ltx_text" style="background-color:#FFFFFF;">.503</span></td>
<td id="S4.T1.47.40.1.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.47.40.1.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.18.10" class="ltx_tr">
<th id="S4.T1.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.18.10.1.1" class="ltx_text ltx_font_smallcaps">IS12<sup id="S4.T1.18.10.1.1.1" class="ltx_sup"><span id="S4.T1.18.10.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.18.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2012</th>
<td id="S4.T1.18.10.3" class="ltx_td ltx_align_left" style="background-color:#787878;"><span id="S4.T1.18.10.3.1" class="ltx_text" style="background-color:#787878;">.679</span></td>
<td id="S4.T1.18.10.4" class="ltx_td ltx_align_left" style="background-color:#A3A3A3;"><span id="S4.T1.18.10.4.1" class="ltx_text" style="background-color:#A3A3A3;">.373</span></td>
</tr>
<tr id="S4.T1.19.11" class="ltx_tr">
<th id="S4.T1.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.19.11.1.1" class="ltx_text ltx_font_smallcaps">IS13<sup id="S4.T1.19.11.1.1.1" class="ltx_sup"><span id="S4.T1.19.11.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.19.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2013</th>
<td id="S4.T1.19.11.3" class="ltx_td ltx_align_left" style="background-color:#8C8C8C;"><span id="S4.T1.19.11.3.1" class="ltx_text" style="background-color:#8C8C8C;">.664</span></td>
<td id="S4.T1.19.11.4" class="ltx_td ltx_align_left" style="background-color:#9E9E9E;"><span id="S4.T1.19.11.4.1" class="ltx_text" style="background-color:#9E9E9E;">.378</span></td>
</tr>
<tr id="S4.T1.20.12" class="ltx_tr">
<th id="S4.T1.20.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.20.12.1.1" class="ltx_text ltx_font_smallcaps">IS13<sup id="S4.T1.20.12.1.1.1" class="ltx_sup"><span id="S4.T1.20.12.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.20.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2013</th>
<td id="S4.T1.20.12.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.20.12.3.1" class="ltx_text" style="background-color:#FFFFFF;">.498</span></td>
<td id="S4.T1.20.12.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.20.12.4.1" class="ltx_text" style="background-color:#FFFFFF;">.201</span></td>
</tr>
<tr id="S4.T1.21.13" class="ltx_tr">
<th id="S4.T1.21.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.21.13.1.1" class="ltx_text ltx_font_smallcaps">eGeMAPS<sup id="S4.T1.21.13.1.1.1" class="ltx_sup"><span id="S4.T1.21.13.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.21.13.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2015</th>
<td id="S4.T1.21.13.3" class="ltx_td ltx_align_left" style="background-color:#C7C7C7;"><span id="S4.T1.21.13.3.1" class="ltx_text" style="background-color:#C7C7C7;">.618</span></td>
<td id="S4.T1.21.13.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.21.13.4.1" class="ltx_text" style="background-color:#FFFFFF;">.245</span></td>
</tr>
<tr id="S4.T1.22.14" class="ltx_tr">
<th id="S4.T1.22.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.22.14.1.1" class="ltx_text ltx_font_smallcaps">eGeMAPS<sup id="S4.T1.22.14.1.1.1" class="ltx_sup"><span id="S4.T1.22.14.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.22.14.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2015</th>
<td id="S4.T1.22.14.3" class="ltx_td ltx_align_left" style="background-color:#878787;"><span id="S4.T1.22.14.3.1" class="ltx_text" style="background-color:#878787;">.667</span></td>
<td id="S4.T1.22.14.4" class="ltx_td ltx_align_left" style="background-color:#8A8A8A;"><span id="S4.T1.22.14.4.1" class="ltx_text" style="background-color:#8A8A8A;">.397</span></td>
</tr>
<tr id="S4.T1.47.41.2" class="ltx_tr">
<th id="S4.T1.47.41.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.41.2.1.1" class="ltx_text ltx_font_smallcaps">Resnet50</span></th>
<th id="S4.T1.47.41.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2015</th>
<td id="S4.T1.47.41.2.3" class="ltx_td ltx_align_left" style="background-color:#6B6B6B;"><span id="S4.T1.47.41.2.3.1" class="ltx_text" style="background-color:#6B6B6B;">.688</span></td>
<td id="S4.T1.47.41.2.4" class="ltx_td ltx_align_left" style="background-color:#666666;"><span id="S4.T1.47.41.2.4.1" class="ltx_text" style="background-color:#666666;">.428</span></td>
</tr>
<tr id="S4.T1.23.15" class="ltx_tr">
<th id="S4.T1.23.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.23.15.1.1" class="ltx_text ltx_font_smallcaps">VGG<sup id="S4.T1.23.15.1.1.1" class="ltx_sup"><span id="S4.T1.23.15.1.1.1.1" class="ltx_text ltx_font_italic">19</span></sup></span></th>
<th id="S4.T1.23.15.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.23.15.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.23.15.3.1" class="ltx_text" style="background-color:#FFFFFF;">.499</span></td>
<td id="S4.T1.23.15.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.23.15.4.1" class="ltx_text" style="background-color:#FFFFFF;">.204</span></td>
</tr>
<tr id="S4.T1.24.16" class="ltx_tr">
<th id="S4.T1.24.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.24.16.1.1" class="ltx_text ltx_font_smallcaps">IS16<sup id="S4.T1.24.16.1.1.1" class="ltx_sup"><span id="S4.T1.24.16.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.24.16.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.24.16.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.24.16.3.1" class="ltx_text" style="background-color:#FFFFFF;">.500</span></td>
<td id="S4.T1.24.16.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.24.16.4.1" class="ltx_text" style="background-color:#FFFFFF;">.201</span></td>
</tr>
<tr id="S4.T1.25.17" class="ltx_tr">
<th id="S4.T1.25.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.25.17.1.1" class="ltx_text ltx_font_smallcaps">VGG<sup id="S4.T1.25.17.1.1.1" class="ltx_sup"><span id="S4.T1.25.17.1.1.1.1" class="ltx_text ltx_font_italic">16</span></sup></span></th>
<th id="S4.T1.25.17.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.25.17.3" class="ltx_td ltx_align_left" style="background-color:#A3A3A3;"><span id="S4.T1.25.17.3.1" class="ltx_text" style="background-color:#A3A3A3;">.646</span></td>
<td id="S4.T1.25.17.4" class="ltx_td ltx_align_left" style="background-color:#828282;"><span id="S4.T1.25.17.4.1" class="ltx_text" style="background-color:#828282;">.404</span></td>
</tr>
<tr id="S4.T1.26.18" class="ltx_tr">
<th id="S4.T1.26.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.26.18.1.1" class="ltx_text ltx_font_smallcaps">IS16<sup id="S4.T1.26.18.1.1.1" class="ltx_sup"><span id="S4.T1.26.18.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<th id="S4.T1.26.18.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.26.18.3" class="ltx_td ltx_align_left" style="background-color:#969696;"><span id="S4.T1.26.18.3.1" class="ltx_text" style="background-color:#969696;">.655</span></td>
<td id="S4.T1.26.18.4" class="ltx_td ltx_align_left" style="background-color:#999999;"><span id="S4.T1.26.18.4.1" class="ltx_text" style="background-color:#999999;">.382</span></td>
</tr>
<tr id="S4.T1.27.19" class="ltx_tr">
<th id="S4.T1.27.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.27.19.1.1" class="ltx_text ltx_font_smallcaps">VGG<sup id="S4.T1.27.19.1.1.1" class="ltx_sup"><span id="S4.T1.27.19.1.1.1.1" class="ltx_text ltx_font_italic">13</span></sup></span></th>
<th id="S4.T1.27.19.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.27.19.3" class="ltx_td ltx_align_left" style="background-color:#8A8A8A;"><span id="S4.T1.27.19.3.1" class="ltx_text" style="background-color:#8A8A8A;">.665</span></td>
<td id="S4.T1.27.19.4" class="ltx_td ltx_align_left" style="background-color:#969696;"><span id="S4.T1.27.19.4.1" class="ltx_text" style="background-color:#969696;">.385</span></td>
</tr>
<tr id="S4.T1.28.20" class="ltx_tr">
<th id="S4.T1.28.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.28.20.1.1" class="ltx_text ltx_font_smallcaps">VGG<sup id="S4.T1.28.20.1.1.1" class="ltx_sup"><span id="S4.T1.28.20.1.1.1.1" class="ltx_text ltx_font_italic">11</span></sup></span></th>
<th id="S4.T1.28.20.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2016</th>
<td id="S4.T1.28.20.3" class="ltx_td ltx_align_left" style="background-color:#8A8A8A;"><span id="S4.T1.28.20.3.1" class="ltx_text" style="background-color:#8A8A8A;">.666</span></td>
<td id="S4.T1.28.20.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.28.20.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.47.42.3" class="ltx_tr">
<th id="S4.T1.47.42.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.42.3.1.1" class="ltx_text ltx_font_smallcaps">EffNet</span></th>
<th id="S4.T1.47.42.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2019</th>
<td id="S4.T1.47.42.3.3" class="ltx_td ltx_align_left" style="background-color:#858585;"><span id="S4.T1.47.42.3.3.1" class="ltx_text" style="background-color:#858585;">.669</span></td>
<td id="S4.T1.47.42.3.4" class="ltx_td ltx_align_left" style="background-color:#BABABA;"><span id="S4.T1.47.42.3.4.1" class="ltx_text" style="background-color:#BABABA;">.353</span></td>
</tr>
<tr id="S4.T1.29.21" class="ltx_tr">
<th id="S4.T1.29.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.29.21.1.1" class="ltx_text ltx_font_smallcaps">CRNN<sup id="S4.T1.29.21.1.1.1" class="ltx_sup"><span id="S4.T1.29.21.1.1.1.1" class="ltx_text ltx_font_italic">18</span></sup></span></th>
<th id="S4.T1.29.21.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2018</th>
<td id="S4.T1.29.21.3" class="ltx_td ltx_align_left" style="background-color:#787878;"><span id="S4.T1.29.21.3.1" class="ltx_text" style="background-color:#787878;">.680</span></td>
<td id="S4.T1.29.21.4" class="ltx_td ltx_align_left" style="background-color:#8F8F8F;"><span id="S4.T1.29.21.4.1" class="ltx_text" style="background-color:#8F8F8F;">.392</span></td>
</tr>
<tr id="S4.T1.30.22" class="ltx_tr">
<th id="S4.T1.30.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.30.22.1.1" class="ltx_text ltx_font_smallcaps">CRNN<sup id="S4.T1.30.22.1.1.1" class="ltx_sup"><span id="S4.T1.30.22.1.1.1.1" class="ltx_text ltx_font_italic">19</span></sup></span></th>
<th id="S4.T1.30.22.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2019</th>
<td id="S4.T1.30.22.3" class="ltx_td ltx_align_left" style="background-color:#737373;"><span id="S4.T1.30.22.3.1" class="ltx_text" style="background-color:#737373;">.683</span></td>
<td id="S4.T1.30.22.4" class="ltx_td ltx_align_left" style="background-color:#A6A6A6;"><span id="S4.T1.30.22.4.1" class="ltx_text" style="background-color:#A6A6A6;">.372</span></td>
</tr>
<tr id="S4.T1.31.23" class="ltx_tr">
<th id="S4.T1.31.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.31.23.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T1.31.23.1.1.1" class="ltx_sup"><span id="S4.T1.31.23.1.1.1.1" class="ltx_text ltx_font_italic">l</span></sup></span></th>
<th id="S4.T1.31.23.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.31.23.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.31.23.3.1" class="ltx_text" style="background-color:#FFFFFF;">.500</span></td>
<td id="S4.T1.31.23.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.31.23.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.32.24" class="ltx_tr">
<th id="S4.T1.32.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.32.24.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T1.32.24.1.1.1" class="ltx_sup"><span id="S4.T1.32.24.1.1.1.1" class="ltx_text ltx_font_italic">b</span></sup></span></th>
<th id="S4.T1.32.24.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.32.24.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.32.24.3.1" class="ltx_text" style="background-color:#FFFFFF;">.500</span></td>
<td id="S4.T1.32.24.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.32.24.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.33.25" class="ltx_tr">
<th id="S4.T1.33.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.33.25.1.1" class="ltx_text ltx_font_smallcaps">ConvNeXt<sup id="S4.T1.33.25.1.1.1" class="ltx_sup"><span id="S4.T1.33.25.1.1.1.1" class="ltx_text ltx_font_italic">t</span></sup></span></th>
<th id="S4.T1.33.25.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.33.25.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.33.25.3.1" class="ltx_text" style="background-color:#FFFFFF;">.500</span></td>
<td id="S4.T1.33.25.4" class="ltx_td ltx_align_left" style="background-color:#858585;"><span id="S4.T1.33.25.4.1" class="ltx_text" style="background-color:#858585;">.400</span></td>
</tr>
<tr id="S4.T1.34.26" class="ltx_tr">
<th id="S4.T1.34.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.34.26.1.1" class="ltx_text ltx_font_smallcaps">ConvNeXt<sup id="S4.T1.34.26.1.1.1" class="ltx_sup"><span id="S4.T1.34.26.1.1.1.1" class="ltx_text ltx_font_italic">l</span></sup></span></th>
<th id="S4.T1.34.26.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.34.26.3" class="ltx_td ltx_align_left" style="background-color:#8C8C8C;"><span id="S4.T1.34.26.3.1" class="ltx_text" style="background-color:#8C8C8C;">.663</span></td>
<td id="S4.T1.34.26.4" class="ltx_td ltx_align_left" style="background-color:#7A7A7A;"><span id="S4.T1.34.26.4.1" class="ltx_text" style="background-color:#7A7A7A;">.409</span></td>
</tr>
<tr id="S4.T1.35.27" class="ltx_tr">
<th id="S4.T1.35.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.35.27.1.1" class="ltx_text ltx_font_smallcaps">ConvNeXt<sup id="S4.T1.35.27.1.1.1" class="ltx_sup"><span id="S4.T1.35.27.1.1.1.1" class="ltx_text ltx_font_italic">b</span></sup></span></th>
<th id="S4.T1.35.27.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.35.27.3" class="ltx_td ltx_align_left" style="background-color:#8A8A8A;"><span id="S4.T1.35.27.3.1" class="ltx_text" style="background-color:#8A8A8A;">.665</span></td>
<td id="S4.T1.35.27.4" class="ltx_td ltx_align_left" style="background-color:#787878;"><span id="S4.T1.35.27.4.1" class="ltx_text" style="background-color:#787878;">.412</span></td>
</tr>
<tr id="S4.T1.36.28" class="ltx_tr">
<th id="S4.T1.36.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.36.28.1.1" class="ltx_text ltx_font_smallcaps">ConvNeXt<sup id="S4.T1.36.28.1.1.1" class="ltx_sup"><span id="S4.T1.36.28.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.36.28.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.36.28.3" class="ltx_td ltx_align_left" style="background-color:#808080;"><span id="S4.T1.36.28.3.1" class="ltx_text" style="background-color:#808080;">.674</span></td>
<td id="S4.T1.36.28.4" class="ltx_td ltx_align_left" style="background-color:#808080;"><span id="S4.T1.36.28.4.1" class="ltx_text" style="background-color:#808080;">.406</span></td>
</tr>
<tr id="S4.T1.47.43.4" class="ltx_tr">
<th id="S4.T1.47.43.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.43.4.1.1" class="ltx_text ltx_font_smallcaps">ETDNN</span></th>
<th id="S4.T1.47.43.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.47.43.4.3" class="ltx_td ltx_align_left" style="background-color:#787878;"><span id="S4.T1.47.43.4.3.1" class="ltx_text" style="background-color:#787878;">.678</span></td>
<td id="S4.T1.47.43.4.4" class="ltx_td ltx_align_left" style="background-color:#808080;"><span id="S4.T1.47.43.4.4.1" class="ltx_text" style="background-color:#808080;">.404</span></td>
</tr>
<tr id="S4.T1.47.44.5" class="ltx_tr">
<th id="S4.T1.47.44.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.44.5.1.1" class="ltx_text ltx_font_smallcaps">CNN14</span></th>
<th id="S4.T1.47.44.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2020</th>
<td id="S4.T1.47.44.5.3" class="ltx_td ltx_align_left" style="background-color:#666666;"><span id="S4.T1.47.44.5.3.1" class="ltx_text" style="background-color:#666666;">.692</span></td>
<td id="S4.T1.47.44.5.4" class="ltx_td ltx_align_left" style="background-color:#8C8C8C;"><span id="S4.T1.47.44.5.4.1" class="ltx_text" style="background-color:#8C8C8C;">.394</span></td>
</tr>
<tr id="S4.T1.37.29" class="ltx_tr">
<th id="S4.T1.37.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.37.29.1.1" class="ltx_text ltx_font_smallcaps">HUB<sup id="S4.T1.37.29.1.1.1" class="ltx_sup"><span id="S4.T1.37.29.1.1.1.1" class="ltx_text ltx_font_italic">b</span></sup></span></th>
<th id="S4.T1.37.29.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.37.29.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.37.29.3.1" class="ltx_text" style="background-color:#FFFFFF;">.500</span></td>
<td id="S4.T1.37.29.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.37.29.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.38.30" class="ltx_tr">
<th id="S4.T1.38.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.38.30.1.1" class="ltx_text ltx_font_smallcaps">Swin<sup id="S4.T1.38.30.1.1.1" class="ltx_sup"><span id="S4.T1.38.30.1.1.1.1" class="ltx_text ltx_font_italic">b</span></sup></span></th>
<th id="S4.T1.38.30.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.38.30.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.38.30.3.1" class="ltx_text" style="background-color:#FFFFFF;">.528</span></td>
<td id="S4.T1.38.30.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.38.30.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.39.31" class="ltx_tr">
<th id="S4.T1.39.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.39.31.1.1" class="ltx_text ltx_font_smallcaps">Swin<sup id="S4.T1.39.31.1.1.1" class="ltx_sup"><span id="S4.T1.39.31.1.1.1.1" class="ltx_text ltx_font_italic">t</span></sup></span></th>
<th id="S4.T1.39.31.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.39.31.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.39.31.3.1" class="ltx_text" style="background-color:#FFFFFF;">.530</span></td>
<td id="S4.T1.39.31.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.39.31.4.1" class="ltx_text" style="background-color:#FFFFFF;">.242</span></td>
</tr>
<tr id="S4.T1.47.45.6" class="ltx_tr">
<th id="S4.T1.47.45.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.45.6.1.1" class="ltx_text ltx_font_smallcaps">AST</span></th>
<th id="S4.T1.47.45.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.47.45.6.3" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.47.45.6.3.1" class="ltx_text" style="background-color:#FFFFFF;">.535</span></td>
<td id="S4.T1.47.45.6.4" class="ltx_td ltx_align_left" style="background-color:#F5F5F5;"><span id="S4.T1.47.45.6.4.1" class="ltx_text" style="background-color:#F5F5F5;">.300</span></td>
</tr>
<tr id="S4.T1.40.32" class="ltx_tr">
<th id="S4.T1.40.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.40.32.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T1.40.32.1.1.1" class="ltx_sup"><span id="S4.T1.40.32.1.1.1.1" class="ltx_text ltx_font_italic">m</span></sup></span></th>
<th id="S4.T1.40.32.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.40.32.3" class="ltx_td ltx_align_left" style="background-color:#ABABAB;"><span id="S4.T1.40.32.3.1" class="ltx_text" style="background-color:#ABABAB;">.640</span></td>
<td id="S4.T1.40.32.4" class="ltx_td ltx_align_left" style="background-color:#858585;"><span id="S4.T1.40.32.4.1" class="ltx_text" style="background-color:#858585;">.402</span></td>
</tr>
<tr id="S4.T1.41.33" class="ltx_tr">
<th id="S4.T1.41.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.41.33.1.1" class="ltx_text ltx_font_smallcaps">HUB<sup id="S4.T1.41.33.1.1.1" class="ltx_sup"><span id="S4.T1.41.33.1.1.1.1" class="ltx_text ltx_font_italic">l</span></sup></span></th>
<th id="S4.T1.41.33.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.41.33.3" class="ltx_td ltx_align_left" style="background-color:#878787;"><span id="S4.T1.41.33.3.1" class="ltx_text" style="background-color:#878787;">.667</span></td>
<td id="S4.T1.41.33.4" class="ltx_td ltx_align_left" style="background-color:#707070;"><span id="S4.T1.41.33.4.1" class="ltx_text" style="background-color:#707070;">.418</span></td>
</tr>
<tr id="S4.T1.42.34" class="ltx_tr">
<th id="S4.T1.42.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.42.34.1.1" class="ltx_text ltx_font_smallcaps">Swin<sup id="S4.T1.42.34.1.1.1" class="ltx_sup"><span id="S4.T1.42.34.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.42.34.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.42.34.3" class="ltx_td ltx_align_left" style="background-color:#808080;"><span id="S4.T1.42.34.3.1" class="ltx_text" style="background-color:#808080;">.672</span></td>
<td id="S4.T1.42.34.4" class="ltx_td ltx_align_left" style="background-color:#EDEDED;"><span id="S4.T1.42.34.4.1" class="ltx_text" style="background-color:#EDEDED;">.306</span></td>
</tr>
<tr id="S4.T1.43.35" class="ltx_tr">
<th id="S4.T1.43.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.43.35.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T1.43.35.1.1.1" class="ltx_sup"><span id="S4.T1.43.35.1.1.1.1" class="ltx_text ltx_font_italic">r</span></sup></span></th>
<th id="S4.T1.43.35.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2021</th>
<td id="S4.T1.43.35.3" class="ltx_td ltx_align_left" style="background-color:#707070;"><span id="S4.T1.43.35.3.1" class="ltx_text" style="background-color:#707070;">.684</span></td>
<td id="S4.T1.43.35.4" class="ltx_td ltx_align_left" style="background-color:#7A7A7A;"><span id="S4.T1.43.35.4.1" class="ltx_text" style="background-color:#7A7A7A;">.411</span></td>
</tr>
<tr id="S4.T1.44.36" class="ltx_tr">
<th id="S4.T1.44.36.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.44.36.1.1" class="ltx_text ltx_font_smallcaps">Whisper<sup id="S4.T1.44.36.1.1.1" class="ltx_sup"><span id="S4.T1.44.36.1.1.1.1" class="ltx_text ltx_font_italic">s</span></sup></span></th>
<th id="S4.T1.44.36.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2023</th>
<td id="S4.T1.44.36.3" class="ltx_td ltx_align_left" style="background-color:#969696;"><span id="S4.T1.44.36.3.1" class="ltx_text" style="background-color:#969696;">.656</span></td>
<td id="S4.T1.44.36.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.44.36.4.1" class="ltx_text" style="background-color:#FFFFFF;">.279</span></td>
</tr>
<tr id="S4.T1.45.37" class="ltx_tr">
<th id="S4.T1.45.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.45.37.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T1.45.37.1.1.1" class="ltx_sup"><span id="S4.T1.45.37.1.1.1.1" class="ltx_text ltx_font_italic">e</span></sup></span></th>
<th id="S4.T1.45.37.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2023</th>
<td id="S4.T1.45.37.3" class="ltx_td ltx_align_left" style="background-color:#737373;"><span id="S4.T1.45.37.3.1" class="ltx_text" style="background-color:#737373;">.684</span></td>
<td id="S4.T1.45.37.4" class="ltx_td ltx_align_left" style="background-color:#787878;"><span id="S4.T1.45.37.4.1" class="ltx_text" style="background-color:#787878;">.411</span></td>
</tr>
<tr id="S4.T1.46.38" class="ltx_tr">
<th id="S4.T1.46.38.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.46.38.1.1" class="ltx_text ltx_font_smallcaps">Whisper<sup id="S4.T1.46.38.1.1.1" class="ltx_sup"><span id="S4.T1.46.38.1.1.1.1" class="ltx_text ltx_font_italic">t</span></sup></span></th>
<th id="S4.T1.46.38.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2023</th>
<td id="S4.T1.46.38.3" class="ltx_td ltx_align_left" style="background-color:#707070;"><span id="S4.T1.46.38.3.1" class="ltx_text" style="background-color:#707070;">.684</span></td>
<td id="S4.T1.46.38.4" class="ltx_td ltx_align_left" style="background-color:#9C9C9C;"><span id="S4.T1.46.38.4.1" class="ltx_text" style="background-color:#9C9C9C;">.380</span></td>
</tr>
<tr id="S4.T1.47.39" class="ltx_tr">
<th id="S4.T1.47.39.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.47.39.1.1" class="ltx_text ltx_font_smallcaps">Whisper<sup id="S4.T1.47.39.1.1.1" class="ltx_sup"><span id="S4.T1.47.39.1.1.1.1" class="ltx_text ltx_font_italic">b</span></sup></span></th>
<th id="S4.T1.47.39.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">2023</th>
<td id="S4.T1.47.39.3" class="ltx_td ltx_align_left" style="background-color:#6E6E6E;"><span id="S4.T1.47.39.3.1" class="ltx_text" style="background-color:#6E6E6E;">.686</span></td>
<td id="S4.T1.47.39.4" class="ltx_td ltx_align_left" style="background-color:#FFFFFF;"><span id="S4.T1.47.39.4.1" class="ltx_text" style="background-color:#FFFFFF;">.200</span></td>
</tr>
<tr id="S4.T1.47.46.7" class="ltx_tr">
<th id="S4.T1.47.46.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Late Fusion (All)</th>
<th id="S4.T1.47.46.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">-</th>
<td id="S4.T1.47.46.7.3" class="ltx_td ltx_align_left ltx_border_t">.676</td>
<td id="S4.T1.47.46.7.4" class="ltx_td ltx_align_left ltx_border_t">.346</td>
</tr>
<tr id="S4.T1.47.47.8" class="ltx_tr">
<th id="S4.T1.47.47.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Late Fusion (Top-5)</th>
<th id="S4.T1.47.47.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">-</th>
<td id="S4.T1.47.47.8.3" class="ltx_td ltx_align_left ltx_border_bb">.708</td>
<td id="S4.T1.47.47.8.4" class="ltx_td ltx_align_left ltx_border_bb">.434</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.10.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:90%;">
UAR results for best-performing architectures in the <em id="S4.T2.2.1.1" class="ltx_emph ltx_font_italic">tuning phase</em>.
Showing best results obtained after tuning hyperparameters (batch size, optimiser, learning rate) and keeping the best-performing combination on the official test set.
Also including <math id="S4.T2.2.1.m1.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S4.T2.2.1.m1.1b"><mrow id="S4.T2.2.1.m1.1.1" xref="S4.T2.2.1.m1.1.1.cmml"><mn id="S4.T2.2.1.m1.1.1.2" xref="S4.T2.2.1.m1.1.1.2.cmml">95</mn><mo id="S4.T2.2.1.m1.1.1.1" xref="S4.T2.2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><apply id="S4.T2.2.1.m1.1.1.cmml" xref="S4.T2.2.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.T2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">95\%</annotation></semantics></math> confidence intervals for our models computed with bootstrapping.
SOTA results taken from original works.
</span></figcaption>
<table id="S4.T2.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2" class="ltx_tr">
<th id="S4.T2.4.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mn id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><cn type="integer" id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">2</annotation></semantics></math>-class</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T2.4.2.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T2.4.2.2.m1.1a"><mn id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><cn type="integer" id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">5</annotation></semantics></math>-class</td>
</tr>
<tr id="S4.T2.7.6.1" class="ltx_tr">
<th id="S4.T2.7.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">2009 Baseline</th>
<td id="S4.T2.7.6.1.2" class="ltx_td ltx_align_center ltx_border_t">.677</td>
<td id="S4.T2.7.6.1.3" class="ltx_td ltx_align_center ltx_border_t">.382</td>
</tr>
<tr id="S4.T2.7.7.2" class="ltx_tr">
<th id="S4.T2.7.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2009 Winners</th>
<td id="S4.T2.7.7.2.2" class="ltx_td ltx_align_center">.703</td>
<td id="S4.T2.7.7.2.3" class="ltx_td ltx_align_center">.417</td>
</tr>
<tr id="S4.T2.7.8.3" class="ltx_tr">
<th id="S4.T2.7.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2009 Fusion</th>
<td id="S4.T2.7.8.3.2" class="ltx_td ltx_align_center">.712</td>
<td id="S4.T2.7.8.3.3" class="ltx_td ltx_align_center">.440</td>
</tr>
<tr id="S4.T2.7.9.4" class="ltx_tr">
<th id="S4.T2.7.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite></th>
<td id="S4.T2.7.9.4.2" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.7.9.4.3" class="ltx_td ltx_align_center">.454</td>
</tr>
<tr id="S4.T2.5.3" class="ltx_tr">
<th id="S4.T2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.5.3.1.1" class="ltx_text ltx_font_smallcaps">IS10<sup id="S4.T2.5.3.1.1.1" class="ltx_sup"><span id="S4.T2.5.3.1.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></th>
<td id="S4.T2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">.685 [.674 - .696]</td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">.394 [.377 - .411]</td>
</tr>
<tr id="S4.T2.7.10.5" class="ltx_tr">
<th id="S4.T2.7.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.7.10.5.1.1" class="ltx_text ltx_font_smallcaps">Resnet50</span></th>
<td id="S4.T2.7.10.5.2" class="ltx_td ltx_align_center">.690 [.680 - .701]</td>
<td id="S4.T2.7.10.5.3" class="ltx_td ltx_align_center">.423 [.405 - .441]</td>
</tr>
<tr id="S4.T2.7.11.6" class="ltx_tr">
<th id="S4.T2.7.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.7.11.6.1.1" class="ltx_text ltx_font_smallcaps">CNN14</span></th>
<td id="S4.T2.7.11.6.2" class="ltx_td ltx_align_center">.672 [.661 - .683]</td>
<td id="S4.T2.7.11.6.3" class="ltx_td ltx_align_center">.448 [.428 - .467]</td>
</tr>
<tr id="S4.T2.6.4" class="ltx_tr">
<th id="S4.T2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.6.4.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.T2.6.4.1.1.1" class="ltx_sup"><span id="S4.T2.6.4.1.1.1.1" class="ltx_text ltx_font_italic">e</span></sup></span></th>
<td id="S4.T2.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.4.2.1" class="ltx_text ltx_font_bold">.717 [.706 - .728]</span></td>
<td id="S4.T2.6.4.3" class="ltx_td ltx_align_center">.448 [.431 - .465]</td>
</tr>
<tr id="S4.T2.7.5" class="ltx_tr">
<th id="S4.T2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.7.5.1.1" class="ltx_text ltx_font_smallcaps">Whisper<sup id="S4.T2.7.5.1.1.1" class="ltx_sup"><span id="S4.T2.7.5.1.1.1.1" class="ltx_text ltx_font_italic">t</span></sup></span></th>
<td id="S4.T2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">.707 [.696 - .718]</td>
<td id="S4.T2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.7.5.3.1" class="ltx_text ltx_font_bold">.454 [.437 - .472]</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.4" class="ltx_p">Results for the <em id="S4.p1.4.1" class="ltx_emph ltx_font_italic">exploration phase</em> are presented in <a href="#S4.T1" title="In 4 Results &amp; Discussion ‣ INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
The best-performing model for the 2-class problem is <span id="S4.p1.4.2" class="ltx_text ltx_font_smallcaps">CNN14</span>, with an <span title="" class="ltx_glossaryref">unweighted average recall (UAR)</span> of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext=".692" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">.692</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="float" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">.692</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">.692</annotation></semantics></math>, whereas for the 5-class problem it is <span id="S4.p1.4.3" class="ltx_text ltx_font_smallcaps">ResNet50</span>, with a <span title="" class="ltx_glossaryref">UAR</span> of <math id="S4.p1.2.m2.1" class="ltx_Math" alttext=".428" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">.428</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="float" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">.428</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">.428</annotation></semantics></math>.
We further observe that some models have failed to converge and yield chance (or near-chance) performance – most likely caused by the choice of hyperparameters, which favour some models more than others.
Notably, most of these results are below the challenge winners (<span title="" class="ltx_glossaryref">UAR</span>: <math id="S4.p1.3.m3.1" class="ltx_Math" alttext=".703/.417" display="inline"><semantics id="S4.p1.3.m3.1a"><mrow id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><mn id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml">.703</mn><mo id="S4.p1.3.m3.1.1.1" xref="S4.p1.3.m3.1.1.1.cmml">/</mo><mn id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml">.417</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><divide id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1"></divide><cn type="float" id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2">.703</cn><cn type="float" id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3">.417</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">.703/.417</annotation></semantics></math>) and several are in the same `ballpark' as the original baseline (<span title="" class="ltx_glossaryref">UAR</span>: <math id="S4.p1.4.m4.1" class="ltx_Math" alttext=".677/.382" display="inline"><semantics id="S4.p1.4.m4.1a"><mrow id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mn id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">.677</mn><mo id="S4.p1.4.m4.1.1.1" xref="S4.p1.4.m4.1.1.1.cmml">/</mo><mn id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml">.382</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><divide id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1.1"></divide><cn type="float" id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">.677</cn><cn type="float" id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3">.382</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">.677/.382</annotation></semantics></math>).
As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>, a fusion of our top models (here we only take 5) improves performance.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">Turning to the <em id="S4.p2.4.3" class="ltx_emph ltx_font_italic">tuning phase</em>, <a href="#S4.T2" title="In 4 Results &amp; Discussion ‣ INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows the <em id="S4.p2.4.4" class="ltx_emph ltx_font_italic">best</em> performance for the top five models
from <a href="#S4.T1" title="In 4 Results &amp; Discussion ‣ INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> after optimising standard hyperparameters (optimiser, learning rate, batch size).
In this case, <span id="S4.p2.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.p2.1.1.1" class="ltx_sup"><span id="S4.p2.1.1.1.1" class="ltx_text ltx_font_italic">e</span></sup></span> yields the best performance for the 2-class problem (<math id="S4.p2.2.m1.1" class="ltx_Math" alttext=".717" display="inline"><semantics id="S4.p2.2.m1.1a"><mn id="S4.p2.2.m1.1.1" xref="S4.p2.2.m1.1.1.cmml">.717</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m1.1b"><cn type="float" id="S4.p2.2.m1.1.1.cmml" xref="S4.p2.2.m1.1.1">.717</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m1.1c">.717</annotation></semantics></math>) and <span id="S4.p2.3.2" class="ltx_text ltx_font_smallcaps">Whisper<sup id="S4.p2.3.2.1" class="ltx_sup"><span id="S4.p2.3.2.1.1" class="ltx_text ltx_font_italic">t</span></sup></span> for the 5-class problem (<math id="S4.p2.4.m2.1" class="ltx_Math" alttext=".454" display="inline"><semantics id="S4.p2.4.m2.1a"><mn id="S4.p2.4.m2.1.1" xref="S4.p2.4.m2.1.1.cmml">.454</mn><annotation-xml encoding="MathML-Content" id="S4.p2.4.m2.1b"><cn type="float" id="S4.p2.4.m2.1.1.cmml" xref="S4.p2.4.m2.1.1">.454</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m2.1c">.454</annotation></semantics></math>) – in both cases, we reach results better than the challenge winners, albeit with the gains for the 2-class problem being marginal.
Given that <span id="S4.p2.4.5" class="ltx_text ltx_font_smallcaps">Whisper</span> has been trained for multilingual <span title="" class="ltx_glossaryref">ASR</span> (including German), and that previous performance improvements on valence prediction for English speech heavily depended on implicit linguistic knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>, we expect <span id="S4.p2.4.6" class="ltx_text ltx_font_smallcaps">Whisper</span>'s success to be also attributed to that aspect.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">However, it is still the case that all models we have tested remain close to or even below the original challenge baseline and winners, and especially the fusion of the top challenge submissions.
This remains so even after selecting only the best-performing model out of all tested hyperparameters, essentially following a generally bad practice of overfitting.
This was done intentionally to gauge performance under the most optimistic of settings – that of virtually unrestricted evaluation runs.
We note that the original challenge participants were given <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">25</annotation></semantics></math> runs each.
This shows
how the gains we obtain here
must be further tempered to account for more runs on our side.
</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2406.06401/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="207" height="207" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2406.06401/assets/x2.png" id="S4.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="207" height="207" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.7.3.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S4.F1.4.2" class="ltx_text" style="font-size:90%;">
Test set confusion matrices (in %) for the best-performing <span id="S4.F1.4.2.2" class="ltx_text ltx_font_smallcaps">CNN14</span> and <span id="S4.F1.3.1.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.F1.3.1.1.1" class="ltx_sup"><span id="S4.F1.3.1.1.1.1" class="ltx_text ltx_font_italic">e</span></sup></span> models on the <math id="S4.F1.4.2.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.F1.4.2.m1.1b"><mn id="S4.F1.4.2.m1.1.1" xref="S4.F1.4.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F1.4.2.m1.1c"><cn type="integer" id="S4.F1.4.2.m1.1.1.cmml" xref="S4.F1.4.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.4.2.m1.1d">5</annotation></semantics></math>-class problem. </span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.8" class="ltx_p"><span id="S4.p4.8.1" class="ltx_text ltx_font_bold">Do newer/larger models perform better?</span> Interestingly, when looking at the results of the exploration phase, there is no correlation of <span title="" class="ltx_glossaryref">UAR</span> performance with the year of publication (Spearman's <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\rho=.12/.09" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">ρ</mi><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mn id="S4.p4.1.m1.1.1.3.2" xref="S4.p4.1.m1.1.1.3.2.cmml">.12</mn><mo id="S4.p4.1.m1.1.1.3.1" xref="S4.p4.1.m1.1.1.3.1.cmml">/</mo><mn id="S4.p4.1.m1.1.1.3.3" xref="S4.p4.1.m1.1.1.3.3.cmml">.09</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><eq id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></eq><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">𝜌</ci><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><divide id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3.1"></divide><cn type="float" id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3.2">.12</cn><cn type="float" id="S4.p4.1.m1.1.1.3.3.cmml" xref="S4.p4.1.m1.1.1.3.3">.09</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\rho=.12/.09</annotation></semantics></math> for the <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="2-" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mn id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">2</mn><mo id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1">limit-from</csymbol><cn type="integer" id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">2</cn><minus id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">2-</annotation></semantics></math>/<math id="S4.p4.3.m3.1" class="ltx_Math" alttext="5-" display="inline"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mn id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml">5</mn><mo id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1">limit-from</csymbol><cn type="integer" id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">5</cn><minus id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">5-</annotation></semantics></math>class problem), and the correlation with the amount of <span title="" class="ltx_glossaryref">multiply-addition counts (MACs)</span> and trainable parameters is also very low (<math id="S4.p4.4.m4.1" class="ltx_Math" alttext="\rho=.15/.23" display="inline"><semantics id="S4.p4.4.m4.1a"><mrow id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">ρ</mi><mo id="S4.p4.4.m4.1.1.1" xref="S4.p4.4.m4.1.1.1.cmml">=</mo><mrow id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml"><mn id="S4.p4.4.m4.1.1.3.2" xref="S4.p4.4.m4.1.1.3.2.cmml">.15</mn><mo id="S4.p4.4.m4.1.1.3.1" xref="S4.p4.4.m4.1.1.3.1.cmml">/</mo><mn id="S4.p4.4.m4.1.1.3.3" xref="S4.p4.4.m4.1.1.3.3.cmml">.23</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><eq id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1.1"></eq><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">𝜌</ci><apply id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3"><divide id="S4.p4.4.m4.1.1.3.1.cmml" xref="S4.p4.4.m4.1.1.3.1"></divide><cn type="float" id="S4.p4.4.m4.1.1.3.2.cmml" xref="S4.p4.4.m4.1.1.3.2">.15</cn><cn type="float" id="S4.p4.4.m4.1.1.3.3.cmml" xref="S4.p4.4.m4.1.1.3.3">.23</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">\rho=.15/.23</annotation></semantics></math> and <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="\rho=-.08/.09" display="inline"><semantics id="S4.p4.5.m5.1a"><mrow id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml"><mi id="S4.p4.5.m5.1.1.2" xref="S4.p4.5.m5.1.1.2.cmml">ρ</mi><mo id="S4.p4.5.m5.1.1.1" xref="S4.p4.5.m5.1.1.1.cmml">=</mo><mrow id="S4.p4.5.m5.1.1.3" xref="S4.p4.5.m5.1.1.3.cmml"><mo id="S4.p4.5.m5.1.1.3a" xref="S4.p4.5.m5.1.1.3.cmml">−</mo><mrow id="S4.p4.5.m5.1.1.3.2" xref="S4.p4.5.m5.1.1.3.2.cmml"><mn id="S4.p4.5.m5.1.1.3.2.2" xref="S4.p4.5.m5.1.1.3.2.2.cmml">.08</mn><mo id="S4.p4.5.m5.1.1.3.2.1" xref="S4.p4.5.m5.1.1.3.2.1.cmml">/</mo><mn id="S4.p4.5.m5.1.1.3.2.3" xref="S4.p4.5.m5.1.1.3.2.3.cmml">.09</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><apply id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1"><eq id="S4.p4.5.m5.1.1.1.cmml" xref="S4.p4.5.m5.1.1.1"></eq><ci id="S4.p4.5.m5.1.1.2.cmml" xref="S4.p4.5.m5.1.1.2">𝜌</ci><apply id="S4.p4.5.m5.1.1.3.cmml" xref="S4.p4.5.m5.1.1.3"><minus id="S4.p4.5.m5.1.1.3.1.cmml" xref="S4.p4.5.m5.1.1.3"></minus><apply id="S4.p4.5.m5.1.1.3.2.cmml" xref="S4.p4.5.m5.1.1.3.2"><divide id="S4.p4.5.m5.1.1.3.2.1.cmml" xref="S4.p4.5.m5.1.1.3.2.1"></divide><cn type="float" id="S4.p4.5.m5.1.1.3.2.2.cmml" xref="S4.p4.5.m5.1.1.3.2.2">.08</cn><cn type="float" id="S4.p4.5.m5.1.1.3.2.3.cmml" xref="S4.p4.5.m5.1.1.3.2.3">.09</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">\rho=-.08/.09</annotation></semantics></math>) – note that model <span title="" class="ltx_glossaryref">MACs</span> and parameters do not account for feature extraction.
This further illustrates how neither more recent nor more complex models are able to surpass the prior state-of-the-art.
Finally, the ranking of model performance between the <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p4.6.m6.1a"><mn id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><cn type="integer" id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">2</annotation></semantics></math>- and <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p4.7.m7.1a"><mn id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><cn type="integer" id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">5</annotation></semantics></math>-class problems is moderate (<math id="S4.p4.8.m8.1" class="ltx_Math" alttext="\rho=.47" display="inline"><semantics id="S4.p4.8.m8.1a"><mrow id="S4.p4.8.m8.1.1" xref="S4.p4.8.m8.1.1.cmml"><mi id="S4.p4.8.m8.1.1.2" xref="S4.p4.8.m8.1.1.2.cmml">ρ</mi><mo id="S4.p4.8.m8.1.1.1" xref="S4.p4.8.m8.1.1.1.cmml">=</mo><mn id="S4.p4.8.m8.1.1.3" xref="S4.p4.8.m8.1.1.3.cmml">.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.8.m8.1b"><apply id="S4.p4.8.m8.1.1.cmml" xref="S4.p4.8.m8.1.1"><eq id="S4.p4.8.m8.1.1.1.cmml" xref="S4.p4.8.m8.1.1.1"></eq><ci id="S4.p4.8.m8.1.1.2.cmml" xref="S4.p4.8.m8.1.1.2">𝜌</ci><cn type="float" id="S4.p4.8.m8.1.1.3.cmml" xref="S4.p4.8.m8.1.1.3">.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">\rho=.47</annotation></semantics></math>); this shows that models are not consistently good when given the same data but different labels (i. e., our findings are consistent with the standard ``no free-lunch'' theorem).
Surprisingly, some models even show near chance-level performance on one task while performing well on the other.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.7" class="ltx_p"><span id="S4.p5.7.2" class="ltx_text ltx_font_bold">Agreement between different models:</span>
Different models agree with one another to a moderate or good extent.
The average pairwise agreement (percentage of instances where two models agree) for all models of the exploration phase is <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mn id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">70</mn><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">70\%</annotation></semantics></math> and <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="55\%" display="inline"><semantics id="S4.p5.2.m2.1a"><mrow id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mn id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">55</mn><mo id="S4.p5.2.m2.1.1.1" xref="S4.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">55\%</annotation></semantics></math> for the <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p5.3.m3.1a"><mn id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><cn type="integer" id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">2</annotation></semantics></math>- and <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p5.4.m4.1a"><mn id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><cn type="integer" id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">5</annotation></semantics></math>-class models, which rises to <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S4.p5.5.m5.1a"><mrow id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml"><mn id="S4.p5.5.m5.1.1.2" xref="S4.p5.5.m5.1.1.2.cmml">80</mn><mo id="S4.p5.5.m5.1.1.1" xref="S4.p5.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><apply id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1"><csymbol cd="latexml" id="S4.p5.5.m5.1.1.1.cmml" xref="S4.p5.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.p5.5.m5.1.1.2.cmml" xref="S4.p5.5.m5.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">80\%</annotation></semantics></math> and <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="57\%" display="inline"><semantics id="S4.p5.6.m6.1a"><mrow id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml"><mn id="S4.p5.6.m6.1.1.2" xref="S4.p5.6.m6.1.1.2.cmml">57</mn><mo id="S4.p5.6.m6.1.1.1" xref="S4.p5.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><apply id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1"><csymbol cd="latexml" id="S4.p5.6.m6.1.1.1.cmml" xref="S4.p5.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.p5.6.m6.1.1.2.cmml" xref="S4.p5.6.m6.1.1.2">57</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">57\%</annotation></semantics></math>, respectively, when considering only the top-5 ones.
Additionally, this is exemplified by considering the confusion matrices
in <a href="#S4.F1" title="In 4 Results &amp; Discussion ‣ INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>
of the best-performing <span id="S4.p5.7.3" class="ltx_text ltx_font_smallcaps">CNN14</span> and <span id="S4.p5.7.1" class="ltx_text ltx_font_smallcaps">W2V2<sup id="S4.p5.7.1.1" class="ltx_sup"><span id="S4.p5.7.1.1.1" class="ltx_text ltx_font_italic">e</span></sup></span> from the tuning phase – even though they result in an almost identical <span title="" class="ltx_glossaryref">UAR</span>, their behaviour on the test set is not very similar.
For example, <span id="S4.p5.7.4" class="ltx_text ltx_font_smallcaps">CNN14</span> shows a higher recall for the angry and emphatic classes, to the detriment of more neutral samples misclassified as such.
Overall, this demonstrates that models trained on similar data do not converge to an identical solution – a finding congruent with the literature on underspecification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Model vs human performance:</span>
We also investigate whether samples that are harder to classify for humans are also harder for models.
The standard FAU-AIBO release comes with annotator confidences per instance, computed by taking the percentage of annotators who agree with the gold standard;
we thus define `difficulty` as <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.p6.1.m1.1a"><mn id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><cn type="integer" id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">1</annotation></semantics></math> minus that confidence.
We then make the following observations when considering all models of the tuning phase:</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.5" class="ltx_p">a) We first adopt a model-agnostic measure of difficulty, which we define as the number of models who disagree with the max-vote computed by all models on each instance – this is akin to the computation of difficulty for the human annotators.
Spearman's <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.p7.1.m1.1a"><mi id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><ci id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\rho</annotation></semantics></math> between this measure and annotator disagreement is moderate (<math id="S4.p7.2.m2.1" class="ltx_Math" alttext=".33" display="inline"><semantics id="S4.p7.2.m2.1a"><mn id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">.33</mn><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><cn type="float" id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">.33</annotation></semantics></math> and <math id="S4.p7.3.m3.1" class="ltx_Math" alttext=".20" display="inline"><semantics id="S4.p7.3.m3.1a"><mn id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">.20</mn><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><cn type="float" id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">.20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">.20</annotation></semantics></math> for the <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p7.4.m4.1a"><mn id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><cn type="integer" id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">2</annotation></semantics></math>- and <math id="S4.p7.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p7.5.m5.1a"><mn id="S4.p7.5.m5.1.1" xref="S4.p7.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p7.5.m5.1b"><cn type="integer" id="S4.p7.5.m5.1.1.cmml" xref="S4.p7.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.5.m5.1c">5</annotation></semantics></math>-class problems).</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.5" class="ltx_p">b) We then adopt a model-specific measure of difficulty, defined as the cross-entropy loss for each instance, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>.
Different models have different rankings of instance difficulty, with average pairwise <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.p8.1.m1.1a"><mi id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><ci id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">\rho</annotation></semantics></math> being <math id="S4.p8.2.m2.1" class="ltx_Math" alttext=".51" display="inline"><semantics id="S4.p8.2.m2.1a"><mn id="S4.p8.2.m2.1.1" xref="S4.p8.2.m2.1.1.cmml">.51</mn><annotation-xml encoding="MathML-Content" id="S4.p8.2.m2.1b"><cn type="float" id="S4.p8.2.m2.1.1.cmml" xref="S4.p8.2.m2.1.1">.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.2.m2.1c">.51</annotation></semantics></math> and <math id="S4.p8.3.m3.1" class="ltx_Math" alttext=".33" display="inline"><semantics id="S4.p8.3.m3.1a"><mn id="S4.p8.3.m3.1.1" xref="S4.p8.3.m3.1.1.cmml">.33</mn><annotation-xml encoding="MathML-Content" id="S4.p8.3.m3.1b"><cn type="float" id="S4.p8.3.m3.1.1.cmml" xref="S4.p8.3.m3.1.1">.33</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.3.m3.1c">.33</annotation></semantics></math> for the <math id="S4.p8.4.m4.1" class="ltx_Math" alttext="2-" display="inline"><semantics id="S4.p8.4.m4.1a"><mrow id="S4.p8.4.m4.1.1" xref="S4.p8.4.m4.1.1.cmml"><mn id="S4.p8.4.m4.1.1.2" xref="S4.p8.4.m4.1.1.2.cmml">2</mn><mo id="S4.p8.4.m4.1.1.3" xref="S4.p8.4.m4.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.4.m4.1b"><apply id="S4.p8.4.m4.1.1.cmml" xref="S4.p8.4.m4.1.1"><csymbol cd="latexml" id="S4.p8.4.m4.1.1.1.cmml" xref="S4.p8.4.m4.1.1">limit-from</csymbol><cn type="integer" id="S4.p8.4.m4.1.1.2.cmml" xref="S4.p8.4.m4.1.1.2">2</cn><minus id="S4.p8.4.m4.1.1.3.cmml" xref="S4.p8.4.m4.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.4.m4.1c">2-</annotation></semantics></math> and <math id="S4.p8.5.m5.1" class="ltx_Math" alttext="5-" display="inline"><semantics id="S4.p8.5.m5.1a"><mrow id="S4.p8.5.m5.1.1" xref="S4.p8.5.m5.1.1.cmml"><mn id="S4.p8.5.m5.1.1.2" xref="S4.p8.5.m5.1.1.2.cmml">5</mn><mo id="S4.p8.5.m5.1.1.3" xref="S4.p8.5.m5.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.5.m5.1b"><apply id="S4.p8.5.m5.1.1.cmml" xref="S4.p8.5.m5.1.1"><csymbol cd="latexml" id="S4.p8.5.m5.1.1.1.cmml" xref="S4.p8.5.m5.1.1">limit-from</csymbol><cn type="integer" id="S4.p8.5.m5.1.1.2.cmml" xref="S4.p8.5.m5.1.1.2">5</cn><minus id="S4.p8.5.m5.1.1.3.cmml" xref="S4.p8.5.m5.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.5.m5.1c">5-</annotation></semantics></math>class problems.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.6" class="ltx_p">c) Finally, we compute the Spearman <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.p9.1.m1.1a"><mi id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><ci id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">\rho</annotation></semantics></math> between each model's <span title="" class="ltx_glossaryref">UAR</span> and its Spearman <math id="S4.p9.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.p9.2.m2.1a"><mi id="S4.p9.2.m2.1.1" xref="S4.p9.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.p9.2.m2.1b"><ci id="S4.p9.2.m2.1.1.cmml" xref="S4.p9.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.2.m2.1c">\rho</annotation></semantics></math> with annotator disagreement;
here, <math id="S4.p9.3.m3.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.p9.3.m3.1a"><mi id="S4.p9.3.m3.1.1" xref="S4.p9.3.m3.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S4.p9.3.m3.1b"><ci id="S4.p9.3.m3.1.1.cmml" xref="S4.p9.3.m3.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.3.m3.1c">\rho</annotation></semantics></math> is <math id="S4.p9.4.m4.1" class="ltx_math_unparsed" alttext="-.38/-.07" display="inline"><semantics id="S4.p9.4.m4.1a"><mrow id="S4.p9.4.m4.1b"><mo id="S4.p9.4.m4.1.1">−</mo><mn id="S4.p9.4.m4.1.2">.38</mn><mo rspace="0em" id="S4.p9.4.m4.1.3">/</mo><mo lspace="0em" id="S4.p9.4.m4.1.4">−</mo><mn id="S4.p9.4.m4.1.5">.07</mn></mrow><annotation encoding="application/x-tex" id="S4.p9.4.m4.1c">-.38/-.07</annotation></semantics></math> for the <math id="S4.p9.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p9.5.m5.1a"><mn id="S4.p9.5.m5.1.1" xref="S4.p9.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p9.5.m5.1b"><cn type="integer" id="S4.p9.5.m5.1.1.cmml" xref="S4.p9.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.5.m5.1c">2</annotation></semantics></math>/<math id="S4.p9.6.m6.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.p9.6.m6.1a"><mn id="S4.p9.6.m6.1.1" xref="S4.p9.6.m6.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.p9.6.m6.1b"><cn type="integer" id="S4.p9.6.m6.1.1.cmml" xref="S4.p9.6.m6.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.6.m6.1c">5</annotation></semantics></math>-class problem, indicating that larger agreement with human annotators does not lead to better performance (rather, the opposite).
Collectively, our results indicate that models appear to learn differently than humans, with small agreement to what constitutes an easy or hard example.</p>
</div>
<div id="S4.p10" class="ltx_para ltx_noindent">
<p id="S4.p10.1" class="ltx_p"><span id="S4.p10.1.1" class="ltx_text ltx_font_bold">Limitations:</span>
Our study is obviously limited with respect to the approaches we tried; with hundreds of papers published on <span title="" class="ltx_glossaryref">SER</span> on a yearly basis, it was impossible to evaluate all of them.
We thus opted for the simplest ones: fine-tuning large <span title="" class="ltx_glossaryref">DNNs</span> previously shown to be successful on other datasets using a range of standard hyperparameters.
Furthermore, we have focused exclusively on one dataset; we intend to explore whether these findings generalise to other datasets in a follow-up work.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have conducted a large-scale study of several modern <span title="" class="ltx_glossaryref">DNN</span> architectures – most of them pre-trained on large datasets – on the data of the INTERSPEECH 2009 Emotion Challenge. Given standard parameters, we were only able to marginally outperform the state-of-the-art achieved by challenge participants, with several models scoring even below the baseline, and some failing to converge altogether.
Further optimising hyperparameters led us to outperform the challenge winners by small margins.
Our subsequent analysis showed that performance improvements have not been consistent over time and are not caused by increased model size.
Moreover, we have found that different models converge to different (sometimes complementary) solutions while differing in how challenging they find individual instances compared to human annotators.
Collectively, our findings suggest that recent success achieved by <span title="" class="ltx_glossaryref">DNN</span> models must be tempered, at least when considering the FAU-AIBO setup.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work was funded from the DFG’s Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS) and the EU H2020 project No. 101135556 (INDUX-R), the Munich Data Science Institute (MDSI), and the Munich Center for Machine Learning (MCML). We would also like to remember our colleague Stefan Steidl – dedicated co-organiser of the ComParE challenges – who unexpectedly passed away at young age in October 2018.
</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Björn Schuller, Stefan Steidl and Anton Batliner
</span>
<span class="ltx_bibblock">``The Interspeech 2009 Emotion Challenge''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2009, pp. 312–315
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Björn Schuller, Michel Valstar, Florian Eyben, Gary McKeown, Roddy Cowie and Maja Pantic
</span>
<span class="ltx_bibblock">``AVEC 2011–the first international audio/visual emotion challenge''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proc. ACII</em>, 2011, pp. 415–424
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Lukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis, Xinchen Du, Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Björn W Schuller and Iulia Lefter
</span>
<span class="ltx_bibblock">``Muse 2020 challenge and workshop: Multimodal sentiment analysis, emotion-target engagement and trustworthiness detection in real-life media: Emotional car reviews in-the-wild''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop</em>, 2020, pp. 35–44
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Pablo Barros, Nikhil Churamani, Egor Lakomkin, Henrique Siqueira, Alexander Sutherland and Stefan Wermter
</span>
<span class="ltx_bibblock">``The OMG-emotion behavior dataset''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">2018 International Joint Conference on Neural Networks (IJCNN)</em>, 2018, pp. 1–7
</span>
<span class="ltx_bibblock">IEEE
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee and Shrikanth S Narayanan
</span>
<span class="ltx_bibblock">``IEMOCAP: Interactive emotional dyadic motion capture database''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">42</span>
</span>
<span class="ltx_bibblock">Springer, 2008, pp. 335–359
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Reza Lotfian and Carlos Busso
</span>
<span class="ltx_bibblock">``Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">10.4</span>
</span>
<span class="ltx_bibblock">IEEE, 2017, pp. 471–483
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">A. Batliner, C. Hacker, S. Steidl, E. Nöth, S. D'Arcy, M. Russell and M. Wong
</span>
<span class="ltx_bibblock">````You stupid tin box'' - children interacting with the AIBO robot: A cross-linguistic emotional speech corpus''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Proc. LREC</em>, 2004, pp. 171–174
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> S.
</span>
<span class="ltx_bibblock">``Automatic Classification of Emotion-Related User States in Spontaneous Children's Speech'' (PhD thesis, FAU Erlangen-Nuremberg)
</span>
<span class="ltx_bibblock">Berlin: Logos Verlag, 2009
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Nicholas Cummins, Shahin Amiriparian, Gerhard Hagerer, Anton Batliner, Stefan Steidl and Björn Schuller
</span>
<span class="ltx_bibblock">``An Image-based Deep Spectrum Feature Representation for the Recognition of Emotional Speech''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Proc. ACM MM</em>, 2017, pp. 478–484
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Haytham M Fayek, Margaret Lech and Lawrence Cavedon
</span>
<span class="ltx_bibblock">``Evaluating deep learning architectures for speech emotion recognition''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em> <span id="bib.bibx10.2.2" class="ltx_text ltx_font_bold">92</span>
</span>
<span class="ltx_bibblock">Elsevier, 2017, pp. 60–68
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Ziping Zhao, Zhongtian Bao, Yiqin Zhao, Zixing Zhang, Nicholas Cummins, Zhao Ren and Björn Schuller
</span>
<span class="ltx_bibblock">``Exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em> <span id="bib.bibx11.2.2" class="ltx_text ltx_font_bold">7</span>
</span>
<span class="ltx_bibblock">IEEE, 2019, pp. 97515–97525
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Johannes Wagner, Andreas Triantafyllopoulos, Hagen Wierstorf, Maximilian Schmitt, Felix Burkhardt, Florian Eyben and Björn W Schuller
</span>
<span class="ltx_bibblock">``Dawn of the transformer era in speech emotion recognition: closing the valence gap''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> <span id="bib.bibx12.2.2" class="ltx_text ltx_font_bold">45.09</span>
</span>
<span class="ltx_bibblock">IEEE, 2023, pp. 10745–10759
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Pierre Dumouchel, Najim Dehak, Yazid Attabi, Reda Dehak and Narjes Boufaden
</span>
<span class="ltx_bibblock">``Cepstral and long-term features for emotion recognition''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2009, pp. 344–347
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Chi-Chun Lee, Emily Mower, Carlos Busso, Sungbok Lee and Shrikanth Narayanan
</span>
<span class="ltx_bibblock">``Emotion recognition using a hierarchical binary decision tree approach''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Speech Communication</em> <span id="bib.bibx14.2.2" class="ltx_text ltx_font_bold">53.9-10</span>
</span>
<span class="ltx_bibblock">Elsevier, 2011, pp. 1162–1171
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Marcel Kockmann, Lukás Burget and Jan Cernockỳ
</span>
<span class="ltx_bibblock">``Brno University of Technology system for Interspeech 2009 emotion challenge.''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2009, pp. 348–351
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Björn Schuller, Anton Batliner, Stefan Steidl and Dino Seppi
</span>
<span class="ltx_bibblock">``Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Speech Communication</em> <span id="bib.bibx16.2.2" class="ltx_text ltx_font_bold">53.9-10</span>
</span>
<span class="ltx_bibblock">Elsevier, 2011, pp. 1062–1087
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Andreas Triantafyllopoulos, Shuo Liu and Björn W Schuller
</span>
<span class="ltx_bibblock">``Deep speaker conditioning for speech emotion recognition''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Proc. ICME</em>, 2021, pp. 1–6
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Florian Eyben, Klaus R Scherer, Björn W Schuller, Johan Sundberg, Elisabeth André, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka and Shrikanth S Narayanan
</span>
<span class="ltx_bibblock">``The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em> <span id="bib.bibx18.2.2" class="ltx_text ltx_font_bold">7.2</span>
</span>
<span class="ltx_bibblock">IEEE, 2015, pp. 190–202
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Florian Eyben, Martin Wöllmer and Björn Schuller
</span>
<span class="ltx_bibblock">``openSMILE: the Munich versatile and fast open-source audio feature extractor''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Proc. ACM MM</em>, 2010, pp. 1459–1462
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Yuan Gong, Yu-An Chung and James Glass
</span>
<span class="ltx_bibblock">``AST: Audio Spectrogram Transformer''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2021, pp. 571–575
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Panagiotis Tzirakis, Jiehao Zhang and Bjorn W Schuller
</span>
<span class="ltx_bibblock">``End-to-end speech emotion recognition using deep neural networks''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2018, pp. 5089–5093
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Jianfeng Zhao, Xia Mao and Lijiang Chen
</span>
<span class="ltx_bibblock">``Speech emotion recognition using deep 1D &amp; 2D CNN LSTM networks''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Biomedical signal processing and control</em> <span id="bib.bibx22.2.2" class="ltx_text ltx_font_bold">47</span>
</span>
<span class="ltx_bibblock">Elsevier, 2019, pp. 312–323
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Brecht Desplanques, Jenthe Thienpondt and Kris Demuynck
</span>
<span class="ltx_bibblock">``ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2020, pp. 3830–3834
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang and Mark D Plumbley
</span>
<span class="ltx_bibblock">``Panns: Large-scale pretrained audio neural networks for audio pattern recognition''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> <span id="bib.bibx24.2.2" class="ltx_text ltx_font_bold">28</span>
</span>
<span class="ltx_bibblock">IEEE, 2020, pp. 2880–2894
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Andreas Triantafyllopoulos and Björn W Schuller
</span>
<span class="ltx_bibblock">``The role of task and acoustic similarity in audio transfer learning: Insights from the speech emotion recognition case''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2021, pp. 7268–7272
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey and Ilya Sutskever
</span>
<span class="ltx_bibblock">``Robust speech recognition via large-scale weak supervision''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023, pp. 28492–28518
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed and Michael Auli
</span>
<span class="ltx_bibblock">``wav2vec 2.0: A framework for self-supervised learning of speech representations''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em> <span id="bib.bibx27.2.2" class="ltx_text ltx_font_bold">33</span>, 2020, pp. 12449–12460
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov and Abdelrahman Mohamed
</span>
<span class="ltx_bibblock">``Hubert: Self-supervised speech representation learning by masked prediction of hidden units''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> <span id="bib.bibx28.2.2" class="ltx_text ltx_font_bold">29</span>
</span>
<span class="ltx_bibblock">IEEE, 2021, pp. 3451–3460
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Andreas Triantafyllopoulos, Johannes Wagner, Hagen Wierstorf, Maximilian Schmitt, Uwe Reichel, Florian Eyben, Felix Burkhardt and Björn W Schuller
</span>
<span class="ltx_bibblock">``Probing speech emotion recognition transformers for linguistic knowledge''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2022, pp. 146–150
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino and Emmanuel Dupoux
</span>
<span class="ltx_bibblock">``VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">Proc. ACL</em>, 2021, pp. 993–1003
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve and Michael Auli
</span>
<span class="ltx_bibblock">``Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH</em>, 2021, pp. 721–725
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein and Matthew D Hoffman
</span>
<span class="ltx_bibblock">``Underspecification presents challenges for credibility in modern machine learning''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em> <span id="bib.bibx32.2.2" class="ltx_text ltx_font_bold">23.1</span>
</span>
<span class="ltx_bibblock">JMLRORG, 2022, pp. 10237–10297
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Guy Hacohen and Daphna Weinshall
</span>
<span class="ltx_bibblock">``On the power of curriculum learning in training deep networks''
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2019, pp. 2535–2544
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.06400" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.06401" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.06401">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.06401" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.06402" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:57:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
