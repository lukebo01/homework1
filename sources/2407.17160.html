<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.17160] A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives</title><meta property="og:description" content="In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.17160">

<!--Generated on Mon Aug  5 15:47:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">JanLehečka
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>Josef V.Psutka
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>LubošŠmídl
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>PavelIrcing
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>JosefPsutka




</p>
</div>
<h1 class="ltx_title ltx_title_document">A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-language sentences.
Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage.
Our results suggest that monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers.
We also performed the same experiments on the public CommonVoice dataset to verify our results.
We are contributing to the research community by releasing our pre-trained models to the public.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech recognition, bilingual models, trilingual models, oral history archives
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">One of the most valuable possessions mankind has is the heritage from previous generations in the form of historical documents and recordings. This heritage preserves the memory of humanity which should not be forgotten.
Since collections of historical documents and recordings can grow huge in size, one of the key challenges of our age is to preserve and curate this heritage and make it as accessible and searchable to the public and researchers as possible in order to enable advanced analyzing, studying, and learning of new lessons from our history.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we focus mainly on one very important and unique oral history archive: MALACH. It is an audiovisual archive initially collected in the 1990s to preserve the authentic memories of Holocaust survivors. This archive stores vast and extremely valuable testimonies from our recent history recorded through audiovisual interviews.
Today, these interviews are stored in the Visual History Archive (VHA) at the Shoah Foundation Institute (SFI) at the University of Southern California (USC)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://vha.usc.edu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vha.usc.edu</a></span></span></span>, along with other interviews with witnesses to the history of the entire 20th century (more than 56k interviews). The Holocaust part of the archive contains testimonies in 32 languages of the personal memories of people who survived the World War II Holocaust. In this paper, we denote this archive and derived datasets as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">MALACH</span> (Multilingual Access to Large Spoken Archives) after the name of the first project that began in 2001 and laid the foundations for work on this unique archive.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://malach.umiacs.umd.edu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://malach.umiacs.umd.edu</a></span></span></span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The natural way how to process large oral history archives to make the content more accessible is to transcribe the speech using an Automatic Speech Recognition (ASR) system.
Although ASR systems have improved rapidly in the last years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, automatically transcribing interviews from the MALACH is still a challenge since the interviews contain spontaneous speech full of disfluencies, emotional excitements, mixed-language sentences, and heavy accents, and are often influenced by the high age of speakers (the average age of all speakers at the time of recording was about 75 years) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The large number of mixed-language sentences, mainly the German phrases present in interviews across all other languages<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>E.g. a sentence from an English interview: <span id="footnote3.1" class="ltx_text ltx_font_italic">“I said yes, Herr Lagerführer, ich habe gehalten Hemdentashen, so he give me two more.”</span></span></span></span>, together with the natural multilingualism of this archive (32 languages with many non-native speakers) motivated us to study multilingual aspects of this archive and try to answer some interesting questions:
Would for example adding some German speech data into the training process of the English ASR model solve the problem of transcribing mixed-language sentences?
More generally, would the bilingual pre-trained models be more suitable for this task than monolingual models? And how about trilingual models – could they be even more suitable? Ultimately, would a large-scale massively multilingual ASR model transcribe the interviews better than a set of per-language monolingual ASR models?
How much will results from these approaches differ?</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To answer these questions, in this work, we present a comparative analysis over a large set of experiments with different language combinations in both pre-training and fine-tuning of ASR systems based on Wav2Vec 2.0 models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> while simplifying the problem only to 3 languages: English, German, and Czech.
Since some of the MALACH datasets are not publically available, we fine-tuned, evaluated, and compared our models also on another well-known and publicly available dataset – CommonVoice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> – to see if our findings are applicable also to other multilingual datasets than oral history archives.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The original MALACH project took place between 2001 and 2006.
The WER of the ASR systems developed within the project reached 39.40% for English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and 38.57% for Czech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> by the end of the project in 2006. German part was not processed with ASR at that time. Even after the project finished, the efficiency of the ASR systems has continuously improved using new approaches, so in 2011 the WER of 27.11% was achieved for Czech recordings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. New training methods based on DNN brought further improvement of WER (21.70% for English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and 19.11% for Czech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>). The best WER without using end-to-end approaches reached 17.85% for English and 14.65% for Czech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
After the introduction of end-to-end Transformer-based audio models, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> reported a significant improvement for the Czech dataset (WER=10.48%) and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for English (WER=13.5%). The last state-of-the-art results we are aware of are 12.88% for English, 17.08% for German, and 8.43% for Czech reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">All recent works on MALACH datasets (and a majority of other ASR datasets) use either monolingual models or large massively multilingual models (pre-trained on 100+ languages). However, no attention has been paid to bilingual and trilingual ASR models in the literature so far.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Wav2Vec 2.0 model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">After the introduction of the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, a new era of AI began. Not long after that, Transformer-based models also established a new paradigm in the task of automatic speech recognition by introducing Wav2Vec 2.0<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For brevity, we will omit the version number in the following text and denote this model only as <span id="footnote4.1" class="ltx_text ltx_font_typewriter">Wav2Vec</span>.</span></span></span> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
It is a Transformer encoder pre-trained to reconstruct the corrupted signals.
The raw audio signal input is processed by the model into a sequence of frame-level <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">contextualized speech representations</span> encoding individual audio frames within their context.
The training of Wav2Vec ASR models typically consists of two main phases: self-supervised pre-training and supervised fine-tuning. When fine-tuning, the model is supplemented with the final Connectionist Temporal Classification (CTC) layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, in which the most probable sequence of text tokens (i.e., the predicted transcription) is decoded.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Selected languages</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To simplify the experiments, we chose 3 languages well-represented in both fine-tuning datasets we were working with (see Sec. <a href="#S5" title="5 Fine-tuning datasets ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>): English, German, and Czech. They all belong to the Indo-European language family, Czech belongs to the Balto-Slavic branch while English and German are Germanic languages with higher mutual lexical similarity. We chose these languages for several reasons: (1) German is a language whose phrases intertwine throughout the MALACH archive, so it should be included; (2) English is the most represented and studied part of both datasets; and (3) Czech is well-represented in both datasets while it is an example of language from a completely different language branch. This language selection allows us to test various language combinations and see whether lexical similarity will be somehow reflected in results from multilingual models. We didn't include more languages to keep the number of possible language combinations reasonably small for experimenting and the results interpretable.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Pre-trained models</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We started from the Wav2Vec-base English model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> pre-trained from 50k hours from Libri-light dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. We used the model as a base for English monolingual ASR models.
To ensure comparable results of our experiments, we adopted the exact same pre-training setup for all other models we pre-trained from scratch, and scaled pre-training data for the other two languages to the same amount. For Czech and German, we collected 50k hours of speech from public sources, mainly from the VoxPopuli dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and a mix of self-crawled publicly available podcasts and audiobooks.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2407.17160/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Scheme of pre-training mono-, bi- and trilingual Wav2Vec models.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The scheme of pre-training our models is depicted in Fig. <a href="#S4.F1" title="Figure 1 ‣ 4 Pre-trained models ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
First, we pre-trained German and Czech monolingual Wav2Vec-base models from 50k hours of speech data each. Then, we prepared a mixture of all pairs of languages to train bilingual models from 100k hours each. Finally, we combine data from all three languages to pre-train a trilingual model from 150k hours of the equally balanced trilingual dataset. We experimented with three monolingual, three bilingual, and one trilingual model. Except for the monolingual English model, we pre-trained all models from scratch using the exact same setting as used for the English model, i.e., we trained the base model for 400 thousand steps with a batch size of about 1.6 hours. We used <span id="S4.p2.1.1" class="ltx_text ltx_font_typewriter">Fairseq</span> tool<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/pytorch/fairseq" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pytorch/fairseq</a></span></span></span> for pre-training all models. Pre-training of each model took approx. five days on a machine with eight NVIDIA A100 GPUs.
We are releasing all newly pre-trained models publicly to the research community<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://huggingface.co/fav-kky" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/fav-kky</a></span></span></span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Large-scale multilingual models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To compare our Wav2Vec models also with large-scale multilingual models, we selected <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">Wav2Vec-XLS-R-300M</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">Whisper</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Wav2Vec-XLS-R-300M is a popular model pre-trained by Meta AI on 128 languages and approximately 436 thousand hours of unlabeled speech data. We experimented with the 300M variant, which has more than <math id="S4.SS1.p1.1.m1.1" class="ltx_math_unparsed" alttext="3\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1b"><mn id="S4.SS1.p1.1.m1.1.1">3</mn><mo lspace="0.222em" id="S4.SS1.p1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">3\times</annotation></semantics></math> more parameters than the Wav2Vec-base model.
Whisper is another popular model trained by OpenAI on 99 languages from 680,000 hours of multilingual and multitask labeled data. It is an encoder-decoder model already fine-tuned on multilingual ASR tasks by the authors, so it can also be used as a zero-shot speech recognizer without fine-tuning. We experimented with three sizes of the Whisper model: <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">base</span>, <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">small</span>, and <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_typewriter">large</span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Fine-tuning datasets</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We were experimenting with two multilingual datasets: CommonVoice and MALACH.
For both datasets and all selected languages, we cleaned all transcripts by removing non-speech events and punctuation and mapping texts into lowercase. The data statistics are shown in Tab. <a href="#S5.T1" title="Table 1 ‣ 5 Fine-tuning datasets ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Fine-tuning datasets. We show the number of hours, words in transcripts (in thousands), and the average length of train/dev/test segments of audio (in seconds). Note that these statistics are from balanced trilingual datasets we used in this paper; the full datasets contain more data.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding:0.5pt 2.0pt;"></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:0.5pt 2.0pt;"></th>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">English</td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_border_tt" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">German</td>
<td id="S5.T1.1.1.1.6" class="ltx_td ltx_border_tt" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 2.0pt;" colspan="3">Czech</td>
</tr>
<tr id="S5.T1.1.2.2" class="ltx_tr">
<th id="S5.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 2.0pt;"></th>
<th id="S5.T1.1.2.2.2" class="ltx_td ltx_th ltx_th_row" style="padding:0.5pt 2.0pt;"></th>
<td id="S5.T1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">train</td>
<td id="S5.T1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">dev</td>
<td id="S5.T1.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">test</td>
<td id="S5.T1.1.2.2.6" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.2.2.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">train</td>
<td id="S5.T1.1.2.2.8" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">dev</td>
<td id="S5.T1.1.2.2.9" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">test</td>
<td id="S5.T1.1.2.2.10" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.2.2.11" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">train</td>
<td id="S5.T1.1.2.2.12" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">dev</td>
<td id="S5.T1.1.2.2.13" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">test</td>
</tr>
<tr id="S5.T1.1.3.3" class="ltx_tr">
<th id="S5.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="3">
<span id="S5.T1.1.3.3.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S5.T1.1.3.3.1.1.1" class="ltx_p">
<span id="S5.T1.1.3.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:3.4pt;height:31.4pt;vertical-align:-14.0pt;"><span class="ltx_transformed_inner" style="width:31.4pt;transform:translate(-13.99pt,0pt) rotate(-90deg) ;">
<span id="S5.T1.1.3.3.1.1.1.1.1" class="ltx_p"><span id="S5.T1.1.3.3.1.1.1.1.1.1" class="ltx_text" style="font-size:50%;">CommonVoice</span></span>
</span></span></span>
</span>
</th>
<th id="S5.T1.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.3.3.2.1" class="ltx_text ltx_font_italic"># hours</span></th>
<td id="S5.T1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">25.0</td>
<td id="S5.T1.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">27.2</td>
<td id="S5.T1.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">27.0</td>
<td id="S5.T1.1.3.3.6" class="ltx_td ltx_border_t" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">25.0</td>
<td id="S5.T1.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">27.4</td>
<td id="S5.T1.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">27.4</td>
<td id="S5.T1.1.3.3.10" class="ltx_td ltx_border_t" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.3.3.11" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">25.0</td>
<td id="S5.T1.1.3.3.12" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">11.6</td>
<td id="S5.T1.1.3.3.13" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">11.5</td>
</tr>
<tr id="S5.T1.1.4.4" class="ltx_tr">
<th id="S5.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.4.4.1.1" class="ltx_text ltx_font_italic"># words</span></th>
<td id="S5.T1.1.4.4.2" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">161</td>
<td id="S5.T1.1.4.4.3" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">157</td>
<td id="S5.T1.1.4.4.4" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">153</td>
<td id="S5.T1.1.4.4.5" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.4.4.6" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">150</td>
<td id="S5.T1.1.4.4.7" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">150</td>
<td id="S5.T1.1.4.4.8" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">148</td>
<td id="S5.T1.1.4.4.9" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.4.4.10" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">149</td>
<td id="S5.T1.1.4.4.11" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">67</td>
<td id="S5.T1.1.4.4.12" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">65</td>
</tr>
<tr id="S5.T1.1.5.5" class="ltx_tr">
<th id="S5.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.5.5.1.1" class="ltx_text ltx_font_italic">avg-len</span></th>
<td id="S5.T1.1.5.5.2" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">5.7</td>
<td id="S5.T1.1.5.5.3" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">6.0</td>
<td id="S5.T1.1.5.5.4" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">5.9</td>
<td id="S5.T1.1.5.5.5" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.5.5.6" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">5.7</td>
<td id="S5.T1.1.5.5.7" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">6.1</td>
<td id="S5.T1.1.5.5.8" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">6.1</td>
<td id="S5.T1.1.5.5.9" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.5.5.10" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">4.8</td>
<td id="S5.T1.1.5.5.11" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">4.6</td>
<td id="S5.T1.1.5.5.12" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">4.6</td>
</tr>
<tr id="S5.T1.1.6.6" class="ltx_tr">
<th id="S5.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 2.0pt;" rowspan="3">
<span id="S5.T1.1.6.6.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S5.T1.1.6.6.1.1.1" class="ltx_p">
<span id="S5.T1.1.6.6.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:3.4pt;height:22.4pt;vertical-align:-9.5pt;"><span class="ltx_transformed_inner" style="width:22.4pt;transform:translate(-9.51pt,0pt) rotate(-90deg) ;">
<span id="S5.T1.1.6.6.1.1.1.1.1" class="ltx_p"><span id="S5.T1.1.6.6.1.1.1.1.1.1" class="ltx_text" style="font-size:50%;">MALACH</span></span>
</span></span></span>
</span>
</th>
<th id="S5.T1.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.6.6.2.1" class="ltx_text ltx_font_italic"># hours</span></th>
<td id="S5.T1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">85.0</td>
<td id="S5.T1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">9.2</td>
<td id="S5.T1.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">4.3</td>
<td id="S5.T1.1.6.6.6" class="ltx_td ltx_border_t" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.6.6.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">85.0</td>
<td id="S5.T1.1.6.6.8" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">33.0</td>
<td id="S5.T1.1.6.6.9" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">80.8</td>
<td id="S5.T1.1.6.6.10" class="ltx_td ltx_border_t" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.6.6.11" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">85.0</td>
<td id="S5.T1.1.6.6.12" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">19.2</td>
<td id="S5.T1.1.6.6.13" class="ltx_td ltx_align_right ltx_border_t" style="padding:0.5pt 2.0pt;">9.0</td>
</tr>
<tr id="S5.T1.1.7.7" class="ltx_tr">
<th id="S5.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.7.7.1.1" class="ltx_text ltx_font_italic"># words</span></th>
<td id="S5.T1.1.7.7.2" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">669</td>
<td id="S5.T1.1.7.7.3" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">73</td>
<td id="S5.T1.1.7.7.4" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">36</td>
<td id="S5.T1.1.7.7.5" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.7.7.6" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">632</td>
<td id="S5.T1.1.7.7.7" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">251</td>
<td id="S5.T1.1.7.7.8" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">563</td>
<td id="S5.T1.1.7.7.9" class="ltx_td" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.7.7.10" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">600</td>
<td id="S5.T1.1.7.7.11" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">137</td>
<td id="S5.T1.1.7.7.12" class="ltx_td ltx_align_right" style="padding:0.5pt 2.0pt;">63</td>
</tr>
<tr id="S5.T1.1.8.8" class="ltx_tr">
<th id="S5.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.5pt 2.0pt;"><span id="S5.T1.1.8.8.1.1" class="ltx_text ltx_font_italic">avg-len</span></th>
<td id="S5.T1.1.8.8.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">26.8</td>
<td id="S5.T1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">24.0</td>
<td id="S5.T1.1.8.8.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">5.3</td>
<td id="S5.T1.1.8.8.5" class="ltx_td ltx_border_bb" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.8.8.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">20.3</td>
<td id="S5.T1.1.8.8.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">20.3</td>
<td id="S5.T1.1.8.8.8" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">1692</td>
<td id="S5.T1.1.8.8.9" class="ltx_td ltx_border_bb" style="padding:0.5pt 2.0pt;"></td>
<td id="S5.T1.1.8.8.10" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">22.9</td>
<td id="S5.T1.1.8.8.11" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">22.9</td>
<td id="S5.T1.1.8.8.12" class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.5pt 2.0pt;">10.7</td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>CommonVoice</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The CommonVoice is a crowdsourced dataset collected by Mozilla <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We used corpus version 16.0 containing 19,673 validated hours in 120 languages. English portion contains 1,718 hours, German 912, and Czech 26 hours of training data. Since a mixture of these datasets is highly unbalanced and every mixed-language fine-tuning we ran with the full datasets ended in favor of English ASR with poor performance for the other two languages, we decided to balance the dataset equally and use only a randomly selected subset with 25 hours of training data per language.
With this significant reduction of English and German training data, we ensure equal importance and fair conditions for all languages during the training.
The development and test splits were used completely without any change.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>MALACH</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The full MALACH archive is a monumental collection with over 100,000 hours of interviews in 32 languages. About half of the archive is in English, although most English interviews are given by non-native speakers. The annotated English part contains 375 hours of speech, the German part almost 2,000 hours, and the Czech part 130 hours. Similarly to the CommonVoice dataset, we decided to balance this dataset and use an equal amount of training data for each language. The smallest training split is in the Czech part with about 87 hours of training data, so we randomly selected a subset with 85 hours of training data per language. We used full development and test splits without any additional changes. The test part had no speaker overlaps with train or development parts in all languages.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For English and Czech, we used datasets released under the Linguistic Data Consortium (LDC) – English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Czech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. We adopted the same train-dev-test splits as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and segmented train and development parts using time labels from the annotations into segments not exceeding 30 s, which is a reasonable limit of input examples during training due to GPU memory limits. The test parts for these two languages were already cleaned and contained only selected shorter segments (usually covering the maximum length of a single speaker's utterance without overlaps).
As we found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the Czech MALACH transcripts contain a mix of formal and colloquial Czech, causing a mismatch between train and test data, so we converted all Czech training transcripts into formal Czech to close the gap.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">For German, we adopted the same data splitting and pre-processing as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> with additional removing of the non-speech token <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">ah</span> from transcripts as we observed inconsistent annotations of this token. We used the full unsegmented test split without any further segmentation of filtration, so the recordings in the German test dataset are much longer and less clean than test data from other languages.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We fine-tuned all Wav2Vec models with the same setting as the base model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, i.e., we trained for 80 thousand steps with a batch size of about 26 minutes per step, and the learning rate warmed up over the first 8 000 steps to a maximum value of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">×</mo><msup id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml"><mn id="S6.p1.1.m1.1.1.3.2" xref="S6.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.p1.1.m1.1.1.3.3" xref="S6.p1.1.m1.1.1.3.3.cmml"><mo id="S6.p1.1.m1.1.1.3.3a" xref="S6.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S6.p1.1.m1.1.1.3.3.2" xref="S6.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">2</cn><apply id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.3.1.cmml" xref="S6.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S6.p1.1.m1.1.1.3.2.cmml" xref="S6.p1.1.m1.1.1.3.2">10</cn><apply id="S6.p1.1.m1.1.1.3.3.cmml" xref="S6.p1.1.m1.1.1.3.3"><minus id="S6.p1.1.m1.1.1.3.3.1.cmml" xref="S6.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S6.p1.1.m1.1.1.3.3.2.cmml" xref="S6.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">2\times 10^{-5}</annotation></semantics></math>, where it was held for the next 32 000 steps, and finally decayed exponentially to zero. The weights of the feature encoder were frozen for the first 10 000 steps of the fine-tuning.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.4" class="ltx_p">Whisper models were fine-tuned differently because we observed some overfitting tendencies. For each model size, dataset, and language, we run 4 different fine-tunings with learning rates set to <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="1\times 10^{-5}" display="inline"><semantics id="S6.p2.1.m1.1a"><mrow id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1.cmml">×</mo><msup id="S6.p2.1.m1.1.1.3" xref="S6.p2.1.m1.1.1.3.cmml"><mn id="S6.p2.1.m1.1.1.3.2" xref="S6.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.p2.1.m1.1.1.3.3" xref="S6.p2.1.m1.1.1.3.3.cmml"><mo id="S6.p2.1.m1.1.1.3.3a" xref="S6.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S6.p2.1.m1.1.1.3.3.2" xref="S6.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><times id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1"></times><cn type="integer" id="S6.p2.1.m1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.2">1</cn><apply id="S6.p2.1.m1.1.1.3.cmml" xref="S6.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.p2.1.m1.1.1.3.1.cmml" xref="S6.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S6.p2.1.m1.1.1.3.2.cmml" xref="S6.p2.1.m1.1.1.3.2">10</cn><apply id="S6.p2.1.m1.1.1.3.3.cmml" xref="S6.p2.1.m1.1.1.3.3"><minus id="S6.p2.1.m1.1.1.3.3.1.cmml" xref="S6.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S6.p2.1.m1.1.1.3.3.2.cmml" xref="S6.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">1\times 10^{-5}</annotation></semantics></math>, <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S6.p2.2.m2.1a"><mrow id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mn id="S6.p2.2.m2.1.1.2" xref="S6.p2.2.m2.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p2.2.m2.1.1.1" xref="S6.p2.2.m2.1.1.1.cmml">×</mo><msup id="S6.p2.2.m2.1.1.3" xref="S6.p2.2.m2.1.1.3.cmml"><mn id="S6.p2.2.m2.1.1.3.2" xref="S6.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S6.p2.2.m2.1.1.3.3" xref="S6.p2.2.m2.1.1.3.3.cmml"><mo id="S6.p2.2.m2.1.1.3.3a" xref="S6.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S6.p2.2.m2.1.1.3.3.2" xref="S6.p2.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><times id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1"></times><cn type="integer" id="S6.p2.2.m2.1.1.2.cmml" xref="S6.p2.2.m2.1.1.2">5</cn><apply id="S6.p2.2.m2.1.1.3.cmml" xref="S6.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S6.p2.2.m2.1.1.3.1.cmml" xref="S6.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S6.p2.2.m2.1.1.3.2.cmml" xref="S6.p2.2.m2.1.1.3.2">10</cn><apply id="S6.p2.2.m2.1.1.3.3.cmml" xref="S6.p2.2.m2.1.1.3.3"><minus id="S6.p2.2.m2.1.1.3.3.1.cmml" xref="S6.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S6.p2.2.m2.1.1.3.3.2.cmml" xref="S6.p2.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">5\times 10^{-5}</annotation></semantics></math>, <math id="S6.p2.3.m3.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S6.p2.3.m3.1a"><mrow id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml"><mn id="S6.p2.3.m3.1.1.2" xref="S6.p2.3.m3.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p2.3.m3.1.1.1" xref="S6.p2.3.m3.1.1.1.cmml">×</mo><msup id="S6.p2.3.m3.1.1.3" xref="S6.p2.3.m3.1.1.3.cmml"><mn id="S6.p2.3.m3.1.1.3.2" xref="S6.p2.3.m3.1.1.3.2.cmml">10</mn><mrow id="S6.p2.3.m3.1.1.3.3" xref="S6.p2.3.m3.1.1.3.3.cmml"><mo id="S6.p2.3.m3.1.1.3.3a" xref="S6.p2.3.m3.1.1.3.3.cmml">−</mo><mn id="S6.p2.3.m3.1.1.3.3.2" xref="S6.p2.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><apply id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1"><times id="S6.p2.3.m3.1.1.1.cmml" xref="S6.p2.3.m3.1.1.1"></times><cn type="integer" id="S6.p2.3.m3.1.1.2.cmml" xref="S6.p2.3.m3.1.1.2">1</cn><apply id="S6.p2.3.m3.1.1.3.cmml" xref="S6.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S6.p2.3.m3.1.1.3.1.cmml" xref="S6.p2.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S6.p2.3.m3.1.1.3.2.cmml" xref="S6.p2.3.m3.1.1.3.2">10</cn><apply id="S6.p2.3.m3.1.1.3.3.cmml" xref="S6.p2.3.m3.1.1.3.3"><minus id="S6.p2.3.m3.1.1.3.3.1.cmml" xref="S6.p2.3.m3.1.1.3.3"></minus><cn type="integer" id="S6.p2.3.m3.1.1.3.3.2.cmml" xref="S6.p2.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">1\times 10^{-4}</annotation></semantics></math> and <math id="S6.p2.4.m4.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S6.p2.4.m4.1a"><mrow id="S6.p2.4.m4.1.1" xref="S6.p2.4.m4.1.1.cmml"><mn id="S6.p2.4.m4.1.1.2" xref="S6.p2.4.m4.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S6.p2.4.m4.1.1.1" xref="S6.p2.4.m4.1.1.1.cmml">×</mo><msup id="S6.p2.4.m4.1.1.3" xref="S6.p2.4.m4.1.1.3.cmml"><mn id="S6.p2.4.m4.1.1.3.2" xref="S6.p2.4.m4.1.1.3.2.cmml">10</mn><mrow id="S6.p2.4.m4.1.1.3.3" xref="S6.p2.4.m4.1.1.3.3.cmml"><mo id="S6.p2.4.m4.1.1.3.3a" xref="S6.p2.4.m4.1.1.3.3.cmml">−</mo><mn id="S6.p2.4.m4.1.1.3.3.2" xref="S6.p2.4.m4.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.1b"><apply id="S6.p2.4.m4.1.1.cmml" xref="S6.p2.4.m4.1.1"><times id="S6.p2.4.m4.1.1.1.cmml" xref="S6.p2.4.m4.1.1.1"></times><cn type="integer" id="S6.p2.4.m4.1.1.2.cmml" xref="S6.p2.4.m4.1.1.2">5</cn><apply id="S6.p2.4.m4.1.1.3.cmml" xref="S6.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S6.p2.4.m4.1.1.3.1.cmml" xref="S6.p2.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S6.p2.4.m4.1.1.3.2.cmml" xref="S6.p2.4.m4.1.1.3.2">10</cn><apply id="S6.p2.4.m4.1.1.3.3.cmml" xref="S6.p2.4.m4.1.1.3.3"><minus id="S6.p2.4.m4.1.1.3.3.1.cmml" xref="S6.p2.4.m4.1.1.3.3"></minus><cn type="integer" id="S6.p2.4.m4.1.1.3.3.2.cmml" xref="S6.p2.4.m4.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.1c">5\times 10^{-4}</annotation></semantics></math>. We trained all models for 10 epochs with a batch size of 32 and measured the error rate after each epoch on the development dataset. Finally, we chose a checkpoint with the lowest error rate for evaluation.
We fine-tuned Wav2Vec models with <span id="S6.p2.4.1" class="ltx_text ltx_font_typewriter">Fairseq</span> and Whisper with the <span id="S6.p2.4.2" class="ltx_text ltx_font_typewriter">Transformers</span> library<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/huggingface/transformers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/transformers</a></span></span></span>, both on a machine with eight NVIDIA A100 GPUs. The training took approx. 12 hours (Wav2Vec-base), 30 hours (Wav2Vec-XLS-R), resp. less than 3 hours (Whisper).</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We compared models in terms of word error rate (WER). Since all transcripts were in lowercase and cleaned from punctuation, our fine-tuned models cannot predict punctuation or upper-cased characters, so we did not consider casing and punctuation differences with the reference as errors.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>WER <math id="S6.T2.2.m1.1" class="ltx_math_unparsed" alttext="[\%]" display="inline"><semantics id="S6.T2.2.m1.1b"><mrow id="S6.T2.2.m1.1c"><mo stretchy="false" id="S6.T2.2.m1.1.1">[</mo><mo id="S6.T2.2.m1.1.2">%</mo><mo stretchy="false" id="S6.T2.2.m1.1.3">]</mo></mrow><annotation encoding="application/x-tex" id="S6.T2.2.m1.1d">[\%]</annotation></semantics></math> for monolingual and various multilingual ASR models evaluated on relevant test splits of two multilingual datasets: CommonVoice and MALACH. We use ISO 639-1 codes to denote individual languages: CS=Czech, EN=English, and DE=German.</figcaption>
<table id="S6.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.3.1.1" class="ltx_tr">
<td id="S6.T2.3.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.1.1.2" class="ltx_td ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"># of</td>
<td id="S6.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"># of</td>
<td id="S6.T2.3.1.1.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Pre-training</td>
<td id="S6.T2.3.1.1.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Fine-tuning</td>
<td id="S6.T2.3.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;" colspan="3">CommonVoice</td>
<td id="S6.T2.3.1.1.8" class="ltx_td ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.1.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;" colspan="3">MALACH</td>
</tr>
<tr id="S6.T2.3.2.2" class="ltx_tr">
<td id="S6.T2.3.2.2.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">Row</td>
<td id="S6.T2.3.2.2.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">Model type</td>
<td id="S6.T2.3.2.2.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">params</td>
<td id="S6.T2.3.2.2.4" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">langs</td>
<td id="S6.T2.3.2.2.5" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">langs</td>
<td id="S6.T2.3.2.2.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">langs</td>
<td id="S6.T2.3.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS</td>
<td id="S6.T2.3.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">EN</td>
<td id="S6.T2.3.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">DE</td>
<td id="S6.T2.3.2.2.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS</td>
<td id="S6.T2.3.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">EN</td>
<td id="S6.T2.3.2.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">DE</td>
</tr>
<tr id="S6.T2.3.3.3" class="ltx_tr">
<td id="S6.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">1</td>
<td id="S6.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">Wav2Vec-base</td>
<td id="S6.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">95M</td>
<td id="S6.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">1</td>
<td id="S6.T2.3.3.3.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.3.3.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.7.1" class="ltx_text ltx_font_bold">11.36</span></td>
<td id="S6.T2.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.8.1" class="ltx_text ltx_font_bold">34.07</span></td>
<td id="S6.T2.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.9.1" class="ltx_text ltx_font_bold">17.23</span></td>
<td id="S6.T2.3.3.3.10" class="ltx_td ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.11.1" class="ltx_text ltx_font_bold">11.19</span></td>
<td id="S6.T2.3.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.12.1" class="ltx_text ltx_font_bold">19.89</span></td>
<td id="S6.T2.3.3.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.3.3.13.1" class="ltx_text ltx_font_bold">20.55</span></td>
</tr>
<tr id="S6.T2.3.4.4" class="ltx_tr">
<td id="S6.T2.3.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">2</td>
<td id="S6.T2.3.4.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">Wav2Vec-base</td>
<td id="S6.T2.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">95M</td>
<td id="S6.T2.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">2</td>
<td id="S6.T2.3.4.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS+EN</td>
<td id="S6.T2.3.4.4.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN</td>
<td id="S6.T2.3.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">15.08</td>
<td id="S6.T2.3.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">37.24</td>
<td id="S6.T2.3.4.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.4.4.10" class="ltx_td ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.4.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">13.62</td>
<td id="S6.T2.3.4.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">21.93</td>
<td id="S6.T2.3.4.4.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
</tr>
<tr id="S6.T2.3.5.5" class="ltx_tr">
<td id="S6.T2.3.5.5.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">3</td>
<td id="S6.T2.3.5.5.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">(bilingual)</td>
<td id="S6.T2.3.5.5.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.5.5.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.5.5.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.5.5.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS+EN</td>
<td id="S6.T2.3.5.5.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.99</td>
<td id="S6.T2.3.5.5.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">36.66</td>
<td id="S6.T2.3.5.5.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.5.5.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.5.5.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.89</td>
<td id="S6.T2.3.5.5.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">23.12</td>
<td id="S6.T2.3.5.5.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
</tr>
<tr id="S6.T2.3.6.6" class="ltx_tr">
<td id="S6.T2.3.6.6.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">4</td>
<td id="S6.T2.3.6.6.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.6.6.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.6.6.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.6.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS+DE</td>
<td id="S6.T2.3.6.6.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS/DE</td>
<td id="S6.T2.3.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">14.97</td>
<td id="S6.T2.3.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.6.6.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">21.16</td>
<td id="S6.T2.3.6.6.10" class="ltx_td ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.6.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">13.16</td>
<td id="S6.T2.3.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.6.6.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">24.49</td>
</tr>
<tr id="S6.T2.3.7.7" class="ltx_tr">
<td id="S6.T2.3.7.7.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">5</td>
<td id="S6.T2.3.7.7.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.7.7.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.7.7.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.7.7.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.7.7.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS+DE</td>
<td id="S6.T2.3.7.7.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.69</td>
<td id="S6.T2.3.7.7.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.7.7.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">21.19</td>
<td id="S6.T2.3.7.7.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.7.7.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.20</td>
<td id="S6.T2.3.7.7.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.7.7.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">25.10</td>
</tr>
<tr id="S6.T2.3.8.8" class="ltx_tr">
<td id="S6.T2.3.8.8.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">6</td>
<td id="S6.T2.3.8.8.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.8.8.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.8.8.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.8.8.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">EN+DE</td>
<td id="S6.T2.3.8.8.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">EN/DE</td>
<td id="S6.T2.3.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">36.71</td>
<td id="S6.T2.3.8.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">20.16</td>
<td id="S6.T2.3.8.8.10" class="ltx_td ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.8.8.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.8.8.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">21.07</td>
<td id="S6.T2.3.8.8.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">23.39</td>
</tr>
<tr id="S6.T2.3.9.9" class="ltx_tr">
<td id="S6.T2.3.9.9.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">7</td>
<td id="S6.T2.3.9.9.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.9.9.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.9.9.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.9.9.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.9.9.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">EN+DE</td>
<td id="S6.T2.3.9.9.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.9.9.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">36.14</td>
<td id="S6.T2.3.9.9.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">20.73</td>
<td id="S6.T2.3.9.9.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.9.9.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.9.9.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.53</td>
<td id="S6.T2.3.9.9.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">24.70</td>
</tr>
<tr id="S6.T2.3.10.10" class="ltx_tr">
<td id="S6.T2.3.10.10.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">8</td>
<td id="S6.T2.3.10.10.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">Wav2Vec-base</td>
<td id="S6.T2.3.10.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">95M</td>
<td id="S6.T2.3.10.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">3</td>
<td id="S6.T2.3.10.10.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS+EN+DE</td>
<td id="S6.T2.3.10.10.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.10.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">16.52</td>
<td id="S6.T2.3.10.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">38.70</td>
<td id="S6.T2.3.10.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">21.72</td>
<td id="S6.T2.3.10.10.10" class="ltx_td ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.10.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">14.19</td>
<td id="S6.T2.3.10.10.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">21.66</td>
<td id="S6.T2.3.10.10.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.0pt;padding-right:6.0pt;">24.87</td>
</tr>
<tr id="S6.T2.3.11.11" class="ltx_tr">
<td id="S6.T2.3.11.11.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">9</td>
<td id="S6.T2.3.11.11.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">(trilingual)</td>
<td id="S6.T2.3.11.11.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.11.11.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.11.11.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.11.11.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS+EN</td>
<td id="S6.T2.3.11.11.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">16.80</td>
<td id="S6.T2.3.11.11.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">38.16</td>
<td id="S6.T2.3.11.11.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.11.11.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.11.11.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">15.66</td>
<td id="S6.T2.3.11.11.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.35</td>
<td id="S6.T2.3.11.11.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
</tr>
<tr id="S6.T2.3.12.12" class="ltx_tr">
<td id="S6.T2.3.12.12.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">10</td>
<td id="S6.T2.3.12.12.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.12.12.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.12.12.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.12.12.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.12.12.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS+DE</td>
<td id="S6.T2.3.12.12.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">16.47</td>
<td id="S6.T2.3.12.12.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.12.12.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.11</td>
<td id="S6.T2.3.12.12.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.12.12.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">15.49</td>
<td id="S6.T2.3.12.12.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.12.12.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">26.56</td>
</tr>
<tr id="S6.T2.3.13.13" class="ltx_tr">
<td id="S6.T2.3.13.13.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">11</td>
<td id="S6.T2.3.13.13.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.13.13.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.13.13.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.13.13.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.13.13.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">EN+DE</td>
<td id="S6.T2.3.13.13.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.13.13.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">38.09</td>
<td id="S6.T2.3.13.13.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.30</td>
<td id="S6.T2.3.13.13.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.13.13.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.13.13.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.74</td>
<td id="S6.T2.3.13.13.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">26.41</td>
</tr>
<tr id="S6.T2.3.14.14" class="ltx_tr">
<td id="S6.T2.3.14.14.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">12</td>
<td id="S6.T2.3.14.14.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.14.14.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.14.14.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.14.14.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.14.14.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS+EN+DE</td>
<td id="S6.T2.3.14.14.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">16.97</td>
<td id="S6.T2.3.14.14.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">38.35</td>
<td id="S6.T2.3.14.14.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">22.86</td>
<td id="S6.T2.3.14.14.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.14.14.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">16.82</td>
<td id="S6.T2.3.14.14.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">24.29</td>
<td id="S6.T2.3.14.14.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">28.49</td>
</tr>
<tr id="S6.T2.3.15.15" class="ltx_tr">
<td id="S6.T2.3.15.15.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">13</td>
<td id="S6.T2.3.15.15.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">Wav2Vec-XLS-R</td>
<td id="S6.T2.3.15.15.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">315M</td>
<td id="S6.T2.3.15.15.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">128</td>
<td id="S6.T2.3.15.15.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S6.T2.3.15.15.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.15.15.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.15.15.7.1" class="ltx_text ltx_font_bold">11.37</span></td>
<td id="S6.T2.3.15.15.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">26.96</td>
<td id="S6.T2.3.15.15.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">14.27</td>
<td id="S6.T2.3.15.15.10" class="ltx_td ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.15.15.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">12.46</td>
<td id="S6.T2.3.15.15.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">17.23</td>
<td id="S6.T2.3.15.15.13" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.0pt;padding-right:6.0pt;">20.11</td>
</tr>
<tr id="S6.T2.3.16.16" class="ltx_tr">
<td id="S6.T2.3.16.16.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14</td>
<td id="S6.T2.3.16.16.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">Whisper-base</td>
<td id="S6.T2.3.16.16.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">74M</td>
<td id="S6.T2.3.16.16.4" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">99</td>
<td id="S6.T2.3.16.16.5" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S6.T2.3.16.16.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.16.16.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">70.62</td>
<td id="S6.T2.3.16.16.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">25.35</td>
<td id="S6.T2.3.16.16.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">28.66</td>
<td id="S6.T2.3.16.16.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.16.16.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">58.39</td>
<td id="S6.T2.3.16.16.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">25.01</td>
<td id="S6.T2.3.16.16.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">34.03</td>
</tr>
<tr id="S6.T2.3.17.17" class="ltx_tr">
<td id="S6.T2.3.17.17.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">15</td>
<td id="S6.T2.3.17.17.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.17.17.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.17.17.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.17.17.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.17.17.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.17.17.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">26.69</td>
<td id="S6.T2.3.17.17.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">24.81</td>
<td id="S6.T2.3.17.17.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">27.42</td>
<td id="S6.T2.3.17.17.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.17.17.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">18.38</td>
<td id="S6.T2.3.17.17.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">40.44</td>
<td id="S6.T2.3.17.17.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">30.88</td>
</tr>
<tr id="S6.T2.3.18.18" class="ltx_tr">
<td id="S6.T2.3.18.18.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">16</td>
<td id="S6.T2.3.18.18.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">Whisper-small</td>
<td id="S6.T2.3.18.18.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">244M</td>
<td id="S6.T2.3.18.18.4" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">99</td>
<td id="S6.T2.3.18.18.5" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S6.T2.3.18.18.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.18.18.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">38.03</td>
<td id="S6.T2.3.18.18.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">17.07</td>
<td id="S6.T2.3.18.18.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">15.10</td>
<td id="S6.T2.3.18.18.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.18.18.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">35.69</td>
<td id="S6.T2.3.18.18.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">20.11</td>
<td id="S6.T2.3.18.18.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">26.11</td>
</tr>
<tr id="S6.T2.3.19.19" class="ltx_tr">
<td id="S6.T2.3.19.19.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">17</td>
<td id="S6.T2.3.19.19.2" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.19.19.3" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.19.19.4" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.19.19.5" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.19.19.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.19.19.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">18.51</td>
<td id="S6.T2.3.19.19.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">29.27</td>
<td id="S6.T2.3.19.19.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">15.74</td>
<td id="S6.T2.3.19.19.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.19.19.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">12.99</td>
<td id="S6.T2.3.19.19.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">64.08</td>
<td id="S6.T2.3.19.19.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">36.04</td>
</tr>
<tr id="S6.T2.3.20.20" class="ltx_tr">
<td id="S6.T2.3.20.20.1" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">18</td>
<td id="S6.T2.3.20.20.2" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">Whisper-large</td>
<td id="S6.T2.3.20.20.3" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">1,550M</td>
<td id="S6.T2.3.20.20.4" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">99</td>
<td id="S6.T2.3.20.20.5" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S6.T2.3.20.20.6" class="ltx_td ltx_align_left" style="padding-left:6.0pt;padding-right:6.0pt;">-</td>
<td id="S6.T2.3.20.20.7" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">14.23</td>
<td id="S6.T2.3.20.20.8" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.20.20.8.1" class="ltx_text ltx_font_bold">11.67</span></td>
<td id="S6.T2.3.20.20.9" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.20.20.9.1" class="ltx_text ltx_font_bold">7.29</span></td>
<td id="S6.T2.3.20.20.10" class="ltx_td" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.20.20.11" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">21.65</td>
<td id="S6.T2.3.20.20.12" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.20.20.12.1" class="ltx_text ltx_font_bold">16.97</span></td>
<td id="S6.T2.3.20.20.13" class="ltx_td ltx_align_center" style="padding-left:6.0pt;padding-right:6.0pt;">21.59</td>
</tr>
<tr id="S6.T2.3.21.21" class="ltx_tr">
<td id="S6.T2.3.21.21.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">19</td>
<td id="S6.T2.3.21.21.2" class="ltx_td ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.21.21.3" class="ltx_td ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.21.21.4" class="ltx_td ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.21.21.5" class="ltx_td ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.21.21.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">CS/EN/DE</td>
<td id="S6.T2.3.21.21.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">15.14</td>
<td id="S6.T2.3.21.21.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">13.59</td>
<td id="S6.T2.3.21.21.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">8.95</td>
<td id="S6.T2.3.21.21.10" class="ltx_td ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"></td>
<td id="S6.T2.3.21.21.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.21.21.11.1" class="ltx_text ltx_font_bold">10.19</span></td>
<td id="S6.T2.3.21.21.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;">27.10</td>
<td id="S6.T2.3.21.21.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.0pt;padding-right:6.0pt;"><span id="S6.T2.3.21.21.13.1" class="ltx_text ltx_font_bold">19.59</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Our results are tabulated in Tab. <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In the first part of the table (rows 1-12), we are comparing monolingual Wav2Vec models with bilingual models in all possible language combinations, and with the trilingual model. We fine-tuned each pre-trained model on all combinations of languages that were possible. For <span id="S6.p4.1.1" class="ltx_text ltx_font_italic">monolingual fine-tuning</span> (i.e. we used data from one language only during the fine-tuning), we aggregated results into a single row and separated individual fine-tuning languages by a slash in the language columns. For example, on the 8th row, we took a trilingual pre-trained model (pre-training languages were CS+EN+DE), fine-tuned 6 different ASR models, one per each language (CS/EN/DE) and each dataset (CommonVoice, MALACH), and evaluated each model on corresponding test data. On the contrary, on the 12th row, we took the same pre-trained trilingual model and fine-tuned 2 different ASR models (one per dataset) from a mixture of 3 languages in the fine-tuning data (denoted as CS+EN+DE). In other words, we denote the joining of datasets for multilingual training by “+” (the resulting model is one multilingual model per dataset), and a set of language-specific monolingual training runs by “/”
resulting in one monolingual model per each language and dataset.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">In the second part of Tab. <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (rows 13-19), we present results using large-scale multilingual models. It is important to consider individual models' sizes and fairly compare only models of similar sizes, so we also included the column with the number of trainable parameters in the table. We fine-tuned all 4 models (Wav2Vec-XLS-R and three sizes of Whisper) on all languages and datasets. The zero-shot performance of the Whisper models is reported on rows 14, 16, and 18.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<figure id="S7.F2" class="ltx_figure"><img src="/html/2407.17160/assets/x2.png" id="S7.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>WER change after adding more languages into pre-training. We abbreviated CommonVoice to CV. We plot also confidence intervals at 95% confidence level.</figcaption>
</figure>
<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our results suggest that adding more languages into the pre-training phase while keeping the model at the same size did not bring any improvement for either dataset. On the contrary, we observed a trend in WER increasing when adding more languages.
We plotted this interesting trend in Fig. <a href="#S7.F2" title="Figure 2 ‣ 7 Discussion ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where for each dataset and each language, we compared the monolingual model (row 1 in Tab. <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), the average WER scored by bilingual models (rows 2, 4, and 6) and the trilingual model fine-tuned on a single language (row 8). This suggests that even when our dataset is multilingual and contains a lot of mixed-language sentences, the best we can do (when we want to keep the model reasonably small for production) is to train the monolingual model on each language separately from scratch.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">It is worth noting that our best results in Tab. <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are not state-of-the-art results. We are just comparing different models under the same conditions. Significantly better results could be scored with monolingual models when using more training data and a language model in the CTC decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">As for adding more languages into fine-tuning (assuming we already have a pre-trained multilingual model), we also did not observe any significant improvements. For the CommonVoice dataset, the results are moreover the same no matter whether we fine-tune a multilingual model or more monolingual models. For the MALACH dataset, we observed even an increase of WER after adding more languages into fine-tuning (compare e.g. rows 2 vs. 3, 4 vs. 5, 6 vs. 7, or 8-12 in Tab. <a href="#S6.T2" title="Table 2 ‣ 6 Experiments ‣ A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Large-scale multilingual models could outperform monolingual Wav2Vec-base models, but only at the cost of a many times higher number of parameters, and thus higher computational complexity leading to higher price and carbon footprint for each decoded word. If we compare models with similar sizes as Wav2Vec-base (Whisper-base and -small), we can, again, observe the superiority of monolingual models.
In several cases, the Whisper model tended to hallucinate after the fine-tuning, leading to higher WER than its zero-shot performance.
The Whisper models scored significantly better results on English and German CommonVoice datasets, which is probably due to the presence of the full datasets in the training process.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we have presented a comparative analysis over a large set of experiments with different
language combinations in both pre-training and fine-tuning of
ASR systems based on Wav2Vec 2.0 models. We evaluated our models on two multilingual datasets and three languages. Our results suggest that monolingual Wav2vec models are, in most cases, superior to multilingual models. Only large-scale multilingual models, many times larger, can outperform monolingual Wav2Vec models, but only at the cost of much higher decoding complexity and carbon footprint for each transcribed word.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgements</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This research was supported by the Ministry of the Interior of the Czech Republic, project No. VJ01010108 and by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:90254).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ``Wav2Vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 12 449–12 460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, ``XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022, pp. 2278–2282.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. V. Psutka, A. Pražák, and J. Vaněk, ``Recognition of heavily accented and emotional speech of English and Czech Holocaust survivors using various DNN architectures,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Speech and Computer</em>, A. Karpov and R. Potapova, Eds.   Cham: Springer International Publishing, 2021, pp. 553–564.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Lehečka, J. Švec, J. V. Psutka, and P. Ircing, ``Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of INTERSPEECH 2023</em>, 2023, pp. 201–205.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, ``Common voice: A massively-multilingual speech corpus,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</em>, 2020, pp. 4211–4215.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajič, D. Oard, M. Picheny, J. Psutka, B. Ramabhadran, D. Soergel, T. Ward, and Wei-Jing Zhu, ``Automatic recognition of spontaneous speech for access to multilingual oral history archives,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Speech and Audio Processing</em>, vol. 12, no. 4, pp. 420–435, 2004.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Psutka, P. Ircing, J. V. Psutka, J. Hajič, W. Byrne, and J. Mírovský, ``Automatic transcription of Czech, Russian and Slovak spontaneous speech in the MALACH project,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Eurospeech 2005</em>.   ISCA, 2005, pp. 1349–1352.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Psutka, J. Švec, J. V. Psutka, J. Vaněk, A. Pražák, L. Šmídl, and P. Ircing, ``System for fast lexical and phonetic spoken term detection in a Czech cultural heritage archive,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">EURASIP Journal on Audio, Speech, and Music Processing</em>, vol. 2011, no. 1, pp. 1–10, 2011.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Picheny, Z. Tüske, B. Kingsbury, K. Audhkhasi, X. Cui, and G. Saon, ``Challenging the boundaries of speech recognition: The MALACH corpus,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Interspeech 2019</em>, 2019, pp. 326–330.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Švec, J. Psutka, L. Šmídl, and J. Trmal, ``A relevance score estimation for spoken term detection based on RNN-generated pronunciation embeddings,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Interspeech 2017</em>, 2017, pp. 2934–2938.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Lehečka, J. V. Psutka, and J. Psutka, ``Transformer-based automatic speech recognition of formal and colloquial Czech in MALACH project,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Text, Speech, and Dialogue</em>, P. Sojka, A. Horák, I. Kopeček, and K. Pala, Eds.   Cham: Springer International Publishing, 2022, pp. 301–312.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Picheny, Q. Yang, D. Zhang, and L. Zhang, ``The MALACH Corpus: Results with End-to-End Architectures and Pretraining,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 5097–5101.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ``Attention is all you need,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, ``Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural nets,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML '06: Proceedings of the International Conference on Machine Learning</em>, 2006.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Libri-light: A benchmark for asr with limited or no supervision,'' in <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 7669–7673.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, ``VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>.   Online: Association for Computational Linguistics, Aug. 2021, pp. 993–1003. [Online]. Available: <a target="_blank" href="https://aclanthology.org/2021.acl-long.80" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-long.80</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. Ramabhadran, S. Gustman, W. Byrne, J. Hajič, D. Oard, J. S. Olsson, M. Picheny, and J. Psutka, ``USC-SFI MALACH Interviews and Transcripts English LDC2012S05,'' Philadelphia: Linguistic Data Consortium, <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC2012s05" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC2012s05</a>, 2012.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Psutka, V. Radová, P. Ircing, J. Matoušek, and L. Müller, ``USC-SFI MALACH Interviews and Transcripts Czech LDC2014S04,'' Philadelphia: Linguistic Data Consortium, <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC2014S04" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC2014S04</a>, 2014.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.17159" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.17160" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.17160">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.17160" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.17161" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 15:47:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
