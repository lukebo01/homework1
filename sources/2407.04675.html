<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.04675] Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition</title><meta property="og:description" content="Modern automatic speech recognition (ASR) model is required to accurately transcribe diverse speech signals (from different domains, languages, accents, etc) given the specific contextual information in various applica…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.04675">

<!--Generated on Mon Aug  5 14:37:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seed Team, ByteDance 
<br class="ltx_break">
</span><span class="ltx_author_notes">Please cite this work as "Seed-ASR (2024)". The list of authors can be found at the end of the document.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Modern automatic speech recognition (ASR) model is required to accurately transcribe diverse speech signals (from different domains, languages, accents, etc) given the specific contextual information in various application scenarios. Classic end-to-end models fused with extra language models perform well, but mainly in data matching scenarios and are gradually approaching a bottleneck. In this work, we introduce Seed-ASR<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Seed-ASR capabilities have been applied in a variety of ByteDance products, and have provided technical commercialization services in China with Doubao speech recognition models.</span></span></span>, a large language model (LLM) based speech recognition model. Seed-ASR is developed based on the framework of audio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting continuous speech representations together with contextual information into the LLM. Through stage-wise large-scale training and the elicitation of context-aware capabilities in LLM, Seed-ASR demonstrates significant improvement over end-to-end models on comprehensive evaluation sets, including multiple domains, accents/dialects and languages. Additionally, Seed-ASR can be further deployed to support specific needs in various scenarios without requiring extra language models. Compared to recently released large ASR models, Seed-ASR achieves 10%-40% reduction in word (or character, for Chinese) error rates on Chinese and English public test sets, further demonstrating its powerful performance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">We present Seed-ASR, an LLM-based large-scale ASR model. Aiming to become a "smarter" speech recognition model, Seed-ASR is developed under the framework of audio conditioned LLM (AcLLM), leveraging the capability of LLMs by inputting continuous speech representation together with instruction and contextual information into the LLM. Seed-ASR has five major features:</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">High Recognition Accuracy</span>: By training on over 20 million hours of speech data and nearly 900 thousand hours of paired ASR data, our Chinese multi-dialect model, Seed-ASR (CN), and our multilingual model, Seed-ASR (ML), achieve impressive results on public datasets and our in-house comprehensive evaluation sets (shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>);</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Large Model Capacity</span>: Seed-ASR employs an audio encoder with nearly 2 billion parameters and a Mixture of Experts (MoE) LLM with tens of billions of parameters for modeling. The experiments of scaling law on ASR tasks underpin our decision to choose large models;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multiple language Support</span>: While maintaining high accuracy, Seed-ASR (CN) supports transcribing Mandarin and 13 Chinese dialects with a single model. Additionally, Seed-ASR (ML) recognizes speech of English and 7 other languages, and is being extended to support more than 40 languages;</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Context-aware Ability</span>: Seed-ASR utilizes a range of contextual information, including historical dialogues, video editing history, and meeting participation details, in a unified model to capture essential indicators related to speech content. This integration substantially boosts keyword recall in ASR evaluation sets across various scenarios.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i5.p1.3" class="ltx_p"><span id="S1.I1.i5.p1.3.1" class="ltx_text ltx_font_bold">Stage-wise Training Recipe</span>: The development of Seed-ASR goes through a simple and effective training recipe: self-supervised learning (SSL) of audio encoder <math id="S1.I1.i5.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.I1.i5.p1.1.m1.1a"><mo stretchy="false" id="S1.I1.i5.p1.1.m1.1.1" xref="S1.I1.i5.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i5.p1.1.m1.1b"><ci id="S1.I1.i5.p1.1.m1.1.1.cmml" xref="S1.I1.i5.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.p1.1.m1.1c">\rightarrow</annotation></semantics></math> supervised fine-tuning (SFT) <math id="S1.I1.i5.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.I1.i5.p1.2.m2.1a"><mo stretchy="false" id="S1.I1.i5.p1.2.m2.1.1" xref="S1.I1.i5.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i5.p1.2.m2.1b"><ci id="S1.I1.i5.p1.2.m2.1.1.cmml" xref="S1.I1.i5.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.p1.2.m2.1c">\rightarrow</annotation></semantics></math> context SFT <math id="S1.I1.i5.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.I1.i5.p1.3.m3.1a"><mo stretchy="false" id="S1.I1.i5.p1.3.m3.1.1" xref="S1.I1.i5.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i5.p1.3.m3.1b"><ci id="S1.I1.i5.p1.3.m3.1.1.cmml" xref="S1.I1.i5.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.p1.3.m3.1c">\rightarrow</annotation></semantics></math> reinforcement learning (RL). Each stage has a distinct role, ensuring stage-by-stage performance improvement of Seed-ASR.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.04675/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The comparison of ASR performance between Seed-ASR and other strong released models on our internal multi-domain evaluation sets and public sets, covering both Mandarin and English. The MLS en-US result of Whisper Large-v3 is obtained by locally decoding the MLS en-US test set because there is no reported WER on published papers or technical reports.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Different from existing LLM-based ASR models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, Seed-ASR aims for extensive improvements in ASR performance over the state-of-the-art in ASR technology across multiple languages including Chinese and English, tailored for a broad array of application scenarios featuring varied speech types and contexts. To achieve this, we build a series of high-quality evaluation sets that include a wide range of speech inputs, such as different domains, accents/dialects, languages and speech duration. These sets also cover evaluation of the customization capability of an ASR system under different application scenarios (e.g. the keyword accuracy and consistency in conversations). In designing Seed-ASR, we chose the path of large-scale training, leveraging both substantial model capacity and extensive training data to enhance generalization ability. We also elaborate the customization capability by training the model to account for contexts provided to the AcLLM framework, forming a unified and compact model structure for different scenarios. On our multi-dimensional evaluation sets, Seed-ASR demonstrates more comprehensive and powerful model capability compared to the classic end-to-end models. The performance advantage of Seed-ASR is further evidenced in public test sets and our subjective understanding evaluations. In the following sections, we will introduce our motivation, methods, models and evaluation in detail.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Motivation</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Since the rise of neural networks (NNs) and deep learning in 2010s, the modeling of automatic speech recognition (ASR) has experienced an upgrade from the hybrid framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> that only relies on NN-based acoustic models to the end-to-end (E2E) framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> in which the entire NN models are trained to output transcriptions directly. Although significant progress has been made in recognition accuracy as measured by word error rate (WER), current end-to-end ASR models are still not "smart" enough, which is limited by the model capacity and from-scratch training manner. Specifically, it cannot efficiently utilize rich common sense knowledge and conduct contextual reasoning during the recognition process, thus inevitably relying on the complicated fusion strategy with extra language models. With the rapid development of LLM technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the potential of AI continues to grow. Automatic speech recognition (ASR), as a classic task in AI, also stands at the brink of advancements in its model framework.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">The upgrade of the ASR model could get inspirations from the technical advancements of LLM, which can be attributed to three main aspects:</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Unified model framework. LLM employs a decoder-only framework based on the next-token-prediction. It sequences the input and output text, relying on the self-attention mechanism to establish dependencies between tokens in sequences, thereby unifying text understanding and text generation;</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The power of scaling law. Large-scale model parameters provide crucial capacity for LLM to learn knowledge from diverse data sources. For example, from GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the number of parameters increases from 1.5 billion to 175 billion, enabling GPT-3 to exhibit better generalization and emergent abilities;</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i3.p1.1" class="ltx_p">Comprehensive training pipeline, ChatGPT goes through three stages: pre-training, supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). In the stage of pre-training, LLM is trained on a large amount of text data, which makes it store extensive knowledge. In the stage of SFT, LLM is further fine-tuned on higher-quality, task-oriented data, enhancing its ability to reason with context and understand task instructions. Finally, in the RLHF stage, the training objective shifts to align the LLM’s behavior with human preferences with the help of reinforcement learning;</p>
</div>
</li>
</ul>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">Since the task of ASR is to convert speech to text, its text generation process is consistent with that of LLMs. The extensive text knowledge and contextual reasoning capabilities stored in LLMs make them potential components for providing semantic guidance to ASR. The remaining core challenge is how to enable LLMs to better "understand" speech, a modality that is different from text.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<figure id="S3.F2" class="ltx_figure"><img src="" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The model framework used in Seed-ASR. When contexts are provided, the instruction is "There are relevant contexts, transcribe the speech into text:". Otherwise, the instruction is "Transcribe the speech into text:". </figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Framework and Training Recipe</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Based on the aforementioned motivation, we propose Seed-ASR, a large-scale speech recognition model built on the framework of audio conditioned LLM (AcLLM). By inputting encoded continuous speech representations together with a task instruction and relevant contexts into a pretrained LLM, Seed-ASR can leverage the rich text knowledge and the reasoning ability of the LLM to generate the corresponding text transcription of speech. The overall framework is shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Audio is a different modality from text. To enable LLMs better understand diverse speech inputs, we adopt the concept of large-scale pretraining in LLMs. Specifically, we construct an audio encoder with nearly 2 billion parameters and conduct self-supervised learning (SSL) on tens of millions of hours of data.
The pre-trained audio encoder gains strong speech representation ability, which facilitates rapid convergence during supervised fine-tuning (SFT). After the large-scale SSL stage, we implement a simple and effective stage-wise training recipe within the framework of AcLLM (shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Framework and Training Recipe ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In the stage of SFT, we establish the mapping relationship between speech and text by training on a large amount of speech-text pairs. In the stage of context SFT, we use a relatively small amount of context-speech-text triples to elicit the LLM’s ability to capture speech-relevant clues from context. These triple data can be customized according to specific scenarios. In the stage of reinforcement learning, we apply the training criteria of MWER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and some improvements to further strengthen the ability of our model. In the following subsections, we will introduce these methods in more detail.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.04675/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="48" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The stage-wise training recipe for the development of Seed-ASR. SSL represents self-supervised learning, SFT represents supervised fine-tuning, RL represents reinforcement learning.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SSL of Audio Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Large-scale SSL enables audio encoders to capture rich information from speech. Inspired by the BERT-based speech SSL framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we developed our audio encoder, a conformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> model that captures both global and local structures stored in audio signals. In this work, we primarily focus on speech signal. Since it is trained on large-scale unsupervised data, we term the trained audio encoder as
LUISE, which represents <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">L</span>arge-scale <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">U</span>nsupervised <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">I</span>terative <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_bold">S</span>peech <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_bold">E</span>ncoder.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2407.04675/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="207" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The training procedure of our audio encoder LUISE.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Adhering to the concept of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, LUISE adopts a learning paradigm of masked language prediction. The training procedure is illustrated in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 SSL of Audio Encoder ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, the sequence of mel-filterbank feature extracted from the waveform is first input to the tokenizer module to obtain discrete labels for each frame. Then, the training of LUISE is conducted using the cross-entropy criterion, with the loss function calculated only for the masked frames. After training, the softmax layer is removed, and the encoder part of LUISE is used for subsequent supervised fine-tuning.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">We utilize an iterative fixed tokenizer method to obtain the corresponding discrete labels for each frame. In the first iteration, we apply a random-projection layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to project speech feature to a randomly initialized codebook, and map them to discrete labels through finding the nearest vector in the codebook. In the second iteration, we perform K-means clustering on the representations of an intermediate layer of the previously trained encoder to obtain a new codebook. The discrete labels are then obtained by finding the closest vector in the new codebook to the representation from the same intermediate layer. During the selection of the intermediate layer, we freeze the parameters of encoder trained in the first iteration, and add a mapping layer and connectionist temporal classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> loss to each intermediate layer for supervised fine-tuning. Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 SSL of Audio Encoder ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the word error rate (WER) obtained from supervised fine-tuning on the representation of each intermediate layer. For LUISE with 2 billion parameters, the output at the 25th layer (out of 32 layers) demonstrates the best semantic representation and is used for the generation of discrete labels in subsequent iterations.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2407.04675/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The probing experiment of the layer with the best semantic representations in LUISE. The result of word error rate is obtained by conducting greedy search with a CTC model. </figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>SFT</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">After the training on large-scale speech-only data, LUISE has developed strong speech representation capabilities. It outputs continuous representation containing rich speech and semantic information at a frame rate of 40ms. In order for AcLLM to better understand the corresponding text content in speech, we need to map the semantic information from the encoded representation into the semantic space of the LLM. To achieve this, we use the following two methods:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">In the model structure, we introduce a converter module to connect our audio encoder (LUISE) with the LLM (as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The converter includes a downsampling module and a linear projection layer. We find that different downsampling methods work equally well, so we utilize the most concise method: frame splicing. Specifically, we input 4 consecutive frames of speech representation to the linear layer after splicing them in the feature dimension. Consequently, the frame rate of the speech representations in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> inputted to the LLM is 160ms;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p">In terms of the training method, we adopt the strategy of "learnable audio encoder + learnable converter + fixed LLM", which maximizes the retention of the LLM’s rich semantic knowledge and reasoning abilities by keeping its parameters unchanged. The learnable audio encoder and converter parameters ensure that the semantic information contained in the speech representation is aligned to the semantic space of the LLM. During the training process, the cross-entropy loss function is used, with only the token positions that generate the transcribed text participating in the cross-entropy calculation;</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Context SFT</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">After training on large-scale speech-text pair data, our SFT model achieves strong performance on test sets covering multiple domains. However, the training manner of the SFT model determines that it lacks the ability to recognize ambiguous speech content given contextual information (contexts). These issues are more pronounced in scenarios involving accents (with speech ambiguity), and homonyms or rare words (with semantic ambiguity). Therefore, we introduce context-aware training and the method of joint beam search to enhance the model’s ability to utilize context effectively (an example is present in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.4 Context SFT ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i1.p1.1" class="ltx_p">Context-aware training: First, we use our internal LLM to generate contexts related to the transcription of speech. It performs better than using the history transcription in long-form speech as the contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> in our experiments. Using the generated natural language contexts could also provide more complete semantics than sampled words in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, thus enabling the learning of reasoning in addition to copying the relevant transcription content from contexts. Then, we build a dataset of &lt;context, speech, text&gt; triples, which are mixed with a certain proportion of general ASR data (speech-text pair data) for context-aware training. As shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, during context-aware training, we input the contexts and speech representations into the LLM. The goal of this training is to enhance the model’s ability to capture speech content-related clues from the contexts.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i2.p1.5" class="ltx_p">Joint beam search: We find that directly using the native beam search suffers from serious hallucination problem.
To address this, we propose a decoding strategy of joint beam search to alleviate this problem. Specifically, we use joint beam search to find the optimal score <math id="S3.I2.i2.p1.1.m1.3" class="ltx_Math" alttext="P_{\text{joint}}(\bm{y}|\bm{x},\bm{c})" display="inline"><semantics id="S3.I2.i2.p1.1.m1.3a"><mrow id="S3.I2.i2.p1.1.m1.3.3" xref="S3.I2.i2.p1.1.m1.3.3.cmml"><msub id="S3.I2.i2.p1.1.m1.3.3.3" xref="S3.I2.i2.p1.1.m1.3.3.3.cmml"><mi id="S3.I2.i2.p1.1.m1.3.3.3.2" xref="S3.I2.i2.p1.1.m1.3.3.3.2.cmml">P</mi><mtext id="S3.I2.i2.p1.1.m1.3.3.3.3" xref="S3.I2.i2.p1.1.m1.3.3.3.3a.cmml">joint</mtext></msub><mo lspace="0em" rspace="0em" id="S3.I2.i2.p1.1.m1.3.3.2" xref="S3.I2.i2.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S3.I2.i2.p1.1.m1.3.3.1.1" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.I2.i2.p1.1.m1.3.3.1.1.2" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.I2.i2.p1.1.m1.3.3.1.1.1" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.I2.i2.p1.1.m1.3.3.1.1.1.2" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.2.cmml">𝒚</mi><mo fence="false" id="S3.I2.i2.p1.1.m1.3.3.1.1.1.1" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.2" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml">𝒙</mi><mo id="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.2.1" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.I2.i2.p1.1.m1.2.2" xref="S3.I2.i2.p1.1.m1.2.2.cmml">𝒄</mi></mrow></mrow><mo stretchy="false" id="S3.I2.i2.p1.1.m1.3.3.1.1.3" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.3b"><apply id="S3.I2.i2.p1.1.m1.3.3.cmml" xref="S3.I2.i2.p1.1.m1.3.3"><times id="S3.I2.i2.p1.1.m1.3.3.2.cmml" xref="S3.I2.i2.p1.1.m1.3.3.2"></times><apply id="S3.I2.i2.p1.1.m1.3.3.3.cmml" xref="S3.I2.i2.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.I2.i2.p1.1.m1.3.3.3.1.cmml" xref="S3.I2.i2.p1.1.m1.3.3.3">subscript</csymbol><ci id="S3.I2.i2.p1.1.m1.3.3.3.2.cmml" xref="S3.I2.i2.p1.1.m1.3.3.3.2">𝑃</ci><ci id="S3.I2.i2.p1.1.m1.3.3.3.3a.cmml" xref="S3.I2.i2.p1.1.m1.3.3.3.3"><mtext mathsize="70%" id="S3.I2.i2.p1.1.m1.3.3.3.3.cmml" xref="S3.I2.i2.p1.1.m1.3.3.3.3">joint</mtext></ci></apply><apply id="S3.I2.i2.p1.1.m1.3.3.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.I2.i2.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.I2.i2.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.2">𝒚</ci><list id="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.I2.i2.p1.1.m1.3.3.1.1.1.3.2"><ci id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">𝒙</ci><ci id="S3.I2.i2.p1.1.m1.2.2.cmml" xref="S3.I2.i2.p1.1.m1.2.2">𝒄</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.3c">P_{\text{joint}}(\bm{y}|\bm{x},\bm{c})</annotation></semantics></math>, where <math id="S3.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="\bm{y}" display="inline"><semantics id="S3.I2.i2.p1.2.m2.1a"><mi id="S3.I2.i2.p1.2.m2.1.1" xref="S3.I2.i2.p1.2.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.2.m2.1b"><ci id="S3.I2.i2.p1.2.m2.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.2.m2.1c">\bm{y}</annotation></semantics></math> represents the predicted hypothesis, <math id="S3.I2.i2.p1.3.m3.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.I2.i2.p1.3.m3.1a"><mi id="S3.I2.i2.p1.3.m3.1.1" xref="S3.I2.i2.p1.3.m3.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.3.m3.1b"><ci id="S3.I2.i2.p1.3.m3.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.3.m3.1c">\bm{x}</annotation></semantics></math> is the speech information, and <math id="S3.I2.i2.p1.4.m4.1" class="ltx_Math" alttext="\bm{c}" display="inline"><semantics id="S3.I2.i2.p1.4.m4.1a"><mi id="S3.I2.i2.p1.4.m4.1.1" xref="S3.I2.i2.p1.4.m4.1.1.cmml">𝒄</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.4.m4.1b"><ci id="S3.I2.i2.p1.4.m4.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1">𝒄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.4.m4.1c">\bm{c}</annotation></semantics></math> is the given contextual information. The hyper-parameter <math id="S3.I2.i2.p1.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.I2.i2.p1.5.m5.1a"><mi id="S3.I2.i2.p1.5.m5.1.1" xref="S3.I2.i2.p1.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.5.m5.1b"><ci id="S3.I2.i2.p1.5.m5.1.1.cmml" xref="S3.I2.i2.p1.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.5.m5.1c">\alpha</annotation></semantics></math> is used to balance the importance of speech information and contextual information during the decoding:</p>
</div>
<div id="S3.I2.i2.p2" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="P_{\text{joint}}(\bm{y}|\bm{x},\bm{c})=\frac{\alpha}{1+\alpha}*P(\bm{y}|\bm{x},\bm{c})+\frac{1}{1+\alpha}*P(\bm{y}|\bm{x})" display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml"><msub id="S3.E1.m1.5.5.1.3" xref="S3.E1.m1.5.5.1.3.cmml"><mi id="S3.E1.m1.5.5.1.3.2" xref="S3.E1.m1.5.5.1.3.2.cmml">P</mi><mtext id="S3.E1.m1.5.5.1.3.3" xref="S3.E1.m1.5.5.1.3.3a.cmml">joint</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">𝒚</mi><mo fence="false" id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝒙</mi><mo id="S3.E1.m1.5.5.1.1.1.1.3.2.1" xref="S3.E1.m1.5.5.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝒄</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.4" xref="S3.E1.m1.7.7.4.cmml">=</mo><mrow id="S3.E1.m1.7.7.3" xref="S3.E1.m1.7.7.3.cmml"><mrow id="S3.E1.m1.6.6.2.1" xref="S3.E1.m1.6.6.2.1.cmml"><mrow id="S3.E1.m1.6.6.2.1.3" xref="S3.E1.m1.6.6.2.1.3.cmml"><mfrac id="S3.E1.m1.6.6.2.1.3.2" xref="S3.E1.m1.6.6.2.1.3.2.cmml"><mi id="S3.E1.m1.6.6.2.1.3.2.2" xref="S3.E1.m1.6.6.2.1.3.2.2.cmml">α</mi><mrow id="S3.E1.m1.6.6.2.1.3.2.3" xref="S3.E1.m1.6.6.2.1.3.2.3.cmml"><mn id="S3.E1.m1.6.6.2.1.3.2.3.2" xref="S3.E1.m1.6.6.2.1.3.2.3.2.cmml">1</mn><mo id="S3.E1.m1.6.6.2.1.3.2.3.1" xref="S3.E1.m1.6.6.2.1.3.2.3.1.cmml">+</mo><mi id="S3.E1.m1.6.6.2.1.3.2.3.3" xref="S3.E1.m1.6.6.2.1.3.2.3.3.cmml">α</mi></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.6.6.2.1.3.1" xref="S3.E1.m1.6.6.2.1.3.1.cmml">∗</mo><mi id="S3.E1.m1.6.6.2.1.3.3" xref="S3.E1.m1.6.6.2.1.3.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.2.1.2" xref="S3.E1.m1.6.6.2.1.2.cmml">​</mo><mrow id="S3.E1.m1.6.6.2.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.2.1.1.1.2" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.2.1.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.2.1.1.1.1.2" xref="S3.E1.m1.6.6.2.1.1.1.1.2.cmml">𝒚</mi><mo fence="false" id="S3.E1.m1.6.6.2.1.1.1.1.1" xref="S3.E1.m1.6.6.2.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.6.6.2.1.1.1.1.3.2" xref="S3.E1.m1.6.6.2.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">𝒙</mi><mo id="S3.E1.m1.6.6.2.1.1.1.1.3.2.1" xref="S3.E1.m1.6.6.2.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">𝒄</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.6.6.2.1.1.1.3" xref="S3.E1.m1.6.6.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.3.3" xref="S3.E1.m1.7.7.3.3.cmml">+</mo><mrow id="S3.E1.m1.7.7.3.2" xref="S3.E1.m1.7.7.3.2.cmml"><mrow id="S3.E1.m1.7.7.3.2.3" xref="S3.E1.m1.7.7.3.2.3.cmml"><mfrac id="S3.E1.m1.7.7.3.2.3.2" xref="S3.E1.m1.7.7.3.2.3.2.cmml"><mn id="S3.E1.m1.7.7.3.2.3.2.2" xref="S3.E1.m1.7.7.3.2.3.2.2.cmml">1</mn><mrow id="S3.E1.m1.7.7.3.2.3.2.3" xref="S3.E1.m1.7.7.3.2.3.2.3.cmml"><mn id="S3.E1.m1.7.7.3.2.3.2.3.2" xref="S3.E1.m1.7.7.3.2.3.2.3.2.cmml">1</mn><mo id="S3.E1.m1.7.7.3.2.3.2.3.1" xref="S3.E1.m1.7.7.3.2.3.2.3.1.cmml">+</mo><mi id="S3.E1.m1.7.7.3.2.3.2.3.3" xref="S3.E1.m1.7.7.3.2.3.2.3.3.cmml">α</mi></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.7.7.3.2.3.1" xref="S3.E1.m1.7.7.3.2.3.1.cmml">∗</mo><mi id="S3.E1.m1.7.7.3.2.3.3" xref="S3.E1.m1.7.7.3.2.3.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.3.2.2" xref="S3.E1.m1.7.7.3.2.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.3.2.1.1" xref="S3.E1.m1.7.7.3.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.3.2.1.1.2" xref="S3.E1.m1.7.7.3.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.3.2.1.1.1" xref="S3.E1.m1.7.7.3.2.1.1.1.cmml"><mi id="S3.E1.m1.7.7.3.2.1.1.1.2" xref="S3.E1.m1.7.7.3.2.1.1.1.2.cmml">𝒚</mi><mo fence="false" id="S3.E1.m1.7.7.3.2.1.1.1.1" xref="S3.E1.m1.7.7.3.2.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.7.7.3.2.1.1.1.3" xref="S3.E1.m1.7.7.3.2.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.E1.m1.7.7.3.2.1.1.3" xref="S3.E1.m1.7.7.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7"><eq id="S3.E1.m1.7.7.4.cmml" xref="S3.E1.m1.7.7.4"></eq><apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1"><times id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2"></times><apply id="S3.E1.m1.5.5.1.3.cmml" xref="S3.E1.m1.5.5.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.3.1.cmml" xref="S3.E1.m1.5.5.1.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.3.2.cmml" xref="S3.E1.m1.5.5.1.3.2">𝑃</ci><ci id="S3.E1.m1.5.5.1.3.3a.cmml" xref="S3.E1.m1.5.5.1.3.3"><mtext mathsize="70%" id="S3.E1.m1.5.5.1.3.3.cmml" xref="S3.E1.m1.5.5.1.3.3">joint</mtext></ci></apply><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2">𝒚</ci><list id="S3.E1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝒙</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝒄</ci></list></apply></apply><apply id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.3"><plus id="S3.E1.m1.7.7.3.3.cmml" xref="S3.E1.m1.7.7.3.3"></plus><apply id="S3.E1.m1.6.6.2.1.cmml" xref="S3.E1.m1.6.6.2.1"><times id="S3.E1.m1.6.6.2.1.2.cmml" xref="S3.E1.m1.6.6.2.1.2"></times><apply id="S3.E1.m1.6.6.2.1.3.cmml" xref="S3.E1.m1.6.6.2.1.3"><times id="S3.E1.m1.6.6.2.1.3.1.cmml" xref="S3.E1.m1.6.6.2.1.3.1"></times><apply id="S3.E1.m1.6.6.2.1.3.2.cmml" xref="S3.E1.m1.6.6.2.1.3.2"><divide id="S3.E1.m1.6.6.2.1.3.2.1.cmml" xref="S3.E1.m1.6.6.2.1.3.2"></divide><ci id="S3.E1.m1.6.6.2.1.3.2.2.cmml" xref="S3.E1.m1.6.6.2.1.3.2.2">𝛼</ci><apply id="S3.E1.m1.6.6.2.1.3.2.3.cmml" xref="S3.E1.m1.6.6.2.1.3.2.3"><plus id="S3.E1.m1.6.6.2.1.3.2.3.1.cmml" xref="S3.E1.m1.6.6.2.1.3.2.3.1"></plus><cn type="integer" id="S3.E1.m1.6.6.2.1.3.2.3.2.cmml" xref="S3.E1.m1.6.6.2.1.3.2.3.2">1</cn><ci id="S3.E1.m1.6.6.2.1.3.2.3.3.cmml" xref="S3.E1.m1.6.6.2.1.3.2.3.3">𝛼</ci></apply></apply><ci id="S3.E1.m1.6.6.2.1.3.3.cmml" xref="S3.E1.m1.6.6.2.1.3.3">𝑃</ci></apply><apply id="S3.E1.m1.6.6.2.1.1.1.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.2.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.6.6.2.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.2">𝒚</ci><list id="S3.E1.m1.6.6.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.2.1.1.1.1.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝒙</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝒄</ci></list></apply></apply><apply id="S3.E1.m1.7.7.3.2.cmml" xref="S3.E1.m1.7.7.3.2"><times id="S3.E1.m1.7.7.3.2.2.cmml" xref="S3.E1.m1.7.7.3.2.2"></times><apply id="S3.E1.m1.7.7.3.2.3.cmml" xref="S3.E1.m1.7.7.3.2.3"><times id="S3.E1.m1.7.7.3.2.3.1.cmml" xref="S3.E1.m1.7.7.3.2.3.1"></times><apply id="S3.E1.m1.7.7.3.2.3.2.cmml" xref="S3.E1.m1.7.7.3.2.3.2"><divide id="S3.E1.m1.7.7.3.2.3.2.1.cmml" xref="S3.E1.m1.7.7.3.2.3.2"></divide><cn type="integer" id="S3.E1.m1.7.7.3.2.3.2.2.cmml" xref="S3.E1.m1.7.7.3.2.3.2.2">1</cn><apply id="S3.E1.m1.7.7.3.2.3.2.3.cmml" xref="S3.E1.m1.7.7.3.2.3.2.3"><plus id="S3.E1.m1.7.7.3.2.3.2.3.1.cmml" xref="S3.E1.m1.7.7.3.2.3.2.3.1"></plus><cn type="integer" id="S3.E1.m1.7.7.3.2.3.2.3.2.cmml" xref="S3.E1.m1.7.7.3.2.3.2.3.2">1</cn><ci id="S3.E1.m1.7.7.3.2.3.2.3.3.cmml" xref="S3.E1.m1.7.7.3.2.3.2.3.3">𝛼</ci></apply></apply><ci id="S3.E1.m1.7.7.3.2.3.3.cmml" xref="S3.E1.m1.7.7.3.2.3.3">𝑃</ci></apply><apply id="S3.E1.m1.7.7.3.2.1.1.1.cmml" xref="S3.E1.m1.7.7.3.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.7.7.3.2.1.1.1.1.cmml" xref="S3.E1.m1.7.7.3.2.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.7.7.3.2.1.1.1.2.cmml" xref="S3.E1.m1.7.7.3.2.1.1.1.2">𝒚</ci><ci id="S3.E1.m1.7.7.3.2.1.1.1.3.cmml" xref="S3.E1.m1.7.7.3.2.1.1.1.3">𝒙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">P_{\text{joint}}(\bm{y}|\bm{x},\bm{c})=\frac{\alpha}{1+\alpha}*P(\bm{y}|\bm{x},\bm{c})+\frac{1}{1+\alpha}*P(\bm{y}|\bm{x})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.I2.i2.p3" class="ltx_para ltx_noindent">
<p id="S3.I2.i2.p3.1" class="ltx_p">Simultaneously, we introduce a pruning strategy that first uses context-independent score <math id="S3.I2.i2.p3.1.m1.1" class="ltx_Math" alttext="P(\bm{y}|\bm{x})" display="inline"><semantics id="S3.I2.i2.p3.1.m1.1a"><mrow id="S3.I2.i2.p3.1.m1.1.1" xref="S3.I2.i2.p3.1.m1.1.1.cmml"><mi id="S3.I2.i2.p3.1.m1.1.1.3" xref="S3.I2.i2.p3.1.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.I2.i2.p3.1.m1.1.1.2" xref="S3.I2.i2.p3.1.m1.1.1.2.cmml">​</mo><mrow id="S3.I2.i2.p3.1.m1.1.1.1.1" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.I2.i2.p3.1.m1.1.1.1.1.2" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.I2.i2.p3.1.m1.1.1.1.1.1" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.I2.i2.p3.1.m1.1.1.1.1.1.2" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.2.cmml">𝒚</mi><mo fence="false" id="S3.I2.i2.p3.1.m1.1.1.1.1.1.1" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.I2.i2.p3.1.m1.1.1.1.1.1.3" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.I2.i2.p3.1.m1.1.1.1.1.3" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p3.1.m1.1b"><apply id="S3.I2.i2.p3.1.m1.1.1.cmml" xref="S3.I2.i2.p3.1.m1.1.1"><times id="S3.I2.i2.p3.1.m1.1.1.2.cmml" xref="S3.I2.i2.p3.1.m1.1.1.2"></times><ci id="S3.I2.i2.p3.1.m1.1.1.3.cmml" xref="S3.I2.i2.p3.1.m1.1.1.3">𝑃</ci><apply id="S3.I2.i2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.I2.i2.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.I2.i2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.I2.i2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.2">𝒚</ci><ci id="S3.I2.i2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.I2.i2.p3.1.m1.1.1.1.1.1.3">𝒙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p3.1.m1.1c">P(\bm{y}|\bm{x})</annotation></semantics></math> to filter out acoustically implausible candidate tokens, and then applies joint beam search to the remaining candidate tokens. The pruning strategy plays an important role in alleviating hallucination.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2407.04675/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An example of transcribing speech with or without contexts.</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>RL</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">Since the training in the SFT and Context SFT stages is based on the cross-entropy objective function, there is a mismatch with the evaluation metrics used during inference (e.g. WER). With the successful development of reinforcement learning (RL), it can learn relatively optimal decision-making strategies in sequence modeling tasks. Therefore, we introduce the RL stage by constructing a reward function based on ASR metrics.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p">Word error rate (WER) is often considered a core metric for evaluating the performance of ASR models, but certain parts of content (e.g. keyword) in a sentence plays a more crucial role in the understanding of the whole sentence. Therefore, we also introduce the metric of weighted WER (WWER) as an additional reward function, emphasizing the importance of keyword errors. Specifically, we apply minimum word error rate (MWER) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> as another training objective interpolated with the cross-entropy objective <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{CE}}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><msub id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3a.cmml">CE</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">ℒ</ci><ci id="S3.SS5.p2.1.m1.1.1.3a.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3">CE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\mathcal{L}_{\text{CE}}</annotation></semantics></math> in our RL stage:</p>
</div>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\mathcal{L}^{\text{N-best}}_{\text{mwer}}(\bm{x},\bm{y^{*}})=\frac{1}{N}\sum_{\bm{y_{i}}\in\text{N-best(x, N)}}\hat{P}(\bm{y_{i}}|\bm{x})(\mathcal{W}(\bm{y_{i}},\bm{y^{*}})-\bar{W})+\lambda\mathcal{L}_{\text{CE}}" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><msubsup id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.3.2.2" xref="S3.E2.m1.2.2.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E2.m1.2.2.1.3.3" xref="S3.E2.m1.2.2.1.3.3a.cmml">mwer</mtext><mtext id="S3.E2.m1.2.2.1.3.2.3" xref="S3.E2.m1.2.2.1.3.2.3a.cmml">N-best</mtext></msubsup><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝒙</mi><mo id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.2.cmml">,</mo><msup id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.4" xref="S3.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml">=</mo><mrow id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml"><mrow id="S3.E2.m1.4.4.3.2" xref="S3.E2.m1.4.4.3.2.cmml"><mfrac id="S3.E2.m1.4.4.3.2.4" xref="S3.E2.m1.4.4.3.2.4.cmml"><mn id="S3.E2.m1.4.4.3.2.4.2" xref="S3.E2.m1.4.4.3.2.4.2.cmml">1</mn><mi id="S3.E2.m1.4.4.3.2.4.3" xref="S3.E2.m1.4.4.3.2.4.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.2.3" xref="S3.E2.m1.4.4.3.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.3.2.2" xref="S3.E2.m1.4.4.3.2.2.cmml"><munder id="S3.E2.m1.4.4.3.2.2.3" xref="S3.E2.m1.4.4.3.2.2.3.cmml"><mo movablelimits="false" id="S3.E2.m1.4.4.3.2.2.3.2" xref="S3.E2.m1.4.4.3.2.2.3.2.cmml">∑</mo><mrow id="S3.E2.m1.4.4.3.2.2.3.3" xref="S3.E2.m1.4.4.3.2.2.3.3.cmml"><msub id="S3.E2.m1.4.4.3.2.2.3.3.2" xref="S3.E2.m1.4.4.3.2.2.3.3.2.cmml"><mi id="S3.E2.m1.4.4.3.2.2.3.3.2.2" xref="S3.E2.m1.4.4.3.2.2.3.3.2.2.cmml">𝒚</mi><mi id="S3.E2.m1.4.4.3.2.2.3.3.2.3" xref="S3.E2.m1.4.4.3.2.2.3.3.2.3.cmml">𝒊</mi></msub><mo id="S3.E2.m1.4.4.3.2.2.3.3.1" xref="S3.E2.m1.4.4.3.2.2.3.3.1.cmml">∈</mo><mtext id="S3.E2.m1.4.4.3.2.2.3.3.3" xref="S3.E2.m1.4.4.3.2.2.3.3.3a.cmml">N-best(x, N)</mtext></mrow></munder><mrow id="S3.E2.m1.4.4.3.2.2.2" xref="S3.E2.m1.4.4.3.2.2.2.cmml"><mover accent="true" id="S3.E2.m1.4.4.3.2.2.2.4" xref="S3.E2.m1.4.4.3.2.2.2.4.cmml"><mi id="S3.E2.m1.4.4.3.2.2.2.4.2" xref="S3.E2.m1.4.4.3.2.2.2.4.2.cmml">P</mi><mo id="S3.E2.m1.4.4.3.2.2.2.4.1" xref="S3.E2.m1.4.4.3.2.2.2.4.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.2.2.2.3" xref="S3.E2.m1.4.4.3.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.3.3.2.1.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.2.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.3.cmml">𝒊</mi></msub><mo fence="false" id="S3.E2.m1.3.3.2.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.E2.m1.3.3.2.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.2.2.2.3a" xref="S3.E2.m1.4.4.3.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.3.2.2.2.2.1" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.3.2.2.2.2.1.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.3.2.2.2.2.1.1" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.cmml"><mrow id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.4" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.4.cmml">𝒲</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.2.cmml">𝒚</mi><mi id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.3.cmml">𝒊</mi></msub><mo id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.4" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.3.cmml">,</mo><msup id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.5" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.3.2.2.2.2.1.1.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.3.cmml">−</mo><mover accent="true" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.cmml"><mi id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.2" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.2.cmml">W</mi><mo id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.1" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.1.cmml">¯</mo></mover></mrow><mo stretchy="false" id="S3.E2.m1.4.4.3.2.2.2.2.1.3" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.3.cmml">+</mo><mrow id="S3.E2.m1.4.4.3.4" xref="S3.E2.m1.4.4.3.4.cmml"><mi id="S3.E2.m1.4.4.3.4.2" xref="S3.E2.m1.4.4.3.4.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.4.1" xref="S3.E2.m1.4.4.3.4.1.cmml">​</mo><msub id="S3.E2.m1.4.4.3.4.3" xref="S3.E2.m1.4.4.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.3.4.3.2" xref="S3.E2.m1.4.4.3.4.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.4.4.3.4.3.3" xref="S3.E2.m1.4.4.3.4.3.3a.cmml">CE</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"></eq><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><apply id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.3.1.cmml" xref="S3.E2.m1.2.2.1.3">subscript</csymbol><apply id="S3.E2.m1.2.2.1.3.2.cmml" xref="S3.E2.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.3">superscript</csymbol><ci id="S3.E2.m1.2.2.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.3.2.2">ℒ</ci><ci id="S3.E2.m1.2.2.1.3.2.3a.cmml" xref="S3.E2.m1.2.2.1.3.2.3"><mtext mathsize="70%" id="S3.E2.m1.2.2.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.3.2.3">N-best</mtext></ci></apply><ci id="S3.E2.m1.2.2.1.3.3a.cmml" xref="S3.E2.m1.2.2.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.2.2.1.3.3.cmml" xref="S3.E2.m1.2.2.1.3.3">mwer</mtext></ci></apply><interval closure="open" id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝒙</ci><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2">𝒚</ci><times id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3"></times></apply></interval></apply><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"><plus id="S3.E2.m1.4.4.3.3.cmml" xref="S3.E2.m1.4.4.3.3"></plus><apply id="S3.E2.m1.4.4.3.2.cmml" xref="S3.E2.m1.4.4.3.2"><times id="S3.E2.m1.4.4.3.2.3.cmml" xref="S3.E2.m1.4.4.3.2.3"></times><apply id="S3.E2.m1.4.4.3.2.4.cmml" xref="S3.E2.m1.4.4.3.2.4"><divide id="S3.E2.m1.4.4.3.2.4.1.cmml" xref="S3.E2.m1.4.4.3.2.4"></divide><cn type="integer" id="S3.E2.m1.4.4.3.2.4.2.cmml" xref="S3.E2.m1.4.4.3.2.4.2">1</cn><ci id="S3.E2.m1.4.4.3.2.4.3.cmml" xref="S3.E2.m1.4.4.3.2.4.3">𝑁</ci></apply><apply id="S3.E2.m1.4.4.3.2.2.cmml" xref="S3.E2.m1.4.4.3.2.2"><apply id="S3.E2.m1.4.4.3.2.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.2.2.3.1.cmml" xref="S3.E2.m1.4.4.3.2.2.3">subscript</csymbol><sum id="S3.E2.m1.4.4.3.2.2.3.2.cmml" xref="S3.E2.m1.4.4.3.2.2.3.2"></sum><apply id="S3.E2.m1.4.4.3.2.2.3.3.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3"><in id="S3.E2.m1.4.4.3.2.2.3.3.1.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.1"></in><apply id="S3.E2.m1.4.4.3.2.2.3.3.2.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.2.2.3.3.2.1.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.3.2.2.3.3.2.2.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.2.2">𝒚</ci><ci id="S3.E2.m1.4.4.3.2.2.3.3.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.2.3">𝒊</ci></apply><ci id="S3.E2.m1.4.4.3.2.2.3.3.3a.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.3.2.2.3.3.3.cmml" xref="S3.E2.m1.4.4.3.2.2.3.3.3">N-best(x, N)</mtext></ci></apply></apply><apply id="S3.E2.m1.4.4.3.2.2.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2"><times id="S3.E2.m1.4.4.3.2.2.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.3"></times><apply id="S3.E2.m1.4.4.3.2.2.2.4.cmml" xref="S3.E2.m1.4.4.3.2.2.2.4"><ci id="S3.E2.m1.4.4.3.2.2.2.4.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.4.1">^</ci><ci id="S3.E2.m1.4.4.3.2.2.2.4.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.4.2">𝑃</ci></apply><apply id="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.2">𝒚</ci><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.2.3">𝒊</ci></apply><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.3">𝒙</ci></apply><apply id="S3.E2.m1.4.4.3.2.2.2.2.1.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1"><minus id="S3.E2.m1.4.4.3.2.2.2.2.1.1.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.3"></minus><apply id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2"><times id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.3"></times><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.4.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.4">𝒲</ci><interval closure="open" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2"><apply id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.2">𝒚</ci><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.1.1.1.1.3">𝒊</ci></apply><apply id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.2">𝒚</ci><times id="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.2.2.2.2.3"></times></apply></interval></apply><apply id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4"><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.1.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.1">¯</ci><ci id="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.2.cmml" xref="S3.E2.m1.4.4.3.2.2.2.2.1.1.4.2">𝑊</ci></apply></apply></apply></apply></apply><apply id="S3.E2.m1.4.4.3.4.cmml" xref="S3.E2.m1.4.4.3.4"><times id="S3.E2.m1.4.4.3.4.1.cmml" xref="S3.E2.m1.4.4.3.4.1"></times><ci id="S3.E2.m1.4.4.3.4.2.cmml" xref="S3.E2.m1.4.4.3.4.2">𝜆</ci><apply id="S3.E2.m1.4.4.3.4.3.cmml" xref="S3.E2.m1.4.4.3.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.4.3.1.cmml" xref="S3.E2.m1.4.4.3.4.3">subscript</csymbol><ci id="S3.E2.m1.4.4.3.4.3.2.cmml" xref="S3.E2.m1.4.4.3.4.3.2">ℒ</ci><ci id="S3.E2.m1.4.4.3.4.3.3a.cmml" xref="S3.E2.m1.4.4.3.4.3.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.3.4.3.3.cmml" xref="S3.E2.m1.4.4.3.4.3.3">CE</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathcal{L}^{\text{N-best}}_{\text{mwer}}(\bm{x},\bm{y^{*}})=\frac{1}{N}\sum_{\bm{y_{i}}\in\text{N-best(x, N)}}\hat{P}(\bm{y_{i}}|\bm{x})(\mathcal{W}(\bm{y_{i}},\bm{y^{*}})-\bar{W})+\lambda\mathcal{L}_{\text{CE}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p4" class="ltx_para ltx_noindent">
<p id="S3.SS5.p4.7" class="ltx_p">where <math id="S3.SS5.p4.1.m1.2" class="ltx_Math" alttext="\mathcal{W}(\bm{y^{*}},\bm{y_{i}})" display="inline"><semantics id="S3.SS5.p4.1.m1.2a"><mrow id="S3.SS5.p4.1.m1.2.2" xref="S3.SS5.p4.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p4.1.m1.2.2.4" xref="S3.SS5.p4.1.m1.2.2.4.cmml">𝒲</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p4.1.m1.2.2.3" xref="S3.SS5.p4.1.m1.2.2.3.cmml">​</mo><mrow id="S3.SS5.p4.1.m1.2.2.2.2" xref="S3.SS5.p4.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS5.p4.1.m1.2.2.2.2.3" xref="S3.SS5.p4.1.m1.2.2.2.3.cmml">(</mo><msup id="S3.SS5.p4.1.m1.1.1.1.1.1" xref="S3.SS5.p4.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS5.p4.1.m1.1.1.1.1.1.2" xref="S3.SS5.p4.1.m1.1.1.1.1.1.2.cmml">𝒚</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS5.p4.1.m1.1.1.1.1.1.3" xref="S3.SS5.p4.1.m1.1.1.1.1.1.3.cmml">∗</mo></msup><mo id="S3.SS5.p4.1.m1.2.2.2.2.4" xref="S3.SS5.p4.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS5.p4.1.m1.2.2.2.2.2" xref="S3.SS5.p4.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS5.p4.1.m1.2.2.2.2.2.2" xref="S3.SS5.p4.1.m1.2.2.2.2.2.2.cmml">𝒚</mi><mi id="S3.SS5.p4.1.m1.2.2.2.2.2.3" xref="S3.SS5.p4.1.m1.2.2.2.2.2.3.cmml">𝒊</mi></msub><mo stretchy="false" id="S3.SS5.p4.1.m1.2.2.2.2.5" xref="S3.SS5.p4.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.2b"><apply id="S3.SS5.p4.1.m1.2.2.cmml" xref="S3.SS5.p4.1.m1.2.2"><times id="S3.SS5.p4.1.m1.2.2.3.cmml" xref="S3.SS5.p4.1.m1.2.2.3"></times><ci id="S3.SS5.p4.1.m1.2.2.4.cmml" xref="S3.SS5.p4.1.m1.2.2.4">𝒲</ci><interval closure="open" id="S3.SS5.p4.1.m1.2.2.2.3.cmml" xref="S3.SS5.p4.1.m1.2.2.2.2"><apply id="S3.SS5.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS5.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS5.p4.1.m1.1.1.1.1.1.2">𝒚</ci><times id="S3.SS5.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS5.p4.1.m1.1.1.1.1.1.3"></times></apply><apply id="S3.SS5.p4.1.m1.2.2.2.2.2.cmml" xref="S3.SS5.p4.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS5.p4.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS5.p4.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS5.p4.1.m1.2.2.2.2.2.2">𝒚</ci><ci id="S3.SS5.p4.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS5.p4.1.m1.2.2.2.2.2.3">𝒊</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.2c">\mathcal{W}(\bm{y^{*}},\bm{y_{i}})</annotation></semantics></math> represents the WER value or WWER value (where the weight of the keyword error is increased) between the ground-truth (<math id="S3.SS5.p4.2.m2.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S3.SS5.p4.2.m2.1a"><msup id="S3.SS5.p4.2.m2.1.1" xref="S3.SS5.p4.2.m2.1.1.cmml"><mi id="S3.SS5.p4.2.m2.1.1.2" xref="S3.SS5.p4.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS5.p4.2.m2.1.1.3" xref="S3.SS5.p4.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.2.m2.1b"><apply id="S3.SS5.p4.2.m2.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.2.m2.1.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1">superscript</csymbol><ci id="S3.SS5.p4.2.m2.1.1.2.cmml" xref="S3.SS5.p4.2.m2.1.1.2">𝑦</ci><times id="S3.SS5.p4.2.m2.1.1.3.cmml" xref="S3.SS5.p4.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.2.m2.1c">y^{*}</annotation></semantics></math>) and each hypothesis <math id="S3.SS5.p4.3.m3.1" class="ltx_Math" alttext="\bm{y_{i}}" display="inline"><semantics id="S3.SS5.p4.3.m3.1a"><msub id="S3.SS5.p4.3.m3.1.1" xref="S3.SS5.p4.3.m3.1.1.cmml"><mi id="S3.SS5.p4.3.m3.1.1.2" xref="S3.SS5.p4.3.m3.1.1.2.cmml">𝒚</mi><mi id="S3.SS5.p4.3.m3.1.1.3" xref="S3.SS5.p4.3.m3.1.1.3.cmml">𝒊</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.3.m3.1b"><apply id="S3.SS5.p4.3.m3.1.1.cmml" xref="S3.SS5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.3.m3.1.1.1.cmml" xref="S3.SS5.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p4.3.m3.1.1.2.cmml" xref="S3.SS5.p4.3.m3.1.1.2">𝒚</ci><ci id="S3.SS5.p4.3.m3.1.1.3.cmml" xref="S3.SS5.p4.3.m3.1.1.3">𝒊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.3.m3.1c">\bm{y_{i}}</annotation></semantics></math> in <math id="S3.SS5.p4.4.m4.2" class="ltx_Math" alttext="\text{N-best}(x,N)" display="inline"><semantics id="S3.SS5.p4.4.m4.2a"><mrow id="S3.SS5.p4.4.m4.2.3" xref="S3.SS5.p4.4.m4.2.3.cmml"><mtext id="S3.SS5.p4.4.m4.2.3.2" xref="S3.SS5.p4.4.m4.2.3.2a.cmml">N-best</mtext><mo lspace="0em" rspace="0em" id="S3.SS5.p4.4.m4.2.3.1" xref="S3.SS5.p4.4.m4.2.3.1.cmml">​</mo><mrow id="S3.SS5.p4.4.m4.2.3.3.2" xref="S3.SS5.p4.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS5.p4.4.m4.2.3.3.2.1" xref="S3.SS5.p4.4.m4.2.3.3.1.cmml">(</mo><mi id="S3.SS5.p4.4.m4.1.1" xref="S3.SS5.p4.4.m4.1.1.cmml">x</mi><mo id="S3.SS5.p4.4.m4.2.3.3.2.2" xref="S3.SS5.p4.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS5.p4.4.m4.2.2" xref="S3.SS5.p4.4.m4.2.2.cmml">N</mi><mo stretchy="false" id="S3.SS5.p4.4.m4.2.3.3.2.3" xref="S3.SS5.p4.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.4.m4.2b"><apply id="S3.SS5.p4.4.m4.2.3.cmml" xref="S3.SS5.p4.4.m4.2.3"><times id="S3.SS5.p4.4.m4.2.3.1.cmml" xref="S3.SS5.p4.4.m4.2.3.1"></times><ci id="S3.SS5.p4.4.m4.2.3.2a.cmml" xref="S3.SS5.p4.4.m4.2.3.2"><mtext id="S3.SS5.p4.4.m4.2.3.2.cmml" xref="S3.SS5.p4.4.m4.2.3.2">N-best</mtext></ci><interval closure="open" id="S3.SS5.p4.4.m4.2.3.3.1.cmml" xref="S3.SS5.p4.4.m4.2.3.3.2"><ci id="S3.SS5.p4.4.m4.1.1.cmml" xref="S3.SS5.p4.4.m4.1.1">𝑥</ci><ci id="S3.SS5.p4.4.m4.2.2.cmml" xref="S3.SS5.p4.4.m4.2.2">𝑁</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.4.m4.2c">\text{N-best}(x,N)</annotation></semantics></math>. <math id="S3.SS5.p4.5.m5.1" class="ltx_Math" alttext="\bar{W}" display="inline"><semantics id="S3.SS5.p4.5.m5.1a"><mover accent="true" id="S3.SS5.p4.5.m5.1.1" xref="S3.SS5.p4.5.m5.1.1.cmml"><mi id="S3.SS5.p4.5.m5.1.1.2" xref="S3.SS5.p4.5.m5.1.1.2.cmml">W</mi><mo id="S3.SS5.p4.5.m5.1.1.1" xref="S3.SS5.p4.5.m5.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.5.m5.1b"><apply id="S3.SS5.p4.5.m5.1.1.cmml" xref="S3.SS5.p4.5.m5.1.1"><ci id="S3.SS5.p4.5.m5.1.1.1.cmml" xref="S3.SS5.p4.5.m5.1.1.1">¯</ci><ci id="S3.SS5.p4.5.m5.1.1.2.cmml" xref="S3.SS5.p4.5.m5.1.1.2">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.5.m5.1c">\bar{W}</annotation></semantics></math> represents the average WER or WWER of N-best hypotheses. <math id="S3.SS5.p4.6.m6.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS5.p4.6.m6.1a"><mi id="S3.SS5.p4.6.m6.1.1" xref="S3.SS5.p4.6.m6.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.6.m6.1b"><ci id="S3.SS5.p4.6.m6.1.1.cmml" xref="S3.SS5.p4.6.m6.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.6.m6.1c">\lambda</annotation></semantics></math> is the interpolation coefficient. <math id="S3.SS5.p4.7.m7.1" class="ltx_Math" alttext="\hat{P}(\bm{y_{i}}|\bm{x})" display="inline"><semantics id="S3.SS5.p4.7.m7.1a"><mrow id="S3.SS5.p4.7.m7.1.1" xref="S3.SS5.p4.7.m7.1.1.cmml"><mover accent="true" id="S3.SS5.p4.7.m7.1.1.3" xref="S3.SS5.p4.7.m7.1.1.3.cmml"><mi id="S3.SS5.p4.7.m7.1.1.3.2" xref="S3.SS5.p4.7.m7.1.1.3.2.cmml">P</mi><mo id="S3.SS5.p4.7.m7.1.1.3.1" xref="S3.SS5.p4.7.m7.1.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.SS5.p4.7.m7.1.1.2" xref="S3.SS5.p4.7.m7.1.1.2.cmml">​</mo><mrow id="S3.SS5.p4.7.m7.1.1.1.1" xref="S3.SS5.p4.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.p4.7.m7.1.1.1.1.2" xref="S3.SS5.p4.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.p4.7.m7.1.1.1.1.1" xref="S3.SS5.p4.7.m7.1.1.1.1.1.cmml"><msub id="S3.SS5.p4.7.m7.1.1.1.1.1.2" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2.cmml"><mi id="S3.SS5.p4.7.m7.1.1.1.1.1.2.2" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.SS5.p4.7.m7.1.1.1.1.1.2.3" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2.3.cmml">𝒊</mi></msub><mo fence="false" id="S3.SS5.p4.7.m7.1.1.1.1.1.1" xref="S3.SS5.p4.7.m7.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS5.p4.7.m7.1.1.1.1.1.3" xref="S3.SS5.p4.7.m7.1.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.SS5.p4.7.m7.1.1.1.1.3" xref="S3.SS5.p4.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.7.m7.1b"><apply id="S3.SS5.p4.7.m7.1.1.cmml" xref="S3.SS5.p4.7.m7.1.1"><times id="S3.SS5.p4.7.m7.1.1.2.cmml" xref="S3.SS5.p4.7.m7.1.1.2"></times><apply id="S3.SS5.p4.7.m7.1.1.3.cmml" xref="S3.SS5.p4.7.m7.1.1.3"><ci id="S3.SS5.p4.7.m7.1.1.3.1.cmml" xref="S3.SS5.p4.7.m7.1.1.3.1">^</ci><ci id="S3.SS5.p4.7.m7.1.1.3.2.cmml" xref="S3.SS5.p4.7.m7.1.1.3.2">𝑃</ci></apply><apply id="S3.SS5.p4.7.m7.1.1.1.1.1.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1"><csymbol cd="latexml" id="S3.SS5.p4.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS5.p4.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p4.7.m7.1.1.1.1.1.2.1.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS5.p4.7.m7.1.1.1.1.1.2.2.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2.2">𝒚</ci><ci id="S3.SS5.p4.7.m7.1.1.1.1.1.2.3.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.2.3">𝒊</ci></apply><ci id="S3.SS5.p4.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS5.p4.7.m7.1.1.1.1.1.3">𝒙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.7.m7.1c">\hat{P}(\bm{y_{i}}|\bm{x})</annotation></semantics></math> represents the normalized likelihood probability of hypotheses, which is calculated as follows:</p>
</div>
<div id="S3.SS5.p5" class="ltx_para ltx_noindent">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="\hat{P}(\bm{y_{i}}|\bm{x})=\frac{P(\bm{y_{i}}|\bm{x})}{\sum_{\bm{y_{i}}\in\text{N-best(x, N)}}P(\bm{y_{i}}|\bm{x})}" display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.cmml"><mover accent="true" id="S3.E3.m1.3.3.1.3" xref="S3.E3.m1.3.3.1.3.cmml"><mi id="S3.E3.m1.3.3.1.3.2" xref="S3.E3.m1.3.3.1.3.2.cmml">P</mi><mo id="S3.E3.m1.3.3.1.3.1" xref="S3.E3.m1.3.3.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.3.3.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.2.3.cmml">𝒊</mi></msub><mo fence="false" id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.2" xref="S3.E3.m1.3.3.2.cmml">=</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml">𝒊</mi></msub><mo fence="false" id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><msub id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml"><mo id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml"><msub id="S3.E3.m1.2.2.2.2.3.2" xref="S3.E3.m1.2.2.2.2.3.2.cmml"><mi id="S3.E3.m1.2.2.2.2.3.2.2" xref="S3.E3.m1.2.2.2.2.3.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.2.2.2.2.3.2.3" xref="S3.E3.m1.2.2.2.2.3.2.3.cmml">𝒊</mi></msub><mo id="S3.E3.m1.2.2.2.2.3.1" xref="S3.E3.m1.2.2.2.2.3.1.cmml">∈</mo><mtext id="S3.E3.m1.2.2.2.2.3.3" xref="S3.E3.m1.2.2.2.2.3.3a.cmml">N-best(x, N)</mtext></mrow></msub><mrow id="S3.E3.m1.2.2.2.1" xref="S3.E3.m1.2.2.2.1.cmml"><mi id="S3.E3.m1.2.2.2.1.3" xref="S3.E3.m1.2.2.2.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.1.2" xref="S3.E3.m1.2.2.2.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.1.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.2.1.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml"><msub id="S3.E3.m1.2.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.1.2.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.1.2.2" xref="S3.E3.m1.2.2.2.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.2.2.2.1.1.1.1.2.3" xref="S3.E3.m1.2.2.2.1.1.1.1.2.3.cmml">𝒊</mi></msub><mo fence="false" id="S3.E3.m1.2.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.2.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.1.3.cmml">𝒙</mi></mrow><mo stretchy="false" id="S3.E3.m1.2.2.2.1.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><eq id="S3.E3.m1.3.3.2.cmml" xref="S3.E3.m1.3.3.2"></eq><apply id="S3.E3.m1.3.3.1.cmml" xref="S3.E3.m1.3.3.1"><times id="S3.E3.m1.3.3.1.2.cmml" xref="S3.E3.m1.3.3.1.2"></times><apply id="S3.E3.m1.3.3.1.3.cmml" xref="S3.E3.m1.3.3.1.3"><ci id="S3.E3.m1.3.3.1.3.1.cmml" xref="S3.E3.m1.3.3.1.3.1">^</ci><ci id="S3.E3.m1.3.3.1.3.2.cmml" xref="S3.E3.m1.3.3.1.3.2">𝑃</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2.2">𝒚</ci><ci id="S3.E3.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2.3">𝒊</ci></apply><ci id="S3.E3.m1.3.3.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3">𝒙</ci></apply></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝑃</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝒚</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">𝒊</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝒙</ci></apply></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><apply id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2">subscript</csymbol><sum id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2"></sum><apply id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.3"><in id="S3.E3.m1.2.2.2.2.3.1.cmml" xref="S3.E3.m1.2.2.2.2.3.1"></in><apply id="S3.E3.m1.2.2.2.2.3.2.cmml" xref="S3.E3.m1.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.3.2.1.cmml" xref="S3.E3.m1.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.3.2.2.cmml" xref="S3.E3.m1.2.2.2.2.3.2.2">𝒚</ci><ci id="S3.E3.m1.2.2.2.2.3.2.3.cmml" xref="S3.E3.m1.2.2.2.2.3.2.3">𝒊</ci></apply><ci id="S3.E3.m1.2.2.2.2.3.3a.cmml" xref="S3.E3.m1.2.2.2.2.3.3"><mtext mathsize="70%" id="S3.E3.m1.2.2.2.2.3.3.cmml" xref="S3.E3.m1.2.2.2.2.3.3">N-best(x, N)</mtext></ci></apply></apply><apply id="S3.E3.m1.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.1"><times id="S3.E3.m1.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.1.2"></times><ci id="S3.E3.m1.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.1.3">𝑃</ci><apply id="S3.E3.m1.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2.2">𝒚</ci><ci id="S3.E3.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2.3">𝒊</ci></apply><ci id="S3.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.3">𝒙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\hat{P}(\bm{y_{i}}|\bm{x})=\frac{P(\bm{y_{i}}|\bm{x})}{\sum_{\bm{y_{i}}\in\text{N-best(x, N)}}P(\bm{y_{i}}|\bm{x})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p6" class="ltx_para ltx_noindent">
<p id="S3.SS5.p6.1" class="ltx_p">To improve the training efficiency of RL, we deploy a remote service to generate hypotheses and simultaneously calculate the MWER loss while updating the model parameters on the current server. During the RL training process: 1) we initialize the model parameters with the context SFT model trained from the previous stage; 2) we utilize high-quality data for reinforcement learning training, with a data scale of thousands of hours. 3) to preserve the context-aware capability of the initialization model, our training data also includes a certain proportion of &lt;context, speech, text&gt; triples. After completing the RL training, we obtain our Seed-ASR model.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ablation studies in the stage of RL. Weighted WER as the reward function shows better performance than WER on all three evaluation sets (details of these sets are introduced in Section <a href="#S4.SS1" title="4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). The training data of &lt;contexts, speech, text&gt; triples in RL stage ensure the context-awareness ability does not drop. Seed-ASR utilizes the strategy in the last row. The metric of WER or weighted WER calculates the character error for Chinese, Japanese and Korean, and word error for English and other languages.</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Multidomain WER <math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Hardcase (F1%) <math id="S3.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.2.2.2.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T1.3.3.3.2" class="ltx_text"></span> <span id="S3.T1.3.3.3.1" class="ltx_text">
<span id="S3.T1.3.3.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.3.3.3.1.1.2" class="ltx_tr">
<span id="S3.T1.3.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Context Strict</span></span>
<span id="S3.T1.3.3.3.1.1.1" class="ltx_tr">
<span id="S3.T1.3.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Recall%) <math id="S3.T1.3.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.3.3.3.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.1.1.1.1.m1.1.1" xref="S3.T1.3.3.3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.1.1.1.m1.1b"><ci id="S3.T1.3.3.3.1.1.1.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span></span><span id="S3.T1.3.3.3.3" class="ltx_text"></span></td>
</tr>
<tr id="S3.T1.3.4" class="ltx_tr">
<td id="S3.T1.3.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model after Context SFT</td>
<td id="S3.T1.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.02</td>
<td id="S3.T1.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.39</td>
<td id="S3.T1.3.4.4" class="ltx_td ltx_align_center ltx_border_t">80.63</td>
</tr>
<tr id="S3.T1.3.5" class="ltx_tr">
<td id="S3.T1.3.5.1" class="ltx_td ltx_align_left ltx_border_r">+ RL w/ WER reward</td>
<td id="S3.T1.3.5.2" class="ltx_td ltx_align_center ltx_border_r">1.98</td>
<td id="S3.T1.3.5.3" class="ltx_td ltx_align_center ltx_border_r">93.39</td>
<td id="S3.T1.3.5.4" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="S3.T1.3.6" class="ltx_tr">
<td id="S3.T1.3.6.1" class="ltx_td ltx_align_left ltx_border_r">+ RL w/ Weighted WER reward</td>
<td id="S3.T1.3.6.2" class="ltx_td ltx_align_center ltx_border_r">1.94</td>
<td id="S3.T1.3.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.3.6.3.1" class="ltx_text ltx_font_bold">93.78</span></td>
<td id="S3.T1.3.6.4" class="ltx_td ltx_align_center">78.01</td>
</tr>
<tr id="S3.T1.3.7" class="ltx_tr">
<td id="S3.T1.3.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">    + train w/ context</td>
<td id="S3.T1.3.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.3.7.2.1" class="ltx_text ltx_font_bold">1.94</span></td>
<td id="S3.T1.3.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">93.72</td>
<td id="S3.T1.3.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.3.7.4.1" class="ltx_text ltx_font_bold">80.63</span></td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Observations</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p">In the process of improving the performance of Seed-ASR, we have also obtained some observations:</p>
</div>
<section id="S3.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1 </span>Scaling Law</h4>

<div id="S3.SS6.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS1.p1.1" class="ltx_p">In the realm of LLM, it is observed that larger models can continuously reduce the loss value by training on more data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. To the best of our knowledge, there is no relevant research on scaling laws for audio encoders under LLM-based framework. During the SSL stage, we conduct experiments to explore the performance of LUISE with different model sizes. Specifically, we select five groups of model sizes: 75M, 0.2B, 0.6B, 2B, and 5B. The training data comprises of 7.7 million hours of unsupervised speech-only data covering multiple domains, ensuring the full utilization of the model capacity. Different-sized models maintain consistency in most training configurations, except that as we increase the model size, we proportionally expand the width and depth of the model, appropriately increase the batch size and weight decay, and reduce the learning rate.</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F7.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F7.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:143.1pt;">
<img src="/html/2407.04675/assets/x7.png" id="S3.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="153" height="115" alt="Refer to caption">
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F7.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F7.2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:143.1pt;">
<img src="/html/2407.04675/assets/x8.png" id="S3.F7.2.1.g1" class="ltx_graphics ltx_img_landscape" width="153" height="115" alt="Refer to caption">
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F7.3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F7.3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:143.1pt;">
<img src="/html/2407.04675/assets/x9.png" id="S3.F7.3.1.g1" class="ltx_graphics ltx_img_landscape" width="153" height="115" alt="Refer to caption">
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
(a) depicts the correlation between the pretraining loss of our audio encoder (LUISE) and base-2 logarithm of the model parameter size. (b) depicts the correlation between the greedy WER after the SFT and base-2 logarithm of the model parameter size. (c) depicts the correlation between the greedy WER after SFT and the pretraining loss of LUISE.</figcaption>
</figure>
<div id="S3.SS6.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS1.p2.1" class="ltx_p">We first focus on the correlation between the cross-entropy pretraining loss value on the validation set and the model size. As shown in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.6.1 Scaling Law ‣ 3.6 Observations ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we observed a nearly linear correlation between the two. Additionally, we compared the performance after training on a small-scale SFT data based on the trained LUISE. Greedy search was used for inference. As shown in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.6.1 Scaling Law ‣ 3.6 Observations ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the WER metric on the multidomain evaluation set also exhibits a nearly linear correlation with the model size of LUISE. Furthermore, this reveals a positive correlation between the WER metric on the test set after SFT and the loss function value in the SSL stage in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.6.1 Scaling Law ‣ 3.6 Observations ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. These findings on scaling law provide guidance for our encoder selection (taking into account the balance of performance and efficiency) and subsequent optimization.</p>
</div>
</section>
<section id="S3.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2 </span>Long-form Ability</h4>

<div id="S3.SS6.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS2.p1.1" class="ltx_p">Our Seed-ASR is modeled under the framework of AcLLM, which naturally leverages the semantic knowledge and long-context modeling capabilities of LLM. Therefore, we also explore the option of directly inputting the entire long-form speech into LLM for recognition. This approach effectively avoids two problems associated with segmenting long-form speech for multiple independent inferences: 1) The segmentation process may result in the loss of information at the boundaries, decreasing recognition accuracy; 2) The segmentation process disrupts the strong global context information in long-form speech, affecting both the accuracy and consistency of recognition.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The comparison of performance on long-form video test sets. </figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:83.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.2pt,3.8pt) scale(0.914754961358481,0.914754961358481) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Avg WER</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">video_1</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">video_2</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">video_3</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">video_4</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">video_5</td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Transducer-based E2E Model</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.92</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.83</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.80</td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.80</td>
<td id="S3.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.22</td>
<td id="S3.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">4.66</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Paraformer-large</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">5.97</td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">5.78</td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r">5.36</td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">5.80</td>
<td id="S3.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">6.87</td>
<td id="S3.T2.1.1.3.7" class="ltx_td ltx_align_center">5.96</td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">Our Model after short-form SFT</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">2.28</td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">1.48</td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r">1.99</td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">2.31</td>
<td id="S3.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r">2.64</td>
<td id="S3.T2.1.1.4.7" class="ltx_td ltx_align_center">2.73</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">    + long-form SFT</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.5.2.1" class="ltx_text ltx_font_bold">2.08</span></td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.5.3.1" class="ltx_text ltx_font_bold">1.44</span></td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.5.4.1" class="ltx_text ltx_font_bold">1.96</span></td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.5.5.1" class="ltx_text ltx_font_bold">1.95</span></td>
<td id="S3.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.5.6.1" class="ltx_text ltx_font_bold">2.56</span></td>
<td id="S3.T2.1.1.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.5.7.1" class="ltx_text ltx_font_bold">2.31</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS6.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS2.p2.1" class="ltx_p">Specifically, we build a series of long-form video test sets comprising 5 datasets from different sources. During training, the entire long-form data is inputted into the model without any segmentation processing. The duration distribution of the test set is comparable to that of the training set. As shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.6.2 Long-form Ability ‣ 3.6 Observations ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, using long-form data for both training and testing results in relative WER reduction of nearly 8.8% compared to short-form training, which employs a domain-adaptive VAD to segment long-form speech into several parts for training and testing. The maximum duration of the long-form video test sets is 5 minutes, with scheduler for significant length extension.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Model and Evaluation</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">At present, we focus on the comprehensive improvement of Chinese and multilingual (without Chinese) speech recognition performance in diverse scenarios. Therefore, we present two Seed-ASR models with the same model structure and training recipe: the Chinese multi-dialect model, termed Seed-ASR (CN), and the multilingual model, termed Seed-ASR (ML). While we also have models that support both Chinese and multilingual languages, this report will specifically detail the two Seed-ASR models that focus on Chinese and multilingual (excluding Chinese), respectively.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">Seed-ASR (CN) not only transcribes Mandarin and 13 Chinese dialects with a single model but also demonstrates significant performance improvements over other released large models on the multi-dimensional evaluation sets, including multi-domain, multi-dialect, multi-accent and public set. Additionally, the training in the context SFT stage endows Seed-ASR (CN) with effective context-aware ability as demonstrated on dialogue context evaluation sets. Similarly, Seed-ASR (ML) achieves competitive results compared to other released models on 8 multilingual public sets (including English) and multi-domain evaluation sets, and it is being extended to more than 40 languages. The metric of word error rate (WER) is used as the main objective metric in the following part. Unless otherwise specified, the metric of WER calculates the character error for Chinese, Japanese, Korean, and calculates word error for English and other Languages.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Seed-ASR (CN)</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">Seed-ASR (CN) follows the complete training pipeline shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Framework and Training Recipe ‣ 3 Methods ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the SSL stage, we utilize the LUISE encoder with nearly 2B parameters, and conduct training on nearly eight million hours of Mandarin and Chinese dialect speech data from various domains. In the SFT stage, we use the trained LUISE and a MoE LLM with over ten billion parameters for model initialization. The training data comprises a mixture of Mandarin data containing multiple domain and dialect data. The detailed data distribution in the stage of SSL and SFT is introduced in Appendix <a href="#A1.SS3" title="A.3 Training Dataset Statistics ‣ Appendix A Appendix ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>. In the Context SFT stage, we use a certain proportion of SFT-stage data mixed with some &lt;context, speech, text&gt; triple data for training. In the RL stage, we use the trained context SFT model for initialization, and construct high-quality training data for training. Following this comprehensive training process, we obtain Seed-ASR (CN).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">To comprehensively evaluate the ASR ability of the Seed-ASR (CN) model, we compare it with other released models on public datasets and construct a series of evaluation sets, including the multi-domain sets, multi-source video sets, hardcase sets, multi-dialect sets, multi-accent sets, context-aware sets, and subjective intelligibility evaluation.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Evaluation on Public Set</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">We compare Seed-ASR (CN) with recently released large models on several Chinese ASR benchmarks, including: 1) the test set of aishell-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, marked as aishell1_test, with about 5 hours of read speech; 2) three test sets of aishell-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, marked as aishell2_andriod, aishell2_ios, and aishell2_mic, each set containing about 5 hours of read speech; 3) two test sets of Wenetspeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, marked as wenetspeech_testnet and wenetspeech_testmeeting, containing 23 hours and 15 hours of multi-domain test data, respectively.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The comparison of Seed-ASR (CN) and other released large ASR models on Chinese ASR benchmarks. </figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:137.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.7pt,3.7pt) scale(0.948723458942668,0.948723458942668) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Paraformer-large</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Qwen-Audio</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Hubert+Baichuan2</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Seed-ASR (CN)</td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">aishell1_test</td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.68</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.3</td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.95</td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.5.1" class="ltx_text ltx_font_bold">0.68</span></td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">aishell2_andriod</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">3.13</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">3.3</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r" rowspan="3"><span id="S4.T3.1.1.3.4.1" class="ltx_text">3.5 (avg)</span></td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.5.1" class="ltx_text ltx_font_bold">2.27</span></td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">aishell2_ios</td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">2.85</td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">3.1</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.4.1" class="ltx_text ltx_font_bold">2.27</span></td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">aishell2_mic</td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">3.06</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">3.3</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.4.1" class="ltx_text ltx_font_bold">2.28</span></td>
</tr>
<tr id="S4.T3.1.1.6" class="ltx_tr">
<td id="S4.T3.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">wenetspeech_testnet</td>
<td id="S4.T3.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">6.74</td>
<td id="S4.T3.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">9.5</td>
<td id="S4.T3.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r">6.06</td>
<td id="S4.T3.1.1.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.6.5.1" class="ltx_text ltx_font_bold">4.66</span></td>
</tr>
<tr id="S4.T3.1.1.7" class="ltx_tr">
<td id="S4.T3.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">wenetspeech_testmeeting</td>
<td id="S4.T3.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">6.97</td>
<td id="S4.T3.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">10.87</td>
<td id="S4.T3.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r">6.26</td>
<td id="S4.T3.1.1.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.5.1" class="ltx_text ltx_font_bold">5.69</span></td>
</tr>
<tr id="S4.T3.1.1.8" class="ltx_tr">
<td id="S4.T3.1.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Average-6</td>
<td id="S4.T3.1.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">4.07</td>
<td id="S4.T3.1.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">5.23</td>
<td id="S4.T3.1.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">3.96</td>
<td id="S4.T3.1.1.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.8.5.1" class="ltx_text ltx_font_bold">2.98</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The final result is the average of WER (character for Chinese) of the above 6 test sets. Our baselines for comparison include Paraformer-Large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Qwen-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and a recently released LLM-based ASR model with the structure of Hubert+Baichuan2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Their results presented here are from their respective papers. As shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1 Evaluation on Public Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Seed-ASR (CN) demonstrates a significant performance advantage over other models, achieving state-of-the-art results on these public datasets. For the average WER on the 6 sets, Seed-ASR (CN) achieves more than 24%-40% WER reduction than other published models.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Evaluation on Multi-domain and Multi-source Video Set</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We also conduct a comprehensive performance comparison on the multi-domain evaluation set, which contains high-quality evaluation data from various scenarios including video, live, voice search, meeting, intelligent assistants, etc. The weighted average WER of total 7 sets in multi-domain sets is used as the final metric. We select a transducer-based end-to-end models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with a MoE encoder and over 300M parameters as one of the baselines. Additionally, we also run the results of Paraformer-large (offline decoding) on the multi-domain evaluation set as another baseline. From the results in Table <a href="#S4.T4" title="Table 4 ‣ 4.1.2 Evaluation on Multi-domain and Multi-source Video Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Seed-ASR (CN) shows significant performance advantage, with a relative decrease of more than 47% in the WER metric compared to our strong end-to-end model. On the video evaluation sets covering 7 different subsets, Seed-ASR (CN) also obtains considerable performance improvement. These results demonstrate the strong foundational capabilities of Seed-ASR (CN).</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation results on three sets covering multi-domain, multi-source video and hardcase with proper nouns. The metric of WER is used for the first two sets, and the F1 score of given keyword is used as the metric of hardcase set.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.1.1.1.2" class="ltx_text"></span> <span id="S4.T4.1.1.1.1" class="ltx_text">
<span id="S4.T4.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T4.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multidomain</span></span>
<span id="S4.T4.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T4.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T4.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T4.1.1.1.3" class="ltx_text"></span></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T4.2.2.2.2" class="ltx_text"></span> <span id="S4.T4.2.2.2.1" class="ltx_text">
<span id="S4.T4.2.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.2.2.2.1.1.2" class="ltx_tr">
<span id="S4.T4.2.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Video-avg7</span></span>
<span id="S4.T4.2.2.2.1.1.1" class="ltx_tr">
<span id="S4.T4.2.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T4.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.1.1.1.1.m1.1.1" xref="S4.T4.2.2.2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.1.1.1.m1.1b"><ci id="S4.T4.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T4.2.2.2.3" class="ltx_text"></span></td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.3.3.3.2" class="ltx_text"></span> <span id="S4.T4.3.3.3.1" class="ltx_text">
<span id="S4.T4.3.3.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.3.3.3.1.1.2" class="ltx_tr">
<span id="S4.T4.3.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Hardcase</span></span>
<span id="S4.T4.3.3.3.1.1.1" class="ltx_tr">
<span id="S4.T4.3.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(F1%) <math id="S4.T4.3.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.3.3.3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.3.3.3.1.1.1.1.m1.1.1" xref="S4.T4.3.3.3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.1.1.1.m1.1b"><ci id="S4.T4.3.3.3.1.1.1.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T4.3.3.3.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T4.3.4" class="ltx_tr">
<td id="S4.T4.3.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Transducer-based E2E Model</td>
<td id="S4.T4.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.68</td>
<td id="S4.T4.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.92</td>
<td id="S4.T4.3.4.4" class="ltx_td ltx_align_center ltx_border_t">90.42</td>
</tr>
<tr id="S4.T4.3.5" class="ltx_tr">
<td id="S4.T4.3.5.1" class="ltx_td ltx_align_left ltx_border_r">Paraformer-large</td>
<td id="S4.T4.3.5.2" class="ltx_td ltx_align_center ltx_border_r">5.23</td>
<td id="S4.T4.3.5.3" class="ltx_td ltx_align_center ltx_border_r">5.97</td>
<td id="S4.T4.3.5.4" class="ltx_td ltx_align_center">87.99</td>
</tr>
<tr id="S4.T4.3.6" class="ltx_tr">
<td id="S4.T4.3.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T4.3.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.3.6.2.1" class="ltx_text ltx_font_bold">1.94</span></td>
<td id="S4.T4.3.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.3.6.3.1" class="ltx_text ltx_font_bold">2.70</span></td>
<td id="S4.T4.3.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.3.6.4.1" class="ltx_text ltx_font_bold">93.72</span></td>
</tr>
</table>
</figure>
<div id="S4.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Additionally, we evaluate the high-level ASR capabilities by introducing 10 hardcase test sets that cover utterances contain book titles, car names, idioms, drug names, movie names, ancient poems, product names, music names, etc. These test sets are designed to evaluate the model’s ability to recognize speech content containing proper nouns with strong professionalism and domain specificity, reflecting the ASR model’s knowledge reserve and recognition accuracy. The evaluation metric for the hardcase sets is the F1 score of the given keyword in each sentence. As shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.1.2 Evaluation on Multi-domain and Multi-source Video Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the Seed-ASR (CN) model achieves a 3.3% absolute increase in the F1 value compared to the end-to-end model baseline, demonstrating the effectiveness of the AcLLM model framework in leveraging LLM’s common sense knowledge and semantic reasoning capability.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Evaluation on Multi-dialect Set and Multi-accent Set</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">Since our Seed-ASR (CN) model supports the recognition of Mandarin and 13 Chinese dialects, we also introduce a dialect evaluation set. This set includes a total of 13 dialects (Cantonese, Southwest, Wu, Ji-lu, Zhongyuan, Min, etc.) and uses the same or similar pronunciation of Chinese characters to manually label the text. Specific demos of our dialect evaluation set are available on our website<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://bytedancespeech.github.io/seedasr_tech_report" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bytedancespeech.github.io/seedasr_tech_report</a></span></span></span>. We use WER as the objective metric for this dialect evaluation set.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison on the 13 Chinese dialect evaluation sets.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T5.1.1.2.1" class="ltx_text"></span> <span id="S4.T5.1.1.2.2" class="ltx_text">
<span id="S4.T5.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T5.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Average WER</span></span>
<span id="S4.T5.1.1.2.2.1.2" class="ltx_tr">
<span id="S4.T5.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">on 13 Chinese Dialects</span></span>
</span></span><span id="S4.T5.1.1.2.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Finetuned Whisper Medium-v2</td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_center ltx_border_t">21.68</td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">19.09</td>
</tr>
</table>
</figure>
<div id="S4.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">We utilize a fine-tuned Whisper Medium-v2 with 769M parameters as our baseline. For a fair comparison, we train both Whisper Medium-v2 and Seed-ASR (CN) with the same dialect training set. Seed-ASR (CN) needs to maintain comprehensive capabilities in Mandarin while improving ASR performance on dialects, thus it is trained with a larger proportion of Mandarin data from multiple domains. In contrast, Whisper Medium-v2 shows inferior results on comprehensive evaluation sets such as the multi-domain set. Despite this, the Seed-ASR (CN) model, with its larger modeling capacity, still shows performance advantages over the baseline on the 13 dialect sets, with the average WER across the 13 dialects decreasing from 21.68 to 19.2 (an 11.4% relative WER reduction), and a relative WER reduction of more than 21% on a single dialect test set.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison on the 11 Chinese accent evaluation sets.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T6.1.1.2.1" class="ltx_text"></span> <span id="S4.T6.1.1.2.2" class="ltx_text">
<span id="S4.T6.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T6.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Average WER</span></span>
<span id="S4.T6.1.1.2.2.1.2" class="ltx_tr">
<span id="S4.T6.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">on 11 Chinese Accents</span></span>
</span></span><span id="S4.T6.1.1.2.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Transducer-based E2E Model</td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_t">13.74</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Seed-ASR (CN) (w/o accent SFT data)</td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_align_center">5.90</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_align_center ltx_border_bb">4.96</td>
</tr>
</table>
</figure>
<div id="S4.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">To further verify the recognition performance of Seed-ASR (CN) on diverse speech, we introduce a series of accent evaluation sets, which include 11 Chinese accents from Anhui, Fujian, Gansu, Guangdong, Guizhou, Hunan, Jiangxi, Liaoning, Shaanxi, Shanxi, and Yunnan. Specific accent speech samples are also available on our website<sup id="S4.SS1.SSS3.p3.1.1" class="ltx_sup"><a href="#footnote2" title="footnote 2 ‣ 4.1.3 Evaluation on Multi-dialect Set and Multi-accent Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></sup>. As shown in Table <a href="#S4.T6" title="Table 6 ‣ 4.1.3 Evaluation on Multi-dialect Set and Multi-accent Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Seed-ASR (CN) exhibits significant improvement on the accent test sets compared to our strong E2E model trained from scratch. We also conduct an ablation study by removing the accent SFT data during the training process, yet Seed-ASR (CN) still achieves strong performance on the accent sets. The results on multi-dialect and multi-accent evaluation sets demonstrate the strong robustness of Seed-ASR (CN) in recognizing Chinese speech from different regions.</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Evaluation on Dialogue Context Set</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">In the evaluation of context awareness, we construct a high-quality dialogue context set where dialogue history is used as the contextual information. As shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.1.4 Evaluation on Dialogue Context Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we provide two examples of dialogues. Each test case includes the corresponding dialogue history text and the current recognized speech content. We divide the dialogue context evaluation into two subsets: strict and loose. The strict subset contains samples that have a strong dependence on the historical dialogue to accurately recognize the content of speech, such as person names. The loose subset has a weaker dependence between the historical dialogue and the content of speech, such as proper nouns. We use keyword recall as the evaluation metric.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2407.04675/assets/x10.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Examples of strict and loose evaluation subsets.</figcaption>
</figure>
<div id="S4.SS1.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS4.p2.1" class="ltx_p">On the dialogue evaluation set, Seed-ASR (CN) model shows better keyword recall than a strong end-to-end transducer-based model that utilizes context FST biasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> to improve keyword recall. Compared with Seed-ASR (CN) model that infers without context, the usage of context information brings more than a 15% recall improvement. These results demonstrate the strong ability of our AcLLM model framework in utilizing the context-awareness capabilities of LLM.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>The comparison of Seed-ASR and end-to-end models on our dialogue context sets, which cover strict subset and loose subset. Different decoding strategies are also compared.</figcaption>
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S4.T7.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Decoding method</td>
<td id="S4.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T7.1.1.3.1" class="ltx_text"></span> <span id="S4.T7.1.1.3.2" class="ltx_text">
<span id="S4.T7.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T7.1.1.3.2.1.1" class="ltx_tr">
<span id="S4.T7.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Dialogue Context Set</span></span>
<span id="S4.T7.1.1.3.2.1.2" class="ltx_tr">
<span id="S4.T7.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Strict | Loose</span></span>
</span></span><span id="S4.T7.1.1.3.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T7.1.2" class="ltx_tr">
<td id="S4.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Transducer-based E2E Model</td>
<td id="S4.T7.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Context FST biasing</td>
<td id="S4.T7.1.2.3" class="ltx_td ltx_align_center ltx_border_t">72.77 | 84.58</td>
</tr>
<tr id="S4.T7.1.3" class="ltx_tr">
<td id="S4.T7.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T7.1.3.2" class="ltx_td ltx_align_left ltx_border_r">Beam Search (w/o contexts)</td>
<td id="S4.T7.1.3.3" class="ltx_td ltx_align_center">65.45 | 89.33</td>
</tr>
<tr id="S4.T7.1.4" class="ltx_tr">
<td id="S4.T7.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T7.1.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Joint Beam Search</td>
<td id="S4.T7.1.4.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T7.1.4.3.1" class="ltx_text ltx_font_bold">80.63</span> | <span id="S4.T7.1.4.3.2" class="ltx_text ltx_font_bold">93.89</span>
</td>
</tr>
</table>
</figure>
<div id="S4.SS1.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS4.p3.1" class="ltx_p">On our website<sup id="S4.SS1.SSS4.p3.1.1" class="ltx_sup"><a href="#footnote2" title="footnote 2 ‣ 4.1.3 Evaluation on Multi-dialect Set and Multi-accent Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></sup>, we also provide several demos showcasing the context-aware capabilities of Seed-ASR (CN). In the application scenario of intelligent assistants, the contexts not only include conversation history but also support information such as bot names, bot descriptions, and subtitle history. Additionally, we found that contextual information such as user edit history for video captions and the names of participants in meetings can also enhance the performance of Seed-ASR in their respective applications.</p>
</div>
</section>
<section id="S4.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5 </span>Subjective Evaluation</h4>

<div id="S4.SS1.SSS5.p1" class="ltx_para">
<p id="S4.SS1.SSS5.p1.1" class="ltx_p">In addition to the objective evaluations mentioned above, we also conduct a subjective evaluation to further measure the effectiveness of the Seed-ASR (CN) model. We selecte three well-educated transcribers to transcribe the audio in 5 test scenarios in the multidomain set (videos, live, voice search, meetings, and intelligent assistants). During transcription, the transcribers could listen to the samples multiple times and use search engines to ensure the accuracy of their transcription. After they complete the transcription, we will randomize the results from both the transcribers and the Seed-ASR (CN) model for subjective evaluation. The subjective evaluation metric is intelligibility, and the covered score range is 1-5 points. The scoring standard is shown in the following Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1.5 Subjective Evaluation ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2407.04675/assets/x11.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The scoring standard for subjective evaluation.</figcaption>
</figure>
<div id="S4.SS1.SSS5.p2" class="ltx_para">
<p id="S4.SS1.SSS5.p2.1" class="ltx_p">On the test sets for voice search and voice assistants, the intelligibility of human recognition results is comparable to that of the Seed-ASR (CN) model. However, in live, videos, and meetings, Seed-ASR (CN) demonstrates better subjective intelligibility than humans. Specifically, compared to humans, in the case of professional field vocabulary and complex audio environments, the model can transcribe the content more accurately and give recognition results with higher intelligibility compared with human.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of subjective intelligibility score between Seed-ASR (CN) and three human transcribers.</figcaption>
<div id="S4.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:59pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.5pt,6.9pt) scale(0.807985906821856,0.807985906821856) ;">
<table id="S4.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T8.1.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Voice search</td>
<td id="S4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Live</td>
<td id="S4.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Video</td>
<td id="S4.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Meeting</td>
<td id="S4.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Intelligent assistant</td>
<td id="S4.T8.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Average</td>
</tr>
<tr id="S4.T8.1.1.2" class="ltx_tr">
<td id="S4.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 Human Results</td>
<td id="S4.T8.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.89/4.85/4.87</td>
<td id="S4.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.26/4.58/4.50</td>
<td id="S4.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.60/4.64/4.63</td>
<td id="S4.T8.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.30/4.03/4.37</td>
<td id="S4.T8.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.92/4.85/4.88</td>
<td id="S4.T8.1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T8.1.1.3" class="ltx_tr">
<td id="S4.T8.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Human Average</td>
<td id="S4.T8.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">4.87</td>
<td id="S4.T8.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">4.45</td>
<td id="S4.T8.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r">4.62</td>
<td id="S4.T8.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">4.23</td>
<td id="S4.T8.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">4.88</td>
<td id="S4.T8.1.1.3.7" class="ltx_td ltx_align_center ltx_border_r">4.61</td>
</tr>
<tr id="S4.T8.1.1.4" class="ltx_tr">
<td id="S4.T8.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Seed-ASR (CN)</td>
<td id="S4.T8.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.9</td>
<td id="S4.T8.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.81</td>
<td id="S4.T8.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.89</td>
<td id="S4.T8.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.76</td>
<td id="S4.T8.1.1.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.92</td>
<td id="S4.T8.1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.86</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS1.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.6 </span>Summary</h4>

<div id="S4.SS1.SSS6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS6.p1.2" class="ltx_p">Following a stage-by-stage training recipe including SFT <math id="S4.SS1.SSS6.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS6.p1.1.m1.1a"><mo stretchy="false" id="S4.SS1.SSS6.p1.1.m1.1.1" xref="S4.SS1.SSS6.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS6.p1.1.m1.1b"><ci id="S4.SS1.SSS6.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS6.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS6.p1.1.m1.1c">\rightarrow</annotation></semantics></math> context SFT <math id="S4.SS1.SSS6.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS6.p1.2.m2.1a"><mo stretchy="false" id="S4.SS1.SSS6.p1.2.m2.1.1" xref="S4.SS1.SSS6.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS6.p1.2.m2.1b"><ci id="S4.SS1.SSS6.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS6.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS6.p1.2.m2.1c">\rightarrow</annotation></semantics></math> RL, our Seed-ASR (CN) model is produced. On above comprehensive evaluation sets, we observe that certain capabilities of our Seed-ASR (CN) model are enhanced at different training stages. Here, we present a detailed ablation study on the effect of each stage, with results shown in Table <a href="#S4.T9" title="Table 9 ‣ 4.1.6 Summary ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. First, the introduction of the RL stage brings improvements on most evaluation sets, such as multi-domain, multi-source video, multi-dialect, hardcase, and code-switch. The slight degradation in the accent test set may be due to the training data ratio. Additionally, training in the context SFT stage positively impacts most test sets, notably bringing significant improvement in the recall metric on the context strict test set. This further demonstrates the effectiveness of our context-aware training and decoding strategy in the context SFT stage.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Ablation studies on Seed-ASR (CN) after different stages.</figcaption>
<div id="S4.T9.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:74.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.1pt,19.7pt) scale(0.653314681641479,0.653314681641479) ;">
<table id="S4.T9.7.7" class="ltx_tabular ltx_align_middle">
<tr id="S4.T9.7.7.7" class="ltx_tr">
<td id="S4.T9.7.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Model</td>
<td id="S4.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.1.1.1.1.2" class="ltx_text"></span> <span id="S4.T9.1.1.1.1.1" class="ltx_text">
<span id="S4.T9.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T9.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multi-domain</span></span>
<span id="S4.T9.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T9.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T9.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T9.1.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T9.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.1.1.1.1.3" class="ltx_text"></span></td>
<td id="S4.T9.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.2.2.2.2.2" class="ltx_text"></span> <span id="S4.T9.2.2.2.2.1" class="ltx_text">
<span id="S4.T9.2.2.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.2.2.2.2.1.1.2" class="ltx_tr">
<span id="S4.T9.2.2.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multi-source</span></span>
<span id="S4.T9.2.2.2.2.1.1.3" class="ltx_tr">
<span id="S4.T9.2.2.2.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Video</span></span>
<span id="S4.T9.2.2.2.2.1.1.1" class="ltx_tr">
<span id="S4.T9.2.2.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T9.2.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.2.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.2.2.2.2.1.1.1.1.m1.1.1" xref="S4.T9.2.2.2.2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.2.2.2.2.1.1.1.1.m1.1b"><ci id="S4.T9.2.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S4.T9.2.2.2.2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.2.2.2.2.3" class="ltx_text"></span></td>
<td id="S4.T9.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.3.3.3.3.2" class="ltx_text"></span> <span id="S4.T9.3.3.3.3.1" class="ltx_text">
<span id="S4.T9.3.3.3.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.3.3.3.3.1.1.2" class="ltx_tr">
<span id="S4.T9.3.3.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multi-accent</span></span>
<span id="S4.T9.3.3.3.3.1.1.1" class="ltx_tr">
<span id="S4.T9.3.3.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T9.3.3.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.3.3.3.3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.3.3.3.3.1.1.1.1.m1.1.1" xref="S4.T9.3.3.3.3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.3.3.3.3.1.1.1.1.m1.1b"><ci id="S4.T9.3.3.3.3.1.1.1.1.m1.1.1.cmml" xref="S4.T9.3.3.3.3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.3.3.3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.3.3.3.3.3" class="ltx_text"></span></td>
<td id="S4.T9.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.4.4.4.4.2" class="ltx_text"></span> <span id="S4.T9.4.4.4.4.1" class="ltx_text">
<span id="S4.T9.4.4.4.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.4.4.4.4.1.1.2" class="ltx_tr">
<span id="S4.T9.4.4.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multi-dialect</span></span>
<span id="S4.T9.4.4.4.4.1.1.1" class="ltx_tr">
<span id="S4.T9.4.4.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T9.4.4.4.4.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.4.4.4.4.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.4.4.4.4.1.1.1.1.m1.1.1" xref="S4.T9.4.4.4.4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.4.4.4.4.1.1.1.1.m1.1b"><ci id="S4.T9.4.4.4.4.1.1.1.1.m1.1.1.cmml" xref="S4.T9.4.4.4.4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.4.4.4.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.4.4.4.4.3" class="ltx_text"></span></td>
<td id="S4.T9.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.5.5.5.5.2" class="ltx_text"></span> <span id="S4.T9.5.5.5.5.1" class="ltx_text">
<span id="S4.T9.5.5.5.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.5.5.5.5.1.1.2" class="ltx_tr">
<span id="S4.T9.5.5.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Hardcase</span></span>
<span id="S4.T9.5.5.5.5.1.1.1" class="ltx_tr">
<span id="S4.T9.5.5.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(F1%) <math id="S4.T9.5.5.5.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T9.5.5.5.5.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.5.5.5.5.1.1.1.1.m1.1.1" xref="S4.T9.5.5.5.5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T9.5.5.5.5.1.1.1.1.m1.1b"><ci id="S4.T9.5.5.5.5.1.1.1.1.m1.1.1.cmml" xref="S4.T9.5.5.5.5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.5.5.5.5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.5.5.5.5.3" class="ltx_text"></span></td>
<td id="S4.T9.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T9.6.6.6.6.2" class="ltx_text"></span> <span id="S4.T9.6.6.6.6.1" class="ltx_text">
<span id="S4.T9.6.6.6.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.6.6.6.6.1.1.2" class="ltx_tr">
<span id="S4.T9.6.6.6.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Context-strict</span></span>
<span id="S4.T9.6.6.6.6.1.1.1" class="ltx_tr">
<span id="S4.T9.6.6.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(recall%) <math id="S4.T9.6.6.6.6.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T9.6.6.6.6.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.6.6.6.6.1.1.1.1.m1.1.1" xref="S4.T9.6.6.6.6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T9.6.6.6.6.1.1.1.1.m1.1b"><ci id="S4.T9.6.6.6.6.1.1.1.1.m1.1.1.cmml" xref="S4.T9.6.6.6.6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.6.6.6.6.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.6.6.6.6.3" class="ltx_text"></span></td>
<td id="S4.T9.7.7.7.7" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T9.7.7.7.7.2" class="ltx_text"></span> <span id="S4.T9.7.7.7.7.1" class="ltx_text">
<span id="S4.T9.7.7.7.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.7.7.7.7.1.1.2" class="ltx_tr">
<span id="S4.T9.7.7.7.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Code-switch</span></span>
<span id="S4.T9.7.7.7.7.1.1.1" class="ltx_tr">
<span id="S4.T9.7.7.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER%) <math id="S4.T9.7.7.7.7.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T9.7.7.7.7.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T9.7.7.7.7.1.1.1.1.m1.1.1" xref="S4.T9.7.7.7.7.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T9.7.7.7.7.1.1.1.1.m1.1b"><ci id="S4.T9.7.7.7.7.1.1.1.1.m1.1.1.cmml" xref="S4.T9.7.7.7.7.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.7.7.7.7.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span><span id="S4.T9.7.7.7.7.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T9.7.7.8" class="ltx_tr">
<td id="S4.T9.7.7.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Seed-ASR (CN)</td>
<td id="S4.T9.7.7.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.7.7.8.2.1" class="ltx_text ltx_font_bold">1.94</span></td>
<td id="S4.T9.7.7.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.7.7.8.3.1" class="ltx_text ltx_font_bold">2.7</span></td>
<td id="S4.T9.7.7.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.96</td>
<td id="S4.T9.7.7.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.7.7.8.5.1" class="ltx_text ltx_font_bold">19.09</span></td>
<td id="S4.T9.7.7.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.7.7.8.6.1" class="ltx_text ltx_font_bold">93.72</span></td>
<td id="S4.T9.7.7.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T9.7.7.8.7.1" class="ltx_text ltx_font_bold">80.63</span></td>
<td id="S4.T9.7.7.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T9.7.7.8.8.1" class="ltx_text ltx_font_bold">5.65</span></td>
</tr>
<tr id="S4.T9.7.7.9" class="ltx_tr">
<td id="S4.T9.7.7.9.1" class="ltx_td ltx_align_left ltx_border_r">   w/o RL</td>
<td id="S4.T9.7.7.9.2" class="ltx_td ltx_align_center ltx_border_r">2.02</td>
<td id="S4.T9.7.7.9.3" class="ltx_td ltx_align_center ltx_border_r">2.79</td>
<td id="S4.T9.7.7.9.4" class="ltx_td ltx_align_center ltx_border_r">5.05</td>
<td id="S4.T9.7.7.9.5" class="ltx_td ltx_align_center ltx_border_r">19.48</td>
<td id="S4.T9.7.7.9.6" class="ltx_td ltx_align_center ltx_border_r">93.39</td>
<td id="S4.T9.7.7.9.7" class="ltx_td ltx_align_center ltx_border_r">80.63</td>
<td id="S4.T9.7.7.9.8" class="ltx_td ltx_align_center">6.07</td>
</tr>
<tr id="S4.T9.7.7.10" class="ltx_tr">
<td id="S4.T9.7.7.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">    w/o context SFT</td>
<td id="S4.T9.7.7.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">2.11</td>
<td id="S4.T9.7.7.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">2.82</td>
<td id="S4.T9.7.7.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T9.7.7.10.4.1" class="ltx_text ltx_font_bold">4.89</span></td>
<td id="S4.T9.7.7.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">19.47</td>
<td id="S4.T9.7.7.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">93.43</td>
<td id="S4.T9.7.7.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">61.26</td>
<td id="S4.T9.7.7.10.8" class="ltx_td ltx_align_center ltx_border_bb">5.93</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS1.SSS6.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS6.p2.1" class="ltx_p">The evaluation results demonstrate that Seed-ASR (CN) possesses more comprehensive and powerful model capabilities compared to classic end-to-end models and other released models. The performance advantage of Seed-ASR is evident in public test sets and our subjective intelligibility evaluation, where it even surpasses human transcribers in some domains. Moreover, Seed-ASR has achieved significant recall improvements compared to end-to-end models combined with context FST fusion strategies on the context-aware evaluation set. This unified and concise structure reflects Seed-ASR’s ability to support customized ASR application scenarios. Overall, the evaluation results showcase the powerful capabilities of the Seed-ASR model in various ASR scenarios that handle diverse speech inputs and contexts.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Seed-ASR (ML)</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">As demonstrated above, Seed-ASR (CN) exhibits strong performance in recognizing Mandarin and Chinese dialects. To extend these advantages to languages spoken by users in other countries, we also apply the Seed-ASR methodology to multilingual scenarios, resulting in our multilingual model: Seed-ASR (ML). The training of Seed-ASR (ML) differs from Seed-ASR (CN) primarily in terms of the training data. While Seed-ASR (CN) focuses on Mandarin and Chinese dialects, Seed-ASR (ML) is trained on a diverse set of multilingual data. In the stage of SSL, the audio encoder of Seed-ASR (ML) also utilizes the LUISE with 2B parameters, and is trained with over tens of millions of hours of unsupervised multilingual data from multi-domain sources. In the subsequent stages, we select the training data from our multilingual ASR training sets sum up to hundreds of thousands of hours covering 9 languages: English, Chinese, Arabic, Spanish, French, Indonesian, Japanese, Korean and Portuguese. The detailed data distribution in the stage of SSL and SFT is introduced in Appendix <a href="#A1.SS3" title="A.3 Training Dataset Statistics ‣ Appendix A Appendix ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>. We conduct performance comparisons on our multiple evaluation sets and public datasets.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Evaluation on Multi-domain and Multi-accent Sets</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">On the multi-domain evaluation sets, the covered domains are the same as the multi-domain evaluation sets on Seed-ASR (CN) introduced in Section <a href="#S4.SS1.SSS2" title="4.1.2 Evaluation on Multi-domain and Multi-source Video Set ‣ 4.1 Seed-ASR (CN) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>. The hardcase test sets cover domains ranging from medical health, food and drink, sports, technology, outfit, games, entertainment and beauty. We also build an evaluation of different accents of English, including speakers from Great Britain, United States, Australia, Canada, China, India, Singapore, New Zealand and South Africa. For multilingual evaluation, we report the average WER performance on 7 non-English languages: Arabic (AR), Spanish (ES), French (FR), Indonesian (ID), Japanese (JA), Korean (KO), and Portuguese (PT). As shown in Table <a href="#S4.T10" title="Table 10 ‣ 4.2.1 Evaluation on Multi-domain and Multi-accent Sets ‣ 4.2 Seed-ASR (ML) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the baselines for comparison include Google USM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (API call <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://sites.research.google/usm/</span></span></span>), Whisper Large v3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> (offline decoding) and Universal-1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (API call <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.assemblyai.com/app/</span></span></span>). Since Universal-1 only supports 3 languages in our multilingual multi-domain evaluation sets, its corresponding results are not included here. We attach the language-wise performance comparison on multilingual multi-domain evaluation sets among these models to Appendix <a href="#A1.SS1" title="A.1 Detailed Results of Seed-ASR (ML) ‣ Appendix A Appendix ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. From the results in Table <a href="#S4.T10" title="Table 10 ‣ 4.2.1 Evaluation on Multi-domain and Multi-accent Sets ‣ 4.2 Seed-ASR (ML) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, Seed-ASR (ML) demonstrates relatively over 42% and 40% on English and multilingual multi-domain evaluation sets, respectively, compared to the strongest baselines. Similar significant improvements are also observed on the English multi-accent and hardcase evaluation sets.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Comparison with Google USM, Whisper Large v3 and Universal-1 on English multi-domain, multi-accent, hardcase evaluation sets, and multilingual multi-domain evaluation sets.</figcaption>
<div id="S4.T10.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:49.7pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-179.6pt,20.4pt) scale(0.546932982169824,0.546932982169824) ;">
<table id="S4.T10.4.4" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.4.4.5" class="ltx_tr">
<td id="S4.T10.4.4.5.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T10.4.4.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Google USM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S4.T10.4.4.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Whisper Large v3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S4.T10.4.4.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Universal-1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T10.4.4.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Seed-ASR (ML)</td>
</tr>
<tr id="S4.T10.1.1.1" class="ltx_tr">
<td id="S4.T10.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">English Multi-domain (WER%) <math id="S4.T10.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T10.1.1.1.1.m1.1.1" xref="S4.T10.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.1.1.1.1.m1.1b"><ci id="S4.T10.1.1.1.1.m1.1.1.cmml" xref="S4.T10.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.33</td>
<td id="S4.T10.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.41</td>
<td id="S4.T10.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.95</td>
<td id="S4.T10.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.1.1.1.5.1" class="ltx_text ltx_font_bold">5.34</span></td>
</tr>
<tr id="S4.T10.2.2.2" class="ltx_tr">
<td id="S4.T10.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">English Multi-accent (WER%) <math id="S4.T10.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T10.2.2.2.1.m1.1.1" xref="S4.T10.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.2.2.2.1.m1.1b"><ci id="S4.T10.2.2.2.1.m1.1.1.cmml" xref="S4.T10.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T10.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.19</td>
<td id="S4.T10.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.52</td>
<td id="S4.T10.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.40</td>
<td id="S4.T10.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.2.2.2.5.1" class="ltx_text ltx_font_bold">11.26</span></td>
</tr>
<tr id="S4.T10.3.3.3" class="ltx_tr">
<td id="S4.T10.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">English Hardcase (F1%) <math id="S4.T10.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T10.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T10.3.3.3.1.m1.1.1" xref="S4.T10.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T10.3.3.3.1.m1.1b"><ci id="S4.T10.3.3.3.1.m1.1.1.cmml" xref="S4.T10.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T10.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">63.30</td>
<td id="S4.T10.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.54</td>
<td id="S4.T10.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.82</td>
<td id="S4.T10.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.3.3.3.5.1" class="ltx_text ltx_font_bold">87.94</span></td>
</tr>
<tr id="S4.T10.4.4.4" class="ltx_tr">
<td id="S4.T10.4.4.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt">Multilingual Multi-domain (WER%) <math id="S4.T10.4.4.4.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T10.4.4.4.1.m1.1.1" xref="S4.T10.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.4.4.4.1.m1.1b"><ci id="S4.T10.4.4.4.1.m1.1.1.cmml" xref="S4.T10.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.4.4.4.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T10.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt">21.51</td>
<td id="S4.T10.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt">20.55</td>
<td id="S4.T10.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt">-</td>
<td id="S4.T10.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt"><span id="S4.T10.4.4.4.5.1" class="ltx_text ltx_font_bold">12.16</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Evaluation on Public Sets</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In addition to the internal multi-domain evaluation sets, we also compare Seed-ASR (ML) with other models on public test sets for English and other languages, including Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> test clean/other, MLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, Tedlium 3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Callhome, Switchboard<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, AMI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and Fleurs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Details of the test sets are introduced in Appendix <a href="#A1.SS2" title="A.2 Details of English and Multilingual public test sets used in Seed-ASR (ML) evaluation ‣ Appendix A Appendix ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>. The results are illustrated in Table <a href="#S4.T11" title="Table 11 ‣ 4.2.2 Evaluation on Public Sets ‣ 4.2 Seed-ASR (ML) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. Note that all the results of baseline models are WERs reported by the respective papers or technical reports of the baseline models (Whisper Large-v3 results are from the Universal-1’s technical report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>). As shown in Table <a href="#S4.T11" title="Table 11 ‣ 4.2.2 Evaluation on Public Sets ‣ 4.2 Seed-ASR (ML) ‣ 4 Model and Evaluation ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, Seed-ASR (ML) achieves top performance on most of the test sets across different languages with improvements ranging from 10% to 40%, indicating Seed-ASR (ML)’s generalization ability to domains unseen during training.</p>
</div>
<figure id="S4.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>ASR Results of Seed-ASR (ML) on English and Multilingual Public test sets</figcaption>
<div id="S4.T11.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:143.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-299.7pt,99.2pt) scale(0.419721721372867,0.419721721372867) ;">
<table id="S4.T11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T11.1.1.1" class="ltx_tr">
<td id="S4.T11.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Test set</td>
<td id="S4.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Language</td>
<td id="S4.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Google USM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S4.T11.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Whisper Large-v2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S4.T11.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Whisper Large-v3</td>
<td id="S4.T11.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Universal-1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S4.T11.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Gemini-1.5 Pro<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T11.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">Seed-ASR (ML)</td>
</tr>
<tr id="S4.T11.1.1.2" class="ltx_tr">
<td id="S4.T11.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Librispeech test_clean</td>
<td id="S4.T11.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EN</td>
<td id="S4.T11.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.7</td>
<td id="S4.T11.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8</td>
<td id="S4.T11.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6</td>
<td id="S4.T11.1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T11.1.1.2.8.1" class="ltx_text ltx_font_bold">1.58</span></td>
</tr>
<tr id="S4.T11.1.1.3" class="ltx_tr">
<td id="S4.T11.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Librispeech test_other</td>
<td id="S4.T11.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">EN</td>
<td id="S4.T11.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r">5.2</td>
<td id="S4.T11.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">3.6</td>
<td id="S4.T11.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">3.1</td>
<td id="S4.T11.1.1.3.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.3.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.3.8.1" class="ltx_text ltx_font_bold">2.84</span></td>
</tr>
<tr id="S4.T11.1.1.4" class="ltx_tr">
<td id="S4.T11.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Tedlium 3</td>
<td id="S4.T11.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">EN</td>
<td id="S4.T11.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r">4.0</td>
<td id="S4.T11.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">7.4</td>
<td id="S4.T11.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r">7.5</td>
<td id="S4.T11.1.1.4.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.4.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.4.8.1" class="ltx_text ltx_font_bold">3.11</span></td>
</tr>
<tr id="S4.T11.1.1.5" class="ltx_tr">
<td id="S4.T11.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Switchboard</td>
<td id="S4.T11.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">EN</td>
<td id="S4.T11.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">13.8</td>
<td id="S4.T11.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.5.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.5.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.5.8.1" class="ltx_text ltx_font_bold">11.59</span></td>
</tr>
<tr id="S4.T11.1.1.6" class="ltx_tr">
<td id="S4.T11.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">CallHome</td>
<td id="S4.T11.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">EN</td>
<td id="S4.T11.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r">17.6</td>
<td id="S4.T11.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.6.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.6.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.6.8.1" class="ltx_text ltx_font_bold">12.24</span></td>
</tr>
<tr id="S4.T11.1.1.7" class="ltx_tr">
<td id="S4.T11.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">AMI IHM</td>
<td id="S4.T11.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">EN</td>
<td id="S4.T11.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r">16.9</td>
<td id="S4.T11.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.7.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.7.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.7.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.7.8.1" class="ltx_text ltx_font_bold">13.16</span></td>
</tr>
<tr id="S4.T11.1.1.8" class="ltx_tr">
<td id="S4.T11.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="7"><span id="S4.T11.1.1.8.1.1" class="ltx_text">Fleurs</span></td>
<td id="S4.T11.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EN</td>
<td id="S4.T11.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.4</td>
<td id="S4.T11.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T11.1.1.8.8.1" class="ltx_text ltx_font_bold">3.43</span></td>
</tr>
<tr id="S4.T11.1.1.9" class="ltx_tr">
<td id="S4.T11.1.1.9.1" class="ltx_td ltx_align_center ltx_border_r">AR</td>
<td id="S4.T11.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r">16</td>
<td id="S4.T11.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.9.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.9.7" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.9.7.1" class="ltx_text ltx_font_bold">13.05</span></td>
</tr>
<tr id="S4.T11.1.1.10" class="ltx_tr">
<td id="S4.T11.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r">ES</td>
<td id="S4.T11.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r">3.0</td>
<td id="S4.T11.1.1.10.4" class="ltx_td ltx_align_center ltx_border_r">2.8</td>
<td id="S4.T11.1.1.10.5" class="ltx_td ltx_align_center ltx_border_r">5.0</td>
<td id="S4.T11.1.1.10.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.10.7" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.10.7.1" class="ltx_text ltx_font_bold">2.50</span></td>
</tr>
<tr id="S4.T11.1.1.11" class="ltx_tr">
<td id="S4.T11.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r">FR</td>
<td id="S4.T11.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r">8.3</td>
<td id="S4.T11.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.1.1.11.4.1" class="ltx_text ltx_font_bold">5.6</span></td>
<td id="S4.T11.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r">6.8</td>
<td id="S4.T11.1.1.11.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.11.7" class="ltx_td ltx_align_center">7.09</td>
</tr>
<tr id="S4.T11.1.1.12" class="ltx_tr">
<td id="S4.T11.1.1.12.1" class="ltx_td ltx_align_center ltx_border_r">ID</td>
<td id="S4.T11.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.12.3" class="ltx_td ltx_align_center ltx_border_r">7.1</td>
<td id="S4.T11.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.12.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.12.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.12.7" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.12.7.1" class="ltx_text ltx_font_bold">4.24</span></td>
</tr>
<tr id="S4.T11.1.1.13" class="ltx_tr">
<td id="S4.T11.1.1.13.1" class="ltx_td ltx_align_center ltx_border_r">JA</td>
<td id="S4.T11.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.13.3" class="ltx_td ltx_align_center ltx_border_r">5.3</td>
<td id="S4.T11.1.1.13.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.13.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.13.7" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.13.7.1" class="ltx_text ltx_font_bold">3.46</span></td>
</tr>
<tr id="S4.T11.1.1.14" class="ltx_tr">
<td id="S4.T11.1.1.14.1" class="ltx_td ltx_align_center ltx_border_r">KO</td>
<td id="S4.T11.1.1.14.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.14.3" class="ltx_td ltx_align_center ltx_border_r">14.3</td>
<td id="S4.T11.1.1.14.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.14.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.14.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.14.7" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.14.7.1" class="ltx_text ltx_font_bold">3.25</span></td>
</tr>
<tr id="S4.T11.1.1.15" class="ltx_tr">
<td id="S4.T11.1.1.15.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T11.1.1.15.2" class="ltx_td ltx_align_center ltx_border_r">PT</td>
<td id="S4.T11.1.1.15.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.15.4" class="ltx_td ltx_align_center ltx_border_r">4.3</td>
<td id="S4.T11.1.1.15.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.15.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.15.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.15.8" class="ltx_td ltx_align_center"><span id="S4.T11.1.1.15.8.1" class="ltx_text ltx_font_bold">3.55</span></td>
</tr>
<tr id="S4.T11.1.1.16" class="ltx_tr">
<td id="S4.T11.1.1.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T11.1.1.16.1.1" class="ltx_text">MLS</span></td>
<td id="S4.T11.1.1.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EN</td>
<td id="S4.T11.1.1.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7</td>
<td id="S4.T11.1.1.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.2</td>
<td id="S4.T11.1.1.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T11.1.1.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.6</td>
<td id="S4.T11.1.1.16.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T11.1.1.16.8.1" class="ltx_text ltx_font_bold">4.14</span></td>
</tr>
<tr id="S4.T11.1.1.17" class="ltx_tr">
<td id="S4.T11.1.1.17.1" class="ltx_td ltx_align_center ltx_border_r">ES</td>
<td id="S4.T11.1.1.17.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.17.3" class="ltx_td ltx_align_center ltx_border_r">4.2</td>
<td id="S4.T11.1.1.17.4" class="ltx_td ltx_align_center ltx_border_r">5.7</td>
<td id="S4.T11.1.1.17.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.1.1.17.5.1" class="ltx_text ltx_font_bold">3.3</span></td>
<td id="S4.T11.1.1.17.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.17.7" class="ltx_td ltx_align_center">3.76</td>
</tr>
<tr id="S4.T11.1.1.18" class="ltx_tr">
<td id="S4.T11.1.1.18.1" class="ltx_td ltx_align_center ltx_border_r">FR</td>
<td id="S4.T11.1.1.18.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.18.3" class="ltx_td ltx_align_center ltx_border_r">7.3</td>
<td id="S4.T11.1.1.18.4" class="ltx_td ltx_align_center ltx_border_r">8.1</td>
<td id="S4.T11.1.1.18.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.1.1.18.5.1" class="ltx_text ltx_font_bold">2.3</span></td>
<td id="S4.T11.1.1.18.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T11.1.1.18.7" class="ltx_td ltx_align_center">5.10</td>
</tr>
<tr id="S4.T11.1.1.19" class="ltx_tr">
<td id="S4.T11.1.1.19.1" class="ltx_td ltx_border_bb ltx_border_b ltx_border_r"></td>
<td id="S4.T11.1.1.19.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">PT</td>
<td id="S4.T11.1.1.19.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">-</td>
<td id="S4.T11.1.1.19.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">6.8</td>
<td id="S4.T11.1.1.19.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">-</td>
<td id="S4.T11.1.1.19.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">-</td>
<td id="S4.T11.1.1.19.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">-</td>
<td id="S4.T11.1.1.19.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S4.T11.1.1.19.8.1" class="ltx_text ltx_font_bold">5.04</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Summary</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Similar with Seed-ASR (CN), Seed-ASR (ML) demonstrates exceptional performance across a wide range of evaluation sets comapred to multiple strong baselines. The model excels in recognizing speech with diverse acoustic environments, semantic contexts and accents across multiple languages, underscoring the model’s generalization ability and its effectiveness in processing speech from various unseen domains during training. Overall, the results on above evaluation sets on Chinese and multilingual setting demonstrate the generalization and strong foundation abilities of Seed-ASR in diverse application scenarios covering multi-lingual, multi-dialect, multi-accent, multi-domain, and multiple customization requirements.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">The Seed-ASR models, trained through a stage-by-stage recipe including SFT, context SFT, and RL, demonstrates superior capabilities across various evaluation sets across different acoustic and semantic domains, accents/dialects/languages, and long range speech duration, compared to recently-released strong end-to-end models. The large-scale LUISE pretraining and SFT to connect LUISE and LLM endow Seed-ASR capacity to understand diverse speech content. The introduction of context SFT stage significantly boosts the models’ recall on keywords given related context, showcasing the model’s strong customization ability in utilizing the context-awareness abilities of LLMs. The RL stage further consolidates the alignment between Seed-ASR’s text generation behavior and the requirement for accurate transcription, especially the transcription of semantically important parts. Overall, the results affirm Seed-ASR’s position as a top-performing ASR model for diverse applications involving multiple languages, dialects, accents, domains, and customization needs. In future, we will focus on extending Seed-ASR’s ability to deal with multiple tasks within a single model, further enhance the long-form ability and increase the number of supported languages.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10403</span>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:12449–12460, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">End-to-end attention-based large vocabulary speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 4945–4949. IEEE, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng.

</span>
<span class="ltx_bibblock">Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)</span>, pages 1–5. IEEE, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 4960–4964. IEEE, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu.

</span>
<span class="ltx_bibblock">X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.04160</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al.

</span>
<span class="ltx_bibblock">Wavlm: Large-scale self-supervised pre-training for full stack speech processing.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, 16(6):1505–1518, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Zhehuai Chen, He Huang, Andrei Andrusenko, Oleksii Hrinchuk, Krishna C Puvvada, Jason Li, Subhankar Ghosh, Jagadeesh Balam, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Salm: Speech-augmented language model with in-context learning for speech recognition and translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 13521–13525. IEEE, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Self-supervised learning with random-projection quantizer for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 3915–3924. PMLR, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 24(240):1–113, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.07919</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.

</span>
<span class="ltx_bibblock">Fleurs: Few-shot learning evaluation of universal representations of speech.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</span>, pages 798–805. IEEE, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Linhao Dong and Bo Xu.

</span>
<span class="ltx_bibblock">Cif: Continuous integrate-and-fire for end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6079–6083. IEEE, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu.

</span>
<span class="ltx_bibblock">Aishell-2: Transforming mandarin asr research into industrial scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.10583</span>, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan.

</span>
<span class="ltx_bibblock">Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.08317</span>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Xuelong Geng, Tianyi Xu, Kun Wei, Bingsheng Mu, Hongfei Xue, He Wang, Yangze Li, Pengcheng Guo, Yuhang Dai, Longhao Li, et al.

</span>
<span class="ltx_bibblock">Unveiling the potential of llm-based asr on chinese open-source datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2405.02132</span>, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
John J Godfrey, Edward C Holliman, and Jane McDaniel.

</span>
<span class="ltx_bibblock">Switchboard: Telephone speech corpus for research and development.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Acoustics, speech, and signal processing, ieee international conference on</span>, volume 1, pages 517–520. IEEE Computer Society, 1992.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Alex Graves.

</span>
<span class="ltx_bibblock">Sequence transduction with recurrent neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1211.3711</span>, 2012.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd international conference on Machine learning</span>, pages 369–376, 2006.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented transformer for speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.08100</span>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al.

</span>
<span class="ltx_bibblock">Streaming end-to-end speech recognition for mobile devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6381–6385. IEEE, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve.

</span>
<span class="ltx_bibblock">Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18–22, 2018, Proceedings 20</span>, pages 198–208. Springer, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al.

</span>
<span class="ltx_bibblock">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE Signal processing magazine</span>, 29(6):82–97, 2012.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.15556</span>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed.

</span>
<span class="ltx_bibblock">HuBERT: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, 29:3451–3460, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al.

</span>
<span class="ltx_bibblock">Audiogpt: Understanding and generating speech, music, sound, and talking head.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 38, pages 23802–23804, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.08361</span>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wessel Kraaij, Thomas Hain, Mike Lincoln, and Wilfried Post.

</span>
<span class="ltx_bibblock">The ami meeting corpus.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proc. International Conference on Methods and Techniques in Behavioral Research</span>, 2005.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y. Li, Y. Wu, J. Li, and S. Liu.

</span>
<span class="ltx_bibblock">Prompting large language models for zero-shot domain adaptation in speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv:2306.16007</span>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yajie Miao, Mohammad Gowayyed, and Florian Metze.

</span>
<span class="ltx_bibblock">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">2015 IEEE workshop on automatic speech recognition and understanding (ASRU)</span>, pages 167–174. IEEE, 2015.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Chinese Academy of Social Sciences and City University of Hong Kong.

</span>
<span class="ltx_bibblock">the language atlas of china.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">The Commercial Press</span>, 2012.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">URL https://openai.com/blog/chatgpt</span>, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 5206–5210. IEEE, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Rohit Prabhavalkar, Tara N Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, and Anjuli Kannan.

</span>
<span class="ltx_bibblock">Minimum word error rate training for attention-based sequence-to-sequence models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 4839–4843. IEEE, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert.

</span>
<span class="ltx_bibblock">Mls: A large-scale multilingual dataset for speech research.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.03411</span>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 28492–28518. PMLR, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Francis McCann Ramirez, Luka Chkhetiani, Andrew Ehrenberg, Robert McHardy, Rami Botros, Yash Khare, Andrea Vanzo, Taufiquzzaman Peyash, Gabriel Oexle, Michael Liang, et al.

</span>
<span class="ltx_bibblock">Anatomy of industrial scale multilingual asr.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.09841</span>, 2024.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.05530</span>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
P.K. Rubenstein, C. Asawaroengchai, D.D. Nguyen, et al.

</span>
<span class="ltx_bibblock">AudioPaLM: A large language model that can speak and listen.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">arXiv:2306.12925</span>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang.

</span>
<span class="ltx_bibblock">Salmonn: Towards generic hearing abilities for large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.13289</span>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi.

</span>
<span class="ltx_bibblock">Hybrid ctc/attention architecture for end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, 11(8):1240–1253, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Wu, Y. Gaur, Z. Chen, et al.

</span>
<span class="ltx_bibblock">On decoder-only architecture for speech-to-text and large language model integration.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv:2307.03917</span>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al.

</span>
<span class="ltx_bibblock">Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6182–6186. IEEE, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, et al.

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100 languages.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.01037</span>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Ding Zhao, Tara N Sainath, David Rybach, Pat Rondon, Deepti Bhatia, Bo Li, and Ruoming Pang.

</span>
<span class="ltx_bibblock">Shallow-fusion end-to-end contextual biasing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, pages 1418–1422, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Authors (alphabetical order)</h2>

<figure id="S6.tab1" class="ltx_table">
<table id="S6.tab1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.tab1.1.1" class="ltx_tr">
<td id="S6.tab1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;">Ye Bai</span>
</span>
</td>
<td id="S6.tab1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;">Mingkun Huang</span>
</span>
</td>
<td id="S6.tab1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.1.3.1.1" class="ltx_p" style="width:113.8pt;">Ming Tu</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.2" class="ltx_tr">
<td id="S6.tab1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.2.1.1.1" class="ltx_p" style="width:113.8pt;">Jingping Chen</span>
</span>
</td>
<td id="S6.tab1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.2.2.1.1" class="ltx_p" style="width:113.8pt;">Youjia Huang</span>
</span>
</td>
<td id="S6.tab1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.2.3.1.1" class="ltx_p" style="width:113.8pt;">Bo Wang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.3" class="ltx_tr">
<td id="S6.tab1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.3.1.1.1" class="ltx_p" style="width:113.8pt;">Jitong Chen</span>
</span>
</td>
<td id="S6.tab1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.3.2.1.1" class="ltx_p" style="width:113.8pt;">Jishuo Jin</span>
</span>
</td>
<td id="S6.tab1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.3.3.1.1" class="ltx_p" style="width:113.8pt;">Hao Wang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.4" class="ltx_tr">
<td id="S6.tab1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.4.1.1.1" class="ltx_p" style="width:113.8pt;">Wei Chen</span>
</span>
</td>
<td id="S6.tab1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.4.2.1.1" class="ltx_p" style="width:113.8pt;">Fanliu Kong</span>
</span>
</td>
<td id="S6.tab1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.4.3.1.1" class="ltx_p" style="width:113.8pt;">Yuping Wang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.5" class="ltx_tr">
<td id="S6.tab1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.5.1.1.1" class="ltx_p" style="width:113.8pt;">Zhuo Chen</span>
</span>
</td>
<td id="S6.tab1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.5.2.1.1" class="ltx_p" style="width:113.8pt;">Zongwei Lan</span>
</span>
</td>
<td id="S6.tab1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.5.3.1.1" class="ltx_p" style="width:113.8pt;">Yuxuan Wang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.6" class="ltx_tr">
<td id="S6.tab1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.6.1.1.1" class="ltx_p" style="width:113.8pt;">Chuang Ding</span>
</span>
</td>
<td id="S6.tab1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.6.2.1.1" class="ltx_p" style="width:113.8pt;">Tianyu Li</span>
</span>
</td>
<td id="S6.tab1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.6.3.1.1" class="ltx_p" style="width:113.8pt;">Hanzhang Xia</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.7" class="ltx_tr">
<td id="S6.tab1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.7.1.1.1" class="ltx_p" style="width:113.8pt;">Linhao Dong</span>
</span>
</td>
<td id="S6.tab1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.7.2.1.1" class="ltx_p" style="width:113.8pt;">Xiaoyang Li</span>
</span>
</td>
<td id="S6.tab1.1.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.7.3.1.1" class="ltx_p" style="width:113.8pt;">Rui Xia</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.8" class="ltx_tr">
<td id="S6.tab1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.8.1.1.1" class="ltx_p" style="width:113.8pt;">Qianqian Dong</span>
</span>
</td>
<td id="S6.tab1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.8.2.1.1" class="ltx_p" style="width:113.8pt;">Zeyang Li</span>
</span>
</td>
<td id="S6.tab1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.8.3.1.1" class="ltx_p" style="width:113.8pt;">Shuangyi Xie</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.9" class="ltx_tr">
<td id="S6.tab1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.9.1.1.1" class="ltx_p" style="width:113.8pt;">Yujiao Du</span>
</span>
</td>
<td id="S6.tab1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.9.2.1.1" class="ltx_p" style="width:113.8pt;">Zehua Lin</span>
</span>
</td>
<td id="S6.tab1.1.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.9.3.1.1" class="ltx_p" style="width:113.8pt;">Hongmin Xu</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.10" class="ltx_tr">
<td id="S6.tab1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.10.1.1.1" class="ltx_p" style="width:113.8pt;">Kepan Gao</span>
</span>
</td>
<td id="S6.tab1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.10.2.1.1" class="ltx_p" style="width:113.8pt;">Rui Liu</span>
</span>
</td>
<td id="S6.tab1.1.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.10.3.1.1" class="ltx_p" style="width:113.8pt;">Meng Yang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.11" class="ltx_tr">
<td id="S6.tab1.1.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.11.1.1.1" class="ltx_p" style="width:113.8pt;">Lu Gao</span>
</span>
</td>
<td id="S6.tab1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.11.2.1.1" class="ltx_p" style="width:113.8pt;">Shouda Liu</span>
</span>
</td>
<td id="S6.tab1.1.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.11.3.1.1" class="ltx_p" style="width:113.8pt;">Bihong Zhang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.12" class="ltx_tr">
<td id="S6.tab1.1.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.12.1.1.1" class="ltx_p" style="width:113.8pt;">Yi Guo</span>
</span>
</td>
<td id="S6.tab1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.12.2.1.1" class="ltx_p" style="width:113.8pt;">Lu Lu</span>
</span>
</td>
<td id="S6.tab1.1.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.12.3.1.1" class="ltx_p" style="width:113.8pt;">Jun Zhang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.13" class="ltx_tr">
<td id="S6.tab1.1.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.13.1.1.1" class="ltx_p" style="width:113.8pt;">Minglun Han</span>
</span>
</td>
<td id="S6.tab1.1.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.13.2.1.1" class="ltx_p" style="width:113.8pt;">Yizhou Lu</span>
</span>
</td>
<td id="S6.tab1.1.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.13.3.1.1" class="ltx_p" style="width:113.8pt;">Wanyi Zhang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.14" class="ltx_tr">
<td id="S6.tab1.1.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.14.1.1.1" class="ltx_p" style="width:113.8pt;">Ting Han</span>
</span>
</td>
<td id="S6.tab1.1.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.14.2.1.1" class="ltx_p" style="width:113.8pt;">Jingting Ma</span>
</span>
</td>
<td id="S6.tab1.1.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.14.3.1.1" class="ltx_p" style="width:113.8pt;">Yang Zhang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.15" class="ltx_tr">
<td id="S6.tab1.1.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.15.1.1.1" class="ltx_p" style="width:113.8pt;">Wenchao Hu</span>
</span>
</td>
<td id="S6.tab1.1.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.15.2.1.1" class="ltx_p" style="width:113.8pt;">Shengtao Ma</span>
</span>
</td>
<td id="S6.tab1.1.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.15.3.1.1" class="ltx_p" style="width:113.8pt;">Yawei Zhang</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.16" class="ltx_tr">
<td id="S6.tab1.1.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.16.1.1.1" class="ltx_p" style="width:113.8pt;">Xinying Hu</span>
</span>
</td>
<td id="S6.tab1.1.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.16.2.1.1" class="ltx_p" style="width:113.8pt;">Yulin Pei</span>
</span>
</td>
<td id="S6.tab1.1.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.16.3.1.1" class="ltx_p" style="width:113.8pt;">Yijie Zheng</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.17" class="ltx_tr">
<td id="S6.tab1.1.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.17.1.1.1" class="ltx_p" style="width:113.8pt;">Yuxiang Hu</span>
</span>
</td>
<td id="S6.tab1.1.17.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.17.2.1.1" class="ltx_p" style="width:113.8pt;">Chen Shen</span>
</span>
</td>
<td id="S6.tab1.1.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.17.3.1.1" class="ltx_p" style="width:113.8pt;">Ming Zou</span>
</span>
</td>
</tr>
<tr id="S6.tab1.1.18" class="ltx_tr">
<td id="S6.tab1.1.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.18.1.1.1" class="ltx_p" style="width:113.8pt;">Deyu Hua</span>
</span>
</td>
<td id="S6.tab1.1.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.18.2.1.1" class="ltx_p" style="width:113.8pt;">Tian Tan</span>
</span>
</td>
<td id="S6.tab1.1.18.3" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S6.tab1.1.19" class="ltx_tr">
<td id="S6.tab1.1.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.19.1.1.1" class="ltx_p" style="width:113.8pt;">Lu Huang</span>
</span>
</td>
<td id="S6.tab1.1.19.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.tab1.1.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S6.tab1.1.19.2.1.1" class="ltx_p" style="width:113.8pt;">Xiaogang Tian</span>
</span>
</td>
<td id="S6.tab1.1.19.3" class="ltx_td ltx_align_top"></td>
</tr>
</table>
</figure>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Detailed Results of Seed-ASR (ML)</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">In Table <a href="#A1.T12" title="Table 12 ‣ A.1 Detailed Results of Seed-ASR (ML) ‣ Appendix A Appendix ‣ Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, we present a language-wise comparison among Google USM, Whisper Large-v3, and Seed-ASR (ML) on the multilingual multi-domain evaluation sets for non-English languages. The results clearly demonstrate Seed-ASR (ML)’s advantage in every language, with WER reduction ranging from 26% to 47%. For the two relatively low-resource languages, Arabic (AR) and Indonesian (ID), which are spoken by large populations in the world, Seed-ASR (ML) achieves a relative WER reduction of over 45%.</p>
</div>
<figure id="A1.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Language-wise performance of Seed-ASR (ML) on multilingual multi-domain evaluation sets.</figcaption>
<div id="A1.T12.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:124.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.4pt,9.8pt) scale(0.864485845080195,0.864485845080195) ;">
<table id="A1.T12.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T12.1.1.1" class="ltx_tr">
<td id="A1.T12.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Language</td>
<td id="A1.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Google USM</td>
<td id="A1.T12.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Whisper Large-v3</td>
<td id="A1.T12.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Seed-ASR (ML)</td>
</tr>
<tr id="A1.T12.1.1.2" class="ltx_tr">
<td id="A1.T12.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AR</td>
<td id="A1.T12.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.21</td>
<td id="A1.T12.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.31</td>
<td id="A1.T12.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T12.1.1.2.4.1" class="ltx_text ltx_font_bold">18.69</span></td>
</tr>
<tr id="A1.T12.1.1.3" class="ltx_tr">
<td id="A1.T12.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">ES</td>
<td id="A1.T12.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">15.20</td>
<td id="A1.T12.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">16.68</td>
<td id="A1.T12.1.1.3.4" class="ltx_td ltx_align_center"><span id="A1.T12.1.1.3.4.1" class="ltx_text ltx_font_bold">10.28</span></td>
</tr>
<tr id="A1.T12.1.1.4" class="ltx_tr">
<td id="A1.T12.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">FR</td>
<td id="A1.T12.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">20.48</td>
<td id="A1.T12.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">17.62</td>
<td id="A1.T12.1.1.4.4" class="ltx_td ltx_align_center"><span id="A1.T12.1.1.4.4.1" class="ltx_text ltx_font_bold">12.70</span></td>
</tr>
<tr id="A1.T12.1.1.5" class="ltx_tr">
<td id="A1.T12.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">ID</td>
<td id="A1.T12.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">22.29</td>
<td id="A1.T12.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">20.47</td>
<td id="A1.T12.1.1.5.4" class="ltx_td ltx_align_center"><span id="A1.T12.1.1.5.4.1" class="ltx_text ltx_font_bold">10.86</span></td>
</tr>
<tr id="A1.T12.1.1.6" class="ltx_tr">
<td id="A1.T12.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">JA</td>
<td id="A1.T12.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">24.62</td>
<td id="A1.T12.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">18.57</td>
<td id="A1.T12.1.1.6.4" class="ltx_td ltx_align_center"><span id="A1.T12.1.1.6.4.1" class="ltx_text ltx_font_bold">13.72</span></td>
</tr>
<tr id="A1.T12.1.1.7" class="ltx_tr">
<td id="A1.T12.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">KO</td>
<td id="A1.T12.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">13.88</td>
<td id="A1.T12.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">13.07</td>
<td id="A1.T12.1.1.7.4" class="ltx_td ltx_align_center"><span id="A1.T12.1.1.7.4.1" class="ltx_text ltx_font_bold">7.77</span></td>
</tr>
<tr id="A1.T12.1.1.8" class="ltx_tr">
<td id="A1.T12.1.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">PT</td>
<td id="A1.T12.1.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">19.88</td>
<td id="A1.T12.1.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">18.78</td>
<td id="A1.T12.1.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="A1.T12.1.1.8.4.1" class="ltx_text ltx_font_bold">11.69</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Details of English and Multilingual public test sets used in Seed-ASR (ML) evaluation</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">The details of the English and multilingual public test sets are as follows:</p>
</div>
<div id="A1.SS2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.p2.1" class="ltx_p"><span id="A1.SS2.p2.1.1" class="ltx_text ltx_font_bold">Librispeech</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>: as per usual, we report the WER on the test-clean and test-other sets.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para ltx_noindent">
<p id="A1.SS2.p3.1" class="ltx_p"><span id="A1.SS2.p3.1.1" class="ltx_text ltx_font_bold">Tedlium 3</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>: The test set provided in Tedlium 3 with segmented transcripts is employed.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para ltx_noindent">
<p id="A1.SS2.p4.1" class="ltx_p"><span id="A1.SS2.p4.1.1" class="ltx_text ltx_font_bold">CallHome, Switchboard and AMI IHM</span>: we keep consistent with Whisper v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> by using the two corpora from LDC2002S09 and LDC2002T43 for CallHome and Switchboard. We only report the IHM subset from AMI corpus.</p>
</div>
<div id="A1.SS2.p5" class="ltx_para ltx_noindent">
<p id="A1.SS2.p5.1" class="ltx_p"><span id="A1.SS2.p5.1.1" class="ltx_text ltx_font_bold">Fleurs</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>: since transcripts from Fleurs test sets are annotated and processed with text normalization, we asked linguistics to conduct the inverse text normalization for the 8 languages and calculated the WERs on the corresponding transcripts.</p>
</div>
<div id="A1.SS2.p6" class="ltx_para ltx_noindent">
<p id="A1.SS2.p6.1" class="ltx_p"><span id="A1.SS2.p6.1.1" class="ltx_text ltx_font_bold">MLS</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>: we evaluated the test subset of English, Spanish, French and Portuguese from MLS.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Training Dataset Statistics</h3>

<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.p1.1" class="ltx_p">In this section, we present the statistical information regarding the amount and language of data used in self-supervised learning (SSL) of LUISE and supervised fine-tuning (SFT) of Seed-ASR. This includes four parts: the speech-only data used in the training of LUISE used in Seed-ASR (CN) and Seed-ASR (ML), and the general ASR data used for Seed-ASR (CN) and Seed-ASR (ML).</p>
</div>
<figure id="A1.F10" class="ltx_figure"><img src="/html/2407.04675/assets/x12.png" id="A1.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="232" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Training data statistics of the large-scale self-supervised learning of LUISE used in Seed-ASR (CN). (1) The total amount of training data is 7.7 million hours; (2) Mandarin Chinese has the highest proportion with about 5.6 million hours of speech data, accounting for about 74%; In addition to Mandarin Chinese, we also include other Chinese dialects and categorize them according to the Language Atlas of China <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>; (3) We also include English data from different regions, as well as a small amount of Mandarin-English code-switching data.</figcaption>
</figure>
<figure id="A1.F11" class="ltx_figure"><img src="/html/2407.04675/assets/x13.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="232" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Training data statistics of the large-scale self-supervised learning of LUISE used in Seed-ASR (ML). (1) The total amount of training data is 12.4 million hours; (2) English has the highest proportion with about 6.4 million hours speech data, accounting for 51.52%; (3) In addition to English, we also include data from more than 20 other languages; (4) Categories with less than 0.2% of the data (such as Romanian, Polish, Dutch, Russian, etc.) were grouped into the "Others".</figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2407.04675/assets/x14.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="266" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Training data statistics of the supervised fine-tuning of Seed-ASR (CN). (1) The total amount of training data is 562k hours; (2) Mandarin Chinese has the highest proportion with about 416k hours speech data, accounting for 73.94%; (3) In addition to Mandarin Chinese, we also include data of Chinese dialects and English from different regions; (4) Categories with less than 0.2% of the data (such as Jin, Min, Xiang, Hakka, etc.) were grouped into the "Others".</figcaption>
</figure>
<figure id="A1.F13" class="ltx_figure"><img src="/html/2407.04675/assets/x15.png" id="A1.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="266" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Training data statistics of the supervised fine-tuning of Seed-ASR (ML). (1) The total amount of training data is 314k hours; (2) English has the highest proportion with about 125k hours speech data, accounting for 39.84%; (3) In addition to English, we include some languages that are widely used around the world, as mentioned in the main text.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.04674" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.04675" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.04675">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.04675" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.04677" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:37:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
