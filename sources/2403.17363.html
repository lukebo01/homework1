<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.17363] Extracting Biomedical Entities from Noisy Audio Transcripts</title><meta property="og:description" content="Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Extracting Biomedical Entities from Noisy Audio Transcripts">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Extracting Biomedical Entities from Noisy Audio Transcripts">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.17363">

<!--Generated on Fri Apr  5 15:18:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Extracting Biomedical Entities from Noisy Audio Transcripts</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap. Prior works have primarily studied ASR’s efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings. In addressing the noise challenge, we present an innovative transcript-cleaning method using GPT4, investigating both zero-shot and few-shot methodologies. Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by GPT4, and the challenges GPT4 faces. This paper aims to foster improved understanding and potential solutions for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation practices.

<br class="ltx_break">
<br class="ltx_break">
<span id="id15.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>Named Entity Recognition, Biomedical Informatics, Audio Speech Recognition</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id14" class="ltx_logical-block">
<div id="id14.p1" class="ltx_para">
<p id="id14.p1.1" class="ltx_p ltx_align_center"><span id="id14.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Extracting Biomedical Entities from Noisy Audio Transcripts</span></p>
<br class="ltx_break ltx_centering">
<table id="id13.13" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id5.5.5" class="ltx_tr">
<td id="id5.5.5.5" class="ltx_td ltx_align_center"><span id="id5.5.5.5.5" class="ltx_text ltx_font_bold" style="font-size:120%;">Nima Ebadi<sup id="id5.5.5.5.5.1" class="ltx_sup"><span id="id5.5.5.5.5.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Kellen Morgan<sup id="id5.5.5.5.5.2" class="ltx_sup"><span id="id5.5.5.5.5.2.1" class="ltx_text ltx_font_medium">2</span></sup>, Adrian Tan<sup id="id5.5.5.5.5.3" class="ltx_sup"><span id="id5.5.5.5.5.3.1" class="ltx_text ltx_font_medium">3</span></sup>, Billy Linares<sup id="id5.5.5.5.5.4" class="ltx_sup"><span id="id5.5.5.5.5.4.1" class="ltx_text ltx_font_medium">3</span></sup>, Sheri Osborn<sup id="id5.5.5.5.5.5" class="ltx_sup"><span id="id5.5.5.5.5.5.1" class="ltx_text ltx_font_medium">4</span></sup>,</span></td>
</tr>
<tr id="id8.8.8" class="ltx_tr">
<td id="id8.8.8.3" class="ltx_td ltx_align_center"><span id="id8.8.8.3.3" class="ltx_text ltx_font_bold" style="font-size:120%;">Emma Majors<sup id="id8.8.8.3.3.1" class="ltx_sup"><span id="id8.8.8.3.3.1.1" class="ltx_text ltx_font_medium">5</span></sup>, Jeremy Davis<sup id="id8.8.8.3.3.2" class="ltx_sup"><span id="id8.8.8.3.3.2.1" class="ltx_text ltx_font_medium">5</span></sup>, and Anthony Rios<sup id="id8.8.8.3.3.3" class="ltx_sup"><span id="id8.8.8.3.3.3.1" class="ltx_text ltx_font_medium">4</span></sup></span></td>
</tr>
<tr id="id9.9.9" class="ltx_tr">
<td id="id9.9.9.1" class="ltx_td ltx_align_center">
<sup id="id9.9.9.1.1" class="ltx_sup">1</sup>Department of Electrical and Computer Engineering, The University of Texas at San Antonio</td>
</tr>
<tr id="id10.10.10" class="ltx_tr">
<td id="id10.10.10.1" class="ltx_td ltx_align_center">
<sup id="id10.10.10.1.1" class="ltx_sup">2</sup>Department of Management Science and Statistics, The University of Texas at San Antonio</td>
</tr>
<tr id="id11.11.11" class="ltx_tr">
<td id="id11.11.11.1" class="ltx_td ltx_align_center">
<sup id="id11.11.11.1.1" class="ltx_sup">3</sup>Data Analytics, The University of Texas at San Antonio</td>
</tr>
<tr id="id12.12.12" class="ltx_tr">
<td id="id12.12.12.1" class="ltx_td ltx_align_center">
<sup id="id12.12.12.1.1" class="ltx_sup">4</sup>Department of Information Systems and Cyber Security, The University of Texas at San Antonio</td>
</tr>
<tr id="id13.13.13" class="ltx_tr">
<td id="id13.13.13.1" class="ltx_td ltx_align_center">
<sup id="id13.13.13.1.1" class="ltx_sup">5</sup>Department of Neurology, Division of Neuropsychology, UT Health San Antonio</td>
</tr>
<tr id="id13.13.14.1" class="ltx_tr">
<td id="id13.13.14.1.1" class="ltx_td ltx_align_center">kellen.morgan@my.utsa.edu, emma.majors06@gmail.com, davisj20@uthscsa.edu</td>
</tr>
<tr id="id13.13.15.2" class="ltx_tr">
<td id="id13.13.15.2.1" class="ltx_td ltx_align_center">{nima.ebadi, billy.linares, sheri.orborn, anthony.rios}@utsa.edu</td>
</tr>
</tbody>
</table>
<p id="id14.p1.2" class="ltx_p ltx_align_center"><span id="id14.p1.2.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Speech Recognition (ASR) technology is pivotal in converting spoken language into written text and finds critical applications within clinical contexts. One important use is expediting medical transcription processes and efficiently documenting doctor-patient interactions. This seamless conversion reduces the time and resources traditionally spent on manual transcription, affording healthcare professionals more time for focused patient care. Specifically, ASR can seamlessly integrate into Electronic Health Record (EHR) systems, enabling real-time dictation of diagnoses, treatment plans, and patient notes, thereby augmenting the accuracy and immediacy of clinical documentation. Hence, this technology holds substantial promise in revolutionizing healthcare documentation practices. After successful conversion from audio to text, natural language processing (NLP) tools can be applied to the transcriptions for various tasks <cite class="ltx_cite ltx_citemacro_cite">Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>. Unfortunately, transcription is not accurate, particularly in noisy environments. Moreover, when NLP models are applied to noisy data that does not match the training data distribution, large drops in performance may be observed.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This paper focuses on the biomedical NLP entity recognition (NER) task applied to noisy audio transcripts. Named entity recognition is vital for many important clinical tasks, from extracting social determinants of health mentions from clinical notes to extracting mentions of adverse drug reactions. Clinicians may not be able to capture everything stated to them by a patient (e.g., specific adverse reactions to a drug), particularly if they need to transcribe information after an interaction via rote memory. Hence, if ASR can be used to record patient-clinician interactions, then NER systems can be applied to extract clinically relevant information for later use. We explore the viability of NER systems applied to noisy transcripts to better understand their performance in real-world settings, where records may have multiple speakers and background sounds.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> has recently called this difference in performance the ASR-NLP gap. At a high level, there are two primary causes for the ASR-NLP gap. First, transcription errors can completely change the words mentioned. For instance, if someone mentions the word “headache” (which could be a mention of a drug side effect), but if it is recognized as “headway,” then a traditional NER system would be unable to identify it. Second, the data distribution changes. Models trained on clean, non-transcribed data may capture different patterns in the text that are not available in the transcribed text. The patterns may be as simple as differences in punctuation and capitalization, but such patterns are essential for accurate NER.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Much of the prior work on studying ASR systems, particularly in biomedical domains, has focused on either developing or evaluating ASR systems for novel patient populations <cite class="ltx_cite ltx_citemacro_cite">Tran et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> or training and evaluating NLP systems on carefully corrected and relatively clean transcripts. For work evaluating ASR systems in the clinical domain, there have been low word error rates (WER) reported (e.g., 11% <cite class="ltx_cite ltx_citemacro_cite">Tran et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> 24.3% <cite class="ltx_cite ltx_citemacro_cite">Hacking et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>, and 10% <cite class="ltx_cite ltx_citemacro_cite">King et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>). However, the studies often report results on relatively clean recordings (e.g., without multiple background speakers or substantial background noise). Sometimes transcripts that are very noisy are completely removed from the evaluation data <cite class="ltx_cite ltx_citemacro_cite">King et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, potentially resulting in overly optimistic performance. Prior works have reported WERs much worse than the reported numbers in the clinical setting <cite class="ltx_cite ltx_citemacro_cite">Kodish-Wachs et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>, with WERs in the range of 30% to 60%. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Kodish-Wachs et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> also evaluated concept extraction software on transcriptions. However, they did not compare the performance difference between clean data and noisy transcripts. The numbers are still generally reported on “clean” transcripts with minimal background noise and background speakers. Finally, they do not provide any natural next steps for improving performance. Hence, the results may be much worse when evaluating substantially noisy environments.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we develop a new dataset so biomedical NLP researchers can directly improve and explore the biomedical ASR-NLP gap. Specifically, we introduce a dataset that extracts adverse drug reaction mentions and a dataset that extracts fruits and animals that would be mentioned as part of the Brief Test of Adult Cognition by Telephone (BTACT) exam. To the best of our knowledge, this will be the first publicly available dataset to allow for careful evaluation of the ASR-NLP gap in the biomedical domain.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, based on current research gaps in the ASR-NLP gap for biomedical applications, this paper makes the following contributions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">(i)</span></span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel dataset of nearly 2000 clean and noisy recordings for biomedical-related ASR-NER called BioASR-NER.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset is available at <a target="_blank" href="https://zenodo.org/records/10864063" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10864063</a>.</span></span></span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">(ii)</span></span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce a simple approach to improving model performance via a transcript-cleaning procedure using GPT4. We explore both zero-shot and few-shot methodologies for when ground-truth noisy and cleaned transcription pairs are limited.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i3.1.1.1" class="ltx_text ltx_font_bold">(iii)</span></span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Finally, we perform an informative error analysis showcasing the types of errors made by the transcription software, the type of errors GPT4 corrects, and the types of errors GPT4 cannot handle accurately.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this work, we describe two major research lines relevant to this paper: Biomedical ASR-NLP, which includes work on Biomedical ASR technologies and NLP applied to transcriptions (clean and noisy if available), and Biomedical NER, which discusses some recent work on developing methods to extract biomedical entities from text.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.1.   Biomedical NER</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There have been many datasets and methods developed for the detection of biomedical entities <cite class="ltx_cite ltx_citemacro_cite">Leaman and Gonzalez (<a href="#bib.bib21" title="" class="ltx_ref">2008</a>); Song et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>); Rocktäschel et al. (<a href="#bib.bib35" title="" class="ltx_ref">2012</a>); Chiu et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>); Lee et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>); Sun et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); López-Úbeda et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Weber et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>. Specifically, there are biomedical NER tasks including, but not limited to, extracting mentions of social determinants of health from electronic medical records, detecting adverse drug interactions in patient self-reports <cite class="ltx_cite ltx_citemacro_cite">Karimi et al. (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite>, extracting chemical and drugs mentions <cite class="ltx_cite ltx_citemacro_cite">Rocktäschel et al. (<a href="#bib.bib35" title="" class="ltx_ref">2012</a>)</cite>, and extracting gene mentions in biomedical research articles <cite class="ltx_cite ltx_citemacro_cite">Pyysalo et al. (<a href="#bib.bib30" title="" class="ltx_ref">2007</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Many novel methodological approaches have been developed for each of the tasks. For example, <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> developed a specialized BERT model tailored for biomedical applications, demonstrating improvements over previous state-of-the-art results. Additionally, HunFlair <cite class="ltx_cite ltx_citemacro_cite">Weber et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> introduced a methodology that combines word, contextual, and character embeddings within a unified framework, achieving state-of-the-art performance. <cite class="ltx_cite ltx_citemacro_citet">Tong et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> introduce a multi-task learning framework for biomedical NER that integrates multiple related training objectives to improve entity extraction. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Watanabe et al. (<a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite> improve biomedical NER by incorporating auxiliary learning with multiple datasets. <cite class="ltx_cite ltx_citemacro_citet">Guan and Zhou (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> incorporated information between word pairs to improve biomedical NER performance. And more recently, <cite class="ltx_cite ltx_citemacro_citet">Ghosh et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> explored synthetic data augmentation to improve low-resource biomedical NER. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> improved few-shot NER via contrastive prompt tuning.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Overall, our work is most related to research on out-of-domain performance of information extraction systems <cite class="ltx_cite ltx_citemacro_cite">Rios et al. (<a href="#bib.bib34" title="" class="ltx_ref">2018</a>); Jia et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Poerner et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>); Vu et al. (<a href="#bib.bib46" title="" class="ltx_ref">2020</a>); Nguyen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Poerner et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> train word embeddings on the target domain and the align them to the general domain to improve generalization. <cite class="ltx_cite ltx_citemacro_citet">Nguyen et al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> introduce “hardness” related information to better generalize biomedical NER models across domains. However, contrary to prior research, our work differs in one major way. Specifically, we are focused on a particular kind of domain shift. Prior work has explored two disparate domains such as social media and electronic health records. In our paper, the underlying data does not change. Instead, the style of the content changes because of the noisy channel caused by the transcription process.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.17363/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our data collection process. The process has four main steps: 1) We collect the initial datasets (CADEC and BTACT); 2) Graduate assistants read and record the text in the datasets; 3) We normalize each audio file to the same loudness; and 4) we generate noisy audio files by merging multiple speakers and adding background noise.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.2.   Biomedical ASR-NLP</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As discussed in the Introduction, much of the work on automatic speech recognition (ASR) systems provides overly optimistic word error rates (WER). Many datasets lack background noise and only have a single speaker. Yet, real-world datasets may have background noise, multiple background speakers of various volumes, and even dropped connections. Recent studies and reviews have discussed how digital scribes (ASR systems) are necessary to reduce physician burden to provide more reliable care <cite class="ltx_cite ltx_citemacro_cite">Quiroz et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); van Buchem et al. (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Recently, there have been two major research directions for biomedical applications related to ASR. First, new ASR systems have been proposed directly for particular patient populations <cite class="ltx_cite ltx_citemacro_cite">Kodish-Wachs et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Hacking et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> introduces a novel ASR system for older adults in an interview setting. Likewise, there has been substantial work on developing and improving biomedical ASR systems for languages besides English <cite class="ltx_cite ltx_citemacro_cite">Dhuriya et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>. Second, there has been research that has evaluated commercial ASR systems in the biomedical domain. For example, <cite class="ltx_cite ltx_citemacro_cite">Tran et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> evaluated proprietary ASR systems for their ability to detect non-lexical conversational sounds such as “Mm-hm” and “Uh-uh”, which can be clinically relevant in many scenarios. The authors found that current systems are unable to detect them regularly. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Paats et al. (<a href="#bib.bib28" title="" class="ltx_ref">2015</a>)</cite> evaluated ASR systems in Estonian languages. Finally, there has been research that has developed and evaluated NLP systems on ASR transcripts. For example, <cite class="ltx_cite ltx_citemacro_cite">Ganoe et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> develop NER tools to extract medication mentions in transcripts of primary care conversations. Yet, much of the prior work applying NLP tools to transcripts has used “cleaned” transcripts with limited transcription errors where a human has ensured the transcript is accurate. In this work, we focus on noisy transcripts in the presence of background noise and multiple speakers. Moreover, for work that evaluates ASR systems using WER, that performance does not correlate with the quality of the transcription by a human evaluator and does not correlate with downstream performance on NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Whetten and Kennington (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>); Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">From a methodological standpoint, some recent work has explored reducing transcription errors. To this end, our work is similar to <cite class="ltx_cite ltx_citemacro_citet">Mani et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> that developed a seq2seq method to reduce transcription errors applied after a mainstream ASR process. Our work expands on this direction in two ways. First, we provide a unique dataset in a domain that lacks publicly available data. Second, our work analyzes the impact on NER directly, not WER, which can negatively correlate with NER performance.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Overall, our work is most similar to <cite class="ltx_cite ltx_citemacro_citet">Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>. At a high level <cite class="ltx_cite ltx_citemacro_citet">Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> analyzed the relationship between ASR performance and NER model performance. They found that the NER models make errors on the ASR-transcribed data, even when the entity is contained in the transcript. This can be caused by covariate shift (e.g., we would not expect a model trained on general data to generalize to biomedical articles). However, our work differs in three major ways. First, the focus of this paper is on the biomedical domain. There are limited publicly available datasets that researchers can use to develop new methods for improving downstream tasks (e.g., NER) on noisy transcriptions. Second, our focus is on noisy audio. Specifically, our audio contains multiple background speakers and background noise (e.g., TV sounds). Compared to prior work applying NLP to transcripts, our transcripts are not “clean.” Third, we introduce a simple method of improving NER system performance without training on domain-specific transcribed data, which is advocated by <cite class="ltx_cite ltx_citemacro_citet">Szymański et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>. Obtaining NER annotations on noisy transcriptions is time-consuming and infeasible in a timely manner. Hence, our approach can improve existing NER model performance when applied to noisy transcripts with only a few examples of noisy and clean transcripts (the actual NER annotations are not required).</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.2pt;height:136pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.6pt,11.2pt) scale(0.858344095884296,0.858344095884296) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:1.75pt;padding-bottom:1.75pt;"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.75pt;padding-bottom:1.75pt;">
<span id="S2.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.2.1.1" class="ltx_p" style="width:426.8pt;"><span id="S2.T1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;" rowspan="2"><span id="S2.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">CADEC</span></th>
<td id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;">
<span id="S2.T1.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.2.1.2.1.1" class="ltx_p" style="width:426.8pt;">i actually am taking provacal, but when I bring up the drug, it brings me to lipitor. I have experienced fatigue, hip pain,some joint pain in knee.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;">
<span id="S2.T1.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.3.2.1.1.1" class="ltx_p" style="width:426.8pt;">I would not recommend this drug,my Doctor didn’t explain any risk to taking this drug,although it lowered my cholesterol some,I changed my diet and started an exercise plan,I quit taking the drug 2 months ago and have continually lowered my chol. level.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;" rowspan="5"><span id="S2.T1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Synthetic BTACT</span></th>
<td id="S2.T1.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;">
<span id="S2.T1.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.4.3.2.1.1" class="ltx_p" style="width:426.8pt;">Let me see what I can do. mortar, cod, lemming, vole, quail, pigeon, rodent, laboratory rat strains, turkey breeds, eel, great blue heron, ringneck dove, bonobo, prawn, record. That’s something I’ll need some more time to consider. rodent. I’m concerned that I might not be able to provide a well-informed response. rodent, laboratory rat strains, turkey breeds, eel, great blue heron, ringneck dove, bonobo, prawn, pigeon, record, cockroach, pike</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-top:1.75pt;padding-bottom:1.75pt;">
<span id="S2.T1.1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.5.4.1.1.1" class="ltx_p" style="width:426.8pt;">Okay, let’s get to work. loquat, mouse melon, soda, kiwifruit, cucumber, lime, plantain, white currant, mouse melon, height, rambutan, apple, cucumber, citrus, lime, jackfruit, goji berry, loquat.</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Modified examples from the CADEC and Synthetic BTACT datasets.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This paper uses CADEC <cite class="ltx_cite ltx_citemacro_cite">Karimi et al. (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite> and a Synthetic BTACT dataset. CADEC is a popular NER dataset for extracting adverse drug reactions from experiences written by patients, and the Synthetic BTACT dataset is a novel dataset we created that simulates questions of the Brief Test of Adult Cognition by Telephone (BTACT). For both datasets, we have research assistants read each item and record an audio file of the reading. We generate noisy audio files by merging the files of multiple speakers and background noises/sounds. A high-level overview of the data collection process is shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Biomedical NER ‣ 2. Related Work ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The details of the curation and creation are described in the following subsections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Dataset Curation</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CADEC.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The CSIRO Adverse Drug Event Corpus (CADEC)<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The dataset is publicly accessible at https://data.csiro.au.</span></span></span> is an extensively annotated collection of medical forum posts centered on patient-reported Adverse Drug Events (ADEs). Derived from social media discussions, the corpus comprises text predominantly written in colloquial language, often straying from conventional English grammar and punctuation norms. The annotations reference various concepts, including drugs, adverse effects, symptoms, and associated diseases, all linked to controlled vocabularies such as SNOMED Clinical Terms and MedDRA. Rigorous annotation guidelines, multi-stage annotations, inter-annotator agreement assessments, and a final review by a clinical terminologist ensure the high quality of annotations. This corpus, initially sourced from <a href="Askapatient.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">Askapatient.com</a>, proves invaluable for research in information extraction and broader text mining from social media, especially for identifying potential adverse drug reactions directly reported by patients. This resource empowers patients by encouraging the sharing of side effects and success stories, advocating for informed health decisions through real-life experiences with drug treatments. <a href="AskaPatient.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">AskaPatient.com</a> provides tools to support and inform the engaged patient. Overall, the entity types in the dataset are adverse drug reaction (ADR), drug, finding, disease, and symptom. Examples can be found in Table <a href="#S2.T1" title="Table 1 ‣ 2.2. Biomedical ASR-NLP ‣ 2. Related Work ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:89pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(96.2pt,-19.7pt) scale(1.79717908783377,1.79717908783377) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Text Files</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"># Audio</th>
<th id="S3.T2.1.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"># Types</th>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">CADEC</td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">1,250</td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">1,000</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">5</td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<td id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">Syntehtic BTACT</td>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_right ltx_border_bb">1,000</td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_bb">1,000</td>
<td id="S3.T2.1.1.3.3.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>This table reports the basic dataset statistics for both CADEC and Synthetic BTACT, including the number of audio files (# Audio) and the number of named entity classes (# Types).</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.17363/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the training procedure and the prediction strategies we explore to improve biomedical NER performance.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic BTACT.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The Brief Test of Adult Cognition by Telephone (BTACT) is a succinct yet comprehensive screening tool designed for assessing cognitive function, particularly in the context of dementia. Administered either in person or over the phone, this battery of tests evaluates key cognitive domains, including episodic verbal memory, working memory, verbal fluency, inductive reasoning, and processing speed. Developed for use in the National Survey of Midlife Development in the United States (MIDUS), the BTACT combines adapted neuropsychological tests with novel subtests. Extensive research has validated its effectiveness as a dementia screening measure across a diverse range of individuals, providing a valuable tool for early detection and intervention in cognitive decline. This versatile assessment tool holds promise for enhancing dementia diagnosis and care, particularly in situations where in-person evaluation may not be feasible.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">We create synthetic BTACT subtest answers for the questions, “List as many animals as possible in 30 seconds” and “List as many fruits as possible in 30 seconds.” Specifically, we randomly generate a list of fruits or animals using publicly available lexicons.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/imsky/wordlists" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/imsky/wordlists</a></span></span></span> Next, we randomly inject incorrect entities (non-animal and non-fruit) into the respective lists. Next, we randomly add an introduction sentence (e.g., “Okay, let me try to list as many as I can.”) and interjections (e.g., “Let me think for a second”) in the middle of the lists. The entity types are “animal”, “fruit”, and “other.” Examples can be found in Table <a href="#S2.T1" title="Table 1 ‣ 2.2. Biomedical ASR-NLP ‣ 2. Related Work ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Audio Recordings and mixing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Next, we had research assistants read each of the scripts from both datasets. Luckily, the examples in both datasets are written in first-person, which also helps more natural readings. In total, we had five diverse assistants with respect to age and gender that helped the recording process. Next, each recording was normalized to ensure the volume (loudness) was the same across all speakers using pyloudnorm <cite class="ltx_cite ltx_citemacro_cite">Steinmetz and Reiss (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">After generating audio recordings from both datasets, we randomly sampled the signal-to-interference (SNR) ratio to merge each audio file with the audio files of 2 to 3 other speakers and a background noise/sound. The SNR used for the CADEC and the Synthetic BTACT dataset differs when we generate noisy files. We differ in the SNR ranges because the CADEC dataset has more “signal” because of the relative fluency of the text. Intuitively, the transcription models can understand how to extract words when they follow common syntactic patterns (e.g., a noun follows a determiner). However, the Synthetic BTACT data contains large lists of nouns, and the relation of one noun to the next provides little information for prediction. An example of this phenomenon can be seen in Figure <a href="#S2.T1" title="Table 1 ‣ 2.2. Biomedical ASR-NLP ‣ 2. Related Work ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For the CADEC dataset, we randomly mix each audio file with other users using an SNR sampled from <math id="S3.SS2.p2.1.m1.3" class="ltx_Math" alttext="\{-1,0,6\}" display="inline"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3.1" xref="S3.SS2.p2.1.m1.3.3.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.1.2" xref="S3.SS2.p2.1.m1.3.3.2.cmml">{</mo><mrow id="S3.SS2.p2.1.m1.3.3.1.1" xref="S3.SS2.p2.1.m1.3.3.1.1.cmml"><mo id="S3.SS2.p2.1.m1.3.3.1.1a" xref="S3.SS2.p2.1.m1.3.3.1.1.cmml">−</mo><mn id="S3.SS2.p2.1.m1.3.3.1.1.2" xref="S3.SS2.p2.1.m1.3.3.1.1.2.cmml">1</mn></mrow><mo id="S3.SS2.p2.1.m1.3.3.1.3" xref="S3.SS2.p2.1.m1.3.3.2.cmml">,</mo><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p2.1.m1.3.3.1.4" xref="S3.SS2.p2.1.m1.3.3.2.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">6</mn><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.1.5" xref="S3.SS2.p2.1.m1.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><set id="S3.SS2.p2.1.m1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.1"><apply id="S3.SS2.p2.1.m1.3.3.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1"><minus id="S3.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1"></minus><cn type="integer" id="S3.SS2.p2.1.m1.3.3.1.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1.2">1</cn></apply><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.p2.1.m1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2">6</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">\{-1,0,6\}</annotation></semantics></math> (negative scores mean the background is “louder” than the main speaker), and the background noise SNR is sampled from <math id="S3.SS2.p2.2.m2.6" class="ltx_Math" alttext="\{-1,0,3,6,9,12\}" display="inline"><semantics id="S3.SS2.p2.2.m2.6a"><mrow id="S3.SS2.p2.2.m2.6.6.1" xref="S3.SS2.p2.2.m2.6.6.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.6.6.1.2" xref="S3.SS2.p2.2.m2.6.6.2.cmml">{</mo><mrow id="S3.SS2.p2.2.m2.6.6.1.1" xref="S3.SS2.p2.2.m2.6.6.1.1.cmml"><mo id="S3.SS2.p2.2.m2.6.6.1.1a" xref="S3.SS2.p2.2.m2.6.6.1.1.cmml">−</mo><mn id="S3.SS2.p2.2.m2.6.6.1.1.2" xref="S3.SS2.p2.2.m2.6.6.1.1.2.cmml">1</mn></mrow><mo id="S3.SS2.p2.2.m2.6.6.1.3" xref="S3.SS2.p2.2.m2.6.6.2.cmml">,</mo><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">0</mn><mo id="S3.SS2.p2.2.m2.6.6.1.4" xref="S3.SS2.p2.2.m2.6.6.2.cmml">,</mo><mn id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml">3</mn><mo id="S3.SS2.p2.2.m2.6.6.1.5" xref="S3.SS2.p2.2.m2.6.6.2.cmml">,</mo><mn id="S3.SS2.p2.2.m2.3.3" xref="S3.SS2.p2.2.m2.3.3.cmml">6</mn><mo id="S3.SS2.p2.2.m2.6.6.1.6" xref="S3.SS2.p2.2.m2.6.6.2.cmml">,</mo><mn id="S3.SS2.p2.2.m2.4.4" xref="S3.SS2.p2.2.m2.4.4.cmml">9</mn><mo id="S3.SS2.p2.2.m2.6.6.1.7" xref="S3.SS2.p2.2.m2.6.6.2.cmml">,</mo><mn id="S3.SS2.p2.2.m2.5.5" xref="S3.SS2.p2.2.m2.5.5.cmml">12</mn><mo stretchy="false" id="S3.SS2.p2.2.m2.6.6.1.8" xref="S3.SS2.p2.2.m2.6.6.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.6b"><set id="S3.SS2.p2.2.m2.6.6.2.cmml" xref="S3.SS2.p2.2.m2.6.6.1"><apply id="S3.SS2.p2.2.m2.6.6.1.1.cmml" xref="S3.SS2.p2.2.m2.6.6.1.1"><minus id="S3.SS2.p2.2.m2.6.6.1.1.1.cmml" xref="S3.SS2.p2.2.m2.6.6.1.1"></minus><cn type="integer" id="S3.SS2.p2.2.m2.6.6.1.1.2.cmml" xref="S3.SS2.p2.2.m2.6.6.1.1.2">1</cn></apply><cn type="integer" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">0</cn><cn type="integer" id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2">3</cn><cn type="integer" id="S3.SS2.p2.2.m2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3">6</cn><cn type="integer" id="S3.SS2.p2.2.m2.4.4.cmml" xref="S3.SS2.p2.2.m2.4.4">9</cn><cn type="integer" id="S3.SS2.p2.2.m2.5.5.cmml" xref="S3.SS2.p2.2.m2.5.5">12</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.6c">\{-1,0,3,6,9,12\}</annotation></semantics></math>. For the Synthetic BTACT dataset, we randomly sample the SNR from the set <math id="S3.SS2.p2.3.m3.3" class="ltx_Math" alttext="\{4,6,9\}" display="inline"><semantics id="S3.SS2.p2.3.m3.3a"><mrow id="S3.SS2.p2.3.m3.3.4.2" xref="S3.SS2.p2.3.m3.3.4.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.3.4.2.1" xref="S3.SS2.p2.3.m3.3.4.1.cmml">{</mo><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">4</mn><mo id="S3.SS2.p2.3.m3.3.4.2.2" xref="S3.SS2.p2.3.m3.3.4.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml">6</mn><mo id="S3.SS2.p2.3.m3.3.4.2.3" xref="S3.SS2.p2.3.m3.3.4.1.cmml">,</mo><mn id="S3.SS2.p2.3.m3.3.3" xref="S3.SS2.p2.3.m3.3.3.cmml">9</mn><mo stretchy="false" id="S3.SS2.p2.3.m3.3.4.2.4" xref="S3.SS2.p2.3.m3.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.3b"><set id="S3.SS2.p2.3.m3.3.4.1.cmml" xref="S3.SS2.p2.3.m3.3.4.2"><cn type="integer" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">4</cn><cn type="integer" id="S3.SS2.p2.3.m3.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2">6</cn><cn type="integer" id="S3.SS2.p2.3.m3.3.3.cmml" xref="S3.SS2.p2.3.m3.3.3">9</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.3c">\{4,6,9\}</annotation></semantics></math>, and the background SNR is sampled from <math id="S3.SS2.p2.4.m4.4" class="ltx_Math" alttext="\{3,6,9,12\}" display="inline"><semantics id="S3.SS2.p2.4.m4.4a"><mrow id="S3.SS2.p2.4.m4.4.5.2" xref="S3.SS2.p2.4.m4.4.5.1.cmml"><mo stretchy="false" id="S3.SS2.p2.4.m4.4.5.2.1" xref="S3.SS2.p2.4.m4.4.5.1.cmml">{</mo><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">3</mn><mo id="S3.SS2.p2.4.m4.4.5.2.2" xref="S3.SS2.p2.4.m4.4.5.1.cmml">,</mo><mn id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml">6</mn><mo id="S3.SS2.p2.4.m4.4.5.2.3" xref="S3.SS2.p2.4.m4.4.5.1.cmml">,</mo><mn id="S3.SS2.p2.4.m4.3.3" xref="S3.SS2.p2.4.m4.3.3.cmml">9</mn><mo id="S3.SS2.p2.4.m4.4.5.2.4" xref="S3.SS2.p2.4.m4.4.5.1.cmml">,</mo><mn id="S3.SS2.p2.4.m4.4.4" xref="S3.SS2.p2.4.m4.4.4.cmml">12</mn><mo stretchy="false" id="S3.SS2.p2.4.m4.4.5.2.5" xref="S3.SS2.p2.4.m4.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.4b"><set id="S3.SS2.p2.4.m4.4.5.1.cmml" xref="S3.SS2.p2.4.m4.4.5.2"><cn type="integer" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">3</cn><cn type="integer" id="S3.SS2.p2.4.m4.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2">6</cn><cn type="integer" id="S3.SS2.p2.4.m4.3.3.cmml" xref="S3.SS2.p2.4.m4.3.3">9</cn><cn type="integer" id="S3.SS2.p2.4.m4.4.4.cmml" xref="S3.SS2.p2.4.m4.4.4">12</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.4c">\{3,6,9,12\}</annotation></semantics></math>. The background noise types include kitchen, TV, home appliances, music, and other ambient noises sampled from recordings of "daily life" environments. Overall, this mixing strategy is based on the work by <cite class="ltx_cite ltx_citemacro_citet">Ji et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Methods</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This study attempts to investigate the performance of NER systems on biomedical ASR-transcribed data. In this regard, after developing a set of baselines trained on original scripts, we transcribe the noisy audio and evaluate the NER performances on the noisy and original transcripts.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In addition, we introduce a simple framework to improve the NER system performance that requires no training on domain-specific transcribed data. We use the fourth iteration of the Generative Pre-trained Transformer (GPT4) <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> to post-process the ASR transcripts using its advanced capacity in contextual understanding and extensive knowledge, which covers a very broad range of biomedical concepts. During this post-processing, GPT4 is provided instructions to evaluate and refine the transcribed outputs to improve the downstream NER performance. In this regard, we study two approaches: zero-shot prompting and few-shot in-context learning.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.17363/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="322" height="423" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of the zero-shot prompting strategy we use for GPT4.</figcaption>
</figure>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NER baselines.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">Using the dataset, we develop a set of solid and representative baselines for biomedical NER and evaluate them on original scripts as well as the ASR-transcribed data. However, we use the original scripts for training the models using the widely adopted BIO tagging scheme <cite class="ltx_cite ltx_citemacro_cite">Sang and De Meulder (<a href="#bib.bib36" title="" class="ltx_ref">2003</a>)</cite>. We use pretrained language models such as BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, BioBERT <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> and Flair’s<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Flair is a framework for many NLP tasks including NER, POS tagging and text classification which provides a variety of embeddings as well as modules to combine with pretrained language models. <a target="_blank" href="https://flairnlp.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://flairnlp.github.io/</a></span></span></span> pretrained word embeddings such as GloVe that are fine-tuned on news articles and PubMed datasets. We combine these with the widely adopted BiLSTM-CRF <cite class="ltx_cite ltx_citemacro_cite">Lample et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>); Sui et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ASR.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">As for the ASR module, we use Whisper from OpenAI <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> because of its high-quality transcriptions and wide adoption in the literature <cite class="ltx_cite ltx_citemacro_cite">Zhuo et al. (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>. We use Whisper in an audio streaming fashion in which the audio input is divided into overlapping chunks, and the overlapping content is post-processed using Llama-2-7b <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> by giving instructions to concatenate the transcribed chunks–we also have tested with GPT4, but there has been a minimal improvement. The size of the chunk is also chosen based on optimum WER.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot Prompting.</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">Taking advantage of GPT4’s understanding of context and comprehensive knowledge in various biomedical domains, we instruct it to refine the transcript, knowing that the end goal is to improve the performance of the downstream NER system evaluated based on detecting the correct terms and categories.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.17363/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of the few-shot prompting strategy we use for GPT4.</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p2.1" class="ltx_p">The format of the zero-shot prompting is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4. Methods ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In addition, we provide the following instructions and contextual information: <span id="S4.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">i)</span> We explain the general topic discussed in each dataset; for example, for CADEC, we mention that the data is a transcribed medical conversation about adverse drug reactions. For BTACT dataset, we provide examples of valid animals/fruits. <span id="S4.SS0.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_bold">ii)</span> We mention that the audio is noisy, and some words may have been incorrectly transcribed. It has to detect the inappropriate terms and also rephrase them to phonetically similar, yet more appropriate ones. <span id="S4.SS0.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_bold">iii)</span> We also explain the multi-speaker nature of the noise and mention the possibility that some words may be transcribed from the background speakers. This way, GPT4 provides a more concise transcript or at least removes the off-topic sentences to improve the performance of the downstream NER.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Few-shot In-Context Learning.</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">We also leverage in-context learning to provide sample ASR noisy transcripts alongside the corresponding ground-truth script and tagged named entities. This way, we teach the GPT4 model through direct examples, and it learns to identify the relationships between the errors and correct similar errors in the test transcripts. We show the prompt format we generally use in Figure <a href="#S4.F4" title="Figure 4 ‣ Zero-shot Prompting. ‣ 4. Methods ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The GPT4 may also uncover unique relationships and come up with innovative approaches to refine the transcript. By exposing the model to various examples, from common transcription inaccuracies to more complex ones, GPT4 is encouraged to mimic corrections and understand the underlying principles of these mistakes <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Gutierrez et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Jin et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>); Cheng et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p2.1" class="ltx_p">To choose the set of examples, we randomly sample examples from the training set and cluster them based on their type of errors. In this regard, we look at the NER precision, recall, and F1; differences between the transcript and the original script along with unrecognized or misrecognized named entities. We also ask the GPT4 model to provide its insights about what might have caused the errors and how it can fix the errors. This insight is also used in the clustering algorithm. Finally, a set of varied examples is chosen to be provided to our in-context few-shot learning approach<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We have tested the effect of every cluster as well as various combinations, but providing examples from all clusters results in the maximum improvement.</span></span></span>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:546.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(79.3pt,-100.0pt) scale(1.57719606652816,1.57719606652816) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Precision</td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">Recall</td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">F1</td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T3.1.1.2.2.1.1" class="ltx_text">Original</span></th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">.669</td>
<td id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">.569</td>
<td id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">.615</td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<th id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BioBERT</th>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_right">.665</td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_right">.583</td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_right">.622</td>
</tr>
<tr id="S4.T3.1.1.4.4" class="ltx_tr">
<th id="S4.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S4.T3.1.1.4.4.2" class="ltx_td ltx_align_right">.679</td>
<td id="S4.T3.1.1.4.4.3" class="ltx_td ltx_align_right">.585</td>
<td id="S4.T3.1.1.4.4.4" class="ltx_td ltx_align_right">.629</td>
</tr>
<tr id="S4.T3.1.1.5.5" class="ltx_tr">
<th id="S4.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S4.T3.1.1.5.5.2" class="ltx_td ltx_align_right">.663</td>
<td id="S4.T3.1.1.5.5.3" class="ltx_td ltx_align_right">.658</td>
<td id="S4.T3.1.1.5.5.4" class="ltx_td ltx_align_right">.660</td>
</tr>
<tr id="S4.T3.1.1.6.6" class="ltx_tr">
<th id="S4.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Average</th>
<td id="S4.T3.1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t">.669</td>
<td id="S4.T3.1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">.599</td>
<td id="S4.T3.1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t">.631</td>
</tr>
<tr id="S4.T3.1.1.7.7" class="ltx_tr">
<th id="S4.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="5"><span id="S4.T3.1.1.7.7.1.1" class="ltx_text">Whisper</span></th>
<th id="S4.T3.1.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">BERT</th>
<td id="S4.T3.1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_tt">.215</td>
<td id="S4.T3.1.1.7.7.4" class="ltx_td ltx_align_right ltx_border_tt">.215</td>
<td id="S4.T3.1.1.7.7.5" class="ltx_td ltx_align_right ltx_border_tt">.229</td>
</tr>
<tr id="S4.T3.1.1.8.8" class="ltx_tr">
<th id="S4.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BioBERT</th>
<td id="S4.T3.1.1.8.8.2" class="ltx_td ltx_align_right">.243</td>
<td id="S4.T3.1.1.8.8.3" class="ltx_td ltx_align_right">.208</td>
<td id="S4.T3.1.1.8.8.4" class="ltx_td ltx_align_right">.224</td>
</tr>
<tr id="S4.T3.1.1.9.9" class="ltx_tr">
<th id="S4.T3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S4.T3.1.1.9.9.2" class="ltx_td ltx_align_right">.242</td>
<td id="S4.T3.1.1.9.9.3" class="ltx_td ltx_align_right">.272</td>
<td id="S4.T3.1.1.9.9.4" class="ltx_td ltx_align_right">.256</td>
</tr>
<tr id="S4.T3.1.1.10.10" class="ltx_tr">
<th id="S4.T3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S4.T3.1.1.10.10.2" class="ltx_td ltx_align_right">.236</td>
<td id="S4.T3.1.1.10.10.3" class="ltx_td ltx_align_right">.242</td>
<td id="S4.T3.1.1.10.10.4" class="ltx_td ltx_align_right">.239</td>
</tr>
<tr id="S4.T3.1.1.11.11" class="ltx_tr">
<th id="S4.T3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Average</th>
<td id="S4.T3.1.1.11.11.2" class="ltx_td ltx_align_right ltx_border_t">.234</td>
<td id="S4.T3.1.1.11.11.3" class="ltx_td ltx_align_right ltx_border_t">.234</td>
<td id="S4.T3.1.1.11.11.4" class="ltx_td ltx_align_right ltx_border_t">.237</td>
</tr>
<tr id="S4.T3.1.1.12.12" class="ltx_tr">
<th id="S4.T3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T3.1.1.12.12.1.1" class="ltx_text">+GPT4</span></th>
<th id="S4.T3.1.1.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S4.T3.1.1.12.12.3" class="ltx_td ltx_align_right ltx_border_t">.363</td>
<td id="S4.T3.1.1.12.12.4" class="ltx_td ltx_align_right ltx_border_t">.336</td>
<td id="S4.T3.1.1.12.12.5" class="ltx_td ltx_align_right ltx_border_t">.349</td>
</tr>
<tr id="S4.T3.1.1.13.13" class="ltx_tr">
<th id="S4.T3.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BioBERT</th>
<td id="S4.T3.1.1.13.13.2" class="ltx_td ltx_align_right">.363</td>
<td id="S4.T3.1.1.13.13.3" class="ltx_td ltx_align_right">.347</td>
<td id="S4.T3.1.1.13.13.4" class="ltx_td ltx_align_right">.355</td>
</tr>
<tr id="S4.T3.1.1.14.14" class="ltx_tr">
<th id="S4.T3.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S4.T3.1.1.14.14.2" class="ltx_td ltx_align_right">.362</td>
<td id="S4.T3.1.1.14.14.3" class="ltx_td ltx_align_right">.334</td>
<td id="S4.T3.1.1.14.14.4" class="ltx_td ltx_align_right">.347</td>
</tr>
<tr id="S4.T3.1.1.15.15" class="ltx_tr">
<th id="S4.T3.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S4.T3.1.1.15.15.2" class="ltx_td ltx_align_right">.352</td>
<td id="S4.T3.1.1.15.15.3" class="ltx_td ltx_align_right">.389</td>
<td id="S4.T3.1.1.15.15.4" class="ltx_td ltx_align_right">.369</td>
</tr>
<tr id="S4.T3.1.1.16.16" class="ltx_tr">
<th id="S4.T3.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Average</th>
<td id="S4.T3.1.1.16.16.2" class="ltx_td ltx_align_right ltx_border_t">.360</td>
<td id="S4.T3.1.1.16.16.3" class="ltx_td ltx_align_right ltx_border_t">.351</td>
<td id="S4.T3.1.1.16.16.4" class="ltx_td ltx_align_right ltx_border_t">.355</td>
</tr>
<tr id="S4.T3.1.1.17.17" class="ltx_tr">
<th id="S4.T3.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="5"><span id="S4.T3.1.1.17.17.1.1" class="ltx_text">+GPT4+Few-shot</span></th>
<th id="S4.T3.1.1.17.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S4.T3.1.1.17.17.3" class="ltx_td ltx_align_right ltx_border_t">.371</td>
<td id="S4.T3.1.1.17.17.4" class="ltx_td ltx_align_right ltx_border_t">.358</td>
<td id="S4.T3.1.1.17.17.5" class="ltx_td ltx_align_right ltx_border_t">.364</td>
</tr>
<tr id="S4.T3.1.1.18.18" class="ltx_tr">
<th id="S4.T3.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BioBERT</th>
<td id="S4.T3.1.1.18.18.2" class="ltx_td ltx_align_right">.387</td>
<td id="S4.T3.1.1.18.18.3" class="ltx_td ltx_align_right">.383</td>
<td id="S4.T3.1.1.18.18.4" class="ltx_td ltx_align_right">.385</td>
</tr>
<tr id="S4.T3.1.1.19.19" class="ltx_tr">
<th id="S4.T3.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S4.T3.1.1.19.19.2" class="ltx_td ltx_align_right"><span id="S4.T3.1.1.19.19.2.1" class="ltx_text ltx_font_bold">.374</span></td>
<td id="S4.T3.1.1.19.19.3" class="ltx_td ltx_align_right">.360</td>
<td id="S4.T3.1.1.19.19.4" class="ltx_td ltx_align_right">.367</td>
</tr>
<tr id="S4.T3.1.1.20.20" class="ltx_tr">
<th id="S4.T3.1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S4.T3.1.1.20.20.2" class="ltx_td ltx_align_right">.367</td>
<td id="S4.T3.1.1.20.20.3" class="ltx_td ltx_align_right"><span id="S4.T3.1.1.20.20.3.1" class="ltx_text ltx_font_bold">.415</span></td>
<td id="S4.T3.1.1.20.20.4" class="ltx_td ltx_align_right"><span id="S4.T3.1.1.20.20.4.1" class="ltx_text ltx_font_bold">.389</span></td>
</tr>
<tr id="S4.T3.1.1.21.21" class="ltx_tr">
<th id="S4.T3.1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Average</th>
<td id="S4.T3.1.1.21.21.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.21.21.2.1" class="ltx_text ltx_font_bold">.375</span></td>
<td id="S4.T3.1.1.21.21.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.21.21.3.1" class="ltx_text ltx_font_bold">.379</span></td>
<td id="S4.T3.1.1.21.21.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.21.21.4.1" class="ltx_text ltx_font_bold">.376</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>CADEC ASR dataset results</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">In this section, we evaluate and contrast ASR-NER performances on biomedical noisy transcripts with those of the two proposed methods. We evaluate the models based on their performance in detecting the named entities as well as their categories. We use micro precision, recall, and f1 as the evaluation metrics in which a correct prediction happens only if the named entity and the corresponding tag category <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="\hat{y^{\hat{c_{i}}}_{i}}=(\hat{n_{i}},\hat{c_{i}})" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3" xref="S5.p1.1.m1.2.3.cmml"><mover accent="true" id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.2.cmml"><msubsup id="S5.p1.1.m1.2.3.2.2" xref="S5.p1.1.m1.2.3.2.2.cmml"><mi id="S5.p1.1.m1.2.3.2.2.2.2" xref="S5.p1.1.m1.2.3.2.2.2.2.cmml">y</mi><mi id="S5.p1.1.m1.2.3.2.2.3" xref="S5.p1.1.m1.2.3.2.2.3.cmml">i</mi><mover accent="true" id="S5.p1.1.m1.2.3.2.2.2.3" xref="S5.p1.1.m1.2.3.2.2.2.3.cmml"><msub id="S5.p1.1.m1.2.3.2.2.2.3.2" xref="S5.p1.1.m1.2.3.2.2.2.3.2.cmml"><mi id="S5.p1.1.m1.2.3.2.2.2.3.2.2" xref="S5.p1.1.m1.2.3.2.2.2.3.2.2.cmml">c</mi><mi id="S5.p1.1.m1.2.3.2.2.2.3.2.3" xref="S5.p1.1.m1.2.3.2.2.2.3.2.3.cmml">i</mi></msub><mo id="S5.p1.1.m1.2.3.2.2.2.3.1" xref="S5.p1.1.m1.2.3.2.2.2.3.1.cmml">^</mo></mover></msubsup><mo id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.2.1.cmml">^</mo></mover><mo id="S5.p1.1.m1.2.3.1" xref="S5.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S5.p1.1.m1.2.3.3.2" xref="S5.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S5.p1.1.m1.2.3.3.2.1" xref="S5.p1.1.m1.2.3.3.1.cmml">(</mo><mover accent="true" id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><msub id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml"><mi id="S5.p1.1.m1.1.1.2.2" xref="S5.p1.1.m1.1.1.2.2.cmml">n</mi><mi id="S5.p1.1.m1.1.1.2.3" xref="S5.p1.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">^</mo></mover><mo id="S5.p1.1.m1.2.3.3.2.2" xref="S5.p1.1.m1.2.3.3.1.cmml">,</mo><mover accent="true" id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml"><msub id="S5.p1.1.m1.2.2.2" xref="S5.p1.1.m1.2.2.2.cmml"><mi id="S5.p1.1.m1.2.2.2.2" xref="S5.p1.1.m1.2.2.2.2.cmml">c</mi><mi id="S5.p1.1.m1.2.2.2.3" xref="S5.p1.1.m1.2.2.2.3.cmml">i</mi></msub><mo id="S5.p1.1.m1.2.2.1" xref="S5.p1.1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S5.p1.1.m1.2.3.3.2.3" xref="S5.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><apply id="S5.p1.1.m1.2.3.cmml" xref="S5.p1.1.m1.2.3"><eq id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.1"></eq><apply id="S5.p1.1.m1.2.3.2.cmml" xref="S5.p1.1.m1.2.3.2"><ci id="S5.p1.1.m1.2.3.2.1.cmml" xref="S5.p1.1.m1.2.3.2.1">^</ci><apply id="S5.p1.1.m1.2.3.2.2.cmml" xref="S5.p1.1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.2.3.2.2.1.cmml" xref="S5.p1.1.m1.2.3.2.2">subscript</csymbol><apply id="S5.p1.1.m1.2.3.2.2.2.cmml" xref="S5.p1.1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.2.3.2.2.2.1.cmml" xref="S5.p1.1.m1.2.3.2.2">superscript</csymbol><ci id="S5.p1.1.m1.2.3.2.2.2.2.cmml" xref="S5.p1.1.m1.2.3.2.2.2.2">𝑦</ci><apply id="S5.p1.1.m1.2.3.2.2.2.3.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3"><ci id="S5.p1.1.m1.2.3.2.2.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3.1">^</ci><apply id="S5.p1.1.m1.2.3.2.2.2.3.2.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.2.3.2.2.2.3.2.1.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3.2">subscript</csymbol><ci id="S5.p1.1.m1.2.3.2.2.2.3.2.2.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3.2.2">𝑐</ci><ci id="S5.p1.1.m1.2.3.2.2.2.3.2.3.cmml" xref="S5.p1.1.m1.2.3.2.2.2.3.2.3">𝑖</ci></apply></apply></apply><ci id="S5.p1.1.m1.2.3.2.2.3.cmml" xref="S5.p1.1.m1.2.3.2.2.3">𝑖</ci></apply></apply><interval closure="open" id="S5.p1.1.m1.2.3.3.1.cmml" xref="S5.p1.1.m1.2.3.3.2"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><ci id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1">^</ci><apply id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.2.1.cmml" xref="S5.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.2.cmml" xref="S5.p1.1.m1.1.1.2.2">𝑛</ci><ci id="S5.p1.1.m1.1.1.2.3.cmml" xref="S5.p1.1.m1.1.1.2.3">𝑖</ci></apply></apply><apply id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2"><ci id="S5.p1.1.m1.2.2.1.cmml" xref="S5.p1.1.m1.2.2.1">^</ci><apply id="S5.p1.1.m1.2.2.2.cmml" xref="S5.p1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.2.2.2.1.cmml" xref="S5.p1.1.m1.2.2.2">subscript</csymbol><ci id="S5.p1.1.m1.2.2.2.2.cmml" xref="S5.p1.1.m1.2.2.2.2">𝑐</ci><ci id="S5.p1.1.m1.2.2.2.3.cmml" xref="S5.p1.1.m1.2.2.2.3">𝑖</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">\hat{y^{\hat{c_{i}}}_{i}}=(\hat{n_{i}},\hat{c_{i}})</annotation></semantics></math> is predicted correctly and matches those of the corresponding original ground truth <math id="S5.p1.2.m2.2" class="ltx_Math" alttext="y^{c_{i}}_{i}=(n_{i},c_{i})" display="inline"><semantics id="S5.p1.2.m2.2a"><mrow id="S5.p1.2.m2.2.2" xref="S5.p1.2.m2.2.2.cmml"><msubsup id="S5.p1.2.m2.2.2.4" xref="S5.p1.2.m2.2.2.4.cmml"><mi id="S5.p1.2.m2.2.2.4.2.2" xref="S5.p1.2.m2.2.2.4.2.2.cmml">y</mi><mi id="S5.p1.2.m2.2.2.4.3" xref="S5.p1.2.m2.2.2.4.3.cmml">i</mi><msub id="S5.p1.2.m2.2.2.4.2.3" xref="S5.p1.2.m2.2.2.4.2.3.cmml"><mi id="S5.p1.2.m2.2.2.4.2.3.2" xref="S5.p1.2.m2.2.2.4.2.3.2.cmml">c</mi><mi id="S5.p1.2.m2.2.2.4.2.3.3" xref="S5.p1.2.m2.2.2.4.2.3.3.cmml">i</mi></msub></msubsup><mo id="S5.p1.2.m2.2.2.3" xref="S5.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S5.p1.2.m2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S5.p1.2.m2.2.2.2.2.3" xref="S5.p1.2.m2.2.2.2.3.cmml">(</mo><msub id="S5.p1.2.m2.1.1.1.1.1" xref="S5.p1.2.m2.1.1.1.1.1.cmml"><mi id="S5.p1.2.m2.1.1.1.1.1.2" xref="S5.p1.2.m2.1.1.1.1.1.2.cmml">n</mi><mi id="S5.p1.2.m2.1.1.1.1.1.3" xref="S5.p1.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p1.2.m2.2.2.2.2.4" xref="S5.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S5.p1.2.m2.2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.2.cmml"><mi id="S5.p1.2.m2.2.2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.2.2.cmml">c</mi><mi id="S5.p1.2.m2.2.2.2.2.2.3" xref="S5.p1.2.m2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S5.p1.2.m2.2.2.2.2.5" xref="S5.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.2b"><apply id="S5.p1.2.m2.2.2.cmml" xref="S5.p1.2.m2.2.2"><eq id="S5.p1.2.m2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.3"></eq><apply id="S5.p1.2.m2.2.2.4.cmml" xref="S5.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.4.1.cmml" xref="S5.p1.2.m2.2.2.4">subscript</csymbol><apply id="S5.p1.2.m2.2.2.4.2.cmml" xref="S5.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.4.2.1.cmml" xref="S5.p1.2.m2.2.2.4">superscript</csymbol><ci id="S5.p1.2.m2.2.2.4.2.2.cmml" xref="S5.p1.2.m2.2.2.4.2.2">𝑦</ci><apply id="S5.p1.2.m2.2.2.4.2.3.cmml" xref="S5.p1.2.m2.2.2.4.2.3"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.4.2.3.1.cmml" xref="S5.p1.2.m2.2.2.4.2.3">subscript</csymbol><ci id="S5.p1.2.m2.2.2.4.2.3.2.cmml" xref="S5.p1.2.m2.2.2.4.2.3.2">𝑐</ci><ci id="S5.p1.2.m2.2.2.4.2.3.3.cmml" xref="S5.p1.2.m2.2.2.4.2.3.3">𝑖</ci></apply></apply><ci id="S5.p1.2.m2.2.2.4.3.cmml" xref="S5.p1.2.m2.2.2.4.3">𝑖</ci></apply><interval closure="open" id="S5.p1.2.m2.2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2.2"><apply id="S5.p1.2.m2.1.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S5.p1.2.m2.1.1.1.1.1.2.cmml" xref="S5.p1.2.m2.1.1.1.1.1.2">𝑛</ci><ci id="S5.p1.2.m2.1.1.1.1.1.3.cmml" xref="S5.p1.2.m2.1.1.1.1.1.3">𝑖</ci></apply><apply id="S5.p1.2.m2.2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.2.2.2.1.cmml" xref="S5.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S5.p1.2.m2.2.2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2.2.2">𝑐</ci><ci id="S5.p1.2.m2.2.2.2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.2c">y^{c_{i}}_{i}=(n_{i},c_{i})</annotation></semantics></math>. If a script has multiple pairs of the same named entity and category, we treat each as a separate prediction to account for repeated terms. This way, our evaluation would be very similar to CoNLL’s <cite class="ltx_cite ltx_citemacro_cite">Sang and De Meulder (<a href="#bib.bib36" title="" class="ltx_ref">2003</a>)</cite>; however, we do not perform a Span detection or consider the BIO tags as the position of words changes in all of our noisy transcript datasets due to the existence of mistranscribed words.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ Few-shot In-Context Learning. ‣ 4. Methods ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the performances on the CADEC dataset. As you can see, and was expected, the performance on the NER has significantly dropped (on average 62% of micro f1 scores) on the ASR noisy transcript data for different reasons, including mistranscribed words/terms, the words that are picked up from the background and those that are missed to name but a few reasons. Another reason is the covariate shift between the training and testing set. For example, the ASR output may write the full name of a drug/disease, but the original script may use abbreviations, or they may use different punctuation. PLM-based models’ performances have dropped less due to their robustness to covariate shift and their ability to handle Out-Of-Vocabulary (OOV) scenarios (in comparison with traditional pretrained embeddings such as GloVe). T5 performs consistently better on the recall and precision, which shows its superior robustness, and it would potentially be an ideal choice for fine-tuning if one decides to fine-tune the NER on the noisy transcript data.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:473.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.4pt,-96.6pt) scale(1.6889693488685,1.6889693488685) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Corpus</th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<td id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Precision</td>
<td id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">Recall</td>
<td id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">F1</td>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T4.1.1.2.2.1.1" class="ltx_text">Original</span></th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">.974</td>
<td id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">.972</td>
<td id="S5.T4.1.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">.973</td>
</tr>
<tr id="S5.T4.1.1.3.3" class="ltx_tr">
<th id="S5.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S5.T4.1.1.3.3.2" class="ltx_td ltx_align_right">.962</td>
<td id="S5.T4.1.1.3.3.3" class="ltx_td ltx_align_right">.968</td>
<td id="S5.T4.1.1.3.3.4" class="ltx_td ltx_align_right">.965</td>
</tr>
<tr id="S5.T4.1.1.4.4" class="ltx_tr">
<th id="S5.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S5.T4.1.1.4.4.2" class="ltx_td ltx_align_right">.942</td>
<td id="S5.T4.1.1.4.4.3" class="ltx_td ltx_align_right">.963</td>
<td id="S5.T4.1.1.4.4.4" class="ltx_td ltx_align_right">.953</td>
</tr>
<tr id="S5.T4.1.1.5.5" class="ltx_tr">
<th id="S5.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AVG</th>
<td id="S5.T4.1.1.5.5.2" class="ltx_td ltx_align_right ltx_border_t">.959</td>
<td id="S5.T4.1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_t">.968</td>
<td id="S5.T4.1.1.5.5.4" class="ltx_td ltx_align_right ltx_border_t">.964</td>
</tr>
<tr id="S5.T4.1.1.6.6" class="ltx_tr">
<th id="S5.T4.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="4"><span id="S5.T4.1.1.6.6.1.1" class="ltx_text">Whisper</span></th>
<th id="S5.T4.1.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">BERT</th>
<td id="S5.T4.1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_tt">.528</td>
<td id="S5.T4.1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_tt">.525</td>
<td id="S5.T4.1.1.6.6.5" class="ltx_td ltx_align_right ltx_border_tt">.526</td>
</tr>
<tr id="S5.T4.1.1.7.7" class="ltx_tr">
<th id="S5.T4.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S5.T4.1.1.7.7.2" class="ltx_td ltx_align_right">.554</td>
<td id="S5.T4.1.1.7.7.3" class="ltx_td ltx_align_right">.609</td>
<td id="S5.T4.1.1.7.7.4" class="ltx_td ltx_align_right">.580</td>
</tr>
<tr id="S5.T4.1.1.8.8" class="ltx_tr">
<th id="S5.T4.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S5.T4.1.1.8.8.2" class="ltx_td ltx_align_right">.555</td>
<td id="S5.T4.1.1.8.8.3" class="ltx_td ltx_align_right">.624</td>
<td id="S5.T4.1.1.8.8.4" class="ltx_td ltx_align_right">.587</td>
</tr>
<tr id="S5.T4.1.1.9.9" class="ltx_tr">
<th id="S5.T4.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AVG</th>
<td id="S5.T4.1.1.9.9.2" class="ltx_td ltx_align_right ltx_border_t">.546</td>
<td id="S5.T4.1.1.9.9.3" class="ltx_td ltx_align_right ltx_border_t">.586</td>
<td id="S5.T4.1.1.9.9.4" class="ltx_td ltx_align_right ltx_border_t">.564</td>
</tr>
<tr id="S5.T4.1.1.10.10" class="ltx_tr">
<th id="S5.T4.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T4.1.1.10.10.1.1" class="ltx_text">+GPT4</span></th>
<th id="S5.T4.1.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S5.T4.1.1.10.10.3" class="ltx_td ltx_align_right ltx_border_t">.554</td>
<td id="S5.T4.1.1.10.10.4" class="ltx_td ltx_align_right ltx_border_t">.584</td>
<td id="S5.T4.1.1.10.10.5" class="ltx_td ltx_align_right ltx_border_t">.570</td>
</tr>
<tr id="S5.T4.1.1.11.11" class="ltx_tr">
<th id="S5.T4.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S5.T4.1.1.11.11.2" class="ltx_td ltx_align_right">.571</td>
<td id="S5.T4.1.1.11.11.3" class="ltx_td ltx_align_right">.609</td>
<td id="S5.T4.1.1.11.11.4" class="ltx_td ltx_align_right">.589</td>
</tr>
<tr id="S5.T4.1.1.12.12" class="ltx_tr">
<th id="S5.T4.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S5.T4.1.1.12.12.2" class="ltx_td ltx_align_right">.580</td>
<td id="S5.T4.1.1.12.12.3" class="ltx_td ltx_align_right">.614</td>
<td id="S5.T4.1.1.12.12.4" class="ltx_td ltx_align_right">.597</td>
</tr>
<tr id="S5.T4.1.1.13.13" class="ltx_tr">
<th id="S5.T4.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AVG</th>
<td id="S5.T4.1.1.13.13.2" class="ltx_td ltx_align_right ltx_border_t">.568</td>
<td id="S5.T4.1.1.13.13.3" class="ltx_td ltx_align_right ltx_border_t">.602</td>
<td id="S5.T4.1.1.13.13.4" class="ltx_td ltx_align_right ltx_border_t">.585</td>
</tr>
<tr id="S5.T4.1.1.14.14" class="ltx_tr">
<th id="S5.T4.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S5.T4.1.1.14.14.1.1" class="ltx_text">+GPT4+Few-shot</span></th>
<th id="S5.T4.1.1.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BERT</th>
<td id="S5.T4.1.1.14.14.3" class="ltx_td ltx_align_right ltx_border_t">.567</td>
<td id="S5.T4.1.1.14.14.4" class="ltx_td ltx_align_right ltx_border_t">.598</td>
<td id="S5.T4.1.1.14.14.5" class="ltx_td ltx_align_right ltx_border_t">.584</td>
</tr>
<tr id="S5.T4.1.1.15.15" class="ltx_tr">
<th id="S5.T4.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T5</th>
<td id="S5.T4.1.1.15.15.2" class="ltx_td ltx_align_right">.611</td>
<td id="S5.T4.1.1.15.15.3" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.15.15.3.1" class="ltx_text ltx_font_bold">.659</span></td>
<td id="S5.T4.1.1.15.15.4" class="ltx_td ltx_align_right">.634</td>
</tr>
<tr id="S5.T4.1.1.16.16" class="ltx_tr">
<th id="S5.T4.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flair</th>
<td id="S5.T4.1.1.16.16.2" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.16.16.2.1" class="ltx_text ltx_font_bold">.620</span></td>
<td id="S5.T4.1.1.16.16.3" class="ltx_td ltx_align_right">.649</td>
<td id="S5.T4.1.1.16.16.4" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.16.16.4.1" class="ltx_text ltx_font_bold">.634</span></td>
</tr>
<tr id="S5.T4.1.1.17.17" class="ltx_tr">
<th id="S5.T4.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">AVG</th>
<td id="S5.T4.1.1.17.17.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">.599</td>
<td id="S5.T4.1.1.17.17.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">.635</td>
<td id="S5.T4.1.1.17.17.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T4.1.1.17.17.4.1" class="ltx_text ltx_font_bold">.617</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Overall results on the Synthetic BTACT ASR dataset. </figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The <span id="S5.p3.1.1" class="ltx_text ltx_font_bold">zero-shot prompting</span> consistently improves the micro f1 score by an average of 0.14 across all models, which is 59% improvement, fixing the 22% of the drop (reduces the 62% drop to 40%). This shows the value and effectiveness of providing the context and GPT4’s capability of utilizing its knowledge to refine the transcripts. With the improvement of the transcripts and reduction in noise models, performances improved, and Flair pretrained embedding surpassed other models, consistent with the original scripts’ results. BioBERT and Flair improved more than others, especially T5, because GPT4 may replace some terms with new medical terms that T5 has not seen in the training set, but BioBERT and Flair PubMed embeddings are good at recognizing them.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The <span id="S5.p4.1.1" class="ltx_text ltx_font_bold">few-shot in-context learning</span> additionally improves the micro f1 score by an average of 0.03 across all models. This is 79% improvement from the ASR noisy transcripts that happen consistently across all models and fixes the f1 drop by 27%. Furthermore, the difference between the performance of the models is more similar to those of the original scripts, even in comparison with the zero-shot prompting, which is because the post-transcribed scripts are more similar to the original ones. However, the BioBERT model, which is the best-performing model, has shown the most improvement by improving by 32% in terms of micro-F1. Flair and BioBERT are the best models for zero-shot and few-shot learning when used with GPT4 because of their pretraining on medical terms. We hypothesize that more specialized language models that use medical databases for pretraining can potentially improve performance.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5. Results ‣ Extracting Biomedical Entities from Noisy Audio Transcripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performances on the Synthetic BTACT dataset. Similar to the CADEC dataset, we see that the performance significantly drops on the noisy transcripts, and zero- or few-shot learning will consistently improve the performance. The generalization of improvements across the two datasets suggests the effectiveness of the proposed approaches. However, the improvement on BTACT data is substantially lower than CADEC–on BTACT data, the micro-f1 drop reduces from 41.4% in the noisy ASR-NER to 35.9% using few-shot learning, i.e., 5.5% improvement. This is due to the unnatural style of the BTACT data and the lack of context to be leveraged by GPT4.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Discussion and Error Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The error in ASR-NER can come from different sources, such as noise in the original script, NER errors, or ASR errors in which words may be mistranscribed or not transcribed from the background speaker by mistake. The noise from the original script includes grammatical errors, typos, or even biomedical misconceptions. Although in a general setting, we cannot address the input data noise, in an ASR setting, this causes the model to either mistranscribe or fix the issue, which causes a mismatch between the predictions and ground-truth tags. For example, (’flu like symtoms’, ’ADR’) is transcribed to (’flu-like symtoms’, ’ADR’). Another example, if the script talks about a drug that can stop multiple episodes of migraines, it should say</p>
<blockquote id="S6.p1.2" class="ltx_quote">
<p id="S6.p1.2.1" class="ltx_p">“<span id="S6.p1.2.1.1" class="ltx_text ltx_font_italic">It stops migraine<span id="S6.p1.2.1.1.1" class="ltx_text ltx_font_bold">s</span></span>”</p>
</blockquote>
<p id="S6.p1.3" class="ltx_p">rather than</p>
<blockquote id="S6.p1.4" class="ltx_quote">
<p id="S6.p1.4.1" class="ltx_p">“<span id="S6.p1.4.1.1" class="ltx_text ltx_font_italic">It stops migraine</span>.”</p>
</blockquote>
<p id="S6.p1.5" class="ltx_p">GPT4 realizes these issues and fixes them using its biomedical knowledge.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The errors that come from the NER system cannot be remedied, but sometimes the noisy transcribed data can add terminologies or a combination of words, which results in errors in the NER model; for example, NER is prone to misidentify pronouns as ADRs, e.g. (we, ADR) or (i, ADR), especially when there are not many named-entities in the input text. Therefore, when the input is very noisy, and ASR cannot transcribe any ADR terms, the output includes many such errors. Furthermore, the covariate shift between the ASR transcript and the original script can also cause errors in the NER system. However, both types of errors are mitigated to some extent by few-shot in-context learning as the GPT4 has seen these errors and changes in the style of the transcript compared to the original script.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The error can also come from Whisper’s limited vocabulary or noisy predictions during uncertainty. For example, Whisper cannot detect many drug names, especially phonetically similar to a more common term. For example, it transcribes Arthotec as “arthritis” or “arthrotype,” but providing the context that the script is about adverse drug effects fixes the problem. However, due to the intrinsic randomness of GPT4, there are examples in which the term is not fixed in either zero- or few-shot learning–this happens for many other terms, and very commonly, one approach has the right answer, which suggests the use of ensemble approaches.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">We also find unique situations where GPT4 hallucinates new contexts. For instance, The original text</p>
<blockquote id="S6.p4.2" class="ltx_quote">
<p id="S6.p4.2.1" class="ltx_p">“<span id="S6.p4.2.1.1" class="ltx_text ltx_font_italic">Both husband and wife on a low dosage (10 mg). We are having extreme reactions to heat.</span>”</p>
</blockquote>
<p id="S6.p4.3" class="ltx_p">is transcribed by Whisper to</p>
<blockquote id="S6.p4.4" class="ltx_quote">
<p id="S6.p4.4.1" class="ltx_p">“<span id="S6.p4.4.1.1" class="ltx_text ltx_font_italic">There you have it everyone, some kind of low dose of temperature, we are having extreme reactions</span>.”.</p>
</blockquote>
<p id="S6.p4.5" class="ltx_p">But, after applying GPT4, it hallucinates a change in temperature, e.g.,</p>
<blockquote id="S6.p4.6" class="ltx_quote">
<p id="S6.p4.6.1" class="ltx_p">“<span id="S6.p4.6.1.1" class="ltx_text ltx_font_italic">We both have extreme reactions to temperature changes</span>.”</p>
</blockquote>
<p id="S6.p4.7" class="ltx_p">The new context has unexpected impacts on the NER models, e.g., “changes in temperature” is detected as an ADR. Future research is required to reduce these hallucinations.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper explores the ASR-NLP gap in the biomedical domain, particularly for noisy audio. This challenge is especially pronounced in biomedical Named Entity Recognition (NER) tasks. While advancements in ASR show promise in controlled environments, real-world noisy conditions present significant obstacles (e.g., lack of publicly available datasets). To address this, we’ve introduced the BioASR-NER dataset, offering a mix of clean and noisy biomedical recordings. Coupled with our innovative GPT4-based transcript cleaning approach, we’ve made strides toward mitigating transcription errors.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">There are two avenues for future research. First, our methodology to fix transcripts is based on the transcription text. Incorporating the audio information, particularly with recent advances in transformer-based audio-representations <cite class="ltx_cite ltx_citemacro_cite">Gong et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>, could substantially improve performance. Second, other biomedical NLP-based tasks, particularly when applied to noisy transcripts, deserve attention. These tasks include tasks such as text summarization <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>)</cite> and question answering <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>. How these models generalize when applied to audio-generated transcripts with background voices and noises is unclear. Hence, we will explore these tasks as future work.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This material is based upon work supported by
the National Science Foundation (NSF) under
Grant No. 2145357.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Bibliographical References</h2>

<div id="S8.p1" class="ltx_para">
<span id="S8.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Peng Chen, Jian Wang, Hongfei Lin, Di Zhao, and Zhihao Yang. 2023.

</span>
<span class="ltx_bibblock">Few-shot biomedical named entity recognition via knowledge-guided
instance generation and prompt contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 39(8):btad496.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Kunming Cheng, Qiang Guo, Yongbin He, Yanqiu Lu, Shuqin Gu, and Haiyang Wu.
2023.

</span>
<span class="ltx_bibblock">Exploring the potential of gpt-4 in biomedical engineering: the dawn
of a new era.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Annals of Biomedical Engineering</em>, pages 1–9.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiu et al. (2021)</span>
<span class="ltx_bibblock">
Yu-Wen Chiu, Wen-Chao Yeh, Sheng-Jie Lin, and Yung-Chun Chang. 2021.

</span>
<span class="ltx_bibblock">Recognizing chemical entity in biomedical literature using a
bert-based ensemble learning methods for the biocreative 2021 nlm-chem track.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the seventh BioCreative challenge evaluation
workshop</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuriya et al. (2022)</span>
<span class="ltx_bibblock">
Ankur Dhuriya, Harveen Singh Chadha, Anirudh Gupta, Priyanshi Shah, Neeraj
Chhimwal, Rishabh Gaur, and Vivek Raghavan. 2022.

</span>
<span class="ltx_bibblock">Improving speech recognition for indic languages using language
model.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.16595</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganoe et al. (2021)</span>
<span class="ltx_bibblock">
Craig H Ganoe, Weiyi Wu, Paul J Barr, William Haslett, Michelle D Dannenberg,
Kyra L Bonasia, James C Finora, Jesse A Schoonmaker, Wambui M Onsando, James
Ryan, et al. 2021.

</span>
<span class="ltx_bibblock">Natural language processing for automated annotation of medication
mentions in primary care visit conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">JAMIA open</em>, 4(3):ooab071.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2022)</span>
<span class="ltx_bibblock">
Yao Ge, Yuting Guo, Yuan-Chi Yang, Mohammed Ali Al-Garadi, and Abeed Sarker.
2022.

</span>
<span class="ltx_bibblock">Few-shot learning for medical text: A systematic review.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.14081</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. (2023)</span>
<span class="ltx_bibblock">
Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, and Dinesh Manocha. 2023.

</span>
<span class="ltx_bibblock">Bioaug: Conditional generation based data augmentation for
low-resource biomedical ner.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10647</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2022)</span>
<span class="ltx_bibblock">
Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. 2022.

</span>
<span class="ltx_bibblock">Ssast: Self-supervised audio spectrogram transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 36, pages 10699–10709.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan and Zhou (2023)</span>
<span class="ltx_bibblock">
Zhengyi Guan and Xiaobing Zhou. 2023.

</span>
<span class="ltx_bibblock">A prefix and attention map discrimination fusion guided attention for
biomedical named entity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">BMC bioinformatics</em>, 24(1):42.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutierrez et al. (2022)</span>
<span class="ltx_bibblock">
Bernal Jimenez Gutierrez, Nikolas McNeal, Clay Washington, You Chen, Lang Li,
Huan Sun, and Yu Su. 2022.

</span>
<span class="ltx_bibblock">Thinking about gpt-3 in-context learning for biomedical ie? think
again.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.08410</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hacking et al. (2023)</span>
<span class="ltx_bibblock">
Coen Hacking, Hilde Verbeek, Jan PH Hamers, and Sil Aarts. 2023.

</span>
<span class="ltx_bibblock">The development of an automatic speech recognition model using
interview data from long-term care for older adults.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association</em>,
30(3):411–417.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2020)</span>
<span class="ltx_bibblock">
Xuan Ji, Meng Yu, Chunlei Zhang, Dan Su, Tao Yu, Xiaoyu Liu, and Dong Yu. 2020.

</span>
<span class="ltx_bibblock">Speaker-aware target speaker enhancement by jointly learning with
speaker embedding extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 7294–7298. IEEE.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2019)</span>
<span class="ltx_bibblock">
Chen Jia, Xiaobo Liang, and Yue Zhang. 2019.

</span>
<span class="ltx_bibblock">Cross-domain ner using cross-domain language modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th annual meeting of the association
for computational linguistics</em>, pages 2464–2474.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2023)</span>
<span class="ltx_bibblock">
Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. 2023.

</span>
<span class="ltx_bibblock">Genegpt: Augmenting large language models with domain tools for
improved access to biomedical information.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2022)</span>
<span class="ltx_bibblock">
Qiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu, Huaiyuan Ying, Chuanqi Tan,
Mosha Chen, Songfang Huang, Xiaozhong Liu, and Sheng Yu. 2022.

</span>
<span class="ltx_bibblock">Biomedical question answering: a survey of approaches and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 55(2):1–36.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimi et al. (2015)</span>
<span class="ltx_bibblock">
Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna Kemp, and Chen Wang. 2015.

</span>
<span class="ltx_bibblock">Cadec: A corpus of adverse drug event annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Journal of biomedical informatics</em>, 55:73–81.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">King et al. (2023)</span>
<span class="ltx_bibblock">
Andrew J King, Derek C Angus, Gregory F Cooper, Danielle L Mowery, Jennifer B
Seaman, Kelly M Potter, Leigh A Bukowski, Ali Al-Khafaji, Scott R Gunn, and
Jeremy M Kahn. 2023.

</span>
<span class="ltx_bibblock">A voice-based digital assistant for intelligent prompting of
evidence-based practices during icu rounds.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, 146:104483.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kodish-Wachs et al. (2018)</span>
<span class="ltx_bibblock">
Jodi Kodish-Wachs, Emin Agassi, Patrick Kenny III, and J Marc Overhage. 2018.

</span>
<span class="ltx_bibblock">A systematic comparison of contemporary automatic speech recognition
engines for conversational clinical speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">AMIA Annual Symposium Proceedings</em>, volume 2018, page 683.
American Medical Informatics Association.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample et al. (2016)</span>
<span class="ltx_bibblock">
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and
Chris Dyer. 2016.

</span>
<span class="ltx_bibblock">Neural architectures for named entity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1603.01360</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leaman and Gonzalez (2008)</span>
<span class="ltx_bibblock">
Robert Leaman and Graciela Gonzalez. 2008.

</span>
<span class="ltx_bibblock">Banner: an executable survey of advances in biomedical named entity
recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Biocomputing 2008</em>, pages 652–663. World Scientific.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2020.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for
biomedical text mining.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 36(4):1234–1240.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">López-Úbeda et al. (2021)</span>
<span class="ltx_bibblock">
Pilar López-Úbeda, Manuel Carlos Díaz-Galiano, L Alfonso
Ureña-López, and M Teresa Martín-Valdivia. 2021.

</span>
<span class="ltx_bibblock">Combining word embeddings to extract chemical and drug entities in
biomedical literature.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">BMC bioinformatics</em>, 22(1):1–18.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mani et al. (2020)</span>
<span class="ltx_bibblock">
Anirudh Mani, Shruti Palaskar, and Sandeep Konam. 2020.

</span>
<span class="ltx_bibblock">Towards understanding asr error correction for medical conversations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the first workshop on natural language
processing for medical conversations</em>, pages 7–11.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2014)</span>
<span class="ltx_bibblock">
Rashmi Mishra, Jiantao Bian, Marcelo Fiszman, Charlene R Weir, Siddhartha
Jonnalagadda, Javed Mostafa, and Guilherme Del Fiol. 2014.

</span>
<span class="ltx_bibblock">Text summarization in the biomedical domain: a systematic review of
recent research.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of biomedical informatics</em>, 52:457–467.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2022)</span>
<span class="ltx_bibblock">
Ngoc Dang Nguyen, Lan Du, Wray Buntine, Changyou Chen, and Richard Beare. 2022.

</span>
<span class="ltx_bibblock">Hardness-guided domain adaptation to recognise biomedical named
entities under low-resource scenarios.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 4063–4071.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paats et al. (2015)</span>
<span class="ltx_bibblock">
A Paats, T Alumäe, E Meister, and I Fridolin. 2015.

</span>
<span class="ltx_bibblock">Evaluation of automatic speech recognition prototype for estonian
language in radiology domain: a pilot study.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">16th Nordic-Baltic Conference on Biomedical Engineering: 16.
NBC &amp; 10. MTD 2014 joint conferences. October 14-16, 2014, Gothenburg,
Sweden</em>, pages 96–99. Springer.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poerner et al. (2020)</span>
<span class="ltx_bibblock">
Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.

</span>
<span class="ltx_bibblock">Inexpensive domain adaptation of pretrained language models: Case
studies on biomedical ner and covid-19 qa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 1482–1490.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pyysalo et al. (2007)</span>
<span class="ltx_bibblock">
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Björne, Jorma Boberg,
Jouni Järvinen, and Tapio Salakoski. 2007.

</span>
<span class="ltx_bibblock">Bioinfer: a corpus for information extraction in the biomedical
domain.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">BMC bioinformatics</em>, 8:1–24.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quiroz et al. (2019)</span>
<span class="ltx_bibblock">
Juan C Quiroz, Liliana Laranjo, Ahmet Baki Kocaballi, Shlomo Berkovsky, Dana
Rezazadegan, and Enrico Coiera. 2019.

</span>
<span class="ltx_bibblock">Challenges of developing a digital scribe to reduce clinical
documentation burden.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 2(1):114.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
28492–28518. PMLR.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 21(1):5485–5551.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rios et al. (2018)</span>
<span class="ltx_bibblock">
Anthony Rios, Ramakanth Kavuluru, and Zhiyong Lu. 2018.

</span>
<span class="ltx_bibblock">Generalizing biomedical relation classification with neural
adversarial domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 34(17):2973–2981.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rocktäschel et al. (2012)</span>
<span class="ltx_bibblock">
Tim Rocktäschel, Michael Weidlich, and Ulf Leser. 2012.

</span>
<span class="ltx_bibblock">Chemspot: a hybrid system for chemical named entity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 28(12):1633–1640.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sang and De Meulder (2003)</span>
<span class="ltx_bibblock">
Erik F Sang and Fien De Meulder. 2003.

</span>
<span class="ltx_bibblock">Introduction to the conll-2003 shared task: Language-independent
named entity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint cs/0306050</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2021)</span>
<span class="ltx_bibblock">
Bosheng Song, Fen Li, Yuansheng Liu, and Xiangxiang Zeng. 2021.

</span>
<span class="ltx_bibblock">Deep learning methods for biomedical named entity recognition: a
survey and qualitative comparison.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Briefings in Bioinformatics</em>, 22(6):bbab282.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinmetz and Reiss (2021)</span>
<span class="ltx_bibblock">
Christian J. Steinmetz and Joshua D. Reiss. 2021.

</span>
<span class="ltx_bibblock">pyloudnorm: A simple yet flexible loudness meter in python.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">150th AES Convention</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui et al. (2021)</span>
<span class="ltx_bibblock">
Dianbo Sui, Zhengkun Tian, Yubo Chen, Kang Liu, and Jun Zhao. 2021.

</span>
<span class="ltx_bibblock">A large-scale chinese multimodal ner dataset with speech clues.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 2807–2818.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2021)</span>
<span class="ltx_bibblock">
Cong Sun, Zhihao Yang, Lei Wang, Yin Zhang, Hongfei Lin, and Jian Wang. 2021.

</span>
<span class="ltx_bibblock">Deep learning with language models improves named entity recognition
for pharmaconer.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">BMC bioinformatics</em>, 22(1):1–16.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szymański et al. (2023)</span>
<span class="ltx_bibblock">
Piotr Szymański, Lukasz Augustyniak, Mikolaj Morzy, Adrian Szymczak,
Krzysztof Surdyk, and Piotr Żelasko. 2023.

</span>
<span class="ltx_bibblock">Why aren’t we ner yet? artifacts of asr errors in named entity
recognition in spontaneous speech transcripts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1746–1761.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong et al. (2021)</span>
<span class="ltx_bibblock">
Yiqi Tong, Yidong Chen, and Xiaodong Shi. 2021.

</span>
<span class="ltx_bibblock">A multi-task approach for improving biomedical named entity
recognition by incorporating multi-granularity information.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pages 4804–4813.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. (2023)</span>
<span class="ltx_bibblock">
Brian D Tran, Kareem Latif, Tera L Reynolds, Jihyun Park, Jennifer
Elston Lafata, Ming Tai-Seale, and Kai Zheng. 2023.

</span>
<span class="ltx_bibblock">“mm-hm,”“uh-uh”: are non-lexical conversational sounds deal
breakers for the ambient clinical documentation technology?

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association</em>,
30(4):703–711.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Buchem et al. (2021)</span>
<span class="ltx_bibblock">
Marieke M van Buchem, Hileen Boosman, Martijn P Bauer, Ilse MJ Kant, Simone A
Cammel, and Ewout W Steyerberg. 2021.

</span>
<span class="ltx_bibblock">The digital scribe in clinical practice: a scoping review and
research agenda.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 4(1):57.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2020)</span>
<span class="ltx_bibblock">
Thuy Vu, Dinh Phung, and Gholamreza Haffari. 2020.

</span>
<span class="ltx_bibblock">Effective unsupervised domain adaptation with adversarially trained
language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 6163–6173.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe et al. (2022)</span>
<span class="ltx_bibblock">
Taiki Watanabe, Tomoya Ichikawa, Akihiro Tamura, Tomoya Iwakura, Chunpeng Ma,
and Tsuneo Kato. 2022.

</span>
<span class="ltx_bibblock">Auxiliary learning for named entity recognition with multiple
auxiliary biomedical training data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st Workshop on Biomedical Language
Processing</em>, pages 130–139.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weber et al. (2021)</span>
<span class="ltx_bibblock">
Leon Weber, Mario Sänger, Jannes Münchmeyer, Maryam Habibi, Ulf Leser,
and Alan Akbik. 2021.

</span>
<span class="ltx_bibblock">Hunflair: an easy-to-use tool for state-of-the-art biomedical named
entity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, 37(17):2792–2794.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whetten and Kennington (2023)</span>
<span class="ltx_bibblock">
Ryan Whetten and Casey Kennington. 2023.

</span>
<span class="ltx_bibblock">Evaluating and improving automatic speech recognition using severity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">The 22nd Workshop on Biomedical Natural Language Processing
and BioNLP Shared Tasks</em>, pages 79–91.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo et al. (2023)</span>
<span class="ltx_bibblock">
Le Zhuo, Ruibin Yuan, Jiahao Pan, Yinghao Ma, Yizhi LI, Ge Zhang, Si Liu, Roger
Dannenberg, Jie Fu, Chenghua Lin, et al. 2023.

</span>
<span class="ltx_bibblock">Lyricwhiz: Robust multilingual zero-shot lyrics transcription by
whispering to chatgpt.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.17103</em>.

</span>
</li>
</ul>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.   Language Resource References</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Complete System Prompts</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In our study, we structure the dialogues with GPT-4 to refine noisy transcriptions of animal names. The dialogue is initiated with a system prompt, followed by a series of user and assistant interactions. Below, we show the system prompt used for the Synthetic BTACT dataset:</p>
<blockquote id="A1.p1.2" class="ltx_quote">
<p id="A1.p1.2.1" class="ltx_p"><span id="A1.p1.2.1.1" class="ltx_text ltx_font_bold">System Prompt:</span>
"I give you a transcript about animals. Animal names are being called out.
There are some names that are transcribed by mistake. Fix them to the
phonetically similar, more proper version if you find it not proper.
Respond with the fixed transcript only! Remember to remove repetitive
statements to make the content concise."</p>
</blockquote>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">For the CADEC dataset, we use the following system prompt:</p>
<blockquote id="A1.p2.2" class="ltx_quote">
<p id="A1.p2.2.1" class="ltx_p"><span id="A1.p2.2.1.1" class="ltx_text ltx_font_bold">System Prompt:</span> "I give you a transcript of medical conversation. There are some technical terms that are transcribed by mistake. Fix them to the phonetically similar, more proper version if you find it not proper. Respond with the fixed transcript only!"</p>
</blockquote>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p"></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.17361" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.17363" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.17363">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.17363" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.17364" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:18:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
