<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.04967] ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild</title><meta property="og:description" content="The growing prominence of the field of audio deepfake detection is driven by its wide range of applications, notably in protecting the public from potential fraud and other malicious activities, prompting the need for …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.04967">

<!--Generated on Thu Sep  5 13:49:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Deepfake audio,  fake detection,  manipulation region location,  source attribution,  competitions.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiangyan Yi, 
Chu Yuan Zhang,
Jianhua Tao, 
Chenglong Wang,
Xinrui Yan,
Yong Ren,
Hao Gu,
Junzuo Zhou
</span><span class="ltx_author_notes">Jiangyan Yi and Chu Yuan Zhang contributed equally.Jiangyan Yi,
Chu Yuan Zhang,
Chenglong Wang,
Xinrui Yan,
Yong Ren,
Hao Gu,
and Junzuo Zhou are with the Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. jiangyan.yi@nlpr.ia.ac.cnJianhua Tao is with the Department of Automation, Tsinghua University, Beijing 100190, China.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The growing prominence of the field of audio deepfake detection is driven by its wide range of applications, notably in protecting the public from potential fraud and other malicious activities, prompting the need for greater attention and research in this area. The ADD 2023 challenge goes beyond binary real/fake classification by emulating real-world scenarios, such as the identification of manipulated intervals in partially fake audio and determining the source responsible for generating any fake audio, both with real-life implications, notably in audio forensics, law enforcement, and construction of reliable and trustworthy evidence. To further foster research in this area, in this article, we describe the dataset that was used in the fake game, manipulation region location and deepfake algorithm recognition tracks of the challenge. We also focus on the analysis of the technical methodologies by the top-performing participants in each task and note the commonalities and differences in their approaches. Finally, we discuss the current technical limitations as identified through the technical analysis, and provide a roadmap for future research directions. The dataset is available for download<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://addchallenge.cn/downloadADD2023</span></span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deepfake audio, fake detection, manipulation region location, source attribution, competitions.

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_publicationid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: 0000–0000/00$00.00 © 2021 IEEE</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent rapid advancements in text-to-speech (TTS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and voice conversion (VC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> technologies over the past decades have made it possible to generate high-quality and realistic audio that can be difficult to distinguish from real audio with the naked ear. Such a technology has a potential to be abused and misused, notably in generating deepfake audio for impersonation, fraud, and other malicious purposes. The rapid progress in speech synthesis and voice conversion technologies means that these attacks can be readily launched by anyone with a computer and a microphone, and rapidly spread through social media. Therefore, it is urgent to develop effective deepfake audio detection methods to protect the public from being deceived by deepfake audio.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In response to this necessity, the Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in 2021 and 2024, notably included a speech deepfake detection (DF) track which specifically focused on the detection of VC and TTS from audio, which further spurs research in this area. The first Audio Deepfake Detection Challenge (ADD 2022) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> was organized
to further promote research on audio deepfake detection. Yet, current efforts on deepfake audio detection focus on the binary classification of real and fake audio, which may sometimes prove insufficient in real-world scenarios. For instance, in the case of audio forensics and law enforcement, it is often crucial to identify the specific intervals within partially fake audio where manipulation occurs; determining the source of the fake audio is also important for attribution and accountability, protecting intellectual property rights, and preventing the spread of misinformation. This improved detection and analysis of deepfake audio is also essential for constructing reliable and trustworthy evidence, not only in court, but also in other areas such as journalism and social media. Hence, greater attention is needed to advance deepfake audio detection beyond binary real/fake classification. These challenges represent the next frontier in combating audio manipulation and deception.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In response to this growing need to advance deepfake audio detection research in this direction, we launched a second Audio Deepfake Detection Challenge (ADD 2023)<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://addchallenge.cn/add2023</span></span></span></span> to further promote research on deepfake audio detection and analysis. The tasks in the challenge are designed to emulate real-world scenarios and to motivate research that goes beyond the traditional binary classification of real and fake audio.
To this end, we also release a dataset for the ADD 2023 challenge, partitioned into four subsets, each corresponding to a task in the challenge. The dataset is described in detail in Section <a href="#S4" title="IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. In addition, we analyze the top-performing participating systems in each track and discuss the future directions of deepfake audio detection research in Section <a href="#S5" title="V Technical Analysis ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> and Section <a href="#S6" title="VI Future Directions ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, respectively.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contributions of this article include: (1) we detail the datasets used in all tracks of the ADD 2023 challenge and provide a comprehensive description of each of the datasets; (2) we analyze the technical methodologies used by the top-performing participants in each task of the ADD 2023 challenge and note the commonalities and differences in their approaches and (3) we discuss the current technical limitations identified during the challenge and provide a roadmap for future research directions.
We hope that the insight gained from our analysis will help to further advance the research on deepfake audio detection and analysis, and that the ADD 2023 challenge will serve as a stepping stone for future research in this area.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.04967/assets/imgs/diag-track-1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="196" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.04967/assets/imgs/diag-track-2.png" id="S1.F1.2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="144" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.04967/assets/imgs/diag-track-3.png" id="S1.F1.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="326" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.6.2" class="ltx_text" style="font-size:90%;">The three tracks of ADD 2023.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Challenge Outline</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we outline the ADD 2023 challenge, which was divided into three tracks (see Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Track 1: Audio fake game (FG)</span>, representing the attack-and-defense game <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> between the attacking party, which was tasked with generating deepfake audio, and the defending party, which was tasked with detecting deepfake audio. This track was further divided into two different yet interconnected sub-tracks, each with two rounds of evaluations, allowing for adaptation and evolution of the participants’ systems:</p>
<ul id="S2.I1.i1.I1" class="ltx_itemize">
<li id="S2.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I1.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Track 1.1: Generation task (FG-G)</span>, representing the attacking party in the process. The participants were tasked with generating deepfake audio that convincingly emulated the characteristics of genuine audio.</p>
</div>
</li>
<li id="S2.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I1.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Track 1.2: Detection task (FG-D)</span>, representing the defending party in the process. The participants were tasked with detecting deepfake audio, some of which were generated by the participants in the FG-G sub-track, and distinguishing it from genuine audio.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Track 2: Manipulation region location (RL)</span>, representing the task of locating the specific intervals within partially fake audio where manipulation occurs. The manipulation region, for the purpose of the challenge, is defined as the interval of the audio signal that is replaced by a different audio signal that is either generated with the same target speaker ID, or from a different genuine recording of the same speaker.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Track 3: Deepfake algorithm recognition (AR)</span>, representing the task of determining the source algorithm responsible for generating a given piece of fake audio, as well as an unknown deepfake algorithm.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Evaluation Metrics</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section presents the evaluation metrics for the four tasks of the ADD 2023, measuring participants’ performance various tasks. The precise metrics used for each task are reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Track 1.1: Generation task (FG-G)</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The evaluation metric for the generation task is the deception success rate (DSR), based on the percentage of utterances misclassified as genuine by the FG-D sub-track detection systems. In the second round, participants must also deceive a target detection system, with performance scored by the weighted sum of detection errors from both the target model and FG-D sub-track models. Each team’s score is the weighted sum of DSRs from both rounds, emphasizing the second round to encourage adapting methods.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Track 1.2: Detection task (FG-D)</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Given the task of Track 1.2 in distinguishing genuine and generated audio samples, the evaluation metric used for this sub-track is the equal error rate (EER). The overall performance of detection systems is evaluated as the weighted average EER (WEER) of the detection systems from both rounds, allowing for more emphasis on the second round.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Track 2: Manipulation region location (RL)</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">The evaluation metric for the RL track is the sentence-level accuracy (<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="A_{s}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">A_{s}</annotation></semantics></math>) and the frame-level <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\text{F}_{1}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mtext id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2a.cmml">F</mtext><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2a.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><mtext id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">F</mtext></ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\text{F}_{1}</annotation></semantics></math> score (<math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="F_{1f}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">F</mi><mrow id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mn id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝐹</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><times id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">1</cn><ci id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">F_{1f}</annotation></semantics></math>). The overall performance of each team in the RL track (Score)
is thus evaluated as a weighted sum of <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="A_{s}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msub id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝐴</ci><ci id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">A_{s}</annotation></semantics></math> and the <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="F_{1f}" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><msub id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">F</mi><mrow id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml"><mn id="S3.SS3.p1.5.m5.1.1.3.2" xref="S3.SS3.p1.5.m5.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.5.m5.1.1.3.1" xref="S3.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.5.m5.1.1.3.3" xref="S3.SS3.p1.5.m5.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">𝐹</ci><apply id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3"><times id="S3.SS3.p1.5.m5.1.1.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.3.1"></times><cn type="integer" id="S3.SS3.p1.5.m5.1.1.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.3.2">1</cn><ci id="S3.SS3.p1.5.m5.1.1.3.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">F_{1f}</annotation></semantics></math>, with more emphasis on the latter, which is more challenging to achieve and also the main objective of this track, without ignoring the overall <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="A_{s}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msub id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">𝐴</ci><ci id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">A_{s}</annotation></semantics></math>, which is also important in real-world applications.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Track 3: Deepfake algorithm recognition (AR)</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">The performances of the participants’ systems in Track 3 are evaluated using the macro-averaged <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\text{F}_{1}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mtext id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2a.cmml">F</mtext><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2a.cmml" xref="S3.SS4.p1.1.m1.1.1.2"><mtext id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">F</mtext></ci><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\text{F}_{1}</annotation></semantics></math> score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">F</mi><mn id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">F_{1}</annotation></semantics></math>), which includes both known and unknown class samples, testing the generalization capabilities of the participants’ systems.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Datasets</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present and describe the datasets<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Available for download at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://addchallenge.cn/downloadADD2023</span></span></span></span> used in the ADD 2023 challenge. Each dataset is used for a specific task that corresponds to a track in the challenge.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Track 1.1: Generation task (FG-G)</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The FG-G track being focused on the generation of deepfake audio, its dataset is composed of speech corpora with transcripts. In this case, we use the AISHELL-3 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> in the training process, which is a large-scale multi-speaker Mandarin speech dataset for TTS that contains roughly 85 hours of speech data from 218 native Mandarin speakers, totalling 88,035 utterances, each with corresponding transcripts in both Chinese characters and Pinyin.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The testing set for the FG-G subtrack, summarized in Table <a href="#S4.T1" title="TABLE I ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, is composed of 998 text sentences (499 per each round) in Simplified Chinese characters, sourced from the Internet, as well as four speaker IDs (two per each round) from the aforementioned AISHELL-3 dataset. For round 1, speakers with the IDs SSB0539 and SSB1448 were chosen, while for round 2, speakers with the IDs SSB0737 and SSB0817 were selected. The text data is used for the generation task, where participants are tasked with generating deepfake audio that convincingly emulates the characteristics of genuine utterances.
Without punctuations, the average length of these sentences is 66.98 characters, with a maximum length of 147 characters, a minimum length of 39 characters, and a variance of 136.78 characters.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Testing set for the FG-G sub-track (Track 1.1).</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S4.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.1.1.1.1.1" class="ltx_p"><span id="S4.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Round</span></span>
</span>
</th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.4.1.1.2.1" class="ltx_text ltx_font_bold"># Speakers</span></th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.4.1.1.3.1" class="ltx_text ltx_font_bold"># Sentences</span></th>
<th id="S4.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.1.1.4.1" class="ltx_text ltx_font_bold"># Utterances</span></th>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<th id="S4.T1.4.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.1.1" class="ltx_text ltx_font_bold"># Male</span></th>
<th id="S4.T1.4.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.4.2.2.2.1" class="ltx_text ltx_font_bold"># Female</span></th>
<th id="S4.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.4.2.2.3.1" class="ltx_text ltx_font_bold">Generated</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.3.1" class="ltx_tr">
<td id="S4.T1.4.3.1.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.4.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.3.1.1.1.1" class="ltx_p"><span id="S4.T1.4.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Round 1</span></span>
</span>
</td>
<td id="S4.T1.4.3.1.2" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S4.T1.4.3.1.3" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S4.T1.4.3.1.4" class="ltx_td ltx_align_right ltx_border_t">499</td>
<td id="S4.T1.4.3.1.5" class="ltx_td ltx_align_right ltx_border_t">998</td>
</tr>
<tr id="S4.T1.4.4.2" class="ltx_tr">
<td id="S4.T1.4.4.2.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T1.4.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.2.1.1.1" class="ltx_p"><span id="S4.T1.4.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Round 2</span></span>
</span>
</td>
<td id="S4.T1.4.4.2.2" class="ltx_td ltx_align_right ltx_border_bb">1</td>
<td id="S4.T1.4.4.2.3" class="ltx_td ltx_align_right ltx_border_bb">1</td>
<td id="S4.T1.4.4.2.4" class="ltx_td ltx_align_right ltx_border_bb">499</td>
<td id="S4.T1.4.4.2.5" class="ltx_td ltx_align_right ltx_border_bb">998</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Dataset for the FG-D sub-track (Track 1.2).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T2.st1.3.2" class="ltx_text" style="font-size:90%;">Training and development sets; the same used for both rounds.</span></figcaption>
<table id="S4.T2.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.st1.4.1.1" class="ltx_tr">
<th id="S4.T2.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" rowspan="2"><span id="S4.T2.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></th>
<th id="S4.T2.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S4.T2.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">Dev</span></th>
</tr>
<tr id="S4.T2.st1.4.2.2" class="ltx_tr">
<th id="S4.T2.st1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.st1.4.2.2.1.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T2.st1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.st1.4.2.2.2.1" class="ltx_text ltx_font_bold"># Utt</span></th>
<th id="S4.T2.st1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.st1.4.2.2.3.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T2.st1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.st1.4.2.2.4.1" class="ltx_text ltx_font_bold"># Utt</span></th>
</tr>
<tr id="S4.T2.st1.4.3.3" class="ltx_tr">
<th id="S4.T2.st1.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.st1.4.3.3.1.1" class="ltx_text ltx_font_bold">Real</span></th>
<th id="S4.T2.st1.4.3.3.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.st1.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.3.3.2.1.1" class="ltx_p">AISHELL-3</span>
</span>
</th>
<th id="S4.T2.st1.4.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T2.st1.4.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">3,012</th>
<th id="S4.T2.st1.4.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T2.st1.4.3.3.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">2,307</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.st1.4.4.1" class="ltx_tr">
<td id="S4.T2.st1.4.4.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="7"><span id="S4.T2.st1.4.4.1.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T2.st1.4.4.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.st1.4.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.4.1.2.1.1" class="ltx_p">HiFiGAN</span>
</span>
</td>
<td id="S4.T2.st1.4.4.1.3" class="ltx_td ltx_align_right ltx_border_t">60</td>
<td id="S4.T2.st1.4.4.1.4" class="ltx_td ltx_align_right ltx_border_t">4,012</td>
<td id="S4.T2.st1.4.4.1.5" class="ltx_td ltx_align_right ltx_border_t">60</td>
<td id="S4.T2.st1.4.4.1.6" class="ltx_td ltx_align_right ltx_border_t">4,387</td>
</tr>
<tr id="S4.T2.st1.4.5.2" class="ltx_tr">
<td id="S4.T2.st1.4.5.2.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st1.4.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.5.2.1.1.1" class="ltx_p">LPCNet</span>
</span>
</td>
<td id="S4.T2.st1.4.5.2.2" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.5.2.3" class="ltx_td ltx_align_right">4,012</td>
<td id="S4.T2.st1.4.5.2.4" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.5.2.5" class="ltx_td ltx_align_right">4,387</td>
</tr>
<tr id="S4.T2.st1.4.6.3" class="ltx_tr">
<td id="S4.T2.st1.4.6.3.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st1.4.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.6.3.1.1.1" class="ltx_p">Multiband MelGAN</span>
</span>
</td>
<td id="S4.T2.st1.4.6.3.2" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.6.3.3" class="ltx_td ltx_align_right">4,012</td>
<td id="S4.T2.st1.4.6.3.4" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.6.3.5" class="ltx_td ltx_align_right">4,387</td>
</tr>
<tr id="S4.T2.st1.4.7.4" class="ltx_tr">
<td id="S4.T2.st1.4.7.4.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st1.4.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.7.4.1.1.1" class="ltx_p">StyleMelGAN</span>
</span>
</td>
<td id="S4.T2.st1.4.7.4.2" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.7.4.3" class="ltx_td ltx_align_right">4,012</td>
<td id="S4.T2.st1.4.7.4.4" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.7.4.5" class="ltx_td ltx_align_right">4,387</td>
</tr>
<tr id="S4.T2.st1.4.8.5" class="ltx_tr">
<td id="S4.T2.st1.4.8.5.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st1.4.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.8.5.1.1.1" class="ltx_p">Parallel WaveGAN</span>
</span>
</td>
<td id="S4.T2.st1.4.8.5.2" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.8.5.3" class="ltx_td ltx_align_right">4,012</td>
<td id="S4.T2.st1.4.8.5.4" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.8.5.5" class="ltx_td ltx_align_right">4,082</td>
</tr>
<tr id="S4.T2.st1.4.9.6" class="ltx_tr">
<td id="S4.T2.st1.4.9.6.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st1.4.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.9.6.1.1.1" class="ltx_p">World</span>
</span>
</td>
<td id="S4.T2.st1.4.9.6.2" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.9.6.3" class="ltx_td ltx_align_right">4,012</td>
<td id="S4.T2.st1.4.9.6.4" class="ltx_td ltx_align_right">60</td>
<td id="S4.T2.st1.4.9.6.5" class="ltx_td ltx_align_right">4,387</td>
</tr>
<tr id="S4.T2.st1.4.10.7" class="ltx_tr">
<td id="S4.T2.st1.4.10.7.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T2.st1.4.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st1.4.10.7.1.1.1" class="ltx_p"><span id="S4.T2.st1.4.10.7.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T2.st1.4.10.7.2" class="ltx_td ltx_align_right ltx_border_bb">360</td>
<td id="S4.T2.st1.4.10.7.3" class="ltx_td ltx_align_right ltx_border_bb">24,072</td>
<td id="S4.T2.st1.4.10.7.4" class="ltx_td ltx_align_right ltx_border_bb">360</td>
<td id="S4.T2.st1.4.10.7.5" class="ltx_td ltx_align_right ltx_border_bb">26,027</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T2.st2.3.2" class="ltx_text" style="font-size:90%;">Testing sets. Note that the utterances for the two rounds are selected randomly and independently, and may have overlaps between them (except FG-G, where the two rounds are disjunct).</span></figcaption>
<table id="S4.T2.st2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.st2.4.1.1" class="ltx_tr">
<td id="S4.T2.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2" rowspan="2"><span id="S4.T2.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></td>
<td id="S4.T2.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">Round 1</span></td>
<td id="S4.T2.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.st2.4.1.1.3.1" class="ltx_text ltx_font_bold">Round 2</span></td>
</tr>
<tr id="S4.T2.st2.4.2.2" class="ltx_tr">
<td id="S4.T2.st2.4.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.st2.4.2.2.1.1" class="ltx_text ltx_font_bold"># Spk</span></td>
<td id="S4.T2.st2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.st2.4.2.2.2.1" class="ltx_text ltx_font_bold"># Utt</span></td>
<td id="S4.T2.st2.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.st2.4.2.2.3.1" class="ltx_text ltx_font_bold"># Spk</span></td>
<td id="S4.T2.st2.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.st2.4.2.2.4.1" class="ltx_text ltx_font_bold"># Utt</span></td>
</tr>
<tr id="S4.T2.st2.4.3.3" class="ltx_tr">
<td id="S4.T2.st2.4.3.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="9"><span id="S4.T2.st2.4.3.3.1.1" class="ltx_text ltx_font_bold">Real</span></td>
<td id="S4.T2.st2.4.3.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.st2.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.3.3.2.1.1" class="ltx_p">AISHELL-3</span>
</span>
</td>
<td id="S4.T2.st2.4.3.3.3" class="ltx_td ltx_align_right ltx_border_t">50</td>
<td id="S4.T2.st2.4.3.3.4" class="ltx_td ltx_align_right ltx_border_t">3,012</td>
<td id="S4.T2.st2.4.3.3.5" class="ltx_td ltx_align_right ltx_border_t">50</td>
<td id="S4.T2.st2.4.3.3.6" class="ltx_td ltx_align_right ltx_border_t">2,307</td>
</tr>
<tr id="S4.T2.st2.4.4.4" class="ltx_tr">
<td id="S4.T2.st2.4.4.4.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.4.4.1.1.1" class="ltx_p">AISHELL-1</span>
</span>
</td>
<td id="S4.T2.st2.4.4.4.2" class="ltx_td ltx_align_right">200</td>
<td id="S4.T2.st2.4.4.4.3" class="ltx_td ltx_align_right">10,000</td>
<td id="S4.T2.st2.4.4.4.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.4.4.5" class="ltx_td ltx_align_right">–</td>
</tr>
<tr id="S4.T2.st2.4.5.5" class="ltx_tr">
<td id="S4.T2.st2.4.5.5.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.5.5.1.1.1" class="ltx_p">THCHS-30</span>
</span>
</td>
<td id="S4.T2.st2.4.5.5.2" class="ltx_td ltx_align_right">30</td>
<td id="S4.T2.st2.4.5.5.3" class="ltx_td ltx_align_right">7,000</td>
<td id="S4.T2.st2.4.5.5.4" class="ltx_td ltx_align_right">30</td>
<td id="S4.T2.st2.4.5.5.5" class="ltx_td ltx_align_right">7,000</td>
</tr>
<tr id="S4.T2.st2.4.6.6" class="ltx_tr">
<td id="S4.T2.st2.4.6.6.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.6.6.1.1.1" class="ltx_p">HI-MIA</span>
</span>
</td>
<td id="S4.T2.st2.4.6.6.2" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.6.6.3" class="ltx_td ltx_align_right">10,000</td>
<td id="S4.T2.st2.4.6.6.4" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.6.6.5" class="ltx_td ltx_align_right">10,000</td>
</tr>
<tr id="S4.T2.st2.4.7.7" class="ltx_tr">
<td id="S4.T2.st2.4.7.7.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.7.7.1.1.1" class="ltx_p">Mobvoi</span>
</span>
</td>
<td id="S4.T2.st2.4.7.7.2" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.7.7.3" class="ltx_td ltx_align_right">20,000</td>
<td id="S4.T2.st2.4.7.7.4" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.7.7.5" class="ltx_td ltx_align_right">20,000</td>
</tr>
<tr id="S4.T2.st2.4.8.8" class="ltx_tr">
<td id="S4.T2.st2.4.8.8.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.8.8.1.1.1" class="ltx_p">AliMeeting</span>
</span>
</td>
<td id="S4.T2.st2.4.8.8.2" class="ltx_td ltx_align_right">50</td>
<td id="S4.T2.st2.4.8.8.3" class="ltx_td ltx_align_right">20,000</td>
<td id="S4.T2.st2.4.8.8.4" class="ltx_td ltx_align_right">50</td>
<td id="S4.T2.st2.4.8.8.5" class="ltx_td ltx_align_right">20,000</td>
</tr>
<tr id="S4.T2.st2.4.9.9" class="ltx_tr">
<td id="S4.T2.st2.4.9.9.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.9.9.1.1.1" class="ltx_p">Phone recording</span>
</span>
</td>
<td id="S4.T2.st2.4.9.9.2" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.9.9.3" class="ltx_td ltx_align_right">10,000</td>
<td id="S4.T2.st2.4.9.9.4" class="ltx_td ltx_align_right">100</td>
<td id="S4.T2.st2.4.9.9.5" class="ltx_td ltx_align_right">10,000</td>
</tr>
<tr id="S4.T2.st2.4.10.10" class="ltx_tr">
<td id="S4.T2.st2.4.10.10.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.10.10.1.1.1" class="ltx_p">ESD</span>
</span>
</td>
<td id="S4.T2.st2.4.10.10.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.10.10.3" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.10.10.4" class="ltx_td ltx_align_right">10</td>
<td id="S4.T2.st2.4.10.10.5" class="ltx_td ltx_align_right">17,500</td>
</tr>
<tr id="S4.T2.st2.4.11.11" class="ltx_tr">
<td id="S4.T2.st2.4.11.11.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.11.11.1.1.1" class="ltx_p"><span id="S4.T2.st2.4.11.11.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T2.st2.4.11.11.2" class="ltx_td ltx_align_right">630</td>
<td id="S4.T2.st2.4.11.11.3" class="ltx_td ltx_align_right">80,012</td>
<td id="S4.T2.st2.4.11.11.4" class="ltx_td ltx_align_right">440</td>
<td id="S4.T2.st2.4.11.11.5" class="ltx_td ltx_align_right">86,807</td>
</tr>
<tr id="S4.T2.st2.4.12.12" class="ltx_tr">
<td id="S4.T2.st2.4.12.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="23"><span id="S4.T2.st2.4.12.12.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T2.st2.4.12.12.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.st2.4.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.12.12.2.1.1" class="ltx_p">HiFiGAN</span>
</span>
</td>
<td id="S4.T2.st2.4.12.12.3" class="ltx_td ltx_align_right ltx_border_t">218</td>
<td id="S4.T2.st2.4.12.12.4" class="ltx_td ltx_align_right ltx_border_t">600</td>
<td id="S4.T2.st2.4.12.12.5" class="ltx_td ltx_align_right ltx_border_t">218</td>
<td id="S4.T2.st2.4.12.12.6" class="ltx_td ltx_align_right ltx_border_t">600</td>
</tr>
<tr id="S4.T2.st2.4.13.13" class="ltx_tr">
<td id="S4.T2.st2.4.13.13.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.13.13.1.1.1" class="ltx_p">LPCNet</span>
</span>
</td>
<td id="S4.T2.st2.4.13.13.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.13.13.3" class="ltx_td ltx_align_right">3,000</td>
<td id="S4.T2.st2.4.13.13.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.13.13.5" class="ltx_td ltx_align_right">3,000</td>
</tr>
<tr id="S4.T2.st2.4.14.14" class="ltx_tr">
<td id="S4.T2.st2.4.14.14.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.14.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.14.14.1.1.1" class="ltx_p">Multiband MelGAN</span>
</span>
</td>
<td id="S4.T2.st2.4.14.14.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.14.14.3" class="ltx_td ltx_align_right">600</td>
<td id="S4.T2.st2.4.14.14.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.14.14.5" class="ltx_td ltx_align_right">600</td>
</tr>
<tr id="S4.T2.st2.4.15.15" class="ltx_tr">
<td id="S4.T2.st2.4.15.15.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.15.15.1.1.1" class="ltx_p">Parallel WaveGAN</span>
</span>
</td>
<td id="S4.T2.st2.4.15.15.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.15.15.3" class="ltx_td ltx_align_right">600</td>
<td id="S4.T2.st2.4.15.15.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.15.15.5" class="ltx_td ltx_align_right">600</td>
</tr>
<tr id="S4.T2.st2.4.16.16" class="ltx_tr">
<td id="S4.T2.st2.4.16.16.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.16.16.1.1.1" class="ltx_p">StyleMelGAN</span>
</span>
</td>
<td id="S4.T2.st2.4.16.16.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.16.16.3" class="ltx_td ltx_align_right">600</td>
<td id="S4.T2.st2.4.16.16.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.16.16.5" class="ltx_td ltx_align_right">600</td>
</tr>
<tr id="S4.T2.st2.4.17.17" class="ltx_tr">
<td id="S4.T2.st2.4.17.17.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.17.17.1.1.1" class="ltx_p">World</span>
</span>
</td>
<td id="S4.T2.st2.4.17.17.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.17.17.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.17.17.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.17.17.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.18.18" class="ltx_tr">
<td id="S4.T2.st2.4.18.18.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.18.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.18.18.1.1.1" class="ltx_p">WaveRNN</span>
</span>
</td>
<td id="S4.T2.st2.4.18.18.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.18.18.3" class="ltx_td ltx_align_right">3,600</td>
<td id="S4.T2.st2.4.18.18.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.18.18.5" class="ltx_td ltx_align_right">3,600</td>
</tr>
<tr id="S4.T2.st2.4.19.19" class="ltx_tr">
<td id="S4.T2.st2.4.19.19.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.19.19.1.1.1" class="ltx_p">VITS</span>
</span>
</td>
<td id="S4.T2.st2.4.19.19.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.19.19.3" class="ltx_td ltx_align_right">1,334</td>
<td id="S4.T2.st2.4.19.19.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.19.19.5" class="ltx_td ltx_align_right">1,334</td>
</tr>
<tr id="S4.T2.st2.4.20.20" class="ltx_tr">
<td id="S4.T2.st2.4.20.20.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.20.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.20.20.1.1.1" class="ltx_p">Grad-TTS</span>
</span>
</td>
<td id="S4.T2.st2.4.20.20.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.20.20.3" class="ltx_td ltx_align_right">1,333</td>
<td id="S4.T2.st2.4.20.20.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.20.20.5" class="ltx_td ltx_align_right">1,333</td>
</tr>
<tr id="S4.T2.st2.4.21.21" class="ltx_tr">
<td id="S4.T2.st2.4.21.21.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.21.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.21.21.1.1.1" class="ltx_p">Speech Edit</span>
</span>
</td>
<td id="S4.T2.st2.4.21.21.2" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.21.21.3" class="ltx_td ltx_align_right">1,333</td>
<td id="S4.T2.st2.4.21.21.4" class="ltx_td ltx_align_right">218</td>
<td id="S4.T2.st2.4.21.21.5" class="ltx_td ltx_align_right">1,333</td>
</tr>
<tr id="S4.T2.st2.4.22.22" class="ltx_tr">
<td id="S4.T2.st2.4.22.22.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.22.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.22.22.1.1.1" class="ltx_p">Alibaba</span>
</span>
</td>
<td id="S4.T2.st2.4.22.22.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.22.22.3" class="ltx_td ltx_align_right">482</td>
<td id="S4.T2.st2.4.22.22.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.22.22.5" class="ltx_td ltx_align_right">482</td>
</tr>
<tr id="S4.T2.st2.4.23.23" class="ltx_tr">
<td id="S4.T2.st2.4.23.23.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.23.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.23.23.1.1.1" class="ltx_p">DeepSound</span>
</span>
</td>
<td id="S4.T2.st2.4.23.23.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.23.23.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.23.23.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.23.23.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.24.24" class="ltx_tr">
<td id="S4.T2.st2.4.24.24.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.24.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.24.24.1.1.1" class="ltx_p">iFlytek</span>
</span>
</td>
<td id="S4.T2.st2.4.24.24.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.24.24.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.24.24.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.24.24.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.25.25" class="ltx_tr">
<td id="S4.T2.st2.4.25.25.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.25.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.25.25.1.1.1" class="ltx_p">Mobvoi</span>
</span>
</td>
<td id="S4.T2.st2.4.25.25.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.25.25.3" class="ltx_td ltx_align_right">397</td>
<td id="S4.T2.st2.4.25.25.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.25.25.5" class="ltx_td ltx_align_right">397</td>
</tr>
<tr id="S4.T2.st2.4.26.26" class="ltx_tr">
<td id="S4.T2.st2.4.26.26.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.26.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.26.26.1.1.1" class="ltx_p">Baidu</span>
</span>
</td>
<td id="S4.T2.st2.4.26.26.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.26.26.3" class="ltx_td ltx_align_right">387</td>
<td id="S4.T2.st2.4.26.26.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.26.26.5" class="ltx_td ltx_align_right">387</td>
</tr>
<tr id="S4.T2.st2.4.27.27" class="ltx_tr">
<td id="S4.T2.st2.4.27.27.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.27.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.27.27.1.1.1" class="ltx_p">Aispeech</span>
</span>
</td>
<td id="S4.T2.st2.4.27.27.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.27.27.3" class="ltx_td ltx_align_right">385</td>
<td id="S4.T2.st2.4.27.27.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.27.27.5" class="ltx_td ltx_align_right">385</td>
</tr>
<tr id="S4.T2.st2.4.28.28" class="ltx_tr">
<td id="S4.T2.st2.4.28.28.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.28.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.28.28.1.1.1" class="ltx_p">DataBaker</span>
</span>
</td>
<td id="S4.T2.st2.4.28.28.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.28.28.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.28.28.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.28.28.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.29.29" class="ltx_tr">
<td id="S4.T2.st2.4.29.29.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.29.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.29.29.1.1.1" class="ltx_p">Sogou</span>
</span>
</td>
<td id="S4.T2.st2.4.29.29.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.29.29.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.29.29.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.29.29.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.30.30" class="ltx_tr">
<td id="S4.T2.st2.4.30.30.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.30.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.30.30.1.1.1" class="ltx_p">Tencent</span>
</span>
</td>
<td id="S4.T2.st2.4.30.30.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.30.30.3" class="ltx_td ltx_align_right">400</td>
<td id="S4.T2.st2.4.30.30.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.30.30.5" class="ltx_td ltx_align_right">400</td>
</tr>
<tr id="S4.T2.st2.4.31.31" class="ltx_tr">
<td id="S4.T2.st2.4.31.31.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.31.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.31.31.1.1.1" class="ltx_p">Sohu</span>
</span>
</td>
<td id="S4.T2.st2.4.31.31.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.31.31.3" class="ltx_td ltx_align_right">349</td>
<td id="S4.T2.st2.4.31.31.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.31.31.5" class="ltx_td ltx_align_right">349</td>
</tr>
<tr id="S4.T2.st2.4.32.32" class="ltx_tr">
<td id="S4.T2.st2.4.32.32.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.32.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.32.32.1.1.1" class="ltx_p">Blizzard</span>
</span>
</td>
<td id="S4.T2.st2.4.32.32.2" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.32.32.3" class="ltx_td ltx_align_right">4,000</td>
<td id="S4.T2.st2.4.32.32.4" class="ltx_td ltx_align_right">–</td>
<td id="S4.T2.st2.4.32.32.5" class="ltx_td ltx_align_right">4,000</td>
</tr>
<tr id="S4.T2.st2.4.33.33" class="ltx_tr">
<td id="S4.T2.st2.4.33.33.1" class="ltx_td ltx_align_justify">
<span id="S4.T2.st2.4.33.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.33.33.1.1.1" class="ltx_p">FG-G</span>
</span>
</td>
<td id="S4.T2.st2.4.33.33.2" class="ltx_td ltx_align_right">2</td>
<td id="S4.T2.st2.4.33.33.3" class="ltx_td ltx_align_right">11,976</td>
<td id="S4.T2.st2.4.33.33.4" class="ltx_td ltx_align_right">2</td>
<td id="S4.T2.st2.4.33.33.5" class="ltx_td ltx_align_right">11,976</td>
</tr>
<tr id="S4.T2.st2.4.34.34" class="ltx_tr">
<td id="S4.T2.st2.4.34.34.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T2.st2.4.34.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.st2.4.34.34.1.1.1" class="ltx_p"><span id="S4.T2.st2.4.34.34.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T2.st2.4.34.34.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.st2.4.34.34.3" class="ltx_td ltx_align_right ltx_border_bb">33,376</td>
<td id="S4.T2.st2.4.34.34.4" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.st2.4.34.34.5" class="ltx_td ltx_align_right ltx_border_bb">33,376</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Dataset for the RL track (Track 2)</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T3.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T3.st1.3.2" class="ltx_text" style="font-size:90%;">Training and development sets.</span></figcaption>
<table id="S4.T3.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.st1.4.1.1" class="ltx_tr">
<th id="S4.T3.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" rowspan="2"><span id="S4.T3.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></th>
<th id="S4.T3.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S4.T3.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">Dev</span></th>
</tr>
<tr id="S4.T3.st1.4.2.2" class="ltx_tr">
<th id="S4.T3.st1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.st1.4.2.2.1.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T3.st1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.st1.4.2.2.2.1" class="ltx_text ltx_font_bold"># Utt</span></th>
<th id="S4.T3.st1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.st1.4.2.2.3.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T3.st1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.st1.4.2.2.4.1" class="ltx_text ltx_font_bold"># Utt</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.st1.4.3.1" class="ltx_tr">
<td id="S4.T3.st1.4.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.st1.4.3.1.1.1" class="ltx_text ltx_font_bold">Real</span></td>
<td id="S4.T3.st1.4.3.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T3.st1.4.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st1.4.3.1.2.1.1" class="ltx_p">AISHELL-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S4.T3.st1.4.3.1.3" class="ltx_td ltx_align_right ltx_border_t">83</td>
<td id="S4.T3.st1.4.3.1.4" class="ltx_td ltx_align_right ltx_border_t">26,554</td>
<td id="S4.T3.st1.4.3.1.5" class="ltx_td ltx_align_right ltx_border_t">38</td>
<td id="S4.T3.st1.4.3.1.6" class="ltx_td ltx_align_right ltx_border_t">8,914</td>
</tr>
<tr id="S4.T3.st1.4.4.2" class="ltx_tr">
<td id="S4.T3.st1.4.4.2.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.st1.4.4.2.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T3.st1.4.4.2.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T3.st1.4.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st1.4.4.2.2.1.1" class="ltx_p">HAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
</span>
</td>
<td id="S4.T3.st1.4.4.2.3" class="ltx_td ltx_align_right ltx_border_bb">83</td>
<td id="S4.T3.st1.4.4.2.4" class="ltx_td ltx_align_right ltx_border_bb">26,554</td>
<td id="S4.T3.st1.4.4.2.5" class="ltx_td ltx_align_right ltx_border_bb">33</td>
<td id="S4.T3.st1.4.4.2.6" class="ltx_td ltx_align_right ltx_border_bb">8,914</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T3.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T3.st2.3.2" class="ltx_text" style="font-size:90%;">Testing sets.</span></figcaption>
<table id="S4.T3.st2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.st2.4.1.1" class="ltx_tr">
<td id="S4.T3.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></td>
<td id="S4.T3.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.st2.4.1.1.2.1" class="ltx_text ltx_font_bold"># Spk</span></td>
<td id="S4.T3.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.st2.4.1.1.3.1" class="ltx_text ltx_font_bold"># Utt</span></td>
</tr>
<tr id="S4.T3.st2.4.2.2" class="ltx_tr">
<td id="S4.T3.st2.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="7"><span id="S4.T3.st2.4.2.2.1.1" class="ltx_text ltx_font_bold">Real</span></td>
<td id="S4.T3.st2.4.2.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T3.st2.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.2.2.2.1.1" class="ltx_p">AISHELL-3</span>
</span>
</td>
<td id="S4.T3.st2.4.2.2.3" class="ltx_td ltx_align_right ltx_border_t">50</td>
<td id="S4.T3.st2.4.2.2.4" class="ltx_td ltx_align_right ltx_border_t">767</td>
</tr>
<tr id="S4.T3.st2.4.3.3" class="ltx_tr">
<td id="S4.T3.st2.4.3.3.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.3.3.1.1.1" class="ltx_p">AISHELL-1</span>
</span>
</td>
<td id="S4.T3.st2.4.3.3.2" class="ltx_td ltx_align_right">200</td>
<td id="S4.T3.st2.4.3.3.3" class="ltx_td ltx_align_right">2,482</td>
</tr>
<tr id="S4.T3.st2.4.4.4" class="ltx_tr">
<td id="S4.T3.st2.4.4.4.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.4.4.1.1.1" class="ltx_p">THCHS-30</span>
</span>
</td>
<td id="S4.T3.st2.4.4.4.2" class="ltx_td ltx_align_right">30</td>
<td id="S4.T3.st2.4.4.4.3" class="ltx_td ltx_align_right">439</td>
</tr>
<tr id="S4.T3.st2.4.5.5" class="ltx_tr">
<td id="S4.T3.st2.4.5.5.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.5.5.1.1.1" class="ltx_p">AliMeeting</span>
</span>
</td>
<td id="S4.T3.st2.4.5.5.2" class="ltx_td ltx_align_right">500</td>
<td id="S4.T3.st2.4.5.5.3" class="ltx_td ltx_align_right">4,953</td>
</tr>
<tr id="S4.T3.st2.4.6.6" class="ltx_tr">
<td id="S4.T3.st2.4.6.6.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.6.6.1.1.1" class="ltx_p">Phone recording</span>
</span>
</td>
<td id="S4.T3.st2.4.6.6.2" class="ltx_td ltx_align_right">150</td>
<td id="S4.T3.st2.4.6.6.3" class="ltx_td ltx_align_right">2,419</td>
</tr>
<tr id="S4.T3.st2.4.7.7" class="ltx_tr">
<td id="S4.T3.st2.4.7.7.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.7.7.1.1.1" class="ltx_p">Mobvoi</span>
</span>
</td>
<td id="S4.T3.st2.4.7.7.2" class="ltx_td ltx_align_right">100</td>
<td id="S4.T3.st2.4.7.7.3" class="ltx_td ltx_align_right">8,940</td>
</tr>
<tr id="S4.T3.st2.4.8.8" class="ltx_tr">
<td id="S4.T3.st2.4.8.8.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.8.8.1.1.1" class="ltx_p"><span id="S4.T3.st2.4.8.8.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T3.st2.4.8.8.2" class="ltx_td ltx_align_right">1,030</td>
<td id="S4.T3.st2.4.8.8.3" class="ltx_td ltx_align_right">20,000</td>
</tr>
<tr id="S4.T3.st2.4.9.9" class="ltx_tr">
<td id="S4.T3.st2.4.9.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T3.st2.4.9.9.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T3.st2.4.9.9.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T3.st2.4.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.9.9.2.1.1" class="ltx_p">Once</span>
</span>
</td>
<td id="S4.T3.st2.4.9.9.3" class="ltx_td ltx_align_right ltx_border_t">18</td>
<td id="S4.T3.st2.4.9.9.4" class="ltx_td ltx_align_right ltx_border_t">10,000</td>
</tr>
<tr id="S4.T3.st2.4.10.10" class="ltx_tr">
<td id="S4.T3.st2.4.10.10.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.10.10.1.1.1" class="ltx_p">Twice</span>
</span>
</td>
<td id="S4.T3.st2.4.10.10.2" class="ltx_td ltx_align_right">26</td>
<td id="S4.T3.st2.4.10.10.3" class="ltx_td ltx_align_right">10,000</td>
</tr>
<tr id="S4.T3.st2.4.11.11" class="ltx_tr">
<td id="S4.T3.st2.4.11.11.1" class="ltx_td ltx_align_justify">
<span id="S4.T3.st2.4.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.11.11.1.1.1" class="ltx_p">Speech edit</span>
</span>
</td>
<td id="S4.T3.st2.4.11.11.2" class="ltx_td ltx_align_right">148</td>
<td id="S4.T3.st2.4.11.11.3" class="ltx_td ltx_align_right">10,000</td>
</tr>
<tr id="S4.T3.st2.4.12.12" class="ltx_tr">
<td id="S4.T3.st2.4.12.12.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T3.st2.4.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.st2.4.12.12.1.1.1" class="ltx_p"><span id="S4.T3.st2.4.12.12.1.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T3.st2.4.12.12.2" class="ltx_td ltx_align_right ltx_border_bb">148</td>
<td id="S4.T3.st2.4.12.12.3" class="ltx_td ltx_align_right ltx_border_bb">30,000</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Dataset for the AR track (Track 3)</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T4.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T4.st1.3.2" class="ltx_text" style="font-size:90%;">Training and development sets.</span></figcaption>
<table id="S4.T4.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.st1.4.1.1" class="ltx_tr">
<th id="S4.T4.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" rowspan="2"><span id="S4.T4.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></th>
<th id="S4.T4.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T4.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S4.T4.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T4.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">Dev</span></th>
</tr>
<tr id="S4.T4.st1.4.2.2" class="ltx_tr">
<th id="S4.T4.st1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.st1.4.2.2.1.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T4.st1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.st1.4.2.2.2.1" class="ltx_text ltx_font_bold"># Utt</span></th>
<th id="S4.T4.st1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.st1.4.2.2.3.1" class="ltx_text ltx_font_bold"># Spk</span></th>
<th id="S4.T4.st1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.st1.4.2.2.4.1" class="ltx_text ltx_font_bold"># Utt</span></th>
</tr>
<tr id="S4.T4.st1.4.3.3" class="ltx_tr">
<th id="S4.T4.st1.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.st1.4.3.3.1.1" class="ltx_text ltx_font_bold">Real</span></th>
<th id="S4.T4.st1.4.3.3.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S4.T4.st1.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.3.3.2.1.1" class="ltx_p">AISHELL-3</span>
</span>
</th>
<th id="S4.T4.st1.4.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">27</th>
<th id="S4.T4.st1.4.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">3,200</th>
<th id="S4.T4.st1.4.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">10</th>
<th id="S4.T4.st1.4.3.3.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">1,200</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.st1.4.4.1" class="ltx_tr">
<td id="S4.T4.st1.4.4.1.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="6"><span id="S4.T4.st1.4.4.1.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T4.st1.4.4.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T4.st1.4.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.4.1.2.1.1" class="ltx_p">Aliyun</span>
</span>
</td>
<td id="S4.T4.st1.4.4.1.3" class="ltx_td ltx_align_right ltx_border_t">10</td>
<td id="S4.T4.st1.4.4.1.4" class="ltx_td ltx_align_right ltx_border_t">3,200</td>
<td id="S4.T4.st1.4.4.1.5" class="ltx_td ltx_align_right ltx_border_t">2</td>
<td id="S4.T4.st1.4.4.1.6" class="ltx_td ltx_align_right ltx_border_t">1,200</td>
</tr>
<tr id="S4.T4.st1.4.5.2" class="ltx_tr">
<td id="S4.T4.st1.4.5.2.1" class="ltx_td ltx_align_justify">
<span id="S4.T4.st1.4.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.5.2.1.1.1" class="ltx_p">DataBaker</span>
</span>
</td>
<td id="S4.T4.st1.4.5.2.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st1.4.5.2.3" class="ltx_td ltx_align_right">3,200</td>
<td id="S4.T4.st1.4.5.2.4" class="ltx_td ltx_align_right">2</td>
<td id="S4.T4.st1.4.5.2.5" class="ltx_td ltx_align_right">1,200</td>
</tr>
<tr id="S4.T4.st1.4.6.3" class="ltx_tr">
<td id="S4.T4.st1.4.6.3.1" class="ltx_td ltx_align_justify">
<span id="S4.T4.st1.4.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.6.3.1.1.1" class="ltx_p">Aispeech</span>
</span>
</td>
<td id="S4.T4.st1.4.6.3.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st1.4.6.3.3" class="ltx_td ltx_align_right">3,200</td>
<td id="S4.T4.st1.4.6.3.4" class="ltx_td ltx_align_right">2</td>
<td id="S4.T4.st1.4.6.3.5" class="ltx_td ltx_align_right">1,200</td>
</tr>
<tr id="S4.T4.st1.4.7.4" class="ltx_tr">
<td id="S4.T4.st1.4.7.4.1" class="ltx_td ltx_align_justify">
<span id="S4.T4.st1.4.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.7.4.1.1.1" class="ltx_p">HiFiGAN</span>
</span>
</td>
<td id="S4.T4.st1.4.7.4.2" class="ltx_td ltx_align_right">40</td>
<td id="S4.T4.st1.4.7.4.3" class="ltx_td ltx_align_right">3,200</td>
<td id="S4.T4.st1.4.7.4.4" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st1.4.7.4.5" class="ltx_td ltx_align_right">1,200</td>
</tr>
<tr id="S4.T4.st1.4.8.5" class="ltx_tr">
<td id="S4.T4.st1.4.8.5.1" class="ltx_td ltx_align_justify">
<span id="S4.T4.st1.4.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.8.5.1.1.1" class="ltx_p">WaveNet</span>
</span>
</td>
<td id="S4.T4.st1.4.8.5.2" class="ltx_td ltx_align_right">40</td>
<td id="S4.T4.st1.4.8.5.3" class="ltx_td ltx_align_right">3,200</td>
<td id="S4.T4.st1.4.8.5.4" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st1.4.8.5.5" class="ltx_td ltx_align_right">1,200</td>
</tr>
<tr id="S4.T4.st1.4.9.6" class="ltx_tr">
<td id="S4.T4.st1.4.9.6.1" class="ltx_td ltx_align_justify">
<span id="S4.T4.st1.4.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.9.6.1.1.1" class="ltx_p">World</span>
</span>
</td>
<td id="S4.T4.st1.4.9.6.2" class="ltx_td ltx_align_right">40</td>
<td id="S4.T4.st1.4.9.6.3" class="ltx_td ltx_align_right">3,200</td>
<td id="S4.T4.st1.4.9.6.4" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st1.4.9.6.5" class="ltx_td ltx_align_right">1,200</td>
</tr>
<tr id="S4.T4.st1.4.10.7" class="ltx_tr">
<td id="S4.T4.st1.4.10.7.1" class="ltx_td ltx_border_bb"></td>
<td id="S4.T4.st1.4.10.7.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T4.st1.4.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st1.4.10.7.2.1.1" class="ltx_p"><span id="S4.T4.st1.4.10.7.2.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T4.st1.4.10.7.3" class="ltx_td ltx_align_right ltx_border_bb">150</td>
<td id="S4.T4.st1.4.10.7.4" class="ltx_td ltx_align_right ltx_border_bb">19,200</td>
<td id="S4.T4.st1.4.10.7.5" class="ltx_td ltx_align_right ltx_border_bb">36</td>
<td id="S4.T4.st1.4.10.7.6" class="ltx_td ltx_align_right ltx_border_bb">7,200</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T4.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T4.st2.3.2" class="ltx_text" style="font-size:90%;">Testing sets.</span></figcaption>
<table id="S4.T4.st2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.st2.4.1.1" class="ltx_tr">
<td id="S4.T4.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T4.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Origin</span></td>
<td id="S4.T4.st2.4.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">Condition</span></td>
<td id="S4.T4.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.st2.4.1.1.3.1" class="ltx_text ltx_font_bold"># Spk</span></td>
<td id="S4.T4.st2.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.st2.4.1.1.4.1" class="ltx_text ltx_font_bold"># Utt</span></td>
</tr>
<tr id="S4.T4.st2.4.2.2" class="ltx_tr">
<td id="S4.T4.st2.4.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2" rowspan="3"><span id="S4.T4.st2.4.2.2.1.1" class="ltx_text ltx_font_bold">Real</span></td>
<td id="S4.T4.st2.4.2.2.2" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.2.2.2.1.1" class="ltx_p">AISHELL-3</span>
</span>
</td>
<td id="S4.T4.st2.4.2.2.3" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.2.2.4" class="ltx_td ltx_align_right ltx_border_t">34</td>
<td id="S4.T4.st2.4.2.2.5" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.3.3" class="ltx_tr">
<td id="S4.T4.st2.4.3.3.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.3.3.2" class="ltx_td ltx_align_right">44</td>
<td id="S4.T4.st2.4.3.3.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.4.4" class="ltx_tr">
<td id="S4.T4.st2.4.4.4.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.4.4.2" class="ltx_td ltx_align_right">20</td>
<td id="S4.T4.st2.4.4.4.3" class="ltx_td ltx_align_right">2,999</td>
</tr>
<tr id="S4.T4.st2.4.5.5" class="ltx_tr">
<td id="S4.T4.st2.4.5.5.1" class="ltx_td"></td>
<td id="S4.T4.st2.4.5.5.2" class="ltx_td"></td>
<td id="S4.T4.st2.4.5.5.3" class="ltx_td ltx_align_justify">
<span id="S4.T4.st2.4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.5.5.3.1.1" class="ltx_p"><span id="S4.T4.st2.4.5.5.3.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
<td id="S4.T4.st2.4.5.5.4" class="ltx_td"></td>
<td id="S4.T4.st2.4.5.5.5" class="ltx_td ltx_align_right">98</td>
<td id="S4.T4.st2.4.5.5.6" class="ltx_td ltx_align_right">10,507</td>
</tr>
<tr id="S4.T4.st2.4.6.6" class="ltx_tr">
<td id="S4.T4.st2.4.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="24"><span id="S4.T4.st2.4.6.6.1.1" class="ltx_text ltx_font_bold">Fake</span></td>
<td id="S4.T4.st2.4.6.6.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="18"><span id="S4.T4.st2.4.6.6.2.1" class="ltx_text ltx_font_bold">Known</span></td>
<td id="S4.T4.st2.4.6.6.3" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.6.6.3.1.1" class="ltx_p">Aliyun</span>
</span>
</td>
<td id="S4.T4.st2.4.6.6.4" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.6.6.5" class="ltx_td ltx_align_right ltx_border_t">4</td>
<td id="S4.T4.st2.4.6.6.6" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.7.7" class="ltx_tr">
<td id="S4.T4.st2.4.7.7.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.7.7.2" class="ltx_td ltx_align_right">13</td>
<td id="S4.T4.st2.4.7.7.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.8.8" class="ltx_tr">
<td id="S4.T4.st2.4.8.8.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.8.8.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st2.4.8.8.3" class="ltx_td ltx_align_right">2,004</td>
</tr>
<tr id="S4.T4.st2.4.9.9" class="ltx_tr">
<td id="S4.T4.st2.4.9.9.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.9.9.1.1.1" class="ltx_p">DataBaker</span>
</span>
</td>
<td id="S4.T4.st2.4.9.9.2" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.9.9.3" class="ltx_td ltx_align_right ltx_border_t">4</td>
<td id="S4.T4.st2.4.9.9.4" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.10.10" class="ltx_tr">
<td id="S4.T4.st2.4.10.10.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.10.10.2" class="ltx_td ltx_align_right">4</td>
<td id="S4.T4.st2.4.10.10.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.11.11" class="ltx_tr">
<td id="S4.T4.st2.4.11.11.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.11.11.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st2.4.11.11.3" class="ltx_td ltx_align_right">2,966</td>
</tr>
<tr id="S4.T4.st2.4.12.12" class="ltx_tr">
<td id="S4.T4.st2.4.12.12.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.12.12.1.1.1" class="ltx_p">Aispeech</span>
</span>
</td>
<td id="S4.T4.st2.4.12.12.2" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.12.12.3" class="ltx_td ltx_align_right ltx_border_t">4</td>
<td id="S4.T4.st2.4.12.12.4" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.13.13" class="ltx_tr">
<td id="S4.T4.st2.4.13.13.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.13.13.2" class="ltx_td ltx_align_right">6</td>
<td id="S4.T4.st2.4.13.13.3" class="ltx_td ltx_align_right">1,822</td>
</tr>
<tr id="S4.T4.st2.4.14.14" class="ltx_tr">
<td id="S4.T4.st2.4.14.14.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.14.14.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st2.4.14.14.3" class="ltx_td ltx_align_right">1,339</td>
</tr>
<tr id="S4.T4.st2.4.15.15" class="ltx_tr">
<td id="S4.T4.st2.4.15.15.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.15.15.1.1.1" class="ltx_p">HiFiGAN</span>
</span>
</td>
<td id="S4.T4.st2.4.15.15.2" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.15.15.3" class="ltx_td ltx_align_right ltx_border_t">34</td>
<td id="S4.T4.st2.4.15.15.4" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.16.16" class="ltx_tr">
<td id="S4.T4.st2.4.16.16.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.16.16.2" class="ltx_td ltx_align_right">43</td>
<td id="S4.T4.st2.4.16.16.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.17.17" class="ltx_tr">
<td id="S4.T4.st2.4.17.17.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.17.17.2" class="ltx_td ltx_align_right">41</td>
<td id="S4.T4.st2.4.17.17.3" class="ltx_td ltx_align_right">2,953</td>
</tr>
<tr id="S4.T4.st2.4.18.18" class="ltx_tr">
<td id="S4.T4.st2.4.18.18.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.18.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.18.18.1.1.1" class="ltx_p">WaveNet</span>
</span>
</td>
<td id="S4.T4.st2.4.18.18.2" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.18.18.3" class="ltx_td ltx_align_right ltx_border_t">34</td>
<td id="S4.T4.st2.4.18.18.4" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.19.19" class="ltx_tr">
<td id="S4.T4.st2.4.19.19.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.19.19.2" class="ltx_td ltx_align_right">47</td>
<td id="S4.T4.st2.4.19.19.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.20.20" class="ltx_tr">
<td id="S4.T4.st2.4.20.20.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.20.20.2" class="ltx_td ltx_align_right">43</td>
<td id="S4.T4.st2.4.20.20.3" class="ltx_td ltx_align_right">2,883</td>
</tr>
<tr id="S4.T4.st2.4.21.21" class="ltx_tr">
<td id="S4.T4.st2.4.21.21.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.21.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.21.21.1.1.1" class="ltx_p">World</span>
</span>
</td>
<td id="S4.T4.st2.4.21.21.2" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.21.21.3" class="ltx_td ltx_align_right ltx_border_t">34</td>
<td id="S4.T4.st2.4.21.21.4" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.22.22" class="ltx_tr">
<td id="S4.T4.st2.4.22.22.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.22.22.2" class="ltx_td ltx_align_right">50</td>
<td id="S4.T4.st2.4.22.22.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.23.23" class="ltx_tr">
<td id="S4.T4.st2.4.23.23.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.23.23.2" class="ltx_td ltx_align_right">30</td>
<td id="S4.T4.st2.4.23.23.3" class="ltx_td ltx_align_right">2,999</td>
</tr>
<tr id="S4.T4.st2.4.24.24" class="ltx_tr">
<td id="S4.T4.st2.4.24.24.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.24.24.1.1" class="ltx_text ltx_font_bold">Unknown</span></td>
<td id="S4.T4.st2.4.24.24.2" class="ltx_td ltx_align_justify ltx_border_t" rowspan="3">
<span id="S4.T4.st2.4.24.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.st2.4.24.24.2.1.1" class="ltx_p">Baidu</span>
</span>
</td>
<td id="S4.T4.st2.4.24.24.3" class="ltx_td ltx_align_left ltx_border_t">Clean</td>
<td id="S4.T4.st2.4.24.24.4" class="ltx_td ltx_align_right ltx_border_t">4</td>
<td id="S4.T4.st2.4.24.24.5" class="ltx_td ltx_align_right ltx_border_t">4,008</td>
</tr>
<tr id="S4.T4.st2.4.25.25" class="ltx_tr">
<td id="S4.T4.st2.4.25.25.1" class="ltx_td ltx_align_left">Noisy</td>
<td id="S4.T4.st2.4.25.25.2" class="ltx_td ltx_align_right">5</td>
<td id="S4.T4.st2.4.25.25.3" class="ltx_td ltx_align_right">3,500</td>
</tr>
<tr id="S4.T4.st2.4.26.26" class="ltx_tr">
<td id="S4.T4.st2.4.26.26.1" class="ltx_td ltx_align_left">Compressed</td>
<td id="S4.T4.st2.4.26.26.2" class="ltx_td ltx_align_right">10</td>
<td id="S4.T4.st2.4.26.26.3" class="ltx_td ltx_align_right">2,961</td>
</tr>
<tr id="S4.T4.st2.4.27.27" class="ltx_tr">
<td id="S4.T4.st2.4.27.27.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" colspan="3"><span id="S4.T4.st2.4.27.27.1.1" class="ltx_text ltx_font_bold">Total (known + unknown)</span></td>
<td id="S4.T4.st2.4.27.27.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">440</td>
<td id="S4.T4.st2.4.27.27.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">68,983</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Track 1.2: Detection task (FG-D)</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The dataset used in the FG-D sub-track contains both real and fake speech, as shown in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, with a sampling frequency of 16 kHz. Note that, while the speech samples are randomly selected for the two rounds of testing, there is no speaker overlap between the two rounds.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Training and development sets</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">In the training and development sets, only the AISHELL-3 dataset is used for genuine speech, totalling 60 speakers and 3,012 utterances for training and 60 speakers and 2,307 utterances for development (see Table <a href="#S4.T2" title="TABLE II ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>(a)). For fake audio, we use samples generated from the following methods:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">HiFiGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>:</span> A GAN-powered model that generates high-fidelity audio from mel-spectrograms.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LPCNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>:</span> A WaveRNN-based vocoder combining linear prediction coding and recurrent neural networks to generate high-quality audio.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multiband MelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>:</span> A vocoder model based on MelGAN with larger receptive fields and multi-band discriminators.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">StyleMelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>:</span> A vocoder model based on MelGAN with temporal adaptive normalization and multiple random-window discriminators.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Parallel WaveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>:</span> A GAN-based vocoder incorporating non-autoregressive WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for parallel and efficient waveform generation.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">World <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>:</span> A traditional vocoder system using spectral analysis to decompose the audio waveform into three main components: fundamental frequency (F0), spectral envelope, and non-periodic component.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Testing set</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">For the testing set of FG-D dataset, we include audio samples from a wider range of sources for both real and fake audio (see Table <a href="#S4.T2" title="TABLE II ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>(b)). The genuine audio portion comes from the following sources:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">AISHELL-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>:</span> A comprehensive and high-fidelity multi-speaker Mandarin speech corpus. We selected a set of 50 distinct speakers from this dataset, resulting in a combined recording duration surpassing 3.5 hours.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">AISHELL-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>:</span> A Mandarin speech corpus containing recordings from three different recording devices: high-fidelity microphones, Android smartphones, and iOS smartphones. We selected a total of 200 different speakers from this dataset, resulting in a combined recording duration exceeding 12 hours.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">THCHS-30 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>:</span> This dataset contains Chinese speech recordings that were recorded in a quiet office setting. We selected voice recordings from 30 distinct speakers, resulting in a cumulative duration of over 8 hours.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">HI-MIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>:</span> Originally a dataset for speaker verification containing recordings of 340 people in rooms designed for the far-field scenario. We have chosen a total of 100 speakers, resulting in a combined recording duration exceeding 11.5 hours.</p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p"><span id="S4.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Mobvoi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>:</span> The MobvoiHotwords corpus consists of a collection of wake-up words collected from a commercial smart speaker by Mobvoi. It includes keyword and non-keyword utterances. We selected 100 different speakers from among them, resulting in a total duration exceeding 24 hours.</p>
</div>
</li>
<li id="S4.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i6.p1" class="ltx_para">
<p id="S4.I2.i6.p1.1" class="ltx_p"><span id="S4.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">AliMeeting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>:</span> The AliMeeting dataset is a multi-speaker Mandarin speech corpus collected from real meetings, encompassing far-field audio captured using an 8-channel microphone array, in addition to near-field audio captured using individual participants’ headset microphones. We have chosen 50 speakers from among them, resulting in a total duration exceeding 23 hours.</p>
</div>
</li>
<li id="S4.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i7.p1" class="ltx_para">
<p id="S4.I2.i7.p1.1" class="ltx_p"><span id="S4.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">ESD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>:</span> Contains speech by 10 native Chinese speakers, with each speaker contributing 350 utterances, covering 5 emotion categories, namely <em id="S4.I2.i7.p1.1.2" class="ltx_emph ltx_font_italic">Neutral</em>, <em id="S4.I2.i7.p1.1.3" class="ltx_emph ltx_font_italic">Happy</em>, <em id="S4.I2.i7.p1.1.4" class="ltx_emph ltx_font_italic">Angry</em>, <em id="S4.I2.i7.p1.1.5" class="ltx_emph ltx_font_italic">Sad</em>, and <em id="S4.I2.i7.p1.1.6" class="ltx_emph ltx_font_italic">Surprise</em>. The average utterance and word duration are 3.22 s and 0.28 s respectively. All data is recorded in a typical indoor environment with an SNR of above 20 dB.</p>
</div>
</li>
<li id="S4.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i8.p1" class="ltx_para">
<p id="S4.I2.i8.p1.1" class="ltx_p"><span id="S4.I2.i8.p1.1.1" class="ltx_text ltx_font_bold">Phone recording:</span> We collected utterances from volunteers in ordinary day-to-day environments. These recordings were made using cellphone microphones and include 100 speakers, with a total duration exceeding 12 hours.</p>
</div>
</li>
</ul>
<p id="S4.SS2.SSS2.p1.2" class="ltx_p">The fake audio portion of the dataset include audio samples generated from the same sources as the training and development sets, as well as several additional sources, roughly divided into three categories: TTS and VC models, external tools, and challenge participants. The sources include:</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p"><span id="S4.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">WaveRNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>:</span> A single-layer recurrent neural network that predicts 16-bit audio waveforms, powered by gated recurrent units.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p"><span id="S4.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">VITS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>:</span> An end-to-end TTS model based on conditional variational autoencoder with adversarial training.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p"><span id="S4.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Grad-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>:</span> A diffusion-based generative model that can generate high-fidelity speech audio. The spectrogram generated by Grad-TTS is converted to waveform using HiFiGAN.</p>
</div>
</li>
<li id="S4.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i4.p1" class="ltx_para">
<p id="S4.I3.i4.p1.1" class="ltx_p"><span id="S4.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Speech Edit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>:</span> A speech editing model employing a context-aware mask prediction network that can edit speech in the time domain.</p>
</div>
</li>
<li id="S4.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i5.p1" class="ltx_para">
<p id="S4.I3.i5.p1.1" class="ltx_p"><span id="S4.I3.i5.p1.1.1" class="ltx_text ltx_font_bold">Aliyun<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_medium">4</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.aliyun.com/nls/tts</span></span></span></span>:</span> An industry-leading voice synthesis solution from Alibaba, powered by the newest deep-learning technologies.</p>
</div>
</li>
<li id="S4.I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i6.p1" class="ltx_para">
<p id="S4.I3.i6.p1.1" class="ltx_p"><span id="S4.I3.i6.p1.1.1" class="ltx_text ltx_font_bold">DeepSound<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_medium">5</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.deepsound.cn</span></span></span></span>:</span> A TTS platform developed by DeepSound, providing a general speech synthesis solution that transforms text into expressive speech.</p>
</div>
</li>
<li id="S4.I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i7.p1" class="ltx_para">
<p id="S4.I3.i7.p1.1" class="ltx_p"><span id="S4.I3.i7.p1.1.1" class="ltx_text ltx_font_bold">iFlytek<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_medium">6</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://global.xfyun.cn/products/text-to-speech</span></span></span></span>:</span> A TTS platform developed by iFlytek, powered by the latest deep learning technologies.</p>
</div>
</li>
<li id="S4.I3.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i8.p1" class="ltx_para">
<p id="S4.I3.i8.p1.1" class="ltx_p"><span id="S4.I3.i8.p1.1.1" class="ltx_text ltx_font_bold">Mobvoi<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span id="footnote7.1.1.1" class="ltx_text ltx_font_medium">7</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.chumenwenwen.com/</span></span></span></span>:</span> A TTS platform developed by Mobvoi and powered by MeetVoice, an end-to-end speech synthesis engine that supports emotion synthesis.</p>
</div>
</li>
<li id="S4.I3.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i9.p1" class="ltx_para">
<p id="S4.I3.i9.p1.1" class="ltx_p"><span id="S4.I3.i9.p1.1.1" class="ltx_text ltx_font_bold">Baidu<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note"><span id="footnote8.1.1.1" class="ltx_text ltx_font_medium">8</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.baidu.com/tech/speech/tts</span></span></span></span>:</span> A deep learning-based TTS system developed by Baidu, powered by the latest deep learning technologies.</p>
</div>
</li>
<li id="S4.I3.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i10.p1" class="ltx_para">
<p id="S4.I3.i10.p1.1" class="ltx_p"><span id="S4.I3.i10.p1.1.1" class="ltx_text ltx_font_bold">AISpeech<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span id="footnote9.1.1.1" class="ltx_text ltx_font_medium">9</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://beta.duiopen.com/openSource/technology/tts</span></span></span></span>:</span> A leading voice synthesis solution in the Chinese industry. able to emulate various types of voices through its TTS and VC platform.</p>
</div>
</li>
<li id="S4.I3.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i11.p1" class="ltx_para">
<p id="S4.I3.i11.p1.1" class="ltx_p"><span id="S4.I3.i11.p1.1.1" class="ltx_text ltx_font_bold">DataBaker<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_medium">10</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.data-baker.com/specs/compose/online</span></span></span></span>:</span> Uses state-of-the-art TTS and VC platform based on the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</li>
<li id="S4.I3.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i12.p1" class="ltx_para">
<p id="S4.I3.i12.p1.1" class="ltx_p"><span id="S4.I3.i12.p1.1.1" class="ltx_text ltx_font_bold">Sogou<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note"><span id="footnote11.1.1.1" class="ltx_text ltx_font_medium">11</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.sogou.com/product/tts</span></span></span></span>:</span> A TTS and VC platform developed by Sogou based on a sequence-to-sequence acoustic model.</p>
</div>
</li>
<li id="S4.I3.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i13.p1" class="ltx_para">
<p id="S4.I3.i13.p1.1" class="ltx_p"><span id="S4.I3.i13.p1.1.1" class="ltx_text ltx_font_bold">Tencent<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span id="footnote12.1.1.1" class="ltx_text ltx_font_medium">12</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">hhttps://www.tencentcloud.com/products/tts</span></span></span></span>:</span> A TTS and VC platform developed by Tencent providing high quality speech synthesis services.</p>
</div>
</li>
<li id="S4.I3.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i14.p1" class="ltx_para">
<p id="S4.I3.i14.p1.1" class="ltx_p"><span id="S4.I3.i14.p1.1.1" class="ltx_text ltx_font_bold">Sohu<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note"><span id="footnote13.1.1.1" class="ltx_text ltx_font_medium">13</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.sohu.com/</span></span></span></span>:</span> A TTS and VC platform developed by Sohu.</p>
</div>
</li>
<li id="S4.I3.i15" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i15.p1" class="ltx_para">
<p id="S4.I3.i15.p1.1" class="ltx_p"><span id="S4.I3.i15.p1.1.1" class="ltx_text ltx_font_bold">Blizzard:</span> The fake audio samples generated by the participants in the Blizzard Challenge 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, with around 250 samples from each team, resulting in 4,000 utterances in total.</p>
</div>
</li>
<li id="S4.I3.i16" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i16.p1" class="ltx_para">
<p id="S4.I3.i16.p1.1" class="ltx_p"><span id="S4.I3.i16.p1.1.1" class="ltx_text ltx_font_bold">FG-G:</span> The fake audio samples generated by the participants in the FG-G sub-track of the ADD 2023 challenge, with 998 samples per team, totalling 11,976 utterances. Note that the two rounds of the FG-G sub-track are disjunct, <em id="S4.I3.i16.p1.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, the samples generated in round 1 are not included in round 2.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Track 2: Manipulation region location (RL)</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The RL track is designed to evaluate the ability of participants to detect and locate the manipulation regions within partially fake audio, therefore the dataset used in this track (see Table <a href="#S4.T3" title="TABLE III ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) contains both completely real audio samples as well as samples spliced with fake audio or real audio from a different recording of the same speaker. The spliced regions either contain a named entity or a word that is semantically an antonym of the original word. For simplicity, each edited sentence contains at most two spliced regions.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Training and development sets</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">The training and development sets (see Table <a href="#S4.T3" title="TABLE III ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>(a)) contain genuine audio sourced from the training and development sets of the AISHELL-3 dataset and partially fake audio sourced from the training and development sets of the HAD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Testing set</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">The testing set of Track 2 (see Table <a href="#S4.T3" title="TABLE III ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>(b)) contains 1,030 speakers and 20,000 utterances for genuine audio from various sources like AISHELL-1, AISHELL-3, THCHS-30, Mobvoi, AliMeeting and Phone recordings. The fake audio samples in the testing set are AISHELL-1 and AISHELL-2 samples spliced with either same-speaker recordings or fake audio generated with the Tacotron 2–LPCNet pipeline, with one or two spliced regions per sentence, as well as utterances processed by the Speech Edit model based on CampNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The fake audio samples were generated using the same speakers as the genuine audio samples. Genuine utterances are then spliced by replacing certain segments with fake audio or other recordings of the same speaker, following a similar procedure as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. These partially-fake samples total 30,000 utterances from 148 speakers.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Track 3: Deepfake algorithm recognition (AR)</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The objective of the AR track is to recognize the deepfake algorithm used to generate a given piece of audio; therefore its dataset contains genuine utterances as well as fake utterances generated by various deepfake algorithms and commercial TTS platforms (see Table <a href="#S4.T4" title="TABLE IV ‣ IV-A Track 1.1: Generation task (FG-G) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The dataset used in this track is based on the datasets presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>,
composed of genuine utterances from the AISHELL-3 dataset and fake utterances generated by various deepfake algorithms from the AISHELL-1 and AISHELL-3 text corpora. The deepfake algorithms used to generate the fake utterances include vocoders like HiFiGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, WaveNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and World <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, as well as commercial solutions from Aliyun<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.aliyun.com/nls/tts</span></span></span></span>, DataBaker<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.data-baker.com/specs/compose/online</span></span></span></span>, and AISpeech<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://beta.duiopen.com/openSource/technology/tts</span></span></span></span>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In the training and development sets, the genuine audio samples are sourced from AISHELL-3, while the fake audio samples are sourced from the aforementioned deepfake algorithms and commercial TTS platforms. The testing set, on the other hand, contains audio samples from the same sources as the training and development sets, but with different speakers. It also includes audio samples from one unknown deepfake algorithm not included in the above list:</p>
<ul id="S4.I4" class="ltx_itemize">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.1" class="ltx_p"><span id="S4.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Baidu<span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note"><span id="footnote17.1.1.1" class="ltx_text ltx_font_medium">17</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.baidu.com/tech/speech/tts</span></span></span></span>:</span> A deep learning-based TTS(TTS) system developed by Baidu, powered by the latest deep learning technologies.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Further augmentations were made to the testing set to emulate real-world scenarios. Initially, a series of high-quality clean audio samples were generated with each of the aforementioned TTS tools. To simulate diverse acoustic environments of the real world, various types and levels of background noise were added to clean audio samples, creating the “noisy” condition to test deepfake algorithm recognition under suboptimal conditions. Additionally, to reflect audio compression commonly found on social media, the Jinshi Video Assistant software<span id="footnote18" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.drmfab.cn/zhushou/</span></span></span></span> was used to compress clean audio, forming the “compressed” condition. These steps assess the algorithm’s adaptability to encoding changes.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">DSR (%) and methods of top-performing systems in Track 1.1 (FG-G) submissions. [“Aug.” = augmentation; “Rep.” = representation; “Arch.” = architecture; “AR” = auto-regressive]</span></figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1" class="ltx_tr">
<th id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Team</th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">DSR (<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S4.T5.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Data Aug.</th>
<th id="S4.T5.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Text Rep.</th>
<th id="S4.T5.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Audio Rep.</th>
<th id="S4.T5.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Output</th>
<th id="S4.T5.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Arch.</th>
<th id="S4.T5.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Vocoder</th>
<th id="S4.T5.1.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Duration</th>
<th id="S4.T5.1.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">Speaker Rep.</th>
<th id="S4.T5.1.1.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">AR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">A01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T5.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">44.97</th>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Noise + reverb</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Phoneme seq.</td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">Tacotron 2</td>
<td id="S4.T5.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">WaveRNN</td>
<td id="S4.T5.1.2.1.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.2.1.10" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">BiLSTM embedding</td>
<td id="S4.T5.1.2.1.11" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">AR</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<th id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">A02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<th id="S4.T5.1.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">43.63</th>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Phoneme seq.</td>
<td id="S4.T5.1.3.2.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.3.2.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.3.2.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">FastPitch</td>
<td id="S4.T5.1.3.2.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">HiFiGAN</td>
<td id="S4.T5.1.3.2.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">MFA</td>
<td id="S4.T5.1.3.2.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">One-hot embedding</td>
<td id="S4.T5.1.3.2.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">NAR</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<th id="S4.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">A03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</th>
<th id="S4.T5.1.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">41.48</th>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Context encoder</td>
<td id="S4.T5.1.4.3.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Linear spec</td>
<td id="S4.T5.1.4.3.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Waveform</td>
<td id="S4.T5.1.4.3.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Hier-TTS</td>
<td id="S4.T5.1.4.3.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.4.3.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.4.3.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">One-hot embedding</td>
<td id="S4.T5.1.4.3.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">AR</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<th id="S4.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">A05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<th id="S4.T5.1.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;">37.35</th>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Concatenation</td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">G2P+BERT</td>
<td id="S4.T5.1.5.4.5" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.5.4.6" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Mel spec</td>
<td id="S4.T5.1.5.4.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">FastSpeech 2</td>
<td id="S4.T5.1.5.4.8" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">HiFiGAN</td>
<td id="S4.T5.1.5.4.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">Kaldi</td>
<td id="S4.T5.1.5.4.10" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">One-hot embedding</td>
<td id="S4.T5.1.5.4.11" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">NAR</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<th id="S4.T5.1.6.5.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th id="S4.T5.1.6.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">of samples</td>
<td id="S4.T5.1.6.5.4" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S4.T5.1.6.5.5" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S4.T5.1.6.5.6" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S4.T5.1.6.5.7" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">+ Conformer</td>
<td id="S4.T5.1.6.5.8" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S4.T5.1.6.5.9" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">force-align</td>
<td id="S4.T5.1.6.5.10" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S4.T5.1.6.5.11" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<th id="S4.T5.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">A06 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<th id="S4.T5.1.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">30.69</th>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">Phoneme seq.</td>
<td id="S4.T5.1.7.6.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">Linear spec</td>
<td id="S4.T5.1.7.6.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">Waveform</td>
<td id="S4.T5.1.7.6.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">VITS</td>
<td id="S4.T5.1.7.6.8" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td id="S4.T5.1.7.6.9" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">MAS</td>
<td id="S4.T5.1.7.6.10" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">One-hot embedding</td>
<td id="S4.T5.1.7.6.11" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.3pt;padding-right:4.3pt;">NAR</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">TABLE VI</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;">WEER (%) and methods of top-performing systems and baselines in Track 1.2 (FG-D). [“Aug.” = augmentation]</span></figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1" class="ltx_tr">
<th id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Team</th>
<th id="S4.T6.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">WEER (<math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mo stretchy="false" id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Data Aug.</td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">Acoustic Features</td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_align_left ltx_border_tt">Back-end Classifiers</td>
<td id="S4.T6.1.1.6" class="ltx_td ltx_align_left ltx_border_tt">Model Fusion</td>
</tr>
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">B01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<th id="S4.T6.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">12.45</th>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Noise; RawBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>; copy synthesis</td>
<td id="S4.T6.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">Wav2Vec 2.0</td>
<td id="S4.T6.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">AASIST (-sinc conv)</td>
<td id="S4.T6.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<th id="S4.T6.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">B02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</th>
<th id="S4.T6.1.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">17.93</th>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_left">Noise</td>
<td id="S4.T6.1.3.2.4" class="ltx_td ltx_align_left">Wav2Vec 2.0</td>
<td id="S4.T6.1.3.2.5" class="ltx_td ltx_align_left">SENet; LCNN; AASIST</td>
<td id="S4.T6.1.3.2.6" class="ltx_td ltx_align_left">Weighted average</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<th id="S4.T6.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">B03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</th>
<th id="S4.T6.1.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">22.13</th>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_left">Noise</td>
<td id="S4.T6.1.4.3.4" class="ltx_td ltx_align_left">CQT spectrogram</td>
<td id="S4.T6.1.4.3.5" class="ltx_td ltx_align_left">LCNN; AASIST</td>
<td id="S4.T6.1.4.3.6" class="ltx_td ltx_align_left">Average</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<th id="S4.T6.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">B04 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<th id="S4.T6.1.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">22.45</th>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.1.5.4.4" class="ltx_td ltx_align_left">Wav2Vec 2.0; WavLM</td>
<td id="S4.T6.1.5.4.5" class="ltx_td ltx_align_left">VAE</td>
<td id="S4.T6.1.5.4.6" class="ltx_td ltx_align_left">Average</td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<th id="S4.T6.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">B05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</th>
<th id="S4.T6.1.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">23.17</th>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_left">Noise</td>
<td id="S4.T6.1.6.5.4" class="ltx_td ltx_align_left">Wav2Vec 2.0</td>
<td id="S4.T6.1.6.5.5" class="ltx_td ltx_align_left">LCNN</td>
<td id="S4.T6.1.6.5.6" class="ltx_td"></td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<th id="S4.T6.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">S01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S4.T6.1.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">53.04</th>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S4.T6.1.7.6.4" class="ltx_td ltx_align_left ltx_border_t">LFCC</td>
<td id="S4.T6.1.7.6.5" class="ltx_td ltx_align_left ltx_border_t">GMM</td>
<td id="S4.T6.1.7.6.6" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S4.T6.1.8.7" class="ltx_tr">
<th id="S4.T6.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">S02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S4.T6.1.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">66.72</th>
<td id="S4.T6.1.8.7.3" class="ltx_td ltx_align_left">-</td>
<td id="S4.T6.1.8.7.4" class="ltx_td ltx_align_left">LFCC</td>
<td id="S4.T6.1.8.7.5" class="ltx_td ltx_align_left">LCNN</td>
<td id="S4.T6.1.8.7.6" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T6.1.9.8" class="ltx_tr">
<th id="S4.T6.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">S03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S4.T6.1.9.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">30.35</th>
<td id="S4.T6.1.9.8.3" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S4.T6.1.9.8.4" class="ltx_td ltx_align_left ltx_border_bb">Wav2Vec 2.0</td>
<td id="S4.T6.1.9.8.5" class="ltx_td ltx_align_left ltx_border_bb">LCNN</td>
<td id="S4.T6.1.9.8.6" class="ltx_td ltx_align_left ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Technical Analysis</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We analyze the technical details of the top-5 performing teams of each track in this section. In the tables are presented not only the performances of top-performing participating teams, but also the results of the baseline systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (denoted in the form of <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">S#</em>, where # is the baseline number).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Track 1.1: Generation task (FG-G)</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Although the use of VC was allowed, most teams opted for a TTS approach. The technical details of the top-5 participating teams are summarized in Table <a href="#S4.T5" title="TABLE V ‣ IV-D Track 3: Deepfake algorithm recognition (AR) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Given the setup of the track, where the target speaker ID is selected from the speakers within the AISHELL-3 corpus, most teams opted to use one-hot embeddings for speaker representation, instead of style embeddings. In the analysis of top-performing systems, we noticed the following:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data augmentation:</span> With the policy prohibiting usage of external speech corpora, data augmentation can be worthwhile. Interestingly, among the participating teams of the FG-G task, the use of data augmentation is not widespread, with only A01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and A05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> reporting any form of data augmentation. Of those two teams, A01 uses reverberation and additive noises to augment their dataset, while A05 concatenates and re-splices training data belonging to the same speaker ID.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Text representation:</span> Most teams opted for phoneme sequence as the text representation, which is a common choice for TTS systems, as it is seen to be closer to the phonetic representation of speech, and thus more suitable for speech synthesis, as well as being more robust to spelling errors and out-of-vocabulary words. A notable exception to this trend is Team A03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which used a context encoder to encode the input text into a fixed-length vector. Another deviation from this is Team A05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, who used a grapheme-to-phoneme model to convert the input text into phoneme sequence, and used BERT for disambiguating homographs.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Architectures:</span> The use of non-autoregressive models like FastSpeech 2 is common in this track, which allows for variations in generation pipelines in the form of duration models. Aside from using models that are non-autoregressive, most teams adopt approaches with intermediate audio representations like Mel spectrogram, which are then passed through vocoder models to generate the final audio waveform. It is worth noting, however, that A06 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> used the VITS architecture, which is fully end-to-end, with its own alignment module of Monotonic Alignment Search, that allows for the generation of audio waveform directly from phoneme sequence by using a variational autoencoder-like encoder-decoder architecture to directly decode into audio waveform.</p>
</div>
</li>
</ol>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T7.10.2.1" class="ltx_text" style="font-size:90%;">TABLE VII</span>: </span><span id="S5.T7.2.1" class="ltx_text" style="font-size:90%;">Scores (%), sentence accuracies <math id="S5.T7.2.1.m1.1" class="ltx_Math" alttext="A_{s}" display="inline"><semantics id="S5.T7.2.1.m1.1b"><msub id="S5.T7.2.1.m1.1.1" xref="S5.T7.2.1.m1.1.1.cmml"><mi id="S5.T7.2.1.m1.1.1.2" xref="S5.T7.2.1.m1.1.1.2.cmml">A</mi><mi id="S5.T7.2.1.m1.1.1.3" xref="S5.T7.2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T7.2.1.m1.1c"><apply id="S5.T7.2.1.m1.1.1.cmml" xref="S5.T7.2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.2.1.m1.1.1.1.cmml" xref="S5.T7.2.1.m1.1.1">subscript</csymbol><ci id="S5.T7.2.1.m1.1.1.2.cmml" xref="S5.T7.2.1.m1.1.1.2">𝐴</ci><ci id="S5.T7.2.1.m1.1.1.3.cmml" xref="S5.T7.2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.1.m1.1d">A_{s}</annotation></semantics></math> (%), frame-wise F<sub id="S5.T7.2.1.1" class="ltx_sub">1</sub> scores (%) and methods of top-performing systems and baseline in Track 2 (RL). [“Aug.” = augmentation]</span></figcaption>
<table id="S5.T7.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.7.5" class="ltx_tr">
<th id="S5.T7.7.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Team</th>
<th id="S5.T7.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Score (<math id="S5.T7.3.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T7.3.1.1.m1.1a"><mo stretchy="false" id="S5.T7.3.1.1.m1.1.1" xref="S5.T7.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T7.3.1.1.m1.1b"><ci id="S5.T7.3.1.1.m1.1.1.cmml" xref="S5.T7.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.1.1.m1.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S5.T7.5.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<math id="S5.T7.4.2.2.m1.1" class="ltx_Math" alttext="A_{s}" display="inline"><semantics id="S5.T7.4.2.2.m1.1a"><msub id="S5.T7.4.2.2.m1.1.1" xref="S5.T7.4.2.2.m1.1.1.cmml"><mi id="S5.T7.4.2.2.m1.1.1.2" xref="S5.T7.4.2.2.m1.1.1.2.cmml">A</mi><mi id="S5.T7.4.2.2.m1.1.1.3" xref="S5.T7.4.2.2.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T7.4.2.2.m1.1b"><apply id="S5.T7.4.2.2.m1.1.1.cmml" xref="S5.T7.4.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.4.2.2.m1.1.1.1.cmml" xref="S5.T7.4.2.2.m1.1.1">subscript</csymbol><ci id="S5.T7.4.2.2.m1.1.1.2.cmml" xref="S5.T7.4.2.2.m1.1.1.2">𝐴</ci><ci id="S5.T7.4.2.2.m1.1.1.3.cmml" xref="S5.T7.4.2.2.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.2.2.m1.1c">A_{s}</annotation></semantics></math> (<math id="S5.T7.5.3.3.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T7.5.3.3.m2.1a"><mo stretchy="false" id="S5.T7.5.3.3.m2.1.1" xref="S5.T7.5.3.3.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T7.5.3.3.m2.1b"><ci id="S5.T7.5.3.3.m2.1.1.cmml" xref="S5.T7.5.3.3.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.5.3.3.m2.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S5.T7.7.5.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<math id="S5.T7.6.4.4.m1.1" class="ltx_Math" alttext="F_{1f}" display="inline"><semantics id="S5.T7.6.4.4.m1.1a"><msub id="S5.T7.6.4.4.m1.1.1" xref="S5.T7.6.4.4.m1.1.1.cmml"><mi id="S5.T7.6.4.4.m1.1.1.2" xref="S5.T7.6.4.4.m1.1.1.2.cmml">F</mi><mrow id="S5.T7.6.4.4.m1.1.1.3" xref="S5.T7.6.4.4.m1.1.1.3.cmml"><mn id="S5.T7.6.4.4.m1.1.1.3.2" xref="S5.T7.6.4.4.m1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.T7.6.4.4.m1.1.1.3.1" xref="S5.T7.6.4.4.m1.1.1.3.1.cmml">​</mo><mi id="S5.T7.6.4.4.m1.1.1.3.3" xref="S5.T7.6.4.4.m1.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T7.6.4.4.m1.1b"><apply id="S5.T7.6.4.4.m1.1.1.cmml" xref="S5.T7.6.4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T7.6.4.4.m1.1.1.1.cmml" xref="S5.T7.6.4.4.m1.1.1">subscript</csymbol><ci id="S5.T7.6.4.4.m1.1.1.2.cmml" xref="S5.T7.6.4.4.m1.1.1.2">𝐹</ci><apply id="S5.T7.6.4.4.m1.1.1.3.cmml" xref="S5.T7.6.4.4.m1.1.1.3"><times id="S5.T7.6.4.4.m1.1.1.3.1.cmml" xref="S5.T7.6.4.4.m1.1.1.3.1"></times><cn type="integer" id="S5.T7.6.4.4.m1.1.1.3.2.cmml" xref="S5.T7.6.4.4.m1.1.1.3.2">1</cn><ci id="S5.T7.6.4.4.m1.1.1.3.3.cmml" xref="S5.T7.6.4.4.m1.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.6.4.4.m1.1c">F_{1f}</annotation></semantics></math> (<math id="S5.T7.7.5.5.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T7.7.5.5.m2.1a"><mo stretchy="false" id="S5.T7.7.5.5.m2.1.1" xref="S5.T7.7.5.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T7.7.5.5.m2.1b"><ci id="S5.T7.7.5.5.m2.1.1.cmml" xref="S5.T7.7.5.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.7.5.5.m2.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S5.T7.7.5.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Data Aug.</th>
<th id="S5.T7.7.5.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Acoustic Features</th>
<th id="S5.T7.7.5.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Back-end Classifiers</th>
<th id="S5.T7.7.5.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model Fusion</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.7.6.1" class="ltx_tr">
<td id="S5.T7.7.6.1.1" class="ltx_td ltx_align_center ltx_border_t">C01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S5.T7.7.6.1.2" class="ltx_td ltx_align_right ltx_border_t">67.13</td>
<td id="S5.T7.7.6.1.3" class="ltx_td ltx_align_right ltx_border_t">82.23</td>
<td id="S5.T7.7.6.1.4" class="ltx_td ltx_align_right ltx_border_t">60.66</td>
<td id="S5.T7.7.6.1.5" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S5.T7.7.6.1.6" class="ltx_td ltx_align_left ltx_border_t">Wav2Vec 2.0</td>
<td id="S5.T7.7.6.1.7" class="ltx_td ltx_align_left ltx_border_t">ResNet–Transformer–LSTM</td>
<td id="S5.T7.7.6.1.8" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T7.7.7.2" class="ltx_tr">
<td id="S5.T7.7.7.2.1" class="ltx_td ltx_align_center">C02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S5.T7.7.7.2.2" class="ltx_td ltx_align_right">62.49</td>
<td id="S5.T7.7.7.2.3" class="ltx_td ltx_align_right">80.91</td>
<td id="S5.T7.7.7.2.4" class="ltx_td ltx_align_right">54.60</td>
<td id="S5.T7.7.7.2.5" class="ltx_td ltx_align_left">Noise; re-splicing</td>
<td id="S5.T7.7.7.2.6" class="ltx_td ltx_align_left">Spectrogram</td>
<td id="S5.T7.7.7.2.7" class="ltx_td ltx_align_left">RCNN-BLSTM</td>
<td id="S5.T7.7.7.2.8" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S5.T7.7.8.3" class="ltx_tr">
<td id="S5.T7.7.8.3.1" class="ltx_td ltx_align_center">C03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S5.T7.7.8.3.2" class="ltx_td ltx_align_right">62.42</td>
<td id="S5.T7.7.8.3.3" class="ltx_td ltx_align_right">79.56</td>
<td id="S5.T7.7.8.3.4" class="ltx_td ltx_align_right">54.50</td>
<td id="S5.T7.7.8.3.5" class="ltx_td ltx_align_left">Noise; reverb</td>
<td id="S5.T7.7.8.3.6" class="ltx_td ltx_align_left">Log-mel spectrogram</td>
<td id="S5.T7.7.8.3.7" class="ltx_td ltx_align_left">RCNN</td>
<td id="S5.T7.7.8.3.8" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S5.T7.7.9.4" class="ltx_tr">
<td id="S5.T7.7.9.4.1" class="ltx_td ltx_align_center">C04 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S5.T7.7.9.4.2" class="ltx_td ltx_align_right">59.62</td>
<td id="S5.T7.7.9.4.3" class="ltx_td ltx_align_right">78.16</td>
<td id="S5.T7.7.9.4.4" class="ltx_td ltx_align_right">51.67</td>
<td id="S5.T7.7.9.4.5" class="ltx_td ltx_align_left">-</td>
<td id="S5.T7.7.9.4.6" class="ltx_td ltx_align_left">Wav2Vec 2.0</td>
<td id="S5.T7.7.9.4.7" class="ltx_td ltx_align_left">Transformer–BLSTM</td>
<td id="S5.T7.7.9.4.8" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S5.T7.7.10.5" class="ltx_tr">
<td id="S5.T7.7.10.5.1" class="ltx_td ltx_align_center">C05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T7.7.10.5.2" class="ltx_td ltx_align_right">59.12</td>
<td id="S5.T7.7.10.5.3" class="ltx_td ltx_align_right">74.52</td>
<td id="S5.T7.7.10.5.4" class="ltx_td ltx_align_right">52.53</td>
<td id="S5.T7.7.10.5.5" class="ltx_td ltx_align_left">Noise</td>
<td id="S5.T7.7.10.5.6" class="ltx_td ltx_align_left">Raw, Wav2Vec 2.0</td>
<td id="S5.T7.7.10.5.7" class="ltx_td ltx_align_left">AASIST, FC layer</td>
<td id="S5.T7.7.10.5.8" class="ltx_td ltx_align_left">Weighted frame-wise</td>
</tr>
<tr id="S5.T7.7.11.6" class="ltx_tr">
<td id="S5.T7.7.11.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">S04 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S5.T7.7.11.6.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">42.25</td>
<td id="S5.T7.7.11.6.3" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S5.T7.7.11.6.4" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S5.T7.7.11.6.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">-</td>
<td id="S5.T7.7.11.6.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">LFCC</td>
<td id="S5.T7.7.11.6.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">LCNN</td>
<td id="S5.T7.7.11.6.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">-</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T8.6.1.1" class="ltx_text" style="font-size:90%;">TABLE VIII</span>: </span><math id="S5.T8.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S5.T8.2.m1.1b"><msub id="S5.T8.2.m1.1.1" xref="S5.T8.2.m1.1.1.cmml"><mi mathsize="90%" id="S5.T8.2.m1.1.1.2" xref="S5.T8.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S5.T8.2.m1.1.1.3" xref="S5.T8.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T8.2.m1.1c"><apply id="S5.T8.2.m1.1.1.cmml" xref="S5.T8.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T8.2.m1.1.1.1.cmml" xref="S5.T8.2.m1.1.1">subscript</csymbol><ci id="S5.T8.2.m1.1.1.2.cmml" xref="S5.T8.2.m1.1.1.2">𝐹</ci><cn type="integer" id="S5.T8.2.m1.1.1.3.cmml" xref="S5.T8.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.2.m1.1d">F_{1}</annotation></semantics></math><span id="S5.T8.7.2" class="ltx_text" style="font-size:90%;"> (%) and methods of top-performing systems and baselines in Track 3 (AR). [“Aug.” = augmentation]</span></figcaption>
<table id="S5.T8.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T8.4.2" class="ltx_tr">
<th id="S5.T8.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">Team</th>
<th id="S5.T8.4.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">
<math id="S5.T8.3.1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S5.T8.3.1.1.m1.1a"><msub id="S5.T8.3.1.1.m1.1.1" xref="S5.T8.3.1.1.m1.1.1.cmml"><mi id="S5.T8.3.1.1.m1.1.1.2" xref="S5.T8.3.1.1.m1.1.1.2.cmml">F</mi><mn id="S5.T8.3.1.1.m1.1.1.3" xref="S5.T8.3.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T8.3.1.1.m1.1b"><apply id="S5.T8.3.1.1.m1.1.1.cmml" xref="S5.T8.3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T8.3.1.1.m1.1.1.1.cmml" xref="S5.T8.3.1.1.m1.1.1">subscript</csymbol><ci id="S5.T8.3.1.1.m1.1.1.2.cmml" xref="S5.T8.3.1.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S5.T8.3.1.1.m1.1.1.3.cmml" xref="S5.T8.3.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.3.1.1.m1.1c">F_{1}</annotation></semantics></math> (<math id="S5.T8.4.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T8.4.2.2.m2.1a"><mo stretchy="false" id="S5.T8.4.2.2.m2.1.1" xref="S5.T8.4.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T8.4.2.2.m2.1b"><ci id="S5.T8.4.2.2.m2.1.1.cmml" xref="S5.T8.4.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.4.2.2.m2.1c">\uparrow</annotation></semantics></math>)</th>
<td id="S5.T8.4.2.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">Data Aug.</td>
<td id="S5.T8.4.2.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">Acoustic Features</td>
<td id="S5.T8.4.2.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">Back-end Classifier</td>
<td id="S5.T8.4.2.7" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">Model Fusion</td>
<td id="S5.T8.4.2.8" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;">OSR Method</td>
</tr>
<tr id="S5.T8.4.3.1" class="ltx_tr">
<th id="S5.T8.4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">D01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</th>
<th id="S5.T8.4.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">89.63</th>
<td id="S5.T8.4.3.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">Noise, reverb, CutMix</td>
<td id="S5.T8.4.3.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">STFT, Wav2Vec 2.0</td>
<td id="S5.T8.4.3.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">SENet, LCNN-LSTM, TDNN</td>
<td id="S5.T8.4.3.1.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">Weighted average</td>
<td id="S5.T8.4.3.1.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">kNN</td>
</tr>
<tr id="S5.T8.4.4.2" class="ltx_tr">
<th id="S5.T8.4.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">D02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</th>
<th id="S5.T8.4.4.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">83.12</th>
<td id="S5.T8.4.4.2.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Noise, reverb</td>
<td id="S5.T8.4.4.2.4" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">log-mel filterbank;</td>
<td id="S5.T8.4.4.2.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">ResNet34SimAM-ASP, ResNet34-GSP,</td>
<td id="S5.T8.4.4.2.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Average (score)</td>
<td id="S5.T8.4.4.2.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">maximum</td>
</tr>
<tr id="S5.T8.4.5.3" class="ltx_tr">
<th id="S5.T8.4.5.3.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.5.3.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.5.3.3" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.5.3.4" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">log-spec</td>
<td id="S5.T8.4.5.3.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">ResNet34SE-ASP, ECAPA-TDNN-ASP,</td>
<td id="S5.T8.4.5.3.6" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.5.3.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">similarity</td>
</tr>
<tr id="S5.T8.4.6.4" class="ltx_tr">
<th id="S5.T8.4.6.4.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.6.4.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.6.4.3" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.6.4.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.6.4.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">LCNN, AASIST-SAP, wav2vec-ECAPA,</td>
<td id="S5.T8.4.6.4.6" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.6.4.7" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
</tr>
<tr id="S5.T8.4.7.5" class="ltx_tr">
<th id="S5.T8.4.7.5.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.7.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.7.5.3" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.7.5.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.7.5.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">wavlm-ECAPA</td>
<td id="S5.T8.4.7.5.6" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.7.5.7" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
</tr>
<tr id="S5.T8.4.8.6" class="ltx_tr">
<th id="S5.T8.4.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">D03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</th>
<th id="S5.T8.4.8.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">75.41</th>
<td id="S5.T8.4.8.6.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Noise, reverb, mixup</td>
<td id="S5.T8.4.8.6.4" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Wav2Vec 2.0</td>
<td id="S5.T8.4.8.6.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">ECAPA-TDNN</td>
<td id="S5.T8.4.8.6.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">-</td>
<td id="S5.T8.4.8.6.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">threshold</td>
</tr>
<tr id="S5.T8.4.9.7" class="ltx_tr">
<th id="S5.T8.4.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">D04 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</th>
<th id="S5.T8.4.9.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">73.55</th>
<td id="S5.T8.4.9.7.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Noise, random sampling,</td>
<td id="S5.T8.4.9.7.4" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">log mel spec, WavLM</td>
<td id="S5.T8.4.9.7.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">ResNet101-Temporal-Frequency</td>
<td id="S5.T8.4.9.7.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Weighted average</td>
<td id="S5.T8.4.9.7.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">threshold</td>
</tr>
<tr id="S5.T8.4.10.8" class="ltx_tr">
<th id="S5.T8.4.10.8.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.10.8.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.10.8.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">time stretching, time</td>
<td id="S5.T8.4.10.8.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.10.8.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Transformer (TFT)</td>
<td id="S5.T8.4.10.8.6" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.10.8.7" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
</tr>
<tr id="S5.T8.4.11.9" class="ltx_tr">
<th id="S5.T8.4.11.9.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.11.9.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.11.9.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">masking, freq. masking</td>
<td id="S5.T8.4.11.9.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.11.9.5" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.11.9.6" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.11.9.7" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
</tr>
<tr id="S5.T8.4.12.10" class="ltx_tr">
<th id="S5.T8.4.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">D05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<th id="S5.T8.4.12.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;">73.52</th>
<td id="S5.T8.4.12.10.3" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Noise, remove silence</td>
<td id="S5.T8.4.12.10.4" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">raw, LFCC, HuBERT</td>
<td id="S5.T8.4.12.10.5" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">RawNet2, SE-Res2Net50, HuBERT</td>
<td id="S5.T8.4.12.10.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">Label fusion;</td>
<td id="S5.T8.4.12.10.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">manifold-based</td>
</tr>
<tr id="S5.T8.4.13.11" class="ltx_tr">
<th id="S5.T8.4.13.11.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.13.11.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.13.11.3" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.13.11.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.13.11.5" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.13.11.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">average (score);</td>
<td id="S5.T8.4.13.11.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">multi-model</td>
</tr>
<tr id="S5.T8.4.14.12" class="ltx_tr">
<th id="S5.T8.4.14.12.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<th id="S5.T8.4.14.12.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:3.7pt;padding-right:3.7pt;"></th>
<td id="S5.T8.4.14.12.3" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.14.12.4" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.14.12.5" class="ltx_td" style="padding-left:3.7pt;padding-right:3.7pt;"></td>
<td id="S5.T8.4.14.12.6" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">concat (feature)</td>
<td id="S5.T8.4.14.12.7" class="ltx_td ltx_align_left" style="padding-left:3.7pt;padding-right:3.7pt;">fusion</td>
</tr>
<tr id="S5.T8.4.15.13" class="ltx_tr">
<th id="S5.T8.4.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">S05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S5.T8.4.15.13.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">53.50</th>
<td id="S5.T8.4.15.13.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">-</td>
<td id="S5.T8.4.15.13.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">LFCC</td>
<td id="S5.T8.4.15.13.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">ResNet</td>
<td id="S5.T8.4.15.13.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">-</td>
<td id="S5.T8.4.15.13.7" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;">threshold</td>
</tr>
<tr id="S5.T8.4.16.14" class="ltx_tr">
<th id="S5.T8.4.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">S06 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<th id="S5.T8.4.16.14.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">54.16</th>
<td id="S5.T8.4.16.14.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">-</td>
<td id="S5.T8.4.16.14.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">LFCC</td>
<td id="S5.T8.4.16.14.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">ResNet</td>
<td id="S5.T8.4.16.14.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">-</td>
<td id="S5.T8.4.16.14.7" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;">OpenMax</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Track 1.2: Detection task (FG-D)</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The technical details of the top-5 participating teams are summarized in Table <a href="#S4.T6" title="TABLE VI ‣ IV-D Track 3: Deepfake algorithm recognition (AR) ‣ IV Datasets ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>. Given the binary nature of the task, it is interesting to note that Team B04 opted to use a variational autoencoder (VAE) as their back-end classifier.
In the analysis of top-performing systems, we noticed the following:</p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Data augmentation:</span> The use of data augmentation in Track 1.2 is widespread and has found much success, with most teams opting to use MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> noise augmentation. The use of reverberation is also popular, with most teams opting to add reverberation to their training data to improve the robustness of their models. Interestingly, Team B01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> opted to furthermore use copy synthesis as one of their data augmentation methods to obtain more training data from the AISHELL-3 corpus, and given the success of their system, this approach may have been effective in the task.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Acoustic Featuress:</span> Most teams opted to incorporate acoustic feature extractions into their systems, and Teams B01 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, B02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and B03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> notably tweaked the AASIST backend, originally an end-to-end model, to accomodate the use of acoustic features like wav2vec and CQT spectrograms.
The reliability and popularity of Wav2Vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> is evident in the fact that most teams opted to use it as their acoustic feature extractor, with the notable exception of Team B03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, who used CQT spectrogram. This popularity is likely due to the fact that deep embedding features are more robust to noise and reverberation, and thus may be more suitable for real-world applications. The promising performance of deep embedding features in anti-deepfake systems is also evidenced in the relatively high performance of the baseline system <span id="S5.I2.i2.p1.1.2" class="ltx_text ltx_font_bold">S03</span> in the FG-D track, which employs Wav2Vec 2.0 as its acoustic feature extractor, and is the best-performing baseline system in this track.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p"><span id="S5.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Back-end classifier:</span> LCNN-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> models are popular among participating teams due to its efficiency and performance. Notably, AASIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, in part based on RawNet2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and incorporating graph attention networks, is also featured in many submissions by participants of Track 1.2, with multiple top-performing teams opting to use AASIST as one of their back-end classifiers, if not the only one. We thus feel that AASIST is representative of progress in anti-deepfake system development. Interestingly, however, participants often favour the use of wav2vec deep embeddings in conjunction with the AASIST backend by removing the sinc-conv layers, presumably to benefit from both the expressivity of wav2vec features as well as the performance of the AASIST model.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p id="S5.I2.i4.p1.1" class="ltx_p"><span id="S5.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Model fusion:</span> While a significant number of teams opted for a single-model approach, many teams opted for a multi-model fusion approach. Among those latter teams, the most popular approach is simple averaging, with the notable exception of Team B02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, who used a weighted average of the predictions from SENet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, LCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and AASIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. This approach is likely effective due to the fact that different models may have different strengths and weaknesses, and thus a multi-model fusion approach may be able to combine the strengths of different models to achieve better performance while mitigating their respective weaknesses.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Track 2: Manipulation region location (RL)</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The technical details of the top-5 participating teams are summarized in Table <a href="#S5.T7" title="TABLE VII ‣ V-A Track 1.1: Generation task (FG-G) ‣ V Technical Analysis ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>. Like in the FG-D task, Wav2Vec 2.0-extracted features are popular among the top-performing teams. As for back-end classification models, LSTM-based models are popular, with the notable exception of Team C05, who used an AASIST-based model. The use of data augmentation is also widespread, with most teams opting to use MUSAN for noise augmentation.
In the analysis of top-performing systems, we noticed the following:</p>
<ol id="S5.I3" class="ltx_enumerate">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Data augmentation:</span> As with other tracks of the challenge, data augmentation sees widespread use in the RL track, with most teams using additive noise augmentation. Interestingly, Team C02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> furthermore opted to use re-splicing as one of their data augmentation methods, which is well suited for this task, given that the RL track is concerned with the detection of manipulation regions, and resplicing allows for the creation of more training data with different manipulation regions.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Acoustic features:</span> Like in the FG-D task, most teams opted to use Wav2Vec 2.0 for acoustic feature extraction, with the notable exception of Teams C02 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and C03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, who used spectrograms as acoustic feature representation. This popularity of Wav2Vec 2.0 further evidences the reliability of deep embedding features in anti-deepfake systems. In addition, it is worth noting that Team C05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> uses both raw waveform as well as wav2vec deep embedding features.</p>
</div>
</li>
<li id="S5.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I3.i3.p1" class="ltx_para">
<p id="S5.I3.i3.p1.1" class="ltx_p"><span id="S5.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Back-end classifiers:</span> Given the sequence nature of the RL task, where a real/fake prediction must be given for each frame, LSTM-based models are popular among the top-performing teams, with the notable exception of Team C05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, who used an AASIST-based model. Judging by the ranking, however, it is evident that LSTM-based recurrent models may still be more effective and better suited for the recurrent nature of the RL task, as Team C05, who used a fusion of AASIST-based model and FC layer classifier, is the only team in the top-5 that did not use an LSTM-based model, and the performance is slightly lower than that of the other teams.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Track 3: Deepfake algorithm recognition (AR)</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The technical details of the top-5 participating teams are summarized in Table <a href="#S5.T8" title="TABLE VIII ‣ V-A Track 1.1: Generation task (FG-G) ‣ V Technical Analysis ‣ ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>. Given the particular nature of open-set recognition (OSR), the out-of-distribution detection method is separately listed in the last column. In the analysis of top-performing systems, we noticed the following:</p>
<ol id="S5.I4" class="ltx_enumerate">
<li id="S5.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I4.i1.p1" class="ltx_para">
<p id="S5.I4.i1.p1.1" class="ltx_p"><span id="S5.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Data augmentation:</span> Most teams opted to use MUSAN for noise augmentation, and a significant number of teams also opted to add reverberation to their training data to improve the robustness of their models. In addition, the top-performing teams also opted to use other methods to further augment their training data, including CutMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and random sampling, time stretching, and time masking. This is likely due to the fact that the AR task is concerned with the recognition of deepfake algorithms, and thus the use of different data augmentation methods may be more effective in capturing the diverse acoustic conditions of deepfake audio in the wild.</p>
</div>
</li>
<li id="S5.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I4.i2.p1" class="ltx_para">
<p id="S5.I4.i2.p1.1" class="ltx_p"><span id="S5.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Acoustic features:</span> Compared to Tracks 1.2 and 2, participants of Track 3 are more diverse in their choice of acoustic features. While a number of teams used Wav2Vec 2.0, a significant number of teams also opted to use log-Mel spectrogram, STFT spectrogram, and even HuBERT-based features. This is likely due to the fact that the AR task is concerned with the recognition of deepfake algorithms, beyond the binary classification of real/fake, and thus a more diverse set of acoustic features may be more suitable for the task, without a clear-cut winner. This also signals the prospect of additional research into the development of acoustic features for anti-deepfake systems.</p>
</div>
</li>
<li id="S5.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I4.i3.p1" class="ltx_para">
<p id="S5.I4.i3.p1.1" class="ltx_p"><span id="S5.I4.i3.p1.1.1" class="ltx_text ltx_font_bold">Back-end classifiers:</span> ResNet-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> models (including Res2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>) are popular among the top-performing teams, with the notable exception of Team D03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, who used a single ECAPA-TDNN-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. The use of ECAPA-TDNN is likely due to the fact that ECAPA-TDNN is a popular model for speaker recognition, and the philosophy of speaker recognition may be applicable to the AR task, which is also a recognition task, albeit of deepfake algorithms instead of speakers. The popularity of ResNet-based models is likely due to the fact that ResNet-based models are relatively simple and efficient, and thus may be more suitable for real-world applications.</p>
</div>
</li>
<li id="S5.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I4.i4.p1" class="ltx_para">
<p id="S5.I4.i4.p1.1" class="ltx_p"><span id="S5.I4.i4.p1.1.1" class="ltx_text ltx_font_bold">Model fusion:</span> The averaging of scores is still popular, but the use of label fusion and feature fusion (in the form of concatenation) also see its use. This is likely due to the fact that the AR task is concerned with the recognition of deepfake algorithms, and thus the use of different models, each of which may be better suited for a particular aspect of the task, may be more effective than a single-model approach.</p>
</div>
</li>
<li id="S5.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S5.I4.i5.p1" class="ltx_para">
<p id="S5.I4.i5.p1.1" class="ltx_p"><span id="S5.I4.i5.p1.1.1" class="ltx_text ltx_font_bold">OSR methods:</span> Given the nature of the AR task and its practicality in real-world applications, as well as the presence of an unknown class, participating teams are required to use an out-of-distribution detection method to detect unknown classes. Thresholding remains the most popular method, to decent success. Other methods, including OpenMax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> and manifold-based multi-model fusion, are also used.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Directions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The ADD 2023 challenge has provided a platform for researchers to develop new technologies to combat deepfake audio; However, there are several limitations that should be addressed in future research efforts. We identify these limitations and suggest future directions for research in the field of deepfake audio detection.</p>
</div>
<div id="S6.p2" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Coping with unseen deepfake technologies and adversarial attacks:</span> The rapid development of audio deepfake generation and adversarial attack technologies, like VALL-E, GPT-4o, VISinger and DiffSinger, etc., brings critical challenges to current existing detection methods. In response to these challenges, deepfake audio generation and detection task are viewed as a rivalry game for participants in the ADD competitions. Despite partly improving the anti-attack ability of the detection model via fake game, there isn’t sufficient adversarial dynamic beyond generation and evaluation of deepfake speech examples. Future research should develop frameworks that enable dynamic, real-time rivalry game scenarios, allowing for a more thorough exploration of defense mechanisms’ effectiveness and methods like continual reinforcement learning.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Improving the interpretability of discrimination:</span> Beyond detecting and locating manipulated regions, future research should aim to identify specific manipulation techniques used in the manipulated audio, providing a more comprehensive understanding of the manipulation process and the reasons of discrimination. Additionally, developing visualization technologies highlighting manipulation regions and enhancing manipulation traces in audio signals can help users understand the detection process and build trust in anti-deepfake technologies.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Improving generalization ability and robustness:</span> Although previous studies have made some attempts on audio deepfake detection and attribution, the generalization and robustness of the models are still poor. The performance of the top-performing models in the ADD competitions are very high but it will degrade significantly when evaluated on the mismatching dataset containing multiple unseen deepfake methods or unseen acoustic conditions etc. Future studies concluding unsupervised domain adaptation, open set continual learning and transfer learning can help models better generalize, making them more reliable in real-world applications.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Considering real-time processing for detection systems:</span> Real-time processing is critical for deploying anti-deepfake technologies in applications such as live streaming and voice-based authentication. Future work should optimize models for low-latency performance and efficient use of computational resources. Techniques like model pruning, quantization, and edge computing can be investigated to achieve these goals, ensuring that detection systems are both responsive and resource efficient.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p"><span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Considering multilingual scenarios:</span> The majority of previously released datasets and detection models are mainly focused on single language, most of which in English and Chinese and few of them in other language like Japanese. But the applicability of anti-deepfake technologies in multilingual scenarios is essential in realistic applications. Future research should focus on developing models that can detect deepfake speech in multiple languages, ensuring that detection technologies are effective across diverse linguistic contexts.</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p"><span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Exploring better evaluation metrics:</span> EER, accuracy, precision, recall and F1-score are employed as the evaluation metric in previous work. However, evaluation metrics should be designed to reflect real-world scenarios. Future research should focus on developing standardized benchmarks and evaluation protocols that simulate real-world conditions to ensure that detection technologies are both theoretically sound and practically viable. Human detection capabilities, as well as the differences between humans and machines also need to be considered for detecting and attributing deepfake audio.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The ADD 2023 challenge aimed to spur innovation and research in detecting and analyzing deepfake speech, attracting 145 teams from 15 countries. This paper presents the challenge’s dataset, provides a technical analysis of top-performing systems and identifies certain limitations and suggests future directions for research in the field of deepfake speech detection, notably in the areas of better coping with unseen attacks, improved interpretability and generalization abilities, real-time processing, multilingual scenarios, and better evaluation metrics. We hope that the release of the dataset and the analysis presented in this paper will inspire further research in the field of deepfake speech detection and contribute to the development of more robust and reliable anti-deepfake technologies.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Tan, T. Qin, F. Soong <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A survey on neural speech synthesis,” <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.15561</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. Sisman, J. Yamagishi, S. King <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “An overview of voice conversion and its challenges: From statistical modeling to deep learning,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 132–157, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Yamagishi, X. Wang, M. Todisco <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection,” in <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">The ASVspoof 2021 Workshop</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Yi, R. Fu, J. Tao <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “ADD 2022: the first audio deep synthesis detection challenge,” in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Peng, H. Fan, W. Wang <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “DFGC 2021: A deepfake game competition,” in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">IJCB</em>, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Yi, J. Tao, R. Fu <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “ADD 2023: the second audio deepfake detection challenge,” in <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X. Yan, J. Yi, J. Tao <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio,” in <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia</em>, ser. DDAM ’22.   Association for Computing Machinery, 2022, pp. 61–68.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Shi, H. Bu, X. Xu <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “AISHELL-3: A multi-speaker Mandarin TTS corpus and the baselines,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11567</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Yi, Y. Bai, J. Tao <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Half-truth: A partially fake audio detection dataset,” in <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">Proc. of INTERSPEECH</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 17 022–17 033, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.-M. Valin and J. Skoglund, “LPCNet: Improving neural speech synthesis through linear prediction,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2019, pp. 5891–5895.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G. Yang, S. Yang, K. Liu <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Multi-band MelGAN: Faster waveform generation for high-quality text-to-speech,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">arXiv:2005.05106 [cs, eess]</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Mustafa, N. Pia, and G. Fuchs, “StyleMelGAN: An efficient high-fidelity adversarial vocoder with temporal adaptive normalization,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021, pp. 6034–6038.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Yamamoto, E. Song, and J.-M. Kim, “Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 6199–6203.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. van den Oord, S. Dieleman, H. Zen <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “WaveNet: A generative model for raw audio,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv:1609.03499 [cs]</em>, Sep. 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Morise, F. Yokomori, and K. Ozawa, “WORLD: a vocoder-based high-quality speech synthesis system for real-time applications,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEICE TRANSACTIONS on Information and Systems</em>, vol. 99, no. 7, pp. 1877–1884, 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Bu, J. Du, X. Na <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,” in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)</em>, 2017, pp. 1–5.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Wang and X. Zhang, “THCHS-30 : A free chinese speech corpus,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.01882</em>, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Qin, H. Bu, and M. Li, “Hi-mia: A far-field text-dependent speaker verification database and the baselines,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 7609–7613.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Hou, Y. Shi, M. Ostendorf <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Region proposal network based small-footprint keyword spotting,” <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">IEEE Signal Process. Lett.</em>, vol. 26, no. 10, pp. 1471–1475, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1109/LSP.2019.2936282</span>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F. Yu, S. Zhang, Y. Fu <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “M2MeT: The ICASSP 2022 multi-channel multi-party meeting transcription challenge,” in <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Zhou, B. Sisman, R. Liu <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Emotional voice conversion: Theory, databases and ESD,” <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">Speech Communication</em>, vol. 137, no. C, p. 1–18, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
N. Kalchbrenner, E. Elsen, K. Simonyan <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Efficient neural audio synthesis,” in <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2018, pp. 2410–2419.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V. Popov, I. Vovk, V. Gogoryan <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Grad-TTS: A diffusion probabilistic model for text-to-speech,” in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning</em>.   PMLR, 2021, pp. 8599–8608.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Wang, J. Yi, R. Fu <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Campnet: Context-aware mask prediction for end-to-end text-based speech editing,” <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 2241–2254, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Attention is all you need,” in <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, I. Guyon, U. V. Luxburg, S. Bengio <em id="bib.bib27.3.3" class="ltx_emph ltx_font_italic">et al.</em>, Eds., vol. 30.   Curran Associates, Inc., 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Zhou, Z.-H. Ling, and S. King, “The Blizzard Challenge 2020,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</em>, vol. 2020, 2020, pp. 1–18.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H. Ma, J. Yi, C. Wang <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “CFAD: A Chinese dataset for fake audio detection,” <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.12308</em>, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2207.12308</span>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Yan, J. Yi, C. Wang <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “System fingerprint recognition for deepfake audio: An initial dataset and investigation,” <em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.10489</em>, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2208.10489</span>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Zhao, Q. Yuan, Y. Duan <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “An end-to-end multi-module audio deepfake generation system for ADD Challenge 2023,” in <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. Zhan, Y. Zhang, and X. Yu, “The NeteaseGames system for fake audio generation task of 2023 Audio Deepfake Detection Challenge,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Hua, J. Lu, P. Shi <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Description of a multi-stage audio spoofing system in ADD Challenge 2023,” in <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Z. Su, J. Liu, Y. Li <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The Transsion deceptive speech synthesis system for ADD Challenge 2023,” in <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C. Wu and Y. Wang, “A research on improving the deception ability of speech generated by TTS system,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
H. Wu, Z. Li, L. Xu <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The USTC-NERCSLIP system for the Track 1.2 of Audio Deepfake Detection (ADD 2023) Challenge,” in <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H. Tak, M. R. Kamble, J. Patino <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Rawboost: A raw data boosting and augmentation method applied to automatic speaker verification anti-spoofing,” <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 6382–6386, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Zhang, J. Lu, Z. Li <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Improving the robustness of deepfake audio detection through confidence calibration,” in <em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Han, T. Kang, S. Choi <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “CAU KU deep fake detection system for ADD 2023 challenge,” in <em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Y. Wang, X. Wang, Y. Chen <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The DKU-MSXF system description for ADD 2023 Track 1.2,” in <em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Xie, H. Cheng, Y. Wang <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Single domain generalization for audio deepfake detection,” in <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Z. Cai, W. Wang, Y. Wang <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The DKU-DUKEECE system for the manipulation region location task of ADD 2023,” in <em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Liu, Z. Su, H. Huang <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “TranssionADD: A multi-frame reinforcement based sequence tagging model for audio deepfake detection,” in <em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
K. Li, X.-M. Zeng, J.-T. Zhang <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Convolutional recurrent neural network and multitask learning for manipulation region location,” in <em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. M. Martín-Doñas and A. Álvarez, “The Vicomtech partial deepfake detection and location system for the 2023 ADD Challenge,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J. Li, L. Li, M. Luo <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Multi-grained backend fusion for manipulation region location of partially fake audio,” in <em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Lu, Y. Zhang, Z. Li <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Detecting unknown speech spoofing algorithms with nearest neighbors,” in <em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
X. Qin, X. Wang, Y. Chen <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “From speaker verification to deepfake algorithm recognition: Our learned lessons from ADD2023 Track3,” in <em id="bib.bib48.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X.-M. Zeng, J.-T. Zhang, K. Li <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deepfake algorithm recognition system with augmented data for ADD 2023 Challenge,” in <em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Z. Wang, Q. Wang, J. Yao <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The NPU-ASLP system for deepfake algorithm recognition in ADD 2023 Challenge,” in <em id="bib.bib50.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. Tian, Y. Chen, Y. Tang <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deepfake algorithm recognition through multi-model fusion based on manifold measure,” in <em id="bib.bib51.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Deepfake Audio Detection and Analysis (DADA 2023)</em>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech, and noise corpus,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
A. Baevski, H. Zhou, A. rahman Mohamed <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” <em id="bib.bib53.2.2" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/2006.11477, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Z. Wu, R. K. Das1, J. Yang <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Light convolutional neural network with feature genuinization for detection of synthetic speech attacks,” in <em id="bib.bib54.2.2" class="ltx_emph ltx_font_italic">Proc. of INTERSPEECH</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J.-w. Jung, H.-S. Heo, H. Tak <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “AASIST: Audio anti-spoofing using integrated spectro-temporal graph attention networks,” in <em id="bib.bib55.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 6367–6371.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
J. W. Jung, S. B. Kim, H. J. Shim <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Improved rawnet with filter-wise rescaling for text-independent speaker verification using raw waveforms,” in <em id="bib.bib56.2.2" class="ltx_emph ltx_font_italic">Proc. of INTERSPEECH</em>, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2018, pp. 7132–7141.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
S. Yun, D. Han, S. J. Oh <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Cutmix: Regularization strategy to train strong classifiers with localizable features,” in <em id="bib.bib58.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 6023–6032.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deep residual learning for image recognition,” <em id="bib.bib59.2.2" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 770–778, 2015.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
S.-H. Gao, M.-M. Cheng, K. Zhao <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Res2net: A new multi-scale backbone architecture,” <em id="bib.bib60.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 43, no. 2, pp. 652–662, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, 2020, pp. 3830–3834.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
A. Bendale and T. E. Boult, “Towards open set deep networks,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2016, pp. 1563–1572.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.04966" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.04967" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.04967">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.04967" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.04968" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:49:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
