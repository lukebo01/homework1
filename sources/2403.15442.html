<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.15442] Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives</title><meta property="og:description" content="Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profoun…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.15442">

<!--Generated on Fri Apr  5 18:10:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\credit</span>
<p id="p1.2" class="ltx_p">Conceptualization; Methodology; Resources; Investigation; Writing original draft; Writing, review, and editing</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\credit</span>
<p id="p2.2" class="ltx_p">Conceptualization; Methodology; Resources; Investigation; Writing original draft; Writing, review, and editing</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\tnotetext</span>
<p id="p3.2" class="ltx_p">[1]The first is the corresponding author.</p>
</div>
<h1 class="ltx_title ltx_title_document">Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Billel Essaid
</span><span class="ltx_author_notes">essaidbill@gmail.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hamza Kheddar
</span><span class="ltx_author_notes">kheddar.hamza@univ-medea.dz</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noureddine Batel
</span><span class="ltx_author_notes">batelnour@gmail.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abderrahmane Lakas
</span><span class="ltx_author_notes">alakas@uaeu.ac.ae</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad E. H. Chowdhury
</span><span class="ltx_author_notes">mchowdhury@qu.edu.qa
<span class="ltx_contact ltx_role_address">LSEA Laboratory, Department of Electrical Engineering, University of Medea, 26000, Algeria
</span>
<span class="ltx_contact ltx_role_address">College of Information Technology, United Arab Emirates University, Al Ain P.O. Box 17551, United Arab Emirates
</span>
<span class="ltx_contact ltx_role_address">Department of Electrical Engineering, Qatar University, Doha 2713, Qatar
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations and difficulties associated with traditional signal processing techniques dedicated to CIs. This review aims to comprehensively review advancements in CI-based ASR and speech enhancement, among other related aspects. The primary objective is to provide a thorough overview of metrics and datasets, exploring the capabilities of AI algorithms in this biomedical field, summarizing and commenting on the best results obtained. Additionally, the review will delve into potential applications and suggest future directions to bridge existing research gaps in this domain.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Profound hearing loss <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>Postoperative cochlear implant <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>Speech therapy rehabilitation <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>Cochlear implant <span id="id5.id4" class="ltx_ERROR undefined">\sep</span>Automatic speech recognition <span id="id6.id5" class="ltx_ERROR undefined">\sep</span>Machine learning

</div>
<figure id="tab1" class="ltx_table">
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">Acronyms and Abbreviations</h2>

<div class="ltx_pagination ltx_role_start_3_columns"></div>
<section id="Sx1.127" class="ltx_glossary ltx_acronym ltx_list_acronym">
<dl id="Sx1.127.127" class="ltx_glossarylist">
<dt id="Sx1.1.1.1" class="ltx_glossaryentry"><span id="Sx1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">CNN</span></dt>
<dd><span id="Sx1.1.1.1.4.1" class="ltx_text" style="font-size:80%;">convolutional neural network</span></dd>
<dt id="Sx1.2.2.2" class="ltx_glossaryentry"><span id="Sx1.2.2.2.1.1" class="ltx_text" style="font-size:80%;">AI</span></dt>
<dd><span id="Sx1.2.2.2.4.1" class="ltx_text" style="font-size:80%;">artificial intelligent</span></dd>
<dt id="Sx1.3.3.3" class="ltx_glossaryentry"><span id="Sx1.3.3.3.1.1" class="ltx_text" style="font-size:80%;">DL</span></dt>
<dd><span id="Sx1.3.3.3.4.1" class="ltx_text" style="font-size:80%;">deep learning</span></dd>
<dt id="Sx1.4.4.4" class="ltx_glossaryentry"><span id="Sx1.4.4.4.1.1" class="ltx_text" style="font-size:80%;">CI</span></dt>
<dd><span id="Sx1.4.4.4.4.1" class="ltx_text" style="font-size:80%;">cochlear implant</span></dd>
<dt id="Sx1.5.5.5" class="ltx_glossaryentry"><span id="Sx1.5.5.5.1.1" class="ltx_text" style="font-size:80%;">ML</span></dt>
<dd><span id="Sx1.5.5.5.4.1" class="ltx_text" style="font-size:80%;">machine learning</span></dd>
<dt id="Sx1.6.6.6" class="ltx_glossaryentry"><span id="Sx1.6.6.6.1.1" class="ltx_text" style="font-size:80%;">FCN</span></dt>
<dd><span id="Sx1.6.6.6.4.1" class="ltx_text" style="font-size:80%;">fully convolutional neural networks</span></dd>
<dt id="Sx1.7.7.7" class="ltx_glossaryentry"><span id="Sx1.7.7.7.1.1" class="ltx_text" style="font-size:80%;">STOI</span></dt>
<dd><span id="Sx1.7.7.7.4.1" class="ltx_text" style="font-size:80%;">short time objective intelligibility</span></dd>
<dt id="Sx1.8.8.8" class="ltx_glossaryentry"><span id="Sx1.8.8.8.1.1" class="ltx_text" style="font-size:80%;">MMSE</span></dt>
<dd><span id="Sx1.8.8.8.4.1" class="ltx_text" style="font-size:80%;">minimum mean square error</span></dd>
<dt id="Sx1.9.9.9" class="ltx_glossaryentry"><span id="Sx1.9.9.9.1.1" class="ltx_text" style="font-size:80%;">DDAE</span></dt>
<dd><span id="Sx1.9.9.9.4.1" class="ltx_text" style="font-size:80%;">deep denoising auto-encoder</span></dd>
<dt id="Sx1.10.10.10" class="ltx_glossaryentry"><span id="Sx1.10.10.10.1.1" class="ltx_text" style="font-size:80%;">SNR</span></dt>
<dd><span id="Sx1.10.10.10.4.1" class="ltx_text" style="font-size:80%;">signal to noise ratio</span></dd>
<dt id="Sx1.11.11.11" class="ltx_glossaryentry"><span id="Sx1.11.11.11.1.1" class="ltx_text" style="font-size:80%;">EAS</span></dt>
<dd><span id="Sx1.11.11.11.4.1" class="ltx_text" style="font-size:80%;">electric and acoustic stimulation</span></dd>
<dt id="Sx1.12.12.12" class="ltx_glossaryentry"><span id="Sx1.12.12.12.1.1" class="ltx_text" style="font-size:80%;">CGRU</span></dt>
<dd><span id="Sx1.12.12.12.4.1" class="ltx_text" style="font-size:80%;"> convolutional recurrent neural network with
gated recurrent units</span></dd>
<dt id="Sx1.13.13.13" class="ltx_glossaryentry"><span id="Sx1.13.13.13.1.1" class="ltx_text" style="font-size:80%;">CWT</span></dt>
<dd><span id="Sx1.13.13.13.4.1" class="ltx_text" style="font-size:80%;"> continuous
wavelet transform</span></dd>
<dt id="Sx1.14.14.14" class="ltx_glossaryentry"><span id="Sx1.14.14.14.1.1" class="ltx_text" style="font-size:80%;">GRU</span></dt>
<dd><span id="Sx1.14.14.14.4.1" class="ltx_text" style="font-size:80%;">gated recurrent units</span></dd>
<dt id="Sx1.15.15.15" class="ltx_glossaryentry"><span id="Sx1.15.15.15.1.1" class="ltx_text" style="font-size:80%;">IGCIP</span></dt>
<dd><span id="Sx1.15.15.15.4.1" class="ltx_text" style="font-size:80%;">image guided cochlear implant programming </span></dd>
<dt id="Sx1.16.16.16" class="ltx_glossaryentry"><span id="Sx1.16.16.16.1.1" class="ltx_text" style="font-size:80%;">CT</span></dt>
<dd><span id="Sx1.16.16.16.4.1" class="ltx_text" style="font-size:80%;">computed tomography</span></dd>
<dt id="Sx1.17.17.17" class="ltx_glossaryentry"><span id="Sx1.17.17.17.1.1" class="ltx_text" style="font-size:80%;">OCT</span></dt>
<dd><span id="Sx1.17.17.17.4.1" class="ltx_text" style="font-size:80%;">optical coherence tomography</span></dd>
<dt id="Sx1.18.18.18" class="ltx_glossaryentry"><span id="Sx1.18.18.18.1.1" class="ltx_text" style="font-size:80%;">MEE</span></dt>
<dd><span id="Sx1.18.18.18.4.1" class="ltx_text" style="font-size:80%;">mean endpoint error</span></dd>
<dt id="Sx1.19.19.19" class="ltx_glossaryentry"><span id="Sx1.19.19.19.1.1" class="ltx_text" style="font-size:80%;">TMHINT</span></dt>
<dd><span id="Sx1.19.19.19.4.1" class="ltx_text" style="font-size:80%;">taiwan mandarin hearing in noise test</span></dd>
<dt id="Sx1.20.20.20" class="ltx_glossaryentry"><span id="Sx1.20.20.20.1.1" class="ltx_text" style="font-size:80%;">Acc</span></dt>
<dd><span id="Sx1.20.20.20.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></dd>
<dt id="Sx1.21.21.21" class="ltx_glossaryentry"><span id="Sx1.21.21.21.1.1" class="ltx_text" style="font-size:80%;">Sen</span></dt>
<dd><span id="Sx1.21.21.21.4.1" class="ltx_text" style="font-size:80%;">Sensitivity</span></dd>
<dt id="Sx1.22.22.22" class="ltx_glossaryentry"><span id="Sx1.22.22.22.1.1" class="ltx_text" style="font-size:80%;">Spe</span></dt>
<dd><span id="Sx1.22.22.22.4.1" class="ltx_text" style="font-size:80%;">Specificity</span></dd>
<dt id="Sx1.23.23.23" class="ltx_glossaryentry"><span id="Sx1.23.23.23.1.1" class="ltx_text" style="font-size:80%;">Pre</span></dt>
<dd><span id="Sx1.23.23.23.4.1" class="ltx_text" style="font-size:80%;">Precision</span></dd>
<dt id="Sx1.24.24.24" class="ltx_glossaryentry"><span id="Sx1.24.24.24.1.1" class="ltx_text" style="font-size:80%;">Rec</span></dt>
<dd><span id="Sx1.24.24.24.4.1" class="ltx_text" style="font-size:80%;">Recall</span></dd>
<dt id="Sx1.25.25.25" class="ltx_glossaryentry"><span id="Sx1.25.25.25.1.1" class="ltx_text" style="font-size:80%;">F1</span></dt>
<dd><span id="Sx1.25.25.25.4.1" class="ltx_text" style="font-size:80%;">F1 score</span></dd>
<dt id="Sx1.26.26.26" class="ltx_glossaryentry"><span id="Sx1.26.26.26.1.1" class="ltx_text" style="font-size:80%;">NCM</span></dt>
<dd><span id="Sx1.26.26.26.4.1" class="ltx_text" style="font-size:80%;">normalized covariance measure</span></dd>
<dt id="Sx1.27.27.27" class="ltx_glossaryentry"><span id="Sx1.27.27.27.1.1" class="ltx_text" style="font-size:80%;">FFT</span></dt>
<dd><span id="Sx1.27.27.27.4.1" class="ltx_text" style="font-size:80%;">fast Fourier transform</span></dd>
<dt id="Sx1.28.28.28" class="ltx_glossaryentry"><span id="Sx1.28.28.28.1.1" class="ltx_text" style="font-size:80%;">ACE</span></dt>
<dd><span id="Sx1.28.28.28.4.1" class="ltx_text" style="font-size:80%;">advanced combination encoder</span></dd>
<dt id="Sx1.29.29.29" class="ltx_glossaryentry"><span id="Sx1.29.29.29.1.1" class="ltx_text" style="font-size:80%;">LGF</span></dt>
<dd><span id="Sx1.29.29.29.4.1" class="ltx_text" style="font-size:80%;">loudness growth function</span></dd>
<dt id="Sx1.30.30.30" class="ltx_glossaryentry"><span id="Sx1.30.30.30.1.1" class="ltx_text" style="font-size:80%;">MMS</span></dt>
<dd><span id="Sx1.30.30.30.4.1" class="ltx_text" style="font-size:80%;">min-max similarity</span></dd>
<dt id="Sx1.31.31.31" class="ltx_glossaryentry"><span id="Sx1.31.31.31.1.1" class="ltx_text" style="font-size:80%;">BF</span></dt>
<dd><span id="Sx1.31.31.31.4.1" class="ltx_text" style="font-size:80%;">boundary F1</span></dd>
<dt id="Sx1.32.32.32" class="ltx_glossaryentry"><span id="Sx1.32.32.32.1.1" class="ltx_text" style="font-size:80%;">ROC</span></dt>
<dd><span id="Sx1.32.32.32.4.1" class="ltx_text" style="font-size:80%;"> receiver operating characteristic curve</span></dd>
<dt id="Sx1.33.33.33" class="ltx_glossaryentry"><span id="Sx1.33.33.33.1.1" class="ltx_text" style="font-size:80%;">NCM</span></dt>
<dd><span id="Sx1.33.33.33.4.1" class="ltx_text" style="font-size:80%;">normalized covariance measure</span></dd>
<dt id="Sx1.34.34.34" class="ltx_glossaryentry"><span id="Sx1.34.34.34.1.1" class="ltx_text" style="font-size:80%;">DCS</span></dt>
<dd><span id="Sx1.34.34.34.4.1" class="ltx_text" style="font-size:80%;">dice coefficient similarity</span></dd>
<dt id="Sx1.35.35.35" class="ltx_glossaryentry"><span id="Sx1.35.35.35.1.1" class="ltx_text" style="font-size:80%;">ASD</span></dt>
<dd><span id="Sx1.35.35.35.4.1" class="ltx_text" style="font-size:80%;">average surface distance</span></dd>
<dt id="Sx1.36.36.36" class="ltx_glossaryentry"><span id="Sx1.36.36.36.1.1" class="ltx_text" style="font-size:80%;">AVD</span></dt>
<dd><span id="Sx1.36.36.36.4.1" class="ltx_text" style="font-size:80%;">average volume difference</span></dd>
<dt id="Sx1.37.37.37" class="ltx_glossaryentry"><span id="Sx1.37.37.37.1.1" class="ltx_text" style="font-size:80%;">JI</span></dt>
<dd><span id="Sx1.37.37.37.4.1" class="ltx_text" style="font-size:80%;">jaccard index</span></dd>
<dt id="Sx1.38.38.38" class="ltx_glossaryentry"><span id="Sx1.38.38.38.1.1" class="ltx_text" style="font-size:80%;">IoU</span></dt>
<dd><span id="Sx1.38.38.38.4.1" class="ltx_text" style="font-size:80%;">intersection over union</span></dd>
<dt id="Sx1.39.39.39" class="ltx_glossaryentry"><span id="Sx1.39.39.39.1.1" class="ltx_text" style="font-size:80%;">MAE</span></dt>
<dd><span id="Sx1.39.39.39.4.1" class="ltx_text" style="font-size:80%;">mean absolute error</span></dd>
<dt id="Sx1.40.40.40" class="ltx_glossaryentry"><span id="Sx1.40.40.40.1.1" class="ltx_text" style="font-size:80%;">HD</span></dt>
<dd><span id="Sx1.40.40.40.4.1" class="ltx_text" style="font-size:80%;">hausdorff distance </span></dd>
<dt id="Sx1.41.41.41" class="ltx_glossaryentry"><span id="Sx1.41.41.41.1.1" class="ltx_text" style="font-size:80%;">VAE</span></dt>
<dd><span id="Sx1.41.41.41.4.1" class="ltx_text" style="font-size:80%;">variational autoencoders</span></dd>
<dt id="Sx1.42.42.42" class="ltx_glossaryentry"><span id="Sx1.42.42.42.1.1" class="ltx_text" style="font-size:80%;">CAE</span></dt>
<dd><span id="Sx1.42.42.42.4.1" class="ltx_text" style="font-size:80%;">convolutional autoencoder</span></dd>
<dt id="Sx1.43.43.43" class="ltx_glossaryentry"><span id="Sx1.43.43.43.1.1" class="ltx_text" style="font-size:80%;">AE</span></dt>
<dd><span id="Sx1.43.43.43.4.1" class="ltx_text" style="font-size:80%;">autoencoder</span></dd>
<dt id="Sx1.44.44.44" class="ltx_glossaryentry"><span id="Sx1.44.44.44.1.1" class="ltx_text" style="font-size:80%;">SOTA</span></dt>
<dd><span id="Sx1.44.44.44.4.1" class="ltx_text" style="font-size:80%;">state-of-the-art</span></dd>
<dt id="Sx1.45.45.45" class="ltx_glossaryentry"><span id="Sx1.45.45.45.1.1" class="ltx_text" style="font-size:80%;">MFCC</span></dt>
<dd><span id="Sx1.45.45.45.4.1" class="ltx_text" style="font-size:80%;">mel-frequency cepstral coefficient</span></dd>
<dt id="Sx1.46.46.46" class="ltx_glossaryentry"><span id="Sx1.46.46.46.1.1" class="ltx_text" style="font-size:80%;">GFCC</span></dt>
<dd><span id="Sx1.46.46.46.4.1" class="ltx_text" style="font-size:80%;">gammatone frequency cepstral coefficient </span></dd>
<dt id="Sx1.47.47.47" class="ltx_glossaryentry"><span id="Sx1.47.47.47.1.1" class="ltx_text" style="font-size:80%;">AMR</span></dt>
<dd><span id="Sx1.47.47.47.4.1" class="ltx_text" style="font-size:80%;">adaptive multi-rate</span></dd>
<dt id="Sx1.48.48.48" class="ltx_glossaryentry"><span id="Sx1.48.48.48.1.1" class="ltx_text" style="font-size:80%;">LSTM</span></dt>
<dd><span id="Sx1.48.48.48.4.1" class="ltx_text" style="font-size:80%;">long short term memory</span></dd>
<dt id="Sx1.49.49.49" class="ltx_glossaryentry"><span id="Sx1.49.49.49.1.1" class="ltx_text" style="font-size:80%;">ASR</span></dt>
<dd><span id="Sx1.49.49.49.4.1" class="ltx_text" style="font-size:80%;">automatic speech recognition</span></dd>
<dt id="Sx1.50.50.50" class="ltx_glossaryentry"><span id="Sx1.50.50.50.1.1" class="ltx_text" style="font-size:80%;">DNN</span></dt>
<dd><span id="Sx1.50.50.50.4.1" class="ltx_text" style="font-size:80%;">deep neural networks</span></dd>
<dt id="Sx1.51.51.51" class="ltx_glossaryentry"><span id="Sx1.51.51.51.1.1" class="ltx_text" style="font-size:80%;">CS</span></dt>
<dd><span id="Sx1.51.51.51.4.1" class="ltx_text" style="font-size:80%;">channel selection</span></dd>
<dt id="Sx1.52.52.52" class="ltx_glossaryentry"><span id="Sx1.52.52.52.1.1" class="ltx_text" style="font-size:80%;">RNN</span></dt>
<dd><span id="Sx1.52.52.52.4.1" class="ltx_text" style="font-size:80%;">recurrent neural network</span></dd>
<dt id="Sx1.53.53.53" class="ltx_glossaryentry"><span id="Sx1.53.53.53.1.1" class="ltx_text" style="font-size:80%;">GAN</span></dt>
<dd><span id="Sx1.53.53.53.4.1" class="ltx_text" style="font-size:80%;">generative adversarial network</span></dd>
<dt id="Sx1.54.54.54" class="ltx_glossaryentry"><span id="Sx1.54.54.54.1.1" class="ltx_text" style="font-size:80%;">PESQ</span></dt>
<dd><span id="Sx1.54.54.54.4.1" class="ltx_text" style="font-size:80%;"> perceptual estimation of speech quality</span></dd>
<dt id="Sx1.55.55.55" class="ltx_glossaryentry"><span id="Sx1.55.55.55.1.1" class="ltx_text" style="font-size:80%;">KLT</span></dt>
<dd><span id="Sx1.55.55.55.4.1" class="ltx_text" style="font-size:80%;">karhunen-loéve transform</span></dd>
<dt id="Sx1.56.56.56" class="ltx_glossaryentry"><span id="Sx1.56.56.56.1.1" class="ltx_text" style="font-size:80%;">logMMSE</span></dt>
<dd><span id="Sx1.56.56.56.4.1" class="ltx_text" style="font-size:80%;">log minimum mean squared error</span></dd>
<dt id="Sx1.57.57.57" class="ltx_glossaryentry"><span id="Sx1.57.57.57.1.1" class="ltx_text" style="font-size:80%;">DAE</span></dt>
<dd><span id="Sx1.57.57.57.4.1" class="ltx_text" style="font-size:80%;">deep autoencoder</span></dd>
<dt id="Sx1.58.58.58" class="ltx_glossaryentry"><span id="Sx1.58.58.58.1.1" class="ltx_text" style="font-size:80%;">ICA</span></dt>
<dd><span id="Sx1.58.58.58.4.1" class="ltx_text" style="font-size:80%;">intra cochlear anatomy</span></dd>
<dt id="Sx1.59.59.59" class="ltx_glossaryentry"><span id="Sx1.59.59.59.1.1" class="ltx_text" style="font-size:80%;">ST</span></dt>
<dd><span id="Sx1.59.59.59.4.1" class="ltx_text" style="font-size:80%;">scala tympani</span></dd>
<dt id="Sx1.60.60.60" class="ltx_glossaryentry"><span id="Sx1.60.60.60.1.1" class="ltx_text" style="font-size:80%;">SV</span></dt>
<dd><span id="Sx1.60.60.60.4.1" class="ltx_text" style="font-size:80%;">scala vestibul</span></dd>
<dt id="Sx1.61.61.61" class="ltx_glossaryentry"><span id="Sx1.61.61.61.1.1" class="ltx_text" style="font-size:80%;">AR</span></dt>
<dd><span id="Sx1.61.61.61.4.1" class="ltx_text" style="font-size:80%;">active region</span></dd>
<dt id="Sx1.62.62.62" class="ltx_glossaryentry"><span id="Sx1.62.62.62.1.1" class="ltx_text" style="font-size:80%;">ASM</span></dt>
<dd><span id="Sx1.62.62.62.4.1" class="ltx_text" style="font-size:80%;">active shape model</span></dd>
<dt id="Sx1.63.63.63" class="ltx_glossaryentry"><span id="Sx1.63.63.63.1.1" class="ltx_text" style="font-size:80%;">NR</span></dt>
<dd><span id="Sx1.63.63.63.4.1" class="ltx_text" style="font-size:80%;">noise reduction</span></dd>
<dt id="Sx1.64.64.64" class="ltx_glossaryentry"><span id="Sx1.64.64.64.1.1" class="ltx_text" style="font-size:80%;">Res-CA</span></dt>
<dd><span id="Sx1.64.64.64.4.1" class="ltx_text" style="font-size:80%;">residual channel attention</span></dd>
<dt id="Sx1.65.65.65" class="ltx_glossaryentry"><span id="Sx1.65.65.65.1.1" class="ltx_text" style="font-size:80%;">GCPFE</span></dt>
<dd><span id="Sx1.65.65.65.4.1" class="ltx_text" style="font-size:80%;">global context-aware pyramid feature extraction</span></dd>
<dt id="Sx1.66.66.66" class="ltx_glossaryentry"><span id="Sx1.66.66.66.1.1" class="ltx_text" style="font-size:80%;">ACE-Loss</span></dt>
<dd><span id="Sx1.66.66.66.4.1" class="ltx_text" style="font-size:80%;">active contour with elastic loss</span></dd>
<dt id="Sx1.67.67.67" class="ltx_glossaryentry"><span id="Sx1.67.67.67.1.1" class="ltx_text" style="font-size:80%;">DS</span></dt>
<dd><span id="Sx1.67.67.67.4.1" class="ltx_text" style="font-size:80%;">deep supervision</span></dd>
<dt id="Sx1.68.68.68" class="ltx_glossaryentry"><span id="Sx1.68.68.68.1.1" class="ltx_text" style="font-size:80%;">UHR</span></dt>
<dd><span id="Sx1.68.68.68.4.1" class="ltx_text" style="font-size:80%;">ultra-high-resolution</span></dd>
<dt id="Sx1.69.69.69" class="ltx_glossaryentry"><span id="Sx1.69.69.69.1.1" class="ltx_text" style="font-size:80%;">ESCSO</span></dt>
<dd><span id="Sx1.69.69.69.4.1" class="ltx_text" style="font-size:80%;">enhanced swarm-based crow search optimization</span></dd>
<dt id="Sx1.70.70.70" class="ltx_glossaryentry"><span id="Sx1.70.70.70.1.1" class="ltx_text" style="font-size:80%;">cGAN</span></dt>
<dd><span id="Sx1.70.70.70.4.1" class="ltx_text" style="font-size:80%;">conditional generative adversarial networks</span></dd>
<dt id="Sx1.71.71.71" class="ltx_glossaryentry"><span id="Sx1.71.71.71.1.1" class="ltx_text" style="font-size:80%;">MAR</span></dt>
<dd><span id="Sx1.71.71.71.4.1" class="ltx_text" style="font-size:80%;">metal artifacts reduction</span></dd>
<dt id="Sx1.72.72.72" class="ltx_glossaryentry"><span id="Sx1.72.72.72.1.1" class="ltx_text" style="font-size:80%;">P2PE</span></dt>
<dd><span id="Sx1.72.72.72.4.1" class="ltx_text" style="font-size:80%;">point to point error</span></dd>
<dt id="Sx1.73.73.73" class="ltx_glossaryentry"><span id="Sx1.73.73.73.1.1" class="ltx_text" style="font-size:80%;">ASE</span></dt>
<dd><span id="Sx1.73.73.73.4.1" class="ltx_text" style="font-size:80%;">average surface error</span></dd>
<dt id="Sx1.74.74.74" class="ltx_glossaryentry"><span id="Sx1.74.74.74.1.1" class="ltx_text" style="font-size:80%;">MARGAN</span></dt>
<dd><span id="Sx1.74.74.74.4.1" class="ltx_text" style="font-size:80%;">metal artifact reduction based generative adversarial networks</span></dd>
<dt id="Sx1.75.75.75" class="ltx_glossaryentry"><span id="Sx1.75.75.75.1.1" class="ltx_text" style="font-size:80%;">ESCSO</span></dt>
<dd><span id="Sx1.75.75.75.4.1" class="ltx_text" style="font-size:80%;"> enhanced swarm based crow search optimization</span></dd>
<dt id="Sx1.76.76.76" class="ltx_glossaryentry"><span id="Sx1.76.76.76.1.1" class="ltx_text" style="font-size:80%;">EEG</span></dt>
<dd><span id="Sx1.76.76.76.4.1" class="ltx_text" style="font-size:80%;">electroencephalography</span></dd>
<dt id="Sx1.77.77.77" class="ltx_glossaryentry"><span id="Sx1.77.77.77.1.1" class="ltx_text" style="font-size:80%;">ERP</span></dt>
<dd><span id="Sx1.77.77.77.4.1" class="ltx_text" style="font-size:80%;">event-related potential</span></dd>
<dt id="Sx1.78.78.78" class="ltx_glossaryentry"><span id="Sx1.78.78.78.1.1" class="ltx_text" style="font-size:80%;">ANN</span></dt>
<dd><span id="Sx1.78.78.78.4.1" class="ltx_text" style="font-size:80%;">artificial neural network</span></dd>
<dt id="Sx1.79.79.79" class="ltx_glossaryentry"><span id="Sx1.79.79.79.1.1" class="ltx_text" style="font-size:80%;">RBNN</span></dt>
<dd><span id="Sx1.79.79.79.4.1" class="ltx_text" style="font-size:80%;">radial basis functions neural networks</span></dd>
<dt id="Sx1.80.80.80" class="ltx_glossaryentry"><span id="Sx1.80.80.80.1.1" class="ltx_text" style="font-size:80%;">KNN</span></dt>
<dd><span id="Sx1.80.80.80.4.1" class="ltx_text" style="font-size:80%;">K-nearest neighbours</span></dd>
<dt id="Sx1.81.81.81" class="ltx_glossaryentry"><span id="Sx1.81.81.81.1.1" class="ltx_text" style="font-size:80%;">RF</span></dt>
<dd><span id="Sx1.81.81.81.4.1" class="ltx_text" style="font-size:80%;">random forests</span></dd>
<dt id="Sx1.82.82.82" class="ltx_glossaryentry"><span id="Sx1.82.82.82.1.1" class="ltx_text" style="font-size:80%;">DRNN</span></dt>
<dd><span id="Sx1.82.82.82.4.1" class="ltx_text" style="font-size:80%;">deep recurrent neural networks</span></dd>
<dt id="Sx1.83.83.83" class="ltx_glossaryentry"><span id="Sx1.83.83.83.1.1" class="ltx_text" style="font-size:80%;">MLP</span></dt>
<dd><span id="Sx1.83.83.83.4.1" class="ltx_text" style="font-size:80%;">multilayer perceptrons</span></dd>
<dt id="Sx1.84.84.84" class="ltx_glossaryentry"><span id="Sx1.84.84.84.1.1" class="ltx_text" style="font-size:80%;">NMF</span></dt>
<dd><span id="Sx1.84.84.84.4.1" class="ltx_text" style="font-size:80%;">non-negative matrix factorization</span></dd>
<dt id="Sx1.85.85.85" class="ltx_glossaryentry"><span id="Sx1.85.85.85.1.1" class="ltx_text" style="font-size:80%;">DCAE</span></dt>
<dd><span id="Sx1.85.85.85.4.1" class="ltx_text" style="font-size:80%;">deep convolutional auto-encoders</span></dd>
<dt id="Sx1.86.86.86" class="ltx_glossaryentry"><span id="Sx1.86.86.86.1.1" class="ltx_text" style="font-size:80%;">ILD</span></dt>
<dd><span id="Sx1.86.86.86.4.1" class="ltx_text" style="font-size:80%;">interaural level difference</span></dd>
<dt id="Sx1.87.87.87" class="ltx_glossaryentry"><span id="Sx1.87.87.87.1.1" class="ltx_text" style="font-size:80%;">RTF</span></dt>
<dd><span id="Sx1.87.87.87.4.1" class="ltx_text" style="font-size:80%;">relative transfer function</span></dd>
<dt id="Sx1.88.88.88" class="ltx_glossaryentry"><span id="Sx1.88.88.88.1.1" class="ltx_text" style="font-size:80%;">SDR</span></dt>
<dd><span id="Sx1.88.88.88.4.1" class="ltx_text" style="font-size:80%;">source-to-distortion ratio</span></dd>
<dt id="Sx1.89.89.89" class="ltx_glossaryentry"><span id="Sx1.89.89.89.1.1" class="ltx_text" style="font-size:80%;">SAR</span></dt>
<dd><span id="Sx1.89.89.89.4.1" class="ltx_text" style="font-size:80%;">source-to-artifact ratio</span></dd>
<dt id="Sx1.90.90.90" class="ltx_glossaryentry"><span id="Sx1.90.90.90.1.1" class="ltx_text" style="font-size:80%;">SIR</span></dt>
<dd><span id="Sx1.90.90.90.4.1" class="ltx_text" style="font-size:80%;">source-to-interference ratio</span></dd>
<dt id="Sx1.91.91.91" class="ltx_glossaryentry"><span id="Sx1.91.91.91.1.1" class="ltx_text" style="font-size:80%;">dB</span></dt>
<dd><span id="Sx1.91.91.91.4.1" class="ltx_text" style="font-size:80%;">decibel</span></dd>
<dt id="Sx1.92.92.92" class="ltx_glossaryentry"><span id="Sx1.92.92.92.1.1" class="ltx_text" style="font-size:80%;">MIMO</span></dt>
<dd><span id="Sx1.92.92.92.4.1" class="ltx_text" style="font-size:80%;">multiple-input multiple-output</span></dd>
<dt id="Sx1.93.93.93" class="ltx_glossaryentry"><span id="Sx1.93.93.93.1.1" class="ltx_text" style="font-size:80%;">EVD</span></dt>
<dd><span id="Sx1.93.93.93.4.1" class="ltx_text" style="font-size:80%;">eigenvector decomposition</span></dd>
<dt id="Sx1.94.94.94" class="ltx_glossaryentry"><span id="Sx1.94.94.94.1.1" class="ltx_text" style="font-size:80%;">ReLU</span></dt>
<dd><span id="Sx1.94.94.94.4.1" class="ltx_text" style="font-size:80%;">rectified linear unit</span></dd>
<dt id="Sx1.95.95.95" class="ltx_glossaryentry"><span id="Sx1.95.95.95.1.1" class="ltx_text" style="font-size:80%;">RNN</span></dt>
<dd><span id="Sx1.95.95.95.4.1" class="ltx_text" style="font-size:80%;">recurrent neural network</span></dd>
<dt id="Sx1.96.96.96" class="ltx_glossaryentry"><span id="Sx1.96.96.96.1.1" class="ltx_text" style="font-size:80%;">MICE</span></dt>
<dd><span id="Sx1.96.96.96.4.1" class="ltx_text" style="font-size:80%;">multiple imputation by chained equations</span></dd>
<dt id="Sx1.97.97.97" class="ltx_glossaryentry"><span id="Sx1.97.97.97.1.1" class="ltx_text" style="font-size:80%;">ECochG</span></dt>
<dd><span id="Sx1.97.97.97.4.1" class="ltx_text" style="font-size:80%;">intracochlear electrocochleography</span></dd>
<dt id="Sx1.98.98.98" class="ltx_glossaryentry"><span id="Sx1.98.98.98.1.1" class="ltx_text" style="font-size:80%;">CM</span></dt>
<dd><span id="Sx1.98.98.98.4.1" class="ltx_text" style="font-size:80%;">cochlear microphonic</span></dd>
<dt id="Sx1.99.99.99" class="ltx_glossaryentry"><span id="Sx1.99.99.99.1.1" class="ltx_text" style="font-size:80%;">FFR</span></dt>
<dd><span id="Sx1.99.99.99.4.1" class="ltx_text" style="font-size:80%;">frequency following responses</span></dd>
<dt id="Sx1.101.101.101" class="ltx_glossaryentry"><math id="Sx1.100.100.100.1.m1.1" class="ltx_Math" alttext="f_{0}" display="inline"><semantics id="Sx1.100.100.100.1.m1.1a"><msub id="Sx1.100.100.100.1.m1.1.1" xref="Sx1.100.100.100.1.m1.1.1.cmml"><mi mathsize="80%" id="Sx1.100.100.100.1.m1.1.1.2" xref="Sx1.100.100.100.1.m1.1.1.2.cmml">f</mi><mn mathsize="80%" id="Sx1.100.100.100.1.m1.1.1.3" xref="Sx1.100.100.100.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Sx1.100.100.100.1.m1.1b"><apply id="Sx1.100.100.100.1.m1.1.1.cmml" xref="Sx1.100.100.100.1.m1.1.1"><csymbol cd="ambiguous" id="Sx1.100.100.100.1.m1.1.1.1.cmml" xref="Sx1.100.100.100.1.m1.1.1">subscript</csymbol><ci id="Sx1.100.100.100.1.m1.1.1.2.cmml" xref="Sx1.100.100.100.1.m1.1.1.2">𝑓</ci><cn type="integer" id="Sx1.100.100.100.1.m1.1.1.3.cmml" xref="Sx1.100.100.100.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.100.100.100.1.m1.1c">f_{0}</annotation></semantics></math></dt>
<dd><span id="Sx1.101.101.101.4.1" class="ltx_text" style="font-size:80%;">fundamental frequency</span></dd>
<dt id="Sx1.102.102.102" class="ltx_glossaryentry"><span id="Sx1.102.102.102.1.1" class="ltx_text" style="font-size:80%;">TFSC</span></dt>
<dd><span id="Sx1.102.102.102.4.1" class="ltx_text" style="font-size:80%;">temporal fine structure cues</span></dd>
<dt id="Sx1.103.103.103" class="ltx_glossaryentry"><span id="Sx1.103.103.103.1.1" class="ltx_text" style="font-size:80%;">RMSE</span></dt>
<dd><span id="Sx1.103.103.103.4.1" class="ltx_text" style="font-size:80%;">root mean square error</span></dd>
<dt id="Sx1.104.104.104" class="ltx_glossaryentry"><span id="Sx1.104.104.104.1.1" class="ltx_text" style="font-size:80%;">BM</span></dt>
<dd><span id="Sx1.104.104.104.4.1" class="ltx_text" style="font-size:80%;">basilar-membrane</span></dd>
<dt id="Sx1.105.105.105" class="ltx_glossaryentry"><span id="Sx1.105.105.105.1.1" class="ltx_text" style="font-size:80%;">ROI</span></dt>
<dd><span id="Sx1.105.105.105.4.1" class="ltx_text" style="font-size:80%;">regions of interest</span></dd>
<dt id="Sx1.106.106.106" class="ltx_glossaryentry"><span id="Sx1.106.106.106.1.1" class="ltx_text" style="font-size:80%;">DRL</span></dt>
<dd><span id="Sx1.106.106.106.4.1" class="ltx_text" style="font-size:80%;">deep reinforcement learning</span></dd>
<dt id="Sx1.107.107.107" class="ltx_glossaryentry"><span id="Sx1.107.107.107.1.1" class="ltx_text" style="font-size:80%;">RL</span></dt>
<dd><span id="Sx1.107.107.107.4.1" class="ltx_text" style="font-size:80%;">reinforcement learning</span></dd>
<dt id="Sx1.108.108.108" class="ltx_glossaryentry"><span id="Sx1.108.108.108.1.1" class="ltx_text" style="font-size:80%;">ABR</span></dt>
<dd><span id="Sx1.108.108.108.4.1" class="ltx_text" style="font-size:80%;">auditory brainstem responses</span></dd>
<dt id="Sx1.109.109.109" class="ltx_glossaryentry"><span id="Sx1.109.109.109.1.1" class="ltx_text" style="font-size:80%;">DBS</span></dt>
<dd><span id="Sx1.109.109.109.4.1" class="ltx_text" style="font-size:80%;">deep brain stimulation</span></dd>
<dt id="Sx1.110.110.110" class="ltx_glossaryentry"><span id="Sx1.110.110.110.1.1" class="ltx_text" style="font-size:80%;">FOX</span></dt>
<dd><span id="Sx1.110.110.110.4.1" class="ltx_text" style="font-size:80%;">fitting to outcomes expert</span></dd>
<dt id="Sx1.111.111.111" class="ltx_glossaryentry"><span id="Sx1.111.111.111.1.1" class="ltx_text" style="font-size:80%;">DSP</span></dt>
<dd><span id="Sx1.111.111.111.4.1" class="ltx_text" style="font-size:80%;">digital signal processor</span></dd>
<dt id="Sx1.112.112.112" class="ltx_glossaryentry"><span id="Sx1.112.112.112.1.1" class="ltx_text" style="font-size:80%;">MHINT</span></dt>
<dd><span id="Sx1.112.112.112.4.1" class="ltx_text" style="font-size:80%;">mandarin of hearing in noise
test</span></dd>
<dt id="Sx1.113.113.113" class="ltx_glossaryentry"><span id="Sx1.113.113.113.1.1" class="ltx_text" style="font-size:80%;">BCP</span></dt>
<dd><span id="Sx1.113.113.113.4.1" class="ltx_text" style="font-size:80%;">bern cocktail party</span></dd>
<dt id="Sx1.114.114.114" class="ltx_glossaryentry"><span id="Sx1.114.114.114.1.1" class="ltx_text" style="font-size:80%;">EST</span></dt>
<dd><span id="Sx1.114.114.114.4.1" class="ltx_text" style="font-size:80%;">effective stimulation threshold</span></dd>
<dt id="Sx1.115.115.115" class="ltx_glossaryentry"><span id="Sx1.115.115.115.1.1" class="ltx_text" style="font-size:80%;">ECAP</span></dt>
<dd><span id="Sx1.115.115.115.4.1" class="ltx_text" style="font-size:80%;">electrically evoked compound action potential</span></dd>
<dt id="Sx1.116.116.116" class="ltx_glossaryentry"><span id="Sx1.116.116.116.1.1" class="ltx_text" style="font-size:80%;">MRI</span></dt>
<dd><span id="Sx1.116.116.116.4.1" class="ltx_text" style="font-size:80%;">magnetic resonance
imaging</span></dd>
<dt id="Sx1.117.117.117" class="ltx_glossaryentry"><span id="Sx1.117.117.117.1.1" class="ltx_text" style="font-size:80%;">BSS</span></dt>
<dd><span id="Sx1.117.117.117.4.1" class="ltx_text" style="font-size:80%;">blind source separation</span></dd>
<dt id="Sx1.118.118.118" class="ltx_glossaryentry"><span id="Sx1.118.118.118.1.1" class="ltx_text" style="font-size:80%;">AEP</span></dt>
<dd><span id="Sx1.118.118.118.4.1" class="ltx_text" style="font-size:80%;">auditory evoked potential</span></dd>
<dt id="Sx1.119.119.119" class="ltx_glossaryentry"><span id="Sx1.119.119.119.1.1" class="ltx_text" style="font-size:80%;">CCA</span></dt>
<dd><span id="Sx1.119.119.119.4.1" class="ltx_text" style="font-size:80%;">canonical correlation analysis</span></dd>
<dt id="Sx1.120.120.120" class="ltx_glossaryentry"><span id="Sx1.120.120.120.1.1" class="ltx_text" style="font-size:80%;">ASE</span></dt>
<dd><span id="Sx1.120.120.120.4.1" class="ltx_text" style="font-size:80%;">average surface error</span></dd>
<dt id="Sx1.121.121.121" class="ltx_glossaryentry"><span id="Sx1.121.121.121.1.1" class="ltx_text" style="font-size:80%;">STOI</span></dt>
<dd><span id="Sx1.121.121.121.4.1" class="ltx_text" style="font-size:80%;">short-time objective intelligibility</span></dd>
<dt id="Sx1.122.122.122" class="ltx_glossaryentry"><span id="Sx1.122.122.122.1.1" class="ltx_text" style="font-size:80%;">TL</span></dt>
<dd><span id="Sx1.122.122.122.4.1" class="ltx_text" style="font-size:80%;">transfer learning</span></dd>
<dt id="Sx1.123.123.123" class="ltx_glossaryentry"><span id="Sx1.123.123.123.1.1" class="ltx_text" style="font-size:80%;">CIS</span></dt>
<dd><span id="Sx1.123.123.123.4.1" class="ltx_text" style="font-size:80%;">continuous interleaved sampling</span></dd>
<dt id="Sx1.124.124.124" class="ltx_glossaryentry"><span id="Sx1.124.124.124.1.1" class="ltx_text" style="font-size:80%;">DTL</span></dt>
<dd><span id="Sx1.124.124.124.4.1" class="ltx_text" style="font-size:80%;">deep transfer learning</span></dd>
<dt id="Sx1.125.125.125" class="ltx_glossaryentry"><span id="Sx1.125.125.125.1.1" class="ltx_text" style="font-size:80%;">FL</span></dt>
<dd><span id="Sx1.125.125.125.4.1" class="ltx_text" style="font-size:80%;">federated learning</span></dd>
<dt id="Sx1.126.126.126" class="ltx_glossaryentry"><span id="Sx1.126.126.126.1.1" class="ltx_text" style="font-size:80%;">CTC</span></dt>
<dd><span id="Sx1.126.126.126.4.1" class="ltx_text" style="font-size:80%;">connectionist temporal classification</span></dd>
<dt id="Sx1.127.127.127" class="ltx_glossaryentry"><span id="Sx1.127.127.127.1.1" class="ltx_text" style="font-size:80%;">BERT</span></dt>
<dd><span id="Sx1.127.127.127.4.1" class="ltx_text" style="font-size:80%;">bidirectional encoder representations from transformer</span></dd>
</dl>
</section>
<div class="ltx_pagination ltx_role_end_3_columns"></div>
</section>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the symphony of modern technology,  <a href="#Sx1.49.49.49"><span href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">automatic speech recognition</span></span></span></a> (<a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a>) emerges as a master, orchestrating a seamless interaction between humans and machines. This transformative technology has quietly become an integral part of our daily lives, influencing how we communicate, access information, and even navigate the intricacies of healthcare. The significance of ASR extends beyond its role in facilitating human-computer interaction; it permeates diverse applications such as voice assistants and virtual agents, speech-to-text conversion, identity verification, and holds particular promise in the realm of biomedical research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> bridges the gap between spoken language and digital communication, enabling the conversion of spoken words into written text with remarkable accuracy. The pervasiveness of ASR technology is evident in the devices we use daily—smartphones, smart speakers, and voice-activated virtual assistants—all seamlessly responding to our spoken commands and queries. The convenience it brings to our lives is undeniable, offering a hands-free and efficient mode of interaction that has become second nature.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> is pivotal in authentication systems, safeguarding the security and privacy of sensitive information. The integrity of audio speech can be verified through techniques-based ASR such as adversarial attacks detections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, steganalysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, speech biometric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and more. Beyond the realm of communication, <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> finds itself at the heart of various applications, each playing a unique role in different domains. <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Speaker recognition</span>, a facet of <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a>, is not merely confined to enhancing security measures. It has evolved into a versatile tool employed in healthcare, where the identification of individuals through their unique vocal signatures holds promise for personalized patient care. This is particularly relevant in scenarios where quick and secure authentication is crucial, such as accessing medical records or authorizing medical procedures. <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Event recognition</span>, another dimension of <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a>, is a game-changer in sectors ranging from security to healthcare. In the former, <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> algorithms analyze audio data to automatically detect and categorize specific events, reinforcing surveillance capabilities. In healthcare, event recognition becomes a powerful tool for monitoring and early detection of health-related events. For instance, in the context of cardiac health, <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> can aid in identifying anomalies in heart sounds, potentially enabling early intervention and preventive measures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">Source separation</span>, the ability to discern and isolate individual sound sources from complex audio signals, is a boon in fields like entertainment and music production. However, its significance extends into the realm of biomedical research, where <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> plays a pivotal role in decoding the intricate language of physiological signals. In the context of <a href="#Sx1.4.4.4"><span href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">cochlear implant</span>s</span></span></a>, source separation becomes a critical component in enhancing the auditory experience for individuals with hearing impairments.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The <span id="S1.p3.1.1" class="ltx_ERROR undefined">\Acp</span>CI are designed to restore hearing in individuals with severe hearing loss or deafness, rely on <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> for optimizing their functionality. <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> contributes significantly to the improvement of speech perception in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users by enhancing the processing and interpretation of auditory signals. <span id="S1.p3.1.2" class="ltx_ERROR undefined">\Acp</span>CI work by converting sound waves into electrical signals that stimulate the auditory nerve, bypassing damaged parts of the inner ear. <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> complements this process by aiding in the recognition and translation of spoken language. The technology plays a crucial role in optimizing speech understanding for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users by refining the interpretation of varied speech patterns, tones, and nuances. Moreover, <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> in the context of <span id="S1.p3.1.3" class="ltx_ERROR undefined">\Acp</span>CI extends beyond basic speech recognition. It contributes to the recognition of environmental sounds, facilitating a more immersive auditory experience for individuals with hearing impairments. This is particularly significant in enhancing the quality of life for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients, allowing them to navigate and engage with their surroundings more effectively.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related work</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Many reviews have been written in the context of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> discussed the advantages offered by  <a href="#Sx1.5.5.5"><span href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">machine learning</span></span></span></a> (<a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>) to cochlear implantation, such as analyzing data to personalize treatment strategies. It enhances accuracy in speech processing optimization, surgical anatomy location prediction, and electrode placement discrimination. Besides, it delves into its applications, including optimizing cochlear implant fitting, predicting patient threshold levels, and automating image-guided CI surgery. The review discusses some open novel opportunities for research, emphasizing the need for high-quality data inputs and addressing concerns about algorithm transparency in clinical decision-making for improved patient care. Similarly, the review written by Manero et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> details some benefits of employing  <a href="#Sx1.2.2.2"><span href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">artificial intelligent</span></span></span></a> (<a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>) in enhancing <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technology, involving adaptive sound processing, acoustic scene classification, and auditory scene analysis. The authors discuss AI-driven advancements aiming to optimize sound signals, adapt to diverse environments, and improve speech perception for individuals with hearing loss, ultimately enhancing their overall quality of life. Also, the review <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> explores three main topics: direct-speech neuroprosthesis, which involves decoding speech from the sensorimotor cortex using <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>, including the synthesis of produced speech from brain activity. The second topic is a top-down exploration of pediatric cochlear implantation using <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, delving into the applications of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> in pediatric cochlear implantation. Lastly, the paper considers the potential of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> to solve the hearing-in-noise problem, examining its capabilities in addressing challenges related to hearing in noisy environments. Moreover, the review <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> critically examines the current landscape of tele-audiology practices, highlighting both their constraints and potential opportunities. Specifically, it explores intervention and rehabilitation efforts for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, focusing on remote programming and the concept of self-fitting <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. Recently, a review written by Henry et al. in 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> conducts a comprehensive review of noise reduction algorithms employed in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. Maintaining a general classification based on the number of microphones used—single or multiple channels—the analysis extends to incorporate recent studies showcasing a growing interest in <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> techniques. The review culminates with an exploration of potential research avenues that hold promise for future advancements in the field. Table <a href="#S1.T1" title="Table 1 ‣ 1.1 Related work ‣ 1 Introduction ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> offers a comparative analysis of the proposed review in contrast to other discussed AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> reviews and surveys.</p>
</div>
<figure id="S1.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of this review against other existing AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> reviews and surveys. Tick marks (✓) indicate that a particular field has been considered, while cross marks (<span id="S1.T1.4.2.1" class="ltx_text">✗</span>) indicate that a field has been left unaddressed. The symbol (★) indicates that the most critical concerns of a field have not been addressed.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S1.T1.5" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S1.T1.5.1" class="ltx_tr">
<td id="S1.T1.5.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S1.T1.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.1.1.1" class="ltx_p">Ref.</span>
</span>
</td>
<td id="S1.T1.5.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S1.T1.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.2.1.1" class="ltx_p">Year</span>
</span>
</td>
<td id="S1.T1.5.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:128.0pt;">
<span id="S1.T1.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.1.3.1.1" class="ltx_p">Description</span>
</span>
</td>
<td id="S1.T1.5.1.4" class="ltx_td ltx_align_center ltx_border_t">Backg.</td>
<td id="S1.T1.5.1.5" class="ltx_td ltx_align_center ltx_border_t">TCI</td>
<td id="S1.T1.5.1.6" class="ltx_td ltx_align_center ltx_border_t">Metrics</td>
<td id="S1.T1.5.1.7" class="ltx_td ltx_align_center ltx_border_t">datasets</td>
<td id="S1.T1.5.1.8" class="ltx_td ltx_align_center ltx_border_t">MLT</td>
<td id="S1.T1.5.1.9" class="ltx_td ltx_align_center ltx_border_t">DLT</td>
<td id="S1.T1.5.1.10" class="ltx_td ltx_align_center ltx_border_t">Apps</td>
<td id="S1.T1.5.1.11" class="ltx_td ltx_align_center ltx_border_t">RGCs</td>
<td id="S1.T1.5.1.12" class="ltx_td ltx_align_center ltx_border_t">FDs</td>
</tr>
<tr id="S1.T1.5.2" class="ltx_tr">
<td id="S1.T1.5.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S1.T1.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.2.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
</span>
</td>
<td id="S1.T1.5.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S1.T1.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.2.2.1.1" class="ltx_p">2019</span>
</span>
</td>
<td id="S1.T1.5.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:128.0pt;">
<span id="S1.T1.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.2.3.1.1" class="ltx_p">The intersection of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a></span>
</span>
</td>
<td id="S1.T1.5.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.5.2.4.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.5.2.5.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.5.2.6.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.5.2.7.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.2.8" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.5.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.5.2.9.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.2.10" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S1.T1.5.2.11" class="ltx_td ltx_align_center ltx_border_t">★</td>
<td id="S1.T1.5.2.12" class="ltx_td ltx_align_center ltx_border_t">★</td>
</tr>
<tr id="S1.T1.5.3" class="ltx_tr">
<td id="S1.T1.5.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.3.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span>
</span>
</td>
<td id="S1.T1.5.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.3.2.1.1" class="ltx_p">2022</span>
</span>
</td>
<td id="S1.T1.5.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:128.0pt;">
<span id="S1.T1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.3.3.1.1" class="ltx_p">Explores materials and devices designed for medical care</span>
</span>
</td>
<td id="S1.T1.5.3.4" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.4.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.5" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.5.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.6" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.6.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.7" class="ltx_td ltx_align_center">★</td>
<td id="S1.T1.5.3.8" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.8.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.9" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.9.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.3.11" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.11.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.3.12" class="ltx_td ltx_align_center"><span id="S1.T1.5.3.12.1" class="ltx_text">✗</span></td>
</tr>
<tr id="S1.T1.5.4" class="ltx_tr">
<td id="S1.T1.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.4.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span>
</span>
</td>
<td id="S1.T1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.4.2.1.1" class="ltx_p">2022</span>
</span>
</td>
<td id="S1.T1.5.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:128.0pt;">
<span id="S1.T1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.4.3.1.1" class="ltx_p">AI in otolaryngology and communication sciences</span>
</span>
</td>
<td id="S1.T1.5.4.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.4.5" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.5.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.4.6" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.6.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.4.7" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.7.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.4.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.4.9" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.9.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.4.10" class="ltx_td ltx_align_center">★</td>
<td id="S1.T1.5.4.11" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.11.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.4.12" class="ltx_td ltx_align_center"><span id="S1.T1.5.4.12.1" class="ltx_text">✗</span></td>
</tr>
<tr id="S1.T1.5.5" class="ltx_tr">
<td id="S1.T1.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.5.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span>
</span>
</td>
<td id="S1.T1.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.5.2.1.1" class="ltx_p">2022</span>
</span>
</td>
<td id="S1.T1.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:128.0pt;">
<span id="S1.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.5.3.1.1" class="ltx_p">Remote audiology services</span>
</span>
</td>
<td id="S1.T1.5.5.4" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.4.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.5" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.5.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.6" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.6.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.7" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.7.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.8" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.8.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.9" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.9.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.10" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.5.11" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.11.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.5.12" class="ltx_td ltx_align_center"><span id="S1.T1.5.5.12.1" class="ltx_text">✗</span></td>
</tr>
<tr id="S1.T1.5.6" class="ltx_tr">
<td id="S1.T1.5.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.6.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span>
</span>
</td>
<td id="S1.T1.5.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S1.T1.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.6.2.1.1" class="ltx_p">2023</span>
</span>
</td>
<td id="S1.T1.5.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:128.0pt;">
<span id="S1.T1.5.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.6.3.1.1" class="ltx_p">Signal processing in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> for noise reduction</span>
</span>
</td>
<td id="S1.T1.5.6.4" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.6.5" class="ltx_td ltx_align_center"><span id="S1.T1.5.6.5.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.6.6" class="ltx_td ltx_align_center"><span id="S1.T1.5.6.6.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.6.7" class="ltx_td ltx_align_center"><span id="S1.T1.5.6.7.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.6.8" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.5.6.9" class="ltx_td ltx_align_center"><span id="S1.T1.5.6.9.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.6.10" class="ltx_td ltx_align_center">★</td>
<td id="S1.T1.5.6.11" class="ltx_td ltx_align_center"><span id="S1.T1.5.6.11.1" class="ltx_text">✗</span></td>
<td id="S1.T1.5.6.12" class="ltx_td ltx_align_center">★</td>
</tr>
<tr id="S1.T1.5.7" class="ltx_tr">
<td id="S1.T1.5.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:14.2pt;">
<span id="S1.T1.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.7.1.1.1" class="ltx_p">Our</span>
</span>
</td>
<td id="S1.T1.5.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:14.2pt;">
<span id="S1.T1.5.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.7.2.1.1" class="ltx_p">2024</span>
</span>
</td>
<td id="S1.T1.5.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:128.0pt;">
<span id="S1.T1.5.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.5.7.3.1.1" class="ltx_p">Advanced <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a></span>
</span>
</td>
<td id="S1.T1.5.7.4" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.5" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.6" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.7" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.8" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.9" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.10" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.11" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.5.7.12" class="ltx_td ltx_align_center ltx_border_b">✓</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.T1.6" class="ltx_p ltx_figure_panel ltx_align_center">Abbreviations: Taxonomy in CI (TCI), ML-based techniques (MLT), DL-based techniques (DLT), Applications (apps), Research gaps and challenges (RGCs) , Future directions (FDs)</p>
</div>
</div>
</figure>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Statistics on investigated papers</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Recently, there has been a surge in publications related to AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. The review methodology entails defining the search strategy and study selection criteria. Criteria for inclusion, like keyword relevance and impact, shape the quality assessment protocol. A comprehensive search was conducted on databases such as Scopus and Web of Science. Keywords were extracted for theme clustering, resulting in a formulated query to gather advanced AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> studies. The research query fetches references from papers containing the keywords "Cochlear implant" or "Hearing loss" and "Artificial intelligence" in their abstracts, titles, or authors’ keywords. It subsequently refines these papers, focusing on those that also include "Machine learning," "Deep learning," or "Reinforcement learning." Figure <a href="#S1.F1" title="Figure 1 ‣ 1.2 Statistics on investigated papers ‣ 1 Introduction ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the most frequently used keywords by the authors in the titles, abstracts, and keywords of the selected papers.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.15442/assets/word-cloud.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="342" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Top keywords in AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> research field.</span></figcaption>
</figure>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ 1.2 Statistics on investigated papers ‣ 1 Introduction ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the distribution of these papers by type and publication year, encompassing articles, conference papers, and reviews. Additionally, it showcases the percentage of publications from before 2015 to 2023, with a total count of 99 papers. Notably, the peak in publications within this field is observed in 2023 (Figure <a href="#S1.F2" title="Figure 2 ‣ 1.2 Statistics on investigated papers ‣ 1 Introduction ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a)), underscoring the heightened interest among researchers. Furthermore, the predominant publication type is research articles, followed by conference papers (Figure <a href="#S1.F2" title="Figure 2 ‣ 1.2 Statistics on investigated papers ‣ 1 Introduction ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b)).</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2403.15442/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Bibliometrics analysis of the papers included in this review. (a) Papers distribution over the last years. (b) Percentage breakdown of paper types included in this review.</span></figcaption>
</figure>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Motivation and contribution</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">The motivation behind conducting a comprehensive review on <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> stems from the imperative to critically assess and consolidate the current state of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> applications in this crucial field. <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> have revolutionized auditory rehabilitation for individuals with hearing impairment, and integrating <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and  <a href="#Sx1.3.3.3"><span href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep learning</span></span></span></a> (<a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>) techniques holds immense potential for further advancements. This review seeks to fill a significant gap in the existing literature by providing a detailed analysis of recent <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> frameworks.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">The primary objective is to present a nuanced understanding of the landscape, categorizing frameworks based on <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> methodologies, available datasets, and key metrics. By addressing this gap, the review aims to offer valuable insights for researchers, clinicians, and technologists involved in the development and improvement of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technologies. Furthermore, the exploration of advanced <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> algorithms, such as transformers and  <a href="#Sx1.107.107.107"><span href="#Sx1.107.107.107" title="reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">reinforcement learning</span></span></span></a> (<a href="#Sx1.107.107.107"><abbr href="#Sx1.107.107.107" title="reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RL</span></span></abbr></a>), in the context of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, underscores the potential for transformative breakthroughs. Ultimately, this research review aspires to contribute to the enhancement of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technologies, fostering innovation and improving the quality of life for individuals with hearing impairment. The principal contributions of this paper can be succinctly outlined as follows:</p>
</div>
<div id="S1.SS3.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Detailing the assessment metrics associated with <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, and elucidating the extensively utilized datasets, whether publicly accessible or generated, employed to validate AI-based <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> methodologies.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The implementation of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, along with a comprehensive elucidation of the taxonomy encompassing <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based CI, is thoroughly expounded upon. Additionally, recommended frameworks for AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> are thoroughly discussed and succinctly summarized in tables for enhanced clarity.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Providing detailed insights into the applications of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> within the domain of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, encompassing functions such as denoising and speech enhancement, segmentation, thresholding, imaging, as well as <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> localization, along with various other functionalities.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Delve into the existing gaps in AI-driven <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, offer insights, and propose novel ideas to address these gaps. Additionally, explore potential avenues for future research to deepen comprehension and provide valuable guidance for subsequent investigations.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS3.p4" class="ltx_para">
<p id="S1.SS3.p4.1" class="ltx_p">The subsequent sections are organized as follows: Section <a href="#S2" title="2 Background in speech processing for ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> delves into the background in speech processing for, outlining datasets and metrics. Section <a href="#S3" title="3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> discusses the methodology employed for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> based on <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>. Section <a href="#S4" title="4 Applications of DL-based medical ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the medical applications and impact of applying <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> on <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. Section 5 offers a comprehensive discussion research gaps, future directions, and perspectives. Finally, Section 6 concludes the paper with implications and future research directions.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background in speech processing for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> </h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speech processing for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> are electronic devices that can be implanted in one or both ears to restore some level of hearing for individuals with partial or severe deafness. <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> comprise an external part with a microphone and speech processor and an internal part with a receiver-stimulator and electrode array as shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Speech processing for ‣ 2 Background in speech processing for ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. They convert sounds into electrical signals, stimulating the auditory nerve to enable sound perception in individuals with profound hearing loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2403.15442/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="142" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Illustration of a <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> depicting the components situated externally and internally within the device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The incoming sound is divided into multiple frequency channels using bandpass filters and then processed by envelope detectors. Non-linear compressors adjust the dynamic range of the envelope for each patient. The compressed envelope amplitudes are then utilized to modulate a fixed-rate biphasic carrier signal.
A current source converts voltage into pulse trains of current, which are delivered to electrodes placed along the cochlea in a non-overlapping manner. This stimulation method is called  <a href="#Sx1.123.123.123"><span href="#Sx1.123.123.123" title="continuous interleaved sampling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">continuous interleaved sampling</span></span></span></a> (<a href="#Sx1.123.123.123"><abbr href="#Sx1.123.123.123" title="continuous interleaved sampling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CIS</span></span></abbr></a>). Another coding strategy, known as  <a href="#Sx1.28.28.28"><span href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">advanced combination encoder</span></span></span></a> (<a href="#Sx1.28.28.28"><abbr href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE</span></span></abbr></a>), uses a greater number of channels and dynamically selects the "n-of-m" bands with the largest envelope amplitudes (prior to compression). Only the corresponding "n" electrodes are stimulated. A popular device widely used for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, such as the cochlear nucleus, typically has 22 channels.
The sound processor, which usually contains a microphone, battery, and other components, can be worn either behind-the-ear (BTE) or off-the-ear (OTE). A headpiece holds a transmitter coil, positioned externally above the ear, while internally, a receiver coil, stimulator, and electrode array are implanted. The SP includes a digital signal processor (DSP) with memory units (maps) that store patient-specific information. An audiologist configures these maps during the fitting process, adjusting thresholds for each electrode, including T-Levels (the softest current levels audible to the <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> user) and C/M-Levels (current levels perceived as comfortably loud), as well as the stimulation rate or programming strategy. Data (pulse amplitude, pulse duration, pulse gap, etc.) and power are sent through the skull via an radio frequency signal from the transmitter coil to the receiver coil. The stimulator decodes the received bitstream and converts it into electric currents to be delivered to the cochlear electrodes. High-frequency signals stimulate electrodes near the base of the cochlea, while low-frequency signals stimulate electrodes near the apex.
The <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> stimulates the auditory nerve afferents, which connect to the central auditory pathways. However, compared to individuals with normal hearing (NH), <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users face more difficulties in speech perception, particularly in noisy environments. Hearing loss can be caused by various factors, including natural aging, genetic predisposition, exposure to loud sounds, and medical treatments. Damage to the hair cells in the inner ear often leads to a reduced dynamic range of hearing, as well as decreased frequency selectivity and discriminative ability in speech processing. To evaluate the effectiveness of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> in speech perception amid noise, listening tests involving both normal hearing individuals and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users are commonly conducted. These tests typically employ a combination of speech utterances from a recognized speech corpus and background noises like speech-weighted noise and babble. Alternatively, vocoder simulations can be utilized alongside speech intelligibility metrics. While sentence-based tests are frequently employed, other stimuli such as vowels, consonants, and phonemes are also used. As a result, noise reduction techniques are increasingly employed to enhance the performance of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> in challenging environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Researchers have utilized numerous datasets to validate their proposed schemes, comprising both widely recognized publicly available sets and locally generated ones. These datasets fall into two categories: speech or images. Table <a href="#S2.T2" title="Table 2 ‣ 2.2 Datasets ‣ 2 Background in speech processing for ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a summary of these datasets, detailing their characteristics, citing studies that have utilized them, and indicating their availability through links or references.</p>
</div>
<figure id="S2.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.3.2" class="ltx_text" style="font-size:90%;">List of publicly available datasets used for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> applications</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.T2.4" class="ltx_p ltx_figure_panel">.


<span id="S2.T2.4.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T2.4.1.1" class="ltx_tr">
<span id="S2.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T2.4.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></span>
<span id="S2.T2.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S2.T2.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.1.2.1.1" class="ltx_p"><span id="S2.T2.4.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">NoS</span></span>
</span></span>
<span id="S2.T2.4.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S2.T2.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.1.3.1.1" class="ltx_p"><span id="S2.T2.4.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">NoC</span></span>
</span></span>
<span id="S2.T2.4.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:313.0pt;">
<span id="S2.T2.4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.1.4.1.1" class="ltx_p"><span id="S2.T2.4.1.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Characteristics</span></span>
</span></span>
<span id="S2.T2.4.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S2.T2.4.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.1.5.1.1" class="ltx_p"><span id="S2.T2.4.1.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Used by</span></span>
</span></span>
<span id="S2.T2.4.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:19.9pt;">
<span id="S2.T2.4.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.1.6.1.1" class="ltx_p"><span id="S2.T2.4.1.1.6.1.1.1" class="ltx_text" style="font-size:70%;">Source</span></span>
</span></span></span>
<span id="S2.T2.4.1.2" class="ltx_tr">
<span id="S2.T2.4.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:5.69046pt;"><a href="#Sx1.112.112.112"><abbr href="#Sx1.112.112.112" title="mandarin of hearing in noise
test" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MHINT</span></span></abbr></a></span>
<span id="S2.T2.4.1.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.2.2.1.1" class="ltx_p"><span id="S2.T2.4.1.2.2.1.1.1" class="ltx_text" style="font-size:70%;">2560 </span>
<br class="ltx_break"></span>
</span></span>
<span id="S2.T2.4.1.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.2.3.1.1" class="ltx_p"><span id="S2.T2.4.1.2.3.1.1.1" class="ltx_text" style="font-size:70%;">8</span></span>
</span></span>
<span id="S2.T2.4.1.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:313.0pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.2.4.1.1" class="ltx_p"><span id="S2.T2.4.1.2.4.1.1.1" class="ltx_text" style="font-size:70%;">It is a test resource that was developed in two versions: MHINT-M for use in Mainland China and MHINT-T for use in Taiwan. The development of MHINT took into consideration the tonal nature of Mandarin, recognizing the importance of lexical tone in designing the test.</span></span>
</span></span>
<span id="S2.T2.4.1.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.2.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.2.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.T2.4.1.2.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.2.5.1.1.3" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.2.5.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.T2.4.1.2.5.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.2.5.1.1.6" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.2.5.1.1.7.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.T2.4.1.2.5.1.1.8.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:19.9pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.2.6.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.2.6.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S2.T2.4.1.2.6.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span></span>
<span id="S2.T2.4.1.3" class="ltx_tr">
<span id="S2.T2.4.1.3.1" class="ltx_td ltx_align_left" style="padding-bottom:5.69046pt;"><span id="S2.T2.4.1.3.1.1" class="ltx_text" style="font-size:70%;">PhonDat 1</span></span>
<span id="S2.T2.4.1.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.3.2.1.1" class="ltx_p"><span id="S2.T2.4.1.3.2.1.1.1" class="ltx_text" style="font-size:70%;">21587</span></span>
</span></span>
<span id="S2.T2.4.1.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.3.3.1.1" class="ltx_p"><span id="S2.T2.4.1.3.3.1.1.1" class="ltx_text" style="font-size:70%;">201</span></span>
</span></span>
<span id="S2.T2.4.1.3.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.3.4.1.1" class="ltx_p"><span id="S2.T2.4.1.3.4.1.1.1" class="ltx_text" style="font-size:70%;">It is from the Bavarian Archive For Speech Signals, contains 21.4 hours of speech data, and it is orthographically transcribed and phonemically annotated.</span></span>
</span></span>
<span id="S2.T2.4.1.3.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.3.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.3.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.T2.4.1.3.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.3.5.1.1.3" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.3.5.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.T2.4.1.3.5.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.3.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.3.6.1.1" class="ltx_p"><span id="S2.T2.4.1.3.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.phonetik.uni-muenchen.de/Bas/BasPD1eng.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.phonetik.uni-muenchen.de/Bas/BasPD1eng.html</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.4" class="ltx_tr">
<span id="S2.T2.4.1.4.1" class="ltx_td ltx_align_left" style="padding-bottom:5.69046pt;"><span id="S2.T2.4.1.4.1.1" class="ltx_text" style="font-size:70%;">TIMIT</span></span>
<span id="S2.T2.4.1.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.4.2.1.1" class="ltx_p"><span id="S2.T2.4.1.4.2.1.1.1" class="ltx_text" style="font-size:70%;">6300</span></span>
</span></span>
<span id="S2.T2.4.1.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.4.3.1.1" class="ltx_p"><span id="S2.T2.4.1.4.3.1.1.1" class="ltx_text" style="font-size:70%;">630</span></span>
</span></span>
<span id="S2.T2.4.1.4.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.4.4.1.1" class="ltx_p"><span id="S2.T2.4.1.4.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset consists of phonemically and lexically transcribed speech from American English speakers belonging to diverse demographics and dialects. It provides comprehensive information with time-aligned orthographic, phonetic, and word transcriptions. Additionally, each utterance is accompanied by its corresponding 16-bit, 16kHz speech waveform file, ensuring a complete and detailed dataset for analysis and experimentation in </span><a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a><span id="S2.T2.4.1.4.4.1.1.2" class="ltx_text" style="font-size:70%;"> and acoustic-phonetic studies.</span></span>
</span></span>
<span id="S2.T2.4.1.4.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.4.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.4.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.T2.4.1.4.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.4.5.1.1.3" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.4.5.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.T2.4.1.4.5.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.4.5.1.1.6" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.4.5.1.1.7.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S2.T2.4.1.4.5.1.1.8.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.4.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;padding-bottom:5.69046pt;">
<span id="S2.T2.4.1.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.4.6.1.1" class="ltx_p"><span id="S2.T2.4.1.4.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/mfekadu/darpa-timit-acousticphonetic-continuous-speech</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.5" class="ltx_tr">
<span id="S2.T2.4.1.5.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.5.1.1" class="ltx_text" style="font-size:70%;">GSC</span></span>
<span id="S2.T2.4.1.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.5.2.1.1" class="ltx_p"><span id="S2.T2.4.1.5.2.1.1.1" class="ltx_text" style="font-size:70%;">18 Hours</span></span>
</span></span>
<span id="S2.T2.4.1.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.5.3.1.1" class="ltx_p"><span id="S2.T2.4.1.5.3.1.1.1" class="ltx_text" style="font-size:70%;">30</span></span>
</span></span>
<span id="S2.T2.4.1.5.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.5.4.1.1" class="ltx_p"><span id="S2.T2.4.1.5.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset was gathered using crowd-sourcing. It consists of 65,000 recordings, each lasting one second, and contains 30 brief words. Among these words, 20 commonly used ones were spoken five times by the majority of participants, while 10 other words (considered unfamiliar) were spoken only once.</span></span>
</span></span>
<span id="S2.T2.4.1.5.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.5.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.5.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S2.T2.4.1.5.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.5.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.5.6.1.1" class="ltx_p"><span id="S2.T2.4.1.5.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.6" class="ltx_tr">
<span id="S2.T2.4.1.6.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.6.1.1" class="ltx_text" style="font-size:70%;">WSJ0-2mix</span></span>
<span id="S2.T2.4.1.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.6.2.1.1" class="ltx_p"><span id="S2.T2.4.1.6.2.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
<span id="S2.T2.4.1.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.6.3.1.1" class="ltx_p"><span id="S2.T2.4.1.6.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
<span id="S2.T2.4.1.6.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.6.4.1.1" class="ltx_p"><span id="S2.T2.4.1.6.4.1.1.1" class="ltx_text" style="font-size:70%;">This dataset is a collection of mixed speech recordings that are used for speech recognition. It utilizes utterances from the Wall Street Journal (WSJ0) corpus to create these mixtures.</span></span>
</span></span>
<span id="S2.T2.4.1.6.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.6.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S2.T2.4.1.6.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.6.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.6.6.1.1" class="ltx_p"><span id="S2.T2.4.1.6.6.1.1.1" class="ltx_text" style="font-size:70%;">Link </span><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.merl.com/research/highlights/deep-clustering" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.merl.com/research/highlights/deep-clustering</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.7" class="ltx_tr">
<span id="S2.T2.4.1.7.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.7.1.1" class="ltx_text" style="font-size:70%;">LibriVox</span></span>
<span id="S2.T2.4.1.7.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.7.2.1.1" class="ltx_p"><span id="S2.T2.4.1.7.2.1.1.1" class="ltx_text" style="font-size:70%;">547 Hours</span></span>
</span></span>
<span id="S2.T2.4.1.7.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.7.3.1.1" class="ltx_p"><span id="S2.T2.4.1.7.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
<span id="S2.T2.4.1.7.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.7.4.1.1" class="ltx_p"><span id="S2.T2.4.1.7.4.1.1.1" class="ltx_text" style="font-size:70%;">The LibriVox dataset consists of sentence-aligned German audio, text, and English translations from audio books.</span></span>
</span></span>
<span id="S2.T2.4.1.7.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.7.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.7.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.T2.4.1.7.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.7.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.7.6.1.1" class="ltx_p"><span id="S2.T2.4.1.7.6.1.1.1" class="ltx_text" style="font-size:70%;">Link </span><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://librivox.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://librivox.org/</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.8" class="ltx_tr">
<span id="S2.T2.4.1.8.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.8.1.1" class="ltx_text" style="font-size:70%;">HSM</span></span>
<span id="S2.T2.4.1.8.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.8.2.1.1" class="ltx_p"><span id="S2.T2.4.1.8.2.1.1.1" class="ltx_text" style="font-size:70%;">20</span></span>
</span></span>
<span id="S2.T2.4.1.8.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.8.3.1.1" class="ltx_p"><span id="S2.T2.4.1.8.3.1.1.1" class="ltx_text" style="font-size:70%;">30</span></span>
</span></span>
<span id="S2.T2.4.1.8.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.8.4.1.1" class="ltx_p"><span id="S2.T2.4.1.8.4.1.1.1" class="ltx_text" style="font-size:70%;">The development of the German HSM Sentence Test aimed to provide an ample amount of test sentences for the repeated assessment of speech comprehension among </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S2.T2.4.1.8.4.1.1.2" class="ltx_text" style="font-size:70%;"> users.</span></span>
</span></span>
<span id="S2.T2.4.1.8.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.8.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.8.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.T2.4.1.8.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.8.5.1.1.3" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.8.5.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.T2.4.1.8.5.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.8.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.8.6.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.8.6.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S2.T2.4.1.8.6.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span></span>
<span id="S2.T2.4.1.9" class="ltx_tr">
<span id="S2.T2.4.1.9.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.9.1.1" class="ltx_text" style="font-size:70%;">CEC1</span></span>
<span id="S2.T2.4.1.9.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.9.2.1.1" class="ltx_p"><span id="S2.T2.4.1.9.2.1.1.1" class="ltx_text" style="font-size:70%;">6000</span></span>
</span></span>
<span id="S2.T2.4.1.9.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.9.3.1.1" class="ltx_p"><span id="S2.T2.4.1.9.3.1.1.1" class="ltx_text" style="font-size:70%;">24</span></span>
</span></span>
<span id="S2.T2.4.1.9.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.9.4.1.1" class="ltx_p"><span id="S2.T2.4.1.9.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset features simulated living rooms with static sources, including a single target speaker, interferer (competing talker or noise), and a large target speech database of English sentences produced by 40 British English speakers.</span></span>
</span></span>
<span id="S2.T2.4.1.9.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.9.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.9.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.T2.4.1.9.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.9.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.9.6.1.1" class="ltx_p"><span id="S2.T2.4.1.9.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://claritychallenge.org/clarity_CEC1_doc/docs/cec1_data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://claritychallenge.org/clarity_CEC1_doc/docs/cec1_data</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.10" class="ltx_tr">
<span id="S2.T2.4.1.10.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.10.1.1" class="ltx_text" style="font-size:70%;">DEMAND</span></span>
<span id="S2.T2.4.1.10.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.10.2.1.1" class="ltx_p"><span id="S2.T2.4.1.10.2.1.1.1" class="ltx_text" style="font-size:70%;">560</span></span>
</span></span>
<span id="S2.T2.4.1.10.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.10.3.1.1" class="ltx_p"><span id="S2.T2.4.1.10.3.1.1.1" class="ltx_text" style="font-size:70%;">6</span></span>
</span></span>
<span id="S2.T2.4.1.10.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.10.4.1.1" class="ltx_p"><span id="S2.T2.4.1.10.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset consists of 15 recordings capturing acoustic noise in various environments. These recordings were made using a 16-channel array, with microphone distances ranging from 5 cm to 21.8 cm.</span></span>
</span></span>
<span id="S2.T2.4.1.10.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.10.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.10.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.T2.4.1.10.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.10.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.10.6.1.1" class="ltx_p"><span id="S2.T2.4.1.10.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://www.kaggle.com/datasets/chrisfilo/demand" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/chrisfilo/demand</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.11" class="ltx_tr">
<span id="S2.T2.4.1.11.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.11.1.1" class="ltx_text" style="font-size:70%;">THCHS-30</span></span>
<span id="S2.T2.4.1.11.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.11.2.1.1" class="ltx_p"><span id="S2.T2.4.1.11.2.1.1.1" class="ltx_text" style="font-size:70%;">35 Hours</span></span>
</span></span>
<span id="S2.T2.4.1.11.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.11.3.1.1" class="ltx_p"><span id="S2.T2.4.1.11.3.1.1.1" class="ltx_text" style="font-size:70%;">50</span></span>
</span></span>
<span id="S2.T2.4.1.11.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.11.4.1.1" class="ltx_p"><span id="S2.T2.4.1.11.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset is a free Chinese speech corpus accompanied by resources such as lexicon and language models.</span></span>
</span></span>
<span id="S2.T2.4.1.11.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.11.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.11.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S2.T2.4.1.11.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.11.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.11.6.1.1" class="ltx_p"><span id="S2.T2.4.1.11.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://www.openslr.org/18/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.openslr.org/18/</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.12" class="ltx_tr">
<span id="S2.T2.4.1.12.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.12.1.1" class="ltx_text" style="font-size:70%;">BCP</span></span>
<span id="S2.T2.4.1.12.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.12.2.1.1" class="ltx_p"><span id="S2.T2.4.1.12.2.1.1.1" class="ltx_text" style="font-size:70%;">55938</span></span>
</span></span>
<span id="S2.T2.4.1.12.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.12.3.1.1" class="ltx_p"><span id="S2.T2.4.1.12.3.1.1.1" class="ltx_text" style="font-size:70%;">20</span></span>
</span></span>
<span id="S2.T2.4.1.12.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.12.4.1.1" class="ltx_p"><span id="S2.T2.4.1.12.4.1.1.1" class="ltx_text" style="font-size:70%;">The  </span><a href="#Sx1.113.113.113"><span href="#Sx1.113.113.113" title="bern cocktail party" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">bern cocktail party</span></span></span></a><span id="S2.T2.4.1.12.4.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.113.113.113"><abbr href="#Sx1.113.113.113" title="bern cocktail party" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">BCP</span></span></abbr></a><span id="S2.T2.4.1.12.4.1.1.3" class="ltx_text" style="font-size:70%;">) dataset contains Cocktail Party scenarios with individuals wearing </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S2.T2.4.1.12.4.1.1.4" class="ltx_text" style="font-size:70%;"> audio processors and a head and torso simulator. Recorded in an acoustic chamber, it includes multi-channel audio, image recordings, and digitized microphone positions for each participant.</span></span>
</span></span>
<span id="S2.T2.4.1.12.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.12.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.12.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S2.T2.4.1.12.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.12.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.12.6.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.12.6.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S2.T2.4.1.12.6.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span></span>
<span id="S2.T2.4.1.13" class="ltx_tr">
<span id="S2.T2.4.1.13.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.13.1.1" class="ltx_text" style="font-size:70%;">ikala</span></span>
<span id="S2.T2.4.1.13.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.13.2.1.1" class="ltx_p"><span id="S2.T2.4.1.13.2.1.1.1" class="ltx_text" style="font-size:70%;">252-30s</span></span>
</span></span>
<span id="S2.T2.4.1.13.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.13.3.1.1" class="ltx_p"><span id="S2.T2.4.1.13.3.1.1.1" class="ltx_text" style="font-size:70%;">206</span></span>
</span></span>
<span id="S2.T2.4.1.13.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.13.4.1.1" class="ltx_p"><span id="S2.T2.4.1.13.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset comprises audio recordings consisting of vocal and backing track music with a sampling rate of 44100 Hz. Each music track is a stereo recording, where one channel contains the singing voice and the other channel contains the background music. All tracks were performed by professional musicians and featured a group of six singers, evenly split between three females and three males.</span></span>
</span></span>
<span id="S2.T2.4.1.13.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.13.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.13.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.T2.4.1.13.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.13.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.13.6.1.1" class="ltx_p"><span id="S2.T2.4.1.13.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://zenodo.org/records/3532214" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/3532214</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.14" class="ltx_tr">
<span id="S2.T2.4.1.14.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.14.1.1" class="ltx_text" style="font-size:70%;">MUSDB</span></span>
<span id="S2.T2.4.1.14.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.14.2.1.1" class="ltx_p"><span id="S2.T2.4.1.14.2.1.1.1" class="ltx_text" style="font-size:70%;">150</span></span>
</span></span>
<span id="S2.T2.4.1.14.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.14.3.1.1" class="ltx_p"><span id="S2.T2.4.1.14.3.1.1.1" class="ltx_text" style="font-size:70%;">4</span></span>
</span></span>
<span id="S2.T2.4.1.14.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.14.4.1.1" class="ltx_p"><span id="S2.T2.4.1.14.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset is a collection of music tracks specifically designed for music source separation research. It consists of professionally mixed songs across various genres, with individual tracks isolated for vocals, drums, bass, and other accompaniment.</span></span>
</span></span>
<span id="S2.T2.4.1.14.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.14.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.14.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.T2.4.1.14.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.14.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.14.6.1.1" class="ltx_p"><span id="S2.T2.4.1.14.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://sigsep.github.io/datasets/musdb.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sigsep.github.io/datasets/musdb.html</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.15" class="ltx_tr">
<span id="S2.T2.4.1.15.1" class="ltx_td ltx_align_left"><span id="S2.T2.4.1.15.1.1" class="ltx_text" style="font-size:70%;">CQ500</span></span>
<span id="S2.T2.4.1.15.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S2.T2.4.1.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.15.2.1.1" class="ltx_p"><span id="S2.T2.4.1.15.2.1.1.1" class="ltx_text" style="font-size:70%;">491</span></span>
</span></span>
<span id="S2.T2.4.1.15.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S2.T2.4.1.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.15.3.1.1" class="ltx_p"><span id="S2.T2.4.1.15.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
<span id="S2.T2.4.1.15.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:313.0pt;">
<span id="S2.T2.4.1.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.15.4.1.1" class="ltx_p"><span id="S2.T2.4.1.15.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset includes anonymized dicoms files, along with the interpretations provided by radiologists. The interpretations were conducted by three radiologists who have 8, 12, and 20 years of experience in interpreting cranial CT scans, respectively.</span></span>
</span></span>
<span id="S2.T2.4.1.15.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T2.4.1.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.15.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.15.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S2.T2.4.1.15.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.15.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:19.9pt;">
<span id="S2.T2.4.1.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.15.6.1.1" class="ltx_p"><span id="S2.T2.4.1.15.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="http://headctstudy.qure.ai/dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://headctstudy.qure.ai/dataset</a></span></span></span></span>
</span></span></span>
<span id="S2.T2.4.1.16" class="ltx_tr">
<span id="S2.T2.4.1.16.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T2.4.1.16.1.1" class="ltx_text" style="font-size:70%;">ECochG</span></span>
<span id="S2.T2.4.1.16.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:28.5pt;">
<span id="S2.T2.4.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.16.2.1.1" class="ltx_p"><span id="S2.T2.4.1.16.2.1.1.1" class="ltx_text" style="font-size:70%;">21</span></span>
</span></span>
<span id="S2.T2.4.1.16.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:14.2pt;">
<span id="S2.T2.4.1.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.16.3.1.1" class="ltx_p"><span id="S2.T2.4.1.16.3.1.1.1" class="ltx_text" style="font-size:70%;">-</span></span>
</span></span>
<span id="S2.T2.4.1.16.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:313.0pt;">
<span id="S2.T2.4.1.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.16.4.1.1" class="ltx_p"><span id="S2.T2.4.1.16.4.1.1.1" class="ltx_text" style="font-size:70%;">The dataset captures inner ear potentials in response to acoustic stimulation, specifically focusing on cochlear implant recipients.</span></span>
</span></span>
<span id="S2.T2.4.1.16.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:42.7pt;">
<span id="S2.T2.4.1.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.16.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.16.5.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S2.T2.4.1.16.5.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.16.5.1.1.3" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.16.5.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S2.T2.4.1.16.5.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T2.4.1.16.5.1.1.6" class="ltx_text" style="font-size:70%;">,</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T2.4.1.16.5.1.1.7.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T2.4.1.16.5.1.1.8.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span></span>
<span id="S2.T2.4.1.16.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:19.9pt;">
<span id="S2.T2.4.1.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.4.1.16.6.1.1" class="ltx_p"><span id="S2.T2.4.1.16.6.1.1.1" class="ltx_text" style="font-size:70%;">Link</span><span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a target="_blank" href="https://zenodo.org/records/7092661" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/7092661</a></span></span></span></span>
</span></span></span>
</span><span id="S2.T2.4.2" class="ltx_text" style="font-size:70%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.T2.5" class="ltx_p ltx_figure_panel ltx_align_left"><span id="S2.T2.5.1" class="ltx_text" style="font-size:70%;">Abbreviations: Number of samples (NoS), Number of classes (NoC)</span></p>
</div>
</div>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Metrics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Multiple evaluation metrics are utilized during the training and validation of any <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> models, including AI-based <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. These metrics, integral to the confusion matrix, are widely known and applicable across various data types such as speech or image. They include  <a href="#Sx1.20.20.20"><span href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">Accuracy</span></span></span></a> (<a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a>),  <a href="#Sx1.21.21.21"><span href="#Sx1.21.21.21" title="Sensitivity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">Sensitivity</span></span></span></a> (<a href="#Sx1.21.21.21"><abbr href="#Sx1.21.21.21" title="Sensitivity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Sen</span></span></abbr></a>),  <a href="#Sx1.24.24.24"><span href="#Sx1.24.24.24" title="Recall" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">Recall</span></span></span></a> (<a href="#Sx1.24.24.24"><abbr href="#Sx1.24.24.24" title="Recall" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Rec</span></span></abbr></a>),  <a href="#Sx1.22.22.22"><span href="#Sx1.22.22.22" title="Specificity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">Specificity</span></span></span></a> (<a href="#Sx1.22.22.22"><abbr href="#Sx1.22.22.22" title="Specificity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Spe</span></span></abbr></a>),  <a href="#Sx1.23.23.23"><span href="#Sx1.23.23.23" title="Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">Precision</span></span></span></a> (<a href="#Sx1.23.23.23"><abbr href="#Sx1.23.23.23" title="Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Pre</span></span></abbr></a>),  <a href="#Sx1.25.25.25"><span href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">F1 score</span></span></span></a> (<a href="#Sx1.25.25.25"><abbr href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">F1</span></span></abbr></a>), and  <a href="#Sx1.32.32.32"><span href="#Sx1.32.32.32" title=" receiver operating characteristic curve" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;"> receiver operating characteristic curve</span></span></span></a> (<a href="#Sx1.32.32.32"><abbr href="#Sx1.32.32.32" title=" receiver operating characteristic curve" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ROC</span></span></abbr></a>). Moreover, different metrics play roles in prediction task. For instance,  <a href="#Sx1.38.38.38"><span href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">intersection over union</span></span></span></a> (<a href="#Sx1.38.38.38"><abbr href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IoU</span></span></abbr></a>) assesses overlap, and  <a href="#Sx1.39.39.39"><span href="#Sx1.39.39.39" title="mean absolute error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">mean absolute error</span></span></span></a> (<a href="#Sx1.39.39.39"><abbr href="#Sx1.39.39.39" title="mean absolute error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MAE</span></span></abbr></a>) quantifies absolute differences.For a comprehensive understanding of the metrics discussed, including their equations, refer to the details provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Other metrics that are widely used for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> are summarized in Table <a href="#S2.T3" title="Table 3 ‣ 2.3 Metrics ‣ 2 Background in speech processing for ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<table id="S2.T3.19" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.19.20" class="ltx_tr">
<td id="S2.T3.19.20.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<div id="S2.T3.19.20.1.1" class="ltx_block ltx_align_top">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_block"><span id="S2.T3.19.20.1.1.4.1.1" class="ltx_text" style="font-size:129%;">Table 3</span>: </span><span id="S2.T3.19.20.1.1.5.2" class="ltx_text" style="font-size:129%;">An overview of the metrics employed for evaluating <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> methods.</span></figcaption>
</div>
</td>
<td id="S2.T3.19.20.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;"></td>
<td id="S2.T3.19.20.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;"></td>
</tr>
<tr id="S2.T3.19.21" class="ltx_tr">
<td id="S2.T3.19.21.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S2.T3.19.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.21.1.1.1" class="ltx_p"><span id="S2.T3.19.21.1.1.1.1" class="ltx_text" style="font-size:70%;">Metric</span></span>
</span>
</td>
<td id="S2.T3.19.21.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S2.T3.19.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.21.2.1.1" class="ltx_p"><span id="S2.T3.19.21.2.1.1.1" class="ltx_text" style="font-size:70%;">Formula</span></span>
</span>
</td>
<td id="S2.T3.19.21.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:270.3pt;">
<span id="S2.T3.19.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.21.3.1.1" class="ltx_p"><span id="S2.T3.19.21.3.1.1.1" class="ltx_text" style="font-size:70%;">Description</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;padding-bottom:17.07182pt;">
<span id="S2.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.2.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;padding-bottom:17.07182pt;">
<span id="S2.T3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.1.1.1" class="ltx_p"><math id="S2.T3.1.1.1.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\frac{{2\cdot|A\cap B|}}{{|A|+|B|}}" display="inline"><semantics id="S2.T3.1.1.1.1.1.m1.3a"><mstyle displaystyle="true" id="S2.T3.1.1.1.1.1.m1.3.3" xref="S2.T3.1.1.1.1.1.m1.3.3.cmml"><mfrac id="S2.T3.1.1.1.1.1.m1.3.3a" xref="S2.T3.1.1.1.1.1.m1.3.3.cmml"><mrow id="S2.T3.1.1.1.1.1.m1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.cmml"><mn mathsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.3.cmml">2</mn><mo lspace="0.222em" mathsize="70%" rspace="0.222em" id="S2.T3.1.1.1.1.1.m1.1.1.1.2" xref="S2.T3.1.1.1.1.1.m1.1.1.1.2.cmml">⋅</mo><mrow id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml">A</mi><mo mathsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">∩</mo><mi mathsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml">B</mi></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><mrow id="S2.T3.1.1.1.1.1.m1.3.3.3" xref="S2.T3.1.1.1.1.1.m1.3.3.3.cmml"><mrow id="S2.T3.1.1.1.1.1.m1.3.3.3.4.2" xref="S2.T3.1.1.1.1.1.m1.3.3.3.4.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.4.2.1" xref="S2.T3.1.1.1.1.1.m1.3.3.3.4.1.1.cmml">|</mo><mi mathsize="70%" id="S2.T3.1.1.1.1.1.m1.2.2.2.1" xref="S2.T3.1.1.1.1.1.m1.2.2.2.1.cmml">A</mi><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.4.2.2" xref="S2.T3.1.1.1.1.1.m1.3.3.3.4.1.1.cmml">|</mo></mrow><mo mathsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.3" xref="S2.T3.1.1.1.1.1.m1.3.3.3.3.cmml">+</mo><mrow id="S2.T3.1.1.1.1.1.m1.3.3.3.5.2" xref="S2.T3.1.1.1.1.1.m1.3.3.3.5.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.5.2.1" xref="S2.T3.1.1.1.1.1.m1.3.3.3.5.1.1.cmml">|</mo><mi mathsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.2" xref="S2.T3.1.1.1.1.1.m1.3.3.3.2.cmml">B</mi><mo maxsize="70%" minsize="70%" id="S2.T3.1.1.1.1.1.m1.3.3.3.5.2.2" xref="S2.T3.1.1.1.1.1.m1.3.3.3.5.1.1.cmml">|</mo></mrow></mrow></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="S2.T3.1.1.1.1.1.m1.3b"><apply id="S2.T3.1.1.1.1.1.m1.3.3.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3"><divide id="S2.T3.1.1.1.1.1.m1.3.3.4.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3"></divide><apply id="S2.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1"><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.2">⋅</ci><cn type="integer" id="S2.T3.1.1.1.1.1.m1.1.1.1.3.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.3">2</cn><apply id="S2.T3.1.1.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1"><abs id="S2.T3.1.1.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.2"></abs><apply id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1"><intersect id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.1"></intersect><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.2">𝐴</ci><ci id="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T3.1.1.1.1.1.m1.1.1.1.1.1.1.3">𝐵</ci></apply></apply></apply><apply id="S2.T3.1.1.1.1.1.m1.3.3.3.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3"><plus id="S2.T3.1.1.1.1.1.m1.3.3.3.3.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.3"></plus><apply id="S2.T3.1.1.1.1.1.m1.3.3.3.4.1.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.4.2"><abs id="S2.T3.1.1.1.1.1.m1.3.3.3.4.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.4.2.1"></abs><ci id="S2.T3.1.1.1.1.1.m1.2.2.2.1.cmml" xref="S2.T3.1.1.1.1.1.m1.2.2.2.1">𝐴</ci></apply><apply id="S2.T3.1.1.1.1.1.m1.3.3.3.5.1.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.5.2"><abs id="S2.T3.1.1.1.1.1.m1.3.3.3.5.1.1.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.5.2.1"></abs><ci id="S2.T3.1.1.1.1.1.m1.3.3.3.2.cmml" xref="S2.T3.1.1.1.1.1.m1.3.3.3.2">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.1.1.1.1.1.m1.3c">\displaystyle\frac{{2\cdot|A\cap B|}}{{|A|+|B|}}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:270.3pt;padding-bottom:17.07182pt;">
<span id="S2.T3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.3.1.1" class="ltx_p"><span id="S2.T3.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">The metric  </span><a href="#Sx1.34.34.34"><span href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">dice coefficient similarity</span></span></span></a><span id="S2.T3.1.1.3.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S2.T3.1.1.3.1.1.3" class="ltx_text" style="font-size:70%;">) is used to evaluate the performance of the vestibule segmentation network. The Dice coefficient is a widely used similarity metric in image segmentation tasks. It measures the overlap between the predicted segmentation mask and the ground truth mask.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.2.2" class="ltx_tr">
<td id="S2.T3.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:17.07182pt;">
<span id="S2.T3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.2.1.1" class="ltx_p"><a href="#Sx1.35.35.35"><abbr href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASD</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:17.07182pt;">
<span id="S2.T3.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.1.1.1" class="ltx_p"><math id="S2.T3.2.2.1.1.1.m1.12" class="ltx_Math" alttext="\displaystyle\frac{\sum_{a\in S(A)}{\min_{b\in S(B)}(a-b)}+\sum_{b\in S(B)}{\min_{a\in S(A)}(b-a)}}{|S(A)|+|S(B)|}" display="inline"><semantics id="S2.T3.2.2.1.1.1.m1.12a"><mstyle displaystyle="true" id="S2.T3.2.2.1.1.1.m1.12.12" xref="S2.T3.2.2.1.1.1.m1.12.12.cmml"><mfrac id="S2.T3.2.2.1.1.1.m1.12.12a" xref="S2.T3.2.2.1.1.1.m1.12.12.cmml"><mrow id="S2.T3.2.2.1.1.1.m1.8.8.8" xref="S2.T3.2.2.1.1.1.m1.8.8.8.cmml"><mrow id="S2.T3.2.2.1.1.1.m1.6.6.6.6" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.cmml"><msub id="S2.T3.2.2.1.1.1.m1.6.6.6.6.3" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.2" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.2.cmml">∑</mo><mrow id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.3.cmml">a</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.2.cmml">∈</mo><mrow id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.2" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.1" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.3.2" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.3.2.1" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.1.cmml">A</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.3.2.2" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml"><msub id="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1" xref="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.2.cmml">min</mi><mrow id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.3" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.3.cmml">b</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.2" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.2.cmml">∈</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.2" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.1" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.3.2" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.3.2.1" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.1" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.1.cmml">B</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.3.2.2" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2a" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml">⁡</mo><mrow id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.2" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml">(</mo><mrow id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.2" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.2.cmml">a</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.1" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.1.cmml">−</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.3" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.3.cmml">b</mi></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.3" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml">)</mo></mrow></mrow></mrow><mo mathsize="70%" rspace="0.055em" id="S2.T3.2.2.1.1.1.m1.8.8.8.9" xref="S2.T3.2.2.1.1.1.m1.8.8.8.9.cmml">+</mo><mrow id="S2.T3.2.2.1.1.1.m1.8.8.8.8" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.cmml"><msub id="S2.T3.2.2.1.1.1.m1.8.8.8.8.3" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.2" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.2.cmml">∑</mo><mrow id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.3" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.3.cmml">b</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.2" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.2.cmml">∈</mo><mrow id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.2" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.1" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.3.2" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.3.2.1" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.1" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.1.cmml">B</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.3.2.2" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml"><msub id="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1" xref="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.2" xref="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.2.cmml">min</mi><mrow id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.3" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.3.cmml">a</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.2" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.2.cmml">∈</mo><mrow id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.2" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.1" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.3.2" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.3.2.1" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.1" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.1.cmml">A</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.3.2.2" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2a" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml">⁡</mo><mrow id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.2" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml">(</mo><mrow id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.2" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.2.cmml">b</mi><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.1" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.1.cmml">−</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.3" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.3.cmml">a</mi></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.3" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mrow id="S2.T3.2.2.1.1.1.m1.12.12.12" xref="S2.T3.2.2.1.1.1.m1.12.12.12.cmml"><mrow id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.2" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.2.1.cmml">|</mo><mrow id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.2" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.1" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.3.2" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.3.2.1" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.9.9.9.1" xref="S2.T3.2.2.1.1.1.m1.9.9.9.1.cmml">A</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.3.2.2" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.3" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.2.1.cmml">|</mo></mrow><mo mathsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.5" xref="S2.T3.2.2.1.1.1.m1.12.12.12.5.cmml">+</mo><mrow id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.2" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.2.1.cmml">|</mo><mrow id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.cmml"><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.2" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.1" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.1.cmml">​</mo><mrow id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.3.2" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.3.2.1" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.2.2.1.1.1.m1.10.10.10.2" xref="S2.T3.2.2.1.1.1.m1.10.10.10.2.cmml">B</mi><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.3.2.2" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.3" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.2.1.cmml">|</mo></mrow></mrow></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="S2.T3.2.2.1.1.1.m1.12b"><apply id="S2.T3.2.2.1.1.1.m1.12.12.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12"><divide id="S2.T3.2.2.1.1.1.m1.12.12.13.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12"></divide><apply id="S2.T3.2.2.1.1.1.m1.8.8.8.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8"><plus id="S2.T3.2.2.1.1.1.m1.8.8.8.9.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.9"></plus><apply id="S2.T3.2.2.1.1.1.m1.6.6.6.6.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6"><apply id="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.3"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.3">subscript</csymbol><sum id="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.3.2"></sum><apply id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1"><in id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.2"></in><ci id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.3">𝑎</ci><apply id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4"><times id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.1.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.1"></times><ci id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.2.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.4.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.1.1.1.1.1.1">𝐴</ci></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.3.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2"><apply id="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1">subscript</csymbol><min id="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.5.5.5.5.1.1.1.2"></min><apply id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1"><in id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.2"></in><ci id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.3">𝑏</ci><apply id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4"><times id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.1"></times><ci id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.2.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.4.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.2.2.2.2.1.1">𝐵</ci></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1"><minus id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.1"></minus><ci id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.2">𝑎</ci><ci id="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.6.6.6.6.2.2.2.1.3">𝑏</ci></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.8.8.8.8.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8"><apply id="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.3"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.3">subscript</csymbol><sum id="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.3.2"></sum><apply id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1"><in id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.2"></in><ci id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.3">𝑏</ci><apply id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4"><times id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.1.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.1"></times><ci id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.2.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.4.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.3.3.3.3.1.1">𝐵</ci></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.3.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2"><apply id="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1"><csymbol cd="ambiguous" id="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1">subscript</csymbol><min id="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.7.7.7.7.1.1.1.2"></min><apply id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1"><in id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.2"></in><ci id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.3">𝑎</ci><apply id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4"><times id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.1.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.1"></times><ci id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.2.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.4.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.4.4.4.4.1.1">𝐴</ci></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1"><minus id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.1"></minus><ci id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.2">𝑏</ci><ci id="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.3.cmml" xref="S2.T3.2.2.1.1.1.m1.8.8.8.8.2.2.2.1.3">𝑎</ci></apply></apply></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.12.12.12.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12"><plus id="S2.T3.2.2.1.1.1.m1.12.12.12.5.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.5"></plus><apply id="S2.T3.2.2.1.1.1.m1.11.11.11.3.2.cmml" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1"><abs id="S2.T3.2.2.1.1.1.m1.11.11.11.3.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.2"></abs><apply id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1"><times id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.1"></times><ci id="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.11.11.11.3.1.1.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.9.9.9.1.cmml" xref="S2.T3.2.2.1.1.1.m1.9.9.9.1">𝐴</ci></apply></apply><apply id="S2.T3.2.2.1.1.1.m1.12.12.12.4.2.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1"><abs id="S2.T3.2.2.1.1.1.m1.12.12.12.4.2.1.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.2"></abs><apply id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1"><times id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.1.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.1"></times><ci id="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.2.cmml" xref="S2.T3.2.2.1.1.1.m1.12.12.12.4.1.1.2">𝑆</ci><ci id="S2.T3.2.2.1.1.1.m1.10.10.10.2.cmml" xref="S2.T3.2.2.1.1.1.m1.10.10.10.2">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.2.2.1.1.1.m1.12c">\displaystyle\frac{\sum_{a\in S(A)}{\min_{b\in S(B)}(a-b)}+\sum_{b\in S(B)}{\min_{a\in S(A)}(b-a)}}{|S(A)|+|S(B)|}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;padding-bottom:17.07182pt;">
<span id="S2.T3.2.2.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.2.2.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.2.2.3.1.2" class="ltx_p"><span id="S2.T3.2.2.3.1.2.1" class="ltx_text" style="font-size:70%;">ASD is a commonly used evaluation measure in medical image segmentation tasks. It quantifies the average distance between the surfaces of two segmented objects, typically the predicted segmentation and the ground truth.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.3.3" class="ltx_tr">
<td id="S2.T3.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:11.38092pt;">
<span id="S2.T3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.3.3.2.1.1" class="ltx_p"><a href="#Sx1.36.36.36"><abbr href="#Sx1.36.36.36" title="average volume difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AVD</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:11.38092pt;">
<span id="S2.T3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.3.3.1.1.1" class="ltx_p"><math id="S2.T3.3.3.1.1.1.m1.7" class="ltx_Math" alttext="\displaystyle\max(\text{d}(A,B),\text{d}(B,A))" display="inline"><semantics id="S2.T3.3.3.1.1.1.m1.7a"><mrow id="S2.T3.3.3.1.1.1.m1.7.7.2" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml"><mi mathsize="70%" id="S2.T3.3.3.1.1.1.m1.5.5" xref="S2.T3.3.3.1.1.1.m1.5.5.cmml">max</mi><mo id="S2.T3.3.3.1.1.1.m1.7.7.2a" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml">⁡</mo><mrow id="S2.T3.3.3.1.1.1.m1.7.7.2.2" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.3" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml">(</mo><mrow id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.cmml"><mtext mathsize="70%" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2a.cmml">d</mtext><mo lspace="0em" rspace="0em" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.1" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.1.cmml">​</mo><mrow id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.2" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.2.1" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.3.3.1.1.1.m1.1.1" xref="S2.T3.3.3.1.1.1.m1.1.1.cmml">A</mi><mo mathsize="70%" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.2.2" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.3.3.1.1.1.m1.2.2" xref="S2.T3.3.3.1.1.1.m1.2.2.cmml">B</mi><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.2.3" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo mathsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.4" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml">,</mo><mrow id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.cmml"><mtext mathsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2a.cmml">d</mtext><mo lspace="0em" rspace="0em" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.1" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.1.cmml">​</mo><mrow id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.2" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.2.1" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.3.3.1.1.1.m1.3.3" xref="S2.T3.3.3.1.1.1.m1.3.3.cmml">B</mi><mo mathsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.2.2" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.3.3.1.1.1.m1.4.4" xref="S2.T3.3.3.1.1.1.m1.4.4.cmml">A</mi><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.2.3" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.5" xref="S2.T3.3.3.1.1.1.m1.7.7.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.3.3.1.1.1.m1.7b"><apply id="S2.T3.3.3.1.1.1.m1.7.7.3.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2"><max id="S2.T3.3.3.1.1.1.m1.5.5.cmml" xref="S2.T3.3.3.1.1.1.m1.5.5"></max><apply id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1"><times id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.1"></times><ci id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2a.cmml" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2"><mtext mathsize="70%" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2.cmml" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.2">d</mtext></ci><interval closure="open" id="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.1.cmml" xref="S2.T3.3.3.1.1.1.m1.6.6.1.1.1.3.2"><ci id="S2.T3.3.3.1.1.1.m1.1.1.cmml" xref="S2.T3.3.3.1.1.1.m1.1.1">𝐴</ci><ci id="S2.T3.3.3.1.1.1.m1.2.2.cmml" xref="S2.T3.3.3.1.1.1.m1.2.2">𝐵</ci></interval></apply><apply id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2"><times id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.1.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.1"></times><ci id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2a.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2"><mtext mathsize="70%" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.2">d</mtext></ci><interval closure="open" id="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.1.cmml" xref="S2.T3.3.3.1.1.1.m1.7.7.2.2.2.3.2"><ci id="S2.T3.3.3.1.1.1.m1.3.3.cmml" xref="S2.T3.3.3.1.1.1.m1.3.3">𝐵</ci><ci id="S2.T3.3.3.1.1.1.m1.4.4.cmml" xref="S2.T3.3.3.1.1.1.m1.4.4">𝐴</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.3.3.1.1.1.m1.7c">\displaystyle\max(\text{d}(A,B),\text{d}(B,A))</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;padding-bottom:11.38092pt;">
<span id="S2.T3.3.3.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.3.3.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.3.3.3.1.2" class="ltx_p"><span id="S2.T3.3.3.3.1.2.1" class="ltx_text" style="font-size:70%;">AVD quantifies the average difference in volume between a predicted segmentation and a reference or ground truth segmentation..</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.4.4" class="ltx_tr">
<td id="S2.T3.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:11.38109pt;">
<span id="S2.T3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.4.4.2.1.1" class="ltx_p"><a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:11.38109pt;">
<span id="S2.T3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.4.4.1.1.1" class="ltx_p"><math id="S2.T3.4.4.1.1.1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\mathrm{10\log_{10}\Big{(}\frac{Signal}{Noise}\Big{)}_{dB}}" display="inline"><semantics id="S2.T3.4.4.1.1.1.m1.1a"><mrow id="S2.T3.4.4.1.1.1.m1.1b"><mn mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.2">10</mn><msub id="S2.T3.4.4.1.1.1.m1.1.3"><mi mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.3.2">log</mi><mn mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.3.3">10</mn></msub><msub id="S2.T3.4.4.1.1.1.m1.1.4"><mrow id="S2.T3.4.4.1.1.1.m1.1.4.2"><mo maxsize="160%" minsize="160%" id="S2.T3.4.4.1.1.1.m1.1.4.2.1">(</mo><mstyle displaystyle="true" id="S2.T3.4.4.1.1.1.m1.1.1"><mfrac id="S2.T3.4.4.1.1.1.m1.1.1a"><mi mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.1.2">Signal</mi><mi mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.1.3">Noise</mi></mfrac></mstyle><mo maxsize="160%" minsize="160%" id="S2.T3.4.4.1.1.1.m1.1.4.2.2">)</mo></mrow><mi mathsize="70%" id="S2.T3.4.4.1.1.1.m1.1.4.3">dB</mi></msub></mrow><annotation encoding="application/x-tex" id="S2.T3.4.4.1.1.1.m1.1c">\displaystyle\mathrm{10\log_{10}\Big{(}\frac{Signal}{Noise}\Big{)}_{dB}}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;padding-bottom:11.38109pt;">
<span id="S2.T3.4.4.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.4.4.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.4.4.3.1.2" class="ltx_p"><span id="S2.T3.4.4.3.1.2.1" class="ltx_text" style="font-size:70%;">SNR is a measure of the quality of the speech signal. It is commonly used to evaluate the quality of the stego-speech (the speech signal after the hidden information has been embedded). A lower SNR indicates that the steganography technique has introduced more distortion to the speech signal.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.9.9" class="ltx_tr">
<td id="S2.T3.9.9.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:11.38109pt;">
<span id="S2.T3.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.9.9.6.1.1" class="ltx_p"><a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:11.38109pt;">
<span id="S2.T3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.5.5.1.1.1" class="ltx_p"><math id="S2.T3.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\frac{\sum E_{ij}}{N}" display="inline"><semantics id="S2.T3.5.5.1.1.1.m1.1a"><mstyle displaystyle="true" id="S2.T3.5.5.1.1.1.m1.1.1" xref="S2.T3.5.5.1.1.1.m1.1.1.cmml"><mfrac id="S2.T3.5.5.1.1.1.m1.1.1a" xref="S2.T3.5.5.1.1.1.m1.1.1.cmml"><mrow id="S2.T3.5.5.1.1.1.m1.1.1.2" xref="S2.T3.5.5.1.1.1.m1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S2.T3.5.5.1.1.1.m1.1.1.2.1" xref="S2.T3.5.5.1.1.1.m1.1.1.2.1.cmml">∑</mo><msub id="S2.T3.5.5.1.1.1.m1.1.1.2.2" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.cmml"><mi mathsize="70%" id="S2.T3.5.5.1.1.1.m1.1.1.2.2.2" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.2.cmml">E</mi><mrow id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.cmml"><mi mathsize="70%" id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.2" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.1" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.3" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.3.cmml">j</mi></mrow></msub></mrow><mi mathsize="70%" id="S2.T3.5.5.1.1.1.m1.1.1.3" xref="S2.T3.5.5.1.1.1.m1.1.1.3.cmml">N</mi></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="S2.T3.5.5.1.1.1.m1.1b"><apply id="S2.T3.5.5.1.1.1.m1.1.1.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1"><divide id="S2.T3.5.5.1.1.1.m1.1.1.1.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1"></divide><apply id="S2.T3.5.5.1.1.1.m1.1.1.2.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2"><sum id="S2.T3.5.5.1.1.1.m1.1.1.2.1.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.1"></sum><apply id="S2.T3.5.5.1.1.1.m1.1.1.2.2.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.T3.5.5.1.1.1.m1.1.1.2.2.1.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2">subscript</csymbol><ci id="S2.T3.5.5.1.1.1.m1.1.1.2.2.2.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.2">𝐸</ci><apply id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3"><times id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.1.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.1"></times><ci id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.2.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.2">𝑖</ci><ci id="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.3.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.2.2.3.3">𝑗</ci></apply></apply></apply><ci id="S2.T3.5.5.1.1.1.m1.1.1.3.cmml" xref="S2.T3.5.5.1.1.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.5.5.1.1.1.m1.1c">\displaystyle\frac{\sum E_{ij}}{N}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.9.9.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;padding-bottom:11.38109pt;">
<span id="S2.T3.9.9.5.4" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.9.9.5.4.4" class="ltx_p"><span id="S2.T3.9.9.5.4.4.1" class="ltx_text" style="font-size:70%;">Is the  </span><a href="#Sx1.120.120.120"><span href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">average surface error</span></span></span></a><span id="S2.T3.9.9.5.4.4.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a><span id="S2.T3.9.9.5.4.4.3" class="ltx_text" style="font-size:70%;">). The distances between corresponding points on the measured surface and the reference surface are computed using </span><math id="S2.T3.6.6.2.1.1.m1.5" class="ltx_Math" alttext="\displaystyle E_{ij}=|I_{1}(i,j)-I_{2}(k,l)|" display="inline"><semantics id="S2.T3.6.6.2.1.1.m1.5a"><mrow id="S2.T3.6.6.2.1.1.m1.5.5" xref="S2.T3.6.6.2.1.1.m1.5.5.cmml"><msub id="S2.T3.6.6.2.1.1.m1.5.5.3" xref="S2.T3.6.6.2.1.1.m1.5.5.3.cmml"><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.3.2" xref="S2.T3.6.6.2.1.1.m1.5.5.3.2.cmml">E</mi><mrow id="S2.T3.6.6.2.1.1.m1.5.5.3.3" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.cmml"><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.3.3.2" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.6.6.2.1.1.m1.5.5.3.3.1" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.3.3.3" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.3.cmml">j</mi></mrow></msub><mo mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.2" xref="S2.T3.6.6.2.1.1.m1.5.5.2.cmml">=</mo><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.2.1.cmml">|</mo><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.cmml"><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.cmml"><msub id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.cmml"><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.2.cmml">I</mi><mn mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.1.cmml">​</mo><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.2.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.1.1" xref="S2.T3.6.6.2.1.1.m1.1.1.cmml">i</mi><mo mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.2.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.2.2" xref="S2.T3.6.6.2.1.1.m1.2.2.cmml">j</mi><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.2.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.1.cmml">−</mo><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.cmml"><msub id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.cmml"><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.2.cmml">I</mi><mn mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.1.cmml">​</mo><mrow id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.2.1" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.3.3" xref="S2.T3.6.6.2.1.1.m1.3.3.cmml">k</mi><mo mathsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.2.2" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.6.6.2.1.1.m1.4.4" xref="S2.T3.6.6.2.1.1.m1.4.4.cmml">l</mi><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.2.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.3" xref="S2.T3.6.6.2.1.1.m1.5.5.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.6.6.2.1.1.m1.5b"><apply id="S2.T3.6.6.2.1.1.m1.5.5.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5"><eq id="S2.T3.6.6.2.1.1.m1.5.5.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.2"></eq><apply id="S2.T3.6.6.2.1.1.m1.5.5.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3"><csymbol cd="ambiguous" id="S2.T3.6.6.2.1.1.m1.5.5.3.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3">subscript</csymbol><ci id="S2.T3.6.6.2.1.1.m1.5.5.3.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3.2">𝐸</ci><apply id="S2.T3.6.6.2.1.1.m1.5.5.3.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3"><times id="S2.T3.6.6.2.1.1.m1.5.5.3.3.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.1"></times><ci id="S2.T3.6.6.2.1.1.m1.5.5.3.3.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.2">𝑖</ci><ci id="S2.T3.6.6.2.1.1.m1.5.5.3.3.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.3.3.3">𝑗</ci></apply></apply><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1"><abs id="S2.T3.6.6.2.1.1.m1.5.5.1.2.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.2"></abs><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1"><minus id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.1"></minus><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2"><times id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.1"></times><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2">subscript</csymbol><ci id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.2">𝐼</ci><cn type="integer" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.2.3">1</cn></apply><interval closure="open" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.2.3.2"><ci id="S2.T3.6.6.2.1.1.m1.1.1.cmml" xref="S2.T3.6.6.2.1.1.m1.1.1">𝑖</ci><ci id="S2.T3.6.6.2.1.1.m1.2.2.cmml" xref="S2.T3.6.6.2.1.1.m1.2.2">𝑗</ci></interval></apply><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3"><times id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.1"></times><apply id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2">subscript</csymbol><ci id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.2.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.2">𝐼</ci><cn type="integer" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.3.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.2.3">2</cn></apply><interval closure="open" id="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.1.cmml" xref="S2.T3.6.6.2.1.1.m1.5.5.1.1.1.3.3.2"><ci id="S2.T3.6.6.2.1.1.m1.3.3.cmml" xref="S2.T3.6.6.2.1.1.m1.3.3">𝑘</ci><ci id="S2.T3.6.6.2.1.1.m1.4.4.cmml" xref="S2.T3.6.6.2.1.1.m1.4.4">𝑙</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.6.6.2.1.1.m1.5c">\displaystyle E_{ij}=|I_{1}(i,j)-I_{2}(k,l)|</annotation></semantics></math><span id="S2.T3.9.9.5.4.4.4" class="ltx_text" style="font-size:70%;">, known as  </span><a href="#Sx1.72.72.72"><span href="#Sx1.72.72.72" title="point to point error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">point to point error</span></span></span></a><span id="S2.T3.9.9.5.4.4.5" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.72.72.72"><abbr href="#Sx1.72.72.72" title="point to point error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">P2PE</span></span></abbr></a><span id="S2.T3.9.9.5.4.4.6" class="ltx_text" style="font-size:70%;">). </span><math id="S2.T3.7.7.3.2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.T3.7.7.3.2.2.m2.1a"><mi mathsize="70%" id="S2.T3.7.7.3.2.2.m2.1.1" xref="S2.T3.7.7.3.2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.T3.7.7.3.2.2.m2.1b"><ci id="S2.T3.7.7.3.2.2.m2.1.1.cmml" xref="S2.T3.7.7.3.2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.7.7.3.2.2.m2.1c">N</annotation></semantics></math><span id="S2.T3.9.9.5.4.4.7" class="ltx_text" style="font-size:70%;"> is the total number of correspondence points. Normalize average error by dividing by the imaging system’s dynamic range. </span><math id="S2.T3.8.8.4.3.3.m3.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S2.T3.8.8.4.3.3.m3.2a"><mrow id="S2.T3.8.8.4.3.3.m3.2.3.2" xref="S2.T3.8.8.4.3.3.m3.2.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.8.8.4.3.3.m3.2.3.2.1" xref="S2.T3.8.8.4.3.3.m3.2.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.8.8.4.3.3.m3.1.1" xref="S2.T3.8.8.4.3.3.m3.1.1.cmml">i</mi><mo mathsize="70%" id="S2.T3.8.8.4.3.3.m3.2.3.2.2" xref="S2.T3.8.8.4.3.3.m3.2.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.8.8.4.3.3.m3.2.2" xref="S2.T3.8.8.4.3.3.m3.2.2.cmml">j</mi><mo maxsize="70%" minsize="70%" id="S2.T3.8.8.4.3.3.m3.2.3.2.3" xref="S2.T3.8.8.4.3.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.8.8.4.3.3.m3.2b"><interval closure="open" id="S2.T3.8.8.4.3.3.m3.2.3.1.cmml" xref="S2.T3.8.8.4.3.3.m3.2.3.2"><ci id="S2.T3.8.8.4.3.3.m3.1.1.cmml" xref="S2.T3.8.8.4.3.3.m3.1.1">𝑖</ci><ci id="S2.T3.8.8.4.3.3.m3.2.2.cmml" xref="S2.T3.8.8.4.3.3.m3.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.8.8.4.3.3.m3.2c">(i,j)</annotation></semantics></math><span id="S2.T3.9.9.5.4.4.8" class="ltx_text" style="font-size:70%;">, and </span><math id="S2.T3.9.9.5.4.4.m4.2" class="ltx_Math" alttext="(k,l)" display="inline"><semantics id="S2.T3.9.9.5.4.4.m4.2a"><mrow id="S2.T3.9.9.5.4.4.m4.2.3.2" xref="S2.T3.9.9.5.4.4.m4.2.3.1.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.9.9.5.4.4.m4.2.3.2.1" xref="S2.T3.9.9.5.4.4.m4.2.3.1.cmml">(</mo><mi mathsize="70%" id="S2.T3.9.9.5.4.4.m4.1.1" xref="S2.T3.9.9.5.4.4.m4.1.1.cmml">k</mi><mo mathsize="70%" id="S2.T3.9.9.5.4.4.m4.2.3.2.2" xref="S2.T3.9.9.5.4.4.m4.2.3.1.cmml">,</mo><mi mathsize="70%" id="S2.T3.9.9.5.4.4.m4.2.2" xref="S2.T3.9.9.5.4.4.m4.2.2.cmml">l</mi><mo maxsize="70%" minsize="70%" id="S2.T3.9.9.5.4.4.m4.2.3.2.3" xref="S2.T3.9.9.5.4.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.9.9.5.4.4.m4.2b"><interval closure="open" id="S2.T3.9.9.5.4.4.m4.2.3.1.cmml" xref="S2.T3.9.9.5.4.4.m4.2.3.2"><ci id="S2.T3.9.9.5.4.4.m4.1.1.cmml" xref="S2.T3.9.9.5.4.4.m4.1.1">𝑘</ci><ci id="S2.T3.9.9.5.4.4.m4.2.2.cmml" xref="S2.T3.9.9.5.4.4.m4.2.2">𝑙</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.9.9.5.4.4.m4.2c">(k,l)</annotation></semantics></math><span id="S2.T3.9.9.5.4.4.9" class="ltx_text" style="font-size:70%;"> represent pixel values in the first and second images, respectively.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.10.10" class="ltx_tr">
<td id="S2.T3.10.10.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:14.22636pt;">
<span id="S2.T3.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.10.10.2.1.1" class="ltx_p"><a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.10.10.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:14.22636pt;">
<span id="S2.T3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.10.10.1.1.1" class="ltx_p"><math id="S2.T3.10.10.1.1.1.m1.2" class="ltx_Math" alttext="20\log_{10}\left(\frac{L}{R}\right)" display="inline"><semantics id="S2.T3.10.10.1.1.1.m1.2a"><mrow id="S2.T3.10.10.1.1.1.m1.2.2" xref="S2.T3.10.10.1.1.1.m1.2.2.cmml"><mn mathsize="70%" id="S2.T3.10.10.1.1.1.m1.2.2.3" xref="S2.T3.10.10.1.1.1.m1.2.2.3.cmml">20</mn><mo lspace="0.167em" rspace="0em" id="S2.T3.10.10.1.1.1.m1.2.2.2" xref="S2.T3.10.10.1.1.1.m1.2.2.2.cmml">​</mo><mrow id="S2.T3.10.10.1.1.1.m1.2.2.1.1" xref="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml"><msub id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.2" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.2.cmml">log</mi><mn mathsize="70%" id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.3" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.3.cmml">10</mn></msub><mo id="S2.T3.10.10.1.1.1.m1.2.2.1.1a" xref="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml">⁡</mo><mrow id="S2.T3.10.10.1.1.1.m1.2.2.1.1.2" xref="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml"><mo id="S2.T3.10.10.1.1.1.m1.2.2.1.1.2.1" xref="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml">(</mo><mfrac id="S2.T3.10.10.1.1.1.m1.1.1" xref="S2.T3.10.10.1.1.1.m1.1.1.cmml"><mi mathsize="70%" id="S2.T3.10.10.1.1.1.m1.1.1.2" xref="S2.T3.10.10.1.1.1.m1.1.1.2.cmml">L</mi><mi mathsize="70%" id="S2.T3.10.10.1.1.1.m1.1.1.3" xref="S2.T3.10.10.1.1.1.m1.1.1.3.cmml">R</mi></mfrac><mo id="S2.T3.10.10.1.1.1.m1.2.2.1.1.2.2" xref="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.10.10.1.1.1.m1.2b"><apply id="S2.T3.10.10.1.1.1.m1.2.2.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2"><times id="S2.T3.10.10.1.1.1.m1.2.2.2.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.2"></times><cn type="integer" id="S2.T3.10.10.1.1.1.m1.2.2.3.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.3">20</cn><apply id="S2.T3.10.10.1.1.1.m1.2.2.1.2.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1"><apply id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.1.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1">subscript</csymbol><log id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.2.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.2"></log><cn type="integer" id="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.3.cmml" xref="S2.T3.10.10.1.1.1.m1.2.2.1.1.1.3">10</cn></apply><apply id="S2.T3.10.10.1.1.1.m1.1.1.cmml" xref="S2.T3.10.10.1.1.1.m1.1.1"><divide id="S2.T3.10.10.1.1.1.m1.1.1.1.cmml" xref="S2.T3.10.10.1.1.1.m1.1.1"></divide><ci id="S2.T3.10.10.1.1.1.m1.1.1.2.cmml" xref="S2.T3.10.10.1.1.1.m1.1.1.2">𝐿</ci><ci id="S2.T3.10.10.1.1.1.m1.1.1.3.cmml" xref="S2.T3.10.10.1.1.1.m1.1.1.3">𝑅</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.10.10.1.1.1.m1.2c">20\log_{10}\left(\frac{L}{R}\right)</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.10.10.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;padding-bottom:14.22636pt;">
<span id="S2.T3.10.10.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.10.10.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.10.10.3.1.2" class="ltx_p"><span id="S2.T3.10.10.3.1.2.1" class="ltx_text" style="font-size:70%;">ILD a psychoacoustic metric, measuring sound level differences between left (L) and right (R) ears, reflects cues essential for sound localization, enhancing spatial awareness in auditory perception for directional sound source identification.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.14.14" class="ltx_tr">
<td id="S2.T3.14.14.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T3.14.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.14.14.5.1.1" class="ltx_p"><a href="#Sx1.18.18.18"><abbr href="#Sx1.18.18.18" title="mean endpoint error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MEE</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.11.11.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;">
<span id="S2.T3.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.11.11.1.1.1" class="ltx_p"><math id="S2.T3.11.11.1.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\frac{1}{N}\sum_{i=1}^{N}|L_{i}-\hat{L}_{i}|" display="inline"><semantics id="S2.T3.11.11.1.1.1.m1.1a"><mrow id="S2.T3.11.11.1.1.1.m1.1.1" xref="S2.T3.11.11.1.1.1.m1.1.1.cmml"><mstyle displaystyle="true" id="S2.T3.11.11.1.1.1.m1.1.1.3" xref="S2.T3.11.11.1.1.1.m1.1.1.3.cmml"><mfrac id="S2.T3.11.11.1.1.1.m1.1.1.3a" xref="S2.T3.11.11.1.1.1.m1.1.1.3.cmml"><mn mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.3.2" xref="S2.T3.11.11.1.1.1.m1.1.1.3.2.cmml">1</mn><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.3.3" xref="S2.T3.11.11.1.1.1.m1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.T3.11.11.1.1.1.m1.1.1.2" xref="S2.T3.11.11.1.1.1.m1.1.1.2.cmml">​</mo><mrow id="S2.T3.11.11.1.1.1.m1.1.1.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.T3.11.11.1.1.1.m1.1.1.1.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.cmml"><munderover id="S2.T3.11.11.1.1.1.m1.1.1.1.2a" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" movablelimits="false" stretchy="true" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.cmml"><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.cmml"><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.2.cmml">L</mi><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.cmml"><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.2" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.2.cmml">L</mi><mo mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.1" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi mathsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.11.11.1.1.1.m1.1b"><apply id="S2.T3.11.11.1.1.1.m1.1.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1"><times id="S2.T3.11.11.1.1.1.m1.1.1.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.2"></times><apply id="S2.T3.11.11.1.1.1.m1.1.1.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.3"><divide id="S2.T3.11.11.1.1.1.m1.1.1.3.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.3"></divide><cn type="integer" id="S2.T3.11.11.1.1.1.m1.1.1.3.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.3.2">1</cn><ci id="S2.T3.11.11.1.1.1.m1.1.1.3.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.3.3">𝑁</ci></apply><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1"><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2">superscript</csymbol><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2">subscript</csymbol><sum id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.2"></sum><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3"><eq id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.1"></eq><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.2.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.2.3">𝑁</ci></apply><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1"><abs id="S2.T3.11.11.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.2"></abs><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1"><minus id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.1"></minus><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.2">𝐿</ci><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2"><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.2.2">𝐿</ci></apply><ci id="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.T3.11.11.1.1.1.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.11.11.1.1.1.m1.1c">\displaystyle\frac{1}{N}\sum_{i=1}^{N}|L_{i}-\hat{L}_{i}|</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.14.14.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;">
<span id="S2.T3.14.14.4.3" class="ltx_inline-block ltx_align_top"><span id="S2.T3.14.14.4.3.4" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.14.14.4.3.3" class="ltx_p"><span id="S2.T3.14.14.4.3.3.1" class="ltx_text" style="font-size:70%;">MEE measures the average absolute difference between the true </span><math id="S2.T3.12.12.2.1.1.m1.1" class="ltx_Math" alttext="L_{i}" display="inline"><semantics id="S2.T3.12.12.2.1.1.m1.1a"><msub id="S2.T3.12.12.2.1.1.m1.1.1" xref="S2.T3.12.12.2.1.1.m1.1.1.cmml"><mi mathsize="70%" id="S2.T3.12.12.2.1.1.m1.1.1.2" xref="S2.T3.12.12.2.1.1.m1.1.1.2.cmml">L</mi><mi mathsize="70%" id="S2.T3.12.12.2.1.1.m1.1.1.3" xref="S2.T3.12.12.2.1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.12.12.2.1.1.m1.1b"><apply id="S2.T3.12.12.2.1.1.m1.1.1.cmml" xref="S2.T3.12.12.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T3.12.12.2.1.1.m1.1.1.1.cmml" xref="S2.T3.12.12.2.1.1.m1.1.1">subscript</csymbol><ci id="S2.T3.12.12.2.1.1.m1.1.1.2.cmml" xref="S2.T3.12.12.2.1.1.m1.1.1.2">𝐿</ci><ci id="S2.T3.12.12.2.1.1.m1.1.1.3.cmml" xref="S2.T3.12.12.2.1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.12.12.2.1.1.m1.1c">L_{i}</annotation></semantics></math><span id="S2.T3.14.14.4.3.3.2" class="ltx_text" style="font-size:70%;"> and estimated </span><math id="S2.T3.13.13.3.2.2.m2.1" class="ltx_Math" alttext="\hat{L}_{i}" display="inline"><semantics id="S2.T3.13.13.3.2.2.m2.1a"><msub id="S2.T3.13.13.3.2.2.m2.1.1" xref="S2.T3.13.13.3.2.2.m2.1.1.cmml"><mover accent="true" id="S2.T3.13.13.3.2.2.m2.1.1.2" xref="S2.T3.13.13.3.2.2.m2.1.1.2.cmml"><mi mathsize="70%" id="S2.T3.13.13.3.2.2.m2.1.1.2.2" xref="S2.T3.13.13.3.2.2.m2.1.1.2.2.cmml">L</mi><mo mathsize="70%" id="S2.T3.13.13.3.2.2.m2.1.1.2.1" xref="S2.T3.13.13.3.2.2.m2.1.1.2.1.cmml">^</mo></mover><mi mathsize="70%" id="S2.T3.13.13.3.2.2.m2.1.1.3" xref="S2.T3.13.13.3.2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T3.13.13.3.2.2.m2.1b"><apply id="S2.T3.13.13.3.2.2.m2.1.1.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.T3.13.13.3.2.2.m2.1.1.1.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1">subscript</csymbol><apply id="S2.T3.13.13.3.2.2.m2.1.1.2.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1.2"><ci id="S2.T3.13.13.3.2.2.m2.1.1.2.1.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1.2.1">^</ci><ci id="S2.T3.13.13.3.2.2.m2.1.1.2.2.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1.2.2">𝐿</ci></apply><ci id="S2.T3.13.13.3.2.2.m2.1.1.3.cmml" xref="S2.T3.13.13.3.2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.13.13.3.2.2.m2.1c">\hat{L}_{i}</annotation></semantics></math><span id="S2.T3.14.14.4.3.3.3" class="ltx_text" style="font-size:70%;"> endpoint locations across multiple utterances </span><math id="S2.T3.14.14.4.3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.T3.14.14.4.3.3.m3.1a"><mi mathsize="70%" id="S2.T3.14.14.4.3.3.m3.1.1" xref="S2.T3.14.14.4.3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.T3.14.14.4.3.3.m3.1b"><ci id="S2.T3.14.14.4.3.3.m3.1.1.cmml" xref="S2.T3.14.14.4.3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.14.14.4.3.3.m3.1c">N</annotation></semantics></math><span id="S2.T3.14.14.4.3.3.4" class="ltx_text" style="font-size:70%;">.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.17.17" class="ltx_tr">
<td id="S2.T3.17.17.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T3.17.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.17.17.4.1.1" class="ltx_p"><a href="#Sx1.8.8.8"><abbr href="#Sx1.8.8.8" title="minimum mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMSE</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.15.15.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;">
<span id="S2.T3.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.15.15.1.1.1" class="ltx_p"><span id="S2.T3.15.15.1.1.1.1" class="ltx_text" style="font-size:70%;">Estimator </span><math id="S2.T3.15.15.1.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\hat{x}_{MMSE}=E[X|Y]" display="inline"><semantics id="S2.T3.15.15.1.1.1.m1.1a"><mrow id="S2.T3.15.15.1.1.1.m1.1.1" xref="S2.T3.15.15.1.1.1.m1.1.1.cmml"><msub id="S2.T3.15.15.1.1.1.m1.1.1.3" xref="S2.T3.15.15.1.1.1.m1.1.1.3.cmml"><mover accent="true" id="S2.T3.15.15.1.1.1.m1.1.1.3.2" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2.cmml"><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.2.2" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2.2.cmml">x</mi><mo mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.2.1" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S2.T3.15.15.1.1.1.m1.1.1.3.3" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.cmml"><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.2" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.1" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.3" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.1a" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.4" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.1b" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.3.3.5" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.5.cmml">E</mi></mrow></msub><mo mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.2" xref="S2.T3.15.15.1.1.1.m1.1.1.2.cmml">=</mo><mrow id="S2.T3.15.15.1.1.1.m1.1.1.1" xref="S2.T3.15.15.1.1.1.m1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.3" xref="S2.T3.15.15.1.1.1.m1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.T3.15.15.1.1.1.m1.1.1.1.2" xref="S2.T3.15.15.1.1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.2" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.2.cmml">X</mi><mo fence="false" mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.1" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.1.cmml">|</mo><mi mathsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.3" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo maxsize="70%" minsize="70%" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.15.15.1.1.1.m1.1b"><apply id="S2.T3.15.15.1.1.1.m1.1.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1"><eq id="S2.T3.15.15.1.1.1.m1.1.1.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.2"></eq><apply id="S2.T3.15.15.1.1.1.m1.1.1.3.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.15.15.1.1.1.m1.1.1.3.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3">subscript</csymbol><apply id="S2.T3.15.15.1.1.1.m1.1.1.3.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2"><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.2.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2.1">^</ci><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.2.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.2.2">𝑥</ci></apply><apply id="S2.T3.15.15.1.1.1.m1.1.1.3.3.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3"><times id="S2.T3.15.15.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.1"></times><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.2">𝑀</ci><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.3.3.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.3">𝑀</ci><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.3.4.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.4">𝑆</ci><ci id="S2.T3.15.15.1.1.1.m1.1.1.3.3.5.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.3.3.5">𝐸</ci></apply></apply><apply id="S2.T3.15.15.1.1.1.m1.1.1.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1"><times id="S2.T3.15.15.1.1.1.m1.1.1.1.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.2"></times><ci id="S2.T3.15.15.1.1.1.m1.1.1.1.3.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.3">𝐸</ci><apply id="S2.T3.15.15.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.2">𝑋</ci><ci id="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T3.15.15.1.1.1.m1.1.1.1.1.1.1.3">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.15.15.1.1.1.m1.1c">\displaystyle\hat{x}_{MMSE}=E[X|Y]</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.17.17.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;">
<span id="S2.T3.17.17.3.2" class="ltx_inline-block ltx_align_top"><span id="S2.T3.17.17.3.2.3" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.17.17.3.2.2" class="ltx_p"><span id="S2.T3.17.17.3.2.2.1" class="ltx_text" style="font-size:70%;">MMSE is a statistical estimation technique used in speech enhancement to minimize the mean square error between the estimated </span><math id="S2.T3.16.16.2.1.1.m1.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.T3.16.16.2.1.1.m1.1a"><mi mathsize="70%" id="S2.T3.16.16.2.1.1.m1.1.1" xref="S2.T3.16.16.2.1.1.m1.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.T3.16.16.2.1.1.m1.1b"><ci id="S2.T3.16.16.2.1.1.m1.1.1.cmml" xref="S2.T3.16.16.2.1.1.m1.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.16.16.2.1.1.m1.1c">Y</annotation></semantics></math><span id="S2.T3.17.17.3.2.2.2" class="ltx_text" style="font-size:70%;"> and true </span><math id="S2.T3.17.17.3.2.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.T3.17.17.3.2.2.m2.1a"><mi mathsize="70%" id="S2.T3.17.17.3.2.2.m2.1.1" xref="S2.T3.17.17.3.2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.T3.17.17.3.2.2.m2.1b"><ci id="S2.T3.17.17.3.2.2.m2.1.1.cmml" xref="S2.T3.17.17.3.2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.17.17.3.2.2.m2.1c">X</annotation></semantics></math><span id="S2.T3.17.17.3.2.2.3" class="ltx_text" style="font-size:70%;"> clean speech signals</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.18.18" class="ltx_tr">
<td id="S2.T3.18.18.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T3.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.18.18.2.1.1" class="ltx_p"><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.18.18.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;">
<span id="S2.T3.18.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.18.18.1.1.1" class="ltx_p"><math id="S2.T3.18.18.1.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\frac{\sum_{t=1}^{T}g(t)\cdot\text{STOI}_{\text{frame}}(t)}{\sum_{t=1}^{T}g(t)}" display="inline"><semantics id="S2.T3.18.18.1.1.1.m1.3a"><mstyle displaystyle="true" id="S2.T3.18.18.1.1.1.m1.3.3" xref="S2.T3.18.18.1.1.1.m1.3.3.cmml"><mfrac id="S2.T3.18.18.1.1.1.m1.3.3a" xref="S2.T3.18.18.1.1.1.m1.3.3.cmml"><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.cmml"><msubsup id="S2.T3.18.18.1.1.1.m1.2.2.2.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.2.cmml">∑</mo><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.cmml"><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.2.cmml">t</mi><mo mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.1.cmml">=</mo><mn mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.3.cmml">T</mi></msubsup><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.4" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.cmml"><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.cmml"><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.cmml"><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.1.cmml">​</mo><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.3.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.3.2.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.cmml">(</mo><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.1.1.1.1" xref="S2.T3.18.18.1.1.1.m1.1.1.1.1.cmml">t</mi><mo maxsize="70%" minsize="70%" rspace="0.055em" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.3.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.cmml">)</mo></mrow></mrow><mo mathsize="70%" rspace="0.222em" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.1.cmml">⋅</mo><msub id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.cmml"><mtext mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2a.cmml">STOI</mtext><mtext mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3a.cmml">frame</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.1.cmml">​</mo><mrow id="S2.T3.18.18.1.1.1.m1.2.2.2.4.3.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.3.2.1" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.cmml">(</mo><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.2.cmml">t</mi><mo maxsize="70%" minsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.3.2.2" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.T3.18.18.1.1.1.m1.3.3.3" xref="S2.T3.18.18.1.1.1.m1.3.3.3.cmml"><msubsup id="S2.T3.18.18.1.1.1.m1.3.3.3.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.cmml"><mo maxsize="70%" minsize="70%" stretchy="true" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.2.cmml">∑</mo><mrow id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.cmml"><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.2.cmml">t</mi><mo mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.1" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.1.cmml">=</mo><mn mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.3" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.3.cmml">1</mn></mrow><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.3" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.3.cmml">T</mi></msubsup><mrow id="S2.T3.18.18.1.1.1.m1.3.3.3.3" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.cmml"><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.3.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.T3.18.18.1.1.1.m1.3.3.3.3.1" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.1.cmml">​</mo><mrow id="S2.T3.18.18.1.1.1.m1.3.3.3.3.3.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.cmml"><mo maxsize="70%" minsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.3.3.2.1" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.cmml">(</mo><mi mathsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.1" xref="S2.T3.18.18.1.1.1.m1.3.3.3.1.cmml">t</mi><mo maxsize="70%" minsize="70%" id="S2.T3.18.18.1.1.1.m1.3.3.3.3.3.2.2" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="S2.T3.18.18.1.1.1.m1.3b"><apply id="S2.T3.18.18.1.1.1.m1.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3"><divide id="S2.T3.18.18.1.1.1.m1.3.3.4.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3"></divide><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2"><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3">superscript</csymbol><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3">subscript</csymbol><sum id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.2"></sum><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3"><eq id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.1"></eq><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.2">𝑡</ci><cn type="integer" id="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.3.3">𝑇</ci></apply><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.4.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4"><times id="S2.T3.18.18.1.1.1.m1.2.2.2.4.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.1"></times><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2"><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.1">⋅</ci><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2"><times id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.1"></times><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.2.2">𝑔</ci><ci id="S2.T3.18.18.1.1.1.m1.1.1.1.1.cmml" xref="S2.T3.18.18.1.1.1.m1.1.1.1.1">𝑡</ci></apply><apply id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3"><csymbol cd="ambiguous" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3">subscript</csymbol><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2a.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2"><mtext mathsize="70%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.2">STOI</mtext></ci><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3a.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3"><mtext mathsize="49%" id="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.4.2.3.3">frame</mtext></ci></apply></apply><ci id="S2.T3.18.18.1.1.1.m1.2.2.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.2.2.2.2">𝑡</ci></apply></apply><apply id="S2.T3.18.18.1.1.1.m1.3.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3"><apply id="S2.T3.18.18.1.1.1.m1.3.3.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.1.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2">superscript</csymbol><apply id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.1.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2">subscript</csymbol><sum id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.2.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.2"></sum><apply id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3"><eq id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.1"></eq><ci id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.2">𝑡</ci><cn type="integer" id="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.2.3.3">1</cn></apply></apply><ci id="S2.T3.18.18.1.1.1.m1.3.3.3.2.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.2.3">𝑇</ci></apply><apply id="S2.T3.18.18.1.1.1.m1.3.3.3.3.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3"><times id="S2.T3.18.18.1.1.1.m1.3.3.3.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.1"></times><ci id="S2.T3.18.18.1.1.1.m1.3.3.3.3.2.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.3.2">𝑔</ci><ci id="S2.T3.18.18.1.1.1.m1.3.3.3.1.cmml" xref="S2.T3.18.18.1.1.1.m1.3.3.3.1">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.18.18.1.1.1.m1.3c">\displaystyle\frac{\sum_{t=1}^{T}g(t)\cdot\text{STOI}_{\text{frame}}(t)}{\sum_{t=1}^{T}g(t)}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.18.18.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;">
<span id="S2.T3.18.18.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.18.18.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.18.18.3.1.2" class="ltx_p"><span id="S2.T3.18.18.3.1.2.1" class="ltx_text" style="font-size:70%;">STOI is a metric used to assess the intelligibility of time-frequency weighted noisy speech. It is based on the idea that human speech perception relies on the availability of important acoustic features in short time frames </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T3.18.18.3.1.2.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.T3.18.18.3.1.2.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T3.18.18.3.1.2.4" class="ltx_text" style="font-size:70%;">.</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.19.22" class="ltx_tr">
<td id="S2.T3.19.22.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S2.T3.19.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.22.1.1.1" class="ltx_p"><a href="#Sx1.33.33.33"><abbr href="#Sx1.33.33.33" title="normalized covariance measure" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NCM</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.19.22.2" class="ltx_td ltx_align_middle" style="width:170.7pt;"></td>
<td id="S2.T3.19.22.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:270.3pt;">
<span id="S2.T3.19.22.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.19.22.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.19.22.3.1.2" class="ltx_p"><span id="S2.T3.19.22.3.1.2.1" class="ltx_text" style="font-size:70%;">NCM</span></span>
</span>
</td>
</tr>
<tr id="S2.T3.19.19" class="ltx_tr">
<td id="S2.T3.19.19.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:42.7pt;">
<span id="S2.T3.19.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.19.2.1.1" class="ltx_p"><a href="#Sx1.88.88.88"><abbr href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SDR</span></span></abbr></a><span id="S2.T3.19.19.2.1.1.1" class="ltx_text" style="font-size:70%;">, </span><a href="#Sx1.89.89.89"><abbr href="#Sx1.89.89.89" title="source-to-artifact ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SAR</span></span></abbr></a><span id="S2.T3.19.19.2.1.1.2" class="ltx_text" style="font-size:70%;">, and </span><a href="#Sx1.90.90.90"><abbr href="#Sx1.90.90.90" title="source-to-interference ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SIR</span></span></abbr></a></span>
</span>
</td>
<td id="S2.T3.19.19.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:170.7pt;">
<span id="S2.T3.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.19.19.1.1.1" class="ltx_p"><math id="S2.T3.19.19.1.1.1.m1.9" class="ltx_Math" alttext="\displaystyle 10log_{10}\frac{{\left\|s_{target}\right\|}^{2}}{{\left\|e_{total}\right\|}^{2}},\hskip 8.5359pt10log_{10}\frac{{\left\|s_{target}+e_{inter}+e_{noise}\right\|}^{2}}{{\left\|e_{artifacts}\right\|}^{2}},\newline
\hskip 8.5359pt10log_{10}\frac{{\left\|s_{target}\right\|}^{2}}{{\left\|e_{inter}\right\|}^{2}}" display="inline"><semantics id="S2.T3.19.19.1.1.1.m1.9a"><mrow id="S2.T3.19.19.1.1.1.m1.9.9.3" xref="S2.T3.19.19.1.1.1.m1.9.9.4.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.7.7.1.1" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.cmml"><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.2" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.1" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.3" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.1a" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.4" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.1b" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.1.cmml">​</mo><msub id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.2" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.2.cmml">g</mi><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.3" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.3.cmml">10</mn></msub><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.1c" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.1.cmml">​</mo><mstyle displaystyle="true" id="S2.T3.19.19.1.1.1.m1.2.2" xref="S2.T3.19.19.1.1.1.m1.2.2.cmml"><mfrac id="S2.T3.19.19.1.1.1.m1.2.2a" xref="S2.T3.19.19.1.1.1.m1.2.2.cmml"><msup id="S2.T3.19.19.1.1.1.m1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.1.1.1.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.2.1.cmml">‖</mo><msub id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.2.cmml">s</mi><mrow id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1a" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.4" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1b" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.5" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1c" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.6" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1d" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.7" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.7.cmml">t</mi></mrow></msub><mo id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.1.1.1.3.cmml">2</mn></msup><msup id="S2.T3.19.19.1.1.1.m1.2.2.2" xref="S2.T3.19.19.1.1.1.m1.2.2.2.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.2" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.2.1.cmml">‖</mo><msub id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.2.cmml">e</mi><mrow id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1a" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.4" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1b" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.5" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1c" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.6" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.6.cmml">l</mi></mrow></msub><mo id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.3" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.2.2.2.3" xref="S2.T3.19.19.1.1.1.m1.2.2.2.3.cmml">2</mn></msup></mfrac></mstyle></mrow><mo mathsize="70%" rspace="1.017em" id="S2.T3.19.19.1.1.1.m1.9.9.3.4" xref="S2.T3.19.19.1.1.1.m1.9.9.4.cmml">,</mo><mrow id="S2.T3.19.19.1.1.1.m1.8.8.2.2" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.cmml"><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.2" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.1" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.3" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.1a" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.4" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.1b" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.1.cmml">​</mo><msub id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.2" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.2.cmml">g</mi><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.3" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.3.cmml">10</mn></msub><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.1c" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.1.cmml">​</mo><mstyle displaystyle="true" id="S2.T3.19.19.1.1.1.m1.4.4" xref="S2.T3.19.19.1.1.1.m1.4.4.cmml"><mfrac id="S2.T3.19.19.1.1.1.m1.4.4a" xref="S2.T3.19.19.1.1.1.m1.4.4.cmml"><msup id="S2.T3.19.19.1.1.1.m1.3.3.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.2.1.cmml">‖</mo><mrow id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.cmml"><msub id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.2.cmml">s</mi><mrow id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1a" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.4" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1b" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.5" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1c" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.6" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1d" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.7" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.7.cmml">t</mi></mrow></msub><mo mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1.cmml">+</mo><msub id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.2.cmml">e</mi><mrow id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1a" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.4" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1b" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.5" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1c" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.6" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.6.cmml">r</mi></mrow></msub><mo mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1a" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1.cmml">+</mo><msub id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.2.cmml">e</mi><mrow id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.2" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1a" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.4" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1b" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.5" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1c" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.6" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.6.cmml">e</mi></mrow></msub></mrow><mo id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.3.3.1.3" xref="S2.T3.19.19.1.1.1.m1.3.3.1.3.cmml">2</mn></msup><msup id="S2.T3.19.19.1.1.1.m1.4.4.2" xref="S2.T3.19.19.1.1.1.m1.4.4.2.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.2" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.2.1.cmml">‖</mo><msub id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.2.cmml">e</mi><mrow id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1a" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.4" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1b" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.5" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1c" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.6" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.6.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1d" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.7" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1e" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.8" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1f" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.9" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1g" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.10" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.10.cmml">s</mi></mrow></msub><mo id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.3" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.4.4.2.3" xref="S2.T3.19.19.1.1.1.m1.4.4.2.3.cmml">2</mn></msup></mfrac></mstyle></mrow><mo mathsize="70%" rspace="1.017em" id="S2.T3.19.19.1.1.1.m1.9.9.3.5" xref="S2.T3.19.19.1.1.1.m1.9.9.4.cmml">,</mo><mrow id="S2.T3.19.19.1.1.1.m1.9.9.3.3" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.cmml"><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.2" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.1" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.3" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.1a" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.4" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.1b" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.1.cmml">​</mo><msub id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.2" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.2.cmml">g</mi><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.3" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.3.cmml">10</mn></msub><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.1c" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.1.cmml">​</mo><mstyle displaystyle="true" id="S2.T3.19.19.1.1.1.m1.6.6" xref="S2.T3.19.19.1.1.1.m1.6.6.cmml"><mfrac id="S2.T3.19.19.1.1.1.m1.6.6a" xref="S2.T3.19.19.1.1.1.m1.6.6.cmml"><msup id="S2.T3.19.19.1.1.1.m1.5.5.1" xref="S2.T3.19.19.1.1.1.m1.5.5.1.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.2.1.cmml">‖</mo><msub id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.2.cmml">s</mi><mrow id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1a" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.4" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1b" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.5" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1c" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.6" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1d" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.7" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.7.cmml">t</mi></mrow></msub><mo id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.5.5.1.3" xref="S2.T3.19.19.1.1.1.m1.5.5.1.3.cmml">2</mn></msup><msup id="S2.T3.19.19.1.1.1.m1.6.6.2" xref="S2.T3.19.19.1.1.1.m1.6.6.2.cmml"><mrow id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.2.cmml"><mo id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.2" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.2.1.cmml">‖</mo><msub id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.2" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.2.cmml">e</mi><mrow id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.cmml"><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.2" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.3" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1a" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.4" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1b" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.5" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1c" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.6" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.6.cmml">r</mi></mrow></msub><mo id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.3" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.2.1.cmml">‖</mo></mrow><mn mathsize="70%" id="S2.T3.19.19.1.1.1.m1.6.6.2.3" xref="S2.T3.19.19.1.1.1.m1.6.6.2.3.cmml">2</mn></msup></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T3.19.19.1.1.1.m1.9b"><list id="S2.T3.19.19.1.1.1.m1.9.9.4.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3"><apply id="S2.T3.19.19.1.1.1.m1.7.7.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1"><times id="S2.T3.19.19.1.1.1.m1.7.7.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.1"></times><cn type="integer" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.2">10</cn><ci id="S2.T3.19.19.1.1.1.m1.7.7.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.3">𝑙</ci><ci id="S2.T3.19.19.1.1.1.m1.7.7.1.1.4.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.4">𝑜</ci><apply id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.1.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.2.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.2">𝑔</ci><cn type="integer" id="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.3.cmml" xref="S2.T3.19.19.1.1.1.m1.7.7.1.1.5.3">10</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2"><divide id="S2.T3.19.19.1.1.1.m1.2.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2"></divide><apply id="S2.T3.19.19.1.1.1.m1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.1.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.2">𝑠</ci><apply id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3"><times id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.3">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.4">𝑟</ci><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.5">𝑔</ci><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.6">𝑒</ci><ci id="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.7.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.1.1.1.3.7">𝑡</ci></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.1.1.1.3">2</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.2.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.2.2.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.2.2.2.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.2">𝑒</ci><apply id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3"><times id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.2">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.3">𝑜</ci><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.4">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.5">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.1.1.1.3.6">𝑙</ci></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.2.2.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.2.2.2.3">2</cn></apply></apply></apply><apply id="S2.T3.19.19.1.1.1.m1.8.8.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2"><times id="S2.T3.19.19.1.1.1.m1.8.8.2.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.1"></times><cn type="integer" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.2">10</cn><ci id="S2.T3.19.19.1.1.1.m1.8.8.2.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.3">𝑙</ci><ci id="S2.T3.19.19.1.1.1.m1.8.8.2.2.4.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.4">𝑜</ci><apply id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.1.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.2.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.2">𝑔</ci><cn type="integer" id="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.3.cmml" xref="S2.T3.19.19.1.1.1.m1.8.8.2.2.5.3">10</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.4.4.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4"><divide id="S2.T3.19.19.1.1.1.m1.4.4.3.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4"></divide><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.3.3.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1"><plus id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.1"></plus><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.2">𝑠</ci><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3"><times id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.2">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.3">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.4">𝑟</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.5">𝑔</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.6">𝑒</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.7.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.2.3.7">𝑡</ci></apply></apply><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.2">𝑒</ci><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3"><times id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.2">𝑖</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.3">𝑛</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.4">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.5">𝑒</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.3.3.6">𝑟</ci></apply></apply><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.2">𝑒</ci><apply id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3"><times id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.2">𝑛</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.3">𝑜</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.4">𝑖</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.5">𝑠</ci><ci id="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.1.1.1.4.3.6">𝑒</ci></apply></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.3.3.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.3.3.1.3">2</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.4.4.2.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.4.4.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.4.4.2.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.2">𝑒</ci><apply id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3"><times id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.2">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.3">𝑟</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.4">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.5">𝑖</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.6">𝑓</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.7.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.7">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.8.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.8">𝑐</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.9.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.9">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.10.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.1.1.1.3.10">𝑠</ci></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.4.4.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.4.4.2.3">2</cn></apply></apply></apply><apply id="S2.T3.19.19.1.1.1.m1.9.9.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3"><times id="S2.T3.19.19.1.1.1.m1.9.9.3.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.1"></times><cn type="integer" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.2">10</cn><ci id="S2.T3.19.19.1.1.1.m1.9.9.3.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.3">𝑙</ci><ci id="S2.T3.19.19.1.1.1.m1.9.9.3.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.4">𝑜</ci><apply id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.1.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.2.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.2">𝑔</ci><cn type="integer" id="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.3.cmml" xref="S2.T3.19.19.1.1.1.m1.9.9.3.3.5.3">10</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.6.6.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6"><divide id="S2.T3.19.19.1.1.1.m1.6.6.3.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6"></divide><apply id="S2.T3.19.19.1.1.1.m1.5.5.1.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.5.5.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.5.5.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.2">𝑠</ci><apply id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3"><times id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.2">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.3">𝑎</ci><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.4">𝑟</ci><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.5">𝑔</ci><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.6">𝑒</ci><ci id="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.7.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.1.1.1.3.7">𝑡</ci></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.5.5.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.5.5.1.3">2</cn></apply><apply id="S2.T3.19.19.1.1.1.m1.6.6.2.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.6.6.2.2.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2">superscript</csymbol><apply id="S2.T3.19.19.1.1.1.m1.6.6.2.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1"><csymbol cd="latexml" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.2.1.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.2">norm</csymbol><apply id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1"><csymbol cd="ambiguous" id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.1.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1">subscript</csymbol><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.2.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.2">𝑒</ci><apply id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3"><times id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.1"></times><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.2.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.2">𝑖</ci><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.3.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.3">𝑛</ci><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.4.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.4">𝑡</ci><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.5.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.5">𝑒</ci><ci id="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.6.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.1.1.1.3.6">𝑟</ci></apply></apply></apply><cn type="integer" id="S2.T3.19.19.1.1.1.m1.6.6.2.3.cmml" xref="S2.T3.19.19.1.1.1.m1.6.6.2.3">2</cn></apply></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.19.19.1.1.1.m1.9c">\displaystyle 10log_{10}\frac{{\left\|s_{target}\right\|}^{2}}{{\left\|e_{total}\right\|}^{2}},\hskip 8.5359pt10log_{10}\frac{{\left\|s_{target}+e_{inter}+e_{noise}\right\|}^{2}}{{\left\|e_{artifacts}\right\|}^{2}},\newline
\hskip 8.5359pt10log_{10}\frac{{\left\|s_{target}\right\|}^{2}}{{\left\|e_{inter}\right\|}^{2}}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T3.19.19.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:270.3pt;">
<span id="S2.T3.19.19.3.1" class="ltx_inline-block ltx_align_top"><span id="S2.T3.19.19.3.1.1" class="ltx_ERROR undefined">\Ac</span>
<span id="S2.T3.19.19.3.1.2" class="ltx_p"><span id="S2.T3.19.19.3.1.2.1" class="ltx_text" style="font-size:70%;">SDR,  </span><a href="#Sx1.89.89.89"><span href="#Sx1.89.89.89" title="source-to-artifact ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">source-to-artifact ratio</span></span></span></a><span id="S2.T3.19.19.3.1.2.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.89.89.89"><abbr href="#Sx1.89.89.89" title="source-to-artifact ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SAR</span></span></abbr></a><span id="S2.T3.19.19.3.1.2.3" class="ltx_text" style="font-size:70%;">), and  </span><a href="#Sx1.90.90.90"><span href="#Sx1.90.90.90" title="source-to-interference ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">source-to-interference ratio</span></span></span></a><span id="S2.T3.19.19.3.1.2.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.90.90.90"><abbr href="#Sx1.90.90.90" title="source-to-interference ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SIR</span></span></abbr></a><span id="S2.T3.19.19.3.1.2.5" class="ltx_text" style="font-size:70%;">) are metrics objectively assess and compare speech source-separation algorithms based on accuracy and minimization of distortions and interference.  </span><a href="#Sx1.88.88.88"><span href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">source-to-distortion ratio</span></span></span></a><span id="S2.T3.19.19.3.1.2.6" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.88.88.88"><abbr href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SDR</span></span></abbr></a><span id="S2.T3.19.19.3.1.2.7" class="ltx_text" style="font-size:70%;">) gauges source separation quality by comparing true source power to introduced distortion. </span><a href="#Sx1.89.89.89"><abbr href="#Sx1.89.89.89" title="source-to-artifact ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SAR</span></span></abbr></a><span id="S2.T3.19.19.3.1.2.8" class="ltx_text" style="font-size:70%;"> evaluates source separation from artifacts or noise, while </span><a href="#Sx1.90.90.90"><abbr href="#Sx1.90.90.90" title="source-to-interference ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SIR</span></span></abbr></a><span id="S2.T3.19.19.3.1.2.9" class="ltx_text" style="font-size:70%;"> measures the ratio of true source power to interference after separation.</span></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Taxonomy of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>-based AI techniques </h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Several artificial intelligence techniques have been employed to enhance the efficacy of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. While some rely on 1D data, others process information in a 2D image format. Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes all AI algorithms utilized, alongside the features employed and hybrid AI methodologies. Additionally, Table <a href="#S3.T4" title="Table 4 ‣ 3.3 -based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides a summary of DL-based techniques utilized in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> hearing devices.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2403.15442/assets/x3.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Taxonomy of the employed AI techniques for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>-based AI Implementation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<span id="S3.SS1.p1.1" class="ltx_ERROR undefined">\Ac</span>
<p id="S3.SS1.p1.2" class="ltx_p">CI programming involves adjusting device settings to optimize sound perception for individual users. This includes setting stimulation levels, electrode configurations, and signal processing parameters to enhance speech understanding and auditory experiences based on patient feedback and objective measures. In 2010, govaerts et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> describes the development of an intelligent agent, called  <a href="#Sx1.110.110.110"><span href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">fitting to outcomes expert</span></span></span></a> (<a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a>), for optimizing <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> programming as described in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 \AcML-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The agent analyzes map settings and psycho-acoustic test results to recommend and execute modifications to improve outcomes. The tool focuses on an outcome-driven approach, reducing fitting time and improving the quality of fitting. It introduces principles of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in the <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> fitting process. The study proposed objective measures and group electrode settings as strategies to reduce fitting time.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2403.15442/assets/x4.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="222" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;"> The fundamental operating concept of the <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> involves inputting an initial program and multiple psychoacoustic test outcomes. <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> processes this information and generates fitting suggestions as its output. When integrated with proprietary outcome and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> fitting software, the shaded boxes represent its functionality, while the unfilled boxes represent its standalone capability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Audiqueen is a dataset with A and E (A&amp;E) phoneme discrimination.</span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Similarly in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, all have employed <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> for programming <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. Vaerenberg et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> discusses the use of <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> for programming <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> sound processors in new users. <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> modifies maps based on specific outcome measures using heuristic logic and deterministic rules. The study showed positive results and optimized performance after three months of programming, with good speech audiometry and loudness scaling outcomes. The paper highlights the importance of individualized programming parameters and the need for outcome-based adjustments rather than relying solely on comfort. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, computer-assisted <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> fitting using <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> assessed its impact on speech understanding. Results from 25 recipients showed that 84% benefited from suggested map changes, significantly improving speech understanding thanks to the learning capacity of <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a>. This approach offers standardized, systematic <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> fitting, enhancing auditory performance.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The COCH gene, referred to as the Cochlin gene, is responsible for encoding the cochlin protein situated on chromosome 14 in humans, primarily expressed in the inner ear. Cochlin predominantly functions within the cochlea, a spiral-shaped structure involved in the process of hearing, contributing to its structural integrity and proper operation. Wathour et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> discusses the use of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> fitting through two case studies. The first case involves a 75-year-old lady who received a left ear implant due to gradual and severe hearing loss in both ears without a clear cause. In the second case, a 72-year-old man with a COCH gene mutation causing profound hearing loss in both ears underwent a right ear implant, to assess whether <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> programming using the <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> software <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> application, could improve <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> performance. The results showed that <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>-assisted fitting led to improvements in auditory outcomes for adult <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients who had previously undergone manual fitting. The <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> suggestions helped improve word recognition scores and loudness scaling curves. Likewise, Waltzman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> incorporate <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in programming <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, aimed to assess, on fifty-five adult’s <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> , the performance and standardization of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-based programming. The results showed that the <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-based <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> for some patients performed better, while others had similar results, yet he majority preferred the <a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a> system.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_ERROR undefined">\Ac</span>ML-based methods</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> is a subfield of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> that focuses on developing algorithms and statistical models that enable computer systems to improve their performance in a specific task by learning features from input data. The research in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> focuses on algorithm-based hearing and speech therapy rehabilitation after cochlear implantation, particularly for older individuals. They propose the development of an <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>-based application that offers personalized hearing therapy tailored to the patient’s needs, such as select exercises, adjust difficulty levels, and analyze patient difficulties. It operates independently, reducing the reliance on local speech therapists, cost-effective, and accessible alternative to traditional therapy, improving outcomes and quality of life for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients.
In addition, Torresen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> discusses the use of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> techniques to streamline the adjustment process for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. The goal is to predict optimal adjustment values for new patients based on data from previous patients. By analyzing data from 158 former patients, the study shows that while fully automatic adjustments are not possible, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> can provide a good starting point for manual adjustment. The research also identifies the most important electrodes to measure for predicting levels of other electrodes. This approach has the potential to reduce programming time, benefit patients, and improve speech recognition scores, particularly for young children and patients with post-lingual deafness. Henry et al. in their <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> investigates the importance of acoustic features in optimizing intelligibility for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> in noisy environments. The study employs <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> algorithms and extracts acoustic features from speech and noise mixtures to train a  <a href="#Sx1.50.50.50"><span href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep neural networks</span></span></span></a> (<a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a>). The results, using various metrics, reveal that frequency domain features, particularly Gammatone features, perform best for normal hearing, while Mel spectrogram features exhibit the best overall performance for hearing impairment. The study suggests a stronger correlation between  <a href="#Sx1.121.121.121"><span href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">short-time objective intelligibility</span></span></span></a> (<a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a>) and  <a href="#Sx1.33.33.33"><span href="#Sx1.33.33.33" title="normalized covariance measure" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">normalized covariance measure</span></span></span></a> (<a href="#Sx1.33.33.33"><abbr href="#Sx1.33.33.33" title="normalized covariance measure" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NCM</span></span></abbr></a>) in predicting intelligibility for hearing-impaired listeners. The findings can aid in designing adaptive intelligibility enhancement systems for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> based on noise characteristics.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Moreover, the research in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> focuses on imputing missing values in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> candidate audiometric data. This study assessed the performance of various imputation algorithms using a dataset of 7,451 audiograms from <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> patients. The results showed that the quantity of missing data affected the imputation performance, with greater amounts leading to poorer results. The distribution of sparsity in the audiometric data was found to be non-uniform, with inter-octave frequencies being less commonly tested. The  <a href="#Sx1.96.96.96"><span href="#Sx1.96.96.96" title="multiple imputation by chained equations" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">multiple imputation by chained equations</span></span></span></a> (<a href="#Sx1.96.96.96"><abbr href="#Sx1.96.96.96" title="multiple imputation by chained equations" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MICE</span></span></abbr></a>) method, safely imputed up to six missing data points in an 11-frequency audiogram, consistently outperformed other models. This study highlights the importance of imputation techniques in maximizing datasets in hearing healthcare research. Xu et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> explores the objective discrimination of bimodal speech using <a href="#Sx1.99.99.99"><span href="#Sx1.99.99.99" title="frequency following responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">frequency following responses</span>s</span></span></a>. The study investigates the neural encoding of  <a href="#Sx1.101.101.101"><span href="#Sx1.101.101.101" title="fundamental frequency" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">fundamental frequency</span></span></span></a> (<a href="#Sx1.101.101.101"><abbr href="#Sx1.101.101.101" title="fundamental frequency" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><math class="ltx_Math" alttext="f_{0}" display="inline"><semantics><msub><mi mathsize="80%">f</mi><mn mathsize="80%">0</mn></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑓</ci><cn type="integer">0</cn></apply></annotation-xml><annotation encoding="application/x-tex">f_{0}</annotation></semantics></math></span></abbr></a>), called also pitch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and  <a href="#Sx1.102.102.102"><span href="#Sx1.102.102.102" title="temporal fine structure cues" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">temporal fine structure cues</span></span></span></a> (<a href="#Sx1.102.102.102"><abbr href="#Sx1.102.102.102" title="temporal fine structure cues" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">TFSC</span></span></abbr></a>) in simulated bimodal speech conditions. The results show that increasing acoustic bandwidth enhances the neural representation of <a href="#Sx1.101.101.101"><abbr href="#Sx1.101.101.101" title="fundamental frequency" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><math class="ltx_Math" alttext="f_{0}" display="inline"><semantics><msub><mi mathsize="80%">f</mi><mn mathsize="80%">0</mn></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑓</ci><cn type="integer">0</cn></apply></annotation-xml><annotation encoding="application/x-tex">f_{0}</annotation></semantics></math></span></abbr></a> and <a href="#Sx1.102.102.102"><abbr href="#Sx1.102.102.102" title="temporal fine structure cues" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">TFSC</span></span></abbr></a> components in the non-implanted ear. Moreover, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> algorithms successfully classify and discriminate <a href="#Sx1.99.99.99"><abbr href="#Sx1.99.99.99" title="frequency following responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">FFR</span>s</span></abbr></a> based on spectral differences between vowels. The findings suggest that the enhancement of <a href="#Sx1.101.101.101"><abbr href="#Sx1.101.101.101" title="fundamental frequency" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><math class="ltx_Math" alttext="f_{0}" display="inline"><semantics><msub><mi mathsize="80%">f</mi><mn mathsize="80%">0</mn></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑓</ci><cn type="integer">0</cn></apply></annotation-xml><annotation encoding="application/x-tex">f_{0}</annotation></semantics></math></span></abbr></a> and <a href="#Sx1.102.102.102"><abbr href="#Sx1.102.102.102" title="temporal fine structure cues" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">TFSC</span></span></abbr></a> neural encoding with increasing bandwidth is predictive of perceptual bimodal benefit in speech-in-noise tasks. <a href="#Sx1.99.99.99"><abbr href="#Sx1.99.99.99" title="frequency following responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">FFR</span>s</span></abbr></a> may serve as a useful tool for objectively assessing individual variability in bimodal hearing. The research conducted by Crowson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> aimed to predict postoperative <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> performance using supervised <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>. The authors used neural networks and decision tree-based ensemble algorithms on a dataset of 1,604 adults who received <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. They included 282 text and numerical variables related to demographics, audiometric data, and patient-reported outcomes. The results showed that the neural network model achieved a 1-year postoperative performance prediction  <a href="#Sx1.103.103.103"><span href="#Sx1.103.103.103" title="root mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">root mean square error</span></span></span></a> (<a href="#Sx1.103.103.103"><abbr href="#Sx1.103.103.103" title="root mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RMSE</span></span></abbr></a>) of 0.57 and classification accuracy of 95.4%. When both text and numerical variables were used, the <a href="#Sx1.103.103.103"><abbr href="#Sx1.103.103.103" title="root mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RMSE</span></span></abbr></a> was 25.0% and classification accuracy was 73.3%. The study identified influential variables such as preoperative sentence-test performance, age at surgery, and specific questionnaire responses. The findings suggest that supervised <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> can predict <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> performance and provide insights into factors affecting outcomes.
In the same context of prediction, Mikulskis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> focuse on predicting the attachment of broad-spectrum pathogens to coating materials for biomedical devices as illustrated in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 \AcML-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The authors employ <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> methods to generate quantitative predictions for pathogen attachment to a large library of polymers. This approach aims to accelerate the discovery of materials that resist bacterial biofilm formation, reducing the rate of infections associated with medical devices. The study highlights the need for new materials that prevent bacterial colonization and biofilm development, particularly in the context of antibiotic resistance. The results demonstrate the potential of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> in designing polymers with low pathogen attachment, offering promising candidate materials for implantable and indwelling medical devices. Similarly, Alohali et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> focuses on using <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> algorithms to predict the post-operative electrode impedances in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> patients. The study used a dataset of 80 pediatric patients and considered factors such as patient age and intraoperative electrode impedance. The results showed that the best algorithm varied by channel, with Bayesian linear regression and neural networks providing the best results for 75% of the channels. The accuracy level ranged between 83% and 100% in half of the channels one year after surgery. Additionally, the patient’s age alone showed good prediction results for 50% of the channels at six months or one year after surgery, suggesting it could be a predictor of electrode impedance.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2403.15442/assets/x5.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="150" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;"> Diagram illustrating the procedures utilized in the production of microarrays, analysis of pathogens, data modeling, and forecasting the attachment of pathogens to novel polymers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a>-based methods</h3>

<div id="S3.SS3.p1" class="ltx_para">
<span id="S3.SS3.p1.1" class="ltx_ERROR undefined">\Acp</span>
<p id="S3.SS3.p1.2" class="ltx_p">CNN are a class of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> algorithms widely used in computer vision tasks. Their architecture includes convolutional layers that automatically learn hierarchical features from input data. The core convolution for 2D data operation is defined by the equation:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.9" class="ltx_Math" alttext="S(i,j)=(I*K)(i,j)=\sum_{m}\sum_{n}I(m,n)\cdot K(i-m,j-n)" display="block"><semantics id="S3.E1.m1.9a"><mrow id="S3.E1.m1.9.9" xref="S3.E1.m1.9.9.cmml"><mrow id="S3.E1.m1.9.9.5" xref="S3.E1.m1.9.9.5.cmml"><mi id="S3.E1.m1.9.9.5.2" xref="S3.E1.m1.9.9.5.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.5.1" xref="S3.E1.m1.9.9.5.1.cmml">​</mo><mrow id="S3.E1.m1.9.9.5.3.2" xref="S3.E1.m1.9.9.5.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.5.3.2.1" xref="S3.E1.m1.9.9.5.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">i</mi><mo id="S3.E1.m1.9.9.5.3.2.2" xref="S3.E1.m1.9.9.5.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.9.9.5.3.2.3" xref="S3.E1.m1.9.9.5.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.9.9.6" xref="S3.E1.m1.9.9.6.cmml">=</mo><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.2.cmml">I</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.7.7.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.cmml">∗</mo><mi id="S3.E1.m1.7.7.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.3.cmml">K</mi></mrow><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.3.2" xref="S3.E1.m1.7.7.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.3.2.1" xref="S3.E1.m1.7.7.1.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">i</mi><mo id="S3.E1.m1.7.7.1.3.2.2" xref="S3.E1.m1.7.7.1.3.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.3.2.3" xref="S3.E1.m1.7.7.1.3.1.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.9.9.7" xref="S3.E1.m1.9.9.7.cmml">=</mo><mrow id="S3.E1.m1.9.9.3" xref="S3.E1.m1.9.9.3.cmml"><munder id="S3.E1.m1.9.9.3.3" xref="S3.E1.m1.9.9.3.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.9.9.3.3.2" xref="S3.E1.m1.9.9.3.3.2.cmml">∑</mo><mi id="S3.E1.m1.9.9.3.3.3" xref="S3.E1.m1.9.9.3.3.3.cmml">m</mi></munder><mrow id="S3.E1.m1.9.9.3.2" xref="S3.E1.m1.9.9.3.2.cmml"><munder id="S3.E1.m1.9.9.3.2.3" xref="S3.E1.m1.9.9.3.2.3.cmml"><mo movablelimits="false" id="S3.E1.m1.9.9.3.2.3.2" xref="S3.E1.m1.9.9.3.2.3.2.cmml">∑</mo><mi id="S3.E1.m1.9.9.3.2.3.3" xref="S3.E1.m1.9.9.3.2.3.3.cmml">n</mi></munder><mrow id="S3.E1.m1.9.9.3.2.2" xref="S3.E1.m1.9.9.3.2.2.cmml"><mrow id="S3.E1.m1.9.9.3.2.2.4" xref="S3.E1.m1.9.9.3.2.2.4.cmml"><mrow id="S3.E1.m1.9.9.3.2.2.4.2" xref="S3.E1.m1.9.9.3.2.2.4.2.cmml"><mi id="S3.E1.m1.9.9.3.2.2.4.2.2" xref="S3.E1.m1.9.9.3.2.2.4.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.3.2.2.4.2.1" xref="S3.E1.m1.9.9.3.2.2.4.2.1.cmml">​</mo><mrow id="S3.E1.m1.9.9.3.2.2.4.2.3.2" xref="S3.E1.m1.9.9.3.2.2.4.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.3.2.2.4.2.3.2.1" xref="S3.E1.m1.9.9.3.2.2.4.2.3.1.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">m</mi><mo id="S3.E1.m1.9.9.3.2.2.4.2.3.2.2" xref="S3.E1.m1.9.9.3.2.2.4.2.3.1.cmml">,</mo><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">n</mi><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.9.9.3.2.2.4.2.3.2.3" xref="S3.E1.m1.9.9.3.2.2.4.2.3.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E1.m1.9.9.3.2.2.4.1" xref="S3.E1.m1.9.9.3.2.2.4.1.cmml">⋅</mo><mi id="S3.E1.m1.9.9.3.2.2.4.3" xref="S3.E1.m1.9.9.3.2.2.4.3.cmml">K</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.3.2.2.3" xref="S3.E1.m1.9.9.3.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.9.9.3.2.2.2.2" xref="S3.E1.m1.9.9.3.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.3.2.2.2.2.3" xref="S3.E1.m1.9.9.3.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.8.8.2.1.1.1.1.1" xref="S3.E1.m1.8.8.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.8.8.2.1.1.1.1.1.2" xref="S3.E1.m1.8.8.2.1.1.1.1.1.2.cmml">i</mi><mo id="S3.E1.m1.8.8.2.1.1.1.1.1.1" xref="S3.E1.m1.8.8.2.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.8.8.2.1.1.1.1.1.3" xref="S3.E1.m1.8.8.2.1.1.1.1.1.3.cmml">m</mi></mrow><mo id="S3.E1.m1.9.9.3.2.2.2.2.4" xref="S3.E1.m1.9.9.3.2.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.9.9.3.2.2.2.2.2" xref="S3.E1.m1.9.9.3.2.2.2.2.2.cmml"><mi id="S3.E1.m1.9.9.3.2.2.2.2.2.2" xref="S3.E1.m1.9.9.3.2.2.2.2.2.2.cmml">j</mi><mo id="S3.E1.m1.9.9.3.2.2.2.2.2.1" xref="S3.E1.m1.9.9.3.2.2.2.2.2.1.cmml">−</mo><mi id="S3.E1.m1.9.9.3.2.2.2.2.2.3" xref="S3.E1.m1.9.9.3.2.2.2.2.2.3.cmml">n</mi></mrow><mo stretchy="false" id="S3.E1.m1.9.9.3.2.2.2.2.5" xref="S3.E1.m1.9.9.3.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.9b"><apply id="S3.E1.m1.9.9.cmml" xref="S3.E1.m1.9.9"><and id="S3.E1.m1.9.9a.cmml" xref="S3.E1.m1.9.9"></and><apply id="S3.E1.m1.9.9b.cmml" xref="S3.E1.m1.9.9"><eq id="S3.E1.m1.9.9.6.cmml" xref="S3.E1.m1.9.9.6"></eq><apply id="S3.E1.m1.9.9.5.cmml" xref="S3.E1.m1.9.9.5"><times id="S3.E1.m1.9.9.5.1.cmml" xref="S3.E1.m1.9.9.5.1"></times><ci id="S3.E1.m1.9.9.5.2.cmml" xref="S3.E1.m1.9.9.5.2">𝑆</ci><interval closure="open" id="S3.E1.m1.9.9.5.3.1.cmml" xref="S3.E1.m1.9.9.5.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑖</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑗</ci></interval></apply><apply id="S3.E1.m1.7.7.1.cmml" xref="S3.E1.m1.7.7.1"><times id="S3.E1.m1.7.7.1.2.cmml" xref="S3.E1.m1.7.7.1.2"></times><apply id="S3.E1.m1.7.7.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1"></times><ci id="S3.E1.m1.7.7.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2">𝐼</ci><ci id="S3.E1.m1.7.7.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.3">𝐾</ci></apply><interval closure="open" id="S3.E1.m1.7.7.1.3.1.cmml" xref="S3.E1.m1.7.7.1.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑖</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑗</ci></interval></apply></apply><apply id="S3.E1.m1.9.9c.cmml" xref="S3.E1.m1.9.9"><eq id="S3.E1.m1.9.9.7.cmml" xref="S3.E1.m1.9.9.7"></eq><share href="#S3.E1.m1.7.7.1.cmml" id="S3.E1.m1.9.9d.cmml" xref="S3.E1.m1.9.9"></share><apply id="S3.E1.m1.9.9.3.cmml" xref="S3.E1.m1.9.9.3"><apply id="S3.E1.m1.9.9.3.3.cmml" xref="S3.E1.m1.9.9.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.3.3.1.cmml" xref="S3.E1.m1.9.9.3.3">subscript</csymbol><sum id="S3.E1.m1.9.9.3.3.2.cmml" xref="S3.E1.m1.9.9.3.3.2"></sum><ci id="S3.E1.m1.9.9.3.3.3.cmml" xref="S3.E1.m1.9.9.3.3.3">𝑚</ci></apply><apply id="S3.E1.m1.9.9.3.2.cmml" xref="S3.E1.m1.9.9.3.2"><apply id="S3.E1.m1.9.9.3.2.3.cmml" xref="S3.E1.m1.9.9.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.3.2.3.1.cmml" xref="S3.E1.m1.9.9.3.2.3">subscript</csymbol><sum id="S3.E1.m1.9.9.3.2.3.2.cmml" xref="S3.E1.m1.9.9.3.2.3.2"></sum><ci id="S3.E1.m1.9.9.3.2.3.3.cmml" xref="S3.E1.m1.9.9.3.2.3.3">𝑛</ci></apply><apply id="S3.E1.m1.9.9.3.2.2.cmml" xref="S3.E1.m1.9.9.3.2.2"><times id="S3.E1.m1.9.9.3.2.2.3.cmml" xref="S3.E1.m1.9.9.3.2.2.3"></times><apply id="S3.E1.m1.9.9.3.2.2.4.cmml" xref="S3.E1.m1.9.9.3.2.2.4"><ci id="S3.E1.m1.9.9.3.2.2.4.1.cmml" xref="S3.E1.m1.9.9.3.2.2.4.1">⋅</ci><apply id="S3.E1.m1.9.9.3.2.2.4.2.cmml" xref="S3.E1.m1.9.9.3.2.2.4.2"><times id="S3.E1.m1.9.9.3.2.2.4.2.1.cmml" xref="S3.E1.m1.9.9.3.2.2.4.2.1"></times><ci id="S3.E1.m1.9.9.3.2.2.4.2.2.cmml" xref="S3.E1.m1.9.9.3.2.2.4.2.2">𝐼</ci><interval closure="open" id="S3.E1.m1.9.9.3.2.2.4.2.3.1.cmml" xref="S3.E1.m1.9.9.3.2.2.4.2.3.2"><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑚</ci><ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">𝑛</ci></interval></apply><ci id="S3.E1.m1.9.9.3.2.2.4.3.cmml" xref="S3.E1.m1.9.9.3.2.2.4.3">𝐾</ci></apply><interval closure="open" id="S3.E1.m1.9.9.3.2.2.2.3.cmml" xref="S3.E1.m1.9.9.3.2.2.2.2"><apply id="S3.E1.m1.8.8.2.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.2.1.1.1.1.1"><minus id="S3.E1.m1.8.8.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.2.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.8.8.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.2.1.1.1.1.1.2">𝑖</ci><ci id="S3.E1.m1.8.8.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.2.1.1.1.1.1.3">𝑚</ci></apply><apply id="S3.E1.m1.9.9.3.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.3.2.2.2.2.2"><minus id="S3.E1.m1.9.9.3.2.2.2.2.2.1.cmml" xref="S3.E1.m1.9.9.3.2.2.2.2.2.1"></minus><ci id="S3.E1.m1.9.9.3.2.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.3.2.2.2.2.2.2">𝑗</ci><ci id="S3.E1.m1.9.9.3.2.2.2.2.2.3.cmml" xref="S3.E1.m1.9.9.3.2.2.2.2.2.3">𝑛</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.9c">S(i,j)=(I*K)(i,j)=\sum_{m}\sum_{n}I(m,n)\cdot K(i-m,j-n)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">Here, <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">I</annotation></semantics></math> represents the input 2D data, <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">K</annotation></semantics></math> is the convolutional kernel, and <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">S</annotation></semantics></math> is the output feature map. <span id="S3.SS3.p3.3.1" class="ltx_ERROR undefined">\Acp</span>CNN excel at recognizing spatial patterns, making them essential in image recognition, object detection, and other visual tasks. Additionally, there exist 1D <a href="#Sx1.1.1.1"><span href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">convolutional neural network</span>s</span></span></a>, which are effective for sequential data analysis, such as in natural language processing or time series applications. <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> is widely used for the interdisciplinary nature of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, which involves aspects of neurobiology, signal processing, and medical technology.
For example, the proposed work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> introduces a novel pathological voice identification system using signal processing and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>. It employs <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> models with bandpass and optimized gammatone filters to mimic human cochlear vibration patterns. The system processes speech samples and utilizes a <span id="S3.SS3.p3.3.2" class="ltx_ERROR undefined">\Ac</span>CNN for final pathological voice identification. Results show discrimination of pathological voices with <a href="#Sx1.25.25.25"><abbr href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">F1</span></span></abbr></a> scores of 77.6% (bandpass) and 78.7% (gammatone). The paper addresses voice pathology causes, compares filter models, and proposes a non-invasive, objective assessment system. It contributes to the field with a comprehensive performance analysis, achieving high accuracy and demonstrating effectiveness compared to related works. Addtionally, in the scheme proposed by Wang <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the  <a href="#Sx1.6.6.6"><span href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">fully convolutional neural networks</span></span></span></a> (<a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a>) model is evaluated for enhancing speech intelligibility in mismatched training and testing conditions. Using 2,560 Mandarin utterances and 100 noise types, the study compares <a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a> with traditional  <a href="#Sx1.8.8.8"><span href="#Sx1.8.8.8" title="minimum mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">minimum mean square error</span></span></span></a> (<a href="#Sx1.8.8.8"><abbr href="#Sx1.8.8.8" title="minimum mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMSE</span></span></abbr></a>) and  <a href="#Sx1.9.9.9"><span href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep denoising auto-encoder</span></span></span></a> (<a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>) models. Two sets of experiments are conducted for normal and vocoded speech. The <a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a> model demonstrates superior performance, maintaining clearer speech structures, especially in mid-low frequency regions crucial for intelligibility. Objective evaluations using <a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a> scores and a listening test confirm <a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a>’s effectiveness under challenging  <a href="#Sx1.10.10.10"><span href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">signal to noise ratio</span></span></span></a> (<a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a>) conditions, outperforming <a href="#Sx1.8.8.8"><abbr href="#Sx1.8.8.8" title="minimum mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMSE</span></span></abbr></a> and <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>. The study suggests <a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a> as a promising choice for  <a href="#Sx1.11.11.11"><span href="#Sx1.11.11.11" title="electric and acoustic stimulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">electric and acoustic stimulation</span></span></span></a> (<a href="#Sx1.11.11.11"><abbr href="#Sx1.11.11.11" title="electric and acoustic stimulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EAS</span></span></abbr></a>) speech processors.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Moving on, the research paper in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> presents a novel approach to optimize stimulus energy for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. A <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> was developed as a surrogate model for a biophysical auditory nerve fiber model, significantly reducing simulation time while maintaining high accuracy. The <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> was then used in conjunction with an evolutionary algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to optimize the shape of the stimulus waveform, resulting in energy-efficient waveforms and potential improvements in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technology. Traditional computational models of the cochlea, which represent it as a transmission line, are computationally expensive due to their cascaded architecture and the inclusion of non-linearities. As a result, they are not suitable for real-time applications such as hearing aids, robotics, and <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a>. For the aforementioned conditions, the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> presents a hybrid approach, called CoNNear<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://github.com/HearingTechnology/CoNNear_periphery" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/HearingTechnology/CoNNear_periphery</a></span></span></span>, which combines <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a>, capable of performing end-to-end waveform predictions in real-time, with computational neuroscience to create a real-time model of human cochlear mechanics and filter tuning. The <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> filter weights were trained using simulated  <a href="#Sx1.104.104.104"><span href="#Sx1.104.104.104" title="basilar-membrane" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">basilar-membrane</span></span></span></a> (<a href="#Sx1.104.104.104"><abbr href="#Sx1.104.104.104" title="basilar-membrane" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">BM</span></span></abbr></a>) displacements from cochlear channels, and the model’s performance was evaluated using basic acoustic stimuli. The CoNNear model is designed to capture the tuning, level-dependence, and longitudinal coupling characteristics of human cochlear processing. It converts acoustic speech stimuli into <a href="#Sx1.104.104.104"><abbr href="#Sx1.104.104.104" title="basilar-membrane" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">BM</span></span></abbr></a> displacement waveforms across 201 cochlear filters. Its computational efficiency and ability to capture human cochlear characteristics make it suitable for developing human-like machine-hearing applications.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">The research paper in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> explores the utilization of a <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> in simulating speech processing with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. The study investigates the effect of channel interaction, a phenomenon that degrades spectral resolution in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> delivered speech, on learning in neural networks. By modifying speech spectrograms to approximate <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> delivered signals, the <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> is trained to classify them. The findings suggest that early in training, the presence of channel interaction negatively impacts performance. This indicates that the spectral degradation caused by channel interaction conflicts with perceptual expectations acquired from high-resolution speech. The study highlights the potential for reducing channel interaction to enhance learning and improve speech processing in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users, particularly those who have adapted to high-resolution speech.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">Schuerch et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> focus on the objectification of  <a href="#Sx1.97.97.97"><span href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">intracochlear electrocochleography</span></span></span></a> (<a href="#Sx1.97.97.97"><abbr href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECochG</span></span></abbr></a>) using AlexNet, <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> architecture, to automate and standardize the assessment and analysis of  <a href="#Sx1.98.98.98"><span href="#Sx1.98.98.98" title="cochlear microphonic" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">cochlear microphonic</span></span></span></a> (<a href="#Sx1.98.98.98"><abbr href="#Sx1.98.98.98" title="cochlear microphonic" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CM</span></span></abbr></a>) signals in <a href="#Sx1.97.97.97"><abbr href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECochG</span></span></abbr></a> recordings for clinical practice and research. The authors compared three different methods: correlation analysis, Hotelling’s T2 test, and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, to detect <a href="#Sx1.98.98.98"><abbr href="#Sx1.98.98.98" title="cochlear microphonic" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CM</span></span></abbr></a> signals. The <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> algorithm performed the best, followed closely by Hotelling’s T2 test, while the correlation method slightly underperformed. The automated methods achieved excellent discrimination performance in detecting <a href="#Sx1.98.98.98"><abbr href="#Sx1.98.98.98" title="cochlear microphonic" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CM</span></span></abbr></a> signals with an accuracy up to 92%, providing fast, accurate, and examiner-independent evaluation of <a href="#Sx1.97.97.97"><abbr href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECochG</span></span></abbr></a> measurements.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">Moreover, Arias et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> presents a methodology for speech processing using <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a>. The study aims to improve the representation learning capabilities of <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a> by combining multiple time-frequency representations of speech signals. The proposed approach involves generating multi-channel spectrograms by combining continuous wavelet transform, Mel-spectrograms, and Gammatone spectrograms. These spectrograms are utilized as input data for the <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> models. The effectiveness of the approach is evaluated in two applications: automatic detection of speech deficits in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users and phoneme class recognition. The results demonstrate the advantages of using multi-channel spectrograms with <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a>, showcasing improved performance in speech analysis tasks. The  <a href="#Sx1.12.12.12"><span href="#Sx1.12.12.12" title=" convolutional recurrent neural network with
gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;"> convolutional recurrent neural network with
gated recurrent units</span></span></span></a> (<a href="#Sx1.12.12.12"><abbr href="#Sx1.12.12.12" title=" convolutional recurrent neural network with
gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CGRU</span></span></abbr></a>) architecture is utilized, as illustrated in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3 -based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The input sequences consist of 3D-channel inputs created by combining Mel-spectrograms, Cochleagrams, and  <a href="#Sx1.13.13.13"><span href="#Sx1.13.13.13" title=" continuous
wavelet transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;"> continuous
wavelet transform</span></span></span></a> (<a href="#Sx1.13.13.13"><abbr href="#Sx1.13.13.13" title=" continuous
wavelet transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CWT</span></span></abbr></a>) with Morlet wavelets. Convolution is applied solely on the frequency axis in order to preserve the time information. The resulting feature maps are subsequently fed into a 2-stacked bidirectional  <a href="#Sx1.14.14.14"><span href="#Sx1.14.14.14" title="gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">gated recurrent units</span></span></span></a> (<a href="#Sx1.14.14.14"><abbr href="#Sx1.14.14.14" title="gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GRU</span></span></abbr></a>). A softmax function is employed to predict the phoneme label for each speech segment in the input signal.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2403.15442/assets/x6.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="366" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.3.2" class="ltx_text" style="font-size:90%;"> <a href="#Sx1.12.12.12"><abbr href="#Sx1.12.12.12" title=" convolutional recurrent neural network with
gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CGRU</span></span></abbr></a> architecture, focusing on input sequences composed of 3D-channel data generated from Mel-spectrograms, Cochleagrams, and <a href="#Sx1.13.13.13"><abbr href="#Sx1.13.13.13" title=" continuous
wavelet transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CWT</span></span></abbr></a> using Morlet wavelets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p">This paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduces a novel method for automatically detecting speech disorders in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users using a multi-channel <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a>. The model processes 2-channel input comprising Mel-scaled and Gammatone filter bank spectrograms derived from speech signals. Testing on 107 <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users and 94 healthy controls demonstrates improved performance with 2-channel spectrograms. The study addresses a gap in acoustic analysis of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> user speech, proposing a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> approach with potential applications beyond <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users. Experimental results indicate the effectiveness of the proposed <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a>-based method, offering promise for speech disorder detection and potential extensions to other pathologies or paralinguistic aspects that employ <a href="#Sx1.45.45.45"><span href="#Sx1.45.45.45" title="mel-frequency cepstral coefficient" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">mel-frequency cepstral coefficient</span>s</span></span></a> and <a href="#Sx1.46.46.46"><span href="#Sx1.46.46.46" title="gammatone frequency cepstral coefficient " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">gammatone frequency cepstral coefficient </span>s</span></span></a> features.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<p id="S3.SS3.p9.1" class="ltx_p">For 2D <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a>, the following work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> introduces  <a href="#Sx1.15.15.15"><span href="#Sx1.15.15.15" title="image guided cochlear implant programming " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">image guided cochlear implant programming </span></span></span></a> (<a href="#Sx1.15.15.15"><abbr href="#Sx1.15.15.15" title="image guided cochlear implant programming " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IGCIP</span></span></abbr></a>), enhancing <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> outcomes using image processing. <a href="#Sx1.15.15.15"><abbr href="#Sx1.15.15.15" title="image guided cochlear implant programming " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IGCIP</span></span></abbr></a> segments intra-cochlear anatomy in  <a href="#Sx1.16.16.16"><span href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">computed tomography</span></span></span></a> (<a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a>) images, aiding electrode localization for programming. The scheme addresses challenges in automating this process due to varied image acquisition protocols. The proposed solution employs a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based approach, utilizing <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a> to detect the presence and location of inner ears in head <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> volumes. The <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CNN</span>s</span></abbr></a> is trained on a dataset with 95.97% classification accuracy. Results indicate potential for automatic labeling of <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, with a focus on further 3D algorithm development. However, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> presents a machine-learning approach to optimize stimulus energy for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. A <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> was developed as a surrogate model for a biophysical auditory nerve fiber model, significantly reducing simulation time while maintaining high accuracy. The <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> was then used in conjunction with an evolutionary algorithm to optimize the shape of the stimulus waveform, resulting in energy-efficient waveforms. The proposed surrogate model offers an efficient replacement for the original model, allowing for larger-scale experiments and potential improvements in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technology.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.1" class="ltx_p">The work proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> introduces sliding window based <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> (SlideCNN), a novel <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> approach for auditory spatial scene recognition with limited annotated data. The proposed method converts auditory spatial scenes into spectrogram images and utilizes a SlideCNN for image classification. Compared to existing models, SlideCNN achieves a significant improvement in prediction accuracy, with a 12% increase. By leveraging limited annotated samples, SlideCNN demonstrates an 85% accuracy in detecting real-life indoor and outdoor scenes. The results have practical implications for analyzing auditory scenes with limited annotated data, benefiting individuals with hearing aids and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>.</p>
</div>
<div id="S3.SS3.p11" class="ltx_para">
<p id="S3.SS3.p11.1" class="ltx_p">This paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> focuses on advancing laser bone ablation in microsurgery using 4D  <a href="#Sx1.17.17.17"><span href="#Sx1.17.17.17" title="optical coherence tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">optical coherence tomography</span></span></span></a> (<a href="#Sx1.17.17.17"><abbr href="#Sx1.17.17.17" title="optical coherence tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">OCT</span></span></abbr></a>). The challenge lies in automatic control without external tracking systems. The paper introduces a 2.5D scene flow estimation method using <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> for <a href="#Sx1.17.17.17"><abbr href="#Sx1.17.17.17" title="optical coherence tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">OCT</span></span></abbr></a> images, enhancing laser ablation control. A two-stage approach involves lateral scene flow computation followed by depth flow estimation. Training is semi-supervised, combining ground truth error and reconstruction error. The method achieves a  <a href="#Sx1.18.18.18"><span href="#Sx1.18.18.18" title="mean endpoint error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">mean endpoint error</span></span></span></a> (<a href="#Sx1.18.18.18"><abbr href="#Sx1.18.18.18" title="mean endpoint error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MEE</span></span></abbr></a>) of (4.7 <math id="S3.SS3.p11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.SS3.p11.1.m1.1a"><mo id="S3.SS3.p11.1.m1.1.1" xref="S3.SS3.p11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p11.1.m1.1b"><csymbol cd="latexml" id="S3.SS3.p11.1.m1.1.1.cmml" xref="S3.SS3.p11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p11.1.m1.1c">\pm</annotation></semantics></math> 3.5) voxel, enabling markerless tracking for image guidance and automated laser ablation control in minimally invasive cochlear implantation.</p>
</div>
<figure id="S3.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.9.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.10.2" class="ltx_text" style="font-size:90%;">Summary of some proposed methods based on differents <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> techniques. When comparing the work with numerous existing schemes, only the best-performing one will be highlighted. </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T4.7" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S3.T4.7.8" class="ltx_tr">
<td id="S3.T4.7.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;">
<span id="S3.T4.7.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.1.1.1" class="ltx_p"><span id="S3.T4.7.8.1.1.1.1" class="ltx_text" style="font-size:70%;">Ref.</span></span>
</span>
</td>
<td id="S3.T4.7.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T4.7.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.2.1.1" class="ltx_p"><span id="S3.T4.7.8.2.1.1.1" class="ltx_text" style="font-size:70%;">DL method</span></span>
</span>
</td>
<td id="S3.T4.7.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T4.7.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.3.1.1" class="ltx_p"><span id="S3.T4.7.8.3.1.1.1" class="ltx_text" style="font-size:70%;">Domain</span></span>
</span>
</td>
<td id="S3.T4.7.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;">
<span id="S3.T4.7.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.4.1.1" class="ltx_p"><span id="S3.T4.7.8.4.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td id="S3.T4.7.8.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S3.T4.7.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.5.1.1" class="ltx_p"><span id="S3.T4.7.8.5.1.1.1" class="ltx_text" style="font-size:70%;">Explanation</span></span>
</span>
</td>
<td id="S3.T4.7.8.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:56.9pt;">
<span id="S3.T4.7.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.6.1.1" class="ltx_p"><span id="S3.T4.7.8.6.1.1.1" class="ltx_text" style="font-size:70%;">CTODLM</span></span>
</span>
</td>
<td id="S3.T4.7.8.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:56.9pt;">
<span id="S3.T4.7.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.7.1.1" class="ltx_p"><span id="S3.T4.7.8.7.1.1.1" class="ltx_text" style="font-size:70%;">Best performance</span></span>
</span>
</td>
<td id="S3.T4.7.8.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S3.T4.7.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.8.8.1.1" class="ltx_p"><span id="S3.T4.7.8.8.1.1.1" class="ltx_text" style="font-size:70%;">Improvement</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.9" class="ltx_tr">
<td id="S3.T4.7.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T4.7.9.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.2.1.1" class="ltx_p"><a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.3.1.1" class="ltx_p"><span id="S3.T4.7.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.9.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.4.1.1" class="ltx_p"><span id="S3.T4.7.9.4.1.1.1" class="ltx_text" style="font-size:70%;">MHINT</span></span>
</span>
</td>
<td id="S3.T4.7.9.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.5.1.1" class="ltx_p"><span id="S3.T4.7.9.5.1.1.1" class="ltx_text" style="font-size:70%;">Experiment the effectiveness of </span><a href="#Sx1.6.6.6"><abbr href="#Sx1.6.6.6" title="fully convolutional neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FCN</span></span></abbr></a><span id="S3.T4.7.9.5.1.1.2" class="ltx_text" style="font-size:70%;"> in restoring clean speech signals from noisy counterparts and enhancing the intelligibility of speech in </span><a href="#Sx1.11.11.11"><abbr href="#Sx1.11.11.11" title="electric and acoustic stimulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EAS</span></span></abbr></a><span id="S3.T4.7.9.5.1.1.3" class="ltx_text" style="font-size:70%;"> devices.</span></span>
</span>
</td>
<td id="S3.T4.7.9.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.6.1.1" class="ltx_p"><a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.9.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.7.1.1" class="ltx_p"><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a><span id="S3.T4.7.9.7.1.1.1" class="ltx_text" style="font-size:70%;">=0.85 (</span><a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a><span id="S3.T4.7.9.7.1.1.2" class="ltx_text" style="font-size:70%;">=9dB)</span></span>
</span>
</td>
<td id="S3.T4.7.9.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.9.8.1.1" class="ltx_p"><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a><span id="S3.T4.7.9.8.1.1.1" class="ltx_text" style="font-size:70%;">=0.03</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.3.3" class="ltx_tr">
<td id="S3.T4.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.3.3.4.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S3.T4.3.3.4.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.5.1.1" class="ltx_p"><a href="#Sx1.12.12.12"><abbr href="#Sx1.12.12.12" title=" convolutional recurrent neural network with
gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CGRU</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.6.1.1" class="ltx_p"><span id="S3.T4.3.3.6.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.3.3.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.7.1.1" class="ltx_p"><span id="S3.T4.3.3.7.1.1.1" class="ltx_text" style="font-size:70%;">PhonDat 1</span></span>
</span>
</td>
<td id="S3.T4.3.3.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.8.1.1" class="ltx_p"><span id="S3.T4.3.3.8.1.1.1" class="ltx_text" style="font-size:70%;">Using of onset and offset
transitions in speech analysis
and The input
sequences are 3D-channel inputs</span></span>
</span>
</td>
<td id="S3.T4.3.3.9" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.9.1.1" class="ltx_p"><a href="#Sx1.12.12.12"><abbr href="#Sx1.12.12.12" title=" convolutional recurrent neural network with
gated recurrent units" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CGRU</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.3.3" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.3.3.3" class="ltx_p"><a href="#Sx1.23.23.23"><abbr href="#Sx1.23.23.23" title="Precision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Pre</span></span></abbr></a><span id="S3.T4.3.3.3.3.3.1" class="ltx_text" style="font-size:70%;">=0.68
</span><math id="S3.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm\ 0.22" display="inline"><semantics id="S3.T4.1.1.1.1.1.m1.1a"><mrow id="S3.T4.1.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S3.T4.1.1.1.1.1.m1.1.1a" xref="S3.T4.1.1.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S3.T4.1.1.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.1.1.m1.1.1.2.cmml"> 0.22</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.T4.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T4.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.1.1.m1.1.1.2">0.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.1.m1.1c">\pm\ 0.22</annotation></semantics></math><span id="S3.T4.3.3.3.3.3.2" class="ltx_text" style="font-size:70%;"> </span>
<br class="ltx_break"><a href="#Sx1.24.24.24"><abbr href="#Sx1.24.24.24" title="Recall" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Rec</span></span></abbr></a><span id="S3.T4.3.3.3.3.3.3" class="ltx_text" style="font-size:70%;">=0.69 </span><math id="S3.T4.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\pm\ 0.21" display="inline"><semantics id="S3.T4.2.2.2.2.2.m2.1a"><mrow id="S3.T4.2.2.2.2.2.m2.1.1" xref="S3.T4.2.2.2.2.2.m2.1.1.cmml"><mo mathsize="70%" id="S3.T4.2.2.2.2.2.m2.1.1a" xref="S3.T4.2.2.2.2.2.m2.1.1.cmml">±</mo><mn mathsize="70%" id="S3.T4.2.2.2.2.2.m2.1.1.2" xref="S3.T4.2.2.2.2.2.m2.1.1.2.cmml"> 0.21</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.2.2.m2.1b"><apply id="S3.T4.2.2.2.2.2.m2.1.1.cmml" xref="S3.T4.2.2.2.2.2.m2.1.1"><csymbol cd="latexml" id="S3.T4.2.2.2.2.2.m2.1.1.1.cmml" xref="S3.T4.2.2.2.2.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T4.2.2.2.2.2.m2.1.1.2.cmml" xref="S3.T4.2.2.2.2.2.m2.1.1.2">0.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.2.2.m2.1c">\pm\ 0.21</annotation></semantics></math><span id="S3.T4.3.3.3.3.3.4" class="ltx_text" style="font-size:70%;"> </span>
<br class="ltx_break"><a href="#Sx1.25.25.25"><abbr href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">F1</span></span></abbr></a><span id="S3.T4.3.3.3.3.3.5" class="ltx_text" style="font-size:70%;">=0.69 </span><math id="S3.T4.3.3.3.3.3.m3.1" class="ltx_Math" alttext="\pm\ 0.19" display="inline"><semantics id="S3.T4.3.3.3.3.3.m3.1a"><mrow id="S3.T4.3.3.3.3.3.m3.1.1" xref="S3.T4.3.3.3.3.3.m3.1.1.cmml"><mo mathsize="70%" id="S3.T4.3.3.3.3.3.m3.1.1a" xref="S3.T4.3.3.3.3.3.m3.1.1.cmml">±</mo><mn mathsize="70%" id="S3.T4.3.3.3.3.3.m3.1.1.2" xref="S3.T4.3.3.3.3.3.m3.1.1.2.cmml"> 0.19</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.3.3.m3.1b"><apply id="S3.T4.3.3.3.3.3.m3.1.1.cmml" xref="S3.T4.3.3.3.3.3.m3.1.1"><csymbol cd="latexml" id="S3.T4.3.3.3.3.3.m3.1.1.1.cmml" xref="S3.T4.3.3.3.3.3.m3.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T4.3.3.3.3.3.m3.1.1.2.cmml" xref="S3.T4.3.3.3.3.3.m3.1.1.2">0.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.3.3.m3.1c">\pm\ 0.19</annotation></semantics></math></span>
</span>
</td>
<td id="S3.T4.3.3.10" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.3.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.10.1.1" class="ltx_p"><span id="S3.T4.3.3.10.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.4.4" class="ltx_tr">
<td id="S3.T4.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.2.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.4.4.2.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib59" title="" class="ltx_ref">59</a><span id="S3.T4.4.4.2.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.3.1.1" class="ltx_p"><a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.4.1.1" class="ltx_p"><span id="S3.T4.4.4.4.1.1.1" class="ltx_text" style="font-size:70%;">Image</span></span>
</span>
</td>
<td id="S3.T4.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.5.1.1" class="ltx_p"><span id="S3.T4.4.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.4.4.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.6.1.1" class="ltx_p"><span id="S3.T4.4.4.6.1.1.1" class="ltx_text" style="font-size:70%;">Multiple stages and follows a pyramidal network architecture inspired by SPyNet, which is designed for optical flow estimation.</span></span>
</span>
</td>
<td id="S3.T4.4.4.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.7.1.1" class="ltx_p"><span id="S3.T4.4.4.7.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
<td id="S3.T4.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.1.1.1" class="ltx_p"><a href="#Sx1.18.18.18"><abbr href="#Sx1.18.18.18" title="mean endpoint error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MEE</span></span></abbr></a><span id="S3.T4.4.4.1.1.1.1" class="ltx_text" style="font-size:70%;">=4.7</span><math id="S3.T4.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm\ 3.5" display="inline"><semantics id="S3.T4.4.4.1.1.1.m1.1a"><mrow id="S3.T4.4.4.1.1.1.m1.1.1" xref="S3.T4.4.4.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S3.T4.4.4.1.1.1.m1.1.1a" xref="S3.T4.4.4.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S3.T4.4.4.1.1.1.m1.1.1.2" xref="S3.T4.4.4.1.1.1.m1.1.1.2.cmml"> 3.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.1.1.1.m1.1b"><apply id="S3.T4.4.4.1.1.1.m1.1.1.cmml" xref="S3.T4.4.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.T4.4.4.1.1.1.m1.1.1.1.cmml" xref="S3.T4.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T4.4.4.1.1.1.m1.1.1.2.cmml" xref="S3.T4.4.4.1.1.1.m1.1.1.2">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.1.1.1.m1.1c">\pm\ 3.5</annotation></semantics></math></span>
</span>
</td>
<td id="S3.T4.4.4.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.4.4.8.1.1" class="ltx_p"><span id="S3.T4.4.4.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.10" class="ltx_tr">
<td id="S3.T4.7.10.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.10.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.T4.7.10.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.10.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.2.1.1" class="ltx_p"><span id="S3.T4.7.10.2.1.1.1" class="ltx_text" style="font-size:70%;">multi-channel </span><a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.10.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.3.1.1" class="ltx_p"><span id="S3.T4.7.10.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.10.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.4.1.1" class="ltx_p"><span id="S3.T4.7.10.4.1.1.1" class="ltx_text" style="font-size:70%;">PhonDat 1</span></span>
</span>
</td>
<td id="S3.T4.7.10.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.5.1.1" class="ltx_p"><span id="S3.T4.7.10.5.1.1.1" class="ltx_text" style="font-size:70%;">Developing a methodology for the automatic detection of speech disorders in </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.10.5.1.1.2" class="ltx_text" style="font-size:70%;"> users using a 2D-channel </span><a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a><span id="S3.T4.7.10.5.1.1.3" class="ltx_text" style="font-size:70%;">.</span></span>
</span>
</td>
<td id="S3.T4.7.10.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.6.1.1" class="ltx_p"><span id="S3.T4.7.10.6.1.1.1" class="ltx_text" style="font-size:70%;">SVM</span></span>
</span>
</td>
<td id="S3.T4.7.10.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.7.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.10.7.1.1.1" class="ltx_text" style="font-size:70%;">=83.4,</span>
<br class="ltx_break"><a href="#Sx1.21.21.21"><abbr href="#Sx1.21.21.21" title="Sensitivity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Sen</span></span></abbr></a><span id="S3.T4.7.10.7.1.1.2" class="ltx_text" style="font-size:70%;">=91.7,</span>
<br class="ltx_break"><a href="#Sx1.22.22.22"><abbr href="#Sx1.22.22.22" title="Specificity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Spe</span></span></abbr></a><span id="S3.T4.7.10.7.1.1.3" class="ltx_text" style="font-size:70%;">=78.7</span></span>
</span>
</td>
<td id="S3.T4.7.10.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.10.8.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.10.8.1.1.1" class="ltx_text" style="font-size:70%;">=+0.6,
</span>
<br class="ltx_break"><a href="#Sx1.21.21.21"><abbr href="#Sx1.21.21.21" title="Sensitivity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Sen</span></span></abbr></a><span id="S3.T4.7.10.8.1.1.2" class="ltx_text" style="font-size:70%;">=-1.8, </span>
<br class="ltx_break"><a href="#Sx1.22.22.22"><abbr href="#Sx1.22.22.22" title="Specificity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Spe</span></span></abbr></a><span id="S3.T4.7.10.8.1.1.3" class="ltx_text" style="font-size:70%;">=+2.7</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.11" class="ltx_tr">
<td id="S3.T4.7.11.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.11.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T4.7.11.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.11.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.2.1.1" class="ltx_p"><span id="S3.T4.7.11.2.1.1.1" class="ltx_text" style="font-size:70%;">DDAE</span></span>
</span>
</td>
<td id="S3.T4.7.11.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.3.1.1" class="ltx_p"><span id="S3.T4.7.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.11.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.4.1.1" class="ltx_p"><span id="S3.T4.7.11.4.1.1.1" class="ltx_text" style="font-size:70%;">MHINT</span></span>
</span>
</td>
<td id="S3.T4.7.11.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.5.1.1" class="ltx_p"><span id="S3.T4.7.11.5.1.1.1" class="ltx_text" style="font-size:70%;">The architecture of the DDAE model consisting of five layers with 500 neurons each. Objective evaluations were conducted using STOI and NCM, and a listening test (</span><a href="#Sx1.54.54.54"><abbr href="#Sx1.54.54.54" title=" perceptual estimation of speech quality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">PESQ</span></span></abbr></a><span id="S3.T4.7.11.5.1.1.2" class="ltx_text" style="font-size:70%;">)</span></span>
</span>
</td>
<td id="S3.T4.7.11.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.6.1.1" class="ltx_p"><a href="#Sx1.55.55.55"><abbr href="#Sx1.55.55.55" title="karhunen-loéve transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">KLT</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.11.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.7.1.1" class="ltx_p"><a href="#Sx1.33.33.33"><abbr href="#Sx1.33.33.33" title="normalized covariance measure" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NCM</span></span></abbr></a><span id="S3.T4.7.11.7.1.1.1" class="ltx_text" style="font-size:70%;">=0.25 (</span><a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a><span id="S3.T4.7.11.7.1.1.2" class="ltx_text" style="font-size:70%;">=9dB ) </span>
<br class="ltx_break"><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a><span id="S3.T4.7.11.7.1.1.3" class="ltx_text" style="font-size:70%;">=0.85 (</span><a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a><span id="S3.T4.7.11.7.1.1.4" class="ltx_text" style="font-size:70%;">=12dB)</span></span>
</span>
</td>
<td id="S3.T4.7.11.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.11.8.1.1" class="ltx_p"><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a><span id="S3.T4.7.11.8.1.1.1" class="ltx_text" style="font-size:70%;">=+0.07</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.5.5" class="ltx_tr">
<td id="S3.T4.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.2.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.5.5.2.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib60" title="" class="ltx_ref">60</a><span id="S3.T4.5.5.2.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.3.1.1" class="ltx_p"><span id="S3.T4.5.5.3.1.1.1" class="ltx_text" style="font-size:70%;">Res-CA</span></span>
</span>
</td>
<td id="S3.T4.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.4.1.1" class="ltx_p"><span id="S3.T4.5.5.4.1.1.1" class="ltx_text" style="font-size:70%;">Image</span></span>
</span>
</td>
<td id="S3.T4.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.5.1.1" class="ltx_p"><span id="S3.T4.5.5.5.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.6.1.1" class="ltx_p"><span id="S3.T4.5.5.6.1.1.1" class="ltx_text" style="font-size:70%;">Introduces Res-CA block, GCPFE, ACE loss, and DS mechanism to enhance the DAE’s performance and robustness for compressing CI stimulation patterns.</span></span>
</span>
</td>
<td id="S3.T4.5.5.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.7.1.1" class="ltx_p"><span id="S3.T4.5.5.7.1.1.1" class="ltx_text" style="font-size:70%;">3D-DSD</span></span>
</span>
</td>
<td id="S3.T4.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.1.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S3.T4.5.5.1.1.1.1" class="ltx_text" style="font-size:70%;">=89.91</span><math id="S3.T4.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\pm 1.77" display="inline"><semantics id="S3.T4.5.5.1.1.1.m1.1a"><mrow id="S3.T4.5.5.1.1.1.m1.1.1" xref="S3.T4.5.5.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S3.T4.5.5.1.1.1.m1.1.1a" xref="S3.T4.5.5.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S3.T4.5.5.1.1.1.m1.1.1.2" xref="S3.T4.5.5.1.1.1.m1.1.1.2.cmml">1.77</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.1.1.1.m1.1b"><apply id="S3.T4.5.5.1.1.1.m1.1.1.cmml" xref="S3.T4.5.5.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.T4.5.5.1.1.1.m1.1.1.1.cmml" xref="S3.T4.5.5.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T4.5.5.1.1.1.m1.1.1.2.cmml" xref="S3.T4.5.5.1.1.1.m1.1.1.2">1.77</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.1.1.1.m1.1c">\pm 1.77</annotation></semantics></math><span id="S3.T4.5.5.1.1.1.2" class="ltx_text" style="font-size:70%;"> </span>
<br class="ltx_break"><span id="S3.T4.5.5.1.1.1.3" class="ltx_text" style="font-size:70%;"> </span><a href="#Sx1.35.35.35"><span href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">average surface distance</span></span></span></a><span id="S3.T4.5.5.1.1.1.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.35.35.35"><abbr href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASD</span></span></abbr></a><span id="S3.T4.5.5.1.1.1.5" class="ltx_text" style="font-size:70%;">)=0.13 </span>
<br class="ltx_break"><span id="S3.T4.5.5.1.1.1.6" class="ltx_text" style="font-size:70%;"> </span><a href="#Sx1.36.36.36"><span href="#Sx1.36.36.36" title="average volume difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">average volume difference</span></span></span></a><span id="S3.T4.5.5.1.1.1.7" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.36.36.36"><abbr href="#Sx1.36.36.36" title="average volume difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AVD</span></span></abbr></a><span id="S3.T4.5.5.1.1.1.8" class="ltx_text" style="font-size:70%;">)=0.14</span></span>
</span>
</td>
<td id="S3.T4.5.5.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.5.5.8.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S3.T4.5.5.8.1.1.1" class="ltx_text" style="font-size:70%;">=+0.24 </span>
<br class="ltx_break"><a href="#Sx1.35.35.35"><abbr href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASD</span></span></abbr></a><span id="S3.T4.5.5.8.1.1.2" class="ltx_text" style="font-size:70%;">=+0.05 </span>
<br class="ltx_break"><a href="#Sx1.36.36.36"><abbr href="#Sx1.36.36.36" title="average volume difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AVD</span></span></abbr></a><span id="S3.T4.5.5.8.1.1.3" class="ltx_text" style="font-size:70%;">=-0.04</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.12" class="ltx_tr">
<td id="S3.T4.7.12.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.12.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib61" title="" class="ltx_ref">61</a><span id="S3.T4.7.12.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.12.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.2.1.1" class="ltx_p"><a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a><span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a target="_blank" href="https://github.com/AngeLouCN/Min_Max_Similarity" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AngeLouCN/Min_Max_Similarity</a></span></span></span></span>
</span>
</td>
<td id="S3.T4.7.12.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.3.1.1" class="ltx_p"><span id="S3.T4.7.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Image</span></span>
</span>
</td>
<td id="S3.T4.7.12.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.4.1.1" class="ltx_p"><span id="S3.T4.7.12.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.12.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.5.1.1" class="ltx_p"><span id="S3.T4.7.12.5.1.1.1" class="ltx_text" style="font-size:70%;">The classifiers extract features to create negative pairs, while projectors transform unlabeled predictions into high-dimensional features to measure pixel-wise consistency.</span></span>
</span>
</td>
<td id="S3.T4.7.12.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.6.1.1" class="ltx_p"><span id="S3.T4.7.12.6.1.1.1" class="ltx_text" style="font-size:70%;">TransUNet</span></span>
</span>
</td>
<td id="S3.T4.7.12.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.7.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S3.T4.7.12.7.1.1.1" class="ltx_text" style="font-size:70%;">=0.920 </span>
<br class="ltx_break"><a href="#Sx1.38.38.38"><abbr href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IoU</span></span></abbr></a><span id="S3.T4.7.12.7.1.1.2" class="ltx_text" style="font-size:70%;">=0.861 </span>
<br class="ltx_break"><a href="#Sx1.39.39.39"><abbr href="#Sx1.39.39.39" title="mean absolute error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MAE</span></span></abbr></a><span id="S3.T4.7.12.7.1.1.3" class="ltx_text" style="font-size:70%;">=0.021 </span>
<br class="ltx_break"><a href="#Sx1.25.25.25"><abbr href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">F1</span></span></abbr></a><span id="S3.T4.7.12.7.1.1.4" class="ltx_text" style="font-size:70%;">=0.925</span></span>
</span>
</td>
<td id="S3.T4.7.12.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.12.8.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S3.T4.7.12.8.1.1.1" class="ltx_text" style="font-size:70%;">=+0.032 </span>
<br class="ltx_break"><a href="#Sx1.38.38.38"><abbr href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IoU</span></span></abbr></a><span id="S3.T4.7.12.8.1.1.2" class="ltx_text" style="font-size:70%;">=+0.021 </span>
<br class="ltx_break"><a href="#Sx1.39.39.39"><abbr href="#Sx1.39.39.39" title="mean absolute error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MAE</span></span></abbr></a><span id="S3.T4.7.12.8.1.1.3" class="ltx_text" style="font-size:70%;">=-0.01 </span>
<br class="ltx_break"><a href="#Sx1.25.25.25"><abbr href="#Sx1.25.25.25" title="F1 score" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">F1</span></span></abbr></a><span id="S3.T4.7.12.8.1.1.4" class="ltx_text" style="font-size:70%;">=+0.012</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.13" class="ltx_tr">
<td id="S3.T4.7.13.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.13.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib62" title="" class="ltx_ref">62</a><span id="S3.T4.7.13.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.13.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.2.1.1" class="ltx_p"><a href="#Sx1.74.74.74"><abbr href="#Sx1.74.74.74" title="metal artifact reduction based generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MARGAN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.13.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.3.1.1" class="ltx_p"><span id="S3.T4.7.13.3.1.1.1" class="ltx_text" style="font-size:70%;">Image</span></span>
</span>
</td>
<td id="S3.T4.7.13.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.4.1.1" class="ltx_p"><span id="S3.T4.7.13.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.13.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.5.1.1" class="ltx_p"><a href="#Sx1.74.74.74"><abbr href="#Sx1.74.74.74" title="metal artifact reduction based generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MARGAN</span></span></abbr></a><span id="S3.T4.7.13.5.1.1.1" class="ltx_text" style="font-size:70%;"> is a 3D  </span><a href="#Sx1.53.53.53"><span href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">generative adversarial network</span></span></span></a><span id="S3.T4.7.13.5.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a><span id="S3.T4.7.13.5.1.1.3" class="ltx_text" style="font-size:70%;">)-based approach that uses simulated training data to reduce metal artifacts in </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S3.T4.7.13.5.1.1.4" class="ltx_text" style="font-size:70%;"> images.</span></span>
</span>
</td>
<td id="S3.T4.7.13.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.6.1.1" class="ltx_p"><span id="S3.T4.7.13.6.1.1.1" class="ltx_text" style="font-size:70%;">marLI</span></span>
</span>
</td>
<td id="S3.T4.7.13.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.7.1.1" class="ltx_p"><a href="#Sx1.103.103.103"><abbr href="#Sx1.103.103.103" title="root mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RMSE</span></span></abbr></a><span id="S3.T4.7.13.7.1.1.1" class="ltx_text" style="font-size:70%;"> =0.12 </span>
<br class="ltx_break"><span id="S3.T4.7.13.7.1.1.2" class="ltx_text" style="font-size:70%;">PSNR= 18.31 </span>
<br class="ltx_break"><span id="S3.T4.7.13.7.1.1.3" class="ltx_text" style="font-size:70%;">SSIM= 0.64</span></span>
</span>
</td>
<td id="S3.T4.7.13.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.13.8.1.1" class="ltx_p"><a href="#Sx1.103.103.103"><abbr href="#Sx1.103.103.103" title="root mean square error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RMSE</span></span></abbr></a><span id="S3.T4.7.13.8.1.1.1" class="ltx_text" style="font-size:70%;">=-0.03 </span>
<br class="ltx_break"><span id="S3.T4.7.13.8.1.1.2" class="ltx_text" style="font-size:70%;">PSNR=+1.78 </span>
<br class="ltx_break"><span id="S3.T4.7.13.8.1.1.3" class="ltx_text" style="font-size:70%;">SSIM=+0.08</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.14" class="ltx_tr">
<td id="S3.T4.7.14.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.14.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="S3.T4.7.14.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.14.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.2.1.1" class="ltx_p"><a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.14.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.3.1.1" class="ltx_p"><span id="S3.T4.7.14.3.1.1.1" class="ltx_text" style="font-size:70%;">Image</span></span>
</span>
</td>
<td id="S3.T4.7.14.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.4.1.1" class="ltx_p"><span id="S3.T4.7.14.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.14.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.5.1.1" class="ltx_p"><span id="S3.T4.7.14.5.1.1.1" class="ltx_text" style="font-size:70%;">Active shape model-based method used for segmenting intra-cochlear anatomical structures.</span></span>
</span>
</td>
<td id="S3.T4.7.14.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.6.1.1" class="ltx_p"><span id="S3.T4.7.14.6.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
<td id="S3.T4.7.14.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.7.1.1" class="ltx_p"><a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a><span id="S3.T4.7.14.7.1.1.1" class="ltx_text" style="font-size:70%;">=0.173</span></span>
</span>
</td>
<td id="S3.T4.7.14.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.14.8.1.1" class="ltx_p"><a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a><span id="S3.T4.7.14.8.1.1.1" class="ltx_text" style="font-size:70%;">=-50%</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.7" class="ltx_tr">
<td id="S3.T4.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S3.T4.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.3.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.7.3.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.T4.7.7.3.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.7.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.4.1.1" class="ltx_p"><a href="#Sx1.92.92.92"><abbr href="#Sx1.92.92.92" title="multiple-input multiple-output" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MIMO</span></span></abbr></a><span id="S3.T4.7.7.4.1.1.1" class="ltx_text" style="font-size:70%;">-TasNet using </span><a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.7.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.5.1.1" class="ltx_p"><span id="S3.T4.7.7.5.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.7.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.6.1.1" class="ltx_p"><span id="S3.T4.7.7.6.1.1.1" class="ltx_text" style="font-size:70%;">WSJ0-2mix, </span>
<br class="ltx_break"><span id="S3.T4.7.7.6.1.1.2" class="ltx_text" style="font-size:70%;">ITA, DEMAND</span></span>
</span>
</td>
<td id="S3.T4.7.7.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;">
<span id="S3.T4.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.7.1.1" class="ltx_p"><span id="S3.T4.7.7.7.1.1.1" class="ltx_text" style="font-size:70%;">A speech separation framework using TasNet and  </span><a href="#Sx1.95.95.95"><span href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">recurrent neural network</span></span></span></a><span id="S3.T4.7.7.7.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a><span id="S3.T4.7.7.7.1.1.3" class="ltx_text" style="font-size:70%;">)- </span><a href="#Sx1.93.93.93"><span href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">eigenvector decomposition</span></span></span></a><span id="S3.T4.7.7.7.1.1.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.93.93.93"><abbr href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EVD</span></span></abbr></a><span id="S3.T4.7.7.7.1.1.5" class="ltx_text" style="font-size:70%;">) to preserve spatial cues for CI users.</span></span>
</span>
</td>
<td id="S3.T4.7.7.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.8.1.1" class="ltx_p"><a href="#Sx1.92.92.92"><abbr href="#Sx1.92.92.92" title="multiple-input multiple-output" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MIMO</span></span></abbr></a><span id="S3.T4.7.7.8.1.1.1" class="ltx_text" style="font-size:70%;">-TasNet</span></span>
</span>
</td>
<td id="S3.T4.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.6.6.1.1.1" class="ltx_p"><math id="S3.T4.6.6.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T4.6.6.1.1.1.m1.1a"><mi mathsize="70%" mathvariant="normal" id="S3.T4.6.6.1.1.1.m1.1.1" xref="S3.T4.6.6.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.1.1.1.m1.1b"><ci id="S3.T4.6.6.1.1.1.m1.1.1.cmml" xref="S3.T4.6.6.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.1.1.1.m1.1c">\Delta</annotation></semantics></math><span id="S3.T4.6.6.1.1.1.1" class="ltx_text" style="font-size:70%;"> </span><a href="#Sx1.86.86.86"><span href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">interaural level difference</span></span></span></a><span id="S3.T4.6.6.1.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a><span id="S3.T4.6.6.1.1.1.3" class="ltx_text" style="font-size:70%;">)=0.38  </span><a href="#Sx1.91.91.91"><span href="#Sx1.91.91.91" title="decibel" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">decibel</span></span></span></a><span id="S3.T4.6.6.1.1.1.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.91.91.91"><abbr href="#Sx1.91.91.91" title="decibel" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">dB</span></span></abbr></a><span id="S3.T4.6.6.1.1.1.5" class="ltx_text" style="font-size:70%;">)</span></span>
</span>
</td>
<td id="S3.T4.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S3.T4.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.7.2.1.1" class="ltx_p"><math id="S3.T4.7.7.2.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T4.7.7.2.1.1.m1.1a"><mi mathsize="70%" mathvariant="normal" id="S3.T4.7.7.2.1.1.m1.1.1" xref="S3.T4.7.7.2.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.2.1.1.m1.1b"><ci id="S3.T4.7.7.2.1.1.m1.1.1.cmml" xref="S3.T4.7.7.2.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.2.1.1.m1.1c">\Delta</annotation></semantics></math><a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a><span id="S3.T4.7.7.2.1.1.1" class="ltx_text" style="font-size:70%;">=-0.69 </span><a href="#Sx1.91.91.91"><abbr href="#Sx1.91.91.91" title="decibel" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">dB</span></span></abbr></a></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.15" class="ltx_tr">
<td id="S3.T4.7.15.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;">
<span id="S3.T4.7.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.15.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S3.T4.7.15.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.15.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.2.1.1" class="ltx_p"><a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.15.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.3.1.1" class="ltx_p"><span id="S3.T4.7.15.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.15.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.4.1.1" class="ltx_p"><span id="S3.T4.7.15.4.1.1.1" class="ltx_text" style="font-size:70%;">iKala, MUSDB</span></span>
</span>
</td>
<td id="S3.T4.7.15.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;">
<span id="S3.T4.7.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.5.1.1" class="ltx_p"><span id="S3.T4.7.15.5.1.1.1" class="ltx_text" style="font-size:70%;">A </span><a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a><span id="S3.T4.7.15.5.1.1.2" class="ltx_text" style="font-size:70%;"> algorithm enhances music perception for </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.15.5.1.1.3" class="ltx_text" style="font-size:70%;"> users, addressing pitch and timbre limitations. Objective measurements and listener experiments reveal preferences for enhanced vocals.</span></span>
</span>
</td>
<td id="S3.T4.7.15.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.7.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.6.1.1" class="ltx_p"><span id="S3.T4.7.15.6.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
<td id="S3.T4.7.15.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.7.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.7.1.1" class="ltx_p"><a href="#Sx1.88.88.88"><abbr href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SDR</span></span></abbr></a><span id="S3.T4.7.15.7.1.1.1" class="ltx_text" style="font-size:70%;">=8dB (with iKala) </span>
<br class="ltx_break"><a href="#Sx1.88.88.88"><abbr href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SDR</span></span></abbr></a><span id="S3.T4.7.15.7.1.1.2" class="ltx_text" style="font-size:70%;">=5.5dB (with MUSDB)</span></span>
</span>
</td>
<td id="S3.T4.7.15.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;">
<span id="S3.T4.7.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.15.8.1.1" class="ltx_p"><span id="S3.T4.7.15.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.16" class="ltx_tr">
<td id="S3.T4.7.16.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.16.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib64" title="" class="ltx_ref">64</a><span id="S3.T4.7.16.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.16.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.2.1.1" class="ltx_p"><span id="S3.T4.7.16.2.1.1.1" class="ltx_text" style="font-size:70%;">Hybrid DNN</span></span>
</span>
</td>
<td id="S3.T4.7.16.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.3.1.1" class="ltx_p"><span id="S3.T4.7.16.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.16.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.4.1.1" class="ltx_p"><span id="S3.T4.7.16.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.16.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.5.1.1" class="ltx_p"><span id="S3.T4.7.16.5.1.1.1" class="ltx_text" style="font-size:70%;">Hearing training system consists of a speech training database, automatic lip-reading using a hybrid neural network, comparison of lip shapes, and providing a standard lip-reading sequence.</span></span>
</span>
</td>
<td id="S3.T4.7.16.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.6.1.1" class="ltx_p"><span id="S3.T4.7.16.6.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
<td id="S3.T4.7.16.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.7.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.16.7.1.1.1" class="ltx_text" style="font-size:70%;">=98 (gesture recognition)
</span><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.16.7.1.1.2" class="ltx_text" style="font-size:70%;">=87 (lip-reading recognition)</span></span>
</span>
</td>
<td id="S3.T4.7.16.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:5.69046pt;">
<span id="S3.T4.7.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.16.8.1.1" class="ltx_p"><span id="S3.T4.7.16.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.17" class="ltx_tr">
<td id="S3.T4.7.17.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.17.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib65" title="" class="ltx_ref">65</a><span id="S3.T4.7.17.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.17.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.2.1.1" class="ltx_p"><a href="#Sx1.75.75.75"><abbr href="#Sx1.75.75.75" title=" enhanced swarm based crow search optimization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ESCSO</span></span></abbr></a><span id="S3.T4.7.17.2.1.1.1" class="ltx_text" style="font-size:70%;">-based LSTM</span></span>
</span>
</td>
<td id="S3.T4.7.17.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.3.1.1" class="ltx_p"><a href="#Sx1.77.77.77"><abbr href="#Sx1.77.77.77" title="event-related potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ERP</span></span></abbr></a><span id="S3.T4.7.17.3.1.1.1" class="ltx_text" style="font-size:70%;"> of </span><a href="#Sx1.76.76.76"><abbr href="#Sx1.76.76.76" title="electroencephalography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EEG</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.17.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.4.1.1" class="ltx_p"><span id="S3.T4.7.17.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.17.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.5.1.1" class="ltx_p"><span id="S3.T4.7.17.5.1.1.1" class="ltx_text" style="font-size:70%;">LSTM-ESCSO is a method that combines  </span><a href="#Sx1.48.48.48"><span href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">long short term memory</span></span></span></a><span id="S3.T4.7.17.5.1.1.2" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a><span id="S3.T4.7.17.5.1.1.3" class="ltx_text" style="font-size:70%;">) network with  </span><a href="#Sx1.75.75.75"><span href="#Sx1.75.75.75" title=" enhanced swarm based crow search optimization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;"> enhanced swarm based crow search optimization</span></span></span></a><span id="S3.T4.7.17.5.1.1.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.75.75.75"><abbr href="#Sx1.75.75.75" title=" enhanced swarm based crow search optimization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ESCSO</span></span></abbr></a><span id="S3.T4.7.17.5.1.1.5" class="ltx_text" style="font-size:70%;">) to improve the performance of predicting </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.17.5.1.1.6" class="ltx_text" style="font-size:70%;"> scores.</span></span>
</span>
</td>
<td id="S3.T4.7.17.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.6.1.1" class="ltx_p"><a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.17.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.7.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.17.7.1.1.1" class="ltx_text" style="font-size:70%;">=96.03</span></span>
</span>
</td>
<td id="S3.T4.7.17.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.17.8.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S3.T4.7.17.8.1.1.1" class="ltx_text" style="font-size:70%;">=+7.14</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.18" class="ltx_tr">
<td id="S3.T4.7.18.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.18.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib66" title="" class="ltx_ref">66</a><span id="S3.T4.7.18.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.18.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.2.1.1" class="ltx_p"><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.18.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.3.1.1" class="ltx_p"><span id="S3.T4.7.18.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.18.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.4.1.1" class="ltx_p"><span id="S3.T4.7.18.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.18.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.5.1.1" class="ltx_p"><span id="S3.T4.7.18.5.1.1.1" class="ltx_text" style="font-size:70%;">The study investigates the utilization of </span><a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a><span id="S3.T4.7.18.5.1.1.2" class="ltx_text" style="font-size:70%;">
in remote </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.18.5.1.1.3" class="ltx_text" style="font-size:70%;"> fitting, demonstrates the feasibility of </span><a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a><span id="S3.T4.7.18.5.1.1.4" class="ltx_text" style="font-size:70%;">-based fitting with remote supervision, highlighting its potential for streamlining the fitting process.</span></span>
</span>
</td>
<td id="S3.T4.7.18.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.6.1.1" class="ltx_p"><span id="S3.T4.7.18.6.1.1.1" class="ltx_text" style="font-size:70%;">Manual fitting</span></span>
</span>
</td>
<td id="S3.T4.7.18.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.7.1.1" class="ltx_p"><span id="S3.T4.7.18.7.1.1.1" class="ltx_text" style="font-size:70%;">66% performed better with </span><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.18.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:14.22636pt;">
<span id="S3.T4.7.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.18.8.1.1" class="ltx_p"><span id="S3.T4.7.18.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.19" class="ltx_tr">
<td id="S3.T4.7.19.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.19.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S3.T4.7.19.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.19.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.2.1.1" class="ltx_p"><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.19.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.3.1.1" class="ltx_p"><span id="S3.T4.7.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.19.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.4.1.1" class="ltx_p"><span id="S3.T4.7.19.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.19.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:170.7pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.5.1.1" class="ltx_p"><span id="S3.T4.7.19.5.1.1.1" class="ltx_text" style="font-size:70%;">A study comparing manual and computer-assisted </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.19.5.1.1.2" class="ltx_text" style="font-size:70%;"> fitting.</span></span>
</span>
</td>
<td id="S3.T4.7.19.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.6.1.1" class="ltx_p"><span id="S3.T4.7.19.6.1.1.1" class="ltx_text" style="font-size:70%;">Manual fitting</span></span>
</span>
</td>
<td id="S3.T4.7.19.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.7.1.1" class="ltx_p"><span id="S3.T4.7.19.7.1.1.1" class="ltx_text" style="font-size:70%;">82% performed better with </span><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.19.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:42.7pt;padding-bottom:11.38092pt;">
<span id="S3.T4.7.19.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.19.8.1.1" class="ltx_p"><span id="S3.T4.7.19.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
<tr id="S3.T4.7.20" class="ltx_tr">
<td id="S3.T4.7.20.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:14.2pt;">
<span id="S3.T4.7.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.7.20.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib67" title="" class="ltx_ref">67</a><span id="S3.T4.7.20.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S3.T4.7.20.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:28.5pt;">
<span id="S3.T4.7.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.2.1.1" class="ltx_p"><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a></span>
</span>
</td>
<td id="S3.T4.7.20.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:28.5pt;">
<span id="S3.T4.7.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.3.1.1" class="ltx_p"><span id="S3.T4.7.20.3.1.1.1" class="ltx_text" style="font-size:70%;">Speech</span></span>
</span>
</td>
<td id="S3.T4.7.20.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:28.5pt;">
<span id="S3.T4.7.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.4.1.1" class="ltx_p"><span id="S3.T4.7.20.4.1.1.1" class="ltx_text" style="font-size:70%;">Created</span></span>
</span>
</td>
<td id="S3.T4.7.20.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:170.7pt;">
<span id="S3.T4.7.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.5.1.1" class="ltx_p"><span id="S3.T4.7.20.5.1.1.1" class="ltx_text" style="font-size:70%;">Examined the impact of an </span><a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a><span id="S3.T4.7.20.5.1.1.2" class="ltx_text" style="font-size:70%;">-based </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.20.5.1.1.3" class="ltx_text" style="font-size:70%;"> programming tool on experienced </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.20.5.1.1.4" class="ltx_text" style="font-size:70%;"> patients. The </span><a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a><span id="S3.T4.7.20.5.1.1.5" class="ltx_text" style="font-size:70%;">-generated </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S3.T4.7.20.5.1.1.6" class="ltx_text" style="font-size:70%;"> map led to improved auditory outcomes.</span></span>
</span>
</td>
<td id="S3.T4.7.20.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:56.9pt;">
<span id="S3.T4.7.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.6.1.1" class="ltx_p"><span id="S3.T4.7.20.6.1.1.1" class="ltx_text" style="font-size:70%;">Manual fitting</span></span>
</span>
</td>
<td id="S3.T4.7.20.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:56.9pt;">
<span id="S3.T4.7.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.7.1.1" class="ltx_p"><span id="S3.T4.7.20.7.1.1.1" class="ltx_text" style="font-size:70%;">89% preferring </span><a href="#Sx1.110.110.110"><abbr href="#Sx1.110.110.110" title="fitting to outcomes expert" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FOX</span></span></abbr></a><span id="S3.T4.7.20.7.1.1.2" class="ltx_text" style="font-size:70%;">’s map</span></span>
</span>
</td>
<td id="S3.T4.7.20.8" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:42.7pt;">
<span id="S3.T4.7.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.7.20.8.1.1" class="ltx_p"><span id="S3.T4.7.20.8.1.1.1" class="ltx_text" style="font-size:70%;">—</span></span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.T4.11" class="ltx_p ltx_figure_panel ltx_align_left"><span id="S3.T4.11.1" class="ltx_text" style="font-size:70%;">Abbreviations: Project link (PL), Not available (NA), Compared to other </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S3.T4.11.2" class="ltx_text" style="font-size:70%;"> methods (CTODLM).</span></p>
</div>
</div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a>-based methods</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">A <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> is a type of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> model consisting of two neural networks, a generator, and a discriminator, engaged in a competitive learning process as presented in Figure <a href="#S3.F8" title="Figure 8 ‣ 3.4 -based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2403.15442/assets/x7.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="253" height="69" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;"> Illustrative depiction of a standard <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a>.</span></figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The generator aims to create realistic data, such as images, while the discriminator tries to differentiate between real and generated samples. This adversarial training dynamic leads to the refinement of the generator’s output, generating increasingly authentic data. The objective is for the generator to produce data that is indistinguishable from real samples. The training process is represented by the minimax game framework, with the <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> objective function given by:</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.9" class="ltx_Math" alttext="\min_{G}\max_{D}V(D,G)=\mathbb{E}_{x\sim p_{\text{data}}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]" display="block"><semantics id="S3.E2.m1.9a"><mrow id="S3.E2.m1.9.9" xref="S3.E2.m1.9.9.cmml"><mrow id="S3.E2.m1.9.9.4" xref="S3.E2.m1.9.9.4.cmml"><mrow id="S3.E2.m1.9.9.4.2" xref="S3.E2.m1.9.9.4.2.cmml"><munder id="S3.E2.m1.9.9.4.2.1" xref="S3.E2.m1.9.9.4.2.1.cmml"><mi id="S3.E2.m1.9.9.4.2.1.2" xref="S3.E2.m1.9.9.4.2.1.2.cmml">min</mi><mi id="S3.E2.m1.9.9.4.2.1.3" xref="S3.E2.m1.9.9.4.2.1.3.cmml">G</mi></munder><mo lspace="0.167em" id="S3.E2.m1.9.9.4.2a" xref="S3.E2.m1.9.9.4.2.cmml">⁡</mo><mrow id="S3.E2.m1.9.9.4.2.2" xref="S3.E2.m1.9.9.4.2.2.cmml"><munder id="S3.E2.m1.9.9.4.2.2.1" xref="S3.E2.m1.9.9.4.2.2.1.cmml"><mi id="S3.E2.m1.9.9.4.2.2.1.2" xref="S3.E2.m1.9.9.4.2.2.1.2.cmml">max</mi><mi id="S3.E2.m1.9.9.4.2.2.1.3" xref="S3.E2.m1.9.9.4.2.2.1.3.cmml">D</mi></munder><mo lspace="0.167em" id="S3.E2.m1.9.9.4.2.2a" xref="S3.E2.m1.9.9.4.2.2.cmml">⁡</mo><mi id="S3.E2.m1.9.9.4.2.2.2" xref="S3.E2.m1.9.9.4.2.2.2.cmml">V</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.9.9.4.1" xref="S3.E2.m1.9.9.4.1.cmml">​</mo><mrow id="S3.E2.m1.9.9.4.3.2" xref="S3.E2.m1.9.9.4.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.9.9.4.3.2.1" xref="S3.E2.m1.9.9.4.3.1.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">D</mi><mo id="S3.E2.m1.9.9.4.3.2.2" xref="S3.E2.m1.9.9.4.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">G</mi><mo stretchy="false" id="S3.E2.m1.9.9.4.3.2.3" xref="S3.E2.m1.9.9.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.9.9.3" xref="S3.E2.m1.9.9.3.cmml">=</mo><mrow id="S3.E2.m1.9.9.2" xref="S3.E2.m1.9.9.2.cmml"><mrow id="S3.E2.m1.8.8.1.1" xref="S3.E2.m1.8.8.1.1.cmml"><msub id="S3.E2.m1.8.8.1.1.3" xref="S3.E2.m1.8.8.1.1.3.cmml"><mi id="S3.E2.m1.8.8.1.1.3.2" xref="S3.E2.m1.8.8.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">x</mi><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">∼</mo><mrow id="S3.E2.m1.1.1.1.4" xref="S3.E2.m1.1.1.1.4.cmml"><msub id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.4.2.cmml"><mi id="S3.E2.m1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.4.2.2.cmml">p</mi><mtext id="S3.E2.m1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.4.2.3a.cmml">data</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.4.1" xref="S3.E2.m1.1.1.1.4.1.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.4.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.4.3.2.1" xref="S3.E2.m1.1.1.1.4.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">x</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.4.3.2.2" xref="S3.E2.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.2" xref="S3.E2.m1.8.8.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.8.8.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1.1.1.1.2" xref="S3.E2.m1.8.8.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.2.1" xref="S3.E2.m1.8.8.1.1.1.1.1.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.8.8.1.1.1.1.1.2a" xref="S3.E2.m1.8.8.1.1.1.1.1.2.cmml">⁡</mo><mi id="S3.E2.m1.8.8.1.1.1.1.1.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.2.2.cmml">D</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.8.8.1.1.1.1.1.3.2" xref="S3.E2.m1.8.8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.1.3.2.1" xref="S3.E2.m1.8.8.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">x</mi><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.1.3.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.8.8.1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E2.m1.9.9.2.3" xref="S3.E2.m1.9.9.2.3.cmml">+</mo><mrow id="S3.E2.m1.9.9.2.2" xref="S3.E2.m1.9.9.2.2.cmml"><msub id="S3.E2.m1.9.9.2.2.3" xref="S3.E2.m1.9.9.2.2.3.cmml"><mi id="S3.E2.m1.9.9.2.2.3.2" xref="S3.E2.m1.9.9.2.2.3.2.cmml">𝔼</mi><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">z</mi><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">∼</mo><mrow id="S3.E2.m1.2.2.1.4" xref="S3.E2.m1.2.2.1.4.cmml"><msub id="S3.E2.m1.2.2.1.4.2" xref="S3.E2.m1.2.2.1.4.2.cmml"><mi id="S3.E2.m1.2.2.1.4.2.2" xref="S3.E2.m1.2.2.1.4.2.2.cmml">p</mi><mi id="S3.E2.m1.2.2.1.4.2.3" xref="S3.E2.m1.2.2.1.4.2.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.4.1" xref="S3.E2.m1.2.2.1.4.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.4.3.2" xref="S3.E2.m1.2.2.1.4.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.4.3.2.1" xref="S3.E2.m1.2.2.1.4.cmml">(</mo><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.2.2.1.4.3.2.2" xref="S3.E2.m1.2.2.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.9.9.2.2.2" xref="S3.E2.m1.9.9.2.2.2.cmml">​</mo><mrow id="S3.E2.m1.9.9.2.2.1.1" xref="S3.E2.m1.9.9.2.2.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.2" xref="S3.E2.m1.9.9.2.2.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml">log</mi><mo id="S3.E2.m1.9.9.2.2.1.1.1.1a" xref="S3.E2.m1.9.9.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.2" xref="S3.E2.m1.9.9.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.3.cmml">1</mn><mo id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">z</mi><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.3" xref="S3.E2.m1.9.9.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.9.9.2.2.1.1.3" xref="S3.E2.m1.9.9.2.2.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.9b"><apply id="S3.E2.m1.9.9.cmml" xref="S3.E2.m1.9.9"><eq id="S3.E2.m1.9.9.3.cmml" xref="S3.E2.m1.9.9.3"></eq><apply id="S3.E2.m1.9.9.4.cmml" xref="S3.E2.m1.9.9.4"><times id="S3.E2.m1.9.9.4.1.cmml" xref="S3.E2.m1.9.9.4.1"></times><apply id="S3.E2.m1.9.9.4.2.cmml" xref="S3.E2.m1.9.9.4.2"><apply id="S3.E2.m1.9.9.4.2.1.cmml" xref="S3.E2.m1.9.9.4.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.4.2.1.1.cmml" xref="S3.E2.m1.9.9.4.2.1">subscript</csymbol><min id="S3.E2.m1.9.9.4.2.1.2.cmml" xref="S3.E2.m1.9.9.4.2.1.2"></min><ci id="S3.E2.m1.9.9.4.2.1.3.cmml" xref="S3.E2.m1.9.9.4.2.1.3">𝐺</ci></apply><apply id="S3.E2.m1.9.9.4.2.2.cmml" xref="S3.E2.m1.9.9.4.2.2"><apply id="S3.E2.m1.9.9.4.2.2.1.cmml" xref="S3.E2.m1.9.9.4.2.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.4.2.2.1.1.cmml" xref="S3.E2.m1.9.9.4.2.2.1">subscript</csymbol><max id="S3.E2.m1.9.9.4.2.2.1.2.cmml" xref="S3.E2.m1.9.9.4.2.2.1.2"></max><ci id="S3.E2.m1.9.9.4.2.2.1.3.cmml" xref="S3.E2.m1.9.9.4.2.2.1.3">𝐷</ci></apply><ci id="S3.E2.m1.9.9.4.2.2.2.cmml" xref="S3.E2.m1.9.9.4.2.2.2">𝑉</ci></apply></apply><interval closure="open" id="S3.E2.m1.9.9.4.3.1.cmml" xref="S3.E2.m1.9.9.4.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝐷</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝐺</ci></interval></apply><apply id="S3.E2.m1.9.9.2.cmml" xref="S3.E2.m1.9.9.2"><plus id="S3.E2.m1.9.9.2.3.cmml" xref="S3.E2.m1.9.9.2.3"></plus><apply id="S3.E2.m1.8.8.1.1.cmml" xref="S3.E2.m1.8.8.1.1"><times id="S3.E2.m1.8.8.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2"></times><apply id="S3.E2.m1.8.8.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.3.1.cmml" xref="S3.E2.m1.8.8.1.1.3">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.3.2.cmml" xref="S3.E2.m1.8.8.1.1.3.2">𝔼</ci><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2">similar-to</csymbol><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝑥</ci><apply id="S3.E2.m1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.4"><times id="S3.E2.m1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.4.1"></times><apply id="S3.E2.m1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.4.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.4.2.2">𝑝</ci><ci id="S3.E2.m1.1.1.1.4.2.3a.cmml" xref="S3.E2.m1.1.1.1.4.2.3"><mtext mathsize="50%" id="S3.E2.m1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.4.2.3">data</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑥</ci></apply></apply></apply><apply id="S3.E2.m1.8.8.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.8.8.1.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1"><times id="S3.E2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1"></times><apply id="S3.E2.m1.8.8.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.2"><log id="S3.E2.m1.8.8.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.2.1"></log><ci id="S3.E2.m1.8.8.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.2.2">𝐷</ci></apply><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝑥</ci></apply></apply></apply><apply id="S3.E2.m1.9.9.2.2.cmml" xref="S3.E2.m1.9.9.2.2"><times id="S3.E2.m1.9.9.2.2.2.cmml" xref="S3.E2.m1.9.9.2.2.2"></times><apply id="S3.E2.m1.9.9.2.2.3.cmml" xref="S3.E2.m1.9.9.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.9.9.2.2.3.1.cmml" xref="S3.E2.m1.9.9.2.2.3">subscript</csymbol><ci id="S3.E2.m1.9.9.2.2.3.2.cmml" xref="S3.E2.m1.9.9.2.2.3.2">𝔼</ci><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2">similar-to</csymbol><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">𝑧</ci><apply id="S3.E2.m1.2.2.1.4.cmml" xref="S3.E2.m1.2.2.1.4"><times id="S3.E2.m1.2.2.1.4.1.cmml" xref="S3.E2.m1.2.2.1.4.1"></times><apply id="S3.E2.m1.2.2.1.4.2.cmml" xref="S3.E2.m1.2.2.1.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.4.2.1.cmml" xref="S3.E2.m1.2.2.1.4.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.4.2.2.cmml" xref="S3.E2.m1.2.2.1.4.2.2">𝑝</ci><ci id="S3.E2.m1.2.2.1.4.2.3.cmml" xref="S3.E2.m1.2.2.1.4.2.3">𝑧</ci></apply><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">𝑧</ci></apply></apply></apply><apply id="S3.E2.m1.9.9.2.2.1.2.cmml" xref="S3.E2.m1.9.9.2.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.9.9.2.2.1.2.1.cmml" xref="S3.E2.m1.9.9.2.2.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.9.9.2.2.1.1.1.2.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1"><log id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7"></log><apply id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1"><minus id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.2"></minus><cn type="integer" id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.3">1</cn><apply id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.3">𝐷</ci><apply id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.9.9.2.2.1.1.1.1.1.1.1.1.1.1.2">𝐺</ci><ci id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6">𝑧</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.9c">\min_{G}\max_{D}V(D,G)=\mathbb{E}_{x\sim p_{\text{data}}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.8" class="ltx_p">In the <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> objective function, <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="\mathbb{E}_{x\sim p_{\text{data}}(x)}" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><msub id="S3.SS4.p4.1.m1.1.2" xref="S3.SS4.p4.1.m1.1.2.cmml"><mi id="S3.SS4.p4.1.m1.1.2.2" xref="S3.SS4.p4.1.m1.1.2.2.cmml">𝔼</mi><mrow id="S3.SS4.p4.1.m1.1.1.1" xref="S3.SS4.p4.1.m1.1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.1.3.cmml">x</mi><mo id="S3.SS4.p4.1.m1.1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.1.2.cmml">∼</mo><mrow id="S3.SS4.p4.1.m1.1.1.1.4" xref="S3.SS4.p4.1.m1.1.1.1.4.cmml"><msub id="S3.SS4.p4.1.m1.1.1.1.4.2" xref="S3.SS4.p4.1.m1.1.1.1.4.2.cmml"><mi id="S3.SS4.p4.1.m1.1.1.1.4.2.2" xref="S3.SS4.p4.1.m1.1.1.1.4.2.2.cmml">p</mi><mtext id="S3.SS4.p4.1.m1.1.1.1.4.2.3" xref="S3.SS4.p4.1.m1.1.1.1.4.2.3a.cmml">data</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p4.1.m1.1.1.1.4.1" xref="S3.SS4.p4.1.m1.1.1.1.4.1.cmml">​</mo><mrow id="S3.SS4.p4.1.m1.1.1.1.4.3.2" xref="S3.SS4.p4.1.m1.1.1.1.4.cmml"><mo stretchy="false" id="S3.SS4.p4.1.m1.1.1.1.4.3.2.1" xref="S3.SS4.p4.1.m1.1.1.1.4.cmml">(</mo><mi id="S3.SS4.p4.1.m1.1.1.1.1" xref="S3.SS4.p4.1.m1.1.1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS4.p4.1.m1.1.1.1.4.3.2.2" xref="S3.SS4.p4.1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.2.1.cmml" xref="S3.SS4.p4.1.m1.1.2">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.2.2.cmml" xref="S3.SS4.p4.1.m1.1.2.2">𝔼</ci><apply id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS4.p4.1.m1.1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.1.2">similar-to</csymbol><ci id="S3.SS4.p4.1.m1.1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.1.3">𝑥</ci><apply id="S3.SS4.p4.1.m1.1.1.1.4.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4"><times id="S3.SS4.p4.1.m1.1.1.1.4.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.1"></times><apply id="S3.SS4.p4.1.m1.1.1.1.4.2.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.1.4.2.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.2">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.1.1.4.2.2.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.2.2">𝑝</ci><ci id="S3.SS4.p4.1.m1.1.1.1.4.2.3a.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.2.3"><mtext mathsize="50%" id="S3.SS4.p4.1.m1.1.1.1.4.2.3.cmml" xref="S3.SS4.p4.1.m1.1.1.1.4.2.3">data</mtext></ci></apply><ci id="S3.SS4.p4.1.m1.1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1.1">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\mathbb{E}_{x\sim p_{\text{data}}(x)}</annotation></semantics></math> and <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="\mathbb{E}_{z\sim p_{z}(z)}" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><msub id="S3.SS4.p4.2.m2.1.2" xref="S3.SS4.p4.2.m2.1.2.cmml"><mi id="S3.SS4.p4.2.m2.1.2.2" xref="S3.SS4.p4.2.m2.1.2.2.cmml">𝔼</mi><mrow id="S3.SS4.p4.2.m2.1.1.1" xref="S3.SS4.p4.2.m2.1.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.1.3" xref="S3.SS4.p4.2.m2.1.1.1.3.cmml">z</mi><mo id="S3.SS4.p4.2.m2.1.1.1.2" xref="S3.SS4.p4.2.m2.1.1.1.2.cmml">∼</mo><mrow id="S3.SS4.p4.2.m2.1.1.1.4" xref="S3.SS4.p4.2.m2.1.1.1.4.cmml"><msub id="S3.SS4.p4.2.m2.1.1.1.4.2" xref="S3.SS4.p4.2.m2.1.1.1.4.2.cmml"><mi id="S3.SS4.p4.2.m2.1.1.1.4.2.2" xref="S3.SS4.p4.2.m2.1.1.1.4.2.2.cmml">p</mi><mi id="S3.SS4.p4.2.m2.1.1.1.4.2.3" xref="S3.SS4.p4.2.m2.1.1.1.4.2.3.cmml">z</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p4.2.m2.1.1.1.4.1" xref="S3.SS4.p4.2.m2.1.1.1.4.1.cmml">​</mo><mrow id="S3.SS4.p4.2.m2.1.1.1.4.3.2" xref="S3.SS4.p4.2.m2.1.1.1.4.cmml"><mo stretchy="false" id="S3.SS4.p4.2.m2.1.1.1.4.3.2.1" xref="S3.SS4.p4.2.m2.1.1.1.4.cmml">(</mo><mi id="S3.SS4.p4.2.m2.1.1.1.1" xref="S3.SS4.p4.2.m2.1.1.1.1.cmml">z</mi><mo stretchy="false" id="S3.SS4.p4.2.m2.1.1.1.4.3.2.2" xref="S3.SS4.p4.2.m2.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.2.cmml" xref="S3.SS4.p4.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.2.1.cmml" xref="S3.SS4.p4.2.m2.1.2">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.2.2.cmml" xref="S3.SS4.p4.2.m2.1.2.2">𝔼</ci><apply id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS4.p4.2.m2.1.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.1.2">similar-to</csymbol><ci id="S3.SS4.p4.2.m2.1.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.1.3">𝑧</ci><apply id="S3.SS4.p4.2.m2.1.1.1.4.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4"><times id="S3.SS4.p4.2.m2.1.1.1.4.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4.1"></times><apply id="S3.SS4.p4.2.m2.1.1.1.4.2.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.1.4.2.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4.2">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.1.4.2.2.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4.2.2">𝑝</ci><ci id="S3.SS4.p4.2.m2.1.1.1.4.2.3.cmml" xref="S3.SS4.p4.2.m2.1.1.1.4.2.3">𝑧</ci></apply><ci id="S3.SS4.p4.2.m2.1.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1.1">𝑧</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">\mathbb{E}_{z\sim p_{z}(z)}</annotation></semantics></math> indicate the expected values over real data samples <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">x</annotation></semantics></math> and noise samples <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><mi id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><ci id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">z</annotation></semantics></math>, respectively. <math id="S3.SS4.p4.5.m5.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS4.p4.5.m5.1a"><mi id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><ci id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">G</annotation></semantics></math> generates samples, <math id="S3.SS4.p4.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS4.p4.6.m6.1a"><mi id="S3.SS4.p4.6.m6.1.1" xref="S3.SS4.p4.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m6.1b"><ci id="S3.SS4.p4.6.m6.1.1.cmml" xref="S3.SS4.p4.6.m6.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m6.1c">D</annotation></semantics></math> discriminates between real and generated samples, <math id="S3.SS4.p4.7.m7.1" class="ltx_Math" alttext="p_{\text{data}}" display="inline"><semantics id="S3.SS4.p4.7.m7.1a"><msub id="S3.SS4.p4.7.m7.1.1" xref="S3.SS4.p4.7.m7.1.1.cmml"><mi id="S3.SS4.p4.7.m7.1.1.2" xref="S3.SS4.p4.7.m7.1.1.2.cmml">p</mi><mtext id="S3.SS4.p4.7.m7.1.1.3" xref="S3.SS4.p4.7.m7.1.1.3a.cmml">data</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m7.1b"><apply id="S3.SS4.p4.7.m7.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.7.m7.1.1.1.cmml" xref="S3.SS4.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS4.p4.7.m7.1.1.2.cmml" xref="S3.SS4.p4.7.m7.1.1.2">𝑝</ci><ci id="S3.SS4.p4.7.m7.1.1.3a.cmml" xref="S3.SS4.p4.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS4.p4.7.m7.1.1.3.cmml" xref="S3.SS4.p4.7.m7.1.1.3">data</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m7.1c">p_{\text{data}}</annotation></semantics></math> and <math id="S3.SS4.p4.8.m8.1" class="ltx_Math" alttext="p_{z}" display="inline"><semantics id="S3.SS4.p4.8.m8.1a"><msub id="S3.SS4.p4.8.m8.1.1" xref="S3.SS4.p4.8.m8.1.1.cmml"><mi id="S3.SS4.p4.8.m8.1.1.2" xref="S3.SS4.p4.8.m8.1.1.2.cmml">p</mi><mi id="S3.SS4.p4.8.m8.1.1.3" xref="S3.SS4.p4.8.m8.1.1.3.cmml">z</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m8.1b"><apply id="S3.SS4.p4.8.m8.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.8.m8.1.1.1.cmml" xref="S3.SS4.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p4.8.m8.1.1.2.cmml" xref="S3.SS4.p4.8.m8.1.1.2">𝑝</ci><ci id="S3.SS4.p4.8.m8.1.1.3.cmml" xref="S3.SS4.p4.8.m8.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m8.1c">p_{z}</annotation></semantics></math> are the distributions of real data and noise, respectively. Using <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a>, the research in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> proposes a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based method for reducing metal artifacts in post-operative <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> imaging. The method utilizes a 3D-<a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> trained on a large number of pre-operative images with simulated metal artifacts. The <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> generates artifact-free images by reducing the metal artifacts. The effectiveness of the method is evaluated quantitatively and qualitatively, showing promising results compared to classical artifact reduction algorithms. The approach overcomes the challenges of post-operative assessment of cochlear implantation caused by metal artifacts, and it does not require registration of pre and post-operative images. The 3D-<a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> improves spatial consistency and is applicable to various types of artifacts. In addition, Wang et al. in theirs paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, proposes a 3D metal artifact reduction algorithm for post-operative high-resolution <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> imaging. The algorithm is based on a <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> that uses simulated physically realistic <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> metal artifacts created by <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> electrodes. The generated images are used to train the network for artifact reduction. The metal artifact reduction-<a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a> based method as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, utilizes a three-step process for reducing metal artifacts. Firstly, a simulation is performed to replicate <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> positioning. Secondly, a physical simulation of CI metal artifacts is conducted. Lastly, a 3D GAN is trained using both simulated and preoperative datasets. The generator component of the GAN generates an image that has reduced metal artifacts, while the discriminator network is responsible for determining whether the input image contains metal artifacts or not. The method was evaluated on clinical <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> postoperative cases and outperformed other general metal artifact reduction approaches. The paper introduces a novel approach that combines the physical simulation of metal artifacts with 3D-<a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a>, providing a promising solution for improving the visual assessment of post-operative imaging in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a>.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">Similarly, for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> metal artifacts reduction also, a  <a href="#Sx1.70.70.70"><span href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">conditional generative adversarial networks</span></span></span></a> (<a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a>) were proposed by Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The approach involves training a <a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a> to learn mapping from artifact-affected <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> to artifact-free <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a>. During inference, the <a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a> generated <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images with removed artifacts. Additionally, a band-wise normalization method was proposed as a preprocessing step to improve the performance of <a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a>. The method was evaluated on post-implantation <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> recipients, and the quality of the artifact-corrected images was quantitatively assessed using <a href="#Sx1.72.72.72"><abbr href="#Sx1.72.72.72" title="point to point error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">P2PE</span></span></abbr></a>. The results demonstrate promising artifact reduction, outperforming the previously proposed techniques. The authors evaluates the quality of artifact-corrected images using a quantitative metric based on segmentations of intracochlear anatomical structures. Specifically, the segmentation results obtained from a previously published method were compared between real preimplantation <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> and artifact-corrected <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> generated by the proposed method. The <a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a> was used as a metric to assess the accuracy of the segmentation. The paper reports that the proposed method achieves an <a href="#Sx1.120.120.120"><abbr href="#Sx1.120.120.120" title="average surface error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASE</span></span></abbr></a> of 0.18 mm, which is approximately half of the error obtained with a previously proposed technique.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>/<a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a>-based methods</h3>

<div id="S3.SS5.p1" class="ltx_para">
<span id="S3.SS5.p1.8" class="ltx_ERROR undefined">\Acp</span>
<p id="S3.SS5.p1.7" class="ltx_p">RNN are a class of artificial neural networks designed for sequential data processing. They maintain hidden state information that is updated at each time step, allowing them to capture temporal dependencies. The hidden state at time <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">t</annotation></semantics></math>, denoted as <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">h</mi><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">ℎ</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">h_{t}</annotation></semantics></math>, is computed based on the input <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝑥</ci><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">x_{t}</annotation></semantics></math>, the previous hidden state <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="h_{t-1}" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><msub id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml"><mi id="S3.SS5.p1.4.m4.1.1.2" xref="S3.SS5.p1.4.m4.1.1.2.cmml">h</mi><mrow id="S3.SS5.p1.4.m4.1.1.3" xref="S3.SS5.p1.4.m4.1.1.3.cmml"><mi id="S3.SS5.p1.4.m4.1.1.3.2" xref="S3.SS5.p1.4.m4.1.1.3.2.cmml">t</mi><mo id="S3.SS5.p1.4.m4.1.1.3.1" xref="S3.SS5.p1.4.m4.1.1.3.1.cmml">−</mo><mn id="S3.SS5.p1.4.m4.1.1.3.3" xref="S3.SS5.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><apply id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.4.m4.1.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p1.4.m4.1.1.2.cmml" xref="S3.SS5.p1.4.m4.1.1.2">ℎ</ci><apply id="S3.SS5.p1.4.m4.1.1.3.cmml" xref="S3.SS5.p1.4.m4.1.1.3"><minus id="S3.SS5.p1.4.m4.1.1.3.1.cmml" xref="S3.SS5.p1.4.m4.1.1.3.1"></minus><ci id="S3.SS5.p1.4.m4.1.1.3.2.cmml" xref="S3.SS5.p1.4.m4.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS5.p1.4.m4.1.1.3.3.cmml" xref="S3.SS5.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">h_{t-1}</annotation></semantics></math>, and model parameters <math id="S3.SS5.p1.5.m5.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS5.p1.5.m5.1a"><mi id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><ci id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">W</annotation></semantics></math> and <math id="S3.SS5.p1.6.m6.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS5.p1.6.m6.1a"><mi id="S3.SS5.p1.6.m6.1.1" xref="S3.SS5.p1.6.m6.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.1b"><ci id="S3.SS5.p1.6.m6.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.1c">U</annotation></semantics></math>, with <math id="S3.SS5.p1.7.m7.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p1.7.m7.1a"><mi id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><ci id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">b</annotation></semantics></math> representing the bias term. The equations governing the hidden state update are given by:</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E3.m1.1" class="ltx_Math" alttext="h_{t}=\sigma(Wx_{t}+Uh_{t-1}+b)" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">h</mi><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S3.E3.m1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml">h</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.2.cmml">t</mi><mo id="S3.E3.m1.1.1.1.1.1.1.3.3.3.1" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.1.cmml">−</mo><mn id="S3.E3.m1.1.1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1a" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E3.m1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.4.cmml">b</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">ℎ</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝜎</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝑊</ci><apply id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><times id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑈</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.2">ℎ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3"><minus id="S3.E3.m1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.1"></minus><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.4">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">h_{t}=\sigma(Wx_{t}+Uh_{t-1}+b)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<p id="S3.SS5.p3.1" class="ltx_p">where, <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mi id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><ci id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\sigma</annotation></semantics></math> is an activation function, typically the hyperbolic tangent or  <a href="#Sx1.94.94.94"><span href="#Sx1.94.94.94" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">rectified linear unit</span></span></span></a> (<a href="#Sx1.94.94.94"><abbr href="#Sx1.94.94.94" title="rectified linear unit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ReLU</span></span></abbr></a>). Several speech processing techniques that utilize <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> are based on <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">RNN</span>s</span></abbr></a>.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p"><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users struggle with music perception, and many studies have shown that enhancing music vocals improves their enjoyment. The study described by Gajęcki. et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> explores source separation algorithms to remix pop songs by emphasizing the lead-singing voice. <span id="S3.SS5.p4.1.1" class="ltx_ERROR undefined">\Ac</span>DCAE,  <a href="#Sx1.82.82.82"><span href="#Sx1.82.82.82" title="deep recurrent neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep recurrent neural networks</span></span></span></a> (<a href="#Sx1.82.82.82"><abbr href="#Sx1.82.82.82" title="deep recurrent neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRNN</span></span></abbr></a>),  <a href="#Sx1.83.83.83"><span href="#Sx1.83.83.83" title="multilayer perceptrons" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">multilayer perceptrons</span></span></span></a> (<a href="#Sx1.83.83.83"><abbr href="#Sx1.83.83.83" title="multilayer perceptrons" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MLP</span></span></abbr></a>), and  <a href="#Sx1.84.84.84"><span href="#Sx1.84.84.84" title="non-negative matrix factorization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">non-negative matrix factorization</span></span></span></a> (<a href="#Sx1.84.84.84"><abbr href="#Sx1.84.84.84" title="non-negative matrix factorization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NMF</span></span></abbr></a>) were evaluated through perceptual experiments involving <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients and normal hearing subjects. The results show that a <a href="#Sx1.83.83.83"><abbr href="#Sx1.83.83.83" title="multilayer perceptrons" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MLP</span></span></abbr></a> and <a href="#Sx1.82.82.82"><abbr href="#Sx1.82.82.82" title="deep recurrent neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRNN</span></span></abbr></a> perform well, providing minimal distortions and artifacts that are not perceived by <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users. The paper also highlights the benefits of the implementation of a <a href="#Sx1.83.83.83"><abbr href="#Sx1.83.83.83" title="multilayer perceptrons" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MLP</span></span></abbr></a> for real-time audio source separation to enhance music for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users due to their reduced computation time.
In addition, The study described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposes a speech separation framework for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users using TasNet and <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>-<a href="#Sx1.93.93.93"><abbr href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EVD</span></span></abbr></a>. TasNet, a non-causal  <a href="#Sx1.92.92.92"><span href="#Sx1.92.92.92" title="multiple-input multiple-output" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">multiple-input multiple-output</span></span></span></a> (<a href="#Sx1.92.92.92"><abbr href="#Sx1.92.92.92" title="multiple-input multiple-output" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MIMO</span></span></abbr></a>)-based method, is employed as the speech separation module. <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>-<a href="#Sx1.93.93.93"><abbr href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EVD</span></span></abbr></a>, which combines <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">RNN</span>s</span></abbr></a> with <a href="#Sx1.93.93.93"><abbr href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EVD</span></span></abbr></a>, is utilized to preserve spatial cues. The framework aims to effectively separate speech and reduce <a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a> errors. The <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>-<a href="#Sx1.93.93.93"><abbr href="#Sx1.93.93.93" title="eigenvector decomposition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EVD</span></span></abbr></a> network is trained using <math id="S3.SS5.p4.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p4.1.m1.1a"><mi mathvariant="normal" id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><ci id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">\Delta</annotation></semantics></math><a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a> as the objective, and an additional <a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a> term is added to the loss function for convergence. The experimental results demonstrate the effectiveness of the proposed framework in preserving <a href="#Sx1.86.86.86"><abbr href="#Sx1.86.86.86" title="interaural level difference" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ILD</span></span></abbr></a> cues for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users in various hearing scenarios.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.11" class="ltx_p">The  <a href="#Sx1.48.48.48"><span href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">long short term memory</span></span></span></a> (<a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a>), an enhanced version of the <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>, addresses limitations observed in <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">RNN</span>s</span></abbr></a> under specific conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Unlike <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a>, <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> excels in preserving past information, making it suitable for tasks with long-term dependencies. Comprising <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> units forming layers, each unit regulates information flow through input, output, and forget gates, allowing for prolonged retention of crucial information. The forward pass equations (<a href="#S3.E4" title="In 3.5 /-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) illustrate this process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The symbols <math id="S3.SS5.p5.1.m1.1" class="ltx_Math" alttext="L_{i}" display="inline"><semantics id="S3.SS5.p5.1.m1.1a"><msub id="S3.SS5.p5.1.m1.1.1" xref="S3.SS5.p5.1.m1.1.1.cmml"><mi id="S3.SS5.p5.1.m1.1.1.2" xref="S3.SS5.p5.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS5.p5.1.m1.1.1.3" xref="S3.SS5.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.1.m1.1b"><apply id="S3.SS5.p5.1.m1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.1.m1.1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p5.1.m1.1.1.2.cmml" xref="S3.SS5.p5.1.m1.1.1.2">𝐿</ci><ci id="S3.SS5.p5.1.m1.1.1.3.cmml" xref="S3.SS5.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.1.m1.1c">L_{i}</annotation></semantics></math> and <math id="S3.SS5.p5.2.m2.1" class="ltx_Math" alttext="L_{j}" display="inline"><semantics id="S3.SS5.p5.2.m2.1a"><msub id="S3.SS5.p5.2.m2.1.1" xref="S3.SS5.p5.2.m2.1.1.cmml"><mi id="S3.SS5.p5.2.m2.1.1.2" xref="S3.SS5.p5.2.m2.1.1.2.cmml">L</mi><mi id="S3.SS5.p5.2.m2.1.1.3" xref="S3.SS5.p5.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.2.m2.1b"><apply id="S3.SS5.p5.2.m2.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.2.m2.1.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p5.2.m2.1.1.2.cmml" xref="S3.SS5.p5.2.m2.1.1.2">𝐿</ci><ci id="S3.SS5.p5.2.m2.1.1.3.cmml" xref="S3.SS5.p5.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.2.m2.1c">L_{j}</annotation></semantics></math> denote input and output, while <math id="S3.SS5.p5.3.m3.1" class="ltx_Math" alttext="A_{f}" display="inline"><semantics id="S3.SS5.p5.3.m3.1a"><msub id="S3.SS5.p5.3.m3.1.1" xref="S3.SS5.p5.3.m3.1.1.cmml"><mi id="S3.SS5.p5.3.m3.1.1.2" xref="S3.SS5.p5.3.m3.1.1.2.cmml">A</mi><mi id="S3.SS5.p5.3.m3.1.1.3" xref="S3.SS5.p5.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.3.m3.1b"><apply id="S3.SS5.p5.3.m3.1.1.cmml" xref="S3.SS5.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.3.m3.1.1.1.cmml" xref="S3.SS5.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p5.3.m3.1.1.2.cmml" xref="S3.SS5.p5.3.m3.1.1.2">𝐴</ci><ci id="S3.SS5.p5.3.m3.1.1.3.cmml" xref="S3.SS5.p5.3.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.3.m3.1c">A_{f}</annotation></semantics></math>, <math id="S3.SS5.p5.4.m4.1" class="ltx_Math" alttext="A_{i}" display="inline"><semantics id="S3.SS5.p5.4.m4.1a"><msub id="S3.SS5.p5.4.m4.1.1" xref="S3.SS5.p5.4.m4.1.1.cmml"><mi id="S3.SS5.p5.4.m4.1.1.2" xref="S3.SS5.p5.4.m4.1.1.2.cmml">A</mi><mi id="S3.SS5.p5.4.m4.1.1.3" xref="S3.SS5.p5.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.4.m4.1b"><apply id="S3.SS5.p5.4.m4.1.1.cmml" xref="S3.SS5.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.4.m4.1.1.1.cmml" xref="S3.SS5.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p5.4.m4.1.1.2.cmml" xref="S3.SS5.p5.4.m4.1.1.2">𝐴</ci><ci id="S3.SS5.p5.4.m4.1.1.3.cmml" xref="S3.SS5.p5.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.4.m4.1c">A_{i}</annotation></semantics></math>, and <math id="S3.SS5.p5.5.m5.1" class="ltx_Math" alttext="A_{j}" display="inline"><semantics id="S3.SS5.p5.5.m5.1a"><msub id="S3.SS5.p5.5.m5.1.1" xref="S3.SS5.p5.5.m5.1.1.cmml"><mi id="S3.SS5.p5.5.m5.1.1.2" xref="S3.SS5.p5.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS5.p5.5.m5.1.1.3" xref="S3.SS5.p5.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.5.m5.1b"><apply id="S3.SS5.p5.5.m5.1.1.cmml" xref="S3.SS5.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.5.m5.1.1.1.cmml" xref="S3.SS5.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p5.5.m5.1.1.2.cmml" xref="S3.SS5.p5.5.m5.1.1.2">𝐴</ci><ci id="S3.SS5.p5.5.m5.1.1.3.cmml" xref="S3.SS5.p5.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.5.m5.1c">A_{j}</annotation></semantics></math> represent activation vectors for forget, input, and output gates. <math id="S3.SS5.p5.6.m6.1" class="ltx_Math" alttext="V_{c}" display="inline"><semantics id="S3.SS5.p5.6.m6.1a"><msub id="S3.SS5.p5.6.m6.1.1" xref="S3.SS5.p5.6.m6.1.1.cmml"><mi id="S3.SS5.p5.6.m6.1.1.2" xref="S3.SS5.p5.6.m6.1.1.2.cmml">V</mi><mi id="S3.SS5.p5.6.m6.1.1.3" xref="S3.SS5.p5.6.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.6.m6.1b"><apply id="S3.SS5.p5.6.m6.1.1.cmml" xref="S3.SS5.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.6.m6.1.1.1.cmml" xref="S3.SS5.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS5.p5.6.m6.1.1.2.cmml" xref="S3.SS5.p5.6.m6.1.1.2">𝑉</ci><ci id="S3.SS5.p5.6.m6.1.1.3.cmml" xref="S3.SS5.p5.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.6.m6.1c">V_{c}</annotation></semantics></math> is the cell state vector, and <math id="S3.SS5.p5.7.m7.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS5.p5.7.m7.1a"><mi id="S3.SS5.p5.7.m7.1.1" xref="S3.SS5.p5.7.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.7.m7.1b"><ci id="S3.SS5.p5.7.m7.1.1.cmml" xref="S3.SS5.p5.7.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.7.m7.1c">\sigma</annotation></semantics></math> for the sigmoid activation function and <math id="S3.SS5.p5.8.m8.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS5.p5.8.m8.1a"><mo id="S3.SS5.p5.8.m8.1.1" xref="S3.SS5.p5.8.m8.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.8.m8.1b"><csymbol cd="latexml" id="S3.SS5.p5.8.m8.1.1.cmml" xref="S3.SS5.p5.8.m8.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.8.m8.1c">\odot</annotation></semantics></math> for element-wise multiplication. This <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> structure with weight matrices <math id="S3.SS5.p5.9.m9.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS5.p5.9.m9.1a"><mi id="S3.SS5.p5.9.m9.1.1" xref="S3.SS5.p5.9.m9.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.9.m9.1b"><ci id="S3.SS5.p5.9.m9.1.1.cmml" xref="S3.SS5.p5.9.m9.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.9.m9.1c">W</annotation></semantics></math> and <math id="S3.SS5.p5.10.m10.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS5.p5.10.m10.1a"><mi id="S3.SS5.p5.10.m10.1.1" xref="S3.SS5.p5.10.m10.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.10.m10.1b"><ci id="S3.SS5.p5.10.m10.1.1.cmml" xref="S3.SS5.p5.10.m10.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.10.m10.1c">U</annotation></semantics></math> and bias vector <math id="S3.SS5.p5.11.m11.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS5.p5.11.m11.1a"><mi id="S3.SS5.p5.11.m11.1.1" xref="S3.SS5.p5.11.m11.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.11.m11.1b"><ci id="S3.SS5.p5.11.m11.1.1.cmml" xref="S3.SS5.p5.11.m11.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.11.m11.1c">b</annotation></semantics></math> is outlined by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<div id="S3.SS5.p6" class="ltx_para">
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td colspan="2" class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.E4.m1.109" class="ltx_math_unparsed" alttext="\displaystyle\begin{split}A_{f}&amp;=\sigma(W_{f}*L_{i}+U_{f}*L_{j-1}+b_{f}),\quad A_{i}=\sigma(W_{i}*L_{i}+U_{i}*L_{j-1}+b_{i}),\\
L_{j}&amp;=A_{j}\odot\tanh(V_{c}),\\
A_{j}&amp;=\sigma(W_{0}*L_{i}+U_{0}*L_{j-1}+b_{j}),\\
V_{c}&amp;=A_{f}*V_{c-1}+A_{i}*\text{cell\_state}(W_{c}*L_{i}+U_{c}*L_{j-1}+b_{c}).\end{split}" display="inline"><semantics id="S3.E4.m1.109a"><mtable columnspacing="0pt" rowspacing="0pt" id="S3.E4.m1.109.109.4"><mtr id="S3.E4.m1.109.109.4a"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.109.109.4b"><msub id="S3.E4.m1.2.2.2.2.2a"><mi id="S3.E4.m1.1.1.1.1.1.1">A</mi><mi id="S3.E4.m1.2.2.2.2.2.2.1">f</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.109.109.4c"><mrow id="S3.E4.m1.106.106.1.106.43.41.41"><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1"><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1"><mi id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.2"></mi><mo id="S3.E4.m1.3.3.3.3.1.1">=</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1"><mi id="S3.E4.m1.4.4.4.4.2.2">σ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.2">​</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1"><mo stretchy="false" id="S3.E4.m1.5.5.5.5.3.3">(</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1"><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.1"><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.1.1"><mi id="S3.E4.m1.6.6.6.6.4.4">W</mi><mi id="S3.E4.m1.7.7.7.7.5.5.1">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.8.8.8.8.6.6">∗</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.1.2"><mi id="S3.E4.m1.9.9.9.9.7.7">L</mi><mi id="S3.E4.m1.10.10.10.10.8.8.1">i</mi></msub></mrow><mo id="S3.E4.m1.11.11.11.11.9.9">+</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.2"><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.2.1"><mi id="S3.E4.m1.12.12.12.12.10.10">U</mi><mi id="S3.E4.m1.13.13.13.13.11.11.1">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.14.14.14.14.12.12">∗</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.2.2"><mi id="S3.E4.m1.15.15.15.15.13.13">L</mi><mrow id="S3.E4.m1.16.16.16.16.14.14.1"><mi id="S3.E4.m1.16.16.16.16.14.14.1.2">j</mi><mo id="S3.E4.m1.16.16.16.16.14.14.1.1">−</mo><mn id="S3.E4.m1.16.16.16.16.14.14.1.3">1</mn></mrow></msub></mrow><mo id="S3.E4.m1.11.11.11.11.9.9a">+</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.1.1.1.1.1.1.3"><mi id="S3.E4.m1.18.18.18.18.16.16">b</mi><mi id="S3.E4.m1.19.19.19.19.17.17.1">f</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.20.20.20.20.18.18">)</mo></mrow></mrow></mrow><mo rspace="1.167em" id="S3.E4.m1.21.21.21.21.19.19">,</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2"><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.2"><mi id="S3.E4.m1.22.22.22.22.20.20">A</mi><mi id="S3.E4.m1.23.23.23.23.21.21.1">i</mi></msub><mo id="S3.E4.m1.24.24.24.24.22.22">=</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1"><mi id="S3.E4.m1.25.25.25.25.23.23">σ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.2">​</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1"><mo stretchy="false" id="S3.E4.m1.26.26.26.26.24.24">(</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1"><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.1"><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.1.1"><mi id="S3.E4.m1.27.27.27.27.25.25">W</mi><mi id="S3.E4.m1.28.28.28.28.26.26.1">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.29.29.29.29.27.27">∗</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.1.2"><mi id="S3.E4.m1.30.30.30.30.28.28">L</mi><mi id="S3.E4.m1.31.31.31.31.29.29.1">i</mi></msub></mrow><mo id="S3.E4.m1.32.32.32.32.30.30">+</mo><mrow id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.2"><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.2.1"><mi id="S3.E4.m1.33.33.33.33.31.31">U</mi><mi id="S3.E4.m1.34.34.34.34.32.32.1">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.35.35.35.35.33.33">∗</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.2.2"><mi id="S3.E4.m1.36.36.36.36.34.34">L</mi><mrow id="S3.E4.m1.37.37.37.37.35.35.1"><mi id="S3.E4.m1.37.37.37.37.35.35.1.2">j</mi><mo id="S3.E4.m1.37.37.37.37.35.35.1.1">−</mo><mn id="S3.E4.m1.37.37.37.37.35.35.1.3">1</mn></mrow></msub></mrow><mo id="S3.E4.m1.32.32.32.32.30.30a">+</mo><msub id="S3.E4.m1.106.106.1.106.43.41.41.1.2.2.1.1.1.1.3"><mi id="S3.E4.m1.39.39.39.39.37.37">b</mi><mi id="S3.E4.m1.40.40.40.40.38.38.1">i</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.41.41.41.41.39.39">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.42.42.42.42.40.40">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.109.109.4d"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.109.109.4e"><msub id="S3.E4.m1.44.44.44.2.2a"><mi id="S3.E4.m1.43.43.43.1.1.1">L</mi><mi id="S3.E4.m1.44.44.44.2.2.2.1">j</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.109.109.4f"><mrow id="S3.E4.m1.107.107.2.107.13.11.11"><mrow id="S3.E4.m1.107.107.2.107.13.11.11.1"><mi id="S3.E4.m1.107.107.2.107.13.11.11.1.2"></mi><mo id="S3.E4.m1.45.45.45.3.1.1">=</mo><mrow id="S3.E4.m1.107.107.2.107.13.11.11.1.1"><msub id="S3.E4.m1.107.107.2.107.13.11.11.1.1.2"><mi id="S3.E4.m1.46.46.46.4.2.2">A</mi><mi id="S3.E4.m1.47.47.47.5.3.3.1">j</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.48.48.48.6.4.4">⊙</mo><mrow id="S3.E4.m1.107.107.2.107.13.11.11.1.1.1.1"><mi id="S3.E4.m1.49.49.49.7.5.5">tanh</mi><mo id="S3.E4.m1.107.107.2.107.13.11.11.1.1.1.1a">⁡</mo><mrow id="S3.E4.m1.107.107.2.107.13.11.11.1.1.1.1.1"><mo stretchy="false" id="S3.E4.m1.50.50.50.8.6.6">(</mo><msub id="S3.E4.m1.107.107.2.107.13.11.11.1.1.1.1.1.1"><mi id="S3.E4.m1.51.51.51.9.7.7">V</mi><mi id="S3.E4.m1.52.52.52.10.8.8.1">c</mi></msub><mo stretchy="false" id="S3.E4.m1.53.53.53.11.9.9">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.54.54.54.12.10.10">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.109.109.4g"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.109.109.4h"><msub id="S3.E4.m1.56.56.56.2.2a"><mi id="S3.E4.m1.55.55.55.1.1.1">A</mi><mi id="S3.E4.m1.56.56.56.2.2.2.1">j</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.109.109.4i"><mrow id="S3.E4.m1.108.108.3.108.22.20.20"><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1"><mi id="S3.E4.m1.108.108.3.108.22.20.20.1.2"></mi><mo id="S3.E4.m1.57.57.57.3.1.1">=</mo><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1.1"><mi id="S3.E4.m1.58.58.58.4.2.2">σ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.108.108.3.108.22.20.20.1.1.2">​</mo><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1"><mo stretchy="false" id="S3.E4.m1.59.59.59.5.3.3">(</mo><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1"><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.1"><msub id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.1.1"><mi id="S3.E4.m1.60.60.60.6.4.4">W</mi><mn id="S3.E4.m1.61.61.61.7.5.5.1">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.62.62.62.8.6.6">∗</mo><msub id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.1.2"><mi id="S3.E4.m1.63.63.63.9.7.7">L</mi><mi id="S3.E4.m1.64.64.64.10.8.8.1">i</mi></msub></mrow><mo id="S3.E4.m1.65.65.65.11.9.9">+</mo><mrow id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.2"><msub id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.2.1"><mi id="S3.E4.m1.66.66.66.12.10.10">U</mi><mn id="S3.E4.m1.67.67.67.13.11.11.1">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.68.68.68.14.12.12">∗</mo><msub id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.2.2"><mi id="S3.E4.m1.69.69.69.15.13.13">L</mi><mrow id="S3.E4.m1.70.70.70.16.14.14.1"><mi id="S3.E4.m1.70.70.70.16.14.14.1.2">j</mi><mo id="S3.E4.m1.70.70.70.16.14.14.1.1">−</mo><mn id="S3.E4.m1.70.70.70.16.14.14.1.3">1</mn></mrow></msub></mrow><mo id="S3.E4.m1.65.65.65.11.9.9a">+</mo><msub id="S3.E4.m1.108.108.3.108.22.20.20.1.1.1.1.1.3"><mi id="S3.E4.m1.72.72.72.18.16.16">b</mi><mi id="S3.E4.m1.73.73.73.19.17.17.1">j</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.74.74.74.20.18.18">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.75.75.75.21.19.19">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.109.109.4j"><mtd class="ltx_align_right" columnalign="right" id="S3.E4.m1.109.109.4k"><msub id="S3.E4.m1.77.77.77.2.2a"><mi id="S3.E4.m1.76.76.76.1.1.1">V</mi><mi id="S3.E4.m1.77.77.77.2.2.2.1">c</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.109.109.4l"><mrow id="S3.E4.m1.109.109.4.109.31.29.29"><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1"><mi id="S3.E4.m1.109.109.4.109.31.29.29.1.2"></mi><mo id="S3.E4.m1.78.78.78.3.1.1">=</mo><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1"><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.2"><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.2.1"><mi id="S3.E4.m1.79.79.79.4.2.2">A</mi><mi id="S3.E4.m1.80.80.80.5.3.3.1">f</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.81.81.81.6.4.4">∗</mo><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.2.2"><mi id="S3.E4.m1.82.82.82.7.5.5">V</mi><mrow id="S3.E4.m1.83.83.83.8.6.6.1"><mi id="S3.E4.m1.83.83.83.8.6.6.1.2">c</mi><mo id="S3.E4.m1.83.83.83.8.6.6.1.1">−</mo><mn id="S3.E4.m1.83.83.83.8.6.6.1.3">1</mn></mrow></msub></mrow><mo id="S3.E4.m1.84.84.84.9.7.7">+</mo><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1"><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.3"><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.3.1"><mi id="S3.E4.m1.85.85.85.10.8.8">A</mi><mi id="S3.E4.m1.86.86.86.11.9.9.1">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.87.87.87.12.10.10">∗</mo><mtext id="S3.E4.m1.88.88.88.13.11.11">cell_state</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.2">​</mo><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1"><mo stretchy="false" id="S3.E4.m1.89.89.89.14.12.12">(</mo><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1"><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.1"><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.1.1"><mi id="S3.E4.m1.90.90.90.15.13.13">W</mi><mi id="S3.E4.m1.91.91.91.16.14.14.1">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.92.92.92.17.15.15">∗</mo><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.1.2"><mi id="S3.E4.m1.93.93.93.18.16.16">L</mi><mi id="S3.E4.m1.94.94.94.19.17.17.1">i</mi></msub></mrow><mo id="S3.E4.m1.95.95.95.20.18.18">+</mo><mrow id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.2"><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.2.1"><mi id="S3.E4.m1.96.96.96.21.19.19">U</mi><mi id="S3.E4.m1.97.97.97.22.20.20.1">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.98.98.98.23.21.21">∗</mo><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.2.2"><mi id="S3.E4.m1.99.99.99.24.22.22">L</mi><mrow id="S3.E4.m1.100.100.100.25.23.23.1"><mi id="S3.E4.m1.100.100.100.25.23.23.1.2">j</mi><mo id="S3.E4.m1.100.100.100.25.23.23.1.1">−</mo><mn id="S3.E4.m1.100.100.100.25.23.23.1.3">1</mn></mrow></msub></mrow><mo id="S3.E4.m1.95.95.95.20.18.18a">+</mo><msub id="S3.E4.m1.109.109.4.109.31.29.29.1.1.1.1.1.1.3"><mi id="S3.E4.m1.102.102.102.27.25.25">b</mi><mi id="S3.E4.m1.103.103.103.28.26.26.1">c</mi></msub></mrow><mo stretchy="false" id="S3.E4.m1.104.104.104.29.27.27">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E4.m1.105.105.105.30.28.28">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S3.E4.m1.109b">\displaystyle\begin{split}A_{f}&amp;=\sigma(W_{f}*L_{i}+U_{f}*L_{j-1}+b_{f}),\quad A_{i}=\sigma(W_{i}*L_{i}+U_{i}*L_{j-1}+b_{i}),\\
L_{j}&amp;=A_{j}\odot\tanh(V_{c}),\\
A_{j}&amp;=\sigma(W_{0}*L_{i}+U_{0}*L_{j-1}+b_{j}),\\
V_{c}&amp;=A_{f}*V_{c-1}+A_{i}*\text{cell\_state}(W_{c}*L_{i}+U_{c}*L_{j-1}+b_{c}).\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p7" class="ltx_para">
<p id="S3.SS5.p7.1" class="ltx_p">Recently, several schemes for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> and utilizing LSTM have been proposed in the literature. The study described by Lu. et al. in 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> introduces a speech training system designed for individuals with hearing impairments, such as those with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, as well as individuals with dysphonia, utilizing automated lip-reading recognition. The system combines <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> and <a href="#Sx1.95.95.95"><abbr href="#Sx1.95.95.95" title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RNN</span></span></abbr></a> to compare mouth shapes and train speech skills. It includes a speech training database, automatic lip-reading using a hybrid neural network, matching lip shapes with sign language vocabulary, and drawing comparison data. The system enables hearing-impaired individuals to analyze and improve their vocal lip shapes independently. It also supports the use of medical devices for correct pronunciation. Experimental results demonstrate the system’s effectiveness in correcting lip shape and enhancing speech ability. The proposed model utilizes ResNet50, MobileNet, and <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> networks for accurate lip-reading recognition. Later on, the scientific paper published by Chu et al. in 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> proposes a causal <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> framework for classifying phonemes in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> to enhance speech intelligibility. The authors trained <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> networks using features extracted at the time-frequency resolution of a <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> processor. They compared <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>-inspired features (log STFT power spectrum, log <a href="#Sx1.28.28.28"><abbr href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE</span></span></abbr></a> power spectrum, and log-mel-filterbank) with traditional <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> features. The results showed that <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>-inspired features outperformed traditional features, providing slightly higher levels of performance. The author claimed that, this study is the first to introduce a classification framework with the potential to categorize phonetic units in real-time in a <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, offering possibilities for improving speech recognition in reverberant environments for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users. In 2023, Huang et al. proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based sound coding strategy for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, called ElectrodeNet. By leveraging <a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a>, <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a>, and <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a>, ElectrodeNet replaces conventional envelope detection in the <a href="#Sx1.28.28.28"><abbr href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE</span></span></abbr></a> strategy. Objective evaluations using measures like <a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a> and <a href="#Sx1.33.33.33"><abbr href="#Sx1.33.33.33" title="normalized covariance measure" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NCM</span></span></abbr></a> demonstrate strong correlations between ElectrodeNet and <a href="#Sx1.28.28.28"><abbr href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE</span></span></abbr></a>. Additionally, subjective tests with normal-hearing listeners confirm the effectiveness of ElectrodeNet in sentence recognition for vocoded Mandarin speech. The study extends ElectrodeNet with ElectrodeNet-CS, incorporating  <a href="#Sx1.51.51.51"><span href="#Sx1.51.51.51" title="channel selection" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">channel selection</span></span></span></a> (<a href="#Sx1.51.51.51"><abbr href="#Sx1.51.51.51" title="channel selection" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CS</span></span></abbr></a>) through a modified <a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a> network. ElectrodeNet-CS produces N-of-M compatible electrode patterns and performs comparably or slightly better than <a href="#Sx1.28.28.28"><abbr href="#Sx1.28.28.28" title="advanced combination encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE</span></span></abbr></a> in terms of <a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a> and sentence recognition. This research showcases the feasibility and potential of deep learning in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> coding strategies, paving the way for future advancements in <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-powered <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> systems. Similarly, the research presented by Jeyalakshmi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> focuses on predicting <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> scores for children aged 5 to 10 using a reconfigured <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> network as illustrated in Figure <a href="#S3.F9" title="Figure 9 ‣ 3.5 /-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The proposed architecture aims to enhance language development skills in children with auditory deprivation, this could be achieved by guiding <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> programming through the analysis of cross-modal data obtained from previously programmed patients. The research utilizes visual cross-modal plasticity and visual evoked potential to discover patterns in the data that can predict outcomes for future patients. The proposed methodology involves the use of <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> network and <a href="#Sx1.75.75.75"><abbr href="#Sx1.75.75.75" title=" enhanced swarm based crow search optimization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ESCSO</span></span></abbr></a> to identify optimal weights. The results demonstrate the superiority of the <a href="#Sx1.75.75.75"><abbr href="#Sx1.75.75.75" title=" enhanced swarm based crow search optimization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ESCSO</span></span></abbr></a>-based <a href="#Sx1.48.48.48"><abbr href="#Sx1.48.48.48" title="long short term memory" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">LSTM</span></span></abbr></a> technique over other methods.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2403.15442/assets/x8.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="208" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;"> The design of the LSTM architecture suggested by Jeyalakshmi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. </span></figcaption>
</figure>
<div id="S3.SS5.p8" class="ltx_para">
<p id="S3.SS5.p8.3" class="ltx_p">In Figure <a href="#S3.F9" title="Figure 9 ‣ 3.5 /-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, "oz," "cz," "t5," and "t6" refer to specific electrode placements or positions on the scalp in the international 10-20 system for  <a href="#Sx1.76.76.76"><span href="#Sx1.76.76.76" title="electroencephalography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">electroencephalography</span></span></span></a> (<a href="#Sx1.76.76.76"><abbr href="#Sx1.76.76.76" title="electroencephalography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EEG</span></span></abbr></a>) or  <a href="#Sx1.77.77.77"><span href="#Sx1.77.77.77" title="event-related potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">event-related potential</span></span></span></a> (<a href="#Sx1.77.77.77"><abbr href="#Sx1.77.77.77" title="event-related potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ERP</span></span></abbr></a>) recordings. These positions represent specific areas on the scalp where electrodes are attached to measure electrical activity in the brain. The amplitude represents the intensity or strength of the electrical signal detected at a particular point on the scalp, reflecting the neural activity in the corresponding brain region. The parameters "N75," "P100," and "N145" refer to specific components or peaks of <a href="#Sx1.77.77.77"><abbr href="#Sx1.77.77.77" title="event-related potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">ERP</span>s</span></abbr></a> obtained from <a href="#Sx1.76.76.76"><abbr href="#Sx1.76.76.76" title="electroencephalography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EEG</span></span></abbr></a> recordings. <a href="#Sx1.77.77.77"><abbr href="#Sx1.77.77.77" title="event-related potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">ERP</span>s</span></abbr></a> are electrical responses recorded from the brain in response to specific stimuli or events, and they reflect the neural processing associated with those stimuli. Besides, <math id="S3.SS5.p8.1.m1.1" class="ltx_Math" alttext="I/P" display="inline"><semantics id="S3.SS5.p8.1.m1.1a"><mrow id="S3.SS5.p8.1.m1.1.1" xref="S3.SS5.p8.1.m1.1.1.cmml"><mi id="S3.SS5.p8.1.m1.1.1.2" xref="S3.SS5.p8.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS5.p8.1.m1.1.1.1" xref="S3.SS5.p8.1.m1.1.1.1.cmml">/</mo><mi id="S3.SS5.p8.1.m1.1.1.3" xref="S3.SS5.p8.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p8.1.m1.1b"><apply id="S3.SS5.p8.1.m1.1.1.cmml" xref="S3.SS5.p8.1.m1.1.1"><divide id="S3.SS5.p8.1.m1.1.1.1.cmml" xref="S3.SS5.p8.1.m1.1.1.1"></divide><ci id="S3.SS5.p8.1.m1.1.1.2.cmml" xref="S3.SS5.p8.1.m1.1.1.2">𝐼</ci><ci id="S3.SS5.p8.1.m1.1.1.3.cmml" xref="S3.SS5.p8.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p8.1.m1.1c">I/P</annotation></semantics></math> represent inputs, <math id="S3.SS5.p8.2.m2.1" class="ltx_Math" alttext="O/P" display="inline"><semantics id="S3.SS5.p8.2.m2.1a"><mrow id="S3.SS5.p8.2.m2.1.1" xref="S3.SS5.p8.2.m2.1.1.cmml"><mi id="S3.SS5.p8.2.m2.1.1.2" xref="S3.SS5.p8.2.m2.1.1.2.cmml">O</mi><mo id="S3.SS5.p8.2.m2.1.1.1" xref="S3.SS5.p8.2.m2.1.1.1.cmml">/</mo><mi id="S3.SS5.p8.2.m2.1.1.3" xref="S3.SS5.p8.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p8.2.m2.1b"><apply id="S3.SS5.p8.2.m2.1.1.cmml" xref="S3.SS5.p8.2.m2.1.1"><divide id="S3.SS5.p8.2.m2.1.1.1.cmml" xref="S3.SS5.p8.2.m2.1.1.1"></divide><ci id="S3.SS5.p8.2.m2.1.1.2.cmml" xref="S3.SS5.p8.2.m2.1.1.2">𝑂</ci><ci id="S3.SS5.p8.2.m2.1.1.3.cmml" xref="S3.SS5.p8.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p8.2.m2.1c">O/P</annotation></semantics></math> for output, and <math id="S3.SS5.p8.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS5.p8.3.m3.1a"><mi id="S3.SS5.p8.3.m3.1.1" xref="S3.SS5.p8.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p8.3.m3.1b"><ci id="S3.SS5.p8.3.m3.1.1.cmml" xref="S3.SS5.p8.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p8.3.m3.1c">W</annotation></semantics></math> represent Weights.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span><a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a>-based methods</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.6" class="ltx_p">An  <a href="#Sx1.43.43.43"><span href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">autoencoder</span></span></span></a> (<a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a>) is a type of neural network designed for unsupervised learning, tasked with encoding input data into a compressed representation and decoding it back to the original form. Examples include <a href="#Sx1.41.41.41"><span href="#Sx1.41.41.41" title="variational autoencoders" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">variational autoencoders</span>s</span></span></a>, which balance data compression with generative modeling, <a href="#Sx1.42.42.42"><span href="#Sx1.42.42.42" title="convolutional autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">convolutional autoencoder</span>s</span></span></a>, which employ convolutional layers for efficient feature learning and reconstruction, and sparse <a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">AE</span>s</span></abbr></a>, which induce sparsity, promoting selectivity in feature representation, among others. The encoding equation typically involves a mapping function, such as <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="h=f(x)" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mrow id="S3.SS6.p1.1.m1.1.2" xref="S3.SS6.p1.1.m1.1.2.cmml"><mi id="S3.SS6.p1.1.m1.1.2.2" xref="S3.SS6.p1.1.m1.1.2.2.cmml">h</mi><mo id="S3.SS6.p1.1.m1.1.2.1" xref="S3.SS6.p1.1.m1.1.2.1.cmml">=</mo><mrow id="S3.SS6.p1.1.m1.1.2.3" xref="S3.SS6.p1.1.m1.1.2.3.cmml"><mi id="S3.SS6.p1.1.m1.1.2.3.2" xref="S3.SS6.p1.1.m1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.1.m1.1.2.3.1" xref="S3.SS6.p1.1.m1.1.2.3.1.cmml">​</mo><mrow id="S3.SS6.p1.1.m1.1.2.3.3.2" xref="S3.SS6.p1.1.m1.1.2.3.cmml"><mo stretchy="false" id="S3.SS6.p1.1.m1.1.2.3.3.2.1" xref="S3.SS6.p1.1.m1.1.2.3.cmml">(</mo><mi id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS6.p1.1.m1.1.2.3.3.2.2" xref="S3.SS6.p1.1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.2"><eq id="S3.SS6.p1.1.m1.1.2.1.cmml" xref="S3.SS6.p1.1.m1.1.2.1"></eq><ci id="S3.SS6.p1.1.m1.1.2.2.cmml" xref="S3.SS6.p1.1.m1.1.2.2">ℎ</ci><apply id="S3.SS6.p1.1.m1.1.2.3.cmml" xref="S3.SS6.p1.1.m1.1.2.3"><times id="S3.SS6.p1.1.m1.1.2.3.1.cmml" xref="S3.SS6.p1.1.m1.1.2.3.1"></times><ci id="S3.SS6.p1.1.m1.1.2.3.2.cmml" xref="S3.SS6.p1.1.m1.1.2.3.2">𝑓</ci><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">h=f(x)</annotation></semantics></math>, where <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">h</annotation></semantics></math> is the encoded representation and <math id="S3.SS6.p1.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS6.p1.3.m3.1a"><mi id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><ci id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">x</annotation></semantics></math> is the input. The decoding equation is the reconstruction of the input, often expressed as <math id="S3.SS6.p1.4.m4.1" class="ltx_Math" alttext="r=g(h)" display="inline"><semantics id="S3.SS6.p1.4.m4.1a"><mrow id="S3.SS6.p1.4.m4.1.2" xref="S3.SS6.p1.4.m4.1.2.cmml"><mi id="S3.SS6.p1.4.m4.1.2.2" xref="S3.SS6.p1.4.m4.1.2.2.cmml">r</mi><mo id="S3.SS6.p1.4.m4.1.2.1" xref="S3.SS6.p1.4.m4.1.2.1.cmml">=</mo><mrow id="S3.SS6.p1.4.m4.1.2.3" xref="S3.SS6.p1.4.m4.1.2.3.cmml"><mi id="S3.SS6.p1.4.m4.1.2.3.2" xref="S3.SS6.p1.4.m4.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p1.4.m4.1.2.3.1" xref="S3.SS6.p1.4.m4.1.2.3.1.cmml">​</mo><mrow id="S3.SS6.p1.4.m4.1.2.3.3.2" xref="S3.SS6.p1.4.m4.1.2.3.cmml"><mo stretchy="false" id="S3.SS6.p1.4.m4.1.2.3.3.2.1" xref="S3.SS6.p1.4.m4.1.2.3.cmml">(</mo><mi id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml">h</mi><mo stretchy="false" id="S3.SS6.p1.4.m4.1.2.3.3.2.2" xref="S3.SS6.p1.4.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><apply id="S3.SS6.p1.4.m4.1.2.cmml" xref="S3.SS6.p1.4.m4.1.2"><eq id="S3.SS6.p1.4.m4.1.2.1.cmml" xref="S3.SS6.p1.4.m4.1.2.1"></eq><ci id="S3.SS6.p1.4.m4.1.2.2.cmml" xref="S3.SS6.p1.4.m4.1.2.2">𝑟</ci><apply id="S3.SS6.p1.4.m4.1.2.3.cmml" xref="S3.SS6.p1.4.m4.1.2.3"><times id="S3.SS6.p1.4.m4.1.2.3.1.cmml" xref="S3.SS6.p1.4.m4.1.2.3.1"></times><ci id="S3.SS6.p1.4.m4.1.2.3.2.cmml" xref="S3.SS6.p1.4.m4.1.2.3.2">𝑔</ci><ci id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">r=g(h)</annotation></semantics></math>, where <math id="S3.SS6.p1.5.m5.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS6.p1.5.m5.1a"><mi id="S3.SS6.p1.5.m5.1.1" xref="S3.SS6.p1.5.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m5.1b"><ci id="S3.SS6.p1.5.m5.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m5.1c">r</annotation></semantics></math> is the reconstructed output and <math id="S3.SS6.p1.6.m6.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS6.p1.6.m6.1a"><mi id="S3.SS6.p1.6.m6.1.1" xref="S3.SS6.p1.6.m6.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m6.1b"><ci id="S3.SS6.p1.6.m6.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m6.1c">g</annotation></semantics></math> is the decoding function. <a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">AE</span>s</span></abbr></a> find applications in data compression, denoising, feature learning, and more. Recently, many research papers for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> have been proposed that are based on <a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">As a point of the case, the scientific paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> delves into the pivotal objective of enhancing speech perception for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users in noisy conditions, recognizing the critical role of  <a href="#Sx1.63.63.63"><span href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">noise reduction</span></span></span></a> (<a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a>) in this pursuit. The proposed method, named <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>-<a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a>, has been proven effective in restoring clean speech. The study focuses on evaluating the <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>-based <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> using envelope-based vocoded speech, mimicking <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> devices.
The procedure of <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>-based <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> can be split into two main stages as shown in Figure <a href="#S3.F10" title="Figure 10 ‣ 3.6 -based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>: training and testing. During the training phase, a collection of pairs of noisy and clean speech signals is prepared. These signals are initially transformed into the frequency domain using an  <a href="#Sx1.27.27.27"><span href="#Sx1.27.27.27" title="fast Fourier transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">fast Fourier transform</span></span></span></a> (<a href="#Sx1.27.27.27"><abbr href="#Sx1.27.27.27" title="fast Fourier transform" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FFT</span></span></abbr></a>). The logarithmic amplitudes of the noisy and clean speech spectra are then used as inputs and outputs, respectively, for the <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a> model.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">Key findings underscore the superior intelligibility of <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>-based <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> in vocoded speech compared to  <a href="#Sx1.44.44.44"><span href="#Sx1.44.44.44" title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">state-of-the-art</span></span></span></a> (<a href="#Sx1.44.44.44"><abbr href="#Sx1.44.44.44" title="state-of-the-art" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SOTA</span></span></abbr></a>) conventional methods, indicating its potential implementation in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> speech processors. However, the study acknowledges the use of noise-vocoded speech simulation for evaluation and emphasizes the need for further validation with real <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients in clinical settings, addressing potential inconsistencies in the transition to actual <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> devices.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2403.15442/assets/x9.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="251" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.3.2" class="ltx_text" style="font-size:90%;"> The configuration of a <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>-based <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> system for enhancing speech perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">A zero-delay  <a href="#Sx1.57.57.57"><span href="#Sx1.57.57.57" title="deep autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep autoencoder</span></span></span></a> (<a href="#Sx1.57.57.57"><abbr href="#Sx1.57.57.57" title="deep autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DAE</span></span></abbr></a>) is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for compressing and transmitting electrical stimulation patterns generated by <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. The goal is to conserve battery power in wireless transmission while maintaining low latency, which is crucial for speech perception in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users. The <a href="#Sx1.57.57.57"><abbr href="#Sx1.57.57.57" title="deep autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DAE</span></span></abbr></a> architecture is optimized using Bayesian optimization and the <a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a>. The results show that the proposed <a href="#Sx1.57.57.57"><abbr href="#Sx1.57.57.57" title="deep autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DAE</span></span></abbr></a> achieves equal or superior speech understanding compared to audio codecs, with reference vocoder <a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a> scores at 13.5 kbit/s. This approach offers a promising solution for efficient and real-time compression of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> stimulation patterns, addressing the constraints of low latency and battery power consumption. Moreover, The research in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> focuses on achieving accurate segmentation of the vestibule in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, a crucial step for clinical diagnosis of congenital ear malformations and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. The challenges addressed include the small size and irregular shape of the vestibule, making segmentation difficult, and the limited availability of labelled samples due to high labour costs. To overcome these challenges, the proposed method introduces a vestibule segmentation network within a basic encoder-decoder framework. Key innovations include the incorporation of a  <a href="#Sx1.64.64.64"><span href="#Sx1.64.64.64" title="residual channel attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">residual channel attention</span></span></span></a> (<a href="#Sx1.64.64.64"><abbr href="#Sx1.64.64.64" title="residual channel attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Res-CA</span></span></abbr></a>) block for channel attention, a  <a href="#Sx1.65.65.65"><span href="#Sx1.65.65.65" title="global context-aware pyramid feature extraction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">global context-aware pyramid feature extraction</span></span></span></a> (<a href="#Sx1.65.65.65"><abbr href="#Sx1.65.65.65" title="global context-aware pyramid feature extraction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GCPFE</span></span></abbr></a>) module for global context information, an ,  <a href="#Sx1.66.66.66"><span href="#Sx1.66.66.66" title="active contour with elastic loss" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">active contour with elastic loss</span></span></span></a> (<a href="#Sx1.66.66.66"><abbr href="#Sx1.66.66.66" title="active contour with elastic loss" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE-Loss</span></span></abbr></a>) function for detailed boundary learning, and a  <a href="#Sx1.67.67.67"><span href="#Sx1.67.67.67" title="deep supervision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep supervision</span></span></span></a> (<a href="#Sx1.67.67.67"><abbr href="#Sx1.67.67.67" title="deep supervision" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DS</span></span></abbr></a>) mechanism to enhance network robustness. The network architecture utilizes ResNet34 as the backbone with skip connections for multi-level feature fusion. Results showcases a high performance, and are supported by comprehensive comparisons, ablation studies, and visualized segmentation outcomes. The study also acknowledges limitations, such as reliance on professional annotations.</p>
</div>
<div id="S3.SS6.p5" class="ltx_para">
<p id="S3.SS6.p5.1" class="ltx_p">In addition, the study presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> aims to enhance the accuracy and robustness of  <a href="#Sx1.58.58.58"><span href="#Sx1.58.58.58" title="intra cochlear anatomy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">intra cochlear anatomy</span></span></span></a> (<a href="#Sx1.58.58.58"><abbr href="#Sx1.58.58.58" title="intra cochlear anatomy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ICA</span></span></abbr></a>) segmentation, a vital component in preoperative decisions, insertion planning, and postoperative adjustments for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> procedures. The <a href="#Sx1.58.58.58"><abbr href="#Sx1.58.58.58" title="intra cochlear anatomy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ICA</span></span></abbr></a> includes structures such as  <a href="#Sx1.59.59.59"><span href="#Sx1.59.59.59" title="scala tympani" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">scala tympani</span></span></span></a> (<a href="#Sx1.59.59.59"><abbr href="#Sx1.59.59.59" title="scala tympani" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ST</span></span></abbr></a>),  <a href="#Sx1.60.60.60"><span href="#Sx1.60.60.60" title="scala vestibul" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">scala vestibul</span></span></span></a> (<a href="#Sx1.60.60.60"><abbr href="#Sx1.60.60.60" title="scala vestibul" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SV</span></span></abbr></a>), and the  <a href="#Sx1.61.61.61"><span href="#Sx1.61.61.61" title="active region" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">active region</span></span></span></a> (<a href="#Sx1.61.61.61"><abbr href="#Sx1.61.61.61" title="active region" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AR</span></span></abbr></a>). The researchers employed two segmentation methods,  <a href="#Sx1.62.62.62"><span href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">active shape model</span></span></span></a> (<a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a>) and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> based on 3D U-Net <a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a>, and combined them to achieve improved accuracy and robustness. A two-level training strategy involved pretraining on clinical <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> using <a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a> and fine-tuning on specimens’ <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CT</span>s</span></abbr></a> with ground truth. Results demonstrated that <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> methods outperformed <a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a> in accuracy. While a trade-off between accuracy and robustness was observed, the combined <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> and <a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a> approach showed improvements in both aspects. The study concludes that the proposed <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> and <a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a> method effectively balances accuracy and robustness for <a href="#Sx1.58.58.58"><abbr href="#Sx1.58.58.58" title="intra cochlear anatomy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ICA</span></span></abbr></a> segmentation, highlighting the potential of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based methods, especially when integrated with <a href="#Sx1.62.62.62"><abbr href="#Sx1.62.62.62" title="active shape model" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASM</span></span></abbr></a>, to enhance <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> procedures.</p>
</div>
<div id="S3.SS6.p6" class="ltx_para">
<p id="S3.SS6.p6.1" class="ltx_p">The proposed  <a href="#Sx1.30.30.30"><span href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">min-max similarity</span></span></span></a> (<a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a>) methodology in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> represents a groundbreaking approach to semi-supervised segmentation networks, particularly in the context of medical applications such as endoscopy surgical tool segmentation and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> surgery. <a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a> is introduced through dual-view training with contrastive learning, utilizing classifiers and projectors to create negative, positive, and negative pairs. The inclusion of pixel-wise contrastive loss ensures the consistency of unlabeled predictions. In the evaluation phase, <a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a> was tested on four public endoscopy surgical tool segmentation datasets and a manually annotated <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> surgery dataset. The results demonstrate its superiority over state-of-the-art semi-supervised and fully supervised segmentation algorithms, both quantitatively and qualitatively. Notably, <a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a> exhibited successful recognition of unknown surgical tools, providing reliable predictions, and achieved real-time video segmentation with an impressive inference speed of about 40 frames per second. This signifies the potential of <a href="#Sx1.30.30.30"><abbr href="#Sx1.30.30.30" title="min-max similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MMS</span></span></abbr></a> as a highly effective and efficient tool in medical image segmentation, showcasing its applicability in real-world surgical scenarios.</p>
</div>
<div id="S3.SS6.p7" class="ltx_para">
<p id="S3.SS6.p7.1" class="ltx_p">Similarly, The primary aim of the study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> is to devise an automated method for the segmentation and measurement of the human cochlea in  <a href="#Sx1.68.68.68"><span href="#Sx1.68.68.68" title="ultra-high-resolution" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">ultra-high-resolution</span></span></span></a> (<a href="#Sx1.68.68.68"><abbr href="#Sx1.68.68.68" title="ultra-high-resolution" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">UHR</span></span></abbr></a>) <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a>-images. The objective is to explore variations in cochlear size to enhance outcomes in cochlear surgery through personalized implant planning. Initially, the input scans undergo a two-step process using a detection module and a pixel-wise classification module for cochlea localization and segmentation, respectively using an <a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a> as illustrated in Figure <a href="#S3.F11" title="Figure 11 ‣ 3.6 -based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. The detection module reduces the search area for the classification module, improving algorithm speed and reducing false positives. Both modules are trained on image patches, allowing for a larger training set size by generating multiple examples from each scan. The segmented cochlear structure then proceeds to a final module that combines <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> and thinning algorithms to extract patient-specific anatomical measurements. <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> is employed in each step to leverage its ability to learn directly from input data, providing automatic results without the need for user-adjustable parameters during testing.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2403.15442/assets/x10.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="338" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S3.F11.3.2" class="ltx_text" style="font-size:90%;"> Encoder-decoder network used in the pixel-wise classification model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>.</span></figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>RL-based methods </h3>

<div id="S3.SS7.p1" class="ltx_para">
<span id="S3.SS7.p1.1" class="ltx_ERROR undefined">\Ac</span>
<p id="S3.SS7.p1.2" class="ltx_p">DRL is a subfield of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> that enable agents to learn and make decisions in complex environments. It involves training an agent to interact with an environment, learn from the outcomes of its actions, and optimize its behavior over time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. In traditional <a href="#Sx1.107.107.107"><abbr href="#Sx1.107.107.107" title="reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RL</span></span></abbr></a>, agents learn by trial and error, receiving feedback in the form of rewards or penalties for their actions. However,  <a href="#Sx1.106.106.106"><span href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep reinforcement learning</span></span></span></a> (<a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a>) incorporates <a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a>, make it capable of learning complex patterns and representations from raw data. This allows <a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a> agents to handle high-dimensional input spaces, such as images or sensor data, and make more sophisticated decisions. Figure <a href="#S3.F12" title="Figure 12 ‣ 3.7 RL-based methods ‣ 3 Taxonomy of -based AI techniques ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> illustrate the priciple of RL.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2403.15442/assets/x11.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="196" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S3.F12.3.2" class="ltx_text" style="font-size:90%;">RL principle.</span></figcaption>
</figure>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">The paper presented by radutoiu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> presents a novel method for accurately localizing <a href="#Sx1.105.105.105"><span href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">regions of interest</span>s</span></span></a> in the inner ear using <a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a>. The proposed method addresses the challenges of robust <a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ROI</span></span></abbr></a> extraction in full head <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> scans, which is crucial for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> surgery. The approach utilizes communicative multi-agent <a href="#Sx1.107.107.107"><abbr href="#Sx1.107.107.107" title="reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">RL</span></span></abbr></a> and landmarks specifically designed to extract orientation parameters. The method achieves an average estimated error of 1.07 mm for landmark localization. The extracted <a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">ROI</span>s</span></abbr></a> demonstrate an <a href="#Sx1.38.38.38"><abbr href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IoU</span></span></abbr></a> of 0.84 and a dice similarity coefficient of 0.91, conducted over 140 full head CT scans, showing promising results for automatic <a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ROI</span></span></abbr></a> extraction in medical imaging.
In addition, Lopez et al. presents in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> a pipeline for characterizing facial and cochlear nerves in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> scans using <a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a>. Key landmarks around these nerves are located using a communicative multi-agent <a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a> model. The pipeline includes automated measurement of the cochlear nerve canal diameter, extraction and segmentation of the cochlear nerve cross-section, and path selection for the facial nerve characterization. The pipeline was developed and evaluated using 119 clinical <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images. The results show accurate characterizations of the nerves in the cochlear region, providing reliable measurements for computer-aided diagnosis and surgery planning. The proposed approach demonstrates the potential of <a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a> for landmark detection in challenging medical imaging tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Applications of DL-based medical <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section explores the application of deep learning in the field of cochlear implants, encompassing tasks such as speech denoising and enhancement, segmentation for precise identification and analysis of cochlear structures, thresholding, imaging, localization of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, and more. Figure <a href="#S4.F13" title="Figure 13 ‣ 4 Applications of DL-based medical ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> provides a comprehensive overview of AI-based applications for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> and theirs associated benefits. Furthermore, Table <a href="#S4.T5" title="Table 5 ‣ 4.1 Speech denoising and enhancement ‣ 4 Applications of DL-based medical ‣ Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes various applications based on AI techniques, highlighting their performance, and pros and cons.</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2403.15442/assets/x12.png" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S4.F13.3.2" class="ltx_text" style="font-size:90%;">Taxonomy of AI-based applications for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> and their benefits.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Speech denoising and enhancement</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The integration of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> has proven invaluable in the field of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. Researchers have harnessed these technologies to tackle numerous challenges and enhance speech perception for individuals with hearing impairments. The work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> employed <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a> approach to reduce unwanted background noise in speech signals. However, Lai et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> devised a <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> system that employed a noise classifier and <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>, specifically tailored for Mandarin-speaking <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. The proposed schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> aim to perform end-to-end speech denoising, with the goal of enhancing speech intelligibility in noisy environments. Gajecki et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> employed <a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a> to develop the Deep ACE method, while Healy et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> utilized <a href="#Sx1.50.50.50"><abbr href="#Sx1.50.50.50" title="deep neural networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DNN</span></span></abbr></a> to separate speech from background noise. These examples underscore the broad spectrum of applications of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>, particularly <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> techniques, in addressing challenges related to noise reduction and enhancing speech intelligibility in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> applications.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Moreover, Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> used <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based speech enhancement algorithms to optimize speech perception for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. Their approach achieved a balance between noise suppression and speech distortion by experimenting with different loss functions. Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> developed environment-specific noise suppression algorithms for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> using <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> techniques. They improved the processed sound by classifying and selecting envelope amplitudes based on the <a href="#Sx1.10.10.10"><abbr href="#Sx1.10.10.10" title="signal to noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SNR</span></span></abbr></a> in each channel. Banerjee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> employed online unsupervised algorithms to learn features from the speech of individuals with severe-to-profound hearing loss, aiming to enhance the audibility of speech through modified signal processing. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> developed an improved <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a> system for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> using <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, specifically <a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a>, and knowledge transfer technology. Their goal was to enhance speech intelligibility in noisy conditions. Fischer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> utilized <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based virtual sensing of head-mounted microphones to improve speech signals in cocktail party scenarios for individuals with hearing loss, resulting in enhanced speech quality and intelligibility, particularly in noisy environments. These studies exemplify the versatility of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> in addressing various challenges associated with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, including <a href="#Sx1.63.63.63"><abbr href="#Sx1.63.63.63" title="noise reduction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">NR</span></span></abbr></a>, speech enhancement, and improved speech perception. Furthermore, the paper by Chu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> explores the application of <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> algorithms to mitigate the effects of reverberation and noise in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, to improve speech intelligibility for individuals with severe hearing loss.</p>
</div>
<figure id="S4.T5" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.7.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.8.2" class="ltx_text" style="font-size:90%;">Summary of the performance and limitations of specific DL applications dedicated to <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. In cases where multiple tests are conducted, only the best performance is reported. </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T5.5" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S4.T5.5.6" class="ltx_tr">
<td id="S4.T5.5.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:14.2pt;">
<span id="S4.T5.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.1.1.1" class="ltx_p"><span id="S4.T5.5.6.1.1.1.1" class="ltx_text" style="font-size:70%;">Ref.</span></span>
</span>
</td>
<td id="S4.T5.5.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:14.2pt;">
<span id="S4.T5.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.2.1.1" class="ltx_p"><span id="S4.T5.5.6.2.1.1.1" class="ltx_text" style="font-size:70%;">Year</span></span>
</span>
</td>
<td id="S4.T5.5.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:28.5pt;">
<span id="S4.T5.5.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.3.1.1" class="ltx_p"><span id="S4.T5.5.6.3.1.1.1" class="ltx_text" style="font-size:70%;">DLM</span></span>
</span>
</td>
<td id="S4.T5.5.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:142.3pt;">
<span id="S4.T5.5.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.4.1.1" class="ltx_p"><span id="S4.T5.5.6.4.1.1.1" class="ltx_text" style="font-size:70%;">Description</span></span>
</span>
</td>
<td id="S4.T5.5.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:56.9pt;">
<span id="S4.T5.5.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.5.1.1" class="ltx_p"><span id="S4.T5.5.6.5.1.1.1" class="ltx_text" style="font-size:70%;">BP (%)</span></span>
</span>
</td>
<td id="S4.T5.5.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:156.5pt;">
<span id="S4.T5.5.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.6.1.1" class="ltx_p"><span id="S4.T5.5.6.6.1.1.1" class="ltx_text" style="font-size:70%;">Limitations</span></span>
</span>
</td>
<td id="S4.T5.5.6.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:14.2pt;">
<span id="S4.T5.5.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.6.7.1.1" class="ltx_p"><span id="S4.T5.5.6.7.1.1.1" class="ltx_text" style="font-size:70%;">PLA</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.7" class="ltx_tr">
<td id="S4.T5.5.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.7.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib82" title="" class="ltx_ref">82</a><span id="S4.T5.5.7.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.2.1.1" class="ltx_p"><span id="S4.T5.5.7.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="S4.T5.5.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.3.1.1" class="ltx_p"><span id="S4.T5.5.7.3.1.1.1" class="ltx_text" style="font-size:70%;">NC and DDAE_T</span></span>
</span>
</td>
<td id="S4.T5.5.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.4.1.1" class="ltx_p"><span id="S4.T5.5.7.4.1.1.1" class="ltx_text" style="font-size:70%;">The </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.7.4.1.1.2" class="ltx_text" style="font-size:70%;"> model comprises Siren noise at 6dB, a classifier, and the </span><a href="#Sx1.9.9.9"><abbr href="#Sx1.9.9.9" title="deep denoising auto-encoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DDAE</span></span></abbr></a><span id="S4.T5.5.7.4.1.1.3" class="ltx_text" style="font-size:70%;">. The  </span><a href="#Sx1.122.122.122"><span href="#Sx1.122.122.122" title="transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">transfer learning</span></span></span></a><span id="S4.T5.5.7.4.1.1.4" class="ltx_text" style="font-size:70%;"> (</span><a href="#Sx1.122.122.122"><abbr href="#Sx1.122.122.122" title="transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">TL</span></span></abbr></a><span id="S4.T5.5.7.4.1.1.5" class="ltx_text" style="font-size:70%;">) is incorporated to help reduce the number of parameters in the model.</span></span>
</span>
</td>
<td id="S4.T5.5.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.5.1.1" class="ltx_p"><span id="S4.T5.5.7.5.1.1.1" class="ltx_text" style="font-size:70%;">PESQ= 3.037 </span>
<br class="ltx_break"><span id="S4.T5.5.7.5.1.1.2" class="ltx_text" style="font-size:70%;">STOI= 0.843</span></span>
</span>
</td>
<td id="S4.T5.5.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.6.1.1" class="ltx_p"><span id="S4.T5.5.7.6.1.1.1" class="ltx_text" style="font-size:70%;">The performance of the </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.7.6.1.1.2" class="ltx_text" style="font-size:70%;"> model can be degraded when there is a mismatch between the training and testing data.</span></span>
</span>
</td>
<td id="S4.T5.5.7.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt ltx_border_t" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.7.7.1.1" class="ltx_p"><span id="S4.T5.5.7.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.8" class="ltx_tr">
<td id="S4.T5.5.8.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.8.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.T5.5.8.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.8.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.2.1.1" class="ltx_p"><span id="S4.T5.5.8.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="S4.T5.5.8.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.3.1.1" class="ltx_p"><span id="S4.T5.5.8.3.1.1.1" class="ltx_text" style="font-size:70%;">DeepACE</span></span>
</span>
</td>
<td id="S4.T5.5.8.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.4.1.1" class="ltx_p"><span id="S4.T5.5.8.4.1.1.1" class="ltx_text" style="font-size:70%;">Takes the raw audio signal as input and generates a denoised electrodogram, which represents the electrical stimulation patterns applied to the electrodes over time.</span></span>
</span>
</td>
<td id="S4.T5.5.8.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.5.1.1" class="ltx_p"><span id="S4.T5.5.8.5.1.1.1" class="ltx_text" style="font-size:70%;">STOI =0.807</span></span>
</span>
</td>
<td id="S4.T5.5.8.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.6.1.1" class="ltx_p"><span id="S4.T5.5.8.6.1.1.1" class="ltx_text" style="font-size:70%;">Heavy data reliance impedes performance with insufficient or biased data, posing generalization challenges. Computational complexity, interpretability issues, and vulnerability are concerns.</span></span>
</span>
</td>
<td id="S4.T5.5.8.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.8.7.1.1" class="ltx_p"><span id="S4.T5.5.8.7.1.1.1" class="ltx_text" style="font-size:70%;">Yes</span><span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a target="_blank" href="https://github.com/APGDHZ/DeepACE2.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/APGDHZ/DeepACE2.0</a></span></span></span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.9" class="ltx_tr">
<td id="S4.T5.5.9.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.9.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.T5.5.9.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.9.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.2.1.1" class="ltx_p"><span id="S4.T5.5.9.2.1.1.1" class="ltx_text" style="font-size:70%;">2021</span></span>
</span>
</td>
<td id="S4.T5.5.9.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.3.1.1" class="ltx_p"><span id="S4.T5.5.9.3.1.1.1" class="ltx_text" style="font-size:70%;">U-Net</span></span>
</span>
</td>
<td id="S4.T5.5.9.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.4.1.1" class="ltx_p"><span id="S4.T5.5.9.4.1.1.1" class="ltx_text" style="font-size:70%;">The proposed approach involves using deep virtual sensing to estimate microphone signals, improving speech quality in cocktail party scenarios for hearing aid and </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S4.T5.5.9.4.1.1.2" class="ltx_text" style="font-size:70%;"> users.</span></span>
</span>
</td>
<td id="S4.T5.5.9.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.5.1.1" class="ltx_p"><a href="#Sx1.54.54.54"><abbr href="#Sx1.54.54.54" title=" perceptual estimation of speech quality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">PESQ</span></span></abbr></a><span id="S4.T5.5.9.5.1.1.1" class="ltx_text" style="font-size:70%;">=2, </span><a href="#Sx1.121.121.121"><abbr href="#Sx1.121.121.121" title="short-time objective intelligibility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">STOI</span></span></abbr></a><span id="S4.T5.5.9.5.1.1.2" class="ltx_text" style="font-size:70%;">=0.53, SI-</span><a href="#Sx1.88.88.88"><abbr href="#Sx1.88.88.88" title="source-to-distortion ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">SDR</span></span></abbr></a><span id="S4.T5.5.9.5.1.1.3" class="ltx_text" style="font-size:70%;">=-28.91dB</span></span>
</span>
</td>
<td id="S4.T5.5.9.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.6.1.1" class="ltx_p"><span id="S4.T5.5.9.6.1.1.1" class="ltx_text" style="font-size:70%;">Challenges include long delay impracticality for hearing aids, binaural cues omission, insufficient network optimization, and exploring robustness in high-reverberation environments.</span></span>
</span>
</td>
<td id="S4.T5.5.9.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.9.7.1.1" class="ltx_p"><span id="S4.T5.5.9.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<td id="S4.T5.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.3.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.2.2.3.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib83" title="" class="ltx_ref">83</a><span id="S4.T5.2.2.3.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.4.1.1" class="ltx_p"><span id="S4.T5.2.2.4.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="S4.T5.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.5.1.1" class="ltx_p"><a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a></span>
</span>
</td>
<td id="S4.T5.2.2.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.6.1.1" class="ltx_p"><span id="S4.T5.2.2.6.1.1.1" class="ltx_text" style="font-size:70%;">A web-based research platform called Nautilus, which utilizes </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.2.2.6.1.1.2" class="ltx_text" style="font-size:70%;"> techniques for automated image processing in cochlear implantation-related studies.</span></span>
</span>
</td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.2.2.2" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S4.T5.2.2.2.2.2.1" class="ltx_text" style="font-size:70%;">= 80 </span><math id="S4.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm 3" display="inline"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.1.1.1.1.1.m1.1.1a" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T5.1.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.1.m1.1.1.2.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.T5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">\pm 3</annotation></semantics></math><span id="S4.T5.2.2.2.2.2.2" class="ltx_text" style="font-size:70%;">%, </span><a href="#Sx1.35.35.35"><abbr href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASD</span></span></abbr></a><span id="S4.T5.2.2.2.2.2.3" class="ltx_text" style="font-size:70%;">= 0.19 </span><math id="S4.T5.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\pm 0.04" display="inline"><semantics id="S4.T5.2.2.2.2.2.m2.1a"><mrow id="S4.T5.2.2.2.2.2.m2.1.1" xref="S4.T5.2.2.2.2.2.m2.1.1.cmml"><mo mathsize="70%" id="S4.T5.2.2.2.2.2.m2.1.1a" xref="S4.T5.2.2.2.2.2.m2.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T5.2.2.2.2.2.m2.1.1.2" xref="S4.T5.2.2.2.2.2.m2.1.1.2.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.2.m2.1b"><apply id="S4.T5.2.2.2.2.2.m2.1.1.cmml" xref="S4.T5.2.2.2.2.2.m2.1.1"><csymbol cd="latexml" id="S4.T5.2.2.2.2.2.m2.1.1.1.cmml" xref="S4.T5.2.2.2.2.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T5.2.2.2.2.2.m2.1.1.2.cmml" xref="S4.T5.2.2.2.2.2.m2.1.1.2">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.2.m2.1c">\pm 0.04</annotation></semantics></math><span id="S4.T5.2.2.2.2.2.4" class="ltx_text" style="font-size:70%;">mm</span></span>
</span>
</td>
<td id="S4.T5.2.2.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.7.1.1" class="ltx_p"><span id="S4.T5.2.2.7.1.1.1" class="ltx_text" style="font-size:70%;">Dependence on large amounts of annotated training data poses a challenge. Ensuring generalizability across various imaging conditions and populations is crucial.</span></span>
</span>
</td>
<td id="S4.T5.2.2.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.8.1.1" class="ltx_p"><span id="S4.T5.2.2.8.1.1.1" class="ltx_text" style="font-size:70%;">Yes</span><span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a href="nautilus_info@oticonmedical.com" title="" class="ltx_ref ltx_href">nautilus_info@oticonmedical.com</a> <span id="footnote16.1" class="ltx_text ltx_font_italic">available upon reasonable request</span></span></span></span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.3.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib60" title="" class="ltx_ref">60</a><span id="S4.T5.3.3.2.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.3.1.1" class="ltx_p"><span id="S4.T5.3.3.3.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.4.1.1" class="ltx_p"><a href="#Sx1.43.43.43"><abbr href="#Sx1.43.43.43" title="autoencoder" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AE</span></span></abbr></a><span id="S4.T5.3.3.4.1.1.1" class="ltx_text" style="font-size:70%;">+</span><a href="#Sx1.64.64.64"><abbr href="#Sx1.64.64.64" title="residual channel attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Res-CA</span></span></abbr></a></span>
</span>
</td>
<td id="S4.T5.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.5.1.1" class="ltx_p"><span id="S4.T5.3.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Is a vestibule segmentation network for CT images. It is based on the basic encoder-decoder framework and incorporates </span><a href="#Sx1.64.64.64"><abbr href="#Sx1.64.64.64" title="residual channel attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Res-CA</span></span></abbr></a><span id="S4.T5.3.3.5.1.1.2" class="ltx_text" style="font-size:70%;"> Block, </span><a href="#Sx1.65.65.65"><abbr href="#Sx1.65.65.65" title="global context-aware pyramid feature extraction" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GCPFE</span></span></abbr></a><span id="S4.T5.3.3.5.1.1.3" class="ltx_text" style="font-size:70%;"> and </span><a href="#Sx1.66.66.66"><abbr href="#Sx1.66.66.66" title="active contour with elastic loss" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ACE-Loss</span></span></abbr></a><span id="S4.T5.3.3.5.1.1.4" class="ltx_text" style="font-size:70%;"> Function</span></span>
</span>
</td>
<td id="S4.T5.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.1.1.1" class="ltx_p"><a href="#Sx1.34.34.34"><abbr href="#Sx1.34.34.34" title="dice coefficient similarity" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DCS</span></span></abbr></a><span id="S4.T5.3.3.1.1.1.1" class="ltx_text" style="font-size:70%;">=94.77 </span><math id="S4.T5.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\pm 2.45" display="inline"><semantics id="S4.T5.3.3.1.1.1.m1.1a"><mrow id="S4.T5.3.3.1.1.1.m1.1.1" xref="S4.T5.3.3.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.3.3.1.1.1.m1.1.1a" xref="S4.T5.3.3.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T5.3.3.1.1.1.m1.1.1.2" xref="S4.T5.3.3.1.1.1.m1.1.1.2.cmml">2.45</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.1.1.1.m1.1b"><apply id="S4.T5.3.3.1.1.1.m1.1.1.cmml" xref="S4.T5.3.3.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.3.3.1.1.1.m1.1.1.1.cmml" xref="S4.T5.3.3.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T5.3.3.1.1.1.m1.1.1.2.cmml" xref="S4.T5.3.3.1.1.1.m1.1.1.2">2.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.1.1.1.m1.1c">\pm 2.45</annotation></semantics></math><span id="S4.T5.3.3.1.1.1.2" class="ltx_text" style="font-size:70%;">%, </span><a href="#Sx1.35.35.35"><abbr href="#Sx1.35.35.35" title="average surface distance" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASD</span></span></abbr></a><span id="S4.T5.3.3.1.1.1.3" class="ltx_text" style="font-size:70%;">=0.06 mm</span></span>
</span>
</td>
<td id="S4.T5.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.6.1.1" class="ltx_p"><span id="S4.T5.3.3.6.1.1.1" class="ltx_text" style="font-size:70%;">Limited training data, small object handling, generalization capability, and computational resource constraints are areas of concern.</span></span>
</span>
</td>
<td id="S4.T5.3.3.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.7.1.1" class="ltx_p"><span id="S4.T5.3.3.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.10" class="ltx_tr">
<td id="S4.T5.5.10.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.10.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib84" title="" class="ltx_ref">84</a><span id="S4.T5.5.10.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.10.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.2.1.1" class="ltx_p"><span id="S4.T5.5.10.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="S4.T5.5.10.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.3.1.1" class="ltx_p"><a href="#Sx1.70.70.70"><abbr href="#Sx1.70.70.70" title="conditional generative adversarial networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">cGAN</span></span></abbr></a></span>
</span>
</td>
<td id="S4.T5.5.10.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.4.1.1" class="ltx_p"><span id="S4.T5.5.10.4.1.1.1" class="ltx_text" style="font-size:70%;">Is a </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.10.4.1.1.2" class="ltx_text" style="font-size:70%;">-based method for accurately localizing </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S4.T5.5.10.4.1.1.3" class="ltx_text" style="font-size:70%;"> electrode contacts in </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S4.T5.5.10.4.1.1.4" class="ltx_text" style="font-size:70%;"> images, facilitating customization of </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S4.T5.5.10.4.1.1.5" class="ltx_text" style="font-size:70%;"> settings.</span></span>
</span>
</td>
<td id="S4.T5.5.10.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.5.1.1" class="ltx_p"><span id="S4.T5.5.10.5.1.1.1" class="ltx_text" style="font-size:70%;">success rate of localization =96.7%</span></span>
</span>
</td>
<td id="S4.T5.5.10.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.6.1.1" class="ltx_p"><span id="S4.T5.5.10.6.1.1.1" class="ltx_text" style="font-size:70%;">coarse resolution, metal artifacts, difficulty isolating contacts from cortical bones, site-dependent image quality, and variability among electrode array manufacturers.</span></span>
</span>
</td>
<td id="S4.T5.5.10.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.10.7.1.1" class="ltx_p"><span id="S4.T5.5.10.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.11" class="ltx_tr">
<td id="S4.T5.5.11.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.11.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T5.5.11.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.11.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.2.1.1" class="ltx_p"><span id="S4.T5.5.11.2.1.1.1" class="ltx_text" style="font-size:70%;">2022</span></span>
</span>
</td>
<td id="S4.T5.5.11.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.3.1.1" class="ltx_p"><a href="#Sx1.106.106.106"><abbr href="#Sx1.106.106.106" title="deep reinforcement learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DRL</span></span></abbr></a></span>
</span>
</td>
<td id="S4.T5.5.11.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.4.1.1" class="ltx_p"><span id="S4.T5.5.11.4.1.1.1" class="ltx_text" style="font-size:70%;">Is a novel method for accurately extracting </span><a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">ROI</span>s</span></abbr></a><span id="S4.T5.5.11.4.1.1.2" class="ltx_text" style="font-size:70%;"> in the inner ear from </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S4.T5.5.11.4.1.1.3" class="ltx_text" style="font-size:70%;"> scans. The approach achieves high precision and demonstrates promising results for surgical planning in cochlear implantation.</span></span>
</span>
</td>
<td id="S4.T5.5.11.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.5.1.1" class="ltx_p"><a href="#Sx1.38.38.38"><abbr href="#Sx1.38.38.38" title="intersection over union" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">IoU</span></span></abbr></a><span id="S4.T5.5.11.5.1.1.1" class="ltx_text" style="font-size:70%;">=0.84 </span>
<br class="ltx_break"><span id="S4.T5.5.11.5.1.1.2" class="ltx_text" style="font-size:70%;">DSC=0.91</span></span>
</span>
</td>
<td id="S4.T5.5.11.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.6.1.1" class="ltx_p"><span id="S4.T5.5.11.6.1.1.1" class="ltx_text" style="font-size:70%;">Small dataset, image quality variability, and computational requirements not specified.</span></span>
</span>
</td>
<td id="S4.T5.5.11.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.11.7.1.1" class="ltx_p"><span id="S4.T5.5.11.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.5" class="ltx_tr">
<td id="S4.T5.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.3.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.5.3.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib75" title="" class="ltx_ref">75</a><span id="S4.T5.5.5.3.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.4.1.1" class="ltx_p"><span id="S4.T5.5.5.4.1.1.1" class="ltx_text" style="font-size:70%;">2020</span></span>
</span>
</td>
<td id="S4.T5.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.5.1.1" class="ltx_p"><span id="S4.T5.5.5.5.1.1.1" class="ltx_text" style="font-size:70%;">U-Net</span></span>
</span>
</td>
<td id="S4.T5.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.6.1.1" class="ltx_p"><span id="S4.T5.5.5.6.1.1.1" class="ltx_text" style="font-size:70%;">Is a multi-scale </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.5.6.1.1.2" class="ltx_text" style="font-size:70%;"> framework for automatic segmentation and measurement of the human cochlea in clinical ultra-high-resolution </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S4.T5.5.5.6.1.1.3" class="ltx_text" style="font-size:70%;"> images, with potential applications in personalized </span><a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a><span id="S4.T5.5.5.6.1.1.4" class="ltx_text" style="font-size:70%;"> surgery.</span></span>
</span>
</td>
<td id="S4.T5.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.2.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.2.2.2" class="ltx_p"><span id="S4.T5.5.5.2.2.2.1" class="ltx_text" style="font-size:70%;">DSC=0.90 </span><math id="S4.T5.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm 0.03" display="inline"><semantics id="S4.T5.4.4.1.1.1.m1.1a"><mrow id="S4.T5.4.4.1.1.1.m1.1.1" xref="S4.T5.4.4.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.4.4.1.1.1.m1.1.1a" xref="S4.T5.4.4.1.1.1.m1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T5.4.4.1.1.1.m1.1.1.2" xref="S4.T5.4.4.1.1.1.m1.1.1.2.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.1.1.m1.1b"><apply id="S4.T5.4.4.1.1.1.m1.1.1.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.4.4.1.1.1.m1.1.1.1.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T5.4.4.1.1.1.m1.1.1.2.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1.2">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.1.1.m1.1c">\pm 0.03</annotation></semantics></math><span id="S4.T5.5.5.2.2.2.2" class="ltx_text" style="font-size:70%;"> </span>
<br class="ltx_break"><span id="S4.T5.5.5.2.2.2.3" class="ltx_text" style="font-size:70%;">BF=0.95</span><math id="S4.T5.5.5.2.2.2.m2.1" class="ltx_Math" alttext="\pm 0.03" display="inline"><semantics id="S4.T5.5.5.2.2.2.m2.1a"><mrow id="S4.T5.5.5.2.2.2.m2.1.1" xref="S4.T5.5.5.2.2.2.m2.1.1.cmml"><mo mathsize="70%" id="S4.T5.5.5.2.2.2.m2.1.1a" xref="S4.T5.5.5.2.2.2.m2.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T5.5.5.2.2.2.m2.1.1.2" xref="S4.T5.5.5.2.2.2.m2.1.1.2.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.2.2.2.m2.1b"><apply id="S4.T5.5.5.2.2.2.m2.1.1.cmml" xref="S4.T5.5.5.2.2.2.m2.1.1"><csymbol cd="latexml" id="S4.T5.5.5.2.2.2.m2.1.1.1.cmml" xref="S4.T5.5.5.2.2.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T5.5.5.2.2.2.m2.1.1.2.cmml" xref="S4.T5.5.5.2.2.2.m2.1.1.2">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.2.2.2.m2.1c">\pm 0.03</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.5.5.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.7.1.1" class="ltx_p"><span id="S4.T5.5.5.7.1.1.1" class="ltx_text" style="font-size:70%;">The work include a small dataset, potential variability in image quality, lack of external validation, and limited assessment of clinical utility and computational requirements.</span></span>
</span>
</td>
<td id="S4.T5.5.5.8" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.8.1.1" class="ltx_p"><span id="S4.T5.5.5.8.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.12" class="ltx_tr">
<td id="S4.T5.5.12.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.12.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib85" title="" class="ltx_ref">85</a><span id="S4.T5.5.12.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.12.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.2.1.1" class="ltx_p"><span id="S4.T5.5.12.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="S4.T5.5.12.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.3.1.1" class="ltx_p"><span id="S4.T5.5.12.3.1.1.1" class="ltx_text" style="font-size:70%;">UNETR</span></span>
</span>
</td>
<td id="S4.T5.5.12.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.4.1.1" class="ltx_p"><span id="S4.T5.5.12.4.1.1.1" class="ltx_text" style="font-size:70%;">Explores the feasibility of using a </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.12.4.1.1.2" class="ltx_text" style="font-size:70%;"> method based on the UNETR model for automatic segmentation of the cochlea in temporal bone </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S4.T5.5.12.4.1.1.3" class="ltx_text" style="font-size:70%;"> images.</span></span>
</span>
</td>
<td id="S4.T5.5.12.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.5.1.1" class="ltx_p"><span id="S4.T5.5.12.5.1.1.1" class="ltx_text" style="font-size:70%;">DSC=0.92</span></span>
</span>
</td>
<td id="S4.T5.5.12.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.6.1.1" class="ltx_p"><span id="S4.T5.5.12.6.1.1.1" class="ltx_text" style="font-size:70%;">The small dataset used, the variability in image quality, and the absence of specifications regarding computational requirements.</span></span>
</span>
</td>
<td id="S4.T5.5.12.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.12.7.1.1" class="ltx_p"><span id="S4.T5.5.12.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.13" class="ltx_tr">
<td id="S4.T5.5.13.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.13.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="S4.T5.5.13.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.13.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.2.1.1" class="ltx_p"><span id="S4.T5.5.13.2.1.1.1" class="ltx_text" style="font-size:70%;">2019</span></span>
</span>
</td>
<td id="S4.T5.5.13.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.3.1.1" class="ltx_p"><span id="S4.T5.5.13.3.1.1.1" class="ltx_text" style="font-size:70%;">3D U-NET</span></span>
</span>
</td>
<td id="S4.T5.5.13.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.4.1.1" class="ltx_p"><span id="S4.T5.5.13.4.1.1.1" class="ltx_text" style="font-size:70%;">Is a two-level training approach using a </span><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a><span id="S4.T5.5.13.4.1.1.2" class="ltx_text" style="font-size:70%;"> method to accurately segment the intra-cochlear anatomy in head </span><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a><span id="S4.T5.5.13.4.1.1.3" class="ltx_text" style="font-size:70%;"> scans. The method combines an active shape model-based method and a 3D U-Net model</span></span>
</span>
</td>
<td id="S4.T5.5.13.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.5.1.1" class="ltx_p"><span id="S4.T5.5.13.5.1.1.1" class="ltx_text" style="font-size:70%;">DSC=0.87</span></span>
</span>
</td>
<td id="S4.T5.5.13.6" class="ltx_td ltx_align_justify ltx_align_middle" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.6.1.1" class="ltx_p"><span id="S4.T5.5.13.6.1.1.1" class="ltx_text" style="font-size:70%;">Limited dataset, variable image quality, lack of external validation, limited assessment of clinical utility, and no specifications on computational requirements.</span></span>
</span>
</td>
<td id="S4.T5.5.13.7" class="ltx_td ltx_align_justify ltx_align_middle" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.13.7.1.1" class="ltx_p"><span id="S4.T5.5.13.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.5.14" class="ltx_tr">
<td id="S4.T5.5.14.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.5.14.1.1.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S4.T5.5.14.1.1.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T5.5.14.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.2.1.1" class="ltx_p"><span id="S4.T5.5.14.2.1.1.1" class="ltx_text" style="font-size:70%;">2023</span></span>
</span>
</td>
<td id="S4.T5.5.14.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:28.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.3.1.1" class="ltx_p"><span id="S4.T5.5.14.3.1.1.1" class="ltx_text" style="font-size:70%;">AlexNet</span></span>
</span>
</td>
<td id="S4.T5.5.14.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:142.3pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.4.1.1" class="ltx_p"><span id="S4.T5.5.14.4.1.1.1" class="ltx_text" style="font-size:70%;">The study assesses repeatability, thresholds, and tonotopic patterns using a DL-based algorithm, providing insights into inner ear function and potential clinical applications.</span></span>
</span>
</td>
<td id="S4.T5.5.14.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:56.9pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.5.1.1" class="ltx_p"><a href="#Sx1.20.20.20"><abbr href="#Sx1.20.20.20" title="Accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">Acc</span></span></abbr></a><span id="S4.T5.5.14.5.1.1.1" class="ltx_text" style="font-size:70%;">=83.8%</span></span>
</span>
</td>
<td id="S4.T5.5.14.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:156.5pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.6.1.1" class="ltx_p"><span id="S4.T5.5.14.6.1.1.1" class="ltx_text" style="font-size:70%;">Potential dependence on the quality of the input data, limited generalizability to different patient populations or implant systems, and the need for further external validation and comparison with expert visual inspection.</span></span>
</span>
</td>
<td id="S4.T5.5.14.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:14.2pt;padding-bottom:14.22636pt;">
<span id="S4.T5.5.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.14.7.1.1" class="ltx_p"><span id="S4.T5.5.14.7.1.1.1" class="ltx_text" style="font-size:70%;">No</span></span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T5.9" class="ltx_p ltx_figure_panel ltx_align_left"><span id="S4.T5.9.1" class="ltx_text" style="font-size:70%;">Abbreviations: Deep learning model (DLM); Best performance (BP); Project link availability (PLA).</span></p>
</div>
</div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Imaging</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> methods have revolutionized <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> applications by leveraging imaging data for enhanced analysis and optimization. Hussain et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> employed image analysis tools, such as the oticon medical nautilus software, to automatically detect landmarks and extract clinically relevant parameters from cochlear <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images. This approach provides valuable insights into cochlear morphology, facilitating the development of less traumatic electrode arrays for cochlear implantation. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> focused on automatically detecting the presence and location of inner ears in head <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, aiming to assist in image-guided <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> programming for patients with profound hearing loss. Regodic et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> introduced an algorithm that utilizes a <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> for automatic fiducial marker detection and localization in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, enhancing registration accuracy, reducing human errors, and shortening intervention time in computer-assisted surgeries. Margeta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> presented Nautilus, a web-based research platform that employs <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> and image processing techniques for automated cochlear image analysis. This platform enables accurate delineation of cochlear structures, detection of electrode locations, and personalized pre- and post-operative metrics, facilitating clinical exploration in cochlear implantation studies. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> proposed the integration of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> techniques into a clinical <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mu</annotation></semantics></math><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> system to optimize imaging performance, improve reconstruction accuracy, and enhance diagnostic capabilities in temporal bone imaging and other clinical applications. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> addressed the reduction of metal artifacts in post-operative <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> imaging using a 3D <a href="#Sx1.53.53.53"><abbr href="#Sx1.53.53.53" title="generative adversarial network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">GAN</span></span></abbr></a>, enabling better analysis of electrode positions and assessment of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> insertion. These advancements highlight the significant role of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in leveraging imaging data for improved <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> analysis, design, and surgical procedures.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In addition to the previous advancements, <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> have been applied to various aspects of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> applications using imaging data. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> utilize <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> for accurate vestibule segmentation in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, which plays a crucial role in the clinical diagnosis of congenital ear malformations and <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> procedures. Kugler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> employ <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> techniques to accurately estimate instrument pose from X-ray images in temporal bone surgery, enabling high-precision navigation and facilitating minimally invasive procedures. Waldeck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> develop an ultra-fast algorithm that utilizes automated cochlear image registration to detect misalignment in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, significantly reducing the time required for diagnosis compared to traditional multiplanar reconstruction analysis. Finally, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> focus on creating a three-dimensional finite element model of the brain based on  <a href="#Sx1.116.116.116"><span href="#Sx1.116.116.116" title="magnetic resonance
imaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">magnetic resonance
imaging</span></span></span></a> (<a href="#Sx1.116.116.116"><abbr href="#Sx1.116.116.116" title="magnetic resonance
imaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">MRI</span></span></abbr></a>) data to analyze and optimize the current flow path induced by <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. This application of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> contributes to the improvement of implant design in the future. These innovative approaches demonstrate the diverse applications of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> research, ranging from scene understanding to precise segmentation, instrument pose estimation, misalignment detection, and implant design optimization.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Segmentation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> have revolutionized <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> segmentation, enabling precise identification and analysis of cochlear structures in various imaging modalities. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> applied a UNETR model to automatically segment cochlear structures in temporal bone <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, enhancing surgical planning and cochlear implantation outcomes. Reda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> developed an automatic segmentation method for intra-cochlear anatomy in post-implantation <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> scans, facilitating the customization of sound processing strategies for individual <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. Moudgalya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> employed a modified V-Net <a href="#Sx1.1.1.1"><abbr href="#Sx1.1.1.1" title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CNN</span></span></abbr></a> to segment cochlear compartments in <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\mu</annotation></semantics></math><a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, enabling precise quantification of local drug delivery for potential treatment of sensorineural hearing loss. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> focused on metal artifact reduction and intra-cochlear anatomy segmentation in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images using a multi-resolution multi-task deep network, benefiting <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. Heutink et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> developed a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> framework for the automatic segmentation and analysis of cochlear structures in ultra-high-resolution <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, providing accurate measurements for personalized implant planning in cochlear surgery. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> utilized a 3D U-Net <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> method to achieve accurate segmentation of intra-cochlear anatomy in head <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images, facilitating optimal programming of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> and improving hearing outcomes. These studies highlight the significant impact of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in advancing <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> segmentation, ultimately leading to improved patient care and treatment outcomes.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Thresholding</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> have been instrumental in the field of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, particularly in thresholding applications. Kuczapski et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> developed a software tool that utilizes <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> to estimate and monitor the  <a href="#Sx1.114.114.114"><span href="#Sx1.114.114.114" title="effective stimulation threshold" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">effective stimulation threshold</span></span></span></a> (<a href="#Sx1.114.114.114"><abbr href="#Sx1.114.114.114" title="effective stimulation threshold" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EST</span></span></abbr></a>) levels in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. By leveraging patient data, audiograms, and fitting settings, this tool aids in the fitting process and predicts changes in hearing levels, enhancing personalized care. Botros et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> introduced AutoNRT, an automated system that combines <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and pattern recognition to measure  <a href="#Sx1.115.115.115"><span href="#Sx1.115.115.115" title="electrically evoked compound action potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">electrically evoked compound action potential</span></span></span></a> (<a href="#Sx1.115.115.115"><abbr href="#Sx1.115.115.115" title="electrically evoked compound action potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECAP</span></span></abbr></a>) thresholds with the Nucleus Freedom <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>. This objective fitting system streamlines clinical procedures and ensures precise and efficient threshold measurements. Furthermore, Schuerch et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> utilized a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based algorithm to objectively evaluate and analyze <a href="#Sx1.97.97.97"><abbr href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECochG</span></span></abbr></a> signals. This algorithm enables the assessment of <a href="#Sx1.97.97.97"><abbr href="#Sx1.97.97.97" title="intracochlear electrocochleography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ECochG</span></span></abbr></a> measurement repeatability, comparison with audiometric thresholds, and identification of signal patterns and tonotopic behavior in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients. Through the integration of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, machine <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a>, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>, these studies have significantly advanced thresholding techniques in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> applications, leading to improved fitting accuracy, streamlined procedures, and objective evaluation of signal responses.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Localization of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> </h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p"><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> methods have been instrumental in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> localization applications, providing accurate and automated solutions. Chi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
proposed a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>-based method for precise localization of electrode contacts in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images. Their approach utilized cGANs to
generate likelihood maps, which were then processed to estimate the exact location of each contact. Radutoiu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> focused
on the automatic extraction of <a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">ROI</span>s</span></abbr></a> in full head <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> scans of the inner ear. By leveraging <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>, they achieved high precision in <a href="#Sx1.105.105.105"><abbr href="#Sx1.105.105.105" title="regions of interest" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ROI</span></span></abbr></a>
localization, facilitating accurate surgical planning for insertion. Noble et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> and Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>, developed AI-based
systems to automatically identify and position electrode arrays in <a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images. These technologies enable large-scale analyses of the
relationship between electrode placement and hearing outcomes, leading to potential advancements in implant design and surgical
techniques. Heutink et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> employed <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> for the automatic segmentation and localization of the cochlea in ultra-high-resolution
<a href="#Sx1.16.16.16"><abbr href="#Sx1.16.16.16" title="computed tomography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CT</span></span></abbr></a> images. This approach allows for precise measurements that can be used in personalized planning, reducing the risk of
intracochlear trauma and optimizing surgical outcomes. These studies showcase the significant contributions of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in
localization applications, enabling accurate and efficient identification, positioning, and analysis of electrode arrays and facilitating
improved surgical planning and outcomes.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Other</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p"><a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> techniques have been employed in various <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> applications, showcasing their potential to enhance hearing outcomes and improve device performance. Bermejo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> introduced a decision support system using a novel probabilistic graphical model to optimize <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> parameters based on audiological tests and the current device status, aiming to optimize the user’s hearing ability. Castaneda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> focused on the use of  <a href="#Sx1.117.117.117"><span href="#Sx1.117.117.117" title="blind source separation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">blind source separation</span></span></span></a> (<a href="#Sx1.117.117.117"><abbr href="#Sx1.117.117.117" title="blind source separation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">BSS</span></span></abbr></a>) and independent component analysis (ICA) to identify <a href="#Sx1.118.118.118"><span href="#Sx1.118.118.118" title="auditory evoked potential" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural"><span class="ltx_text" style="font-size:80%;">auditory evoked potential</span>s</span></span></a> and isolate artifacts in children with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, enabling improved assessment of auditory function. Incerti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> investigated the impact of varying cross-over frequency settings for <a href="#Sx1.11.11.11"><abbr href="#Sx1.11.11.11" title="electric and acoustic stimulation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EAS</span></span></abbr></a> on binaural speech perception, localization, and functional performance in adults with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> and residual hearing, providing valuable insights for personalized device programming. Katthi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> developed a <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> framework based on  <a href="#Sx1.119.119.119"><span href="#Sx1.119.119.119" title="canonical correlation analysis" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">canonical correlation analysis</span></span></span></a> (<a href="#Sx1.119.119.119"><abbr href="#Sx1.119.119.119" title="canonical correlation analysis" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CCA</span></span></abbr></a>) to decode the auditory brain, establishing a strong correlation between audio input and brain activity measured through <a href="#Sx1.76.76.76"><abbr href="#Sx1.76.76.76" title="electroencephalography" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">EEG</span></span></abbr></a> recordings. This research has implications for decoding human auditory attention and improving <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> by leveraging the power of <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Open issues and future directions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">While significant strides have been achieved in integrating <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> into <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, numerous research lacunae persist, offering avenues for further advancements in the field. Here are several potential realms warranting exploration in future studies:
<br class="ltx_break"></p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Real-time signal processing and
personalized design: </span> Investigating real-time adaptive signal processing methods employing <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms has the potential to enhance sound processing for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> recipients, yielding enhanced speech intelligibility outcomes. Enhancements in adaptability to dynamic acoustic environments and real-time optimization of stimulation parameters have the capacity to substantially enhance <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> performance. The authors have observed a gap in the study and implementation of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> models tailored for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> on real-time platforms like field programmable gate arrays (FPGAs). Further research in this burgeoning area holds promise for adapting a variety of existing AI models to enhance real-time capabilities.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Besides, tailoring <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> to meet the unique needs of individual users poses a significant challenge. Investigating <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-driven methodologies leveraging personal data to personalize device configurations based on factors like physiological, auditory, and neural feedback during mobility can enhance both individual outcomes and overall satisfaction.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Predicting the long-term effects:</span> Gaining insight into the enduring <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> is vital for enhancing patient selection, counseling, and device advancement. Utilizing <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> methods to shift through extensive datasets can pinpoint the predictive elements influencing sustained success. These factors may encompass pre-implantation attributes, surgical approaches, and auditory rehabilitation. Constructing predictive models using AI algorithms can furnish valuable perspectives on long-term consequences, thereby informing clinical judgments.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Incorporating multiple sensory and modalities:</span> <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> traditionally prioritize the reinstatement of auditory experiences. Yet, enriching the perception and comprehension of sound can be achieved by integrating additional sensory dimensions like vision and touch, resulting in a multi-modal approach. Investigating <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a>-driven techniques that amalgamate inputs from various senses to enhance speech recognition, spatial sound perception, and overall auditory understanding presents a promising direction for future exploration. Besides, <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> in both ears, when paired with <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms, can enhance speech comprehension. By analyzing sound patterns from both implants, AI adjusts settings to optimize signal processing, improving overall accuracy and clarity of speech perception for users with bilateral implants, and enhancing their auditory experience and communication abilities, as investigated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. However, extensive research possibilities are required to tailor solutions with <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> hardware capabilities, by taking into account computation cost, and <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> model complexity.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Empowering AI-based CI using DTL:</span> <span id="S5.p6.1.2" class="ltx_ERROR undefined">\Ac</span>DTL is a highly efficient <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a> technique enabling the transfer of knowledge from pre-trained models, trained on millions of speech corpora and/or images, to train smaller models with limited data availability. This approach offers significant advantages in producing lightweight <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> models suitable for devices with limited computational resources, such as <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. Only a limited number of studies have explored the impact of  <a href="#Sx1.124.124.124"><span href="#Sx1.124.124.124" title="deep transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">deep transfer learning</span></span></span></a> (<a href="#Sx1.124.124.124"><abbr href="#Sx1.124.124.124" title="deep transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DTL</span></span></abbr></a>) on <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, which has received relatively little attention from researchers. We anticipate further exploration of this promising technique, particularly through the utilization of various <a href="#Sx1.124.124.124"><abbr href="#Sx1.124.124.124" title="deep transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DTL</span></span></abbr></a> sub-techniques, such as domain adaptation, transductive methods like cross-lingual transfer, cross-corpus transfer, zero-shot learning, fine-tuning, among others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S5.p7" class="ltx_para ltx_noindent">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Ensuring data privacy through FL:</span>
<span id="S5.p7.1.2" class="ltx_ERROR undefined">\Ac</span>FL facilitates collaborative model training across decentralized devices by aggregating local updates rather than centralizing data. This preserves user privacy and enhances model performance, particularly beneficial in healthcare applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>. Gathering comprehensive datasets is challenging due to rare anomaly cases and privacy concerns.  <a href="#Sx1.125.125.125"><span href="#Sx1.125.125.125" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">federated learning</span></span></span></a> (<a href="#Sx1.125.125.125"><abbr href="#Sx1.125.125.125" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FL</span></span></abbr></a>) addresses this by training models on distributed, encrypted data from multiple sources, ensuring privacy while maintaining efficacy. Researchers have yet to fully explore FL-based model building for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, neglecting the potential to construct efficient <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> models capable of accommodating diverse classes. Further investigation into this promising technique is warranted, with potential for significant advancements in model robustness and versatility. Moreover, this approach could lead to the development of a pretrained model utilizing <a href="#Sx1.125.125.125"><abbr href="#Sx1.125.125.125" title="federated learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">FL</span></span></abbr></a>, which could be seamlessly integrated with <a href="#Sx1.124.124.124"><abbr href="#Sx1.124.124.124" title="deep transfer learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DTL</span></span></abbr></a>.</p>
</div>
<div id="S5.p8" class="ltx_para ltx_noindent">
<p id="S5.p8.1" class="ltx_p"><span id="S5.p8.1.1" class="ltx_text ltx_font_bold">Transformers-based CI techniques: </span> Transformer-based techniques, such as  <a href="#Sx1.126.126.126"><span href="#Sx1.126.126.126" title="connectionist temporal classification" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">connectionist temporal classification</span></span></span></a> (<a href="#Sx1.126.126.126"><abbr href="#Sx1.126.126.126" title="connectionist temporal classification" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CTC</span></span></abbr></a>),  <a href="#Sx1.127.127.127"><span href="#Sx1.127.127.127" title="bidirectional encoder representations from transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long"><span class="ltx_text" style="font-size:80%;">bidirectional encoder representations from transformer</span></span></span></a> (<a href="#Sx1.127.127.127"><abbr href="#Sx1.127.127.127" title="bidirectional encoder representations from transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">BERT</span></span></abbr></a>), and others, proved in the literature that have the potential to greatly enhance the functioning of <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. By leveraging the self-attention mechanism, transformers can improve speech intelligibility by effectively suppressing background noise and modeling long-range dependencies. They can also aid in acoustic scene analysis, separating and prioritizing important auditory information in complex environments. Transformers can build language models that enhance <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> systems, improving speech comprehension for users. Additionally, transformers enable personalized sound processing by adapting stimulation patterns and processing parameters based on user-specific preferences. They facilitate multi-modal integration, combining audio and visual inputs to enhance speech recognition and sound localization. Furthermore, transformers support long-term learning and adaptation, continually optimizing <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> performance over time. These advancements offer promising prospects for improving auditory experiences and overall quality of life for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users.</p>
</div>
<div id="S5.p9" class="ltx_para ltx_noindent">
<p id="S5.p9.1" class="ltx_p"><span id="S5.p9.1.1" class="ltx_text ltx_font_bold">Exploring chat-bots-based CI capabilities: </span>
Chat-bot techniques offer several opportunities to enhance the functioning of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. They can provide real-time support, troubleshooting, and personalized rehabilitation programs for users, empowering them to address common issues and improve their auditory skills. Chat-bots enable remote monitoring, allowing users to share data and receive adjustments to their device settings without in-person appointments. They also offer emotional and psychological support, fostering a sense of community and well-being. Chat-bots contribute to data collection for research and development, aiding in the improvement of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technology and rehabilitation protocols. Additionally, chat-bots employ<a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> to continuously learn from user interactions, improving their responses and understanding over time. These techniques have the potential to enhance the overall user experience, outcomes, and accessibility of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> services.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This review has provided a comprehensive overview of the advancements in <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a> applications and their impact on <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> and speech enhancement. The integration of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> methods has brought cutting-edge strategies to address the limitations and challenges faced by traditional signal processing techniques in the context of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. Moreover, the application of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> in <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> has led to the emergence of new datasets and evaluation metrics, offering alternative methods for validating proposed schemes without the need for human surgical intervention and traditional tests. The review highlighted the role of <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> in optimizing speech perception and understanding for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> users, contributing to the improvement of their quality of life. <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> not only enhances basic speech recognition but also aids in the recognition of environmental sounds, enabling a more immersive auditory experience.
Furthermore, <a href="#Sx1.49.49.49"><abbr href="#Sx1.49.49.49" title="automatic speech recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ASR</span></span></abbr></a> finds applications in authentication systems, event recognition, source separation, and speaker recognition, extending its reach beyond communication.
Various <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms, belong to <a href="#Sx1.5.5.5"><abbr href="#Sx1.5.5.5" title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">ML</span></span></abbr></a> and <a href="#Sx1.3.3.3"><abbr href="#Sx1.3.3.3" title="deep learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">DL</span></span></abbr></a>, have been explored in the context of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, demonstrating promising results in speech synthesis and noise reduction. These algorithms have shown the potential to overcome challenges associated with multiple sources of speech, environmental noise, and other complex scenarios. The review has summarized and commented on the best results obtained, providing valuable insights into the capabilities of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms in this biomedical field.
Moving forward, the review suggests future directions to bridge existing research gaps in the domain of <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>. It emphasizes the need for high-quality data inputs, algorithm transparency, and collaboration between researchers, clinicians, and industry experts. Addressing these aspects will facilitate the development of more accurate and efficient <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms for <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a>, ultimately benefiting individuals with hearing impairments. The integration of advanced <a href="#Sx1.2.2.2"><abbr href="#Sx1.2.2.2" title="artificial intelligent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">AI</span></span></abbr></a> algorithms has the potential to revolutionize the field of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural"><span class="ltx_text" style="font-size:80%;">CI</span>s</span></abbr></a>, providing individuals with hearing impairments to better communicate and engage with the world around them. Continued research and development in this area hold great promise for the future of <a href="#Sx1.4.4.4"><abbr href="#Sx1.4.4.4" title="cochlear implant" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short"><span class="ltx_text" style="font-size:80%;">CI</span></span></abbr></a> technology.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The second author acknowledges that the study was partially funded by the Algerian Ministry of Higher Education and Scientific Research (Grant No. PRFU–A25N01UN260120230001).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. B. Er, E. Isik, I. Isik, Parkinson’s detection based on combined cnn and lstm using enhanced speech signals with variational mode decomposition, Biomedical Signal Processing and Control 70 (2021) 103006.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer learning for automatic speech recognition: Towards better generalization, Knowledge-Based Systems 277 (2023) 110851.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.knosys.2023.110851" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1016/j.knosys.2023.110851</span></a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Hu, X. Shang, Z. Qin, M. Li, Q. Wang, C. Wang, Adversarial examples for automatic speech recognition: Attacks and countermeasures, IEEE Communications Magazine 57 (10) (2019) 120–126.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Kheddar, D. Megías, High capacity speech steganography for the g723. 1 coder based on quantised line spectral pairs interpolation and cnn auto-encoding, Applied Intelligence 52 (8) (2022) 9441–9459.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Kheddar, M. Hemis, Y. Himeur, D. Megías, A. Amira, Deep learning for steganalysis of diverse data types: A review of methods, taxonomy, challenges and future directions, Neurocomputing (2024) 127528<a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.neucom.2024.127528" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.neucom.2024.127528</span></a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Kheddar, D. Megias, M. Bouzid, Fourier magnitude-based steganography for hiding 2.4 kbps melp secret speech, in: 2018 International Conference on Applied Smart Systems (ICASS), IEEE, 2018, pp. 1–5.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N. Singh, A. Agrawal, R. Khan, Voice biometric: A technology for voice based authentication, Advanced Science, Engineering and Medicine 10 (7-8) (2018) 754–759.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T.-E. Chen, S.-I. Yang, L.-T. Ho, K.-H. Tsai, Y.-H. Chen, Y.-F. Chang, Y.-H. Lai, S.-S. Wang, Y. Tsao, C.-C. Wu, S1 and s2 heart sound recognition using deep neural networks, IEEE Transactions on Biomedical Engineering 64 (2) (2016) 372–380.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. G. Crowson, V. Lin, J. M. Chen, T. C. Chan, Machine learning and cochlear implantation—a structured review of opportunities and challenges, Otology &amp; Neurotology 41 (1) (2020) e36–e45.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Manero, K. E. Crawford, H. Prock-Gibbs, N. Shah, D. Gandhi, M. J. Coathup, Improving disease prevention, diagnosis, and treatment using novel bionic technologies, Bioengineering &amp; Translational Medicine 8 (1) (2023) e10359.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. S. Wilson, D. L. Tucci, D. A. Moses, E. F. Chang, N. M. Young, F.-G. Zeng, N. A. Lesica, A. M. Bur, H. Kavookjian, C. Mussatto, et al., Harnessing the power of artificial intelligence in otolaryngology and the communication sciences, Journal of the Association for Research in Otolaryngology 23 (3) (2022) 319–349.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. L. D’Onofrio, F.-G. Zeng, Tele-audiology: Current state and future directions, Frontiers in Digital Health 3 (2022) 788103.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
F. Henry, M. Glavin, E. Jones, Noise reduction in cochlear implant signal processing: A review and recent developments, IEEE reviews in biomedical engineering (2021).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
O. Macherey, R. P. Carlyon, Cochlear implants, Current Biology 24 (18) (2014) R878–R884.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N. Y.-H. Wang, H.-L. S. Wang, T.-W. Wang, S.-W. Fu, X. Lu, H.-M. Wang, Y. Tsao, Improving the intelligibility of speech for simulated electric and acoustic stimulation using fully convolutional neural networks, IEEE Transactions on Neural Systems and Rehabilitation Engineering 29 (2020) 184–195.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.-H. Lai, F. Chen, S.-S. Wang, X. Lu, Y. Tsao, C.-H. Lee, A deep denoising autoencoder approach to improving the intelligibility of vocoded speech in cochlear implant simulation, IEEE Transactions on Biomedical Engineering 64 (7) (2016) 1568–1578.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S.-S. Wang, Y. Tsao, H.-L. S. Wang, Y.-H. Lai, L. P.-H. Li, A deep learning based noise reduction approach to improve speech intelligibility for cochlear implant recipients in the presence of competing speech noise, in: 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), IEEE, 2017, pp. 808–812.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. L. Wong, S. D. Soli, S. Liu, N. Han, M.-W. Huang, Development of the mandarin hearing in noise test (mhint), Ear and hearing 28 (2) (2007) 70S–74S.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Arias-Vergara, P. Klumpp, J. C. Vasquez-Correa, E. Nöth, J. R. Orozco-Arroyave, M. Schuster, Multi-channel spectrograms for speech processing applications using deep learning methods, Pattern Analysis and Applications 24 (2021) 423–431.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Arias-Vergara, J. C. Vasquez-Correa, S. Gollwitzer, J. R. Orozco-Arroyave, M. Schuster, E. Nöth, Multi-channel convolutional neural networks for automatic detection of speech deficits in cochlear implant users, in: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 24th Iberoamerican Congress, CIARP 2019, Havana, Cuba, October 28-31, 2019, Proceedings 24, Springer, 2019, pp. 679–687.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Hinrichs, F. Ortmann, J. Ostermann, Vector-quantized zero-delay deep autoencoders for the compression of electrical stimulation patterns of cochlear implants using stoi, in: 2022 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), IEEE, 2022, pp. 165–170.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Gajecki, Y. Zhang, W. Nogueira, A deep denoising sound coding strategy for cochlear implants, IEEE Transactions on Biomedical Engineering (2023).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Chu, C. Throckmorton, L. Collins, B. Mainsah, Using machine learning to mitigate the effects of reverberation and noise in cochlear implants, in: Proceedings of Meetings on Acoustics, Vol. 33, AIP Publishing, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Grimm, M. Pettinato, S. Gillis, W. Daelemans, Simulating speech processing with cochlear implants: How does channel interaction affect learning in neural networks?, Plos one 14 (2) (2019) e0212134.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Feng, Y. Tsao, F. Chen, Preservation of interaural level difference cue in a deep learning-based speech separation system for bilateral and bimodal cochlear implants users, in: 2022 International Workshop on Acoustic Signal Enhancement (IWAENC), IEEE, 2022, pp. 1–5.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Gajecki, W. Nogueira, An end-to-end deep learning speech coding and denoising strategy for cochlear implants, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp. 3109–3113.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
I. Hochmair-Desoyer, E. Schulz, L. Moser, M. Schmidt, The hsm sentence test as a tool for evaluating the speech understanding in noise of cochlear implant users., The American journal of otology 18 (6 Suppl) (1997) S83–S83.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Kang, N. Zheng, Q. Meng, Deep learning-based speech enhancement with a loss trading off the speech distortion and the noise residue for cochlear implants, Frontiers in Medicine 8 (2021) 740123.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T. Fischer, M. Caversaccio, W. Wimmer, Speech signal enhancement in cocktail party scenarios by deep learning based virtual sensing of head-mounted microphones, Hearing research 408 (2021) 108294.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
T. Fischer, M. Caversaccio, W. Wimmer, Multichannel acoustic source and image dataset for the cocktail party effect in hearing aid and implant users, Scientific data 7 (1) (2020) 440.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Tahmasebi, T. Gajecki, W. Nogueira, Design and evaluation of a real-time audio source separation algorithm to remix music for cochlear implant users, Frontiers in Neuroscience 14 (2020) 434.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A.-T. Radutoiu, F. Patou, J. Margeta, R. R. Paulsen, P. López Diez, Accurate localization of inner ear regions of interests using deep reinforcement learning, in: International Workshop on Machine Learning in Medical Imaging, Springer, 2022, pp. 416–424.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
K. Schuerch, W. Wimmer, A. Dalbert, C. Rummel, M. Caversaccio, G. Mantokoudis, S. Weder, Objectification of intracochlear electrocochleography using machine learning, Frontiers in neurology 13 (2022) 943816.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Schuerch, W. Wimmer, C. Rummel, M. D. Caversaccio, S. Weder, Objective evaluation of intracochlear electrocochleography: repeatability, thresholds, and tonotopic patterns, Frontiers in neurology 14 (2023).

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
K. Schuerch, W. Wimmer, A. Dalbert, C. Rummel, M. Caversaccio, G. Mantokoudis, T. Gawliczek, S. Weder, An intracochlear electrocochleography dataset-from raw data to objective analysis using deep learning, Scientific data 10 (1) (2023) 157.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Habchi, Y. Himeur, H. Kheddar, A. Boukabou, S. Atalla, A. Chouchane, A. Ouamane, W. Mansoor, Ai in thyroid cancer diagnosis: Techniques, trends, and future directions, Systems 11 (10) (2023) 519.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
C. H. Taal, R. C. Hendriks, R. Heusdens, J. Jensen, A short-time objective intelligibility measure for time-frequency weighted noisy speech, in: 2010 IEEE international conference on acoustics, speech and signal processing, IEEE, 2010, pp. 4214–4217.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
P. J. Govaerts, B. Vaerenberg, G. De Ceulaer, K. Daemers, C. De Beukelaer, K. Schauwers, Development of a software tool using deterministic logic for the optimization of cochlear implant processor programming, Otology &amp; Neurotology 31 (6) (2010) 908–918.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
B. Vaerenberg, P. J. Govaerts, G. De Ceulaer, K. Daemers, K. Schauwers, Experiences of the use of fox, an intelligent agent, for programming cochlear implant sound processors in new users, International Journal of Audiology 50 (1) (2011) 50–58.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. Meeuws, D. Pascoal, I. Bermejo, M. Artaso, G. De Ceulaer, P. J. Govaerts, Computer-assisted ci fitting: Is the learning capacity of the intelligent agent fox beneficial for speech understanding?, Cochlear Implants International 18 (4) (2017) 198–206.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Wathour, P. J. Govaerts, N. Deggouj, From manual to artificial intelligence fitting: two cochlear implant case studies, Cochlear implants international 21 (5) (2020) 299–305.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. B. Waltzman, D. C. Kelsall, The use of artificial intelligence to program cochlear implants, Otology &amp; Neurotology 41 (4) (2020) 452–457.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Wathour, P. J. Govaerts, L. Derue, S. Vanderbemden, H. Huaux, E. Lacroix, N. Deggouj, Prospective comparison between manual and computer-assisted (fox) cochlear implant fitting in newly implanted patients (2023).

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
T. Eichler, W. Rötz, C. Kayser, F. Bröhl, M. Römer, A. H. Witteborg, F. Kummert, T. Sandmeier, C. Schulte, P. Stolz, et al., Algorithm-based hearing and speech therapy rehabilitation after cochlear implantation, Brain Sciences 12 (5) (2022) 580.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Torresen, A. H. Iversen, R. Greisiger, Data from past patients used to streamline adjustment of levels for cochlear implant for new patients, in: 2016 IEEE Symposium Series on Computational Intelligence (SSCI), IEEE, 2016, pp. 1–7.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
F. Henry, A. Parsi, M. Glavin, E. Jones, Experimental investigation of acoustic features to optimize intelligibility in cochlear implants, Sensors 23 (17) (2023) 7553.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
C. Pavelchek, A. P. Michelson, A. Walia, A. Ortmann, J. Herzog, C. A. Buchman, M. A. Shew, Imputation of missing values for cochlear implant candidate audiometric data and potential applications, Plos one 18 (2) (2023) e0281337.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
C. Xu, F.-Y. Cheng, S. Medina, E. Eng, R. Gifford, S. Smith, Objective discrimination of bimodal speech using frequency following responses, Hearing research 437 (2023) 108853.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
H. Kheddar, M. Bouzid, D. Megías, Pitch and fourier magnitude based steganography for hiding 2.4 kbps melp bitstream, IET Signal Processing 13 (3) (2019) 396–407.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
M. G. Crowson, P. Dixon, R. Mahmood, J. W. Lee, D. Shipp, T. Le, V. Lin, J. Chen, T. C. Chan, Predicting postoperative cochlear implant performance using supervised machine learning, Otology &amp; Neurotology 41 (8) (2020) e1013–e1023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
P. Mikulskis, A. Hook, A. A. Dundas, D. Irvine, O. Sanni, D. Anderson, R. Langer, M. R. Alexander, P. Williams, D. A. Winkler, Prediction of broad-spectrum pathogen attachment to coating materials for biomedical devices, ACS applied materials &amp; interfaces 10 (1) (2018) 139–149.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y. A. Alohali, M. S. Fayed, Y. Abdelsamad, F. Almuhawas, A. Alahmadi, T. Mesallam, A. Hagr, Machine learning and cochlear implantation: Predicting the post-operative electrode impedances, Electronics 12 (12) (2023) 2720.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R. Islam, E. Abdel-Raheem, M. Tarique, A novel pathological voice identification technique through simulated cochlear implant processing systems, Applied Sciences 12 (5) (2022) 2398.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. de Nobel, A. V. Kononova, J. J. Briaire, J. H. Frijns, T. H. Bäck, Optimizing stimulus energy for cochlear implants with a machine learning model of the auditory nerve, Hearing Research 432 (2023) 108741.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
T. Bäck, H.-P. Schwefel, An overview of evolutionary algorithms for parameter optimization, Evolutionary computation 1 (1) (1993) 1–23.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
D. Baby, A. Van Den Broucke, S. Verhulst, A convolutional neural-network model of human cochlear mechanics and filter tuning for real-time applications, Nature machine intelligence 3 (2) (2021) 134–143.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
D. Zhang, J. H. Noble, B. M. Dawant, Automatic detection of the inner ears in head ct images using deep convolutional neural networks, in: Medical Imaging 2018: Image Processing, Vol. 10574, SPIE, 2018, pp. 565–572.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
W. Li, T. Gueuret, B. Lin, Poster: Slidecnn: Deep learning for auditory spatial scenes with limited annotated data, in: 2022 IEEE/ACM 7th Symposium on Edge Computing (SEC), IEEE, 2022, pp. 316–317.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
M.-H. Laves, S. Ihler, L. A. Kahrs, T. Ortmaier, Deep-learning-based 2.5 d flow field estimation for maximum intensity projections of 4d optical coherence tomography, in: Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions, and Modeling, Vol. 10951, SPIE, 2019, pp. 189–195.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
M. Chen, L. Zhuo, Z. Zhu, H. Yin, X. Li, Z. Wang, Deeply supervised vestibule segmentation network for ct images with global context-aware pyramid feature extraction, IET Image Processing 17 (4) (2023) 1267–1279.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
A. Lou, K. Tawfik, X. Yao, Z. Liu, J. Noble, Min-max similarity: A contrastive semi-supervised deep learning network for surgical tools segmentation, IEEE Transactions on Medical Imaging (2023).

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Z. Wang, C. Vandersteen, T. Demarcy, D. Gnansia, C. Raffaelli, N. Guevara, H. Delingette, Inner-ear augmented metal artifact reduction with simulation-based 3d generative adversarial networks, Computerized Medical Imaging and Graphics 93 (2021) 101990.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
J. Wang, Y. Zhao, J. H. Noble, B. M. Dawant, Conditional generative adversarial networks for metal artifact reduction in ct images of the ear, in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I, Springer, 2018, pp. 3–11.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Y. Lu, S. Yang, Z. Xu, J. Wang, Speech training system for hearing impaired individuals based on automatic lip-reading recognition, in: Advances in Human Factors and Systems Interaction: Proceedings of the AHFE 2020 Virtual Conference on Human Factors and Systems Interaction, July 16-20, 2020, USA, Springer, 2020, pp. 250–258.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
M. Jeyalakshmi, C. R. Robin, D. Doreen, Predicting cochlear implants score with the aid of reconfigured long short-term memory, Multimedia Tools and Applications 82 (8) (2023) 12537–12556.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M. Meeuws, D. Pascoal, S. Janssens de Varebeke, G. De Ceulaer, P. J. Govaerts, Cochlear implant telemedicine: remote fitting based on psychoacoustic self-tests and artificial intelligence, Cochlear Implants International 21 (5) (2020) 260–268.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Wathour, P. J. Govaerts, E. Lacroix, D. Naïma, Effect of a ci programming fitting tool with artificial intelligence in experienced cochlear implant patients, Otology &amp; Neurotology 44 (3) (2023) 209–215.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Z. Wang, C. Vandersteen, T. Demarcy, D. Gnansia, C. Raffaelli, N. Guevara, H. Delingette, Deep learning based metal artifacts reduction in post-operative cochlear implant ct imaging, in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part VI 22, Springer, 2019, pp. 121–129.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
T. Gajęcki, W. Nogueira, Deep learning models to remix music for cochlear implant users, The Journal of the Acoustical Society of America 143 (6) (2018) 3602–3615.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation 9 (8) (1997) 1735–1780.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, J. Schmidhuber, LSTM: A search space odyssey, IEEE transactions on neural networks and learning systems 28 (10) (2016) 2222–2232.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
K. Chu, L. Collins, B. Mainsah, A causal deep learning framework for classifying phonemes in cochlear implants, in: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 6498–6502.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
E. H.-H. Huang, R. Chao, Y. Tsao, Electrodenet–a deep learning based sound coding strategy for cochlear implants, IEEE Transactions on Cognitive and Developmental Systems (2023).

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Y. Fan, D. Zhang, R. Banalagay, J. Wang, J. H. Noble, B. M. Dawant, Hybrid active shape and deep learning method for the accurate and robust segmentation of the intracochlear anatomy in clinical head ct and cbct images, Journal of Medical Imaging 8 (6) (2021) 064002–064002.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
F. Heutink, V. Koch, B. Verbist, W. J. van der Woude, E. Mylanus, W. Huinck, I. Sechopoulos, M. Caballo, Multi-scale deep learning framework for cochlea localization, segmentation and analysis on clinical ultra-high-resolution ct images, Computer methods and programs in biomedicine 191 (2020) 105387.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
A. Gueriani, H. Kheddar, A. C. Mazari, Deep reinforcement learning for intrusion detection in iot: A survey, in: 2023 2nd International Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, 2023, pp. 1–7.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IC2EM59347.2023.10419560" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IC2EM59347.2023.10419560</span></a>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
P. López Diez, J. V. Sundgaard, F. Patou, J. Margeta, R. R. Paulsen, Facial and cochlear nerves characterization using deep reinforcement learning for landmark detection, in: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part IV 24, Springer, 2021, pp. 519–528.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Y.-H. Lai, Y. Tsao, X. Lu, F. Chen, Y.-T. Su, K.-C. Chen, Y.-H. Chen, L.-C. Chen, L. P.-H. Li, C.-H. Lee, Deep learning–based noise reduction approach to improve speech intelligibility for cochlear implant recipients, Ear and hearing 39 (4) (2018) 795–809.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
E. W. Healy, S. E. Yoho, J. Chen, Y. Wang, D. Wang, An algorithm to increase speech intelligibility for hearing-impaired listeners in novel segments of the same noise type, The Journal of the Acoustical Society of America 138 (3) (2015) 1660–1669.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Y. Hu, P. C. Loizou, Environment-specific noise suppression for improved speech intelligibility by cochlear implant users, The Journal of the Acoustical Society of America 127 (6) (2010) 3689–3695.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
B. Banerjee, L. L. Mendel, J. K. Dutta, H. Shabani, S. Najnin, Identifying hearing deficiencies from statistically learned speech features for personalized tuning of cochlear implants, in: Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
L. P.-H. Li, J.-Y. Han, W.-Z. Zheng, R.-J. Huang, Y.-H. Lai, Improved environment-aware–based noise reduction system for cochlear implant users based on a knowledge transfer approach: Development and usability study, Journal of medical Internet research 23 (10) (2021) e25460.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
J. Margeta, R. Hussain, P. López Diez, A. Morgenstern, T. Demarcy, Z. Wang, D. Gnansia, O. Martinez Manzanera, C. Vandersteen, H. Delingette, et al., A web-based automated image processing research platform for cochlear implantation-related studies, Journal of Clinical Medicine 11 (22) (2022) 6640.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Y. Chi, J. Wang, Y. Zhao, J. H. Noble, B. M. Dawant, A deep-learning-based method for the localization of cochlear implant electrodes in ct images, in: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE, 2019, pp. 1141–1145.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Z. Li, L. Zhou, S. Tan, A. Tang, Application of unetr for automatic cochlear segmentation in temporal bone cts, Auris Nasus Larynx 50 (2) (2023) 212–217.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
D. Zhang, R. Banalagay, J. Wang, Y. Zhao, J. H. Noble, B. M. Dawant, Two-level training of a 3d u-net for accurate segmentation of the intra-cochlear anatomy in head cts with limited ground truth training data, in: Medical Imaging 2019: Image Processing, Vol. 10949, SPIE, 2019, pp. 45–52.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
R. Hussain, A. Frater, R. Calixto, C. Karoui, J. Margeta, Z. Wang, M. Hoen, H. Delingette, F. Patou, C. Raffaelli, et al., Anatomical variations of the human cochlea using an image analysis tool, Journal of Clinical Medicine 12 (2) (2023) 509.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
M. Regodic, Z. Bardosi, W. Freysinger, Automatic fiducial marker detection and localization in ct images: a combined approach, in: Medical Imaging 2020: Image-Guided Procedures, Robotic Interventions, and Modeling, Vol. 11315, SPIE, 2020, pp. 507–514.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
M. Li, Z. Fang, W. Cong, C. Niu, W. Wu, J. Uher, J. Bennett, J. T. Rubinstein, G. Wang, Clinical micro-ct empowered by interior tomography, robotic scanning, and deep learning, IEEE Access 8 (2020) 229018–229032.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
D. Kügler, J. Sehring, A. Stefanov, I. Stenin, J. Kristin, T. Klenzner, J. Schipper, A. Mukhopadhyay, i3posnet: instrument pose estimation from x-ray in temporal bone surgery, International journal of computer assisted radiology and surgery 15 (2020) 1137–1145.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
S. Waldeck, R. Helal, I. Al-Dhamari, S. Schmidt, C. von Falck, R. Chapot, M. Brockmann, D. Overhoff, New ultra-fast algorithm for cochlear implant misalignment detection, European Journal of Radiology 151 (2022) 110283.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
K. Chen, Q. Li, W. Li, H. Lau, A. Ruys, P. Carter, Three-dimensional finite element modeling of cochlear implant induced electrical current flows, in: 2009 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications, IEEE, 2009, pp. 5–7.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
F. A. Reda, T. R. McRackan, R. F. Labadie, B. M. Dawant, J. H. Noble, Automatic segmentation of intra-cochlear anatomy in post-implantation ct of unilateral cochlear implant recipients, Medical image analysis 18 (3) (2014) 605–615.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
S. S. Moudgalya, N. D. Cahill, D. A. Borkholder, Deep volumetric segmentation of murine cochlear compartments from micro-computed tomography images, in: 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC), IEEE, 2020, pp. 1970–1975.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
J. Wang, J. H. Noble, B. M. Dawant, Metal artifact reduction and intra cochlear anatomy segmentation inct images of the ear with a multi-resolution multi-task 3d network, in: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), IEEE, 2020, pp. 596–599.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. M. Kuczapski, A. Stanciu, Assistive tool for cochlear implant fitting: Estimation and monitoring of the effective stimulation thresholds, in: 2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics, IEEE, 2015, pp. 307–311.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
A. Botros, B. van Dijk, M. Killian, Autonrt™: An automated system that measures ecap thresholds with the nucleus® freedom™ cochlear implant via machine intelligence, Artificial Intelligence in Medicine 40 (1) (2007) 15–28.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
J. H. Noble, B. M. Dawant, Automatic graph-based localization of cochlear implant electrodes in CT, in: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part II 18, Springer, 2015, pp. 152–159.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Y. Zhao, B. M. Dawant, R. F. Labadie, J. H. Noble, Automatic localization of cochlear implant electrodes in ct, in: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2014: 17th International Conference, Boston, MA, USA, September 14-18, 2014, Proceedings, Part I 17, Springer, 2014, pp. 331–338.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Y. Zhao, S. Chakravorti, R. F. Labadie, B. M. Dawant, J. H. Noble, Automatic graph-based method for localization of cochlear implant electrode arrays in clinical ct with sub-voxel accuracy, Medical image analysis 52 (2019) 1–12.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
I. Bermejo, F. J. Díez, P. Govaerts, B. Vaerenberg, A probabilistic graphical model for tuning cochlear implants, in: Conference on Artificial Intelligence in Medicine in Europe, Springer, 2013, pp. 150–155.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
N. Castaneda-Villa, C. James, Objective source selection in blind source separation of aeps in children with cochlear implants, in: 2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, IEEE, 2007, pp. 6223–6226.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
P. V. Incerti, T. Y. Ching, R. Cowan, The effect of cross-over frequency on binaural hearing performance of adults using electric-acoustic stimulation, Cochlear implants international 20 (4) (2019) 190–206.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
J. R. Katthi, S. Ganapathy, S. Kothinti, M. Slaney, Deep canonical correlation analysis for decoding the auditory brain, in: 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC), IEEE, 2020, pp. 3505–3508.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
T. Gajecki, W. Nogueira, A fused deep denoising sound coding strategy for bilateral cochlear implants, IEEE Transactions on Biomedical Engineering (2024).

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Y. Himeur, I. Varlamis, H. Kheddar, A. Amira, S. Atalla, Y. Singh, F. Bensaali, W. Mansoor, Federated learning for computer vision, arXiv preprint arXiv:2308.13558 (2023).

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
N. Djeffal, H. Kheddar, D. Addou, A. C. Mazari, Y. Himeur, Automatic speech recognition with bert and ctc transformers: A review, in: 2023 2nd International Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, 2023, pp. 1–8.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IC2EM59347.2023.10419784" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/IC2EM59347.2023.10419784</span></a>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
H. Kheddar, M. Hemis, Y. Himeur, Automatic speech recognition using advanced deep learning approaches: A survey (2024).

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.01255" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2403.01255</span></a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.15441" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.15442" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.15442">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.15442" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.15444" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 18:10:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
