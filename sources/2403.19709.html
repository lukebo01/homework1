<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.19709] Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models</title><meta property="og:description" content="Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstreâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.19709">

<!--Generated on Fri Apr  5 16:16:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evaluated on automatic speech recognition tasks.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold">Index Terms</span>: large pre-trained models, parameter efficient adaptation, recurrent neural networks</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">There has been a paradigm shift towards adapting a single large pre-trained model to multiple downstream tasks. Full model adaptation such as fine-tuning is expensive as the entire model specializes on a single taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Since the per-task parameter overhead becomes as large as all model weights, the full fine-tuning approach is not scalable in applications with a large number of tasks, like personalized speech recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Parameter efficient adaptation methods on the other hand focus on fine-tuning a fraction of model weights (i.e. the final dense layer before softmax) or adding a small number of task specialized parameters. There are two main categories of parameter efficient adaptation of large pre-trained models: soft-prompt tuning and the adapter methods. Adapter layers have shown better performance on a variety of tasks, thanks to its high computational capacity and more parameters. On the other hand, the soft-prompt tuning approaches offer a more flexible, efficient way to adapt and deploy large models as it is straightforward to use the soft prompt vectors for mixed-task batches during inference. However, the capability of the current prompt tuning techniques are limited by the capacity of the prompt vectors. Optimizing them via back-propagation is not a straightforward procedure. As a result, they under-perform on harder text generation tasks, like machine translation and summarizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Furthermore, it is unclear how to combine the existing soft-prompt tuning techniques with streaming speech models due to the changing input and attention window.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S1.F1.1.1.1" class="ltx_text"><img src="/html/2403.19709/assets/x1.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="135" height="101" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Hierarchical Recurrent Adapter (HRA). The yellow box indicates layers of the underlying backbone speech model. The HRA consists of a single recurrent controller and multiple task-level adapter heads. The output of the adapter head is added to the backbone feature for adaptation of downstream speech tasks. In HRA, the adapter heads and the recurrent controller weights are shared across all layers keeping the adapter parameter overhead minimal.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we focus on parameter efficient adapter methods for adaptation of large pre-trained speech models for automatic speech recognition (ASR) tasks. There is a line of works on efficient adapters, including residual adaptersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, Low-Rank Adapter (LoRA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and BitFitÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The residual adapters incorporate a 2-layer feed-forward network (FFN) as adapter for each pre-trained TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> block. The adapter can be placed in parallel or sequential to an entire block or the FFN layers within the block. It utilizes a hidden layer bottleneck to reduce the number of parameters and avoid over-fitting on a small downstream task data. Despite the simplicity, Residual Adapters have been successfully applied to many NLP, speech and vision tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a more recent adapter approach that decomposes the adapter matrix into two low-rank matrices for better parameter efficiency and learn a task-parameter difference for the downstream tasks, similar to MetaNet with Fast-Weight adaptersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In LoRA, the task specific weight matrix can be recovered by multiplying the two small decomposing matrices and the adapter matrices can be added next to any weight matrix. BitFitÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> on the other hand adds no additional parameters and fine-tunes only the bias and scaling vector terms for a new task. Another concurrently developed work is READ that applies a recurrent neural network as adapter for parameter and computation efficiencyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. READ was introduced for adaptation of Large Language Models and focuses on NLP tasks. This approach is also related to feature fusion methods that aims to provide more efficient trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The existing adapter methods were mainly developed for single or few task adaptations settings; and thus their per-task parameter overhead is high in large scale multi-task scenario. To reduce the per-task parameter overhead, we introduce a hierarchical adapter approach dubbed Hierarchical Recurrent Adapter (HRA). HRA is equipped with a recurrent controller network and a set of task-level adapter heads. The recurrent controller network is shared across all tasks while the task-level adapter head is specialized for each task. Since HRA is recurrent along the depth of the large pre-trained model, HRA parameters are shared across the layers as well. Therefore, the per-task parameter overhead becomes only task-level adapter head.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In our extensive experiment on ASR, we show that our HRA achieves better WERs with 2-8x less parameters in single-task as well as multi-task evaluations. The HRA closes the WER gap against the full fine-tuning baseline and improves further.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The contribution of this work is 3-fold. First, we show that an improved model-wise parameter efficiency is achieved by adapter recurrency. Second, this work introduces a modular adaptation model by decomposing the adapter module into controller network and task adapter heads. Finally, we achieve a better task-wise parameter efficiency via the adapter heads.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the proposed Hierarchical Recurrent Adapter consists of a single shared controller and multiple task specific adapter heads. We add one adapter head per task. Only the adapter head parameters are trained when there is new task coming in. We experiment with two simple adapter head architecture: simple linear projection and FFN heads.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The shared controller is responsible for interacting with task specialized adapter heads.
Furthermore, unlike residual adapters our HRA is shared across all layers of a pre-trained large model to keep adapter parameters small.
We provide a detailed description of each component below.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Recurrent Controller</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.4" class="ltx_p">The controller is shared for all layers of the underlying backbone model as well as tasks and is responsible for orchestrating the interaction between the backbone model and task specialized adapter heads. The controller takes in as input the activation <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">x</mi><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">ğ‘¥</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">x_{l}</annotation></semantics></math> at layer <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">l</annotation></semantics></math> of the backbone model and computes a new interaction recurrent vector <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="h_{l}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">h</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">â„</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">h_{l}</annotation></semantics></math> for task-level adapter. Since the controller is a recurrent network, it also takes in its last hidden activation <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="h_{l-1}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">h</mi><mrow id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">l</mi><mo id="S2.SS1.p1.4.m4.1.1.3.1" xref="S2.SS1.p1.4.m4.1.1.3.1.cmml">âˆ’</mo><mn id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">â„</ci><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><minus id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.1"></minus><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">ğ‘™</ci><cn type="integer" id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">h_{l-1}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We choose to parameterize our adapter controller with a lightweight recurrent network for parameter and inference efficiency. Specifically, we use IndRNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as it is computationally cheaper than the other RNN variants and admits ReLU function as its activation without a gradient explosion issue. IndRNN computes its recurrent activation <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="h_{l}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msub id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">h</mi><mi id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">â„</ci><ci id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">h_{l}</annotation></semantics></math> as:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="h_{l}=ReLU(Wx_{l}+uh_{l-1}+b)" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">h</mi><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">l</mi></msub><mo id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.4" xref="S2.E1.m1.1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2a" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.5" xref="S2.E1.m1.1.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2b" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.6" xref="S2.E1.m1.1.1.1.6.cmml">U</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2c" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.2.1" xref="S2.E1.m1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="S2.E1.m1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2.3.2" xref="S2.E1.m1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E1.m1.1.1.1.1.1.1.2.3.3" xref="S2.E1.m1.1.1.1.1.1.1.2.3.3.cmml">l</mi></msub></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><msub id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.3.2.cmml">h</mi><mrow id="S2.E1.m1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.2.cmml">l</mi><mo id="S2.E1.m1.1.1.1.1.1.1.3.3.3.1" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.1.cmml">âˆ’</mo><mn id="S2.E1.m1.1.1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1a" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.E1.m1.1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.1.4.cmml">b</mi></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"></eq><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">â„</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">ğ‘™</ci></apply><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><ci id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3">ğ‘…</ci><ci id="S2.E1.m1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.4">ğ‘’</ci><ci id="S2.E1.m1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.5">ğ¿</ci><ci id="S2.E1.m1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.6">ğ‘ˆ</ci><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><plus id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2"><times id="S2.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.2">ğ‘Š</ci><apply id="S2.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.3.2">ğ‘¥</ci><ci id="S2.E1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.3.3">ğ‘™</ci></apply></apply><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><times id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">ğ‘¢</ci><apply id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3.2">â„</ci><apply id="S2.E1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3"><minus id="S2.E1.m1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.1"></minus><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.2">ğ‘™</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></apply><ci id="S2.E1.m1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.4">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">h_{l}=ReLU(Wx_{l}+uh_{l-1}+b)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.5" class="ltx_p">where <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="x_{l}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><msub id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">x</mi><mi id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">ğ‘¥</ci><ci id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">x_{l}</annotation></semantics></math> is the RNN input feature representation extracted from the <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="l^{th}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msup id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">l</mi><mrow id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml"><mi id="S2.SS1.p3.2.m2.1.1.3.2" xref="S2.SS1.p3.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.2.m2.1.1.3.1" xref="S2.SS1.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p3.2.m2.1.1.3.3" xref="S2.SS1.p3.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">ğ‘™</ci><apply id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3"><times id="S2.SS1.p3.2.m2.1.1.3.1.cmml" xref="S2.SS1.p3.2.m2.1.1.3.1"></times><ci id="S2.SS1.p3.2.m2.1.1.3.2.cmml" xref="S2.SS1.p3.2.m2.1.1.3.2">ğ‘¡</ci><ci id="S2.SS1.p3.2.m2.1.1.3.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">l^{th}</annotation></semantics></math> layer of the backbone speech model and <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">W</annotation></semantics></math>, <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">u</annotation></semantics></math> and <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mi id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><ci id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">b</annotation></semantics></math> are input projection matrix, recurrent scaling vector and the bias term.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Task Adapter Heads</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.5" class="ltx_p">Once the new interaction recurrent vector <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="h_{l}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">h</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">â„</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">h_{l}</annotation></semantics></math> is computed as in EqÂ (<a href="#S2.E1" title="In 2.1 Recurrent Controller â€£ 2 Methods â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), we learn an adapter output <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="o_{l}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">o</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ğ‘œ</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">o_{l}</annotation></semantics></math> for backbone layer <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">l</annotation></semantics></math> by passing it through the task-level adapter head. The adapter output <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="o_{l}" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">o</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">ğ‘œ</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">o_{l}</annotation></semantics></math> is then added back to the original feature activation to obtain task-specific representation <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="x^{\prime}_{l}" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><msubsup id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2.2" xref="S2.SS2.p1.5.m5.1.1.2.2.cmml">x</mi><mi id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml">l</mi><mo id="S2.SS2.p1.5.m5.1.1.2.3" xref="S2.SS2.p1.5.m5.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><apply id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.2.1.cmml" xref="S2.SS2.p1.5.m5.1.1">superscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2.2">ğ‘¥</ci><ci id="S2.SS2.p1.5.m5.1.1.2.3.cmml" xref="S2.SS2.p1.5.m5.1.1.2.3">â€²</ci></apply><ci id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">x^{\prime}_{l}</annotation></semantics></math>:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="x^{\prime}_{l}=x_{l}+o_{l}." display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><msubsup id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.2.2.2.cmml">x</mi><mi id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.2.3.cmml">l</mi><mo id="S2.E2.m1.1.1.1.1.2.2.3" xref="S2.E2.m1.1.1.1.1.2.2.3.cmml">â€²</mo></msubsup><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><msub id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2.2" xref="S2.E2.m1.1.1.1.1.3.2.2.cmml">x</mi><mi id="S2.E2.m1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.3.2.3.cmml">l</mi></msub><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.3.3.2.cmml">o</mi><mi id="S2.E2.m1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow></mrow><mo lspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></eq><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2">subscript</csymbol><apply id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2">ğ‘¥</ci><ci id="S2.E2.m1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.2.3">â€²</ci></apply><ci id="S2.E2.m1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.3">ğ‘™</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><plus id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></plus><apply id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2">ğ‘¥</ci><ci id="S2.E2.m1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.3">ğ‘™</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2">ğ‘œ</ci><ci id="S2.E2.m1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">x^{\prime}_{l}=x_{l}+o_{l}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.2" class="ltx_p">The resulting representation <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="x^{\prime}_{l}" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><msubsup id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2.2" xref="S2.SS2.p3.1.m1.1.1.2.2.cmml">x</mi><mi id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml">l</mi><mo id="S2.SS2.p3.1.m1.1.1.2.3" xref="S2.SS2.p3.1.m1.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">subscript</csymbol><apply id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.2.1.cmml" xref="S2.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS2.p3.1.m1.1.1.2.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2.2">ğ‘¥</ci><ci id="S2.SS2.p3.1.m1.1.1.2.3.cmml" xref="S2.SS2.p3.1.m1.1.1.2.3">â€²</ci></apply><ci id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">x^{\prime}_{l}</annotation></semantics></math> is further given as input to the next backbone layer <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="l+1" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">l</mi><mo id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">+</mo><mn id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><plus id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1"></plus><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">ğ‘™</ci><cn type="integer" id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">l+1</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Similar to the controller, the task adapter head is also shared across the layers of the backbone model resulting in a compact HRA adapter for all tasks. We consider linear project matrix and a 2-layer FFN for the adapter head.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Linear Adapter Head</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.2" class="ltx_p">We can use a simple linear projection matrix as task-level memory, so to adapt to a new task we incorporate and fine-tune only a single linear projection matrix. Given the controller hidden state <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="h_{l}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><msub id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml">h</mi><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><apply id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.2">â„</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">h_{l}</annotation></semantics></math> the linear projection head then computes the output <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="o_{l}" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><msub id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.1.1.2" xref="S2.SS2.SSS1.p1.2.m2.1.1.2.cmml">o</mi><mi id="S2.SS2.SSS1.p1.2.m2.1.1.3" xref="S2.SS2.SSS1.p1.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><apply id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1.2">ğ‘œ</ci><ci id="S2.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">o_{l}</annotation></semantics></math> as:</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="o_{l}=M_{n}h_{l}" display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><msub id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.2.2" xref="S2.E3.m1.1.1.2.2.cmml">o</mi><mi id="S2.E3.m1.1.1.2.3" xref="S2.E3.m1.1.1.2.3.cmml">l</mi></msub><mo id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml"><msub id="S2.E3.m1.1.1.3.2" xref="S2.E3.m1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.3.2.2" xref="S2.E3.m1.1.1.3.2.2.cmml">M</mi><mi id="S2.E3.m1.1.1.3.2.3" xref="S2.E3.m1.1.1.3.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.3.1" xref="S2.E3.m1.1.1.3.1.cmml">â€‹</mo><msub id="S2.E3.m1.1.1.3.3" xref="S2.E3.m1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.3.3.2" xref="S2.E3.m1.1.1.3.3.2.cmml">h</mi><mi id="S2.E3.m1.1.1.3.3.3" xref="S2.E3.m1.1.1.3.3.3.cmml">l</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><eq id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"></eq><apply id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.2.2">ğ‘œ</ci><ci id="S2.E3.m1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.2.3">ğ‘™</ci></apply><apply id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3"><times id="S2.E3.m1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.3.1"></times><apply id="S2.E3.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.3.2">subscript</csymbol><ci id="S2.E3.m1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.3.2.2">ğ‘€</ci><ci id="S2.E3.m1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.3.2.3">ğ‘›</ci></apply><apply id="S2.E3.m1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.3.3">subscript</csymbol><ci id="S2.E3.m1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.3.3.2">â„</ci><ci id="S2.E3.m1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.3.3.3">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">o_{l}=M_{n}h_{l}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS1.p2.2" class="ltx_p">where <math id="S2.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="M_{n}" display="inline"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><msub id="S2.SS2.SSS1.p2.1.m1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.1.m1.1.1.2" xref="S2.SS2.SSS1.p2.1.m1.1.1.2.cmml">M</mi><mi id="S2.SS2.SSS1.p2.1.m1.1.1.3" xref="S2.SS2.SSS1.p2.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><apply id="S2.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.2">ğ‘€</ci><ci id="S2.SS2.SSS1.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">M_{n}</annotation></semantics></math> is the task-specific project matrix and <math id="S2.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.SSS1.p2.2.m2.1a"><mi id="S2.SS2.SSS1.p2.2.m2.1.1" xref="S2.SS2.SSS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m2.1b"><ci id="S2.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m2.1c">n</annotation></semantics></math> is the task index.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Feed-Forward Adapter Head</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">We can apply a 2-layer FF neural network with ReLU activation as the task-level adapter head. In this case, the adapter output is computed as:</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.5" class="ltx_Math" alttext="o_{t}=M_{2,n}ReLU(M_{1,n}h_{t})" display="block"><semantics id="S2.E4.m1.5a"><mrow id="S2.E4.m1.5.5" xref="S2.E4.m1.5.5.cmml"><msub id="S2.E4.m1.5.5.3" xref="S2.E4.m1.5.5.3.cmml"><mi id="S2.E4.m1.5.5.3.2" xref="S2.E4.m1.5.5.3.2.cmml">o</mi><mi id="S2.E4.m1.5.5.3.3" xref="S2.E4.m1.5.5.3.3.cmml">t</mi></msub><mo id="S2.E4.m1.5.5.2" xref="S2.E4.m1.5.5.2.cmml">=</mo><mrow id="S2.E4.m1.5.5.1" xref="S2.E4.m1.5.5.1.cmml"><msub id="S2.E4.m1.5.5.1.3" xref="S2.E4.m1.5.5.1.3.cmml"><mi id="S2.E4.m1.5.5.1.3.2" xref="S2.E4.m1.5.5.1.3.2.cmml">M</mi><mrow id="S2.E4.m1.2.2.2.4" xref="S2.E4.m1.2.2.2.3.cmml"><mn id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml">2</mn><mo id="S2.E4.m1.2.2.2.4.1" xref="S2.E4.m1.2.2.2.3.cmml">,</mo><mi id="S2.E4.m1.2.2.2.2" xref="S2.E4.m1.2.2.2.2.cmml">n</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.2" xref="S2.E4.m1.5.5.1.2.cmml">â€‹</mo><mi id="S2.E4.m1.5.5.1.4" xref="S2.E4.m1.5.5.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.2a" xref="S2.E4.m1.5.5.1.2.cmml">â€‹</mo><mi id="S2.E4.m1.5.5.1.5" xref="S2.E4.m1.5.5.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.2b" xref="S2.E4.m1.5.5.1.2.cmml">â€‹</mo><mi id="S2.E4.m1.5.5.1.6" xref="S2.E4.m1.5.5.1.6.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.2c" xref="S2.E4.m1.5.5.1.2.cmml">â€‹</mo><mi id="S2.E4.m1.5.5.1.7" xref="S2.E4.m1.5.5.1.7.cmml">U</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.2d" xref="S2.E4.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.5.5.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.5.5.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.5.5.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.cmml"><msub id="S2.E4.m1.5.5.1.1.1.1.2" xref="S2.E4.m1.5.5.1.1.1.1.2.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.2.2" xref="S2.E4.m1.5.5.1.1.1.1.2.2.cmml">M</mi><mrow id="S2.E4.m1.4.4.2.4" xref="S2.E4.m1.4.4.2.3.cmml"><mn id="S2.E4.m1.3.3.1.1" xref="S2.E4.m1.3.3.1.1.cmml">1</mn><mo id="S2.E4.m1.4.4.2.4.1" xref="S2.E4.m1.4.4.2.3.cmml">,</mo><mi id="S2.E4.m1.4.4.2.2" xref="S2.E4.m1.4.4.2.2.cmml">n</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.5.5.1.1.1.1.1" xref="S2.E4.m1.5.5.1.1.1.1.1.cmml">â€‹</mo><msub id="S2.E4.m1.5.5.1.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.3.cmml"><mi id="S2.E4.m1.5.5.1.1.1.1.3.2" xref="S2.E4.m1.5.5.1.1.1.1.3.2.cmml">h</mi><mi id="S2.E4.m1.5.5.1.1.1.1.3.3" xref="S2.E4.m1.5.5.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S2.E4.m1.5.5.1.1.1.3" xref="S2.E4.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.5b"><apply id="S2.E4.m1.5.5.cmml" xref="S2.E4.m1.5.5"><eq id="S2.E4.m1.5.5.2.cmml" xref="S2.E4.m1.5.5.2"></eq><apply id="S2.E4.m1.5.5.3.cmml" xref="S2.E4.m1.5.5.3"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.3.1.cmml" xref="S2.E4.m1.5.5.3">subscript</csymbol><ci id="S2.E4.m1.5.5.3.2.cmml" xref="S2.E4.m1.5.5.3.2">ğ‘œ</ci><ci id="S2.E4.m1.5.5.3.3.cmml" xref="S2.E4.m1.5.5.3.3">ğ‘¡</ci></apply><apply id="S2.E4.m1.5.5.1.cmml" xref="S2.E4.m1.5.5.1"><times id="S2.E4.m1.5.5.1.2.cmml" xref="S2.E4.m1.5.5.1.2"></times><apply id="S2.E4.m1.5.5.1.3.cmml" xref="S2.E4.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.3.1.cmml" xref="S2.E4.m1.5.5.1.3">subscript</csymbol><ci id="S2.E4.m1.5.5.1.3.2.cmml" xref="S2.E4.m1.5.5.1.3.2">ğ‘€</ci><list id="S2.E4.m1.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.4"><cn type="integer" id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1">2</cn><ci id="S2.E4.m1.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2">ğ‘›</ci></list></apply><ci id="S2.E4.m1.5.5.1.4.cmml" xref="S2.E4.m1.5.5.1.4">ğ‘…</ci><ci id="S2.E4.m1.5.5.1.5.cmml" xref="S2.E4.m1.5.5.1.5">ğ‘’</ci><ci id="S2.E4.m1.5.5.1.6.cmml" xref="S2.E4.m1.5.5.1.6">ğ¿</ci><ci id="S2.E4.m1.5.5.1.7.cmml" xref="S2.E4.m1.5.5.1.7">ğ‘ˆ</ci><apply id="S2.E4.m1.5.5.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1"><times id="S2.E4.m1.5.5.1.1.1.1.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.1"></times><apply id="S2.E4.m1.5.5.1.1.1.1.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.2.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.2.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.2.2">ğ‘€</ci><list id="S2.E4.m1.4.4.2.3.cmml" xref="S2.E4.m1.4.4.2.4"><cn type="integer" id="S2.E4.m1.3.3.1.1.cmml" xref="S2.E4.m1.3.3.1.1">1</cn><ci id="S2.E4.m1.4.4.2.2.cmml" xref="S2.E4.m1.4.4.2.2">ğ‘›</ci></list></apply><apply id="S2.E4.m1.5.5.1.1.1.1.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.5.5.1.1.1.1.3.1.cmml" xref="S2.E4.m1.5.5.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.5.5.1.1.1.1.3.2.cmml" xref="S2.E4.m1.5.5.1.1.1.1.3.2">â„</ci><ci id="S2.E4.m1.5.5.1.1.1.1.3.3.cmml" xref="S2.E4.m1.5.5.1.1.1.1.3.3">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.5c">o_{t}=M_{2,n}ReLU(M_{1,n}h_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS2.p2.3" class="ltx_p">where <math id="S2.SS2.SSS2.p2.1.m1.2" class="ltx_Math" alttext="M_{2,n}" display="inline"><semantics id="S2.SS2.SSS2.p2.1.m1.2a"><msub id="S2.SS2.SSS2.p2.1.m1.2.3" xref="S2.SS2.SSS2.p2.1.m1.2.3.cmml"><mi id="S2.SS2.SSS2.p2.1.m1.2.3.2" xref="S2.SS2.SSS2.p2.1.m1.2.3.2.cmml">M</mi><mrow id="S2.SS2.SSS2.p2.1.m1.2.2.2.4" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml"><mn id="S2.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.1.cmml">2</mn><mo id="S2.SS2.SSS2.p2.1.m1.2.2.2.4.1" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS2.p2.1.m1.2.2.2.2" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.2b"><apply id="S2.SS2.SSS2.p2.1.m1.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3">subscript</csymbol><ci id="S2.SS2.SSS2.p2.1.m1.2.3.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.3.2">ğ‘€</ci><list id="S2.SS2.SSS2.p2.1.m1.2.2.2.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.4"><cn type="integer" id="S2.SS2.SSS2.p2.1.m1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.1">2</cn><ci id="S2.SS2.SSS2.p2.1.m1.2.2.2.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.2.2.2.2">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.2c">M_{2,n}</annotation></semantics></math> and <math id="S2.SS2.SSS2.p2.2.m2.2" class="ltx_Math" alttext="M_{1,n}" display="inline"><semantics id="S2.SS2.SSS2.p2.2.m2.2a"><msub id="S2.SS2.SSS2.p2.2.m2.2.3" xref="S2.SS2.SSS2.p2.2.m2.2.3.cmml"><mi id="S2.SS2.SSS2.p2.2.m2.2.3.2" xref="S2.SS2.SSS2.p2.2.m2.2.3.2.cmml">M</mi><mrow id="S2.SS2.SSS2.p2.2.m2.2.2.2.4" xref="S2.SS2.SSS2.p2.2.m2.2.2.2.3.cmml"><mn id="S2.SS2.SSS2.p2.2.m2.1.1.1.1" xref="S2.SS2.SSS2.p2.2.m2.1.1.1.1.cmml">1</mn><mo id="S2.SS2.SSS2.p2.2.m2.2.2.2.4.1" xref="S2.SS2.SSS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS2.p2.2.m2.2.2.2.2" xref="S2.SS2.SSS2.p2.2.m2.2.2.2.2.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.m2.2b"><apply id="S2.SS2.SSS2.p2.2.m2.2.3.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.2.m2.2.3.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.3">subscript</csymbol><ci id="S2.SS2.SSS2.p2.2.m2.2.3.2.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.3.2">ğ‘€</ci><list id="S2.SS2.SSS2.p2.2.m2.2.2.2.3.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.2.2.4"><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1.1.1">1</cn><ci id="S2.SS2.SSS2.p2.2.m2.2.2.2.2.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.2.2.2">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.m2.2c">M_{1,n}</annotation></semantics></math> are the task-level head weights for the <math id="S2.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="n^{th}" display="inline"><semantics id="S2.SS2.SSS2.p2.3.m3.1a"><msup id="S2.SS2.SSS2.p2.3.m3.1.1" xref="S2.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.SSS2.p2.3.m3.1.1.2" xref="S2.SS2.SSS2.p2.3.m3.1.1.2.cmml">n</mi><mrow id="S2.SS2.SSS2.p2.3.m3.1.1.3" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.cmml"><mi id="S2.SS2.SSS2.p2.3.m3.1.1.3.2" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.3.m3.1.1.3.1" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.3.m3.1.1.3.3" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.3.m3.1b"><apply id="S2.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1">superscript</csymbol><ci id="S2.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.2">ğ‘›</ci><apply id="S2.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.3"><times id="S2.SS2.SSS2.p2.3.m3.1.1.3.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.1"></times><ci id="S2.SS2.SSS2.p2.3.m3.1.1.3.2.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.2">ğ‘¡</ci><ci id="S2.SS2.SSS2.p2.3.m3.1.1.3.3.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.3.m3.1c">n^{th}</annotation></semantics></math> task.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We run two sets of experiments. One focuses on the evaluation of single-task adaptation performance of our proposed HRA adapters and the other is on the multi-task adaptation scenario. For the single-task evaluation, we used a multi-domain corpora as training and voice-search (VS) dataset as test. We also evaluated each model on a harder VS test set with proper nouns like person names.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For the multi-task setup, we use
Euphonia corpora
, atypical speech dataset consisting of over 1 million utterance recordings of over 1000 anonymized speakers with different types and severity levels of speech impairments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pre-trained Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We started with a pre-trained Universal Speech Model (USM) Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This model has Â 2 billion parameters and was pre-trained with the BEST-RQ objectiveÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> on large unlabeled multilingual corpora of 12 million hours covering over 300 languages.
We then apply different adapter techniques to the pre-trained USM model for adaptation of ASR tasks.
The adapter methods as well as full model fine-tuning baseline are trained by using the CTC lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for ASR.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Datasets</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">All collected experimental data sets adhere to the Privacy Principles inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and AI Principles inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Multi-domain Corpora</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The multi-domain corpora was used to train the adapter parameters in single-task evaluation experiments. ItÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> consists of anonymized English utterances from domains including voice search, far-field and long-form. The speech transcripts contain a mix of human-transcribed labels and machine-transcribed labels produced by teacher ASR modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Euphonia corpora</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We carefully select 128 speakers with speech impairments from the dysarthric speech [name anonymized for blind review purposes] corpus
Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
, including speakers with ALS, Down-Syndrome, Cerebral Palsy, Parkinsonâ€™s Stroke, and other etiologies. Recording text prompts consists of a variety of domains, such as caregiver phrases, conversational sentences, movie quotes, and assistant phrases. We split 80% for train, 10% for cross-validation and 10% for test on each speaker based on transcript, and there is no transcript overlapping among train set, cross-validation set, and test set. Speaker identifiers are provided along with each data utterance. We separate the test set into 128 sub sets so that each one only consists of one speaker for evaluation purposes.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Single-task adaptation WER results on voice-search (VS) and voice-search with proper nouns (VS w. PN) test sets. # Params. row shows the number of adapter parameters. Our FFN Head HRA outperforms the full fine-tuning baseline at 12.8M parameters.
</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.3.1.1.1.1" class="ltx_text">Model</span></th>
<td id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"># Params.</td>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">VS</td>
<td id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">VS w. PN</td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<th id="S4.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T1.3.2.2.1.1" class="ltx_text">Full Fine-tuning</span></th>
<td id="S4.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">1.8B</td>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_tt">5.3</td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">15.7</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.3.3.3.1.1" class="ltx_text">BitFit</span></th>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">1.3M</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">6.6</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">18.4</td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<th id="S4.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T1.3.4.4.1.1" class="ltx_text">LoRA</span></th>
<td id="S4.T1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_t">2.0M</td>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t">7.5</td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t">19.9</td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<td id="S4.T1.3.5.5.1" class="ltx_td ltx_align_center">4.0M</td>
<td id="S4.T1.3.5.5.2" class="ltx_td ltx_align_center">6.8</td>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_center">19.0</td>
</tr>
<tr id="S4.T1.3.6.6" class="ltx_tr">
<td id="S4.T1.3.6.6.1" class="ltx_td ltx_align_center">7.9M</td>
<td id="S4.T1.3.6.6.2" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T1.3.6.6.3" class="ltx_td ltx_align_center">18.0</td>
</tr>
<tr id="S4.T1.3.7.7" class="ltx_tr">
<th id="S4.T1.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T1.3.7.7.1.1" class="ltx_text">Residual Adapters</span></th>
<td id="S4.T1.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">3.2M</td>
<td id="S4.T1.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">6.3</td>
<td id="S4.T1.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">17.9</td>
</tr>
<tr id="S4.T1.3.8.8" class="ltx_tr">
<td id="S4.T1.3.8.8.1" class="ltx_td ltx_align_center">6.4M</td>
<td id="S4.T1.3.8.8.2" class="ltx_td ltx_align_center">6.2</td>
<td id="S4.T1.3.8.8.3" class="ltx_td ltx_align_center">17.1</td>
</tr>
<tr id="S4.T1.3.9.9" class="ltx_tr">
<td id="S4.T1.3.9.9.1" class="ltx_td ltx_align_center">12.7M</td>
<td id="S4.T1.3.9.9.2" class="ltx_td ltx_align_center">5.8</td>
<td id="S4.T1.3.9.9.3" class="ltx_td ltx_align_center">16.7</td>
</tr>
<tr id="S4.T1.3.10.10" class="ltx_tr">
<th id="S4.T1.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S4.T1.3.10.10.1.1" class="ltx_text">Linear Head HRA (ours)</span></th>
<td id="S4.T1.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">814K</td>
<td id="S4.T1.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">6.2</td>
<td id="S4.T1.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">17.4</td>
</tr>
<tr id="S4.T1.3.11.11" class="ltx_tr">
<td id="S4.T1.3.11.11.1" class="ltx_td ltx_align_center">6.4M</td>
<td id="S4.T1.3.11.11.2" class="ltx_td ltx_align_center">5.4</td>
<td id="S4.T1.3.11.11.3" class="ltx_td ltx_align_center">16.2</td>
</tr>
<tr id="S4.T1.3.12.12" class="ltx_tr">
<td id="S4.T1.3.12.12.1" class="ltx_td ltx_align_center">12.8M</td>
<td id="S4.T1.3.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.12.12.2.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S4.T1.3.12.12.3" class="ltx_td ltx_align_center">15.7</td>
</tr>
<tr id="S4.T1.3.13.13" class="ltx_tr">
<th id="S4.T1.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T1.3.13.13.1.1" class="ltx_text">FFN Head HRA (ours)</span></th>
<td id="S4.T1.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">1.3M</td>
<td id="S4.T1.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t">6.0</td>
<td id="S4.T1.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">17.1</td>
</tr>
<tr id="S4.T1.3.14.14" class="ltx_tr">
<td id="S4.T1.3.14.14.1" class="ltx_td ltx_align_center">13.6M</td>
<td id="S4.T1.3.14.14.2" class="ltx_td ltx_align_center">5.2</td>
<td id="S4.T1.3.14.14.3" class="ltx_td ltx_align_center">15.4</td>
</tr>
<tr id="S4.T1.3.15.15" class="ltx_tr">
<td id="S4.T1.3.15.15.1" class="ltx_td ltx_align_center ltx_border_bb">27.2M</td>
<td id="S4.T1.3.15.15.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.15.15.2.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S4.T1.3.15.15.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.15.15.3.1" class="ltx_text ltx_font_bold">15.3</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Single-task Adaptation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the WER results from our single-task adaptation experiments. Unless otherwise mentioned, all models were trained for 100K iterations.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As for the baselines, we trained a full model fine-tuning as well as other adapter techniques, such as BitFit, LoRA and Residual Adapters. For LoRA, we set its low-rank hyper-parameter to be 4, 8 and 16. We varied the Residual Adapter bottleneck dimension to be 32, 64 and 256 and recurrent dimension of HRA to 256, 2048 and 4096.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The despite the simplicity, BitFit obtains a strong WER of 6.6 on VS test set while LoRA seems to beat BitFit with a WER of 6.4 only after Â 8M parameters. The Residual Adapters on the other hand show robust results across different adapter sizes and the more adapter parameters improve the WER.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The last two sets of rows present our HRA results. Our smallest adapter - the HRA with Linear Head can achieve 6.2 WER at 814K parameters and this WER is already better than BitFit, all LoRA and smaller Residual Adapter results. This adapter matches the WER of the Residual Adapter with 6.4M parameters, showing 8x parameter efficiency. Our Linear Head HRA with 12.8M parameters already outperforms the full fine-tuning baseline and the largest FFN Head HRA further sets a new state-of-the-art WER on both test sets.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<p id="S4.F2.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S4.F2.1.1.1" class="ltx_text"><img src="/html/2403.19709/assets/x2.png" id="S4.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="208" height="128" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>Parameter efficiency of different adapter methods. Lower-left points are more parameter efficient (x-axis is truncated at 15M).</figcaption>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">In FigureÂ <a href="#S4.F2" title="Figure 2 â€£ 4.1 Single-task Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we plotted the WER against the number of adapter parameters. The lower-left points represent more parameter efficient methods as both WER and the number of parameters are lower simultaneously and we can see that HRA methods are mainly clustered around that region.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Multi-task adaptation WER results on
Euphonia
data sets. Our FFN Head HRA achieves the best WER and closes the gap against full fine-tuning baseline. FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.2 Multi-task Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that this model has a sub-linear growth in terms of the size of adapter parameters as the number of tasks increases.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.3.1.1.1.1" class="ltx_text">Model</span></th>
<td id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"># Params.</td>
<td id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Mean</td>
<td id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Median</td>
<td id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">SD</td>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<th id="S4.T2.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.3.2.2.1.1" class="ltx_text">USM Basemodel</span></th>
<td id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_tt">31.5</td>
<td id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">21.8</td>
<td id="S4.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">28.6</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.3.3.3.1.1" class="ltx_text">Full Fine-tuning</span></th>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">232B</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">9.3</td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">5.4</td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">11.1</td>
</tr>
<tr id="S4.T2.3.4.4" class="ltx_tr">
<th id="S4.T2.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T2.3.4.4.1.1" class="ltx_text">LoRA</span></th>
<td id="S4.T2.3.4.4.2" class="ltx_td ltx_align_center ltx_border_t">201M</td>
<td id="S4.T2.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t">10.9</td>
<td id="S4.T2.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t">6.6</td>
<td id="S4.T2.3.4.4.5" class="ltx_td ltx_align_center ltx_border_t">12.4</td>
</tr>
<tr id="S4.T2.3.5.5" class="ltx_tr">
<td id="S4.T2.3.5.5.1" class="ltx_td ltx_align_center">403M</td>
<td id="S4.T2.3.5.5.2" class="ltx_td ltx_align_center">10.9</td>
<td id="S4.T2.3.5.5.3" class="ltx_td ltx_align_center">7.4</td>
<td id="S4.T2.3.5.5.4" class="ltx_td ltx_align_center">11.6</td>
</tr>
<tr id="S4.T2.3.6.6" class="ltx_tr">
<td id="S4.T2.3.6.6.1" class="ltx_td ltx_align_center">805M</td>
<td id="S4.T2.3.6.6.2" class="ltx_td ltx_align_center">12.4</td>
<td id="S4.T2.3.6.6.3" class="ltx_td ltx_align_center">6.9</td>
<td id="S4.T2.3.6.6.4" class="ltx_td ltx_align_center">15.8</td>
</tr>
<tr id="S4.T2.3.7.7" class="ltx_tr">
<th id="S4.T2.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S4.T2.3.7.7.1.1" class="ltx_text">Residual Adapters</span></th>
<td id="S4.T2.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t">410M</td>
<td id="S4.T2.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t">10.2</td>
<td id="S4.T2.3.7.7.4" class="ltx_td ltx_align_center ltx_border_t">6.1</td>
<td id="S4.T2.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t">11.6</td>
</tr>
<tr id="S4.T2.3.8.8" class="ltx_tr">
<td id="S4.T2.3.8.8.1" class="ltx_td ltx_align_center">819M</td>
<td id="S4.T2.3.8.8.2" class="ltx_td ltx_align_center">10.2</td>
<td id="S4.T2.3.8.8.3" class="ltx_td ltx_align_center">6.1</td>
<td id="S4.T2.3.8.8.4" class="ltx_td ltx_align_center">11.2</td>
</tr>
<tr id="S4.T2.3.9.9" class="ltx_tr">
<td id="S4.T2.3.9.9.1" class="ltx_td ltx_align_center">1.6B</td>
<td id="S4.T2.3.9.9.2" class="ltx_td ltx_align_center">10.1</td>
<td id="S4.T2.3.9.9.3" class="ltx_td ltx_align_center">6.2</td>
<td id="S4.T2.3.9.9.4" class="ltx_td ltx_align_center">11.0</td>
</tr>
<tr id="S4.T2.3.10.10" class="ltx_tr">
<th id="S4.T2.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S4.T2.3.10.10.1.1" class="ltx_text">Linear Head HRA</span></th>
<td id="S4.T2.3.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">51M</td>
<td id="S4.T2.3.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">14.6</td>
<td id="S4.T2.3.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">9.7</td>
<td id="S4.T2.3.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">14.2</td>
</tr>
<tr id="S4.T2.3.11.11" class="ltx_tr">
<td id="S4.T2.3.11.11.1" class="ltx_td ltx_align_center">102M</td>
<td id="S4.T2.3.11.11.2" class="ltx_td ltx_align_center">14.5</td>
<td id="S4.T2.3.11.11.3" class="ltx_td ltx_align_center">9.9</td>
<td id="S4.T2.3.11.11.4" class="ltx_td ltx_align_center">13.1</td>
</tr>
<tr id="S4.T2.3.12.12" class="ltx_tr">
<td id="S4.T2.3.12.12.1" class="ltx_td ltx_align_center">203M</td>
<td id="S4.T2.3.12.12.2" class="ltx_td ltx_align_center">16.1</td>
<td id="S4.T2.3.12.12.3" class="ltx_td ltx_align_center">12.0</td>
<td id="S4.T2.3.12.12.4" class="ltx_td ltx_align_center">12.1</td>
</tr>
<tr id="S4.T2.3.13.13" class="ltx_tr">
<th id="S4.T2.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T2.3.13.13.1.1" class="ltx_text">FFN Head HRA</span></th>
<td id="S4.T2.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t">201M</td>
<td id="S4.T2.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.13.13.3.1" class="ltx_text ltx_font_bold">9.9</span></td>
<td id="S4.T2.3.13.13.4" class="ltx_td ltx_align_center ltx_border_t">6.3</td>
<td id="S4.T2.3.13.13.5" class="ltx_td ltx_align_center ltx_border_t">11.2</td>
</tr>
<tr id="S4.T2.3.14.14" class="ltx_tr">
<td id="S4.T2.3.14.14.1" class="ltx_td ltx_align_center">403M</td>
<td id="S4.T2.3.14.14.2" class="ltx_td ltx_align_center">10.2</td>
<td id="S4.T2.3.14.14.3" class="ltx_td ltx_align_center">6.1</td>
<td id="S4.T2.3.14.14.4" class="ltx_td ltx_align_center">11.8</td>
</tr>
<tr id="S4.T2.3.15.15" class="ltx_tr">
<td id="S4.T2.3.15.15.1" class="ltx_td ltx_align_center ltx_border_bb">806M</td>
<td id="S4.T2.3.15.15.2" class="ltx_td ltx_align_center ltx_border_bb">10.4</td>
<td id="S4.T2.3.15.15.3" class="ltx_td ltx_align_center ltx_border_bb">6.2</td>
<td id="S4.T2.3.15.15.4" class="ltx_td ltx_align_center ltx_border_bb">11.3</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-task Adaptation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.1 Single-task Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports the WER results from our multi task adaptation experiments. We build golden baseline from USM model with full model fine-tuning on each speaker respectively, and each model is fine-tuned with data from its corresponding speaker only. For the adapter configurations, we parameterize adapters by a speaker-id and learnable one-hot embedding. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we introduce one-hot-embedding lookup table with entries through one-on-one mapping to corresponding speakers. During training, we randomly select data samples from the 128 speakers in each batch. The recurrent controller network is shared across all 128 speakers while a separate adapter head is inserted for each speaker for specialization. For adapter baseline, we choose to experiment with LoRA and Residual Adapters since it showed a promising performance in the single-task adaptation setup (SectionÂ <a href="#S4.SS1" title="4.1 Single-task Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S4.F3.1.1.1" class="ltx_text"><img src="/html/2403.19709/assets/x3.png" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="128" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text ltx_font_bold">Fig.Â 3</span>: </span>Our HRA is more parameter efficient with increasing number of tasks while obtaining improved WER performance. Depending on the adapter size, some adapters have sub-linear trend in parameter efficiency.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">One major advantages of using the one-hot-embedding is that most of the trainable adapter parameters are independent across speakers, resulting in 128 times training throughput efficiency for multi-task adapter experiments. We observe the FFN Head HRA with 201M total parameters achieves the best WER when compared against Residual Adapter, even more close to the golden baseline (full model fine-tuning).</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.2 Multi-task Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the growth rate of the model size when the number of tasks increase. It is observed that FFN Head HRA has a sub-linear growth in terms of the size of adapter parameters with an increasing number of tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Online Adaptation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Online Adaptation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports the WER results from our multi task adaptation experiments with and without pre-trained controller. We hand picked an extra 128
Euphonia
speaker data as out-of-domain data with respect to the in-domain 128
Euphonia
speaker data mentioned above. We divide the training into two steps. First step, we pre-train the recurrent controller network with out-of-domain data. Second step, we freeze the recurrent controller network, use in-domain data to train the adapter head with random initialization. So the number of actual training parameter is reduced in this setup as we only train the adapter head. Furthermore, this approach provides a solution for sensitive data sets that cannot be trained within one model. If we pre-train the recurrent controller network only on non-Personal Identifiable Information (PII) data, and parameterize the adapter head by speaker, then no speaker needs to share tuning parameters with others.
</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Online adaptation WER results on
Euphonia
data sets. Our FFN Head HRA (S) with pre-trained controller achieves comparable results against the regular setup (only 0.2% WER loss). Paired T-Test shows no statistically significant difference between with and without pre-trained controller.</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Params.</th>
<th id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Mean</th>
<th id="S4.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Paired T-Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.2.1" class="ltx_tr">
<td id="S4.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="3"><span id="S4.T3.3.2.1.1.1" class="ltx_text">Linear Head HRA</span></td>
<td id="S4.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">51M</td>
<td id="S4.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">10.6</td>
<td id="S4.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">-</td>
</tr>
<tr id="S4.T3.3.3.2" class="ltx_tr">
<td id="S4.T3.3.3.2.1" class="ltx_td ltx_align_center">102M</td>
<td id="S4.T3.3.3.2.2" class="ltx_td ltx_align_center">10.9</td>
<td id="S4.T3.3.3.2.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.3.4.3" class="ltx_tr">
<td id="S4.T3.3.4.3.1" class="ltx_td ltx_align_center">203M</td>
<td id="S4.T3.3.4.3.2" class="ltx_td ltx_align_center">11.0</td>
<td id="S4.T3.3.4.3.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.3.5.4" class="ltx_tr">
<td id="S4.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S4.T3.3.5.4.1.1" class="ltx_text">FFN Head HRA</span></td>
<td id="S4.T3.3.5.4.2" class="ltx_td ltx_align_center ltx_border_t">201M</td>
<td id="S4.T3.3.5.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.5.4.3.1" class="ltx_text ltx_font_bold">9.9</span></td>
<td id="S4.T3.3.5.4.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.3.6.5" class="ltx_tr">
<td id="S4.T3.3.6.5.1" class="ltx_td ltx_align_center">403M</td>
<td id="S4.T3.3.6.5.2" class="ltx_td ltx_align_center">10.2</td>
<td id="S4.T3.3.6.5.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.3.7.6" class="ltx_tr">
<td id="S4.T3.3.7.6.1" class="ltx_td ltx_align_center">806M</td>
<td id="S4.T3.3.7.6.2" class="ltx_td ltx_align_center">10.4</td>
<td id="S4.T3.3.7.6.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.3.8.7" class="ltx_tr">
<td id="S4.T3.3.8.7.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="3"><span id="S4.T3.3.8.7.1.1" class="ltx_text">
<span id="S4.T3.3.8.7.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T3.3.8.7.1.1.1.1" class="ltx_p">Linear Head HRA</span>
<span id="S4.T3.3.8.7.1.1.1.2" class="ltx_p">(w/ pre-trained</span>
<span id="S4.T3.3.8.7.1.1.1.3" class="ltx_p">controller)</span>
</span></span></td>
<td id="S4.T3.3.8.7.2" class="ltx_td ltx_align_center ltx_border_tt">51M</td>
<td id="S4.T3.3.8.7.3" class="ltx_td ltx_align_center ltx_border_tt">10.7</td>
<td id="S4.T3.3.8.7.4" class="ltx_td ltx_align_center ltx_border_tt">0.59</td>
</tr>
<tr id="S4.T3.3.9.8" class="ltx_tr">
<td id="S4.T3.3.9.8.1" class="ltx_td ltx_align_center">101M</td>
<td id="S4.T3.3.9.8.2" class="ltx_td ltx_align_center">11.0</td>
<td id="S4.T3.3.9.8.3" class="ltx_td ltx_align_center">0.25</td>
</tr>
<tr id="S4.T3.3.10.9" class="ltx_tr">
<td id="S4.T3.3.10.9.1" class="ltx_td ltx_align_center">202M</td>
<td id="S4.T3.3.10.9.2" class="ltx_td ltx_align_center">11.3</td>
<td id="S4.T3.3.10.9.3" class="ltx_td ltx_align_center">0.03</td>
</tr>
<tr id="S4.T3.3.11.10" class="ltx_tr">
<td id="S4.T3.3.11.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T3.3.11.10.1.1" class="ltx_text">
<span id="S4.T3.3.11.10.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T3.3.11.10.1.1.1.1" class="ltx_p">FFN Head HRA</span>
<span id="S4.T3.3.11.10.1.1.1.2" class="ltx_p">(w/ pre-trained</span>
<span id="S4.T3.3.11.10.1.1.1.3" class="ltx_p">controller)</span>
</span></span></td>
<td id="S4.T3.3.11.10.2" class="ltx_td ltx_align_center ltx_border_t">118M</td>
<td id="S4.T3.3.11.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.11.10.3.1" class="ltx_text ltx_font_bold">10.1</span></td>
<td id="S4.T3.3.11.10.4" class="ltx_td ltx_align_center ltx_border_t">0.17</td>
</tr>
<tr id="S4.T3.3.12.11" class="ltx_tr">
<td id="S4.T3.3.12.11.1" class="ltx_td ltx_align_center">269M</td>
<td id="S4.T3.3.12.11.2" class="ltx_td ltx_align_center">10.3</td>
<td id="S4.T3.3.12.11.3" class="ltx_td ltx_align_center">0.14</td>
</tr>
<tr id="S4.T3.3.13.12" class="ltx_tr">
<td id="S4.T3.3.13.12.1" class="ltx_td ltx_align_center ltx_border_bb">672M</td>
<td id="S4.T3.3.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">10.5</td>
<td id="S4.T3.3.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">0.22</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Model Ablation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Our Linear Head HRA is structurally similar to Residual Adapters. We can obtain Residual Adapters with shared weights by removing the recurrent states of the RNN controller and then further by unshared the weights, we recover the original Residual adapters. In TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.4 Model Ablation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we listed the performance for each of the model variants. Removing the recurrent state resulted in a small regression in WER while unshared weights on top of it improved performance but now the number of parameters is more than 100M.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">We have also performed an ablation on controller RNN architecture. In addition to the IndRNN, we run benchmarks on the standard RNN with <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.1.m1.1.1.1a" xref="S4.SS4.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.1.m1.1.1.4" xref="S4.SS4.p2.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.1.m1.1.1.1b" xref="S4.SS4.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.1.m1.1.1.5" xref="S4.SS4.p2.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><times id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1"></times><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">ğ‘¡</ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">ğ‘</ci><ci id="S4.SS4.p2.1.m1.1.1.4.cmml" xref="S4.SS4.p2.1.m1.1.1.4">ğ‘›</ci><ci id="S4.SS4.p2.1.m1.1.1.5.cmml" xref="S4.SS4.p2.1.m1.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">tanh</annotation></semantics></math> activation and Light GRUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as controller. The results are summarized in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4 Model Ablation â€£ 4 Results â€£ Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. IndRNN and Light GRU both are competitive whereas the RNN with <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.2.m2.1.1.1a" xref="S4.SS4.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.2.m2.1.1.4" xref="S4.SS4.p2.2.m2.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.2.m2.1.1.1b" xref="S4.SS4.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p2.2.m2.1.1.5" xref="S4.SS4.p2.2.m2.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><times id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1"></times><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">ğ‘¡</ci><ci id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">ğ‘</ci><ci id="S4.SS4.p2.2.m2.1.1.4.cmml" xref="S4.SS4.p2.2.m2.1.1.4">ğ‘›</ci><ci id="S4.SS4.p2.2.m2.1.1.5.cmml" xref="S4.SS4.p2.2.m2.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">tanh</annotation></semantics></math> activation underperformed. This confirms that the choice of controller architecture is crucial in our HRA adapters.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Linear Head HRA ablation results.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model variant</th>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Params.</th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VS</th>
<th id="S4.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VS w. PN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.2.1" class="ltx_tr">
<th id="S4.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Linear Head HRA</th>
<td id="S4.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">3.2M</td>
<td id="S4.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">5.7</td>
<td id="S4.T4.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">16.7</td>
</tr>
<tr id="S4.T4.3.3.2" class="ltx_tr">
<th id="S4.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">- Recurrent state</th>
<td id="S4.T4.3.3.2.2" class="ltx_td ltx_align_center">3.2M</td>
<td id="S4.T4.3.3.2.3" class="ltx_td ltx_align_center">5.9</td>
<td id="S4.T4.3.3.2.4" class="ltx_td ltx_align_center">16.8</td>
</tr>
<tr id="S4.T4.3.4.3" class="ltx_tr">
<th id="S4.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Â Â Â Â Â Â  - Weight unshared</th>
<td id="S4.T4.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">102.4M</td>
<td id="S4.T4.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">5.3</td>
<td id="S4.T4.3.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">15.5</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>Recurrent controller ablation results.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Controller variant</th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Params.</th>
<th id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VS</th>
<th id="S4.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VS w. PN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.2.1" class="ltx_tr">
<th id="S4.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">IndRNN</th>
<td id="S4.T5.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">1.6M</td>
<td id="S4.T5.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">6.0</td>
<td id="S4.T5.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">16.9</td>
</tr>
<tr id="S4.T5.3.3.2" class="ltx_tr">
<th id="S4.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RNN</th>
<td id="S4.T5.3.3.2.2" class="ltx_td ltx_align_center">1.9M</td>
<td id="S4.T5.3.3.2.3" class="ltx_td ltx_align_center">6.1</td>
<td id="S4.T5.3.3.2.4" class="ltx_td ltx_align_center">17.1</td>
</tr>
<tr id="S4.T5.3.4.3" class="ltx_tr">
<th id="S4.T5.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Light GRU</th>
<td id="S4.T5.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">2.4</td>
<td id="S4.T5.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">6.0</td>
<td id="S4.T5.3.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">16.9</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented Hierarchical Recurrent Adapters (HRA).
By defining a concept of task-level adapter head in HRA, we allocate a shared single adapter controller for all tasks while allowing an individual adapter head to specialize for a new task. This reduces the per-task adapter parameter overhead and enables more efficient adaptation training and inference. The proposed HRA demonstrated better WERs with 2-8x less parameters in single as well as multi-task evaluations. Furthermore, The HRA outperformed the full fine-tuning baseline, at only 12.8M parameters.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,

</span>
<span class="ltx_bibblock">â€œBERT: Pre-training of deep bidirectional transformers for
language understanding,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, Minneapolis, Minnesota, June
2019, pp. 4171â€“4186, Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
KheÂ Chai Sim, Angad Chandorkar, Fan Gao, Mason Chua, Tsendsuren Munkhdalai, and
FranÃ§oise Beaufays,

</span>
<span class="ltx_bibblock">â€œRobust continuous on-device personalization for automatic speech
recognition.,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2021, pp. 1284â€“1288.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Golan Pundak, Tsendsuren Munkhdalai, and KheÂ Chai Sim,

</span>
<span class="ltx_bibblock">â€œOn-the-fly asr corrections with audio exemplars,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2022</span>, pp. 3148â€“3152, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tsendsuren Munkhdalai, Zelin Wu, Golan Pundak, KheÂ Chai Sim, Jiayang Li, Pat
Rondon, and TaraÂ N Sainath,

</span>
<span class="ltx_bibblock">â€œNam+: Towards scalable end-to-end contextual biasing for adaptive
asr,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</span>. IEEE,
2023, pp. 190â€“196.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen,
Nanning Zheng, and Jian-Guang Lou,

</span>
<span class="ltx_bibblock">â€œInput-tuning: Adapting unfamiliar inputs to frozen pretrained
models,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.03131</span>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi,

</span>
<span class="ltx_bibblock">â€œLearning multiple visual domains with residual adapters,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 30,
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, LuÂ Wang, and Weizhu Chen,

</span>
<span class="ltx_bibblock">â€œLora: Low-rank adaptation of large language models,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
EladÂ Ben Zaken, Shauli Ravfogel, and Yoav Goldberg,

</span>
<span class="ltx_bibblock">â€œBitfit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.10199</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, <span id="bib.bib9.1.1" class="ltx_text ltx_font_caligraphic">L</span>ukasz Kaiser, and Illia Polosukhin,

</span>
<span class="ltx_bibblock">â€œAttention is all you need,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 30,
2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, YuÂ Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,

</span>
<span class="ltx_bibblock">â€œConformer: Convolution-augmented transformer for speech
recognition,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2020</span>, 2020, pp. 5036â€“5040.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Katrin Tomanek, Vicky Zayats, Dirk Padfield, Kara Vaillancourt, and Fadi
Biadsy,

</span>
<span class="ltx_bibblock">â€œResidual adapters for parameter-efficient asr adaptation to
atypical and accented speech,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.06952</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Qiujia Li, BoÂ Li, Dongseong Hwang, TaraÂ N Sainath, and PedroÂ M Mengibar,

</span>
<span class="ltx_bibblock">â€œModular domain adaptation for conformer-based streaming asr,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13408</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tsendsuren Munkhdalai and Hong Yu,

</span>
<span class="ltx_bibblock">â€œMeta networks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>. PMLR, 2017,
pp. 2554â€“2563.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tsendsuren Munkhdalai,

</span>
<span class="ltx_bibblock">â€œSparse meta networks for sequential adaptation and its application
to adaptive language modelling,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.01803</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Sid Wang, John Nguyen, KeÂ Li, and Carole-Jean Wu,

</span>
<span class="ltx_bibblock">â€œRead: Recurrent adaptation of large transformers,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.15348</span>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhouyuan Huo, KheÂ Chai Sim, BoÂ Li, Dongseong Hwang, TaraÂ N Sainath, and Trevor
Strohman,

</span>
<span class="ltx_bibblock">â€œResource-efficient transfer learning from speech foundation model
using hierarchical feature fusion,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shuai Li, Wanqing Li, Chris Cook, and Yanbo Gao,

</span>
<span class="ltx_bibblock">â€œDeep independently recurrent neural network (indrnn),â€

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.06251</span>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
YuÂ Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin
Chen, BoÂ Li, Vera Axelrod, Gary Wang, etÂ al.,

</span>
<span class="ltx_bibblock">â€œGoogle usm: Scaling automatic speech recognition beyond 100
languages,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.01037</span>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Chung-Cheng Chiu, James Qin, YuÂ Zhang, Jiahui Yu, and Yonghui Wu,

</span>
<span class="ltx_bibblock">â€œSelf-supervised learning with random-projection quantizer for
speech recognition,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>. PMLR, 2022,
pp. 3915â€“3924.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen
Schmidhuber,

</span>
<span class="ltx_bibblock">â€œConnectionist temporal classification: labelling unsegmented
sequence data with recurrent neural networks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd international conference on Machine
learning</span>, 2006, pp. 369â€“376.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
â€œGoogleâ€™s privacy principles,â€
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://googleblog.blogspot.com/2010/01/googles-privacy-principles.html</span>,

</span>
<span class="ltx_bibblock">Accessed: 2023-03-01.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
â€œArtificial intelligence at Google: Our principles,â€
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.google/principles</span>,

</span>
<span class="ltx_bibblock">Accessed: 2023-03-01.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Arun Narayanan, Ananya Misra, KheÂ Chai Sim, Golan Pundak, Anshuman Tripathi,
Mohamed Elfeky, Parisa Haghani, Trevor Strohman, and Michiel Bacchiani,

</span>
<span class="ltx_bibblock">â€œToward domain-invariant speech recognition via large scale
training,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2018 IEEE Spoken Language Technology Workshop (SLT)</span>, 2018,
pp. 441â€“447.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Dongseong Hwang, KheÂ Chai Sim, Zhouyuan Huo, and Trevor Strohman,

</span>
<span class="ltx_bibblock">â€œPseudo Label Is Better Than Human Label,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2022</span>, 2022, pp. 1421â€“1425.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Bob MacDonald, Pan-Pan Jiang, Julie Cattiau, Rus Heywood, Richard Cave, Katie
Seaver, Marilyn Ladewig, Jimmy Tobin, Michael Brenner, PhilipÂ Q Nelson,
etÂ al.,

</span>
<span class="ltx_bibblock">â€œDisordered speech data collection: lessons learned at 1 million
utterances from project euphonia,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2021</span>, pp. 4843â€“4847, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Fadi Biadsy, Youzheng Chen, Xia Zhang, Oleg Rybakov, Andrew Rosenberg, and
PedroÂ J Moreno,

</span>
<span class="ltx_bibblock">â€œA scalable model specialization framework for training and
inference using submodels and its application to speech model
personalization,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2022</span>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Mirco Ravanelli, Philemon Brakel, Maurizio Omologo, and Yoshua Bengio,

</span>
<span class="ltx_bibblock">â€œLight gated recurrent units for speech recognition,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Emerging Topics in Computational
Intelligence</span>, vol. 2, no. 2, pp. 92â€“102, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.19708" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.19709" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.19709">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.19709" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.19710" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:16:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
