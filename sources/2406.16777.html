<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.16777] Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024</title><meta property="og:description" content="Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we pre…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.16777">

<!--Generated on Fri Jul  5 21:49:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id3.1.id1" class="ltx_text ltx_font_bold">Sai Koneru</span>,
<span id="id4.2.id2" class="ltx_text ltx_font_bold">Thai-Binh Nguyen</span>,
<span id="id5.3.id3" class="ltx_text ltx_font_bold">Ngoc-Quan Pham</span>,
<span id="id6.4.id4" class="ltx_text ltx_font_bold">Danni Liu</span>,
<span id="id7.5.id5" class="ltx_text ltx_font_bold">Zhaolin Li</span>,

<br class="ltx_break"><span id="id8.6.id6" class="ltx_text ltx_font_bold">Alexander Waibel</span>,
<span id="id9.7.id7" class="ltx_text ltx_font_bold">Jan Niehues</span>

<br class="ltx_break">
<br class="ltx_break">Karlsruhe Institute of Technology

<br class="ltx_break">
<br class="ltx_break"><a href="mailto:email@domain" title="" class="ltx_ref ltx_href">firstname.lastname@kit.edu</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT), and even End-to-End Speech Translation (ST). In this paper, we present KIT’s offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded speech translation. Specifically, we integrate Mistral-7B<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>mistralai/Mistral-7B-Instruct-v0.1</span></span></span> into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of <math id="id1.1.m1.1" class="ltx_Math" alttext="0.3\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">0.3</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="float" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">0.3\%</annotation></semantics></math> in Word Error Rate and <math id="id2.2.m2.1" class="ltx_Math" alttext="0.65\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">0.65</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn type="float" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">0.65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">0.65\%</annotation></semantics></math> in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">
Sai Koneru,
Thai-Binh Nguyen,
Ngoc-Quan Pham,
Danni Liu,
Zhaolin Li,</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Alexander Waibel</span>,
<span id="p1.1.2.1.1.2.2.1.2" class="ltx_text ltx_font_bold">Jan Niehues</span></span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">Karlsruhe Institute of Technology</span></span>
<span id="p1.1.2.1.1.4.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.4.1" class="ltx_td ltx_align_center"><a href="mailto:email@domain" title="" class="ltx_ref ltx_href">firstname.lastname@kit.edu</a></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">This paper provides an overview of Karlsruhe Institute of Technology’s speech translation (ST) system developed for the offline track of IWSLT 2024. We participated in the constrained plus large language models (LLMs) condition, focusing on the translation direction from English to German. Under this condition, LLMs with parameters of around 7 billion are allowed, and they have proven effective in many NLP tasks. One of the interesting aspects of this condition is how one can effectively integrate them into ST systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent years, there has been a significant interest in developing several open-sourced and medium-scale LLMs <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Jiang et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>. The adaptability of LLMs to diverse tasks, using techniques such as In-Context-Learning <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> or Parameter-efficient fine-tuning with 4-bit quantization <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>); Dettmers et al. (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>, enables their exploitation even with limited resources.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With these recent advancements, exploiting LLMs for ST shows great promise and offers several potential benefits. For instance, one common challenge in Automatic Speech Recognition (ASR) is dealing with input noise, which can often render it difficult to comprehend the speaker’s words. However, LLMs, trained on vast amounts of data, may excel at predicting words compared to decoders trained solely during ASR. Moreover, LLMs possess a richer vocabulary and understanding of complex terminology that task-specific ASR systems may lack. Motivated by these advantages, various studies have explored the integration of LLMs into ASR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2024</a>; Pu et al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, Machine Translation (MT) <cite class="ltx_cite ltx_citemacro_citep">(Koneru et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, and ST <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.16777/assets/figures/asr_pe.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">ASR Refinement</span>: The ASR system generates a few candidate hypotheses with beam search, and the LLM generates a new hypothesis based on all the candidates as proposed in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>. We use the top 5 candidates in all our experiments.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite> employ the LLM to generate a new hypothesis based on the N-best list of the ASR model. This strategy relies on the observation that N-best lists tend to exhibit enough diversity, especially during uncertain conditions, allowing accurate transcript prediction by examining the list. On the other hand, for MT, <cite class="ltx_cite ltx_citemacro_citet">Koneru et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> proposes leveraging the LLM to automatically postedit translations by analyzing the source and hypothesis documents to rectify contextual errors. Both approaches are system-agnostic and have demonstrated successful enhancement of system quality. Furthermore, it is also the case that cascaded systems are shown to be superior than end to end systems in previous IWSLT findings and submissions making the leveraging of LLMs easily compatible. <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our system builds on these two approaches to effectively use the LLMs to improve the cascaded ST pipeline by refining the intermediate outputs at both ASR and MT while maintaining its modular structure. We utilize pre-trained models to create the individual components and fine-tune them with the allowed data. Specifically, we employ WavLM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> and MBART50 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> to initialize the ASR, and NLLB-200 (3.3B) <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> for the MT module. As for the LLM, we opt for Mistral 7B Instruction-Tuned <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, considering it to be the most recent model within the allowable options.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We present our main findings below:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.2" class="ltx_p">We demonstrate that LLMs can be tailored to enhance both ASR (Section <a href="#S4.SS1" title="4.1 Automatic Speech Recognition ‣ 4 Results ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and MT systems (Section <a href="#S4.SS2" title="4.2 Cascaded Speech Translation ‣ 4 Results ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), resulting in an absolute improvement of <math id="S1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="0.3\%" display="inline"><semantics id="S1.I1.i1.p1.1.m1.1a"><mrow id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml"><mn id="S1.I1.i1.p1.1.m1.1.1.2" xref="S1.I1.i1.p1.1.m1.1.1.2.cmml">0.3</mn><mo id="S1.I1.i1.p1.1.m1.1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><apply id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i1.p1.1.m1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i1.p1.1.m1.1.1.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">0.3\%</annotation></semantics></math> in Word Error Rate and <math id="S1.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="0.65\%" display="inline"><semantics id="S1.I1.i1.p1.2.m2.1a"><mrow id="S1.I1.i1.p1.2.m2.1.1" xref="S1.I1.i1.p1.2.m2.1.1.cmml"><mn id="S1.I1.i1.p1.2.m2.1.1.2" xref="S1.I1.i1.p1.2.m2.1.1.2.cmml">0.65</mn><mo id="S1.I1.i1.p1.2.m2.1.1.1" xref="S1.I1.i1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.2.m2.1b"><apply id="S1.I1.i1.p1.2.m2.1.1.cmml" xref="S1.I1.i1.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i1.p1.2.m2.1.1.1.cmml" xref="S1.I1.i1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i1.p1.2.m2.1.1.2.cmml" xref="S1.I1.i1.p1.2.m2.1.1.2">0.65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.2.m2.1c">0.65\%</annotation></semantics></math> in COMET, respectively, on the tst2019 test set.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">While we observe significant enhancements in in-domain scenarios, we find that these techniques are not applicable in challenging scenarios (such as Overlapping Speakers, Background noise, etc.) due to poor ASR performance.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.2" class="ltx_p">We demonstrate that employing chunked long-form decoding<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We derive the terminology from this <a target="_blank" href="https://huggingface.co/blog/asr-chunking" title="" class="ltx_ref ltx_href">blog post</a>.</span></span></span> significantly improves ASR performance in challenging scenarios, such as the case of the ITV dev set. Specifically, we observe a decrease in the word error rate from <math id="S1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="37.83\%" display="inline"><semantics id="S1.I1.i3.p1.1.m1.1a"><mrow id="S1.I1.i3.p1.1.m1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.cmml"><mn id="S1.I1.i3.p1.1.m1.1.1.2" xref="S1.I1.i3.p1.1.m1.1.1.2.cmml">37.83</mn><mo id="S1.I1.i3.p1.1.m1.1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.1.m1.1b"><apply id="S1.I1.i3.p1.1.m1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.1.m1.1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i3.p1.1.m1.1.1.2.cmml" xref="S1.I1.i3.p1.1.m1.1.1.2">37.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">37.83\%</annotation></semantics></math> to <math id="S1.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="30.98\%" display="inline"><semantics id="S1.I1.i3.p1.2.m2.1a"><mrow id="S1.I1.i3.p1.2.m2.1.1" xref="S1.I1.i3.p1.2.m2.1.1.cmml"><mn id="S1.I1.i3.p1.2.m2.1.1.2" xref="S1.I1.i3.p1.2.m2.1.1.2.cmml">30.98</mn><mo id="S1.I1.i3.p1.2.m2.1.1.1" xref="S1.I1.i3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.2.m2.1b"><apply id="S1.I1.i3.p1.2.m2.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.2.m2.1.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i3.p1.2.m2.1.1.2.cmml" xref="S1.I1.i3.p1.2.m2.1.1.2">30.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.2.m2.1c">30.98\%</annotation></semantics></math></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section describes the evaluation and training data we use in our experiments. For evaluation, we report results on the tst2019 and ACLdev <cite class="ltx_cite ltx_citemacro_citep">(Salesky et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> test sets to compare with findings from previous works <cite class="ltx_cite ltx_citemacro_citep">(Anastasopoulos et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>; Agarwal et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>. We also use the EPTV (European Parlament activities), Itv (TV Series), and Peloton (Fitness TV) dev sets from the subtitling track consisting of overlapping speakers with different accents to evaluate the ASR performance in challenging scenarios.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">As the data conditions did not change from IWSLT23 to this year, we rely on the data processed from last year’s submission (KIT’23) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. For the training data of ASR, we use the same system that used Common Voice <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, LibriSpeech <cite class="ltx_cite ltx_citemacro_citep">(Panayotov et al., <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>, MuST-C v2 <cite class="ltx_cite ltx_citemacro_citep">(Di Gangi et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, TED-LIUM v3 <cite class="ltx_cite ltx_citemacro_citep">(Hernandez et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, and VoxPopuli <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">While for MT fine-tuning, we use the cleaned training data from last year created from the available parallel data. This includes Europarl v7 and v10 <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a href="#bib.bib15" title="" class="ltx_ref">2005</a>)</cite>, NewsCommentary v16, OpenSubtitles v2018 <cite class="ltx_cite ltx_citemacro_citep">(Lison and Tiedemann, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>, Tatoeba <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a href="#bib.bib30" title="" class="ltx_ref">2012</a>)</cite>, ELRC-CORDIS_News and TED2020 <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> and consists in total of 23 million sentence pairs. For the rest of the paper, we refer to the full parallel data as <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">seed</span> and TED2020 as <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">in-domain</span>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Overview</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we provide an overview of our proposed cascaded system, detailing each individual component. First, the input audio is sent to the ASR system, which undergoes segmentation, and N-best lists are generated for each segmented utterance. Next, the top candidates in the N-best list are fed as input to the LLM, which is trained to refine the ASR output and generate a final ASR hypothesis. Following this, the final ASR hypotheses are passed on to the sentence-level MT system, which produces translations. Finally, the sentence-level automatic transcripts and translations are fed into another adapted LLM, which automatically post-edits and generates a coherent document translation of the talk.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Automatic Speech Recognition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We employed the ASR model from our previous year’s submission <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, considering its effectiveness in transcribing the TED domain. For initialization, we utilized WavLM and mBART50 for the encoder and decoder, respectively, before fine-tuning on the ASR data described in Section <a href="#S2" title="2 Data ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. However, we encountered below-par ASR performance on the challenging sets EPTV, Itv, and Peloton.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We identified several issues that hindered the effectiveness of our ASR model with these sets. Firstly, the model itself was trained on single-talker datasets but inferred with multi-talker noisy datasets, leading to a mismatch in data distribution. Secondly, our typical use of the SHAS model for audio segmentation introduced challenges, as it sometimes missed segmentations and overlooked segments containing human speech.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Data shift is difficult to handle when the training dataset has not changed since last year. We focused more on handling the latter by incorporating long-form decoding. The key idea is to better use context (at the text or signal level) for decoding. The long audio file is chunked into smaller segments with a small overlap between adjacent segments. The model is run over each chunk, and the inferred text is joined at the strides by finding the longest common sequence between overlaps.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR refinement</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2406.16777/assets/figures/mt_pe.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold">Document Level MT Refinement</span>: The LLM trained to post-edit uses sentence-level transcripts and translations to generate a final document-level coherent and consistent translation.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Once we have generated the N-best list, we select the top 5 candidates and utilize an LLM to produce the final hypothesis as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In this step, we can adapt the LLM to the task using either few-shot prompting or LoRA fine-tuning techniques. We choose to fine-tune the LLM with adapters based on the findings from <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>. However, it is crucial to train the LLM under conditions that simulate the test environment, where it should fix errors of our ASR output rather than on the whisper generated in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To generate the dataset for fine-tuning, we perform inference on our in-domain training data using the gold segmentation. We create pairs comprising the N-best list and the corresponding reference. It is worth noting that we utilized the same data to train the ASR system, which is not ideal. However, resource constraints prevented us from following the augmentation procedure that mitigates this, which we explain further in Section <a href="#S3.SS4" title="3.4 Document-level Automatic Post-Editing ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>. Despite this limitation, manual analysis revealed that the ASR did not memorize the training data and produced similar N-best lists to those observed in the test conditions.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Following this, we fine-tuned the Mistral 7B Instruction-tuned LLM <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> using QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>, to predict the gold reference based on the top candidates (see the prompt format below). Importantly, we chose not to shuffle the order of the top candidates when providing it in the prompt, as doing so would eliminate the ranking information provided to the LLM, which could be crucial for its performance.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<pre id="S3.SS2.p4.1" class="ltx_verbatim ltx_font_typewriter">
Punctuate and Post-edit the hypothesis
based on the predictions:
Hyp 1 &lt;SS&gt; Hyp 2 &lt;SS&gt; Hyp 3 ..
Post-edited Hypothesis:
Gold Reference
</pre>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Machine Translation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For building the MT system, we leverage the strong pre-trained model NLLB 200 3.3B <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> that is allowed in the constrained plus LLM track. We perform a two-step fine-tuning approach. Initially, we fine-tune the model on the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">seed</span> data to adapt it to the spoken language domain. Subsequently, in the second step, we conduct in-domain fine-tuning on TED (in-domain) data, given its significance as one of the primary test sets in the offline track. Additionally, we implement checkpoint averaging to improve generalization with the last 3 checkpoints.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Restoring Punctuations</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">It is important to note that the ASR outputs lack punctuation. Therefore, we conducted experiments with two punctuators. First, we utilized the punctuations generated from the LLM ASR refinement process described in Section <a href="#S3.SS2" title="3.2 ASR refinement ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Second, we employed a DeltaLM-based punctuation model, which was utilized in our previous year’s submission <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. We observed that while the punctuations generated by the LLM were semantically correct, they often resulted in long sequences and led to a degradation in MT performance. As a result, we decided to opt for the second choice and segment the text into sentences using manually crafted rules.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<th id="S3.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">tst2019</th>
<th id="S3.T1.3.1.1.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">ACLdev2023</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.2.1" class="ltx_tr">
<th id="S3.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">KIT’23 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> ASR</th>
<td id="S3.T1.3.2.1.2" class="ltx_td ltx_align_left ltx_border_t">3.1</td>
<td id="S3.T1.3.2.1.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">11.3</td>
</tr>
<tr id="S3.T1.3.3.2" class="ltx_tr">
<th id="S3.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">KIT’23 ASR + LLM Refine</th>
<td id="S3.T1.3.3.2.2" class="ltx_td ltx_align_left ltx_border_bb">2.8</td>
<td id="S3.T1.3.3.2.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">10.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>ASR word error rate scores on tst2019 and ACLdev2023 test sets. <math id="S3.T1.2.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.T1.2.m1.1b"><mo id="S3.T1.2.m1.1.1" xref="S3.T1.2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.m1.1c"><plus id="S3.T1.2.m1.1.1.cmml" xref="S3.T1.2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.m1.1d">+</annotation></semantics></math> LLM refine indicates that the N-best list was post-edited to generate the final hypothesis.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Document-level Automatic Post-Editing</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">After translating the individual sentences with the fine-tuned NLLB, the outputs are not coherent as they are translated in isolation. Moreover, any ASR errors that might be fixed by observing the full document will be translated incorrectly. To mitigate this, we perform an additional step of document-level automatic post-editing using the source transcripts and sentence translations shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 ASR refinement ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Similar to the situation outlined in Section <a href="#S3.SS2" title="3.2 ASR refinement ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we encountered a lack of data for fine-tuning the LLM for document-level post-editing. Hence, we adopted the approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Koneru et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> to create the dataset. We divided the in-domain TED data into two halves, each containing English audio, English transcript, and German translation. Subsequently, we fine-tuned MT models on each half using the pre-trained models described in Sections <a href="#S3.SS1" title="3.1 Automatic Speech Recognition ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and <a href="#S3.SS3" title="3.3 Machine Translation ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. Following this, we conducted inference using the gold segmentation with our ASR and MT models trained on one half to the other half. This procedure generated a synthetic dataset with noisy ASR input, MT predictions, and corresponding gold references, leveraging the provided segmentation in the data.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">We then use the synthetic dataset to create instances of document-level post-editing. We go through each talk and divide the transcripts into chunks, each chunk containing a maximum of 256 tokens corresponding to the LLM tokenizer. Then for each chunk, we use the transcript, hypothesis and reference to transform them into the format below and train the LLM to predict the gold reference given the noisy transcript and sentence-level hypothesis.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<pre id="S3.SS4.p4.1" class="ltx_verbatim ltx_font_typewriter">
Noisy English Transcript:
ASR Hyp 1 &lt;SS&gt; ASR Hyp 2 &lt;SS&gt; ....
German Translations:
MT Hyp 1 &lt;SS&gt; MT Hyp 2 &lt;SS&gt; ....
Post-Edited German Translations:
Ref 1 &lt;SS&gt; Ref 2 &lt;SS&gt; ....
</pre>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">We use the delimiter "&lt;SS&gt;" to align with the input and perform sentence-level evaluation. Then, we again fine-tune the Mistral 7B Instruction-tuned LLM <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> using QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>, training it to predict the gold reference given the noisy transcript and translations. We employ the sliding window with payload strategy during decoding as described in <cite class="ltx_cite ltx_citemacro_citet">Koneru et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Automatic Speech Recognition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate the benefit of the additional ASR refinement step described in Section <a href="#S3.SS2" title="3.2 ASR refinement ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we compare the word error rate of our ASR system before and after post-editing, as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.3.1 Restoring Punctuations ‣ 3.3 Machine Translation ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The ASR performance improves in both cases, with a higher absolute improvement observed in the ACLdev set. The LLM is particularly beneficial in the ACLdev set, given that it contains terminology from the scientific domain where the LLM excels. We also observe a relative improvement of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">10\%</annotation></semantics></math> in the TED talks, indicating that ASR refinement is beneficial.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">Model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_italic">EPTV</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_italic">ITV</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_italic">Peloton</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">KIT’23 ASR</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">26.43</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">37.83</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text ltx_font_bold">18.93</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center">KIT’23 ASR + Gold Seg</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.2.1" class="ltx_text ltx_font_bold">16.84</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center">37.21</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center">25.88</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center">KIT’23 ASR + long-form</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">17.54</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.3.3.1" class="ltx_text ltx_font_bold">30.98</span></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center">20.79</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">Seamless v2 <cite class="ltx_cite ltx_citemacro_citep">(Barrault et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">40.94</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">56.94</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">43.47</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>ASR word error rate scores on the EPTV, Peloton and ITV dev set. Best scores for each set are highlighted in bold.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">However, the performance of the same ASR system on the challenge set was below par. We conducted additional ablation studies and present the results in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Automatic Speech Recognition ‣ 4 Results ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the challenge dev sets. We compared last year’s ASR system with three conditions: providing gold segmentation, utilizing long-form decoding, and using the recently developed Seamless V2 <cite class="ltx_cite ltx_citemacro_citep">(Barrault et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We observed that providing gold segmentation achieved a score of <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="16.84" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">16.84</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><cn type="float" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">16.84</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">16.84</annotation></semantics></math>, demonstrating its crucial role in handling this challenging set for EPTV. Moreover, long-form decoding significantly narrowed the gap, decreasing the word error rate for both EPTV and ITV. Meanwhile, our ASR shows the best performance for Peloton without any modifications. Additionally, we evaluated Seamless to assess its robustness and found that its performance was severely lacking in comparison.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Based on these results, we use the ASR with standard segmentation for <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">TED</span> and <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">Peloton</span> test sets. For EPTV and ITV, we use the ASR system with long-form decoding. We found that the LLM cannot refine the N-best list given the poor WER of KIT’23 ASR for the latter test sets and generates long sequences with repetitions for most utterances. <span id="S4.SS1.p4.1.3" class="ltx_text ltx_font_bold">Therefore, we do not perform any ASR or MT LLM refinements for <span id="S4.SS1.p4.1.3.1" class="ltx_text ltx_font_italic">ITV</span> and <span id="S4.SS1.p4.1.3.2" class="ltx_text ltx_font_italic">EPTV</span> sets and generate translations with a standard cascaded ST pipeline.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Cascaded Speech Translation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we evaluate the final quality of our cascaded ST using the mwerSegmenter to realign the hypothesis with the reference segmentation. We report results with BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib23" title="" class="ltx_ref">2002</a>)</cite> and Chrf2 <cite class="ltx_cite ltx_citemacro_citep">(Popović, <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite> computed by Sacrebleu <cite class="ltx_cite ltx_citemacro_citep">(Post, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>. We also report the COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> score using the default model<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Unbabel/wmt22-comet-da</span></span></span>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Two-step Fine-tuning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We presented a two-step fine-tuning approach to adapt our MT system in Section <a href="#S3.SS3" title="3.3 Machine Translation ‣ 3 Overview ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> to the target domain. We report the translation quality on tst2019 test set with this approach (last row) and other models for comparison in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Two-step Fine-tuning ‣ 4 Results ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:210.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(86.8pt,-42.0pt) scale(1.66723213564403,1.66723213564403) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">Model</span></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_italic">tst2019</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<td id="S4.T3.1.1.2.1.1" class="ltx_td ltx_border_t"></td>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BLEU</th>
<th id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Chrf2</th>
<th id="S4.T3.1.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">COMET</th>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">KIT’23 TED*</td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.2.2.1" class="ltx_text ltx_font_bold">28.4</span></td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.2.3.1" class="ltx_text ltx_font_bold">58.8</span></td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.2.4.1" class="ltx_text ltx_font_bold">78.87</span></td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center">NLLB 3.3B</td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center">26.6</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center">57.7</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center">77.41</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center">Seamless v2</td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">25.5</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center">57.0</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_center">76.65</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<td id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_center">NLLB 3.3B + Seed</td>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center">26.9</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center">57.9</td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_nopad_r ltx_align_center">77.87</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<td id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb">NLLB 3.3B + Seed + TED</td>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">27.6</td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">58.5</td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">78.49</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>MT scores using KIT’23 ASR as input calculated by resegmenting with mwerSegmenter. * indicates an unconstrained system that was trained on the same data sources but in more languages than what is allowed for IWSLT24. TED indicates the model adapted for TED and not ACLdev which was the official submission from KIT for IWSLT23</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:867.2pt;height:226.1pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="transform:translate(190.0pt,-49.1pt) scale(1.7799652409162,1.7799652409162) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_italic">Model</span></th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S4.T4.1.1.1.1.2.1" class="ltx_text ltx_font_italic">tst2019</span></th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">ACLdev2023</th>
</tr>
<tr id="S4.T4.1.1.2.2" class="ltx_tr">
<th id="S4.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BLEU</th>
<th id="S4.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Chrf2</th>
<th id="S4.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">COMET</th>
<th id="S4.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">BLEU</th>
<th id="S4.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Chrf2</th>
<th id="S4.T4.1.1.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">COMET</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.3.1" class="ltx_tr">
<th id="S4.T4.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">NLLB 3.3</th>
<td id="S4.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">26.6</td>
<td id="S4.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">57.7</td>
<td id="S4.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.41</td>
<td id="S4.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S4.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">63.9</td>
<td id="S4.T4.1.1.3.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">74.83</td>
</tr>
<tr id="S4.T4.1.1.4.2" class="ltx_tr">
<th id="S4.T4.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">NLLB 3.3 Seed</th>
<td id="S4.T4.1.1.4.2.2" class="ltx_td ltx_align_center">26.9</td>
<td id="S4.T4.1.1.4.2.3" class="ltx_td ltx_align_center">57.9</td>
<td id="S4.T4.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">77.87</td>
<td id="S4.T4.1.1.4.2.5" class="ltx_td ltx_align_center">34.7</td>
<td id="S4.T4.1.1.4.2.6" class="ltx_td ltx_align_center">63.8</td>
<td id="S4.T4.1.1.4.2.7" class="ltx_td ltx_nopad_r ltx_align_center">75.67</td>
</tr>
<tr id="S4.T4.1.1.5.3" class="ltx_tr">
<th id="S4.T4.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ASR Refine + NLLB 3.3 Seed</th>
<td id="S4.T4.1.1.5.3.2" class="ltx_td ltx_align_center">27.3</td>
<td id="S4.T4.1.1.5.3.3" class="ltx_td ltx_align_center">58.3</td>
<td id="S4.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">78.32</td>
<td id="S4.T4.1.1.5.3.5" class="ltx_td ltx_align_center">36.1</td>
<td id="S4.T4.1.1.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.5.3.6.1" class="ltx_text ltx_font_bold">65.0</span></td>
<td id="S4.T4.1.1.5.3.7" class="ltx_td ltx_nopad_r ltx_align_center">77.59</td>
</tr>
<tr id="S4.T4.1.1.6.4" class="ltx_tr">
<th id="S4.T4.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ASR Refine + NLLB 3.3 Seed + TED</th>
<td id="S4.T4.1.1.6.4.2" class="ltx_td ltx_align_center">28.3</td>
<td id="S4.T4.1.1.6.4.3" class="ltx_td ltx_align_center">58.8</td>
<td id="S4.T4.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">78.98</td>
<td id="S4.T4.1.1.6.4.5" class="ltx_td ltx_align_center">34.8</td>
<td id="S4.T4.1.1.6.4.6" class="ltx_td ltx_align_center">63.7</td>
<td id="S4.T4.1.1.6.4.7" class="ltx_td ltx_nopad_r ltx_align_center">77.25</td>
</tr>
<tr id="S4.T4.1.1.7.5" class="ltx_tr">
<th id="S4.T4.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">ASR Refine + NLLB 3.3 Seed + TED + Doc APE</th>
<td id="S4.T4.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.7.5.2.1" class="ltx_text ltx_font_bold">28.7</span></td>
<td id="S4.T4.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.7.5.3.1" class="ltx_text ltx_font_bold">59.1</span></td>
<td id="S4.T4.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.1.1.7.5.4.1" class="ltx_text ltx_font_bold">79.63</span></td>
<td id="S4.T4.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.7.5.5.1" class="ltx_text ltx_font_bold">36.4</span></td>
<td id="S4.T4.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb">64.5</td>
<td id="S4.T4.1.1.7.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.7.5.7.1" class="ltx_text ltx_font_bold">78.64</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>MT scores using KIT’23 ASR as input calculated by resegmenting with mwerSegmenter. ASR Refine indicates an additional ASR refinement step with the LLM. Seed and TED indicate fine-tuning the NLLB 3.3 with seed alone or a two-step process with additional fine-tuning on TED. Doc APE indicates an LLM post-editing refinement to generate a coherent and consistent document. Best scores in each metric and test set are highlighted in bold.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Firstly, we observe that Seamless performs inferiorly to NLLB across all translation metrics. Consequently, we proceeded with NLLB for further experiments.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.3" class="ltx_p">Subsequently, fine-tuning the seed parallel data improved quality across all metrics, notably increasing the score from <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="77.41" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">77.41</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn type="float" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">77.41</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">77.41</annotation></semantics></math> to <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="77.87" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">77.87</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn type="float" id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">77.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">77.87</annotation></semantics></math> in COMET. Following this, with the assistance of second-step fine-tuning, we observed further improvements, resulting in scores reaching <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="78.49" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mn id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">78.49</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><cn type="float" id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">78.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">78.49</annotation></semantics></math>. However, it is important to note that this system still lags behind last year’s submission, which was specifically adapted to the TED domain. Nevertheless, it’s worth highlighting that this system was trained across multiple languages, placing it in the unconstrained condition for IWSLT24. Moreover, we could not replicate a similar adaptation process for NLLB due to resource and time constraints.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>LLM Refinement</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We proposed improving the ASR outputs and converting sentence-level to document-level translations using fine-tuned LLMs. We evaluate the benefits of the individual steps and report the results of our final cascaded ST system in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Two-step Fine-tuning ‣ 4 Results ‣ Blending LLMs into Cascaded Speech Translation: KIT’s Offline Speech Translation System for IWSLT 2024" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> on tst2019 and ACLdev2023 test sets.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">First, the benefits of two-step fine-tuning, ASR refinement, and document post-editing complement each other. Using KITs 23 ASR with NLLB 3.3 B as a baseline, we obtained <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="77.41" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">77.41</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><cn type="float" id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">77.41</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">77.41</annotation></semantics></math> COMET in tst2019 test set. However, including all enhancements led to a total improvement of <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="2.23" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mn id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">2.23</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><cn type="float" id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">2.23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">2.23</annotation></semantics></math> COMET points. Furthermore, the improvements are consistent with both lexical and neural metrics.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Next, we observed that integrating LLMs provides significant benefits in the ACLdev set compared to the TED dev sets. This is plausible due to scientific terminology and accented speakers in the ACLdev set. Both of these challenges are well-suited for LLMs, as the quality of the initial systems is sufficient to utilize context and rectify mistakes reliably.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This system paper presented KIT’s submission for the offline track in the constrained + LLMs condition, focusing on the English-to-German translation direction. Using modular techniques, we successfully integrated LLMs into any cascaded ST pipeline. Additionally, we highlighted the benefits of long-form decoding in scenarios involving noisy and overlapping speech.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For future work, we aim to explore robust techniques for integrating LLMs that can effectively handle challenging scenarios where ASR quality is sub-par. Furthermore, the translation’s latency is quite high as it needs to call the LLM twice. However, integrating quality estimation techniques to decide when we need the LLM can limit the effects of the high latency problem.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is partly supported by the Helmholtz Programme-oriented Funding, with project number 46.24.01, named AI for Language Technologies, funding from the pilot program Core-Informatics of the Helmholtz Association (HGF).
It also received partial support from the European Union’s Horizon research and innovation programme under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETtings BetWEEN People). The work was partly performed on the HoreKa supercomputer funded by the Ministry of Science, Research and the Arts Baden-Württemberg and by the Federal Ministry of Education and Research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. (2023)</span>
<span class="ltx_bibblock">
Milind Agarwal, Sweta Agarwal, Antonios Anastasopoulos, Luisa Bentivogli, Ondřej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, et al. 2023.

</span>
<span class="ltx_bibblock">Findings of the iwslt 2023 evaluation campaign.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anastasopoulos et al. (2021)</span>
<span class="ltx_bibblock">
Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni, Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Alexander Waibel, Changhan Wang, and Matthew Wiesner. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.iwslt-1.1" title="" class="ltx_ref ltx_href">FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</em>, pages 1–29, Bangkok, Thailand (online). Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2020)</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.lrec-1.520/" title="" class="ltx_ref ltx_href">Common voice: A massively-multilingual speech corpus</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020</em>, pages 4218–4222. European Language Resources Association.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2023)</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023.

</span>
<span class="ltx_bibblock">Seamlessm4t-massively multilingual &amp; multimodal machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11596</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, and Eng-Siong Chng. 2024.

</span>
<span class="ltx_bibblock">Hyporadise: An open baseline for generative speech recognition with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.

</span>
<span class="ltx_bibblock">Wavlm: Large-scale self-supervised pre-training for full stack speech processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 16(6):1505–1518.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2024)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Gangi et al. (2019)</span>
<span class="ltx_bibblock">
Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1202" title="" class="ltx_ref ltx_href">MuST-C: a Multilingual Speech Translation Corpus</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2012–2017, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et al. (2018)</span>
<span class="ltx_bibblock">
François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia A. Tomashenko, and Yannick Estève. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-99579-3_21" title="" class="ltx_ref ltx_href">TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Speech and Computer - 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18-22, 2018, Proceedings</em>, volume 11096 of <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 198–208. Springer.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024)</span>
<span class="ltx_bibblock">
Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, and Eng Siong Chng. 2024.

</span>
<span class="ltx_bibblock">Gentranslate: Large language models are generative multilingual speech and machine translators.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06894</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2005.mtsummit-papers.11" title="" class="ltx_ref ltx_href">Europarl: A parallel corpus for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit X: Papers</em>, pages 79–86, Phuket, Thailand.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koneru et al. (2023)</span>
<span class="ltx_bibblock">
Sai Koneru, Miriam Exel, Matthias Huck, and Jan Niehues. 2023.

</span>
<span class="ltx_bibblock">Contextual refinement of translations: Large language models for sentence and document-level post-editing.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14855</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)</span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/L16-1147" title="" class="ltx_ref ltx_href">OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</em>, pages 923–929, Portorož, Slovenia. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, and Jan Niehues. 2023.

</span>
<span class="ltx_bibblock">Kit’s multilingual speech translation system for iwslt 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05320</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock">Multilingual denoising pre-training for neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 8:726–742.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et al. (2022)</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/peft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/peft</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">Librispeech: An ASR corpus based on public domain audio books</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015</em>, pages 5206–5210. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2015)</span>
<span class="ltx_bibblock">
Maja Popović. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W15-3049" title="" class="ltx_ref ltx_href">chrF: character n-gram F-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W18-6319" title="" class="ltx_ref ltx_href">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu et al. (2023)</span>
<span class="ltx_bibblock">
Jie Pu, Thai-Son Nguyen, and Sebastian Stüker. 2023.

</span>
<span class="ltx_bibblock">Multi-stage large language model correction for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.11532</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022)</span>
<span class="ltx_bibblock">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.wmt-1.52" title="" class="ltx_ref ltx_href">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.365" title="" class="ltx_ref ltx_href">Making monolingual sentence embeddings multilingual using knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 4512–4525, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky et al. (2023)</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona Diab, and Jan Niehues. 2023.

</span>
<span class="ltx_bibblock">Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf" title="" class="ltx_ref ltx_href">Parallel data, tools and interfaces in OPUS</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</em>, pages 2214–2218, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.80" title="" class="ltx_ref ltx_href">VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 993–1003, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.4" class="ltx_p">We use the transformers library <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> for fine-tuning our ASR and LLM and the fairseq toolkit <cite class="ltx_cite ltx_citemacro_citep">(Ott et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> for fine-tuning NLLB 3.3B. For the ASR training, we set the <span id="A1.p1.4.1" class="ltx_text ltx_font_italic">batch size</span> to <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="384" display="inline"><semantics id="A1.p1.1.m1.1a"><mn id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">384</mn><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><cn type="integer" id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">384</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">384</annotation></semantics></math>, resulting in approximately 128 minutes per batch. We employ a <span id="A1.p1.4.2" class="ltx_text ltx_font_italic">warmup strategy</span> over <math id="A1.p1.2.m2.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="A1.p1.2.m2.2a"><mrow id="A1.p1.2.m2.2.3.2" xref="A1.p1.2.m2.2.3.1.cmml"><mn id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">2</mn><mo id="A1.p1.2.m2.2.3.2.1" xref="A1.p1.2.m2.2.3.1.cmml">,</mo><mn id="A1.p1.2.m2.2.2" xref="A1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.2b"><list id="A1.p1.2.m2.2.3.1.cmml" xref="A1.p1.2.m2.2.3.2"><cn type="integer" id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">2</cn><cn type="integer" id="A1.p1.2.m2.2.2.cmml" xref="A1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.2c">2,000</annotation></semantics></math> steps and a total of <math id="A1.p1.3.m3.2" class="ltx_Math" alttext="100,000" display="inline"><semantics id="A1.p1.3.m3.2a"><mrow id="A1.p1.3.m3.2.3.2" xref="A1.p1.3.m3.2.3.1.cmml"><mn id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">100</mn><mo id="A1.p1.3.m3.2.3.2.1" xref="A1.p1.3.m3.2.3.1.cmml">,</mo><mn id="A1.p1.3.m3.2.2" xref="A1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.2b"><list id="A1.p1.3.m3.2.3.1.cmml" xref="A1.p1.3.m3.2.3.2"><cn type="integer" id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">100</cn><cn type="integer" id="A1.p1.3.m3.2.2.cmml" xref="A1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.2c">100,000</annotation></semantics></math> training steps. The <span id="A1.p1.4.3" class="ltx_text ltx_font_italic">learning rate</span> is initialized to <math id="A1.p1.4.m4.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="A1.p1.4.m4.1a"><mrow id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml"><mrow id="A1.p1.4.m4.1.1.2" xref="A1.p1.4.m4.1.1.2.cmml"><mn id="A1.p1.4.m4.1.1.2.2" xref="A1.p1.4.m4.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.p1.4.m4.1.1.2.1" xref="A1.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="A1.p1.4.m4.1.1.2.3" xref="A1.p1.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p1.4.m4.1.1.1" xref="A1.p1.4.m4.1.1.1.cmml">−</mo><mn id="A1.p1.4.m4.1.1.3" xref="A1.p1.4.m4.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><apply id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1"><minus id="A1.p1.4.m4.1.1.1.cmml" xref="A1.p1.4.m4.1.1.1"></minus><apply id="A1.p1.4.m4.1.1.2.cmml" xref="A1.p1.4.m4.1.1.2"><times id="A1.p1.4.m4.1.1.2.1.cmml" xref="A1.p1.4.m4.1.1.2.1"></times><cn type="integer" id="A1.p1.4.m4.1.1.2.2.cmml" xref="A1.p1.4.m4.1.1.2.2">1</cn><ci id="A1.p1.4.m4.1.1.2.3.cmml" xref="A1.p1.4.m4.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p1.4.m4.1.1.3.cmml" xref="A1.p1.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">1e-4</annotation></semantics></math>.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.12" class="ltx_p">For the NLLB fine-tuning experiments, we use a <span id="A1.p2.12.2" class="ltx_text ltx_font_italic">learning rate</span> set to <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mrow id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml"><mn id="A1.p2.1.m1.1.1.2.2" xref="A1.p2.1.m1.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A1.p2.1.m1.1.1.2.1" xref="A1.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.p2.1.m1.1.1.2.3" xref="A1.p2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p2.1.m1.1.1.1" xref="A1.p2.1.m1.1.1.1.cmml">−</mo><mn id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><minus id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></minus><apply id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2"><times id="A1.p2.1.m1.1.1.2.1.cmml" xref="A1.p2.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p2.1.m1.1.1.2.2.cmml" xref="A1.p2.1.m1.1.1.2.2">5</cn><ci id="A1.p2.1.m1.1.1.2.3.cmml" xref="A1.p2.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">5e-5</annotation></semantics></math>, <span id="A1.p2.12.3" class="ltx_text ltx_font_italic">label smoothing</span> to <math id="A1.p2.2.m2.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="A1.p2.2.m2.1a"><mn id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><cn type="float" id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">0.1</annotation></semantics></math>, <span id="A1.p2.12.4" class="ltx_text ltx_font_italic">drop out</span> to <math id="A1.p2.3.m3.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="A1.p2.3.m3.1a"><mn id="A1.p2.3.m3.1.1" xref="A1.p2.3.m3.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.p2.3.m3.1b"><cn type="float" id="A1.p2.3.m3.1.1.cmml" xref="A1.p2.3.m3.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.3.m3.1c">0.1</annotation></semantics></math>, <span id="A1.p2.12.5" class="ltx_text ltx_font_italic">attention drop-out</span> to <math id="A1.p2.4.m4.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="A1.p2.4.m4.1a"><mn id="A1.p2.4.m4.1.1" xref="A1.p2.4.m4.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.p2.4.m4.1b"><cn type="float" id="A1.p2.4.m4.1.1.cmml" xref="A1.p2.4.m4.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.4.m4.1c">0.1</annotation></semantics></math>. We use the <span id="A1.p2.12.6" class="ltx_text ltx_font_italic">Adam optimizer</span> with <span id="A1.p2.12.7" class="ltx_text ltx_font_italic">betas</span> to <math id="A1.p2.5.m5.3" class="ltx_Math" alttext="(0,9,0.98)" display="inline"><semantics id="A1.p2.5.m5.3a"><mrow id="A1.p2.5.m5.3.4.2" xref="A1.p2.5.m5.3.4.1.cmml"><mo stretchy="false" id="A1.p2.5.m5.3.4.2.1" xref="A1.p2.5.m5.3.4.1.cmml">(</mo><mn id="A1.p2.5.m5.1.1" xref="A1.p2.5.m5.1.1.cmml">0</mn><mo id="A1.p2.5.m5.3.4.2.2" xref="A1.p2.5.m5.3.4.1.cmml">,</mo><mn id="A1.p2.5.m5.2.2" xref="A1.p2.5.m5.2.2.cmml">9</mn><mo id="A1.p2.5.m5.3.4.2.3" xref="A1.p2.5.m5.3.4.1.cmml">,</mo><mn id="A1.p2.5.m5.3.3" xref="A1.p2.5.m5.3.3.cmml">0.98</mn><mo stretchy="false" id="A1.p2.5.m5.3.4.2.4" xref="A1.p2.5.m5.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.5.m5.3b"><vector id="A1.p2.5.m5.3.4.1.cmml" xref="A1.p2.5.m5.3.4.2"><cn type="integer" id="A1.p2.5.m5.1.1.cmml" xref="A1.p2.5.m5.1.1">0</cn><cn type="integer" id="A1.p2.5.m5.2.2.cmml" xref="A1.p2.5.m5.2.2">9</cn><cn type="float" id="A1.p2.5.m5.3.3.cmml" xref="A1.p2.5.m5.3.3">0.98</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.5.m5.3c">(0,9,0.98)</annotation></semantics></math> and the remaining optimizer parameters to default. We used a <span id="A1.p2.12.8" class="ltx_text ltx_font_italic">batch size</span> of maximum <math id="A1.p2.6.m6.1" class="ltx_Math" alttext="3096" display="inline"><semantics id="A1.p2.6.m6.1a"><mn id="A1.p2.6.m6.1.1" xref="A1.p2.6.m6.1.1.cmml">3096</mn><annotation-xml encoding="MathML-Content" id="A1.p2.6.m6.1b"><cn type="integer" id="A1.p2.6.m6.1.1.cmml" xref="A1.p2.6.m6.1.1">3096</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.6.m6.1c">3096</annotation></semantics></math> making one step, <span id="A1.p2.12.9" class="ltx_text ltx_font_italic">update-freq</span> to <math id="A1.p2.7.m7.1" class="ltx_Math" alttext="16" display="inline"><semantics id="A1.p2.7.m7.1a"><mn id="A1.p2.7.m7.1.1" xref="A1.p2.7.m7.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A1.p2.7.m7.1b"><cn type="integer" id="A1.p2.7.m7.1.1.cmml" xref="A1.p2.7.m7.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.7.m7.1c">16</annotation></semantics></math> and validating on the dev set after every epoch. We stopped the training after the dev loss did not increase after 10 epochs.
For fine-tuning the LLM with QLoRA we use the peft <cite class="ltx_cite ltx_citemacro_citep">(Mangrulkar et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> along with the transformers library. We add LoRA adapters to the target modules [<span id="A1.p2.12.10" class="ltx_text ltx_font_italic">q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj</span>]. We set the <span id="A1.p2.12.11" class="ltx_text ltx_font_italic">adapter rank</span> to <math id="A1.p2.8.m8.1" class="ltx_Math" alttext="16" display="inline"><semantics id="A1.p2.8.m8.1a"><mn id="A1.p2.8.m8.1.1" xref="A1.p2.8.m8.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A1.p2.8.m8.1b"><cn type="integer" id="A1.p2.8.m8.1.1.cmml" xref="A1.p2.8.m8.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.8.m8.1c">16</annotation></semantics></math>, <span id="A1.p2.12.12" class="ltx_text ltx_font_italic">alpha</span> to <math id="A1.p2.9.m9.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A1.p2.9.m9.1a"><mn id="A1.p2.9.m9.1.1" xref="A1.p2.9.m9.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A1.p2.9.m9.1b"><cn type="integer" id="A1.p2.9.m9.1.1.cmml" xref="A1.p2.9.m9.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.9.m9.1c">32</annotation></semantics></math> and <span id="A1.p2.12.13" class="ltx_text ltx_font_italic">lora dropout</span> to <math id="A1.p2.10.m10.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="A1.p2.10.m10.1a"><mn id="A1.p2.10.m10.1.1" xref="A1.p2.10.m10.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.p2.10.m10.1b"><cn type="float" id="A1.p2.10.m10.1.1.cmml" xref="A1.p2.10.m10.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.10.m10.1c">0.1</annotation></semantics></math>. We use a <span id="A1.p2.11.1" class="ltx_text ltx_font_italic">batch size of <math id="A1.p2.11.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="A1.p2.11.1.m1.1a"><mn id="A1.p2.11.1.m1.1.1" xref="A1.p2.11.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.p2.11.1.m1.1b"><cn type="integer" id="A1.p2.11.1.m1.1.1.cmml" xref="A1.p2.11.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.11.1.m1.1c">8</annotation></semantics></math></span>, <span id="A1.p2.12.14" class="ltx_text ltx_font_italic">learning rate</span> of <math id="A1.p2.12.m11.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="A1.p2.12.m11.1a"><mrow id="A1.p2.12.m11.1.1" xref="A1.p2.12.m11.1.1.cmml"><mrow id="A1.p2.12.m11.1.1.2" xref="A1.p2.12.m11.1.1.2.cmml"><mn id="A1.p2.12.m11.1.1.2.2" xref="A1.p2.12.m11.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A1.p2.12.m11.1.1.2.1" xref="A1.p2.12.m11.1.1.2.1.cmml">​</mo><mi id="A1.p2.12.m11.1.1.2.3" xref="A1.p2.12.m11.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p2.12.m11.1.1.1" xref="A1.p2.12.m11.1.1.1.cmml">−</mo><mn id="A1.p2.12.m11.1.1.3" xref="A1.p2.12.m11.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.12.m11.1b"><apply id="A1.p2.12.m11.1.1.cmml" xref="A1.p2.12.m11.1.1"><minus id="A1.p2.12.m11.1.1.1.cmml" xref="A1.p2.12.m11.1.1.1"></minus><apply id="A1.p2.12.m11.1.1.2.cmml" xref="A1.p2.12.m11.1.1.2"><times id="A1.p2.12.m11.1.1.2.1.cmml" xref="A1.p2.12.m11.1.1.2.1"></times><cn type="integer" id="A1.p2.12.m11.1.1.2.2.cmml" xref="A1.p2.12.m11.1.1.2.2">5</cn><ci id="A1.p2.12.m11.1.1.2.3.cmml" xref="A1.p2.12.m11.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.p2.12.m11.1.1.3.cmml" xref="A1.p2.12.m11.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.12.m11.1c">5e-5</annotation></semantics></math> with other parameters set to default. After every 200 steps, we validate and terminate the training if it does not improve 10 consecutive times.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">During inference, we use beam search for all ASR, MT and LLM components. The ASR and MT decode with <span id="A1.p3.1.1" class="ltx_text ltx_font_italic">beam size</span> of 5, whereas the LLM does it with <span id="A1.p3.1.2" class="ltx_text ltx_font_italic">beam size</span> of <math id="A1.p3.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A1.p3.1.m1.1a"><mn id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><cn type="integer" id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">3</annotation></semantics></math>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.16776" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.16777" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.16777">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.16777" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.16778" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:49:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
