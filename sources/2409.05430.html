<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.05430] Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge</title><meta property="og:description" content="The StutteringSpeech Challenge focuses on advancing speech technologies for people who stutter, specifically targeting Stuttering Event Detection (SED) and Automatic Speech Recognition (ASR) in Mandarin. The challenge …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.05430">

<!--Generated on Sun Oct  6 01:56:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The StutteringSpeech Challenge focuses on advancing speech technologies for people who stutter, specifically targeting Stuttering Event Detection (SED) and Automatic Speech Recognition (ASR) in Mandarin. The challenge comprises three tracks: (1) SED, which aims to develop systems for detection of stuttering events; (2) ASR, which focuses on creating robust systems for recognizing stuttered speech; and (3) Research track for innovative approaches utilizing the provided dataset. We utilizes an open-source Mandarin stuttering dataset AS-70, which has been split into new training and test sets for the challenge. This paper presents the dataset, details the challenge tracks, and analyzes the performance of the top systems, highlighting improvements in detection accuracy and reductions in recognition error rates. Our findings underscore the potential of specialized models and augmentation strategies in developing stuttered speech technologies.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Mandarin stuttered speech, stuttering event detection, speech recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Stuttering is a speech impediment that affects approximately 1% of the global population <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, characterized by disruptions such as repetitions, prolongations, and blocks. These disruptions significantly impact social interactions and mental well-being <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, often leading to stress, shame, and low self-esteem for people who stutter (PWS). Consequently, PWS frequently experience communication avoidance and social isolation. Early intervention is crucial for effective treatment, particularly in children. However, regions like Mainland China face a shortage of certified speech therapists, limiting access to necessary support. With the rise of voice-user interfaces in smart home devices and chatbots like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the need for inclusive speech technologies has become more pressing. However, current Automatic Speech Recognition (ASR) systems struggle with stuttering speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, likely due to a lack of data. These problems highlight the need for stuttering event detection (SED) and inclusive speech technologies for PWS.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address these issues, we organized the Mandarin Stuttering Event Detection and Automatic Speech Recognition (StutteringSpeech) Challenge, the first of its kind focused on Mandarin stuttering. The challenge aims to mobilize researchers to develop robust systems for detecting and recognizing stuttered speech, promoting the inclusion of PWS in the advancement of speech technologies.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The StutteringSpeech Challenge consists of three tracks:
Track I SED: This track focuses on developing systems capable of accurately different types of stuttering events in speech. Early detection is essential for timely intervention and treatment, making this track vital for improving the lives of PWS.
Track II ASR: This track aims to create specialized ASR systems that can effectively recognize and transcribe stuttered speech. Current ASR systems struggle with stuttered speech, and this track encourages the development of more inclusive technologies.
Track III Research: This track welcomes open submissions of papers on related topics.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We utilize the AS-70 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the largest and most comprehensive open-source Chinese Mandarin stuttering dataset, as the challenge data.
The dataset includes conversational and voice command reading speech, providing a robust foundation for developing and testing SED and ASR systems. We have repartitioned the dataset to prevent overlap in command texts and avoid overly optimistic results in the command test sets.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper presents the findings of the StutteringSpeech Challenge, providing detailed insights into the dataset, track setup, and the performance of the submitted systems. Key findings from the challenge include: (1) The importance of tailored methods for different stuttered speech events, such as the Zipformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> in handling word or phrase repetitions. (2) The effectiveness of specific data augmentation techniques for different types of stuttered speech events.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The study of stuttering speech and its impact on speech technologies has gained increasing attention recently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Most available datasets, such as FluencyBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and UCLASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, primarily focus on Western languages and are often small in size. The Sep-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> dataset comprises 28,000 three-second podcast audio clips labeled for stuttering events but lacks text transcriptions. Another notable dataset is LibriStutter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> includes text transcriptions but is artificially generated from recordings of fluent speech.
Despite these efforts, there remains a significant resource gap for Mandarin-speaking PWS.
AS-70 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is a comprehensive dataset encompassing conversational and voice command reading speech, complete with manual transcriptions. For this challenge, we utilize the AS-70 dataset to provide a robust foundation for developing SED and ASR systems.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Various neural network architectures have been explored for SED task. ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> adds convolutional layers per feature type and learns how the features should be weighted. StutterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> uses a time-delay neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to capturing contextual aspects of the disfluent utterances. FluentNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> consists of a Squeeze-and-Excitation Residual convolutional neural network followed by bidirectional long short-term memory layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, enhancing the learning of temporal relationships. Additionally, machine learning techniques like multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> are employed to enhance the accuracy of stuttering detection. We use the conformer encoder as the baseline for the SED track <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The Conformer model, with its hybrid architecture combining convolutional operations and self-attention mechanisms, effectively captures both local and global dependencies in speech signals, making it particularly suitable for detecting nuanced stuttering events.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">End-to-end ASR models such as Connectionist Temporal Classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, recurrent neural network transducer (RNN-T) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and attention-based encoder-decoder (AED) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have gained increasing attention over the last few years. Hybrid CTC/attention models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> adopt both CTC and attention decoder loss during training, which results in faster convergence and improves the AED model’s robustness. Additional, most ASR research on stuttered speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> has primarily focused on predicting semantic content rather than exact word-for-word transcriptions. Therefore, in this study, we use a hybrid CTC/attention model u2++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as the baseline for the ASR track.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset and Tracks</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the AS-70 Mandarin dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for this challenge. This dataset was gathered during 70 online voice chat sessions and comprised of conversation and voice command reading. Each recording session begins with an approximately half-hour interview conducted by two native Mandarin speakers who are both PWS. Following the interview, the interviewee is asked to read a prepared list of commands. The effective duration of the dataset is around 50 hours, encompassing recordings from 72 distinct PWS – 2 interviewers and 70 interviewees.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">While AS-70 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> has already partitioned the dataset into training, development, and test sets, we repartition the data according to specific distribution and stuttering severity of speakers detailed in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Dataset ‣ 3 Dataset and Tracks ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This repartitioning is essential because the original partitioning led to overlapping command texts between the training and test sets, although from different speakers, which could result in overly optimistic testing results for the command part.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Speaker numbers per stuttering severity and partition.</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<th id="S3.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Mild</th>
<th id="S3.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Moderate</th>
<th id="S3.T1.3.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Severe</th>
<th id="S3.T1.3.1.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">Sum</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.2.1" class="ltx_tr">
<th id="S3.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Train</th>
<td id="S3.T1.3.2.1.2" class="ltx_td ltx_align_left ltx_border_t">25</td>
<td id="S3.T1.3.2.1.3" class="ltx_td ltx_align_left ltx_border_t">12</td>
<td id="S3.T1.3.2.1.4" class="ltx_td ltx_align_left ltx_border_t">6</td>
<td id="S3.T1.3.2.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">43</td>
</tr>
<tr id="S3.T1.3.3.2" class="ltx_tr">
<th id="S3.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dev</th>
<td id="S3.T1.3.3.2.2" class="ltx_td ltx_align_left">4</td>
<td id="S3.T1.3.3.2.3" class="ltx_td ltx_align_left">2</td>
<td id="S3.T1.3.3.2.4" class="ltx_td ltx_align_left">1</td>
<td id="S3.T1.3.3.2.5" class="ltx_td ltx_nopad_r ltx_align_left">7</td>
</tr>
<tr id="S3.T1.3.4.3" class="ltx_tr">
<th id="S3.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Test</th>
<td id="S3.T1.3.4.3.2" class="ltx_td ltx_align_left">14</td>
<td id="S3.T1.3.4.3.3" class="ltx_td ltx_align_left">3</td>
<td id="S3.T1.3.4.3.4" class="ltx_td ltx_align_left">3</td>
<td id="S3.T1.3.4.3.5" class="ltx_td ltx_nopad_r ltx_align_left">20</td>
</tr>
<tr id="S3.T1.3.5.4" class="ltx_tr">
<th id="S3.T1.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Sum</th>
<td id="S3.T1.3.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">43</td>
<td id="S3.T1.3.5.4.3" class="ltx_td ltx_align_left ltx_border_bb">17</td>
<td id="S3.T1.3.5.4.4" class="ltx_td ltx_align_left ltx_border_bb">10</td>
<td id="S3.T1.3.5.4.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">70</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For the SED task, five types of stuttering event <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are specified by the annotation guidelines, including:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">/p</span>: <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">prolongation</span>. Elongated phoneme.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">/b</span>: <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">block</span>. Gasps for air or stuttered pauses.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">/r</span>: <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">sound repetition</span>. Repeated phoneme that do not constitute an entire character.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">[]</span>: <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_bold">Word/phrase repetition</span>. Designated for marking entire repeated character or phrase.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">/i</span>: <span id="S3.I1.i5.p1.1.2" class="ltx_text ltx_font_bold">interjections</span>. Filler characters due to stuttering e.g., ‘嗯’, ‘啊’, or ‘呃’. Notably, naturally occurring interjections that don’t disrupt the speech flow are excluded.</p>
</div>
</li>
</ul>
<p id="S3.SS1.p3.2" class="ltx_p">The number of utterances of each stuttering event type and the total utterance number are shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Dataset ‣ 3 Dataset and Tracks ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>SED data on the number of utterances for the five stuttering types. Each utterance may contain multiple stuttering types or none at all.</figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.3.1.1" class="ltx_tr">
<td id="S3.T2.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T2.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">/p</span></th>
<th id="S3.T2.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">/b</span></th>
<th id="S3.T2.3.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">/r</span></th>
<th id="S3.T2.3.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.5.1" class="ltx_text ltx_font_bold">[]</span></th>
<th id="S3.T2.3.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.6.1" class="ltx_text ltx_font_bold">/i</span></th>
<th id="S3.T2.3.1.1.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">Sum</th>
</tr>
<tr id="S3.T2.3.2.2" class="ltx_tr">
<td id="S3.T2.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Train</td>
<td id="S3.T2.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t">3,874</td>
<td id="S3.T2.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t">1,611</td>
<td id="S3.T2.3.2.2.4" class="ltx_td ltx_align_left ltx_border_t">2,194</td>
<td id="S3.T2.3.2.2.5" class="ltx_td ltx_align_left ltx_border_t">6,780</td>
<td id="S3.T2.3.2.2.6" class="ltx_td ltx_align_left ltx_border_t">3,252</td>
<td id="S3.T2.3.2.2.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">26,659</td>
</tr>
<tr id="S3.T2.3.3.3" class="ltx_tr">
<td id="S3.T2.3.3.3.1" class="ltx_td ltx_align_left">Dev</td>
<td id="S3.T2.3.3.3.2" class="ltx_td ltx_align_left">450</td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_left">203</td>
<td id="S3.T2.3.3.3.4" class="ltx_td ltx_align_left">286</td>
<td id="S3.T2.3.3.3.5" class="ltx_td ltx_align_left">1,435</td>
<td id="S3.T2.3.3.3.6" class="ltx_td ltx_align_left">591</td>
<td id="S3.T2.3.3.3.7" class="ltx_td ltx_nopad_r ltx_align_left">4,294</td>
</tr>
<tr id="S3.T2.3.4.4" class="ltx_tr">
<td id="S3.T2.3.4.4.1" class="ltx_td ltx_align_left">Test</td>
<td id="S3.T2.3.4.4.2" class="ltx_td ltx_align_left">1,208</td>
<td id="S3.T2.3.4.4.3" class="ltx_td ltx_align_left">486</td>
<td id="S3.T2.3.4.4.4" class="ltx_td ltx_align_left">770</td>
<td id="S3.T2.3.4.4.5" class="ltx_td ltx_align_left">2,527</td>
<td id="S3.T2.3.4.4.6" class="ltx_td ltx_align_left">1,424</td>
<td id="S3.T2.3.4.4.7" class="ltx_td ltx_nopad_r ltx_align_left">11,000</td>
</tr>
<tr id="S3.T2.3.5.5" class="ltx_tr">
<td id="S3.T2.3.5.5.1" class="ltx_td ltx_align_left ltx_border_bb">Sum</td>
<td id="S3.T2.3.5.5.2" class="ltx_td ltx_align_left ltx_border_bb">5,532</td>
<td id="S3.T2.3.5.5.3" class="ltx_td ltx_align_left ltx_border_bb">2,300</td>
<td id="S3.T2.3.5.5.4" class="ltx_td ltx_align_left ltx_border_bb">3,250</td>
<td id="S3.T2.3.5.5.5" class="ltx_td ltx_align_left ltx_border_bb">10,742</td>
<td id="S3.T2.3.5.5.6" class="ltx_td ltx_align_left ltx_border_bb">5,267</td>
<td id="S3.T2.3.5.5.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">41,953</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">For the ASR task, the number of utterances contained in the training, development and test sets are shown in Table <a href="#S3.T3" title="Table 3 ‣ 3.1 Dataset ‣ 3 Dataset and Tracks ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>ASR data on the number of utterances in each partition.</figcaption>
<table id="S3.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.3.1.1" class="ltx_tr">
<td id="S3.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Train</td>
<td id="S3.T3.3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Dev</td>
<td id="S3.T3.3.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Test</td>
<td id="S3.T3.3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">Sum</td>
</tr>
<tr id="S3.T3.3.2.2" class="ltx_tr">
<td id="S3.T3.3.2.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">22,866</td>
<td id="S3.T3.3.2.2.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">3,903</td>
<td id="S3.T3.3.2.2.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">8,559</td>
<td id="S3.T3.3.2.2.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">35,328</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">For a detailed description of this dataset, including the annotation process and statistical analysis, please refer to the papers by Gong et al. and Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tracks</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The challenge comprises of three sub-tracks. For Track I and Track II, participants are restricted from using external audio data, timestamps, pre-trained models, and other information, except for AISHELL-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or acoustic data augmentation resources, such as Musan<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and RIR. Track III permits the use of any resources to improve results. Participants are encouraged to prioritize technological innovation, particularly through the exploration of novel model architectures, rather than relying solely on improvementd data usage.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Track I-Stuttering Event Detection</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">This is a multi-label classification task. Participants are tasked with developing models to identify stuttering events in short speech audio snippets. The five types of stuttering events that may appear in the audio snippets are sound prolongation, sound repetition, character repetition, block, and interjection. Training and development sets containing audio snippets and their corresponding labels were provided to participants at the beginning of the challenge.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">For Track I, the submitted systems are evaluated based on stuttering event detection accuracy, recall, precision, and F1 score on the audio snippets in the test set. The F1 score, which is the harmonic mean of precision and recall, is calculated as follows:</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle F1=2\cdot\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}." display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.2.1.cmml">​</mo><mn id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">1</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><mstyle displaystyle="true" id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mfrac id="S3.E1.m1.1.1.1.1.3.3a" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mtext id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2a.cmml">Precision</mtext><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.1.1.3.3.2.1.cmml">⋅</mo><mtext id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3a.cmml">Recall</mtext></mrow><mrow id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2a.cmml">Precision</mtext><mo id="S3.E1.m1.1.1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.3.1.cmml">+</mo><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml">Recall</mtext></mrow></mfrac></mstyle></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">𝐹</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1">⋅</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">2</cn><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><divide id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3"></divide><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><ci id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.1">⋅</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2"><mtext id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">Precision</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3">Recall</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><plus id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.3.3.3.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">Precision</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3">Recall</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle F1=2\cdot\frac{\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Track II-Automatic Speech Recognition</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The main goal of Track II is to advance the development of ASR systems that can effectively handle stuttering speech. Participants must devise speech-to-text systems that accurately recognize speech containing stuttering events, converting it into clean text with the stuttering event labels removed. Training and development sets containing stuttering speech audio and their corresponding text transcriptions were provided for system development.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.5" class="ltx_p">For Track II, the accuracy of the ASR system is measured by Character Error Rate (CER). The CER indicates the percentage of characters that are incorrectly predicted. It calculates the minimum number of insertions (Ins), substitutions (Subs), and deletions (Del) required to transform the hypothesis output into the reference transcript. Specifically, CER is calculated as follows:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle CER=\frac{N_{\text{Ins }}+N_{\text{Subs }}+N_{\text{Del }}}{N_{\text{Total }}}\times 100\%," display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.1a" xref="S3.E2.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml">R</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mfrac id="S3.E2.m1.1.1.1.1.3.2a" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml"><msub id="S3.E2.m1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.2.2.cmml">N</mi><mtext id="S3.E2.m1.1.1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.1.1.3.2.2.2.3a.cmml">Ins </mtext></msub><mo id="S3.E2.m1.1.1.1.1.3.2.2.1" xref="S3.E2.m1.1.1.1.1.3.2.2.1.cmml">+</mo><msub id="S3.E2.m1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.1.1.3.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.2.3.2.cmml">N</mi><mtext id="S3.E2.m1.1.1.1.1.3.2.2.3.3" xref="S3.E2.m1.1.1.1.1.3.2.2.3.3a.cmml">Subs </mtext></msub><mo id="S3.E2.m1.1.1.1.1.3.2.2.1a" xref="S3.E2.m1.1.1.1.1.3.2.2.1.cmml">+</mo><msub id="S3.E2.m1.1.1.1.1.3.2.2.4" xref="S3.E2.m1.1.1.1.1.3.2.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2.4.2" xref="S3.E2.m1.1.1.1.1.3.2.2.4.2.cmml">N</mi><mtext id="S3.E2.m1.1.1.1.1.3.2.2.4.3" xref="S3.E2.m1.1.1.1.1.3.2.2.4.3a.cmml">Del </mtext></msub></mrow><msub id="S3.E2.m1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.3.2.cmml">N</mi><mtext id="S3.E2.m1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.1.3.2.3.3a.cmml">Total </mtext></msub></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml">×</mo><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mn id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">100</mn><mo id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">%</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1"></times><ci id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2">𝐶</ci><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">𝐸</ci><ci id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4">𝑅</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><divide id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2"></divide><apply id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2"><plus id="S3.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.1"></plus><apply id="S3.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2.2">𝑁</ci><ci id="S3.E2.m1.1.1.1.1.3.2.2.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.2.3">Ins </mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3.2">𝑁</ci><ci id="S3.E2.m1.1.1.1.1.3.2.2.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.3.3">Subs </mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.3.2.2.4.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.4.2">𝑁</ci><ci id="S3.E2.m1.1.1.1.1.3.2.2.4.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.4.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.2.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2.4.3">Del </mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2">𝑁</ci><ci id="S3.E2.m1.1.1.1.1.3.2.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.3">Total </mtext></ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1">percent</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">100</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle CER=\frac{N_{\text{Ins }}+N_{\text{Subs }}+N_{\text{Del }}}{N_{\text{Total }}}\times 100\%,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p2.4" class="ltx_p">where<math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="N_{\text{Ins}}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">N</mi><mtext id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3a.cmml">Ins</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">Ins</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">N_{\text{Ins}}</annotation></semantics></math>, <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="N_{\text{Subs}}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">N</mi><mtext id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3a.cmml">Subs</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">Subs</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">N_{\text{Subs}}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="N_{\text{Del}}" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><msub id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml">N</mi><mtext id="S3.SS2.SSS2.p2.3.m3.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.3a.cmml">Del</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><apply id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p2.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.3">Del</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">N_{\text{Del}}</annotation></semantics></math> represent the number of insertion, substitution, and deletion errors, respectively, and <math id="S3.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="N_{\text{Total}}" display="inline"><semantics id="S3.SS2.SSS2.p2.4.m4.1a"><msub id="S3.SS2.SSS2.p2.4.m4.1.1" xref="S3.SS2.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p2.4.m4.1.1.2" xref="S3.SS2.SSS2.p2.4.m4.1.1.2.cmml">N</mi><mtext id="S3.SS2.SSS2.p2.4.m4.1.1.3" xref="S3.SS2.SSS2.p2.4.m4.1.1.3a.cmml">Total</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.4.m4.1b"><apply id="S3.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1.3">Total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.4.m4.1c">N_{\text{Total}}</annotation></semantics></math> is the total number of characters in the reference transcript. As standard practice, insertions, deletions, and substitutions all contribute to the overall error rate.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Track III Research Paper Track</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Participants are invited to contribute research papers that utilize the stuttering speech dataset and evaluation framework in their experimental setups and analyses. This track provides an opportunity to explore and document innovative approaches and findings related to stuttering speech technologies.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>System Description</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline system</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We provide a Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> baseline for SED track and a U2++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> baseline for ASR track, built using the WeNet toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. These baselines serve as benchmarks to facilitate reproducible research and comparison across different submissions.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2409.05430/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_img_square" width="461" height="458" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Baseline model structure in Track I. Target 0,1,0,0,1 indicates the presence or absence of the five stuttering events.</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Track I-Stuttering Event Detection</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The baseline system consists of multiple Conformer blocks, as depicted in Fig <a href="#S4.F1" title="Figure 1 ‣ 4.1 Baseline system ‣ 4 System Description ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Each block comprises a multi-head self-attention module, a convolution module, and two feed-forward modules, with 4 attention heads and an output size of 256. The convolutional modules use a kernel size of 15 to capture a wide range of temporal features. Due to the limited training data, we use 3 Conformer blocks, resulting in a parameter size of 9.7 million.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">We first extract 80-dimensional filter bank (fbank) features from the speech, feed them into the Conformer encoder, and perform classification through a linear layer. The model employs single-task learning to predict the five stuttering event types, using the multi-label soft margin loss for training. The model is optimized using the MultiLabelSoftMarginLoss function in PyTorch.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">We train the model using only the AS-70 dataset. The training is conducted with an initial learning rate of 0.001 on four NVIDIA 4090 GPUs, with a batch size of 16 per GPU. The model is trained for 100 epochs, with 1,000 warmup steps.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Track II-Automatic Speech Recognition</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The U2++ model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, illustrated in Fig <a href="#S4.F2" title="Figure 2 ‣ 4.1.2 Track II-Automatic Speech Recognition ‣ 4.1 Baseline system ‣ 4 System Description ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, is a unified two-pass framework with bidirectional attention decoders. It incorporates future contextual information through a right-to-left attention decoder to enhance the representational ability of the shared encoder and improve performance during the rescoring stage. The baseline model for Track II consists of 12 Conformer encoder layers, each with 4 attention heads and an output size of 256. The convolutional modules have a kernel size of 8. Additionally, the baseline model includes 3 bitransformer decoder layers, with 3 left-to-right and 3 right-to-left layers.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">We use both the AS-70 dataset and the AISHELL-1 dataset for training. The model is trained with an initial learning rate of 0.001 on four NVIDIA 4090 GPUs, with a batch size of 16 per GPU. It is trained for 100 epochs and 25,000 warmup steps.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.05430/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Baseline U2++ model structure in Track II.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Submission of Track I</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Track I receives a total of 24 modeling results from 5 teams. We have selected the top 3 teams for presentation, in addition to the official system model.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Official system: </span>
We aim to enhance the performance of the Conformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in SED through various simple data augmentation techniques. Specifically, we incorporate speed perturbation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and data balancing to improve the robustness and generalization of the model. Speed perturbation alters the playback speed of audio samples without changing the pitch. A fixed set of perturbation factors 0.8-1.2 are introduced in the speech samples, where the intervals are 0.05. This technique introduces variability into the training data, enabling the model to recognize stuttering events under different temporal conditions.
Additionally, we observe significant differences in the number of utterances for each stuttering event as shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Dataset ‣ 3 Dataset and Tracks ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To address data imbalance, we employ oversampling techniques to balance the dataset. By ensuring equal representation of stuttering and non-stuttering events, we aim to mitigate the model’s bias towards the majority class, thereby improving detection accuracy.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">T018: </span>
Team T018 improves the performance of the SED task through several data augmentation techniques and the introduction of the Zipformer-L architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
They implement a comprehensive data augmentation strategy to simulate Mandarin stuttered speech using the Montreal Forced Aligner (MFA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> trained on the AISHELL-1 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Six types of stuttering data are generated: block, sound repetition, word repetition, block with sound repetition, block with word repetition, and normal speech. The quantities for each type are specified to balance the dataset. The augmentation process involved alignment, segmentation, augmentation, combination, and validation steps.
The model is based on the Zipformer-L architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, a variant of the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> designed explicitly for speech processing tasks. The model consists of several key components for the SED task: an input layer of filter bank features, Zipformer layers, and a classification layer for the five stuttering events.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">T029: </span>
Team T029 introduces a novel SED model which combines the Conformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> with Bidirectional Long Short-Term Memory (BiLSTM) networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The Conformer model extracts acoustic information from stuttered speech, while the BiLSTM networks capture contextual relationships within the speech. This combination simplifies the acquisition of both speech signal representations and contextual relations for stuttering patients. Additionally, T029 explores the impact of multitask settings on the model’s performance to address the issue of imbalanced data. They design a model comprising five binary classification tasks for each stuttering type, with the output layer defined as 5*2 dimensions.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">T031: </span>
Team T031 proposes a Fine-Grained Contrastive Learning (FGCL) framework for SED. This method aims to improve the accuracy of stutter detection by capturing subtle nuances that previous SED techniques have overlooked. FGCL leverages a detailed acoustic analysis to model frame-level probabilities of stuttering events. The team then introduce a novel mining algorithm to identify both easy and confusing frames within audio data. By applying a stutter contrast loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, they refine the representation of these frames, thereby enhancing the model’s ability to distinguish between stuttered and fluent speech.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Submission of Track II</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Track II receives 73 modeling results submitted by 6 teams, and we have selected the top 3 teams for presentation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">T006: </span>
Team T006 focus on enhancing stuttering ASR through comprehensive data augmentation techniques. Their innovative approach includes both signal-based and adversarial methods to extend the variety and volume of speech data available. <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_bold">Speed Perturbation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>: Adjust the time-domain speech signal’s sampling resolution, creating variations in speech rate and volume to replicate the speech rate variations typical of PWS. <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_bold">Tempo Perturbation</span>: Utilize the WSOLA algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to alter speech duration while maintaining pitch, simulating the slower speech cadence observed in PWS. <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_bold">InsertSilence</span>: Randomly insert silent segments into the speech signal, effectively introducing pauses and delays that mimic stuttering blocks, aiding the ASR model in recognizing and processing speech patterns with frequent pauses. <span id="S4.SS3.p2.1.5" class="ltx_text ltx_font_bold">RepeatPart</span>: Repeat parts of the audio signal multiple times, simulating the repetitive articulations often seen in stuttering. <span id="S4.SS3.p2.1.6" class="ltx_text ltx_font_bold">SpecAugment</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>: Apply dynamic masks to spectral features, enhancing the model’s ability to generalize by simulating partial information loss and small speech fragments. <span id="S4.SS3.p2.1.7" class="ltx_text ltx_font_bold">GAN-Based Augmentation</span>: Utilize Generative Adversarial Networks like Parallel WaveGAN, BigVGAN, and Vocos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> to inject fine-grained spectral-temporal variations, capturing detailed differences between stuttering and non-stuttering speech. These multi-stage data enhancement techniques generated 1,443 hours of ASR data. For the model architecture, the team use a 17-layer E-Branchformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and a 6-layer transformer decoder, implemented using the ESPnet toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>SED results (F1 scores) in Track I.</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">TeamID</th>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/p</th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/b</th>
<th id="S4.T4.3.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/r</th>
<th id="S4.T4.3.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.3.1.1.5.1" class="ltx_text ltx_font_bold">[]</span></th>
<th id="S4.T4.3.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/i</th>
<th id="S4.T4.3.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.2.1" class="ltx_tr">
<th id="S4.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">65.12</td>
<td id="S4.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">24.3</td>
<td id="S4.T4.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">41.86</td>
<td id="S4.T4.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t">61.85</td>
<td id="S4.T4.3.2.1.6" class="ltx_td ltx_align_center ltx_border_t">74.87</td>
<td id="S4.T4.3.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">53.6</td>
</tr>
<tr id="S4.T4.3.3.2" class="ltx_tr">
<th id="S4.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Offical</th>
<td id="S4.T4.3.3.2.2" class="ltx_td ltx_align_center">64.67</td>
<td id="S4.T4.3.3.2.3" class="ltx_td ltx_align_center">36.70</td>
<td id="S4.T4.3.3.2.4" class="ltx_td ltx_align_center">57.61</td>
<td id="S4.T4.3.3.2.5" class="ltx_td ltx_align_center">65.81</td>
<td id="S4.T4.3.3.2.6" class="ltx_td ltx_align_center">79.38</td>
<td id="S4.T4.3.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center">60.83</td>
</tr>
<tr id="S4.T4.3.4.3" class="ltx_tr">
<th id="S4.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T018</th>
<td id="S4.T4.3.4.3.2" class="ltx_td ltx_align_center">67.61</td>
<td id="S4.T4.3.4.3.3" class="ltx_td ltx_align_center">31.58</td>
<td id="S4.T4.3.4.3.4" class="ltx_td ltx_align_center">59.25</td>
<td id="S4.T4.3.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.3.4.3.5.1" class="ltx_text ltx_font_bold">81.11</span></td>
<td id="S4.T4.3.4.3.6" class="ltx_td ltx_align_center">83.94</td>
<td id="S4.T4.3.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center">64.7</td>
</tr>
<tr id="S4.T4.3.5.4" class="ltx_tr">
<th id="S4.T4.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T029</th>
<td id="S4.T4.3.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.5.4.2.1" class="ltx_text ltx_font_bold">70.89</span></td>
<td id="S4.T4.3.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.5.4.3.1" class="ltx_text ltx_font_bold">44.07</span></td>
<td id="S4.T4.3.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.3.5.4.4.1" class="ltx_text ltx_font_bold">59.78</span></td>
<td id="S4.T4.3.5.4.5" class="ltx_td ltx_align_center">74.79</td>
<td id="S4.T4.3.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.3.5.4.6.1" class="ltx_text ltx_font_bold">85.15</span></td>
<td id="S4.T4.3.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.3.5.4.7.1" class="ltx_text ltx_font_bold">66.93</span></td>
</tr>
<tr id="S4.T4.3.6.5" class="ltx_tr">
<th id="S4.T4.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">T031</th>
<td id="S4.T4.3.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">65.89</td>
<td id="S4.T4.3.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">24.78</td>
<td id="S4.T4.3.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">50.15</td>
<td id="S4.T4.3.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">64.85</td>
<td id="S4.T4.3.6.5.6" class="ltx_td ltx_align_center ltx_border_bb">77.06</td>
<td id="S4.T4.3.6.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">56.55</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">T018: </span>
Team T018 enhance the performance of the ASR task using the same data augmentation techniques applied in the SED task, combined with a different model architecture. The ASR model is based on the RNN-T architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, with the speech encoder utilizing the Zipformer-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This model architecture comprises the following components: an input layer for filter bank features, a Zipformer encoder, a transducer decoder, and an output layer.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">T051: </span>
Team T051 leverage the ESPnet toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to integrate a CTC and attention-based encoder-decoder network. The encoder utilizes the Branchformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. To enhance the model’s generalization ability and robustness, the team employs SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> for data augmentation during training.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>ASR results (CER%) in Track II.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">TeamID</th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.2.1" class="ltx_text ltx_font_bold">Mild</span></th>
<th id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.3.1" class="ltx_text ltx_font_bold">Moderate</span></th>
<th id="S4.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.4.1" class="ltx_text ltx_font_bold">Server</span></th>
<th id="S4.T5.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.5.1" class="ltx_text ltx_font_bold">Conversation</span></th>
<th id="S4.T5.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.6.1" class="ltx_text ltx_font_bold">Command</span></th>
<th id="S4.T5.3.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.3.1.1.7.1" class="ltx_text ltx_font_bold">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.2.1" class="ltx_tr">
<th id="S4.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T5.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">17.27</td>
<td id="S4.T5.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">19.45</td>
<td id="S4.T5.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">29.53</td>
<td id="S4.T5.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t">17.7</td>
<td id="S4.T5.3.2.1.6" class="ltx_td ltx_align_center ltx_border_t">21.5</td>
<td id="S4.T5.3.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">19.18</td>
</tr>
<tr id="S4.T5.3.3.2" class="ltx_tr">
<th id="S4.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T006</th>
<td id="S4.T5.3.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.2.2.1" class="ltx_text ltx_font_bold">11.16</span></td>
<td id="S4.T5.3.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.2.3.1" class="ltx_text ltx_font_bold">12.38</span></td>
<td id="S4.T5.3.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.2.4.1" class="ltx_text ltx_font_bold">18.54</span></td>
<td id="S4.T5.3.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.2.5.1" class="ltx_text ltx_font_bold">12.13</span></td>
<td id="S4.T5.3.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.3.3.2.6.1" class="ltx_text ltx_font_bold">12.57</span></td>
<td id="S4.T5.3.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.3.3.2.7.1" class="ltx_text ltx_font_bold">12.30</span></td>
</tr>
<tr id="S4.T5.3.4.3" class="ltx_tr">
<th id="S4.T5.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T018</th>
<td id="S4.T5.3.4.3.2" class="ltx_td ltx_align_center">15.36</td>
<td id="S4.T5.3.4.3.3" class="ltx_td ltx_align_center">16.07</td>
<td id="S4.T5.3.4.3.4" class="ltx_td ltx_align_center">25.8</td>
<td id="S4.T5.3.4.3.5" class="ltx_td ltx_align_center">14.29</td>
<td id="S4.T5.3.4.3.6" class="ltx_td ltx_align_center">20.84</td>
<td id="S4.T5.3.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center">16.85</td>
</tr>
<tr id="S4.T5.3.5.4" class="ltx_tr">
<th id="S4.T5.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">T051</th>
<td id="S4.T5.3.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">15.63</td>
<td id="S4.T5.3.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">18.01</td>
<td id="S4.T5.3.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">26.13</td>
<td id="S4.T5.3.5.4.5" class="ltx_td ltx_align_center ltx_border_bb">16.44</td>
<td id="S4.T5.3.5.4.6" class="ltx_td ltx_align_center ltx_border_bb">18.7</td>
<td id="S4.T5.3.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">17.33</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CHALLENGE RESULTS SUMMARY</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section presents the results of the StutteringSpeech Challenge, focusing on Track I and Track II. We provide a detailed analysis of the leaderboard outcomes, highlighting the top-performing teams and their methodologies.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Results and Analysis of Track I</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Overall: </span>
The leaderboard results for Track I are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Submission of Track II ‣ 4 System Description ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Across both results, It is evident that data augmentation techniques play a critical role in improving performance across various models.
Notably, the Conformer-BILSTM model of T029, equipped with five classification heads, achieves the highest mean F1 scores.
The Zipformer model of T018 demonstrates effective detection in word or phrase repetition events due to its robust content information capturing capability.
The FGCL model of T031 outperforms the Conformer model in terms of average F1 score even without data augmentation.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">T029: </span>
Team T029 achieves the highest mean F1 scores across the five stuttering events, with notable performance in four individual events. Their success is attributed to their innovative Conformer-BILSTM model, effectively capturing contextual information after extracting local stuttering acoustic features. Additionally, they use five classification heads to detect each stuttering event separately, simplifying the detection process and enhancing accuracy.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">T018: </span>
Team T018 secures the second-highest average F1 score, with an improvement of 20.7% compared to the baseline. They achieves the highest score in the <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_bold">[]</span> event, significantly outperforming other models in this category. The Zipformer model demonstrates its efficacy in speech recognition tasks due to its robust content information capturing proficiency. <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_bold">[]</span> is word or phrase repetition, the Zipformer exhibits effective detection in this aspect.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">T031: </span>
Team T031 finishes third overall. Their FGCL model showed a 5.5% improvement in the average F1 score compared to the baseline model. It is worth highlighting that T031 did not use data augmentation but focuses solely on model modifications, contributing significantly to the development of SED. Combining their model with data augmentation techniques could enhance performance further.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Official: </span>
Our official system demonstrates a 13.5% improvement in the average F1 score compared to the baseline. While not participating in the rankings, our model, which incorporates only speed perturbations and data balancing, showed notable improvements. Speed perturbation, in particular, is highly effective, likely because stuttering is closely related to speech speed, and altering the speed generates diverse stuttering data.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para ltx_noindent">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">Stuttering Types: </span>
Various data augmentation methods can enhance the accuracy of SED. <span id="S5.SS1.p6.1.2" class="ltx_text ltx_font_bold">/p</span> involves sound prolongation, while <span id="S5.SS1.p6.1.3" class="ltx_text ltx_font_bold">/r</span> is characterized by sound repetition. <span id="S5.SS1.p6.1.4" class="ltx_text ltx_font_bold">[]</span> represents word repetitions. These events are closely related to the duration of speech, which makes speed perturbation a particularly method for enhancing the diversity and accuracy of SED in the official system. <span id="S5.SS1.p6.1.5" class="ltx_text ltx_font_bold">/b</span> involves blocking, and <span id="S5.SS1.p6.1.6" class="ltx_text ltx_font_bold">/i</span> involves interjections, which can be augmented by inserting silent segments or unnatural interjections, thereby generating more diverse data, as exemplified in T018.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results and Analysis of Track II</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Overall: </span>
The leaderboard results are presented in Table <a href="#S4.T5" title="Table 5 ‣ 4.3 Submission of Track II ‣ 4 System Description ‣ Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
It is evident that data augmentation techniques also play a critical role in improving ASR performance.
Team T006’s comprehensive augmentation strategies are particularly noteworthy, especially their enhancements for voice commands. Future tasks on stuttering voice commands can benefit from T006’s approach.
Additionally, the E-branchformer and Branchformer models demonstrates superior performance compared to the Conformer model in stuttering ASR tasks.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">T006: </span>
Team T006 achieves first place in all test sets. Their average CER is 27% lower than the second-place team and 35.87% lower than the baseline model. This can be attributed to their comprehensive data augmentation techniques, including signal-based and adversarial-based augmentations. Their use of the E-branchformer architecture also outperforms Conformer model in ASR task.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">T018: </span>
Team T018 secures second place in Track II, with an average CER reduction of 12.1% compared to the baseline. Their success is partly due to their data augmentation techniques and utilizing the powerful Zipformer model structure for the ASR task. Although their data augmentation is slightly less effective compared to T006, the potential applicability of the Zipformer structure in stuttering ASR tasks warrants further exploration.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">T051: </span>
Team T051 finishes third, with an average CER reduction of 9.6% compared to the baseline. This indicates that the Branchformer structure is more effective in stuttering ASR tasks.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Stuttering Severity: </span>
In all outcomes, CER improvements with the severity of stuttering. None of the participating teams specifically addresses stuttering severity in their models. Future research should focus on developing ASR systems that can better handle varying levels of stuttering severity.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Conversation and Command: </span>
The results show a notable difference in CER between command and conversation scenarios, with the exception of T006. T006 significantly reduces this gap, suggesting that their data augmentation methods improves the recognition rate for command speech, making it more promising for voice wake-up applications in PWS.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The StutteringSpeech Challenge has significantly advanced both ASR and SED systems tailored for PWS. By leveraging the comprehensive AS-70 dataset, the challenge has demonstrated the effectiveness of various data augmentation techniques and novel model architectures. Notably, in the SED task, the Conformer-BILSTM model achieved the highest mean F1 scores, while the Zipformer model excelled in detecting word repetitions.
For the ASR task, comprehensive augmentation strategies were particularly noteworthy, especially their enhancements for voice commands. The E-branchformer and Branchformer models demonstrated superior performance in stuttering ASR tasks.
These findings underscore the importance of tailored approaches in both stuttering SED and ASR tasks and highlight the potential of different data augmentation techniques. Future research should focus on integrating these models with real-world applications, enhancing their robustness to different stuttering severities. By continuing to refine these technologies, we can develop more reliable and accessible speech recognition solutions, ultimately improving the quality of life for PWS.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ehud Yairi, Nicoline Ambrose, and Nancy Cox,

</span>
<span class="ltx_bibblock">“Genetics of stuttering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Journal of Speech, Language, and Hearing Research</span>, pp. 771–784, 1996.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jane Prasse and George Kikano,

</span>
<span class="ltx_bibblock">“Stuttering: An overview,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">American family physician</span>, pp. 1271–6, 2008.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
OpenAI,

</span>
<span class="ltx_bibblock">“Introducing chatgpt,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">URL https://openai.com/blog/chatgpt</span>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Colin Lea, Zifang Huang, Jaya Narain, Lauren Tooley, Dianna Yee, Dung Tien Tran, Panayiotis Georgiou, Jeffrey P Bigham, and Leah Findlater,

</span>
<span class="ltx_bibblock">“From user perceptions to technical improvement: Enabling people who stutter to better use speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</span>. 2023, Association for Computing Machinery.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Olabanji Shonibare, Xiaosu Tong, and Venkatesh Ravichandran,

</span>
<span class="ltx_bibblock">“Enhancing asr for stuttered speech with limited data using detect and pass,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, vol. abs/2202.05396, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Rong Gong, Hongfei Xue, Lezhi Wang, Xin Xu, Qisheng Li, Lei Xie, Hui Bu, Shaomei Wu, Jiaming Zhou, Yong Qin, et al.,

</span>
<span class="ltx_bibblock">“As-70: A mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">interspeech</span>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, and Daniel Povey,

</span>
<span class="ltx_bibblock">“Zipformer: A faster and better encoder for automatic speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Bob MacDonald, Pan-Pan Jiang, Julie Cattiau, Rus Heywood, Richard Cave, Katie Seaver, Marilyn Ladewig, Jimmy Tobin, Michael Brenner, Philip Q Nelson, Jordan R. Green, and Katrin Tomanek,

</span>
<span class="ltx_bibblock">“Disordered speech data collection: Lessons learned at 1 million utterances from project euphonia,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2021, pp. 4833–4837, ISCA.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Vikramjit Mitra, Zifang Huang, Colin Lea, Lauren Tooley, Panayiotis Georgiou, Sachin Kajarekar, and Jefferey Bigham,

</span>
<span class="ltx_bibblock">“Analysis and tuning of a voice assistant system for dysfluent speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2021, pp. 4848–4852, ISCA.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Xin Zhang, Iván Vallés-Pérez, Andreas Stolcke, Chengzhu Yu, Jasha Droppo, Olabanji Shonibare, Roberto Barra-Chicote, and Venkatesh Ravichandran,

</span>
<span class="ltx_bibblock">“Stutter-tts: Controlled synthesis and improved recognition of stuttered speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">NeurIPS 2022 Workshop on SyntheticData4ML</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
N. B. Ratner and B. MacWhinney,

</span>
<span class="ltx_bibblock">“Fluency bank: a new resource for fluency research and practice,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Journal of Fluency Disorders</span>, pp. 69–80, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Peter Howell, Steve Davis, and Jon Bartrip,

</span>
<span class="ltx_bibblock">“The university college london archive of stuttered speech (uclass).,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Journal of speech, language, and hearing research : JSLHR</span>, pp. 556–69, 2009.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Colin Lea, Vikramjit Mitra, Aparna Joshi, Sachin Kajarekar, and Jeffrey Bigham,

</span>
<span class="ltx_bibblock">“Sep-28k: A dataset for stuttering event detection from podcasts with people who stutter,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. 2021, pp. 6798–6802, IEEE.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tedd Kourkounakis, Amirhossein Hajavi, and Ali Etemad,

</span>
<span class="ltx_bibblock">“Fluentnet: End-to-end detection of stuttered speech disfluencies with deep learning,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, pp. 2986–2999, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Shakeel A. Sheikh, Md Sahidullah, Fabrice Hirsch, and Slim Ouni,

</span>
<span class="ltx_bibblock">“Advancing stuttering detection via data augmentation, class-balanced loss and multi-contextual deep learning,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Biomedical and Health Informatics</span>, pp. 2553–2564, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Alexander Waibel, Toshiyuki Hanazawa, Geoffrey E. Hinton, Kiyohiro Shikano, and Kevin J. Lang,

</span>
<span class="ltx_bibblock">“Phoneme recognition using time-delay neural networks,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Acoust. Speech Signal Process.</span>, pp. 328–339, 1989.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Vicky Zayats, Mari Ostendorf, and Hannaneh Hajishirzi,

</span>
<span class="ltx_bibblock">“Disfluency detection using a bidirectional LSTM,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2016, pp. 2523–2527, ISCA.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Sebastian P. Bayerl, Dominik Wagner, Ilja Baumann, Florian Hönig, Tobias Bocklet, Elmar Nöth, and Korbinian Riedhammer,

</span>
<span class="ltx_bibblock">“A stutter seldom comes alone - cross-corpus stuttering detection as a multi-label problem,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">INTERSPEECH</span>. 2023, pp. 1538–1542, ISCA.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,

</span>
<span class="ltx_bibblock">“Conformer: Convolution-augmented transformer for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2020, pp. 5036–5040, ISCA.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino J. Gomez, and Jürgen Schmidhuber,

</span>
<span class="ltx_bibblock">“Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML)</span>, 2006, pp. 369–376.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Alex Graves,

</span>
<span class="ltx_bibblock">“Sequence transduction with recurrent neural networks,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1211.3711</span>, 2012.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals,

</span>
<span class="ltx_bibblock">“Listen, attend and spell,”

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1508.01211</span>, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Suyoun Kim, Takaaki Hori, and Shinji Watanabe,

</span>
<span class="ltx_bibblock">“Joint ctc-attention based end-to-end speech recognition using multi-task learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. IEEE, 2017, pp. 4835–4839.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi,

</span>
<span class="ltx_bibblock">“Hybrid ctc/attention architecture for end-to-end speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, pp. 1240–1253, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei,

</span>
<span class="ltx_bibblock">“Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2021, pp. 4054–4058, ISCA.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Sadeen Alharbi, Anthony J. H. Simons, Shelagh Brumfitt, and Phil D. Green,

</span>
<span class="ltx_bibblock">“Automatic recognition of children’s read speech for stuttering application,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Workshop on Child, Computer and Interaction</span>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Di Wu, Binbin Zhang, Chao Yang, Zhendong Peng, Wenjing Xia, Xiaoyu Chen, and Xin Lei,

</span>
<span class="ltx_bibblock">“U2++: unified two-pass bidirectional end-to-end model for speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2106.05642, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Qisheng Li and Shaomei Wu,

</span>
<span class="ltx_bibblock">“Towards fair and inclusive speech recognition for stuttering: Community-led chinese stuttered speech dataset creation and benchmarking,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">CHI EA ’24</span>, New York, NY, USA, 2024, Association for Computing Machinery.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng,

</span>
<span class="ltx_bibblock">“Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Oriental COCOSDA</span>, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
David Snyder, Guoguo Chen, and Daniel Povey,

</span>
<span class="ltx_bibblock">“MUSAN: A music, speech, and noise corpus,”

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/1510.08484, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur,

</span>
<span class="ltx_bibblock">“Audio augmentation for speech recognition.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, 2015, p. 3586.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger,

</span>
<span class="ltx_bibblock">“Montreal forced aligner: Trainable text-speech alignment using kaldi.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Interspeech</span>. 2017, pp. 498–502, ISCA.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin,

</span>
<span class="ltx_bibblock">“Attention is all you need,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</span>, 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Tedd Kourkounakis, Amirhossein Hajavi, and Ali Etemad,

</span>
<span class="ltx_bibblock">“Detecting multiple speech disfluencies using a deep residual network with bidirectional long short-term memory,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. IEEE, 2020, pp. 6089–6093.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton,

</span>
<span class="ltx_bibblock">“A simple framework for contrastive learning of visual representations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">ICML</span>. 2020, pp. 1597–1607, PMLR.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Werner Verhelst and Marc Roelands,

</span>
<span class="ltx_bibblock">“An overlap-add technique based on waveform similarity (wsola) for high quality time-scale modification of speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">1993 IEEE International Conference on Acoustics, Speech, and Signal Processing</span>. IEEE, 1993, pp. 554–557.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le,

</span>
<span class="ltx_bibblock">“Specaugment: A simple data augmentation method for automatic speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.08779</span>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim,

</span>
<span class="ltx_bibblock">“Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">ICASSP</span>. 2020, pp. 6199–6203, IEEE.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Hubert Siuzdak,

</span>
<span class="ltx_bibblock">“Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis,”

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2306.00814, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu Jeong Han, and Shinji Watanabe,

</span>
<span class="ltx_bibblock">“E-branchformer: Branchformer with enhanced merging for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">SLT</span>. 2022, pp. 84–91, IEEE.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda G. Shapiro, and Hannaneh Hajishirzi,

</span>
<span class="ltx_bibblock">“Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">ECCV</span>. 2018, pp. 561–580, Springer.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yifan Peng, Siddharth Dalmia, Ian R. Lane, and Shinji Watanabe,

</span>
<span class="ltx_bibblock">“Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">ICML</span>. 2022, pp. 17627–17643, PMLR.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.05429" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.05430" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.05430">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.05430" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.05431" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:56:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
